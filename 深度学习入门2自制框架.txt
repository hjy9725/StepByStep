














第1阶段 自

动微分············································

1

步骤

1 作为“箱子”的

变量 ··································

3

1.1 什么是

变量 ····································

3

1.2 实现Variable类

································· 4

1.3 （补充）NumPy的多维

数组 ························ 6

步骤2 创

建变量的函

数 ····································· 8

2.1 什么是函

数 ···································· 8

2.2 Function类的实现

······························· 9

2.3

使用Function类 ······························· 10

步骤

3

函数的连续

调用 ···································· 13

3.1

Exp函数的

实现 ································ 13

3.2

函数的

连续调用································ 14

步

骤4 数值微分

··········································

16

4.1 什么是导数

··································· 16

4.2 数值微分的

实现································ 17

4.3

复合函

数的导数································ 20

4.4 数

值微分存在

的问题

···························· 21

目录

vi

步骤5 反向传

播的理论知

识 ································

22

5.1 链式法则

····································· 22

5.2 反向传播的

推导································ 23

5.3

用计算

图表示·································· 25

步骤

6 手动进行反

向传播

·································· 27

6.1 Variable类的

功能扩展

·························· 27

6.2 Function类

的功能扩展

··························

28

6.3 Square类和Exp类的功

能扩展 ····················

28

6.4 反向

传播的实现

································ 29

步骤7 反向传

播的自动化

·································· 32

7.1

为反向传播

的自动化创

造条件 ···················· 33

7.2

尝试

反向传播·································· 36

7.3 增

加backward方法

····························· 38

步骤

8 从递归到循

环······································

40

8.1 现在的Variable类

······························ 40

8.2 使用循环实

现·································· 41

8.3

代码验证

····································· 42

步骤9 让函数

更易用······································

43

9.1 作为

Python函数使用 ···························

43

9.2 简

化backward方法 ·····························

45

9.3 只支

持ndarray································· 46

步骤10 测试

············································ 50

10.1

Python的单元测试

··························· 50

10.2 square函数反向传

播的测试

···················· 52

10.3 通

过梯度检验

来自动测试

······················

53

10.4 测试小结 ···································

54

目

录  vii

第2阶段 用

自然的代码

表达 ··································· 59

步骤11 可

变长参数（正

向传播篇) ··························· 61

11.1 修

改Function类 ····························· 62

11.2 Add类的实

现································ 64

步骤12

可变

长参数（改进

篇) ······························· 65

12.1

第1项改进

：使函数更容

易使用 ················· 65

12.2

第2项

改进：使函数

更容易实现

················· 67

12.3 add函数的实现

······························

69

步骤13 可变长

参数（反向传

播篇) ···························

70

13.1 支持可

变长参数的

Add类的反向传

播 ············

70

13.2 修改Variable类 ······························

71

13.3 Square类

的实现······························ 73

步骤

14 重复使用同

一个变量 ······························· 75

14.1 问

题的原因·································· 76

14.2

解

决方案 ··································· 77

14.3

重置

导数 ··································· 79

步骤15

复

杂的计算图

（理论篇）····························· 81

15.1 反向

传播的正确

顺序

·························· 82

15.2 当前的

DeZero

······························· 84

15.3 函数的优先

级································

87

步骤16 复杂

的计算图（实

现篇）····························· 88

16.1 增加“辈

分”变量····························· 88

16.2

按照

“辈分”顺序取

出元素 ····················· 90

16.3

Variable类的

backward ························ 92

16.4

代码验证 ··································· 93

步

骤17

内存管理

和循环引用

······························· 97

17.1 内存管理

··································· 97

17.2 引

用计数方式

的内存管理

······················

98

  目录

viii

17.3 循环引

用 ···································100

17.4 weakref模块·································102

17.5 代码

验证

···································104

步骤18 减

少内存使用

量的模式 ····························

106

18.1 不

保留不必要

的导数 ··························106

18.2 回顾

Function类 ·····························109

18.3

使用Config类进

行切换 ························110

18.4 模式

的切换··································111

18.5 使用

with语句切换····························112

步

骤19 让变量更

易用····································

116

19.1 命名变

量 ···································116

19.2 实例变量

ndarray ·····························117

19.3

len函数和print函数

··························119

步骤20 运算符

重载（1）··································· 122

20.1 Mul类的实

现 ································122

20.2

运算符重

载··································125

步骤21 运算

符重载（2）··································· 128

21.1 与ndarray一

起使用···························128

21.2 与float和

int一起使用·························130

21.3 问

题1：左项为float或

int的情况 ················131

21.4

问题

2：左项为ndarray实例

的情况 ··············133

步骤

22 运算符重载

（3）···································

134

22.1 负数 ·······································135

22.2 减法 ·······································136

22.3

除

法 ·······································138

22.4 幂运算

·····································139

步

骤23 打包 ···········································

141

23.1 文件

结构 ···································142

目录  ix

23.2

将

代码移到核

心类 ····························142

23.3 运算符

重载··································144

23.4 实际的

_ _init_ _.py文件

·······················146

23.5 导入dezero ··································147

步

骤24 复杂函数

的求导 ·································· 149

24.1 Sphere函数

·································150

24.2 matyas函数

·································151

24.3 GoldsteinPrice函数

··························152

第

3阶段 实现高

阶导数 ······································

161

步骤

25 计算图的可

视化（1）······························· 163

25.1 安装Graphviz ·······························163

25.2

使

用DOT语言描述

图形 ·······················165

25.3 指定节

点属性································165

25.4 连接

节点 ···································167

步骤26

计

算图的可视

化（2）······························· 169

26.1 可视化代

码的使用示

例

························169

26.2 从计算图

转换为DOT语言

·····················171

26.3

从DOT语言转换

为图像 ·······················174

26.4 代码

验证

···································176

步骤27 泰

勒展开的导

数 ··································

178

27.1 sin函数的实

现 ·······························178

27.2 泰勒展开

的理论知识

··························179

27.3 泰勒展开的

实现······························180

27.4 计算图

的可视化······························182

步

骤28 函数优化

········································

184

28.1 Rosenbrock函数 ·····························184

28.2 求导 ·······································185

目

录 x

28.3 梯度下降

法的实现

····························186

步

骤29 使用牛顿

法进行优化

（手动计算）····················· 190

29.1 使

用牛顿法进

行优化的理

论知识 ················191

29.2

使用

牛顿法实现

优化 ··························195

步骤30 高

阶导数（准备

篇）································

197

30.1 确认工作

①：Variable实例变量·················197

30.2

确

认工作②：Function类 ······················199

30.3 确

认工作③：Variable类的

反向传播

·············201

步

骤31 高阶导数

（理论篇）································ 204

31.1 在反

向传播时进

行的计算 ······················204

31.2

创

建反向传播

的计算图的

方法 ··················206

步骤32 高

阶导数（实现

篇）································

209

32.1 新的DeZero ·································209

32.2 函数

类的反向传

播 ····························210

32.3

实现更有

效的反向传

播（增加模式

控制代码）·······211

32.4 修

改_ _init_

_.py ·····························213

步骤33 使用

牛顿法进行

优化（自动计

算）·····················

215

33.1 求二阶导

数··································215

33.2

使用牛顿

法进行优化

··························217

步骤34 sin函数的

高阶导数 ·······························

219

34.1 sin函

数的实现 ·······························219

34.2 cos函

数的实现·······························220

34.3 sin函

数的高阶导

数

···························221

步骤35 高阶

导数的计算

图 ································

225

35.1 tanh函数的导

数······························226

35.2

tanh函数的实

现······························226

35.3 高阶导数

的计算图可

视化 ······················227

步骤36 DeZero的

其他用途 ······························· 234

目

录  xi

36.1

double backprop的用途 ·······················234

36.2

深

度学习研究

中的应用示

例 ····················236

第4阶段 创

建神经网络

······································

243

步骤37 处理张

量········································ 245

37.1 对各元素

进行计算 ····························245

37.2

使

用张量时的

反向传播 ························247

37.3 使

用张量时的

反向传播（补

充内容）···············249

步骤

38 改变形状的

函数 ·································· 254

38.1 reshape函数的

实现 ···························254

38.2

从Variable对象

调用reshape····················258

38.3 矩阵的

转置··································259

38.4

实际的

transpose函数（补充内

容）················262

步骤39 求和

的函数······································ 264

39.1 sum函数

的反向传播

··························264

39.2 sum函数的实现

······························266

39.3 axis和keepdims ······························268

步骤40

进行

广播的函数

·································· 272

40.1 broadcast_to函数和sum_to函数

················272

40.2 DeZero的broadcast_to函数和sum_to函

数········275

40.3 支持广播

···································277

步骤41 矩阵的

乘积······································ 280

41.1

向量的

内积和矩阵

的乘积 ······················280

41.2 检查

矩阵的形状

······························282

41.3 矩阵乘积的

反向传播 ··························282

步

骤42

线性回归

········································ 288

42.1 玩具数据集

··································288

42.2 线性回归的

理论知识 ··························289

目

录 xii

42.3 线性回归

的实现······························291

42.4 DeZero的mean_squared_error函

数（补充内容

）·····295

步骤43 神经网

络········································

298

43.1 DeZero中的linear函数

························298

43.2

非线性数据

集································301

43.3 激活函数

和神经网络

··························302

43.4

神经网络的

实现······························303

步骤44 汇

总参数的层

···································· 307

44.1 Parameter类的实现···························307

44.2 Layer类

的实现·······························309

44.3 Linear类的

实现 ······························312

44.4

使用Layer实

现神经网络

·······················314

步骤45 汇总层

的层······································ 316

45.1 扩展Layer类

································316

45.2 Model类····································319

45.3 使用Model来解

决问题 ························321

45.4

MLP类·····································323

步

骤46 通过Optimizer更新

参数··························· 325

46.1 Optimizer类·································325

46.2 SGD类的

实现

·······························326

46.3 使用SGD类

来解决问题

·······················327

46.4

SGD以外的优化

方法··························328

步骤47 softmax函

数和交叉熵

误差 ·························

331

47.1 用于切

片操作的函

数 ··························331

47.2 softmax函数 ································334

47.3

交叉

熵误差··································337

步骤

48 多分类·········································· 340

48.1 螺旋

数据集··································340

目录

xiii

48.2 用于训练的

代码······························341

步骤49 Dataset类

和预处理

······························· 346

49.1 Dataset类

的实现·····························346

49.2 大型

数据集的情

况 ····························348

49.3

数据的连

接··································349

49.4 用于训练

的代码······························350

49.5

数据

集的预处理

······························351

步骤50 用于取

出小批量数

据的DataLoader ··················

354

50.1 什么是

迭代器································354

50.2

使用

DataLoader ·····························358

50.3 accuracy函数的实现

··························359

50.4 螺旋数据集

的训练代码

························360

步骤51 MINST的训练

···································

363

51.1 MNIST数据集 ·······························364

51.2 训练

MNIST ·································366

51.3

改进模型 ···································368

第

5阶段 DeZero高级挑

战···································

377

步骤52 支持

GPU ·······································

379

52.1 CuPy的安装和使

用方法 ·······················379

52.2 cuda模块

···································382

52.3 向Variable

/ Layer / DataLoader类添加代

码

·····383

52.4 函数的相

应修改······························386

52.5

在GPU上

训练MNIST ························388

步骤53 模

型的保存和

加载

································ 391

53.1 NumPy的save函数

和load函数

·················391

53.2 Layer类参

数的扁平化

·························394

53.3

Layer类的save函数和

load函数 ·················395

目录 xiv

步

骤54 Dropout和测试模

式

······························ 398

54.1 什么是Dropout

······························398

54.2 Inverted Dropout·····························401

54.3 增

加测试模式

································401

54.4 Dropout的实现

······························402

步骤

55 CNN的机制（1）·································· 404

55.1 CNN的网

络结构 ·····························404

55.2

卷积

运算 ···································405

55.3 填充

·······································407

55.4 步

幅 ·······································408

55.5 输出大小

的计算方法

··························409

步骤56 CNN的机制

（2）··································

411

56.1 三阶张量 ···································411

56.2 结

合方块进行

思考 ····························412

56.3

小批量

处理··································414

56.4 池化层

·····································415

步骤57

conv2d函数和

pooling函数························· 418

57.1 使用im2col展

开······························418

57.2 conv2d函数的实

现 ···························420

57.3

Conv2d层的实现

·····························425

57.4 pooling函数的实现

···························426

步骤58

具有代

表性的CNN（VGG16）······················· 429

58.1 VGG16的实

现·······························429

58.2 已训练的

权重数据 ····························431

58.3

使

用已训练的

VGG16 ·························435

步骤59 使用RNN处

理时间序列

数据

······················· 438

59.1 RNN层的实

现

·······························438

59.2 RNN模型的实

现 ·····························442

目录  xv

59.3

切断

连接的方法

······························445

59.4 正弦波的预

测································446

步骤60

LSTM与数

据加载器 ······························ 451

60.1

用

于时间序列

数据的数据

加载器 ················451

60.2 LSTM层的

实现

······························453

附录A inplace运

算（步骤14的补

充内容）·····················

463

A.1 问题

确认··········································· 463

A.2 关于复

制和覆盖 ····································· 464

A.3 DeZero的

反向传播 ·································· 465

附

录B 实现get_item函数

（步骤47的补充

内容）················· 466

附录C

在

Google Colaboratory上运行 ······················· 469

后 记

····················································· 473

参考文献

··················································· 477











，本书由60个步

骤构成，这60个

步骤又可分

为如图2所示

的5

个阶段。下

面笔者简单

介绍一下每

个阶段的内

容。

前 言  xxi

图2 本

书结构

步骤

1~10

步骤11~24

步骤 25~36

步

骤37~51

步骤

52~60

第1阶

段

第2阶段

第

3阶段

第4阶段

第5阶段

自动

微分

用自然

的代码表达

实现高阶导

数

创建神经

网络

DeZero高级挑

战

 在第1阶段

，我们会为创

建DeZero打好基础

。在这个阶段

，我们只

处理

简单的问题

，用最短的时

间实现自动

微分的功能

（在阅读本书

的

过程中，读

者会逐渐理

解自动微分

的意思）。

 在第

2阶段，我们将

扩展DeZero，使其能

以更自然的

代码来表达

。

在第2阶段结

束时，我们应

该就能使用

平常用到的

Python语法，比

如if语

句和for语句等

来编写代码

。

 在第3阶段，我

们将继续扩

展DeZero，使它可以

计算二阶导

数。要

做到这

一点，需要使

DeZero实现“反向传

播的反向传

播”。通过了解

其工作原理

，我们会发现

DeZero和现代深度

学习框架中

蕴含的新的

可能性。

 在第

4阶段，我们会

让DeZero实现神经

网络的功能

。这样一来，我

们就能用DeZero轻

松地构建神

经网络。

 最后

在第5阶段，我

们将增加对

GPU的支持、模型

的保存和恢

复等

深度学

习必备的功

能，还会研究

CNN和RNN等更先进

的模型。在深

度学习的应

用方面，这些

都是很重要

的问题，但这

些问题都不

好解

决。不过

，在这个阶段

我们会了解

到，（具备DefinebyRun能力

的）

DeZero可以用简

单的代码来

解决这些问

题。

  前

言 xxii

本书

最后完成的

DeZero 已经被发布

到了PyPI（Python

Package Index）

中。PyPI是Python的

软件包仓库

。大家可以在

命令行使用

pip install

dezero命令安装DeZero。当

然，也可以在

本书创建的

DeZero的基础上开

发自己的框

架，并在网上

公开。

踏上创

建DeZero之旅

以上

就是本书的

大致内容。总

结一下，本书

介绍了从零

开始创建原

创框

架DeZero的过

程。DeZero是一个小

而强大的框

架，我们将通

过60个步骤来

完成它。在这

一过程中，读

者会加深对

PyTorch、Chainer和TensorFlow等

现代深

度学习框架

的理解。

最后

再次强调，本

书的目的不

仅仅是创建

原创框架DeZero，更

重要的

是通

过创建DeZero的旅

程，让大家拥

有观察现代

深度学习框

架的“新的视

角”。

拥有了新

的视角，我们

就能更广泛

、更深入地探

索深度学习

领域。发现这

个

新的视角

的过程，才是

本书真正的

价值所在。

好

了，准备工作

完成了，下面

就让我们开

启创建DeZero之旅

吧！

必要的软

件

本书使用

的Python版本及外

部库如下所

示。

 Python 3



NumPy

 Matplotlib



CuPy（可选）

 Pillow（可选

）

DeZero还提供了能

在NVIDIA的GPU上运行

的功能作为

可选项。要实

现这个功能

，我们需要用

到名为CuPy的外

部库。同样，我

们还需要选

配

前 言  xxiii

Pillow这个

图像处理库

。除Python之外，本书

还使用了以

下软件。

 Graphviz

笔者

将在步骤25介

绍如何安装

Graphviz。

文件构成

本

书使用的代

码可以从以

下网址获取

。

ituring.cn/book/2863

各文件夹的

内容如表1所

示。

表1 文件夹

的内容

文件

夹名 说明

dezero DeZero的

源代码

examples 使用

DeZero开发的示例

steps

各步骤的代

码文件（step01.py～step60.py）

tests DeZero的单

元测试

steps文件

夹中的step01.py、step02.py等文

件与本书各

步骤中创建

的文

件一一

对应。我们可

以通过以下

Python命令运行这

些文件（可以

在任何目录

下运行Python命令

）。



$ python steps/step01.py

$



python steps/step02.py



$ cd steps

$



python step31.py

 1

第 1阶段

自动

微分

导数广

泛应用在现

代科学技术

的各个领域

，尤其在包括

深度学习在

内的

机器学

习的各个领

域，导数起着

核心作用。从

某种意义上

来说，深度学

习框

架就是

计算导数的

工具。因此，本

书的主题自

然包括导数

。换言之，如何

利

用计算机

求导是本书

的一个重要

知识点。

马上

要进入的第

1阶段共包括

10个步骤。在这

个阶段，我们

将创建自动

微分的机制

。这里所说的

自动微分指

的是由计算

机（而不是人

）来计算导数

。

具体来说，就

是指在对某

个计算（函数

）编码后，由计

算机自动求

出该计算

的

导数的机制

。

在这个阶段

，我们将创建

代表变量和

函数的两个

类（Variable类和

Function类）。让

人惊讶的是

，有了这两个

类，我们就为

自动微分打

好了基础。

在

第1阶段结束

时，对于简单

的计算（函数

），DeZero应该可以自

动求出它的

导数了。下面

就进入创建

DeZero的首个阶段

吧！

  第

1阶段　自

动微分 2

步骤

1

作为“箱子”的

变量  3

步骤

1

作

为“箱子”的变

量

本书的第

1个步骤是创

建DeZero的组成元

素——变量。变量

是DeZero

最重要的

组成部分。在

这个步骤中

，我们将思考

变量是如何

工作的，并实

现

变量的功

能。

1.1 什么是变

量

首先来看

一下什么是

变量。打开编

程入门类图

书，你会发现

这些图书大

多用类似于

图1-1的示意图

来解释变量

。

图1-1 变量的示

意图

第 1阶段

自动微分 4

然

后这些图书

会告诉你，“箱

子”就是变量

，里面可以存

放数据。这种

把变量比作

“箱子”的说法

（在一定程度

上）很好地解

释了变量的

特点。变量

的

要点可以总

结如下。

“箱子

”和数据是不

同的东西

“箱

子”里可以存

放数据（=赋值

）



朝“箱子”里看

一看就能知

道数据是什

么（=引用）

下面

来实现DeZero的变

量，使它具有

示意图中“箱

子”的特点。

1.2 实

现Variable类

变量的

英语是variable。这里

我们把DeZero的变

量以Variable类的形

式

实现。补充

一句，类名首

字母大写是

Python的一条常见

的编码规则

。这条

规则记

载在Python的PEP8编码

规范中。

下面

编写代码，让

Variable类具备“箱子

”的功能。实现

这个功能的

最

简代码如

下所示。

steps/step01.py

class Variable:

def __init__(self, data):

self.data = data

上面

的代码所做

的是在初始

化时，将传来

的参数设置

为实例变量

data。

虽然这段代

码很简单，但

它可以让Variable类

作为“箱子”使

用，因为实际

的数据被保

存在Variable的data里。结

合下面的使

用示例，我们

能更清晰地

看出这一点

。

步骤 1 作为“箱

子”的变量

5

steps/step01.py

import numpy

as np

data =

np.array(1.0)

x = Variable(data)

print(x.data)

运

行结果

1.0

这个

例子使用了

NumPy的多维数组

来保存放入

“箱子”里的数

据。例子

中的

x是一个Variable实例

，实际的数据

在x里面。换言

之，x不是数据

，而

是存放数

据的实体，也

就是一个存

放数据的“箱

子”。A

本书在展

示代码时，会

在代码的右

上角标明它

出自哪个文

件。在上面的

示

例代码中

，代码的右上

角写着steps/step01.py，这表

示你可以在

本书

的随书

下载文件A 中

的step/step01.py中找到这

段代码。有些

示例代码

的

右上角没有

记载文件名

，这种情况意

味着随书下

载文件里没

有对应的代

码文件。

机器

学习系统使

用多维数组

作为底层数

据结构。因此

，DeZero

的

Variable类要设计

成只能处理

NumPy多维数组的

形式。NumPy多维数

组的

类是numpy.ndarray（np.ndarray）。如

前面的代码

所示，我们可

以使用np.array

函数

来创建它的

实例。另外，本

书后面在提

到numpy.ndarray实例时，都

将

其简称为

ndarray实例。

接下来

为前面代码

中的x赋新的

数据。代码编

写如下。

steps/step01.py

x.data

= np.array(2.0)

print(x.data)

运行

结果

2.0

A 随书下

载文件通过

前言中提到

的网址获取

。——编者注

第 1阶

段　自动微分

6

如这段代码

所示，只要按

照x.data

= ...的方式编

写，x就会被赋

上新

的数据

。这样Variable类就可

以作为一个

“箱子”使用了

。

以上就是步

骤1实现的所

有代码。虽然

现在Variable类只有

3行代码，

但我

们会在此基

础上将DeZero打造

成一个现代

框架。

1.3 （补充）NumPy的

多维数组

最

后简单介绍

一下NumPy的多维

数组。多维数

组是一种数

值等元素按

照一定规则

排列的数据

结构。元素的

排列顺序是

有方向的，这

个方向叫作

维

或轴。图1-2是

一个多维数

组的示例。

标

量 向量 矩阵

1

1 2 3 1

2 3

4 5

6

图1-2 多维数组

的示例

图1-2中

从左到右依

次为零维矩

阵、一维矩阵

、二维矩阵，它

们分别叫

作

标量、向量和

矩阵。标量就

是简单的一

个数。数沿着

一个轴排列

的是向量，

数

沿着两个轴

排列的是矩

阵。

多维数组

也叫张量。图

1-2中从左到右

也可以分别

叫作零阶张

量、一阶张

量

和二阶张量

。

NumPy 的 ndarray实例中有

一个实例变

量叫 ndim，ndim是

number of 

dimensions的缩

写，表示多维

数组的“维度

”。下面我们实

际用一用它

。

步骤 1 作为“箱

子”的变量

7

>>> import numpy

as np

>>> x

= np.array(1)

>>> x.ndim

0

>>> x =

np.array([1, 2, 3])

>>>

x.ndim

1

>>> x

= np.array([[1, 2, 3],

... [4, 5, 6]])

>>> x.ndim

2

上

面的代码是

启动了Python解释

器的交互模

式后，在交互

模式下运行

的（在交互模

式下，代码前

会出现>>>符号

）。如上所述，通

过实例变量

ndim，

我们可以知

道数组的维

度。

只有在处

理向量的时

候，我们才需

要注意“维度

”这个词。比如

有一个向

量

np.array([1, 2,

3])，由于它有3个

元素，所以也

叫三维向量

。向量

的维度

指的是向量

中元素的数

量。而在提到

三维数组时

，数组的维度

是指

轴（而不

是元素）的数

量是3。

如上所

述，ndarray实例可以

用来创建标

量、向量、矩阵

，甚至更高阶

的张量。不过

本书暂时只

涉及标量的

计算。后面（从

步骤37开始）我

们将扩

展DeZero，使

其支持向量

和矩阵的处

理。

  第

1阶段　自

动微分 8

步骤

2

创建变量的

函数

在上一

步骤中，Variable类可

以作为一个

“箱子”使用了

。不过，现在

的

它还只是一

个“普通箱子

”，我们需要一

个机制把这

个“普通箱子

”变为“魔

法箱

子”。要实现这

一点，关键就

在于函数。本

步骤的主题

是函数。

2.1 什么

是函数

什么

是函数？函数

是“定义一个

变量与另一

个变量之间

的对应关系

的规

则”。这个

说法有些生

硬，我们来看

一个具体的

例子。有一个

计算平方的

函

数f(x) = x2

。这时设

y

= f(x)，那么变量y和

x之间的关系

就由函数f决

定。

换言之，“y是

x的平方”的关

系是由函数

f决定的。

从这

个例子可以

看出，函数具

有定义变量

之间对应关

系的作用。图

2-1

是变量与函

数的关系示

意图。

图2-1 变量

和函数的关

系示意图

图

2-1直观地展示

了变量x和y与

函数f之间的

关系。这种用

节点和箭头

来表示计算

的图叫作计

算图。本书用

圆框表示变

量，用方框表

示函数。

步骤

2 创建变量的

函数  9

提到图

，大家可能会

想到柱状图

、饼状图等。不

过在计算机

科学领域，图

是由节点和

边组成的数

据结构。

2.2 Function类的

实现

下面我

们从编程的

角度思考图

2-1表示的函数

。具体来说，就

是假设变

量

x和y是之前实

现的Variable实例，然

后以Function类的形

式实现可以

处

理它们的

函数f。这里有

两点需要注

意。

 在Function类中实

现的方法，其

输入应为Variable实

例，输出应为

Variable实例

 Variable实例的

实际数据存

在于实例变

量data中

在满足

这两点的基

础上，将Function类按

以下方式实

现。

class

Function:

 def __call__(self,

input):

 x =

input.data # 取出数据

y

= x ** 2

# 实际的计算

output =

Variable(y) # 作为Variable返回

return output

上

面的代码实

现了__call__方法。__call__方

法接收input参数

，这里

假定传

来的input是Variable实例

。因此，实际数

据存放在input.data中

。

取出数据后

，方法会执行

相应的计算

（在这个例子

中是求平方

），然后将结果

放到Variable“箱子”里

并返回。

  第

1阶

段　自动微分

10

__call__方法是一个

特殊的Python方法

。定义了这个

方法后，当f =

Function()时

，就可以通过

编写f(...)来调用

__call__方法了。

2.3 使用

Function类

现在我们

来实际使用

一下Function类。这里

将Variable实例的x输

入

Function实例的f中

。

x = Variable(np.array(10))

f = Function()

y

= f(x)

print(type(y)) #

使用type()，获取对

象的类型

print(y.data)

运

行结果

<class

'__main__.Variable'>

100

上面

的代码把Variable和

Function结合起来使

用了。从运行

结果可以看

出，

y的类型是

Variable，其数据存储

在y.data中。

这里实

现的Function类是一

个“对输入值

求平方”的具

体函数。因此

，

将其命名为

Square这样的具体

名称比较合

适。此外，今后

我们还将增

加各种

函数

（如Sin函数和Exp函

数等）。考虑到

这一点，最好

把Function类作为基

类

来实现，并

在这个类的

内部实现所

有DeZero函数都有

的功能。这里

，我们

重新设

计DeZero的函数，以

满足以下两

点要求。

 Function类是

基类，实现所

有函数通用

的功能



具体

函数是在继

承了Function类的类

中实现的

步

骤 2 创建变量

的函数

11

为了

满足这两点

，我们将Function类按

下面的方式

实现。

steps/step02.py

class

Function:

 def __call__(self,

input):

 x =

input.data

 y =

self.forward(x) # 具体的

计算在forward方法

中进行

output = Variable(y)

return output

 def

forward(self, x):

 raise

NotImplementedError()

本节

实现了两个

方法：__call__和forward。__call__方法

执行两项任

务：

从Variable中取出

数据和将计

算结果保存

到Variable中。其中具

体的计算是

通过调用forward方

法完成的。forward方

法的实现会

在继承类中

完成。

Function类的forward方

法会抛出一

个异常，目的

是告诉使用

了Function

类的forward方法

的人，这个方

法应该通过

继承来实现

。

下面实现一

个继承自Function类

并对输入值

进行平方的

类。这个类的

名

字是Square，代码

如下所示。

steps/step02.py

class Square(Function):

 def

forward(self, x):

 return

x ** 2

Square类

继承自Function类，所

以也继承了

__call__方法。因此，我

们

只需在forward方

法中编写具

体的计算代

码，就能完成

Square类的实现。接

下来使用Square类

对Variable进行如下

处理。

  第

1阶段

自动微分 12

steps/step02.py

x

= Variable(np.array(10))

f =

Square()

y = f(x)

print(type(y))

print(y.data)

运

行结果

<class

'__main__.Variable'>

100

可以

看出，得到的

结果和之前

的一样。这样

我们就完成

了步骤2，实现

了Variable和Function类的基

本功能。

我们

暂时只考虑

Function的输入和输

出仅有一个

变量的情况

。从步骤11

开始

，我们将扩展

DeZero，以支持多个

变量。

步骤 3 函

数的连续调

用

13

步骤 3

函数

的连续调用

目前已经创

建了DeZero的变量

和函数。我们

还在上一个

步骤实现了

一

个名为Square的

用于计算平

方的函数类

。在本步骤中

，我 们会实现

一个新

的函

数，并将多个

函数连结起

来进行计算

。

3.1

Exp函数的实现

首先在DeZero中实

现一个新函

数。这里我们

要实现的是

y = ex的计算（其

中

，e是自然对数

，具体值为e

= 2.718 ...)。代

码如下所示

。

steps/step03.py

class Exp(Function):

 def

forward(self, x):

 return

np.exp(x)

与Square类的实现

过程一样，继

承Function类，并在forward方

法中实

现要

计算的内容

。与Square类唯一不

同的是，forward方法

的内容由x **

2

变

成了np.exp(x)。

第 1阶段

自动微分 14

3.2

函

数的连续调

用

Function类的__call__方法

的输入和输

出都是Variable实例

。因此，DeZero

函数自

然可以连续

使用。比如 y

= (ex2

)

2

的

计算，我们可

以像下面这

样编写

它的

计算代码。

steps/step03.py

A

= Square()

B =

Exp()

C = Square()

x = Variable(np.array(0.5))

a

= A(x)

b =

B(a)

y = C(b)

print(y.data)

运

行结果

1.648721270700128

上面

的代码连续

使用了A、B、C这3个

函数。这里的

关键点是中

途出现的

4个

变量x、a、b、y都是Variable实

例。由于Function类的

__call__方法的输

入

和输出都是

Variable实例，所以可

以像上面的

代码一样连

续使用多个

函数。

如图3-1所

示，这里进行

的计算可以

用函数和变

量交替排列

的计算图来

表示。

x

A a B b

C y

图3-1 使用

多个函数的

计算图（圆框

代表变量，方

框代表函数

）

我们可以把

图3-1那种依次

应用多个函

数创建的变

换看作一个

大的函数。

这

种由多个函

数组成的函

数叫作复合

函数。值得注

意的是，即使

组成复合

函

数的每个函

数都是简单

的计算，我们

也可以依次

连续使用它

们来执行复

杂的计算。

步

骤

3 函数的连

续调用  15

为什

么要用计算

图来表示一

系列的计算

呢？答案是这

样能高效地

求出每

个变

量的导数（准

确来说，是做

好了这样的

准备）。这个算

法就是反向

传播（back 

propagation）。从下一

个步骤开始

，我们将扩展

DeZero，以支持反向

传播。

第 1阶段

自动微分 16

步

骤 4

数值微分

现在，我们已

经实现了Variable类

和Function类。实现这

些类的目的

是

自动微分

。在本步骤中

，我们会先复

习导数，并尝

试用一种叫

作数值微分

（numerical

differentiation）的方法来求

导。从下一个

步骤开始，我

们将实现

一

种替代数值

微分的更高

效的算法——反

向传播。

导数

不仅仅在机

器学习领域

，在其他领域

也很重要。从

流体力学、金

融工

程到气

象模拟、工程

设计优化，很

多领域需要

进行导数计

算，而且这些

领

域实际上

都在使用自

动微分的功

能。

4.1 什么是导

数

什么是导

数？简单地说

，导数是变化

率的一种表

示方式。比如

某个物体

的

位置相对于

时间的变化

率就是位置

的导数，即速

度。速度相对

于时间的变

化率就是速

度的导数，即

加速度。像这

样，导数表示

的是变化率

，它被定义

为

在极短时间

内的变化量

。函数f(x)在x处的

导数可用下

面的式子表

示。

f

(x) = limh→0

f(x

+ h) − f(x)

h (4.1)

式子4.1中的

h

lim

→0 表示极限，意

思是h应尽可

能地接近0。式

子4.1中的

步骤

4 数值微分

17

f(x+h)−f(x)

h 为

图4-1中通过两

点的直线的

斜率。

图4-1 曲线

y = f(x)和通过其两

点的直线

如

图4-1所示，函数

f(x)在x和x + h两点之

间的变化率

为 f(x+h

h

)−f(x)

。

让h的值尽

可能地接近

0，就可以求出

x处的变化率

。这就是y

= f(x)的导

数。

另外，在y =

f(x)的

可导区间内

，对于该区间

内的任何x，式

子4.1都成立。

因

此，式子4.1中的

f′

(x)也是一个函

数，我们称之

为f(x)的导函数

。

4.2

数值微分的

实现

下面根

据导数的定

义式，即式子

4.1来实现求导

。需要注意的

是，计

算机不

能处理极限

值。因此，这里

的h表示一个

近似值。例如

我们可以用

h =

0.0001（＝1e-4）这种非常小

的值来计算

式子4.1。利用微

小的差值获

得函

数变化

量的方法叫

作数值微分

。

数值微分使

用非常小的

值h求出的是

真的导数的

近似值。因此

，这个值

第 1阶

段　自动微分

18

包含误差。中

心差分近似

是减小近似

值误差的一

种方法。中心

差分近似计

算

的不是f(x)和

f(x + h)的差，而是f(x −

h)和

f(x + h)的差。图4-2中的

粗

线表示的

就是中心差

分近似。

真的

导数

前向差

分近似

中心

差分近似

图

4-2

比较真的导

数、前向差分

近似和中心

差分近似

如

图4-2所示，计算

过x和x + h这两点

的直线的斜

率的方法称

为前向差

分

近似，计算x − h和

x +

h这两点间斜

率的方法称

为中心差分

近似，中心

差

分近似实际

产生的误差

更小。这一点

的证明超出

了本书的范

围，不过我们

从图4-2中直线

的斜率可以

直观地得出

这个结论。中

心差分近似

的直线斜率

为 f(x+h)

2

−

h

f(x−h)

（注意分母

是2h）。

中心差分

近似比前向

差分近似更

接近真的导

数这个结论

可以通过泰

勒展开

来证

明。相关证明

过程请阅读

参考文献[1]等

资料。

下面我

们来实现使

用中心差分

近似求数值

微分的函数

，该函数的名

称为

步骤

4 数

值微分  19

numerical_diff(f, x, eps=1e-4)。这里

的参数f是被

求导的函数

，是Function

类的实例

。参数x为求导

的变量，是Variable类

的实例。参数

eps代表一个

微

小的值，默认

值是1e-4 A

（eps是epsilon的缩

写）。数值微分

可通过以下

代

码实现。

steps/step04.py

def numerical_diff(f, x,

eps=1e-4):

 x0 =

Variable(x.data - eps)

x1 = Variable(x.data +

eps)

 y0 =

f(x0)

 y1 =

f(x1)

 return (y1.data

- y0.data) / (2

* eps)

只

要注意Variable实例

变量data中包含

实际数据即

可。下面我们

使用这

个函

数求步骤3中

实现的Square类的

导数。

steps/step04.py

f = Square()

x = Variable(np.array(2.0))

dy

= numerical_diff(f, x)

print(dy)

运行结

果

4.000000000004

由上面的

运行结果可

知，y =

x2 在 x =

2.0 时计算

得到的导数

值为

4.000000000004。不包含

误差的导数

值是4.0，可以说

这个结果大

体正确。

导数

也可以通过

解析解的方

式求解。解析

解的方式求

解是指只通

过式子的

变

形推导出答

案。在上面的

例子中，根据

导数的公式

可知，y = x2的导数

为 d

d

x

y =

2x（d

d

x

y

是y对x求导

的符号）。因此

，y = x2在x =

2.0处的导数

为4.0。这个4.0是不

包含误差的

值。前面的数

值微分结果

虽然不是正

好为

4.0，但我们

可以看出误

差是相当小

的。

A 1e-4表示0.0001。

第 1阶

段　自动微分

20

4.3 复合函数的

导数

我们在

前面接触的

是y =

x2这种简单

的函数。接下

来尝试对复

合函数求

导

。下面求 y =

(ex2

)

2 的导

数

d

d

x

y。代码如下

所示。

steps/step04.py

def f(x):

A = Square()

B = Exp()

C = Square()

return C(B(A(x)))

x =

Variable(np.array(0.5))

dy = numerical_diff(f,

x)

print(dy)

运行结

果

3.2974426293330694

上面的代

码将一系列

的计算组合

成了一个名

为f的函数。函

数在Python

中也是

对象，所以可

以作为参数

传给其他函

数。在上面的

例子中，函数

f作

为参数传

给了numerical_diff函数。

运

行上面的代

码，得到的导

数是3.297...。这意味

着，如果x从0.5变

成

一个微小

的值，y值的变

化幅度将是

这个微小值

的3.297...倍。

现在我

们已经成功

实现了“自动

”求导。只要用

代码来定义

要完成的计

算（前面的例

子定义了函

数f），程序就会

自动求出导

数。使用这种

方法，无

论多

么复杂的函

数组合，程序

都能自动求

出导数。今后

函数的种类

越来越多

的

话，不管是什

么计算，只要

是可微函数

，程序就能求

出它的导数

。不过遗

憾的

是，数值微分

的方法存在

一些问题。

步

骤 4 数值微分



21

4.4 数值微分存

在的问题

数

值微分的结

果包含误差

。在多数情况

下，这个误差

非常小，但在

一些

情况下

，计算产生的

误差可能会

很大。

数值微

分的结果中

容易包含误

差的主要原

因是“精度丢

失”。中心差分

近

似等求差

值的方法计

算的是相同

量级数值之

间的差，但由

于精度丢失

，计

算结果中

会出现有效

位数减少的

情况。以有效

位数为4的情

况为例，在计

算两个相近

的值之间的

差时，比如1.234

− 1.233，其

结果为0.001，有效

位数只有1位

。本来可能是

1.234... −1.233... =

0.001434... 之类的结果

，

但由于精度

丢失，结果变

成0.001。同样的情

况也会发生

在数值微分

的差

值计算

中，精度丢失

使结果更容

易包含误差

。

数值微分更

严重的问题

是计算成本

高。具体来说

，在求多个变

量的导数时

，

程序需要计

算每个变量

的导数。有些

神经网络包

含几百万个

以上的变量

（参数），

通过数

值微分对这

么多的变量

求导是不现

实的。这时，反

向传播就派

上了用

场。从

下一个步骤

开始，笔者将

介绍反向传

播。

另外，数值

微分可以轻

松实现，并能

计算出大体

正确的数值

。而反

向传播

是一种复杂

的算法，实现

时容易出现

bug。我们可以使

用数值微分

的结果检查

反向传播的

实现是否正

确。这种做法

叫作梯度检

验（gradient 

checking），它是一种

将数值微分

的结果与反

向传播的结

果进行比较

的方法。

步骤

10实现了梯度

检验。

  第

1阶段

自动微分 22

步

骤 5

反向传播

的理论知识

我们通过数

值微分成功

求出了导数

。但是，数值微

分在计算成

本和精度

方

面存在问题

。反向传播可

以解决这两

个问题。也就

是说，反向传

播不仅能

高

效地求导，还

能帮助我们

得到误差更

小的值。本步

骤只介绍反

向传播的理

论知识，不对

其进行实现

。从下一个步

骤开始，我们

再去实现反

向传播。

5.1

链式

法则

理解反

向传播的关

键是链式法

则（连锁律）。链

（chain）可以理解为

链条、

锁链等

，在这里表示

多个函数连

接在一起使

用。链式法则

意为连接起

来的多

个函

数（复合函数

）的导数可以

分解为各组

成函数的导

数的乘积。

下

面看一个链

式法则的具

体例子。假设

有一个函数

y = F(x)，这个函数

F由

3个函数组成

：a

= A(x)、b = B(a)和y

= C(b)。该函数的

计算图如图

5-1

所示。

图5-1

复合

函数的例子

这时，y对x的导

数可以用式

子5.1表示。

步骤

5 反向传播的

理论知识

23

dy

dx =

dy

db

db

da

da

dx （5.1）

如

式子5.1所示，y对

x的导数可以

表示为各函

数的导数的

乘积。换言之

，

复合函数的

导数可以分

解为各组成

函数导数的

乘积。这就是

链式法则。式

子

5.1所表示的

链式法则也

可以像下面

这样写成包

含 d

d

y

y 的形式。

dy

dx = dy

dy

dy

db

db

da

da

dx （5.2）

dy

dy 是

对自身求导

，导数值永远

为1。计算时通

常省略 d

d

y

y 这种

针对自身

的

导数，但考虑

到反向传播

的实现，这里

特意加上了

这一项。

dy

dy 是y对

y的导数。即使

y变化的幅度

很小，y自身也

会变化同样

大小的值。

因

此，不管对于

什么函数，变

化率总是1。

5.2 反

向传播的推

导

下面仔细

观察式子5.2。式

子5.2表示复合

函数的导数

可以分解为

各函

数导数

的乘积。但是

，它并没有规

定各导数相

乘的顺序。当

然，这一点我

们可以自由

决定。这里，我

们按照式子

5.3的方式以输

出到输入的

顺序进行

计

算A。

d

d

x

y =

d

d

y

y

d

d

y

b

d

d

a

b



d

d

x

a （5.3）

式子5.3按照

从输出到输

入的顺序进

行导数的计

算，计算方向

与平时相反

。

这时，式子5.3的

计算流程如

图5-2所示。

A 我们

也可以考虑

改变括号的

位置使计算

按照输入到

输出的方向

依次进行。这

种计算方法

叫作“前向

模

式的自动微

分”。关于前向

模式的自动

微分，请阅读

步骤10的专栏

部分。

  第

1阶段

自动微分 24

①

②

③

图

5-2 从输出端的

导数开始依

次进行计算

的流程（参见

彩图）

在图5-2中

，导数是按照

从输出y到输

入x的方向依

次相乘计算

得出的。

通过

这种方法，最

终得到 d

d

x

y。图5-3是

相应的计算

图。

图5-3 求 的计

算图

下面仔

细观察图5-3。我

们先从 d

d

y

y (= 1)开始

，计算它与 d

d

y

b 的

乘积。这

里的

d

d

y

b

是函数y=C(b)的导

数。因此，如果

用C′

表示函数

C的导函数，我

们就可以把

式子写成 d

d

y

b = C

(b)。同

样，有 d

d

a

b = B

(a)，dx

da = A

(x)。基

于以

上内容，图5-3可

以简化成图

5-4。

步骤 5

反向传

播的理论知

识  25

图5-4

简化后

的反向传播

计算图（A′

(x)的乘

法在图中简

化表示为节

点A′

(x)）

图5-4中把导

函数和乘号

合并表示为

一个函数节

点。这样导数

计算的流

程

就明确了。从

图5-4中可以看

出，“y对各变量

的导数”从右

向左传播。这

就是反向传

播。这里重要

的一点是传

播的数据都

是y的导数。具

体来说，就

是

d

d

y

y、 d

d

y

b 、 d

dy

a 和 d

dy

x 这种“y对××变

量的导数”在

传播。

像式子

5.3 那样将计算

顺序规定为

从输出到输

入，是为了传

播

y 的导

数。换

言之，就是把

y当作“重要人

物”。如果按照

从输入到输

出的顺

序计

算，输入

x 就是

“重要人物”。在

这种情况下

，传播的导数

将是

dx

dx

→ da

dx →

db

dx → dy

dx 这种形

式，也就是对

x的导数进行

传播。

许多机

器学习问题

采用了以大

量参数作为

输入，以损失

函数作为最

终输

出的形

式。损失函数

的输出（在许

多情况下）是

一个标量值

，它是“重要人

物”。

这意味着

我们需要找

到损失函数

对每个参数

的导数。在这

种情况下，如

果沿

着从输

出到输入的

方向传播导

数，只要传播

一次，就能求

出对所有参

数的导

数。因

为该方法的

计算效率较

高，所以我们

采用反向传

播导数的方

式。

5.3

用计算图

表示

下面我

们把正向传

播的计算图

（图5-1）和反向传

播的计算图

（图5-4）以

上下排

列的方式画

出来。

第 1阶段

自动微分 26

图

5-5

正向传播和

反向传播

从

图5-5可以看出

，正向传播和

反向传播之

间存在明确

的对应关系

。例如，

正向传

播时的变量

a对应于反向

传播时的导

数 d

d

a

y。同样，b对应

于 d

d

y

b ，x对

应于 d

d

x

y。我

们也可以看

出函数之间

存在对应关

系。例如，函数

B的反向传

播

对应于B′

(a)，A对应

于A′

(x)。这样一来

，我们可以认

为变量有普

通值和

导数

值，函数有普

通计算（正向

传播）和求导

计算（反向传

播）。于是，反向

传

播就设计

好了。

最后来

关注一下图

5-5中C′

(b)的函数节

点。它是y

= C(b)的导

数，但要

注意

的是，计算C′

(b)需

要用到b的值

。同理，要计算

B′

(a)就得输入a的

值。

这意味着

进行反向传

播时需要用

到正向传播

中使用的数

据。因此，在实

现反

向传播

时，需要先进

行正向传播

，并且存储各

函数输入的

变量值，也就

是前

面例子

中的x、a和b，之后

就能对每个

函数进行反

向传播的计

算了。

以上就

是反向传播

理论知识的

相关内容，这

是本书的难

点之一。大家

现

在可能还

没完全弄明

白，但是实际

运行代码后

，就会理解得

越来越透彻

。在

下一个步

骤，我们将实

现反向传播

，并通过实际

运行代码来

验证它。

步骤

6

手动进行反

向传播  27

步骤

6

手动进行反

向传播

上一

个步骤介绍

了反向传播

的机制。本步

骤将扩展现

有的Variable类和

Function类

，实现通过反

向传播来求

导的功能。首

先是Variable类。

6.1

Variable类的

功能扩展

下

面实现支持

反向传播的

Variable类。为此，我们

要扩展Variable类，

除

普通值（data）之外

，增加与之对

应的导数值

（grad）。阴影部分是

新增加的代

码。

steps/step06.py

class Variable:

 def

__init__(self, data):

 self.data

= data

 self.grad

= None

上面的代

码在类中增

加了一个新

的实例变量

grad。实例变量data和

grad

都被设置为

NumPy的多维数组

（ndarray）。另外，grad被初始

化为None，我

们要

在通过反向

传播实际计

算导数时将

其设置为求

出的值。

梯度

（gradient）是对包含多

个变量的向

量和矩阵等

求导的导数

。因此

Variable类中增

加了一个名

为grad的变量，grad是

gradient的缩写。

第 1阶

段　自动微分

28

6.2

Function类的功能扩

展

接下来是

Function类。在前面的

步骤中，Function类实

现了进行普

通计

算的正

向传播（forward方法

）的功能。在此

基础上，我们

新增以下两

个功能。



计算

导数的反向

传播（backward方法）功

能

 调用forward方法

时，保有被输

入的Variable实例的

功能

下面的

代码实现了

这两个功能

。

steps/step06.py

class Function:

def __call__(self, input):

x = input.data

y = self.forward(x)

output = Variable(y)

self.input = input #

保存输入的

变量

 return output

def forward(self, x):

raise NotImplementedError()

 def

backward(self, gy):

 raise

NotImplementedError()

在上面

的代码中，__call__方

法将输入的

input设置为实例

变量。这样一

来，

当调用backward方

法时，向函数

输入的Variable实例

就可以作为

self.input

使用。

6.3 Square类和Exp类

的功能扩展

接下来实现

具体函数的

反向传播（backward）。首

先从计算平

方的Square

类开始

。由于y =x2的导数

是

d

d

x

y

= 2x，所以这个

类可以按照

如下方式实

现。

步骤 6

手动

进行反向传

播  29

steps/step06.py

class Square(Function):

 def

forward(self, x):

 y

= x ** 2

return y

 def

backward(self, gy):

 x

= self.input.data

 gx

= 2 * x

* gy

 return

gx

上面的代

码增加了用

于反向传播

的backward方法。这个

方法的参数

gy是

一个ndarray实例

，它是从输出

传播而来的

导数。backward返回的

结果是通

过

这个参数传

播来的导数

和“y

= x2的导数”的

乘积。这个返

回结果会进

一

步向输入

方向传播。

接

下来是计算

y

= ex的Exp类。由于 d

d

x

y = ex，所

以这个类可

以按下面

的

方式实现。

steps/step06.py

class Exp(Function):

def forward(self, x):

y = np.exp(x)

return y

 def

backward(self, gy):

 x

= self.input.data

 gx

= np.exp(x) * gy

return gx

6.4 反

向传播的实

现

这样就做

好准备工作

了。下面我们

尝试通过反

向传播对图

6-1的计算求导

。

  第

1阶段　自动

微分 30

x

A

Square Exp Square

a B b C

y

图6-1 进行

反向传播的

复合函数

首

先编写图6-1所

示的正向传

播的代码。

steps/step06.py

A = Square()

B = Exp()

C

= Square()

x =

Variable(np.array(0.5))

a = A(x)

b = B(a)

y

= C(b)

接

着通过反向

传播计算y的

导数。为此，我

们需要按照

与正向传播

相反

的顺序

调用各函数

的backward方法。图6-2是

这个反向传

播计算的计

算图。

x.grad A.backward a.grad B.backward

b.grad y.grad

(=1)

C.backward

图6-2 反向

传播的计算

图

从图6-2可以

看出各个函

数的backward方法的

调用顺序，也

能看出应该

将backward方法的结

果赋给哪个

变量的grad。下面

是反向传播

的实现。

步骤

6

手动进行反

向传播  31

steps/step06.py

y.grad = np.array(1.0)

b.grad

= C.backward(y.grad)

a.grad =

B.backward(b.grad)

x.grad = A.backward(a.grad)

print(x.grad)

运行

结果

3.297442541400256

反向传

播从

d

d

y

y

= 1开始。因

此，我们将输

出y的导数设

为np.array(1.0)。

之后，只要

按照C→B→A的顺序

调用backward方法即

可。这样就可

以对各变

量

求出导数。

运

行上面的代

码后，得到的

x.grad的结果是3.297442541400256。这

就是

y对x的导

数。顺带一提

，步骤4的数值

微分的结果

是3.2974426293330694，

这两个结

果几乎一样

。这说明反向

传播的实现

是正确的，更

准确地说，这

个

实现大概

率是正确的

。

这样就完成

了反向传播

的实现。虽然

我们得到了

正确的运行

结果，但是

反

向传播的顺

序C→B→A是通过编

码手动指定

的。在下一个

步骤，我们会

把

这项工作

自动化。

第 1阶

段　自动微分

32

步骤

7

反向传

播的自动化

在上一个步

骤中，我们实

现的反向传

播成功运行

。但是，我们不

得不手

动编

写进行反向

传播计算的

代码。这就意

味着每次进

行新的计算

时，都得编

写

这部分代码

。比如在图7-1所

示的情况下

，我们必须为

每个计算图

编写反

向传

播的代码。这

样不但容易

出错，还浪费

时间，所以我

们让Python来做

这

些无聊的事

情吧。

Square Exp

Square Exp

Square

Square

Exp Exp Square

图7-1

各种

计算图的例

子（变量名省

略，函数用类

名表示）

接下

来要做的就

是让反向传

播自动化。准

确来说，就是

要建立这样

一个

机制：无

论普通的计

算流程（正向

传播）中是什

么样的计算

，反向传播都

能

自动进行

。我们马上要

接触到Define-by-Run的核

心了。

步骤 7 反

向传播的自

动化

33

Define-by-Run是在深

度学习中进

行计算时，在

计算之间建

立“连接”的机

制。这种机制

也称为动态

计算图。关于

Define-by-Run及其优点的

详细信息，

请

阅读步骤24的

专栏。

图7-1所示

的计算图都

是流水线式

的计算。因此

，只要以列表

的形式记

录

函数的顺序

，就可以通过

反向回溯自

动进行反向

传播。不过，对

于有分支

的

计算图或多

次使用同一

个变量的复

杂计算图，只

借助简单的

列表就不能

奏

效了。我们

接下来的目

标是建立一

个不管计算

图多么复杂

，都能自动进

行反

向传播

的机制。

其实

只要在列表

的数据结构

上想想办法

，将所做的计

算添加到列

表中，或

许可

以对任意的

计算图准确

地进行反向

传播。这种数

据结构叫作

Wengert

列表（也叫tape）。本

书不对Wengert列表

进行说明，感

兴趣的读者

请阅读

参考

文献[2]和参考

文献[3]等。另外

，关于借助Wengert列

表实现Define￾by-Run的优

点，请阅读参

考文献[4]等。

7.1 为

反向传播的

自动化创造

条件

在实现

反向传播的

自动化之前

，我们先思考

一下变量和

函数之间的

关系。

首先从

函数的角度

来考虑，即思

考“从函数的

角度如何看

待变量”。从函

数

的角度来

看，变量是以

输入和输出

的形式存在

的。如图7-2左图

所示，函数

的

变量包括“输

入变量”（input）和“输

出变量”（output）（图中

的虚线表示

引用）。

f

input

output

x y f

creator

x y

图7-2

从函

数的角度来

看其与变量

的关系（左图

）和从变量的

角度来看其

与函数的关

系（右图）

  第

1阶

段　自动微分

34

那么从变量

的角度来看

，函数是什么

样的呢？这里

要强调的是

变量是由

函

数“创造”的。也

就是说，函数

是变量的“父

母”，是creator（创造者

）。如

果变量没

有作为创造

者的函数，我

们就可以认

为它是由非

函数创造的

，比如

用户给

出的变量。

下

面在代码中

实现图7-2所示

的函数和变

量之间的“连

接”。我们让这

个“连

接”在执

行普通计算

（正向传播）的

那一刻

333

创建

。为此，先在Variable类

中添

加以下

代码。

steps/step07.py

class Variable:

 def

__init__(self, data):

 self.data

= data

 self.grad

= None

 self.creator

= None

 def

set_creator(self, func):

 self.creator

= func

上面的

代码添加了

一个名为creator的

实例变量，之

后添加了用

于设置

creator的set_creator方

法。下面我们

在Function类中添加

以下代码。

steps/step07.py

class Function:

def __call__(self, input):

x = input.data

y = self.forward(x)

output = Variable(y)

output.set_creator(self) # 让

输出变量保

存创造者信

息

self.input = input

self.output = output #

也保存输

出变量

 return output

上面

的代码通过

正向传播的

计算，创建了

一个名为output的

Variable实例。

对于创

建的output变量，代

码让它保存

了“我（函数本

身）是创造者

”的信息。

这是

动态建立“连

接”这一机制

的核心。为了

兼顾下一个

步骤，这里将

输出

步骤

7 反

向传播的自

动化  35

设置为

实例变量output。

DeZero的

动态计算图

的原理是在

执行实际的

计算时，在变

量这个“箱子

”

里记录它的

“连接”。Chainer和PyTorch也采

用了类似的

机制。

这样一

来，Variable和Function之间就

有了“连接”，我

们就可以反

向遍

历计算

图了。具体的

实现代码如

下所示。

A = Square()

B = Exp()

C

= Square()

x =

Variable(np.array(0.5))

a = A(x)

b = B(a)

y

= C(b)

# 反向

遍历计算图

的节点

assert y.creator == C

assert y.creator.input == b

assert y.creator.input.creator == B

assert y.creator.input.creator.input == a

assert y.creator.input.creator.input.creator == A

assert y.creator.input.creator.input.creator.input == x

首先

介绍一下assert（断

言）语句，它的

用法是assert...。如果

这里的...不

为

True，就会抛出异

常。因此可以

使用assert语句来

检查条件是

否得到满足

。

上面的代码

在运行时没

有发生任何

问题（没有抛

出异常），这意

味着assert语

句的

所有条件都

得到了满足

。

上面的代码

通过Variable实例变

量creator找到前一

个Function，然后通

过

Function的实例变量

input找到前一个

Variable。它们的连接

方式如图7-3

所

示。

第 1阶段　自

动微分 36

x A a B

b C y

creator

input creator input creator

input

图7-3 以

y为起点反向

遍历计算图

如图7-3所示，计

算图是由函

数和变量之

间的“连接”构

建而成的。更

重

要的是，这

个“连接”是在

计算实际发

生的时候（数

据在正向传

播中流转的

时

候）形成的

。变量和函数

连接的这个

特征就是Define-by-Run。换

言之，“连接”

是

通过数据的

流转建立起

来的。

图7-3这种

带有“连接”的

数据结构叫

作连接节点

。节点是构成

图的一个

元

素，连接则代

表对另一个

节点的引用

。也就是说，我

们用了一个

叫作“连

接节

点”的数据结

构来表示计

算图。

7.2 尝试反

向传播

下面

利用变量和

函数之间的

连接，尝试实

现反向传播

。首先实现从

y到

b的反向传

播。代码如下

所示（为了便

于理解，这里

附上了计算

图）。

x A a

B b C y

creator input creator input

creator input

y.grad =

np.array(1.0)

C = y.creator

# 1. 获取函数

b =

C.input # 2. 获取函数的

输入

b.grad = C.backward(y.grad) #

3. 调用函

数的backward方法

步

骤 7

反向传播

的自动化  37

上

面的代码从

y的实例变量

creator获取函数，从

函数的input获取

输入

变量，然

后调用函数

的backward方法。下面

是从变量b到

变量a反向传

播的

计算图

和代码。

x A

a B b C

y

creator input creator

input creator input

B

= b.creator # 1.

获取

函数

a = B.input

# 2. 获取函

数的输入

a.grad

= B.backward(b.grad) # 3.

调

用函数的backward方

法

上述代码

执行的反向

传播的逻辑

与之前的相

同。具体来说

，该流程如下

所示。

1. 获取函

数

2. 获取函数

的输入

3. 调用

函数的backward方法

最后是从变

量a到变量x的

反向传播。

x A a B

b C y

creator

input creator input creator

input

A = a.creator

# 1. 获

取函数

x

= A.input # 2.

获取

函数的输入

x.grad = A.backward(a.grad) #

3. 调用函数的

backward方法

print(x.grad)

第 1阶段

自动微分 38

运

行结果

3.297442541400256

这样

就完成了所

有的反向传

播。

7.3 增加

backward方法

从前面这些

反向传播的

代码可以看

出，它们有着

相同的处理

流程。准确

来

说，是从一个

变量到前一

个变量的反

向传播逻辑

相同。为了自

动完成这些

重复的处理

，我们在Variable类中

添加一个新

的方法——backward。

steps/step07.py

class

Variable:

 def __init__(self,

data):

 self.data =

data

 self.grad =

None

 self.creator =

None

 def set_creator(self,

func):

 self.creator =

func

 def backward(self):

f = self.creator #

1. 获取

函数

 if

f is not None:

x = f.input #

2. 获取函

数的输入

 x.grad

= f.backward(self.grad) # 3.

调

用函数的backward方

法

 x.backward() #

调用自己

前面那个变

量的backward方法（递

归）

backward方法和此

前反复出现

的流程基本

相同。具体来

说，它从Variable

的creator获

取函数，并取

出该函数的

输入变量，然

后调用函数

的backward方法。

最后

，它会针对自

己前面的变

量，调用它的

backward方法。这样每

个变量的

backward方

法就会被递

归调用。

步骤

7 反向传播的

自动化

39

如果

Variable实例的creator是None，那

么反向传播

就此结束。这

种情

况意味

着Variable实例是由

非函数创造

的，主要来自

用户提供的

变量。

下面使

用这个新的

Variable自动进行反

向传播。

steps/step07.py

A = Square()

B = Exp()

C

= Square()

x =

Variable(np.array(0.5))

a = A(x)

b = B(a)

y

= C(b)

# 反向

传播

y.grad = np.array(1.0)

y.backward()

print(x.grad)

运行结

果

3.297442541400256

只要像上

面那样调用

变量y的backward方法

，反向传播就

会自动进行

。

运行结果和

之前的一样

。这样我们就

打好了对DeZero来

说最重要的

自动微

分的

基础。

第 1阶段

自动微分 40

步

骤

8

从递归到

循环

在上一

个步骤中，我

们向Variable类添加

了backward方法。考虑

到处理

效率

的改善和今

后的功能扩

展，本步骤将

改进backward方法的

实现方式。

8.1 现

在的Variable类

再来

看一下前面

实现的Variable类的

backward方法。

steps/step07.py

class Variable:

 ...

# 省略代

码

 def

backward(self): 

 f

= self.creator

 if

f is not None:

x = f.input

x.grad = f.backward(self.grad)

x.backward()

我们需要

注意的一点

是backward方法调用

了（朝着输入

方向的）前一

个

变量的backward方

法。由此，“backward方法

内调用backward方法

，被调用的

backward方

法内再次调

用backward方法”的处

理会不断延

续下去（直到

碰到一

个self.creator函

数为None的Variable变量

）。这是一个递

归结构。

步骤

8 从递归到循

环

41

本书为了

节省篇幅，有

时候会省略

部分代码。省

略的部分用

“...”表示（这

个省

略号与出现

在Python解释器的

换行处的“...”含

义不同）。

8.2

使用

循环实现

接

下来将前面

的“使用递归

实现”替换为

“使用循环实

现”。代码如下

所示。

steps/step08.py

class

Variable:

 ...

def backward(self):

 funcs

= [self.creator]

 while

funcs:

 f =

funcs.pop() # 获取函

数

x, y = f.input,

f.output # 获取函数

的输入

x.grad = f.backward(y.grad) #

backward调用

backward方法

 if x.creator

is not None:

funcs.append(x.creator) # 将前一

个函数添加

到列表中

上

面的实现使

用了循环。关

键点在于按

顺序向funcs列表

里添加应该

处

理的函数

。在while循环中，通

过funcs.pop()获取要处

理的函数，将

其作为

变量

f，然后调用函

数f的backward方法。通

过f.input和f.output获取函

数

f的输入变

量和输出变

量，就可以正

确地设置f.backward()的

参数和返回

值。

列表的pop方

法会删除列

表末尾的元

素，并取出这

个元素的值

。例如，对

于 funcs = [1,

2, 3]，如

果执行 x =

funcs.pop()，3就会

被取出，

funcs变为

[1, 2]。

第 1阶段　自动

微分 42

8.3 代码验

证

接下来使

用前面的Variable类

来求导。这里

运行的是和

上一个步骤

相同

的代码

。

steps/step08.py

A = Square()

B = Exp()

C

= Square()

x =

Variable(np.array(0.5))

a = A(x)

b = B(a)

y

= C(b)

# 反向传播

y.grad = np.array(1.0)

y.backward()

print(x.grad)

运

行结果

3.297442541400256

和上

一个步骤得

到的结果一

样。这样实现

方式就从递

归变成了循

环。在

步骤15，我

们将感受到

循环带来的

好处。届时要

处理的是复

杂的计算图

，不

过在使用

循环的情况

下，代码实现

很容易扩展

到复杂的计

算图的处理

，而且

循环的

执行效率也

会变高。

每次

递归调用函

数时，函数都

会将处理过

程中的结果

保留在内存

中（或者

说保

留在栈中），然

后继续处理

。因此一般来

说，循环的效

率更高。不过

，

对现代计算

机来说，使用

少量的内存

是没有问题

的。有时可以

通过尾递归

的技巧，使递

归处理能够

按照循环的

方式执行。

这

样就完成了

反向传播的

基础实现。接

下来，我们将

扩展当前版

本的

DeZero，以执行

更复杂的计

算。下一个步

骤要做的是

提高DeZero的易用

性。

步骤 9 让函

数更易用

43

步

骤 9

让函数更

易用

DeZero现在可

以通过反向

传播进行计

算了。它还拥

有一项名为

Define￾by-Run的能力，可以

在运行时在

计算之间建

立“连接”。为了

使DeZero更加

易用

，本步骤将对

DeZero的函数进行

3项改进。

9.1 作为

Python函数使用

此

前，DeZero中使用的

函数是作为

Python的类实现的

。举例来说，在

使用Square类进行

计算的情况

下，我们需要

编写如下代

码。

x = Variable(np.array(0.5))

f = Square()

y

= f(x)

上面的代

码是分两步

计算平方的

：创建一个Square类

的实例；调用

这

个实例。但

是从用户的

角度来看，分

两步完成有

点啰唆（虽然

可以写成y

= 

Square()(x)，但

观感很差）。用

户更希望把

DeZero的函数当作

Python函数

使用。为

此，需要添加

以下代码。

第

1阶段　自动微

分 44

def square(x):

 f

= Square()

 return

f(x)

def exp(x):

f = Exp()

return f(x)

上面的代

码实现了square和

exp两个函数。这

样我们就可

以将DeZero的

函数

当作Python函数使

用了。顺便提

一下，上面的

代码也可以

像下面这样

写成一行。

steps/step09.py

def square(x):

return Square()(x) # 编

写在一行内

def

exp(x):

 return Exp()(x)

上面的代码

没有像f = Square()这样

通过变量名

f来引用，而是

直接写

成了

Square()(x)，这样也是可

以的。接下来

我们使用一

下前面实现

的两个函数

。

x = Variable(np.array(0.5))

a

= square(x)

b =

exp(a)

y = square(b)

y.grad = np.array(1.0)

y.backward()

print(x.grad)

运行结果

3.297442541400256

如

上所示，用Variable封

装np.array(0.5)后，就可以

像使用NumPy进

行

计算一样来

编码了。另外

，函数现在也

支持连续调

用，示例代码

如下所示。

步

骤 9 让函数更

易用

45

x = Variable(np.array(0.5))

y = square(exp(square(x))) #

连续调

用

y.grad = np.array(1.0)

y.backward()

print(x.grad)

运行结果

3.297442541400256

这样我们就

可以编写更

自然的代码

进行计算。以

上是第1项改

进。

9.2 简化 backward方法

第2项要改进

的地方是减

少用户在反

向传播方面

所做的工作

。具体来说，

就

是省略前面

代码中的y.grad

= np.array(1.0)。每

次反向传播

时我们都要

重

新编写这

行代码，为了

省略它，我们

需要在Variable的backward方

法中添加

阴

影部分的两

行代码。

steps/step09.py

class Variable:

...

 def backward(self):

if self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 x,

y = f.input, f.output

x.grad = f.backward(y.grad)

if x.creator is not

None:

 funcs.append(x.creator)

如上

所示，如果变

量grad为None，则自动

生成导数。代

码中通过np.ones_

  第

1阶段

自动微

分 46

like(self.data)创建了一

个ndarray实例，该实

例的形状和

数据类型与

self.

data的相同，元素

为1。如果self.data是标

量，那么self.grad也是

标量。

之前的

代码中输出

的导数是 np.array(1.0)，而

上面的代码

使用了

np.ones_like()。这么

编写的原因

是Variable中的data和grad的

数据

类型是

一样的。如果

data的数据类型

是32位浮点数

，那么grad的数据

类

型也是32位

浮点数。顺带

一提，如果编

写的是np.array(1.0)，它的

数

据类型就

是64位的浮点

数。

之后如果

再做某个计

算，只需对最

终输出的变

量调用backward方法

就能

求得导

数。下面是示

例代码。

steps/step09.py

x = Variable(np.array(0.5))

y = square(exp(square(x)))

y.backward()

print(x.grad)

运行

结果

3.297442541400256

9.3

只支持

ndarray

DeZero的Variable只支持ndarray实

例的数据。但

是，有些用户

很可能

会不

小心使用float或

int等数据类型

，例如Variable(1.0)和Variable(3)等。

考

虑到这一点

，我们再做一

点优化，使Variable成

为只能容纳

ndarray实例

的“箱子

”。具体来说，就

是当把ndarray实例

以外的数据

放入Variable时，

让DeZero立

即抛出错误

（None是允许放入

的）。这项改进

有望让用户

在早期

发现

问题。下面我

们在Variable类的初

始化部分添

加以下代码

。

步骤

9 让函数

更易用  47

steps/step09.py

class Variable:

def __init__(self, data):

if data is not

None:

 if not

isinstance(d.ata, np.ndarray):}

 raise

TypeError('{} is not supported'.format(type(data)))

self.data = data

self.grad = None

self.creator = None

在上

面的代码中

，如果作为参

数的data不是None，也

不是ndarray实例，

就

会引发TypeError异常

。这时，程序会

输出代码中

指定的字符

串作为错误

提示。

现在，可

以像下面这

样使用Variable。

steps/step09.py

x

= Variable(np.array(1.0)) # OK

x = Variable(None) #

OK

x = Variable(1.0)

# NG：错误

发生！

运行结

果

TypeError:

<class 'float'> is not

supported

在上面的

代码中，如果

数据为ndarray或None，就

可以顺利创

建Variable。

但如果是

其他的数据

类型，比如上

面代码中的

float，DeZero就会抛出一

个

异常。这样

一来，用户就

能立刻知道

自己使用了

错误的数据

类型。

这项改

进也带来一

个问题，这个

问题是由NumPy自

身的特点所

导致的。

在解

释这个问题

之前，我们先

看看下面的

NumPy代码。

x =

np.array([1.0])

y = x

** 2

print(type(x), x.ndim)

print(type(y))

steps/step09.py

x =

np.array([1.0])

y = x

** 2

print(type(x), x.ndim)

print(type(y))

运行结

果

<class 'numpy.ndarray'>

1

<class 'numpy.ndarray'>

第 1阶段　自

动微分 48

运行

结果

<class 'numpy.ndarray'> 1

<class 'numpy.ndarray'>

代码中

的 x

是一维的

ndarray。x ** 2（平方）的结果

y 的数据类型

是

ndarray。这是预期

的结果，问题

出现在下面

这种情况。

x = np.array(1.0)

y = x **

2

print(type(x), x.ndim)

print(type(y))

运

行结果

<class 'numpy.ndarray'> 0

<class 'numpy.float64'>

上面

代码中的x是

零维的ndarray，而x **

2的

结果是np.float64，这是

NumPy的运行方式

A。换言之，如果

用零维的ndarray实

例进行计算

，结果将

是ndarray实

例以外的数

据类型，如numpy.float64、numpy.float32等

。这意

味着DeZero函

数的输出Variable可

能是numpy.float64或numpy.float32类型

的数据。但是

，Variable中的数据只

允许保存ndarray实

例。为了解决

这个问题，

我

们首先准备

以下函数作

为工具函数

。

steps/step09.py

def as_array(x):

if np.isscalar(x):

 return

np.array(x)

 return x

上面的代码

使用np.isscalar函数来

检查numpy.float64等属于

标量的类

型

（它也可以用

来检查Python的int和

float）。下面是使用

np.isscalar函数

的示例

代码。

A

CuPy的实现

方法几乎与

NumPy相同，只是在

“零维ndarray的类型

”这一点上不

同。在使用CuPy

时

，对cupy.ndarray的计算结

果始终是cupy.ndarray，与

维度无关。

步

骤 9

让函数更

易用  49

>>>

import numpy as np

>>> np.isscalar(np.float64(1.0))

True

>>>

np.isscalar(2.0)

True

>>> np.isscalar(np.array(1.0))

False

>>> np.isscalar(np.array([1, 2,

3]))

False

从这些

例子可以看

出，通过np.isscalar(x)可以

判断x是否为

ndarray实

例。如果不

是，则使用as_array函

数将其转换

为ndarray实例。实现

as_array

这个工具函

数之后，在Function类

中添加以下

阴影部分的

代码。

steps/step09.py

class Function:

def __call__(self, input):

x = input.data

y = self.forward(x)

output = Variable(as_array(y))

output.set_creator(self)

 self.input =

input

 self.output =

output

 return output

...

上面的

代码在将正

向传播的结

果 y封装在 Variable中

时使用了

as_

array(y)，这

样可以确保

输出结果output是

ndarray实例的数据

。即使使用零

维的ndarray实例进

行计算，所有

的数据也会

是ndarray实例。

至此

，本步骤的工

作就完成了

。下一个步骤

的主题是测

试DeZero。

第 1阶段　自

动微分 50

步骤

10

测试

在软件

开发中，测试

不可或缺。测

试可以暴露

错误（bug），自动化

测试

可以持

续保持软件

的品质。我们

打造的DeZero框架

也需要测试

。本步骤将

介

绍测试方法

，尤其是深度

学习框架的

测试方法。

随

着软件规模

的扩大，软件

测试的方法

和规定也逐

渐变多。但是

不要把测

试

想得很难，尤

其是在软件

开发的早期

阶段。重要的

是先建立起

测试的意

识

。本步骤要进

行的不是全

面的测试，而

是尽可能简

单的测试。

10.1 Python的

单元测试

在

Python中进行测试

的一个很方

便的做法是

使用标准库

中的unittest。

下面对

前面的步骤

中实现的square函

数进行测试

。代码如下所

示。

steps/step10.py

import unittest

class

SquareTest(unittest.TestCase):

 def test_forward(self):

x = Variable(np.array(2.0))

y = square(x)

expected = np.array(4.0)

self.assertEqual(y.data, expected)

步骤 10

测试

51

前面的代码

首先导入了

unittest，然后实现了

继承unittest.TestCase的

SquareTest类。创

建名字以test开

头的方法后

，将测试逻辑

写在该方法

中。

示例中的

测试用于验

证square函数的输

出与预期值

是否一致。具

体来说，它

验

证的是当输

入为2.0时，输出

是否为4.0。

上面

的例子使用

方法self.assertEqual来验证

square函数的输出

与预

期值是

否一致。这个

方法可以判

断两个给定

对象是否相

等。除了这个

方法，

unittest还提供

了 self.assertGreater和 self.assertTrue等方法

。

关于其他方

法的更多信

息，请阅读参

考文献[8]等资

料。

现在运行

上面的测试

代码。假定该

测试代码在

文件step/step10.py中。

这时

，在终端运行

以下命令。



$ python -m unittest steps/step10.py

像这样在命令中加上参数-m unittest，就能在测试模式下运行Python

文件了。另外，只要在 step10.py文件末尾添加以下代码，就能用 python 

steps/step10.py命令运行测试。

# step10.py

unittest.main()

现在看看测试的输出。运行上面的命令，程序会输出以下结果。

.

----------------------------------------------------------------------

Ran 1 tests in 0.000s

OK

这个输出的意思是“运行了一个测试，结果OK”。也就是说，测试通过了。

如果测试过程中出现了一些问题，程序就会输出“FAIL: test_forward(step10.

SquareTest)”，这表示测试失败了。

  第 1阶段　自动微分 52

10.2 square函数反向传播的测试

接下来添加针对square函数反向传播的测试。在刚刚实现的SquareTest

类中添加以下代码。

steps/step10.py

class SquareTest(unittest.TestCase):

 ...

 def test_backward(self):

 x = Variable(np.array(3.0))

 y = square(x)

 y.backward()

 expected = np.array(6.0)

 self.assertEqual(x.grad, expected)

我们在代码中添加了一个名为test_backward的方法。在方法中通过

y.backward()来求导，然后检查导数的值是否等于预期值。另外，代码中设

置的预期值（expected）6.0是手动计算出来的。

现在再来测试一下上面的代码。输出结果如下所示。

..

----------------------------------------------------------------------

Ran 2 tests in 0.001s

OK

从结果来看，两项测试都通过了。之后可以采用和前面一样的做法来添

加其他的测试用例（输入和预期值）。随着测试用例的增加，square函数的可

靠性也会增加。另外，我们也可以在修改代码时进行测试，以此来反复验证

square函数的状态。

步骤 10 测试  53

10.3 通过梯度检验来自动测试

我们在前面写了一个反向传播的测试方法。其中，导数的预期值是手动

计算出来的。实际上有一种代替手动计算的自动测试方法，该方法叫作梯度

检验（gradient checking）。梯度检验是将数值微分得到的结果与反向传播

得到的结果进行比较。如果二者相差很大，则说明反向传播的实现有问题。

步骤4实现了数值微分。数值微分容易实现，并能得出大体正确的导数值。

因此，可以通过与数值微分的结果进行比较来测试反向传播的实现是否正确。

梯度检验是一种高效的测试方式，我们只要准备输入值即可。下面添

加基于梯度检验的测试方法。这里我们会使用在步骤4中实现的numerical_

diff函数，这个函数的代码也会一并列出，兼作复习。

steps/step10.py

def numerical_diff(f, x, eps=1e-4):

 x0 = Variable(x.data - eps)

 x1 = Variable(x.data + eps)

 y0 = f(x0)

 y1 = f(x1)

 return (y1.data - y0.data) / (2 * eps)

class SquareTest(unittest.TestCase):

 ...

 def test_gradient_check(self):

 x = Variable(np.random.rand(1)) # 生成随机的输入值

 y = square(x)

 y.backward()

 num_grad = numerical_diff(square, x)

 flg = np.allclose(x.grad, num_grad)

 self.assertTrue(flg)

  第 1阶段　自动微分 54

前面的代码会在进行梯度检验的test_gradient_check方法中生成一个随

机的输入值，然后，通过反向传播求出导数，再利用numerical_diff函数通

过数值微分求导，最后检查这两种方法得到的数值是否基本一致。这里使用

的是NumPy函数np.allclose。

np.allclose(a, b)用于判断ndarray实例a和b的值是否接近。多近才算

近是由np.allclose函数的参数rtol和atol指定的，指定方式如np.allclose 

(a, b, rtol=1e-05, atol=1e-08)所示。这时，如果a和b的所有元素满足以

下条件，则返回True。

|a - b| ≤ (atol + rtol * |b|) A

另外，atol和rtol的值有时需要根据要进行梯度检验的计算对象（函数）

加以微调。调整基准可以参照参考文献[5]等资料。添加了上面的梯度检验之后，

再次进行测试。这次得到的结果如下。

...

----------------------------------------------------------------------

Ran 3 tests in 0.001s

OK

在可以自动微分的深度学习框架中，我们可以像上面那样利用梯度检验

建立一个半自动的测试机制。这样可以系统地构建更广泛的测试用例。

10.4 测试小结

在创建DeZero方面，关于测试，了解以上知识就足够了。读者可以根

据前面的步骤编写DeZero的测试代码。不过，本书之后的内容省略了测试

相关的说明。如果读者觉得需要添加测试代码，请自行编写。

A |.|表示绝对值。

步骤 10 测试  55

通常，我们会在一个地方汇总管理所有测试代码文件。本书的测试代码

也统一放在tests目录下（该目录下还包含了另行实现的用于测试的工具函数

的代码）。有兴趣的读者可以看看这些测试代码，其中很多代码与我们在本

步骤编写的代码类似。可以使用以下命令一起运行所有测试文件。

$



python -m unittest discover

tests

像这样使用

discover子命令后，discover会

在其后指定

的目录下搜

索测

试文件

，然后将所有

找到的文件

一起运行。在

默认情况下

，指定目录下

符合

test*.py模式的

文件会被识

别为测试文

件（模式可以

修改）。这样就

能一次性

运

行tests目录下的

所有测试文

件了。

DeZero的tests目录

下也有以Chainer的

结果为正确

答案的测试

。例如在

测试

sigmoid函数时，测试

代码使用DeZero和

Chainer分别对相同

的输入

进行

计算，然后比

较这两个输

出是否大体

相同。

DeZero的GitHub仓库

还集成了Travis CI（参

考文献[9]）。Travis CI是

一

个持续集成

服务。在对DeZero的

GitHub仓库A

的代码

进行push和Pull 

Request操作

时，测试会自

动运行。如果

结果有问题

，Travis CI会通过电子

邮件通知。另

外，DeZero的GitHub仓库首

页会显示图

10-1这样的界面

。

图10-1 DeZero的GitHub仓库的

首页界面

A 具

体网址可通

过ituring.cn/article/521545查看。

第 1阶

段　自动微分

56

如图10-1所示，界

面上会显示

“build: passing”徽章。这个徽

章意味着

测

试通过（如果

测试失败，界

面上会显示

“build: failed”徽章）。通过与

CI工

具的协作

，源代码能够

不断得到测

试。这样可以

确保代码的

可靠性。

DeZero现在

还是一个小

软件，我们会

把它发展成

更大的软件

。引入本

步骤

介绍的测试

机制后，我们

有望持续保

持代码的可

靠性。以上就

是第1阶

段的

内容。



在第1阶

段，我们一步

一个脚印地

创建了DeZero。最初

的DeZero只有

一个

“小箱子”（变量

），现在它已经

发展到能够

运行反向传

播这种复杂

算法的

规模

了。但是，现在

实现的反向

传播只能应

用于简单的

计算。从下一

个阶段

开始

，我们将进一

步扩展DeZero，使其

可以应用于

更复杂的计

算。

步骤 10 测试



57

专栏：自动微

分

深度学习

框架的核心

技术是反向

传播。有些资

料将反向传

播称为“自动

微分”。

需要注

意的是，“自动

微分”指代的

是范围更加

具体的一种

技术，在学术

领域尤

其如

此。下面是对

自动微分的

补充说明。

自

动微分指的

是自动求出

导数的做法

（技术）。“自动求

出导数”是指

由计

算机（而

非人）求出导

数。具体来说

，它是指在对

某个计算（函

数）编码后，

计

算机会自动

求出该计算

的导数的系

统。

计算机程

序求导的方

法主要有3种

。

第1种方法是

数值微分。如

同步骤4中实

现的那样，首

先给变量以

微小的差

异

并执行普通

计算（正向传

播），重复两次

该操作，然后

基于输出的

差值计算得

到

近似的导

数。数值微分

虽然容易实

现，但也存在

一些问题，比

如输出中包

含误差、

在处

理多变量的

函数时计算

成本高等。

第

2种方法是符

号微分（symbolic differentiation）。这是

使用导数公

式求导的

方

法。输入是式

子，输出也是

式子（式子可

以用树状结

构的数据形

式表示）。这种

方法被用在

Mathematica和MATLAB等软件中

。

符号微分的

输出是求导

后的表达式

，即导函数，这

时还没有进

行任何数

值

计算。在得到

导函数之后

，我们就可以

求出某个特

定值（如x = 3.0）

上的

导数了。

符号

微分的问题

是式子很容

易变得臃肿

。特别是在不

考虑优化的

实现中，式

子

很快就会变

“大”（可以说是

式子“大爆炸

”）。但深度学习

中处理的计

算需要高

效

地对大量变

量求出导数

“值”（而不是表

达式），这就需

要使用更合

适的方法了

。

第3种方法是

自动微分。这

是一种采用

链式法则求

导的方法。我

们对某个函

数编码后，可

以通过自动

微分高效地

求出高精度

的导数。反向

传播也是自

动微分

  第

1阶

段　自动微分

58

的一种。更准

确地说，自动

微分可以大

体分为两种

：前向模式的

自动微分和

反向

模式的

自动微分。反

向传播相当

于反向模式

的自动微分

。

反向传播（反

向模式的自

动微分）将导

数从输出到

输入的方向

传播。前向

模

式的自动微

分则与之相

反，导数的传

播方向是从

输入到输出

。这两种

方法

都利用链式

法则来求导

，但路径并不

相同。如果输

出只有一个

，要

计算的是

这个输出变

量的导数，那

使用反向模

式的自动微

分再合适不

过

了。许多机

器学习问题

的输出是一

个变量，所以

使用反向模

式的自动微

分。

本书不对

前向模式的

自动微分做

过多说明，对

前向模式的

自动微分感

兴

趣的读者

可以阅读参

考文献 [6]和参

考文献

[7]。

数值

微分 符号微

分 自动微分

前向模式

反

向模式

（＝反向

传播）

图A-1 汇总

了前面介绍

的计算机程

序求导的方

法

如图A-1所示

，自动微分是

用计算机求

导的一种方

法。深度学习

框架中实现

的是反向模

式的自动微

分。不过有些

资料不区分

前向模式和

反向模式，笼

统地将

反向

传播称为“自

动微分”。

在学

术领域，自动

微分的研究

有很长的历

史，积累了许

多重要的研

究成果。

遗憾

的是，此前自

动微分与机

器学习领域

没有什么交

集。近来，随着

深

度学习的

蓬勃发展，自

动微分领域

受到越来越

多的关注，机

器学习和编

程语言等领

域与自动微

分领域开始

了新的交流

。

 59

第2阶段

用自

然的代码表

达

我们已经

完成了构建

DeZero的第1阶段的

工作，现在它

可以自动求

出特

定计算

的导数。假设

计算图由平

方或指数函

数等函数类

组成（笔直的

计算图），

它的

导数就可以

通过调用backward方

法自动求得

。

下面进入第

2阶段。这个阶

段的主要目

标是扩展当

前的DeZero，使它能

够执行更复

杂的计算。具

体来说，我们

将修改DeZero的基

础代码，使它

能

够处理接

收多个输入

的函数和返

回多个输出

的函数。我们

还将扩展DeZero，

使

它可以用自

然的代码来

表达，例如能

够使用+和*等

运算符。

第2阶

段结束时，DeZero会

被打包为一

个Python包。这样，第

三方也

能使

用DeZero了。下面进

入第2阶段吧

！

  第

2阶段　用自

然的代码表

达 60

步骤

11 可变

长参数（正向

传播篇)  61

步骤

11

可变长参数

（正向传播篇

)

之前涉及的

函数，其输入

输出都只有

一个变量，如

y =

square(x)和y = 

exp(x)等，但有些

函数需要多

个变量作为

输入，例如图

11

1所示的加法

运算

和乘法

运算的情况

。

x0

x1

+ y

x0

x1

* y

图111

加法运算

的计算图和

乘法运算的

计算图（乘法

运算用*表示

）

另外，有些函

数可能有多

个输出，例如

图112所示的函

数。

1

2 3

4 5

6 y0

y1

separate

x

1 2 3

4 5 6

图11

2 有多个

输出的计算

图示例（分割

多维数组的

函数）

第 2阶段

用自然的代

码表达 62

考虑

到这些情况

，我们来扩展

DeZero，使其可以处

理可变长的

输入和输出

。

可变长意味

着参数（或返

回值）的数量

可以发生变

化，数量可以

是不小于1的

任意整数值

。下面修改Function类

以支持可变

长的参数和

返回值。

11.1 修改

Function类

现在修改

Function类以支持多

个输入和输

出。为此，我们

考虑将变量

放

入一个列

表（或元组）中

进行处理。换

言之，修改后

的Function类像之前

一样

接收“一

个参数”并返

回“一个值”。不

同的是，参数

和返回值被

修改为列表

，

列表中包含

需要的变量

。

Python的列表和元

组能保存多

条数据。列表

用[]将数据括

起来，如[1,

2, 

3]；元组

用()将数据括

起来，如(1, 2,

3)。列表

和元组的主

要区别是，

元

组一旦创建

，其元素就不

能改变了。例

如，对于元组

x = (1,

2, 3)，

不能用x[0] =

4等方

法来改变元

素，但如果是

列表，就可以

改变。

首先回

顾一下前面

已经实现的

Function类，代码如下

所示。

steps/step10.py

class

Function:

 def __call__(self,

input):

 x =

input.data

 y =

self.forward(x)

 output =

Variable(as_array(y))

 output.set_creator(self)

self.input = input

self.output = output

return output

 def

forward(self, x):

 raise

NotImplementedError()

 def backward(self,

gy):

 raise NotImplementedError()

步骤 11 可

变长参数（正

向传播篇)

63

Function的

__call__方法将实际

数据从Variable这个

“箱子”里取出

，然

后通过forward方

法进行具体

的计算。然后

，它把结果封

装在Variable中，

并让

结果记住Function是

它的“创造者

”。在此基础上

，我们将__call__方法

的参数和返

回值修改为

列表。

steps/step11.py

class Function:

def __call__(self, inputs):

xs = [x.data for

x in inputs]

ys = self.forward(xs)

outputs = [Variable(as_array(y)) for

y in ys]

for output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

outputs

 return outputs

def forward(self, xs):

raise NotImplementedError()

 def

backward(self, gys):

 raise

NotImplementedError()

上面的

代码将参数

和返回值改

为列表。除了

将变量放入

列表进行处

理这

一点，其

余处理的逻

辑与之前的

一样。另外，这

里使用了列

表生成式写

法创

建了新

的列表。

列表

生成式写法

具体来说就

是xs = [x.data for

x in inputs]这样

的写

法，此处示例

表示对于inputs列

表中的各元

素x取出相应

的数据

（x.data），并创

建一个由这

些元素组成

的新列表。

以

上就是新的

Function类的代码。接

下来，我们使

用这个新的

Function

类来实现一

个具体的函

数。首先实现

执行加法运

算的Add类。

第 2阶

段　用自然的

代码表达 64

11.2 Add类

的实现

下面

实现Add类的forward方

法。需要注意

的是参数和

返回值应该

是列表

（或元

组）。为了满足

这一点，我们

需要将代码

编写成下面

这样。

steps/step11.py

class Add(Function):

def forward(self, xs):

x0, x1 = xs

y = x0 +

x1

 return (y,)

Add类的参

数是包含两

个变量的列

表，所以通过

x0, x1 = xs可以取出xs

列

表的元素。然

后使用这些

元素进行计

算。在返回结

果时，使用return(y,)（也

可以写成“return y,”）来

返回一个元

组。这么处理

后，我们就可

以像下面这

样使用Add类了

。

steps/step11.py

xs

= [Variable(np.array(2)), Variable(np.array(3))] #

初始化为列

表

f = Add()

ys = f(xs) #

ys是元组

y = ys[0]

print(y.data)

运

行结果

5

如上

面的代码所

示，DeZero能正确地

计算出2

+ 3 = 5。输入

变成列表

后

，DeZero可以处理多

个变量；输出

变成元组后

，DeZero可以支持多

个变量。

现在

的正向传播

支持可变长

的参数和返

回值了，不过

实现代码有

些烦琐，因

为

使用Add类的人

需要准备列

表作为输入

变量，并接收

元组作为返

回值。这

种用

法很别扭。在

下一个步骤

，我们将改进

目前的实现

，使代码更加

自然。

步骤 12 可

变长参数（改

进篇)

65

步骤 12

可

变长参数（改

进篇

)

在上一

个步骤，我们

扩展了DeZero以支

持可变长参

数。不过，代码

仍

有改进空

间。为了提高

DeZero的易用性，这

里对它进行

两项改进。第

1项改

进针对

的是使用Add类

（或具体的函

数类）的人，第

2项改进针对

的是实现Add

类

的人。先看第

1项改进。

12.1 第1项

改进：使函数

更容易使用

在上一个步

骤，我们使用

Add类进行了计

算。图121左侧是

当时编写的

代码。

xs = [Variable(np.array(2)),

Variable(np.array(3))]

f = Add()

ys = f(xs)

y

= ys[0]

x0 =

Variable(np.array(2))

x1 = Variable(np.array(3))

f = Add()

y

= f(x0, x1)

图12

1 当前

代码（左）和改

进后的代码

（右）

如图121左侧

的代码所示

，目前Add类的参

数归并到了

列表中，结果

以

元组的形

式返回。不过

图121右侧的代

码更加自然

：不将参数归

并到列表，

第

2阶段　用自然

的代码表达

66

而是直接将

参数传递给

Add类，结果也直

接作为变量

返回。第1项改

进就是

想办

法写出这样

自然的代码

。

下面着手进

行修改。我们

要修改Function类。下

面的阴影部

分是对之前

的代码所做

的修改。

class Function:

def __call__(self, *inputs ):

# ①添加

星号

 xs

= [x.data for x

in inputs]

 ys

= self.forward(xs)

 outputs

= [Variable(as_array(y)) for y

in ys]

 for

output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

outputs

 # ②如果列

表中只有一

个元素，则返

回第

1个元素

return outputs if

len(outputs) > 1 else

outputs[0]

首先看②处。这

行代码在outputs只

有一个元素

时返回的是

该元素，而

不

是一个列表

。这意味着如

果函数的返

回值只有一

个，那么这个

变量将被直

接返回。

接下

来看①处。在定

义函数时，我

们在参数前

添加了一个

星号。这样就

能在不使用

列表的情况

下调用具有

任意个参数

（可变长参数

）的函数。我们

从下面的示

例可以很清

楚地看出可

变长参数的

用法。

>>> def f(*x):

...

print(x)

>>> f(1,2,3)

(1,

2, 3)

>>> f(1,

2, 3, 4, 5,

6)

(1, 2, 3,

4, 5, 6)

如上面

的代码所示

，如果在定义

函数时给参

数加上星号

，那么在调用

函

步骤 12 可变

长参数（改进

篇)

67

数时，所有

参数就能以

带星号的形

式被一次性

拿到。在对代

码做上述修

改之

后，DeZero的函

数类（Add类）可以

按照下面的

方式使用。

x0

= Variable(np.array(2))

x1 =

Variable(np.array(3))

f = Add()

y = f(x0, x1)

print(y.data)

运

行结果

5

这种

编写方式对

使用Add类的人

来说更自然

。到这里我们

就完成了第

1

项改进，下面

开始进行第

2项改进。

12.2 第2项

改进：使函数

更容易实现

第2项改进针

对的是实现

Add类的人。目前

，要实现Add类，需

要编写图

12

2左

侧所示的代

码。

图122 现在的

代码（左）和改

进后的代码

（右）

class Add(Function):

 def

forward(self, xs):

 x0,

x1 = xs

y = x0 +

x1

 return (y,)

class Add(Function):

 def

forward(self, x0, x1):

y = x0 +

x1

 return y

如图122左侧

的代码所示

，具体的处理

编写在Add类的

forward方法中。

在这

个实现中，参

数以列表的

形式传递，返

回值以元组

的形式返回

。当然，

图12

2右侧

的代码更为

理想。在该代

码下，forward方法的

参数直接接

收变量，

直接

返回结果变

量。第2项改进

就是实现这

样的代码。

第

2阶段　用自然

的代码表达

68

我们按如下

方式修改Function类

，完成第2项改

进。

steps/step12.py

class Function:

 def

__call__(self, *inputs):

 xs

= [x.data for x

in inputs]

 ys

= self.forward(*xs) # ①使用星号

解包

if not isinstance(ys, tuple):

# ②对非元

组情况的额

外处理

 ys

= (ys,)

 outputs

= [Variable(as_array(y)) for y

in ys]

 for

output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

outputs

 return outputs

if len(outputs) > 1

else outputs[0]

首先

是①处的self.forward(*xs)。这里

在调用函数

时在参数前

加上了星号

，

由此可解包

列表。解包是

指将列表中

的元素展开

并将这些元

素作为参数

传递

的过程

。例如在xs = [x0, x1]的情

况下，调用self.forward(*xs)就

相当于调

用

self.forward(x0, x1)。

接着是②处。如

果ys不是元组

，就把它修改

为元组。这样

在forward方

法的实

现中，如果返

回的元素只

有1个，就可以

直接返回这

个元素。基于

这

些修改，我

们可以按如

下方式实现

Add类。

steps/step12.py

class Add(Function):

def forward(self, x0, x1):

y = x0 +

x1

 return y

上面的代

码定义了def forward(self, x0, x1):。此

外，结果可以

写成

return y这种只

返回一个元

素的形式，这

样对实现Add类

的人来说，DeZero

就

更好写了。到

这里，我们就

完成了第2项

改进。

步骤

12 可

变长参数（改

进篇)  69

12.3 add函数的

实现

最后添

加代码，使Add类

能作为Python函数

使用。

steps/step12.py

def add(x0, x1):

return Add()(x0, x1)

我们可

以使用这个

add函数进行如

下计算。

steps/step10.py

x0 = Variable(np.array(2))

x1 = Variable(np.array(3))

y

= add(x0, x1)

print(y.data)

运行

结果

5

通过前

面的改进，我

们可以用更

自然的代码

处理函数的

可变长参数

了。

这里只实

现了加法运

算，基于同样

的做法，我们

还可以实现

乘法运算和

除法

运算。不

过，现在支持

可变长参数

的只有正向

传播。在下一

个步骤，我们

将

实现支持

可变长参数

的反向传播

。

第 2阶段　用自

然的代码表

达 70

步骤 13

可变

长参数（反向

传播篇 )

通过

上一个步骤

的修改，现在

函数可以支

持多个输入

和输出了。我

们还

修改了

正向传播的

实现方式，并

确认了修改

后的代码仍

能正确地进

行计算。

修改

完正向传播

后就轮到反

向传播了。本

步骤将修改

反向传播的

实现。

13.1

支持可

变长参数的

Add类的反向传

播 

在实现反

向传播之前

，我们先来看

看图131中加法

运算的计算

图。

x0

x1

+

(forward)

y

gx0

gx1

+’

(backward)

gy

图131

加法运

算计算图的

正向传播和

反向传播（求

y = x0 +

x1的导数的函

数用+'表示）。

如

图131所示，加法

运算的正向

传播有两个

输入和一个

输出。反向传

播

的情况正

好相反，有一

个输入和两

个输出。从表

达式来看，当

y

= x0 + x1时，

由导数公

式可得 x

y

0

= 1， x

y

1 = 1。 ∂

∂

∂

∂

步骤

13

可变长参数

（反向传播篇

)  71

式子（函数）y

= x0 + x1有

两个输入变

量。这种有多

个输入变量

的函数称

为

多元函数。在

多元函数中

，只对一个输

入变量求导

（将其他变量

视为常数），

得

到的是偏导

数。偏导数的

导数符号是

。拿例子中的

x

y

0

来说，它表示

我们只关注

x0并对其求导

，x0以外的变量

一律视为常

数。本书在后

面提

到偏导

数时，也会将

其简称为导

数。此外，即使

是仅有一个

变量的情况

，

在式子中我

们也会使用

符号 。

在加法

运算的反向

传播中，从输

出端传播的

导数乘以1后

的值就是输

入

变量(x0、x1)的导

数。换言之，加

法运算的反

向传播就是

把上游的导

数原封

不动

地“传走”。考虑

到这些内容

，我们按如下

方式实现Add类

。

steps/step13.py

class Add(Function):

 def

forward(self, x0, x1):

y = x0 +

x1

 return y

def backward(self, gy):

return gy, gy

在上面的代

码中，backward方法有

一个输入和

两个输出。当

然，为了支

持

这种有多个

返回值的情

况，我们必须

修改反向传

播的实现。在

DeZero中，

这是通过

Variable类的backward方法实

现的。

13.2 修改Variable类

现在来看看

Variable类的backward方法。复

习一下当前

Variable类的代码，

具

体如下所示

。

steps/step12.py

class Variable:

...

 def backward(self):

∂

∂

∂

∂

第 2阶段　用自

然的代码表

达

72

 if self.grad

is None:

 self.grad

= np.ones_like(self.data)

 funcs

= [self.creator]

 while

funcs:

 f =

funcs.pop() 

 x,

y = f.input, f.output

# ①获取函数

的输入和输

出

 x.grad

= f.backward(y.grad) # ②调用backward方法



if x.creator is not

None:

 funcs.append(x.creator)

这里需要注

意的是阴影

部分的代码

。首先，while循环中

①的部分用于

获取函数的

输入输出变

量。②的部分用

于调用函数

的backward方法。目前

，

①的代码只支

持函数的输

入输出变量

只有一个的

情况。下面我

们对代码进

行

修改，使其

能够支持多

个变量。修改

后的代码如

下所示。

steps/step13.py

class

Variable:

 ...

def backward(self):

 if

self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 gys

= [output.grad for output

in f.outputs] # ①

gxs = f.backward(*gys) #

②

 if not

isinstance(gxs, tuple): # ③

gxs = (gxs,)

for x, gx in

zip(f.inputs, gxs): # ④

x.grad = gx

if x.creator is not

None:

 funcs.append(x.creator)

这里

共修改了4处

。①处将输出变

量outputs的导数汇

总在列表中

。②

步骤 13 可变长

参数（反向传

播篇)

73

处调用

了函数f的反

向传播。这里

调用了f.backward(*gys)这种

参数前带有

星号的函数

对列表进行

解包（展开）。③处

所做的处理

是，当gxs不是元

组时，

将其转

换为元组。

代

码中的②和③处

与上一个步

骤改进正向

传播的做法

相同。②的代码

在调

用Add类的

backward方法时，将参

数解包后传

递。③的代码使

得Add类

的backward方法

可以简单地

返回元素而

不是元组。

代

码中的④处将

反向传播中

传播的导数

设置为Variable的实

例变量

grad。这里

，gxs和f.inputs的每个元

素都是一一

对应的。准确

来说，如果

有

第i个元素，那

么f.input[i]的导数值

对应于gxs[i]。于是

代码中使用

zip

函数和for循环

来设置每一

对的导数。以

上就是Variable类的

新backward方法。

13.3 Square类的

实现

现在Variable类

和Function类已经支

持可变长的

输入和输出

了。我们还

实

现了一个Add类

作为具体的

函数。最后，我

们需要改进

目前使用的

Square类，

使其支持

新的Variable类和Function类

。要修改的地

方只有一处

（阴影部分）。

steps/step13.py

class Square(Function):

 def

forward(self, x):

 y

= x ** 2

return y

 def

backward(self, gy):

 x

= self.inputs[0].data # 修

改前为

x = self.input.data

gx = 2 *

x * gy

return gx

如上

面的代码所

示，由于Function类的

实例变量已

经从input（单数形

式）变为inputs（复数

形式），所以Square需

修改为从inputs中

取出输入变

量x。

第 2阶段　用

自然的代码

表达 74

这样新

的Square类就完成

了。下面使用

add函数和square函数

实际进行计

算。

steps/step13.py

x =

Variable(np.array(2.0))

y = Variable(np.array(3.0))

z = add(square(x), square(y))

z.backward()

print(z.data)

print(x.grad)

print(y.grad)

运行结果

13.0

4.0

6.0

上面的代码

计算了z

= x2 + y2

。在使

用DeZero的情况下

，这个计算可

以写成z = add(square(x), square(y))的形

式。之后只要

调用z.backward()

就能自

动求出导数

了。

通过以上

修改，我们实

现了支持多

个输入和输

出的自动微

分的机制。后

面只要按部

就班地编写

需要的函数

，就可以实现

更复杂的计

算。不过，当前

的DeZero还存在一

个问题。在下

一个步骤，我

们将解决这

个问题。

步骤

14 重复使用同

一个变量

75

步

骤 14

重复使用

同一个变量

当前的DeZero有一

个问题，每当

重复使用同

一个变量，这

个问题就会

出现。

例如，图

141所示的y = add(x,

x)的情

况。

x + y

图141 y =

add(x, x)的计算

图

DeZero在用相同

变量进行加

法运算时不

能正确求导

。下面测试一

下，

看看实际

的结果是什

么样的。

x = Variable(np.array(3.0))

y

= add(x, x)

print('y',

y.data)

y.backward()

print('x.grad', x.grad)

运行

结果

y 6.0

x.grad

1.0

上面的

代码以x = 3.0进行

了加法运算

。在这种情况

下，y的值是6.0，

这

是正确的结

果。但是，x的导

数（x.grad）是 1.0，这是错

误的结果。当

第

2阶段　用自

然的代码表

达 76

y

= x + x时，y

= 2x，所以导

数的正确结

果为 x

y

= 2。

14.1 问题的

原因

为什么

会出现错误

的结果呢？原

因在于下面

Variable类中阴影部

分的代码。

steps/step13.py

class Variable:

...

 def backward(self):

if self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 gys

= [output.grad for output

in f.outputs]

 gxs

= f.backward(*gys)

 if

not isinstance(gxs, tuple):

gxs = (gxs,)

for x, gx in

zip(f.inputs, gxs):

 x.grad

= gx # 这

里有错误！！

if x.creator is not

None:

 funcs.append(x.creator)

如

上所示，当前

代码是直接

用从输出端

传播的导数

进行赋值的

。因此，

在计算

中重复使用

同一个变量

时，传播的导

数的值会被

替换。拿上面

的加法

运算

来说，导数的

传播如图142所

示。

∂

∂

步骤 14 重复

使用同一个

变量

77

x +’ y

1

1

1

图14

2 y = add(x,

x)的反

向传播（箭头

上方和下方

显示了传播

的导数的值

）

图142显示了传

播的导数的

值。在这个例

子中，x的导数

的正确结果

是

1+1=2。也就是说

，我们要求出

传播的导数

的和，但是，目

前的实现方

式

是覆盖这

个值。

14.2 解决方

案

解决方案

很简单。基于

以上内容，我

们按如下方

式修改Variable类。

steps/step14.py

class Variable:

...

 def backward(self):

if self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 gys

= [output.grad for output

in f.outputs]

 gxs

= f.backward(*gys)

 if

not isinstance(gxs, tuple):

gxs = (gxs,)

for x, gx in

zip(f.inputs, gxs):

 if

x.grad is None:

x.grad = gx

else:

 x.grad =

x.grad + gx

第

2阶段　用自然

的代码表达

78

 if

x.creator is not None:

funcs.append(x.creator)

如上面的代

码所示，如果

是第1次设置

导数（grad），做法就

和此前一样

，

直接用从输

出端传来的

导数进行赋

值。之后如果

再传来导数

，则“加上”这

个

导数。

上面的

代码使用x.grad = x.grad +

gx对

导数进行了

加法运算。看

起来

这行代

码也可以改

为x.grad += gx这种使用

了加法赋值

运算符（+=）的形

式，但实际上

这种写法会

出现问题。具

体原因和背

景知识有些

复杂，而且

脱

离了深度学

习的本质，所

以在此不进

行讨论。感兴

趣的读者请

参考本书

的

附录A，其中详

细介绍了这

个问题。

这样

就可以重复

使用同一个

变量了。下面

再来试试之

前出现问题

的计算。

steps/step14.py

x = Variable(np.array(3.0))

y

= add(x, x)

y.backward()

print(x.grad)

运行

结果

2.0

运行上

面的代码，我

们得到了正

确结果2.0。接着

，试着把x连加

三次。

steps/step14.py

x = Variable(np.array(3.0))

y = add(add(x, x),

x)

y.backward()

print(x.grad)

运行结

果

3.0

步骤 14 重复

使用同一个

变量

79

结果是

3.0。根据y = x

+ x + x

= 3x可知导

数为3，与代码

运行结果一

致。

这样，重复

使用同一个

变量的实现

就完成了。

14.3

重

置导数

本步

骤要做的唯

一修改就是

在代码中加

上反向传播

的导数。不过

，我们

要注意

使用同一个

变量进行不

同计算的情

况。笔者以下

面的代码为

例进行说明

。

#

第1个计算

x = Variable(np.array(3.0))

y = add(x, x)

y.backward()

print(x.grad)

# 第

2个计算（使用

同一个x进行

其他计算）

y = add(add(x, x),

x)

y.backward()

print(x.grad)

运

行结果

2.0

5.0

上面

的代码做了

2个导数计算

。假如为了节

省内存要重

复使用Variable

实例

的x，那么在第

2次使用x时，x的

导数会加在

第1次使用x时

的导数上。

因

此，第2次求出

的导数5.0是错

误的计算结

果，正确结果

是3.0。

为了解决

这个问题，我

们要在Variable类中

添加一个名

为cleargrad的

方法来

初始化导数

。

steps/step14.py

class Variable:

 ...

def cleargrad(self):

 self.grad

= None

第 2阶段　用自

然的代码表

达 80

cleargrad方法用于

初始化导数

。我们只要在

方法中设置

self.grad = 

None即可。这个方

法可以帮助

我们利用同

一个变量求

出不同计算

的导数。拿

前

面的例子来

说，代码可以

写成下面这

样。

# 第1个计算

x =

Variable(np.array(3.0))

y = add(x,

x)

y.backward()

print(x.grad) #

2.0

# 第2个计算（使

用同一个x进

行不同的计

算）

x.cleargrad()

y = add(add(x, x),

x)

y.backward()

print(x.grad) #

3.0

运行结果

2.0

3.0

这次正确求

出了第2个计

算的导数（第

2个计算的导

数为3.0，结果正

确）。

因此，在调

用第2个计算

的y.backward()之前调用

x.cleargrad()，就可以重

置

变量中保存

的导数。这样

就可以使用

同一个变量

来执行其他

计算了。

DeZero 的

cleargrad方

法可以用来

解决优化问

题。优化问题

是寻找

函数

的最小值或

最大值的问

题。例如，在步

骤 28 中，我们会

最小化

Rosenbrock函数

，届时将使用

cleargrad方法。

到这里

，本步骤的内

容就结束了

。通过这个步

骤的工作，Variable类

进

一步得到

了提升。不过

还有一个重

要的问题需

要解决。在下

一个步骤，我

们

将解决这

个问题。问题

解决后，Variable类就

完成了。

步骤

15 复杂的计算

图（理论篇）  81

步

骤 15

复杂的计

算图（理论篇

）

前面我们处

理的都是图

15

1这种笔直的

计算图。

x y

图15

1 笔

直的计算图

然而，变量和

函数并不局

限于这种简

单的连接方

式。我们的DeZero已

经得到了一

定的发展，现

在可以创建

像图152这样的

计算图了。

x

y

图

152 更复杂的“连

接”的例子

图

152所示的计算

重复使用了

同一个变量

，也使用了支

持多个变量

的函数。

通过

这样的方式

，我们可以建

立更复杂的

“连接”。不过遗

憾的是，DeZero

不能

正确地求出

这类计算的

导数。准确来

说，它无法正

确地进行这

种复杂“连

接

”的反向传播

。

  第

2阶段　用自

然的代码表

达 82

图的连接

方式叫作网

络拓扑（topology）。本步

骤的目标是

支持各种拓

扑结

构的计

算图。我们将

引入新的思

路，让DeZero不管计

算图的连接

方式是

什么

样的，都能正

确求导。

15.1 反向

传播的正确

顺序

DeZero的问题

出在哪里呢

？为了找出原

因，我们来思

考一下图153这

个相对简单

的计算图。当

前的DeZero针对这

个计算图进

行计算会得

出错误

的导

数。

x

a A y D

b B

c C

图153 中途出

现分支后，分

支又连接在

一起的计算

图示例

我们

要注意图15

3中

的变量a，它是

在计算过程

中出现的变

量。通过上

一

个步骤可知

，对于重复使

用同一变量

的情况，我们

需要在反向

传播时加上

从输出端传

来的导数。因

此，要想求出

a的导数（式子

为 a

y），就要使用

从a

的输出端

传来的两个

导数。这两个

导数传播出

去之后，导数

就可以从a向

x

传播了。基于

以上内容，反

向传播的流

程如图154所示

。

∂

∂

步骤 15 复杂的

计算图（理论

篇）

83

y D

b

c

a y D

b B

c

a

y D

b B

c C

x a

A y D

b

B

c C

图15

4 反向传

播的正确顺

序

图154是由变

量y向变量x传

播导数的流

程。再次强调

一下，这里需

要

注意的是

“在向变量a传

播两个导数

之后，从a向x传

播导数”这一

点。此时，

从函

数的角度来

看，反向传播

是按照D、B、C、A的顺

序进行的。不

过，B和

第 2阶段

用自然的代

码表达 84

C的顺

序可以调换

，所以D、C、B、A的顺序

也正确。在进

行函数A的反

向传

播之前

，要先完成函

数B和函数C的

反向传播。

15.2 当

前的DeZero

当前的

DeZero会按照图15

4所

示的顺序进

行反向传播

吗？先来看看

当

前Variable类的实

现代码。注意

看下面的阴

影部分。

steps/step14.py

class

Variable:

 ...

def backward(self):

 if

self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 gys

= [output.grad for output

in f.outputs]

 gxs

= f.backward(*gys)

 if

not isinstance(gxs, tuple):

gxs = (gxs,)

for x, gx in

zip(f.inputs, gxs):

 if

x.grad is None:

x.grad = gx

else:

 x.grad =

x.grad + gx

if x.creator is not

None:

 funcs.append(x.creator)

需要

注意的是funcs列

表。在while循环中

，我们将待处

理的候选函

数添

加到了

funcs列表的末尾

（funcs.append(x.creator)），然后，从列表

末尾取出

下

一个要处理

的函数（funcs.pop()）。根据

这个处理流

程，反向传播

会按照图

155所

示的流程进

行。

步骤 15 复杂

的计算图（理

论篇）

85

y D

b

c

a y D

b

c C

x

a A y D

b

c C

x

a A y D

b B

c C

x a A y

D

b B

c

C

图155 当前

DeZero的反向传播

的流程

第 2阶

段　用自然的

代码表达

86

从

图155可以看出

，要处理的函

数的顺序为

D、C、A、B、A。问题有两个

：

一个是C后面

是A；一个是函

数A的反向传

播被调用了

两次。对比一

下前面的

代

码，看看为什

么会出现这

样的问题。

首

先，D被添加到

funcs列表中，流程

从状态[D]开始

。接下来取出

函数D，

然后将

D的输入变量

（D.inputs）的创造者B和

C添加到funcs列表

中（图156）。

# funcs = [D]

f = funcs.pop() #

D

...

for x

in f.inputs:

 funcs.append(x.creator)

# funcs = [B,

C]

x a A

y D

b B

c C

图156

函数

D的反向传播

（右侧为funcs列表

相关的代码

）

此时funcs列表的

值是[B, C]。之后，该

列表的最后

一项C会被取

出。然

后C的输

入变量的创

造者A会被添

加到列表中

。此时，funcs列表的

值变为[B,

A]（图157）。

图

157

函数C的反向

传播和相应

的代码

# funcs =

[B, C]

f =

funcs.pop() # C

#

funcs = [B]

...

for x in f.inputs:

funcs.append(x.creator)

# funcs =

[B, A]

x a

A y D

b

B

c C

最后

取出了列表

末尾的A，问题

就出在这里

。这里本应该

取出B，结果取

出了A。

到目前

为止，我们处

理的都是笔

直的计算图

。对于这样的

计算图，我们

可

以从列表

中取出元素

，无须考虑要

处理的函数

的顺序。这是

因为从列表

取

出元素时

，列表中总是

只有一个元

素。

步骤

15 复杂

的计算图（理

论篇）  87

15.3 函数的

优先级

funcs列表

包含了接下

来要处理的

候选函数列

表，但现在的

做法是（直接

）

取出候选列

表中的最后

一个元素。当

然，我们需要

的是从funcs列表

中取出

合适

的函数。拿刚

才的例子来

说，我们要从

[B, A]列表中取出

更接近输出

方

的B。解决这

个问题的办

法是给函数

赋予优先级

。如果B的优先

级比A的优先

级高，就可以

先取出B。

应当

如何设置优

先级呢？其中

一种方法是

“解析”给定的

计算图。比如

可以使用一

种叫作“拓扑

排序”的算法

，根据节点的

连接方式给

节点排序。

这

种排好的顺

序就是优先

级。还有一种

不依赖该算

法的简单方

法，这种方法

我们见过。

我

们见过在进

行普通计算

（即正向传播

）时，函数生成

变量的过程

。换言之，

我们

已经知道了

哪个函数生

成了哪个变

量。由此，我们

可以按照图

158的方

式记录

函数和变量

的“辈分”关系

。

x

第0代 第1代

第

2代 第3代

a A

y D

b B

c C

图158

正

向传播中函

数和变量的

“辈分”关系

图

158所示的“辈分

”关系正好与

优先级相对

应。在反向传

播时，如果

按

照从后代到

先代的顺序

处理，就可以

保证“子辈”在

“父辈”之前被

取出。

以图158为

例，在函数B和

函数A之间做

选择时，会先

取出“后代”B。以

上

就是按照

正确的顺序

进行反向传

播的方法，在

下一个步骤

，我们将实现

这种

方法。

第

2阶段　用自然

的代码表达

88

步骤

16

复杂的

计算图（实现

篇）

本步骤将

实现上一步

骤提到的方

法。首先在正

向传播中实

现“辈分”的

设

置，然后在反

向传播中，按

照从后代到

先代的顺序

取出函数。修

改完成后，

无

论计算图多

么复杂，反向

传播都会按

照正确的顺

序进行。

16.1 增加

“辈分”变量

首

先在Variable类和Function类

中增加实例

变量generation。generation

表示函

数（或变量）属

于哪一代。下

面先来看看

Variable类的代码，类

中增加

的代

码如下所示

。

steps/step16.py

class

Variable:

 def __init__(self,

data):

 if data

is not None:

if not isinstance(data, np.ndarray):

raise TypeError('{} is not

supported'.format(type(data)))

 self.data =

data

 self.grad =

None

 self.creator =

None

 self.generation =

0

 def set_creator(self,

func):

 self.creator =

func

 self.generation =

func.generation + 1

...

步骤 16 复杂的

计算图（实现

篇）

89

Variable类将generation初始

化为0。之后，当

set_creator方法被调用

时，

它将generation的值

设置为父函

数的generation值加1。如

图161所示，由

f.generation的

值为2的函数

创建的变量

，其y.generation的值为3。以

上就

是对Variable类

所做的修改

。

2 3

f y

图161

变量的generation的

关系图（generation的值

显示在节点

上方）

接下来

是 Function类。Function类的 generation被

设置为与输

入变量

的generation相

同的值。如图

162的左图所示

，输入变量只

有一个，它的

generation的值为4，这时

函数的generation的值

也为4。

4 4

3

4

4

y

D

b

c

f

图162 函数

的generation的关系图

在有多个输

入变量的情

况下，要采用

其中最大的

generation的值。如图

16

2的

右图所示，输

入变量有两

个，它们的generation的

值分别为3和

4，此

时要将函

数的generation设为4。为

了满足以上

几点，我们在

Function类中

添加以

下代码。

第 2阶

段　用自然的

代码表达 90

steps/step16.py

class Function(object):

def __call__(self, *inputs):

xs = [x.data for

x in inputs]

ys = self.forward(*xs)

if not isinstance(ys, tuple):

ys = (ys,)

outputs = [Variable(as_array(y)) for

y in ys]

self.generation = max([x.generation for

x in inputs])

for output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

outputs

 return outputs

if len(outputs) > 1

else outputs[0]

 ...

上

面阴影部分

的代码用来

设置Function的generation。

16.2 按照

“辈分”顺序取

出元素

通过

以上修改，在

进行普通计

算（即正向传

播）时，变量和

函数中会设

置好generation的值。下

面来看图16

3所

示的计算图

。

x a A

y D

b B

c C

0 0

1

1

1

2

2 3

2

图16

3 计算图中

“辈分”的例子

从图163可以看

到，函数A的generation的

值是0，函数B和

函数C的

步骤

16

复杂的计算

图（实现篇）  91

generation的

值是1，函数D的

generation的值是2。按照

这种方式设

置之后，

DeZero在反

向传播中就

可以按照正

确的顺序取

出函数了。例

如，generation

的值更大

的函数B和函

数C可以先于

函数A被取出

。

如前所述，在

Variable类的backward方法中

，我们把要处

理的候补函

数

放入funcs列表

中。之后从列

表中取出函

数时，首先取

出generation的

值最大

的函数，这样

就可以按照

正确的顺序

进行函数的

反向传播了

。

接下来要实

现的是按照

“辈分”顺序取

出函数。在此

之前，我们先

用一

个虚拟

的DeZero函数做一

个简单的实

验。

>>>

generations = [2, 0,

1, 4, 2]

>>>

funcs = []

>>>

for g in generations:

... f = Function()

# 虚拟的函

数类

... f.generation

= g

... funcs.append(f)

>>> [f.generation for f

in funcs]

[2, 0,

1, 4, 2]

上面的

代码准备了

虚拟的函数

，并将其添加

到了funcs列表中

。下面我

们从

这个列表中

取出generation的值最

大的函数。代

码如下所示

。

>>> funcs.sort(key=lambda x:

x.generation)

>>> [f.generation for

f in funcs]

[0,

1, 2, 2, 4]

>>> f = funcs.pop()

>>> f.generation

4

上面的代码

通过列表的

sort方法将列表

按generation的值从小

到大的顺

序

排列。具体做

法是指定key=lambda x: x.generation作

为sort方法的参

数，

第 2阶段　用

自然的代码

表达 92

如果x是

列表中的元

素，则列表中

的元素按照

x.generation的值从小到

大的

顺序排

列。然后，使用

pop方法取出列

表的最后一

个元素，这个

元素就是

generation的

值最大的函

数。

我们这里

要做的只是

取出generation的值最

大的函数，所

以没有必要

像

前面那样

对所有元素

重新排序。更

高效的做法

是使用优先

队列算法，不

过本书没有

采用这种方

法，感兴趣的

读者可以自

行实现（提示

：Python有

heapq模块）。

16.3 Variable类的

backward

言归正传，我

们来看一下

Variable类的backward方法是

如何实现的

。重

点看修改

的部分（阴影

部分）。

steps/step16.py

class

Variable:

 ...

def backward(self):

 if

self.grad is None:

self.grad = np.ones_like(self.data)

funcs = []

seen_set = set()

def add_func(f):

 if

f not in seen_set:

funcs.append(f)

 seen_set.add(f)

funcs.sort(key=lambda x: x.generation)

add_func(self.creator)

 while funcs:

f = funcs.pop()

gys = [output.grad for

output in f.outputs]

gxs = f.backward(*gys)

if not isinstance(gxs, tuple):

gxs = (gxs,)

步骤

16 复

杂的计算图

（实现篇）  93

for x, gx in

zip(f.inputs, gxs):

 if

x.grad is None:

x.grad = gx

else:

 x.grad =

x.grad + gx

if x.creator is not

None:

 add_func(x.creator)

上面

的代码添加

了add_func函数。此前

向列表中添

加DeZero函数时调

用的是funcs.append(f)，这里

改为调用add_func函

数。在这个add_func函

数

中，DeZero函数的

列表将按照

generation的值排序。这

样一来，在之

后取出

DeZero函数

时，我们就可

以使用funcs.pop()取出

generation的值最大的

函

数了。

顺带

一提，上面的

代码在backward方法

中定义了add_func函

数。这种用

法

适用于满足

以下两个条

件的情况。

 只

在父方法（backward方

法）中使用



需

要访问父方

法（backward方法）中使

用的变量（funcs、seen_set）

由

于add_func函数满足

这两个条件

，所以我们将

它定义在了

方法中。

上面

的实现使用

了一个名为

seen_set的集合（set）。该集

合的作用是

防止同一个

函数被多次

添加到funcs列表

中，由此可以

防止一个函

数的

backward方法被

错误地多次

调用。

16.4 代码验

证

现在我们

可以按照generation的

值从大到小

的顺序取出

函数了。这样

一来，

无论计

算图多么复

杂，反向传播

应该都能以

正确的顺序

进行。下面试

着求出

图164这

个计算的导

数。计算图和

代码如下所

示。

第 2阶段　用

自然的代码

表达 94

x a square y

add

b

c

square

square

图164 此前

无法正确处

理的计算图

的例子

steps/step16.py

x = Variable(np.array(2.0))

a = square(x)

y

= add(square(a), square(a))

y.backward()

print(y.data)

print(x.grad)

运行

结果

32.0

64.0

从运行

结果来看，求

出的x的导数

是64.0。我们使用

式子来确认

一下，

也就是

求y =

(x2

)

2 +

(x2

)

2

，即y

= 2x4的导数

。由于y′

= 8x3

，所以x = 2.0

时

的导数为64.0，与

上面的运行

结果一致。

我

们终于可以

处理复杂的

计算图了。图

164所示的计算

图比较简单

，但

其实DeZero已经

可以对连接

方式比较复

杂的计算图

求导了，比如

图165这

样的计

算图。

步骤 16 复

杂的计算图

（实现篇）

95

图165 连

接方式更加

复杂的计算

图的例子（步

骤35中实际创

建的y

= tanh(x)的四阶

导

数的计算

图）

第 2阶段　用

自然的代码

表达 96

本步骤

到此结束。这

个步骤是整

本书中比较

难的一个部

分。理解了这

个

步骤的内

容，大家就能

很快感受到

DeZero的威力了。在

下一个步骤

，我们

来看看

DeZero的性能，特别

是它的内存

使用情况。

步

骤

17 内存管理

和循环引用

97

步骤

17

内存管

理和循环引

用

DeZero是一个重

视教学的通

俗易懂的框

架，所以牺牲

了一些性能

。我

们此前确

实也没有关

注速度和内

存的使用情

况。在本步骤

和下一个步

骤，我

们会向

DeZero引入一些可

以提高性能

的技术。首先

来学习一下

Python的内

存管理

。

Python是一门编程

语言，它其实

也是一个执

行Python代码的程

序。这个

程序

通常称为Python解

释器。默认的

Python解释器是CPython，它

是用

C语言实

现的。本步骤

按照CPython的做法

对Python内存管理

进行说明。

17.1 内

存管理

Python会自

动从内存中

删除不再需

要的对象。这

个功能非常

好，有效

减少

了用户自行

管理内存的

情况。不需要

的对象会被

Python解释器（在幕

后）

释放出来

，这样我们就

可以专注于

更重要的编

程任务了。但

是，如果代码

写

得不好，就

可能出现内

存泄漏或内

存不足等情

况，特别是神

经网络经常

处理

大量数

据，更容易碰

到这些情况

。因此，如果内

存管理做得

不好，就很可

能

因内存耗

尽而花费大

量运行时间

（如果在GPU上运

行，会导致内

存不足，程

序

无法继续运

行）。

下面简单

了解一下Python是

如何管理内

存的。Python（准确来

说是

第 2阶段

用自然的代

码表达 98

CPython）使用

两种方式管

理内存：一种

是引用计数

，另一种是分

代垃圾回收

。

这里我们把

后者称为GC（Garbage Collection，垃

圾回收）。首先

来看引用计

数的相关内

容。

有些资料

也将引用计

数方式的内

存管理称为

垃圾回收（GC）。本

书把这种

方

式的内存管

理称为“引用

计数”，不将其

纳入GC的范畴

。

17.2 引用计数方

式的内存管

理

Python的基础内

存管理方式

是引用计数

。引用计数的

机制很简单

（因此

效率很

高）。每个对象

在被创建时

的引用计数

为0，当它被另

一个对象引

用时，

引用计

数加1，当引用

停止时，引用

计数减1。最终

，当引用计数

变为0时，

Python解释

器会回收该

对象。在引用

计数中，当对

象不再被需

要时，会立

即

从内存中删

除。这就是引

用计数方式

的内存管理

。

以下是导致

引用计数增

加的情况。

 使

用赋值运算

符时

 向函数

传递参数时



向容器类型

对象（列表、元

组和类等）添

加对象时

上

述情况会导

致引用计数

增加。示例代

码如下所示

。

class obj:

pass

def f(x):

print(x)

a = obj()

# 引用计数为

1

f(a) #

进入函数后

引用计数为

2

# 离开函数后

引用计数为

1

a

= None # 引用计数为

0

步骤 17 内存管

理和循环引

用

99

在上面的

代码中，a是由

obj()创建的对象

A 的引用。此时

该对象的引

用

计数为1。之

后调用了函

数f(a)，其中a作为

参数传递给

函数，所以在

函数

作用域

内，引用计数

加1（合计为2）。在

对象离开函

数作用域时

，引用计数减

1。

最后，当a = None时，对

象的引用计

数为0（它不再

被任何对象

引用）。此时，

对

象会立即从

内存中释放

。

由此可见，引

用计数的机

制很简单。不

过这个简单

的机制解决

了许多内

存

使用相关的

问题。我们再

来看看下面

的示例代码

。

a

= obj()

b =

obj()

c = obj()

a.b = b

b.c

= c

a =

b = c =

None

上面的代码

创建了a、b、c这3个

对象。a引用了

b，b引用了c。此时

，

对象之间的

关系如图171左

图所示。

a

1

b

2

c

2

a

0

b

1

c

1

图171 对

象关系图（虚

线表示引用

，数字表示引

用计数）

A

在Python中

，一切皆为对

象。类和函数

是对象，基于

类创建的实

例也是对象

。在本步骤中

，我们将

实例

称为对象。

第

2阶段　用自然

的代码表达

100

如图171右图所

示，当a

= b = c

= None时，对象

之间的关系

发生变化。

此

时a的引用计

数变为0（b和c的

引用计数为

1）。因此，a立即被

删除。删除

a导

致b的引用计

数从1变成0，因

此b也被删除

。同理，删除b导

致c的引用

计

数从1变成0，c也

被删除。这是

一种类似于

多米诺骨牌

的机制，可以

一次

性删除

用户不再使

用的对象。

这

就是Python的内存

管理方法——引

用计数。它解

决了很多内

存管理

的问

题，但是，有一

个问题是不

能用引用计

数来解决的

，这个问题就

是循环

引用

。

17.3 循环引用

在

了解循环引

用之前，我们

先来看一段

示例代码。

a = obj()

b

= obj()

c =

obj()

a.b = b

b.c = c

c.a

= a

a =

b = c =

None

上

面的代码与

之前的代码

几乎相同，唯

一的区别是

这次增加了

一个从c

到a的

引用。这时，3个

对象呈环状

相互引用。这

种状态就是

循环引用。a、b、

c之

间的关系如

图17

2所示。

步骤

17 内存管理和

循环引用

101

a

2

b

2

c

2

a

1

b

1

c

1

图

172 循环引用情

况下的对象

关系图（虚线

表示引用）

图

172右图中的a、b、c的

引用计数均

为1。这时用户

已无法访问

这3个

对象（也

就是说，它们

是没有用的

对象）。但是，如

果只设置了

a =

b = c =

None，那么此时因

为循环引用

，引用计数不

会为0，对象也

不会从内存

中释

放出来

。这时就需要

使用第2种方

法了。这种方

法就是GC（准确

来说是分代

垃圾回收）。

GC比

引用计数更

智能，它可以

判断对象是

否有用（GC的原

理很复杂，

本

书不对其进

行介绍）。与引

用计数不同

，GC会在内存不

足等情况下

自动被

Python解释

器调用。GC也支

持显式调用

，具体做法是

导入gc模块，然

后

调用gc.collect()。

GC能够

正确处理循

环引用。因此

在使用Python编程

时，我们通常

不需

要关心

循环引用。不

过，（与没有循

环引用时的

情况相比）使

用GC推迟内存

释放会导致

程序整体的

内存使用量

增加（详见参

考文献[10]）。内存

是机器学习

，

尤其是神经

网络运算时

的重要资源

。因此，在DeZero的开

发过程中，建

议

避免循环

引用。

以上就

是Python的内存管

理的基础知

识。现在我们

把目光转回

DeZero。

其实当前的

DeZero中存在循环

引用，就在图

17

3所示的变量

和函数部分

。

  第

2阶段　用自

然的代码表

达 102

Variable

Variable

outputs

creator

inputs

Function

图173 Variable和Function的循

环引用

如图

173所示，Function实例引

用了输入和

输出的Variable实例

。同时，

Variable实例也

引用了作为

创建者的Function实

例。这时，Function实例

和

Variable实例之间

就存在循环

引用关系。我

们可以使用

作为Python标准模

块的weakref来避免

循环引用。

17.4 weakref模

块

在Python中，我们

可以使用weakref.ref函

数来创建弱

引用。弱引用

是

在不增加

引用计数的

情况下引用

另一个对象

的功能。下面

是使用weakref.

ref函数

的例子。

>>> import weakref

>>> import numpy as

np

>>> a =

np.array([1, 2, 3])

>>>

b = weakref.ref(a)

>>>

b

<weakref at 0x103b7f048;

to 'numpy.ndarray' at 0x103b67e90>

>>> b()

[1 2

3]

上面

的代码选用

了ndarray实例作为

对象。a是它的

引用，b是它的

弱引用。

b的输

出表明它是

ndarray的弱引用（weakref）。我

们可以编写

b()来实际访问

该引用中的

数据。

步骤

17 内

存管理和循

环引用  103

接着

在上面的代

码之后运行

a = None。结果如下所

示。

>>>

a = None

>>>

b

<weakref at 0x103b7f048;

dead>

如代码所

示，ndarray实例通过

引用计数这

一内存管理

方式被删除

，b虽

然引用了

这个对象，但

由于是弱引

用，所以对引

用计数没有

影响。这时我

们

来看b的输

出，会发现有

dead出现，这表明

ndarray实例已经被

删除。

在Python解释

器上运行是

这里展示的

弱引用的示

例代码正常

工作的前提

。

如果是在IPython或

Jupyter Notebook等解释器上

运行，b的输出

中不会

出现

dead，因为这些解

释器会在幕

后持有额外

的引用。

下面

将weakref机制引入

DeZero中。阴影部分

是要向Function类添

加的

代码。

steps/step17.py

import

weakref

class Function:

def __call__(self, *inputs):

xs = [x.data for

x in inputs]

ys = self.forward(*xs)

if not isinstance(ys, tuple):

ys = (ys,)

outputs = [Variable(as_array(y)) for

y in ys]

self.generation = max([x.generation for

x in inputs])

for output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

[weakref.ref(output) for output in

outputs]

 return outputs

if len(outputs) > 1

else outputs[0]

...

第

2阶段　用自然

的代码表达

104

设置实例变

量self.outputs的代码被

改为拥有对

象的弱引用

的代码。这样

，

函数的输出

变量会变成

弱引用。这处

修改完成后

，我们还需要

修改其他类

引

用Function类的outputs的

代码。目前，我

们需要按如

下方式修改

Variable

类的backward方法。

steps/step17.py

class

Variable:

 ...

def backward(self):

 ...

while funcs:

 f

= funcs.pop()

 #

gys = [output.grad for

output in f.outputs]

gys = [output().grad for

output in f.outputs]

...

上

面的代码将

[output.grad for ...]改为[output().grad

for...]。这就

解

决了DeZero中循环

引用的问题

。

17.5 代码验证

下

面我们在没

有循环引用

的新DeZero的基础

上运行以下

代码。

steps/step17.py

for i

in range(10):

 x

= Variable(np.random.randn(10000)) # 大量数

据

y = square(square(square(x))) #

进行复杂

的计算

上面

的代码使用

循环来多次

进行计算。循

环中的引用

情况如图174所

示。

步骤

17 内存

管理和循环

引用  105

x y square square

square

creator

inputs

creator

inputs

creator

inputs

图17

4 x和y被

用户引用时

的关系图

之

后，从图174所示

的状态开始

进行下一个

计算（for语句中

的第2次计

算

）时，x和y将被覆

盖。此时，用户

就不能再次

访问之前的

计算图了。由

于

引用计数

减至0，所以计

算图中使用

的所有内存

会被立刻清

除。这就解决

了

DeZero中循环引

用的问题。

我

们可以借助

memory

profiler（参考文献[11]）等

外部库来监

测Python

中的内存

使用情况。实

际监测前面

代码的内存

使用情况，可

以发现内存

的

使用量没

有增加。

第 2阶

段　用自然的

代码表达 106

步

骤 18

减少内存

使用量的模

式

在上一个

步骤，我们学

习了Python的内存

管理。本步骤

将针对DeZero

的内

存使用量进

行两项改进

。第1项改进是

减少反向传

播消耗的内

存使用量，

这

项改进提供

了立即清除

无用导数的

机制。第2项改

进是提供“不

需要反向

传

播时的模式

”，该模式可以

省去不必要

的计算。

18.1

不保

留不必要的

导数

第1项改

进针对的是

DeZero的反向传播

。目前在DeZero中，所

有的变

量都

保留了导数

，比如下面这

个例子。

x0

= Variable(np.array(1.0))

x1 =

Variable(np.array(1.0))

t = add(x0,

x1)

y = add(x0,

t)

y.backward()

print(y.grad, t.grad)

print(x0.grad, x1.grad)

运行

结果

1.0

1.0

2.0 1.0

步骤

18 减

少内存使用

量的模式  107

在

上面的代码

中，用户提供

的变量是x0和

x1。变量t和y是通

过计算产

生

的。当用y.backward()计算

导数时，所有

变量都会保

留它们的导

数。不过

在多

数情况下，尤

其在机器学

习中，只有终

端变量（x0、x1）的导

数才需要通

过反向传播

求得。在上面

的例子中，y或

t等中间变量

的导数基本

用不到。因此

，

我们可以增

加一种消除

这些中间变

量的导数的

模式。为此，我

们要在当前

Variable类的backward方法中

添加以下阴

影部分的代

码。

steps/step18.py

class Variable:

...

 def backward(self,

retain_grad=False ):

 if

self.grad is None:

self.grad = np.ones_like(self.data)

funcs = []

seen_set = set()

def add_func(f):

 if

f not in seen_set:

funcs.append(f)

 seen_set.add(f)

funcs.sort(key=lambda x: x.generation)

add_func(self.creator)

 while funcs:

f = funcs.pop()

gys = [output().grad for

output in f.outputs]

gxs = f.backward(*gys)

if not isinstance(gxs, tuple):

gxs = (gxs,)

for x, gx in

zip(f.inputs, gxs):

 if

x.grad is None:

x.grad = gx

else:

 x.grad =

x.grad + gx

if x.creator is not

None:

 add_func(x.creator)

if not retain_grad:

for y in f.outputs:

y().grad = None #

y是weakref

  第

2阶段

用自然的代

码表达 108

上面

的代码首先

添加retain_grad作为方

法的参数。如

果retain_grad为

True，那么所

有的变量都

会像之前一

样保留它们

的导数（梯度

）。如果retain_

grad为False（默认

为False），那么所有

中间变量的

导数都会被

重置。其原理

就在于backward方法

的for语句末尾

的y().grad = None，这行代码

的意思是

不

要保留各函

数输出变量

的导数。这样

一来，除终端

变量外，其他

变量的导

数

都不会被保

留。

之所以写

成y().grad = None，是因为y是

弱引用，必须

以y()的形式访

问(上一个步

骤已经引入

了弱引用机

制)。另外，设置

y().grad

= None

之后，引用计

数将变为0，导

数的数据会

从内存中被

删除。

再次运

行前面的测

试代码。

steps/step18.py

x0 = Variable(np.array(1.0))

x1 = Variable(np.array(1.0))

t

= add(x0, x1)

y

= add(x0, t)

y.backward()

print(y.grad, t.grad)

print(x0.grad, x1.grad)

运行

结果

None None

2.0

1.0

在上面

的代码中，中

间变量y和t的

导数已经被

删除。占用的

内存空间

被

立即释放了

出来。这样就

完成了DeZero在内

存使用上的

第1项改进。下

面

进行第2项

改进。在正式

操作之前，我

们先来回顾

一下当前Function类

的代码。

步骤

18 减少内存使

用量的模式

109

18.2

回顾Function类

在DeZero中

计算导数时

，要先进行正

向传播再进

行反向传播

。反向传

播阶

段需要正向

传播阶段的

计算结果，所

以我们需要

保存（记住）这

些结果。

下面

的Function类的阴影

区域就是用

来实际保存

计算结果的

代码。

steps/step18.py

class Function:

def __call__(self, *inputs):

xs = [x.data for

x in inputs]

ys = self.forward(*xs)

if not isinstance(ys, tuple):

ys = (ys,)

outputs = [Variable(as_array(y)) for

y in ys]

self.generation = max([x.generation for

x in inputs])

for output in outputs:

output.set_creator(self)

 self.inputs =

inputs

 self.outputs =

[weakref.ref(output) for output in

outputs]

 return outputs

if len(outputs) > 1

else outputs[0]

在上面

的代码中，函

数的输入被

一个名为 inputs的

实例变量引

用。

inputs引用的变

量，其引用计

数增加了1。这

意味着在调

用__call__方法后，

inputs引

用的变量将

继续保留在

内存中。如果

此时不再引

用inputs，那么引

用

计数将变为

0，inputs将被从内存

中删除。

实例

变量inputs用于反

向传播的计

算。因此，在进

行反向传播

时，要保

留inputs所

引用的变量

。不过有些时

候并不需要

求导，在这种

情况下，我们

没有必要保

留中间计算

的结果，也没

有必要在计

算之间创建

“连接”。

神经网

络分为训练

（train）和推理（inference）两个

阶段。在训练

阶段需要

求

出导数，在推

理阶段只进

行正向传播

。在只进行正

向传播时，我

们可以

把中

间的计算结

果“扔掉”，这将

大幅缩减内

存的使用量

。

第 2阶段　用自

然的代码表

达

110

18.3 使用Config类进

行切换

下面

扩展DeZero，针对只

进行正向传

播的情况进

行优化。首先

需要建

立一

个能在“启用

反向传播模

式”和“禁用反

向传播模式

”之间切换的

机制。

为此，我

们需要使用

下面的Config类。Config是

Configuration（配置）的缩写

。

steps/step18.py

class Config:

 enable_backprop

= True

如上面的代

码所示，Config类是

一个简单的

类，它（目前）只

有一个类属

性，

即布尔类

型的enable_backprop。这个属

性表示“是否

启用反向传

播”，如果

为True，则

表示“启用反

向传播模式

”。

DeZero的“配置”数据

只有一个。因

此，Config类没有被

实例化，而是

作

为类使用

。类只有一个

，而实例可以

创建多个。因

此，在前面的

代码中，

Config类被

设计为拥有

“类属性”。

定义

了Config类之后，就

可以让Function引用

它来切换模

式。实际代码

如下所示。

steps/step18.py

class Function:

def __call__(self, *inputs):

xs = [x.data for

x in inputs]

ys = self.forward(*xs)

if not isinstance(ys, tuple):

ys = (ys,)

outputs = [Variable(as_variable(y)) for

y in ys]

if Config.enable_backprop:

 self.generation

= max([x.generation for x

in inputs]) # ①

设

置“辈分”

 for output

in outputs:

步骤

18 减少内存使

用量的模式



111

 output.set_creator(self) #

②设置“连接”

 self.inputs =

inputs

 self.outputs =

[weakref.ref(output) for output in

outputs]

 return outputs

if len(outputs) > 1

else outputs[0]

如

上面的代码

所示，只有当

Config.enable_backprop为True时，程序才

会执行反向

传播的代码

。在代码中，①处

设置了“辈分

”的值，该值用

于确

定反向

传播时节点

的遍历顺序

，因此在“禁用

反向传播模

式”下不需要

该值。

此外，②的

output.set_creator(self)用于创建计

算的“连接”，在

“禁用反

向传

播模式”下它

也是没有用

的。

18.4 模式的切

换

这样就实

现了启用反

向传播和禁

用反向传播

的机制。使用

这个机制，我

们可以按照

下面的方式

切换模式。

Config.enable_backprop = True

x = Variable(np.ones((100, 100,

100)))

y = square(square(square(x)))

y.backward()

Config.enable_backprop = False

x = Variable(np.ones((100, 100,

100)))

y = square(square(square(x)))

上

面的代码特

意准备了一

个大的多维

数组。它是形

状为(100, 100, 100)

的张量

。这里对这个

张量连续应

用了3次square函数

（对每个元素

进行平方）。

如

果Config.enable_backprop为True，则保留

中间计算的

结果（至少保

留到反

向传

播结束），这样

相应容量的

内存就会被

占用。反之，如

果Config.enable_

backprop为False，中间计

算的结果就

会在使用后

被立即删除

（准确来说，对



第 2阶段　用自

然的代码表

达 112

象在没有

被其他对象

引用时，就会

从内存中被

删除）。

由此便

实现了反向

传播模式的

切换机制。下

面我们采用

更简单的方

式来

切换模

式。

18.5

使用with语句

切换

Python中有一

个语法叫with，用

于自动进行

后处理。该语

法比较有代

表性的一个

使用例子是

文件的打开

和关闭。如果

在不使用with语

句的情况下

向文件写入

一些内容，则

需要按如下

方式编写代

码。

f =

open('sample.txt', 'w')

f.write('hello world!')

f.close()

上面的代

码使用open()打开

一个文件，往

里面写了一

些内容后，使

用

close()关闭文件

。每次都写close()有

点烦琐，而且

可能会忘记

写这行代码

。

为了防止这

种情况发生

，可以像下面

这样使用with语

句。

with open('sample.txt', 'w') as

f:

 f.write('hello world!')

在上面的

代码中，当处

理进入with块时

，文件被打开

，文件在with块

内

保持打开状

态；当处理退

出with块时，文件

被关闭（在幕

后）。因此，通过

with语句，代码可

以自动进行

“进入with块时的

预处理”和“退

出with块时

的后

处理”。

下面使

用with语句切换

到“禁用反向

传播模式”。具

体的使用方

法如下

所示

（using_config方法的实现

将在后面说

明）。

步骤 18 减少

内存使用量

的模式

113

steps/step18.py

with using_config("enable_backprop",

False):

 x =

Variable(np.array(2.0))

 y =

square(x)

如上

所示，只有在

with using_config("enable_backprop", False):语句

中才是

“禁用反向传

播模式”。退出

with语句后，又回

到了正常模

式（启用反

向

传播模式）。

我

们在实际操

作中，常常需

要临时将模

式切换为“禁

用反向传播

模式”。例如，

在

神经网络训

练阶段，为了

（在训练过程

中）评估模型

，常常使用不

需要

梯度的

模式。

下面编

写代码，使模

式切换可以

通过with语句实

现。最简单的

方法是使

用

contextlib模块。这里先

来说明如何

使用contextlib模块，它

的用法如下

所示。

import

contextlib

@contextlib.contextmanager

def config_test():

print('start') # 预处理

try:

yield

 finally:

print('done') # 后处理

with

config_test():

 print('process...')

运行

结果

start

process...

done

第 2阶段

用自然的代

码表达 114

如代

码所示，添加

装饰器@contextlib.contextmanager后，就

可以创建一

个判断上下

文的函数了

。在这个函数

中，yield之前是预

处理的代码

，yield

之后是后处

理的代码。这

样一来，我们

就可以使用

with config_test():了。

在使用with config_test():的

情况下，当处

理进入with块的

作用域时，预

处

理会被调

用，当处理离

开with块的作用

域时，后处理

会被调用。

with块

中可能会发

生异常。如果

with块中发生了

异常，这个异

常也会被

发

送到正在执

行yield的地方。因

此，yield必须用try/finally括

起来。

基于上

述内容，我们

按如下方式

实现using_config函数。

steps/step18.py

import contextlib

@contextlib.contextmanager

def using_config(name, value):

old_value = getattr(Config, name)

setattr(Config, name, value)

try:

 yield

finally:

 setattr(Config, name,

old_value)

using_config(name, value)的

参数name类型是

str，这里将其指

定为Config

的属性

名（类属性名

），然后使用getattr函

数从Config类中获

取指定name的值

，

最后使用setattr函

数来设置新

的值。

这样在

进入with块时，Config类

中用name指定的

属性会被设

置为value

的值。在

退出with块时，这

个属性会恢

复为原始值

（old_value）。现在我们来

实际使用一

下using_config函数。

步骤

18

减少内存使

用量的模式

115

steps/step18.py

with

using_config('enable_backprop', False):

 x

= Variable(np.array(2.0))

 y

= square(x)

如上面的代

码所示，当不

需要反向传

播时，程序只

在with块中执行

正向

传播代

码。这样可以

免去不必要

的计算，节省

内存，不过每

次都要编写

with

using_config('enable_backprop', False):有点烦琐。这

里我们准备

一个名为

no_grad的

函数，代码如

下所示。

steps/step18.py

def no_grad():

 return

using_config('enable_backprop', False)

with no_grad():

x = Variable(np.array(2.0))

y = square(x)

no_grad函数

的实现仅仅

是调用using_config('enable_backprop',

False)

（然后

通过return返回），这

意味着在不

需要计算梯

度时调用no_grad函

数即可。

到这

里，本步骤就

结束了。今后

在不需要计

算梯度，只需

要计算正向

传播时，

请使

用本步骤实

现的“模式切

换”。

第 2阶段　用

自然的代码

表达

116

步骤 19

让

变量更易用

DeZero的基础内容

已经完成，现

在我们可以

创建计算图

并实现自动

微分了。

接下

来的任务是

让DeZero变得更加

易用。首先，我

们来提高Variable类

的

易用程度

。

19.1 命名变量

接

下来我们要

处理很多变

量。如果能对

这些变量加

以区分，处理

起来就

会很

方便。因此，在

本步骤中，我

们将为变量

设置名字。为

此，要在Variable

类中

添加实例变

量name，具体如下

所示。

steps/step19.py

class Variable:

 def

__init__(self, data, name=None ):

if data is not

None:

 if not

isinstance(data, np.ndarray):

 raise

TypeError('{} is not supported'.format(type(data)))

self.data = data

self.name = name

self.grad = None

self.creator = None

self.generation = 0

...

步骤 19 让

变量更易用



117

上面的代码

在初始化参

数中增加了

name=None，并将其设置

给了实例变

量name。这样一来

，我们就可以

通过x = Variable(np.array(1.0),

'input_x')

来将变

量命名为input_x了

。如果不为变

量命名，名字

就是None，变量就

是

一个未命

名的变量。

允

许为变量设

置名称，就能

在计算图的

可视化等场

景中将变量

的名称显示

在图上。计算

图的可视化

在步骤25和步

骤26中实现。

19.2 实

例变量ndarray

Variable起到

包裹数据的

“箱子”的作用

。不过对Variable的用

户来说，

重要

的不是“箱子

”，而是其中的

数据。因此，我

们需要让Variable看

上去是

数据

，即“透明的箱

子”。

如步骤1所

述，数值计算

和机器学习

系统使用多

维数组（张量

）作为底层数

据结构。因此

，Variable类作为ndarray的“专

用箱子”使用

。这里我们

的

目标是让Variable实

例看起来像

ndarray实例。

Variable中有ndarray实

例。ndarray实例内置

了几个多维

数组的实例

变量，

比如下

面代码中使

用的实例变

量shape。

>>> import numpy

as np

>>> x

= np.array([[1, 2, 3],

[4, 5, 6]])

>>>

x.shape

(2, 3)

如上面的

代码所示，多

维数组的形

状可以通过

实例变量shape获

取。顺

带一提

，上面的结果

(2, 3)相当于数学

中的2 × 3矩阵。下

面我们将这

个操

第 2阶段

用自然的代

码表达 118

作扩

展到Variable实例。具

体实现如下

。

steps/step19.py

class Variable:

...

 @property

def shape(self):

 return

self.data.shape

上面的代码

实现了shape方法

，这个方法取

出了实际数

据的shape。这里

比

较重要的一

点是在def shape(self):之前

加上一行@property。这

样一来，

shape方法

就可以作为

实例变量被

访问。我们来

试着操作一

下。

steps/step19.py

x =

Variable(np.array([[1, 2, 3], [4,

5, 6]]))

print(x.shape) #

用x.shape代替x.shape()来

访问

运行结

果

(2, 3)

我们可以

按照上面的

方式通过实

例变量shape取出

数据的形状

。之后可

使用

同样的做法

将ndarray的实例变

量添加到Variable中

。这里添加以

下3个

新的实

例变量。

steps/step19.py

class Variable:

 ...

@property

 def ndim(self):

return self.data.ndim

 @property

def size(self):

 return

self.data.size

步骤

19 让变量更易

用

119

 @property

def dtype(self):

 return

self.data.dtype

上面的代

码添加了ndim、size和

dtype这3个实例变

量，其中ndim是维

度，size是元素数

，dtype是数据类型

。这样就完成

了向Variable添加实

例

变量的工

作。ndarray中还有很

多实例变量

，当然，这些变

量都可以添

加进来。

不过

这项工作很

枯燥，本书就

不带领大家

一一操作了

，请读者根据

实际需要

自

行添加。

到目

前为止，本书

并没有特别

提到ndarray实例的

dtype。如果不指定

dtype，ndarray实例将被初

始化为float64或int64（取

决于实际环

境）。

神经网络

经常使用的

是float32。

19.3

len函数和print函

数

为了能使

用Python的len函数，下

面我们来扩

展Variable类。len函数

是

计算对象数

量的函数，是

Python的内置函数

。下面是它的

使用示例。

>>>

x = [1, 2,

3, 4]

>>> len(x)

4

>>> x =

np.array([1, 2, 3, 4])

>>> len(x)

4

>>>

x = np.array([[1, 2,

3], [4, 5, 6]])

>>> len(x)

2

如

上面的代码

所示，对列表

等应用len函数

后，len函数会返

回其中的元



第 2阶段　用自

然的代码表

达 120

素的数量

。对ndarray实例应用

len，函数会返回

第1个维度的

元素数量。下

面修改代码

，使len函数也可

以应用在Variable上

。

steps/step19.py

class Variable:

...

 def __len__(self):

return len(self.data)

只要像上面

那样实现特

殊方法__len__，就能

对Variable实例应用

len函

数了。这样

一来，我们可

以写出如下

所示的代码

。

x = Variable(np.array([[1, 2,

3], [4, 5, 6]]))

print(len(x))

运行结果

2

在

Python中，像__init__和__len__这种

具有特殊意

义的方法，一

般会

在名称

前后各加两

条下划线。

最

后添加一个

能够轻松查

看Variable内容的功

能，具体来说

就是使用print

函

数将Variable中的数

据打印出来

。先看看如何

使用这个功

能。

x

= Variable(np.array([1, 2, 3]))

print(x)

x = Variable(None)

print(x)

x = Variable(np.array([[1,

2, 3], [4, 5,

6]]))

print(x)

步骤 19

让变

量更易用  121

运

行结果

variable([1 2 3])

variable(None)

variable([[1 2 3]

[4 5 6]])

如果

像上面那样

将Variable实例传给

print函数，要让该

函数打印出

其

中ndarray实例的

内容。输出要

被字符串variable(...)括

起来，以此告

诉用

户这是

一个Variable实例。print函

数还支持数

据为None的情况

和分多行输

出的情况。如

果分多行输

出，可以在第

2行后用空格

调整字符的

起始位置，

以

改善外观。为

了满足以上

几点要求，我

们按如下方

式实现Variable的__

repr__方

法。

steps/step19.py

class Variable:

...

 def __repr__(self):

if self.data is None:

return 'variable(None)'

 p

= str(self.data).replace('\n', '\n' +

' ' * 9)

return 'variable(' + p

+ ')'

我们可以

通过重写__repr__方

法来自定义

print函数输出的

字符串。返

回

值是想要输

出的字符串

。上面的代码

通过str(self.data)将ndarray实例

转

换为字符

串。在幕后，ndarray实

例的__str__函数被

调用，数值转

换为字符串

。

之后，转换后

的字符串会

被字符串variable围

起来。另外，如

果字符串中

有

换行符(\n)，函

数会在换行

符后插入9个

空格。这样一

来，在分多行

输出的

情况

下，数字的起

始位置就会

对齐。

这样就

完成了让Variable类

成为“透明箱

子”的一部分

工作。下一个

步

骤我们将

开展后续的

工作。

第 2阶段

用自然的代

码表达 122

步骤

20

运算符重载

（1）

在上一个步

骤，我们创建

了让Variable成为“透

明箱子”的机

制。不过，

这项

工作还没有

完成，我们还

要让Variable支持+和

*等运算符。这

项工作

完成

后，在a和b是Variable实

例的情况下

，我们就可以

编写y

= a * b这样

的

代码。这项扩

展是本步骤

的目标。

我们

的最终目标

是让Variable实例“看

起来”像ndarray实例

。这样一来，

我

们就可以像

编写NumPy代码一

样使用DeZero。对熟

悉NumPy的人来说

，

这将大大降

低学习DeZero的成

本。

下面扩展

Variable，以支持运算

符+和*。在此之

前，我们需要

实现一个

执

行乘法运算

的函数（加法

运算已在步

骤11实现）。首先

实现执行乘

法运算

的类

Mul（Mul是Multiply的缩写）。

20.1

Mul类

的实现

假设

有乘法运算

y = x0

× x1，可得其导数

为 x

y

0 = x1 和

x

y

1 =

x0。

从这个

结果可知，其

反向传播的

步骤如图201所

示。

∂

∂

∂

∂

步骤

20 运算

符重载（1）  123

某个

函数

某个函

数的

反向传

播

图20

1 乘法运

算的正向传

播（上图）和反

向传播（下图

）

如图201所示，反

向传播中传

播的是最终

输出的L的导

数，准确来说

，

是L对各变量

的导数。这时

，L对变量x0和x1的

导数分别为

x

L

0 =

x1 L

y

和

x

L

1 =

x0

L

y 。

我们对输

出标量的复

合函数感兴

趣，因此在图

201中假设了复

合函数最终

会输出标量

L。

下面实现Mul类

。参照图201，Mul类可

按如下方式

实现。

steps/step20.py

class Mul(Function):

def forward(self, x0, x1):

y = x0 *

x1

 return y

∂

∂

∂

∂

∂

∂

∂

∂

第 2阶段

用自然的代

码表达 124

def backward(self, gy):

x0, x1 = self.inputs[0].data,

self.inputs[1].data

 return gy

* x1, gy *

x0

接下

来使用Mul类来

实现一个Python函

数mul。代码如下

所示。

steps/step20.py

def

mul(x0, x1):

 return

Mul()(x0, x1)

现在可

以使用mul函数

进行乘法运

算了。示例代

码如下所示

。

a

= Variable(np.array(3.0))

b =

Variable(np.array(2.0))

c = Variable(np.array(1.0))

y = add(mul(a, b),

c)

y.backward()

print(y)

print(a.grad)

print(b.grad)

运行结果

variable(7.0)

2.0

3.0

上

面的代码使

用add函数和mul函

数进行计算

，还自动求出

了y的导数。

不

过y =

add(mul(a, b), c)这种写法

让人有些不

舒服。我们当

然更喜欢y =

a * b +

c这

种自然的写

法。为了能使

用运算符+和

*进行计算，下

面我们来

扩

展Variable。要想实现

这个目标，需

要重载运算

符。

重载运算

符后，在使用

运算符+和*时

，实际调用的

就是用户设

置的函数。

在

Python中，我们通过

定义__add__和__mul__等特

殊方法来调

用用户

指定

的函数。

步骤

20 运算符重载

（1）

125

20.2 运算符重载

下面先重载

乘法运算符

*。乘法的特殊

方法是__mul__(self,

other)（参

数

self和other的相关内

容将在后面

解释)。如果定

义（实现）了__mul__方

法，

那么在使

用*进行计算

时，__mul__方法就会

被调用。下面

试着实现Variable

类

的__mul__方法，代码

如下所示。

Variable:

 ...

def __mul__(self, other):

return mul(self, other)

上

面的代码向

已经实现的

Variable类中添加了

__mul__方法。这样在

使

用*进行计

算时，被调用

的就是__mul__方法

，这个方法内

部又会调用

mul函数。

下面我

们用*运算符

做一些计算

。

a =

Variable(np.array(3.0))

b = Variable(np.array(2.0))

y = a *

b

print(y)

运行结果

variable(6.0)

上

面的代码成

功实现了y = a *

b的

计算。当执行

a * b时，实例a的__

mul__(self,

other)方

法被调用。这

时，运算符*左

侧的a作为self参

数、右

侧的b作

为other参数传给

了__mul__方法，具体

如图202所示。

第

2阶段　用自然

的代码表达

126

a *

b

__mul__(self, other)

图20

2 向__mul__方法传

递参数的示

意图

在上面

的例子中，当

执行a *

b的代码

时，首先实例

a的特殊方法

__

mul__方法会被调

用。如果a中没

有实现__mul__方法

，那么实例b中

*

运算符的特

殊方法会被

调用。在这个

例子中b在*运

算符的右侧

，所以调

用的

特殊方法是

__rmul__。

这样就完成

了*运算符的

重载。为此，我

们实现了Variable类

的__mul__

方法。下面

的代码可以

达到同样的

目的。

steps/step20.py

class

Variable:

 ...

Variable.__mul__

= mul

Variable.__add__ =

add

如上所

示，在定义 Variable类

后，又写了 Variable.__mul__

= mul。在

Python中函数也是

对象，所以我

们可以按上

面的公式把

函数赋给方

法。于是，

在调

用Variable实例的__mul__方

法时，mul函数会

被调用。

另外

，上面的代码

还设置了运

算符+的特殊

方法__add__。这样就

实现了

+运算

符的重载。下

面使用+和*进

行一些计算

。

步骤 20 运算符

重载（1）

127

steps/step20.py

a =

Variable(np.array(3.0))

b = Variable(np.array(2.0))

c = Variable(np.array(1.0))

#

y = add(mul(a, b),

c)

y = a

* b + c

y.backward()

print(y)

print(a.grad)

print(b.grad)

运行结

果

variable(7.0)

2.0

3.0

上面的代

码成功实现

了y = a *

b + c的计算。现

在可以使用

+和*自由

地进

行计算了。基

于同样的做

法还可以实

现其他运算

符（如/和-等）的

重载。

在下一

个步骤，我们

将继续实现

这部分的内

容。

  第

2阶段　用

自然的代码

表达 128

步骤

21

运

算符重载（2）

我

们的DeZero越来越

好用了。在有

Variable实例a和b的情

况下，我们可

以写出a *

b或a + b这

样的代码。不

过，现在还不

能使用a *

np.array(2.0)

这种

将Variable实例与ndarray实

例结合起来

的代码，也不

能使用3 + b这

种

将 Variable实例与数

值数据结合

起来的代码

。如果能将Variable实

例

与ndarray实例和

数值数据结

合使用，DeZero会更

加好用。本步

骤将扩展

Variable，使

Variable实例能够与

ndarray实例，以及int和

float等类型的数

据一起使用

。

21.1 与ndarray一起使用

首先扩展Variable，使

Variable实例能够与

ndarray实例一起使

用。实现

这个

目标很简单

。比如有Variable实例

a，在执行代码

a *

np.array(2.0)时，（在

用户看

不到的幕后

）将这个ndarray实例

转换为Variable实例

。换言之，只要

把该实例转

换为Variable(np.array(2.0))，剩下的

计算就和之

前的一样了

。

为此，我们要

准备一个工

具函数as_variable。该函

数会把作为

参数传来

的

对象转换为

Variable实例。代码如

下所示。

步骤

21 运算符重载

（2）  129

steps/step21.py

def as_variable(obj):

if isinstance(obj, Variable):

return obj

 return

Variable(obj)

上面的代码

假定参数obj是

Variable实例或ndarray实例

。如果obj是

Variable实例

，则不做任何

修改直接返

回。否则，将其

转换为Variable实例

并返回。

如下

面的代码所

示，我们在Function类

的__call__方法的开

头添加使用

了as_variable函数的阴

影部分的代

码。

steps/step21.py

class Function:

def __call__(self, *inputs):

inputs = [as_variable(x) for

x in inputs]

xs = [x.data for

x in inputs]

ys = self.forward(*xs)

...

上面的代

码会将作为

参数传来的

inputs中的各元素

x转换为Variable实例

。

因此，如果传

来的是ndarray实例

，它将被转换

为Variable实例。这样

在后

续的处

理中，所有变

量都会变成

Variable实例。

DeZero中使用

的所有函数

（运算）都继承

自Function类。在实际

运算时，

Function类的

__call__方法会被调

用。因此，如果

像上面那样

修改了

Function类的

__call__方法，这个修

改将应用于

DeZero中使用的所

有

函数。

下面

使用新的DeZero做

一些计算。示

例代码如下

所示。

  第

2阶段

用自然的代

码表达 130

steps/step21.py

x

= Variable(np.array(2.0))

y =

x + np.array(3.0)

print(y)

运行

结果

variable(5.0)

这里运

行了代码y =

x + np.array(3.0)。从

输出可以看

出，结果是正

确的。在代码

内部，ndarray实例转

换成了Variable实例

。这样，ndarray和

Variable就能

一起使用了

。

21.2 与float和int一起使

用

下面继续

修改Variable，使其能

与Python的int、float，以及np.float64、

np.int64等

类型一起使

用。假设x是Variable实

例，我们该怎

么做才能让

x

+ 

3.0这样的代码

顺利执行呢

？其中一种方

法是在add函数

中添加以下

阴影部分

的

代码。

steps/step21.py

def add(x0, x1):

x1 = as_array(x1)

return Add()(x0, x1)

上面的

代码使用了

as_array函数。这是我

们在步骤9中

实现的函数

。

如果x1是int或float，使

用这个函数

就可以把它

转换为ndarray实例

。而

ndarray实例（之后

）则会在Function类中

被转换为Variable实

例。这样，我

们

就可以写出

如下代码。

steps/step21.py

x = Variable(np.array(2.0))

y

= x + 3.0

print(y)

步

骤 21 运算符重

载（2）

131

运行结果

variable(5.0)

上面的代码

成功地将float和

Variable实例相加并

计算出结果

。目前我

们只

修改了add函数

，使用同样的

做法还可以

对mul等函数进

行同样的修

改。

这样就能

通过+和*将Variable实

例与int、float结合起

来计算了。不

过当

前实现

仍存在两个

问题。

21.3 问题1：左

项为float或int的情

况

当x是Variable实例

时，使用现在

的DeZero可以正确

运行代码x * 2.0。

但

是，运行代码

2.0

* x会出错。在实

际运行时，系

统提示的错

误信息如下

所示。

y =

2.0 * x

运行结

果

TypeError: unsupported operand type(s)

for *: 'float' and

'Variable'



要想了解

出现如上错

误的原因，可

以看看执行

代码2.0 * x时错误

发生

的过程

。在执行代码

2.0

* x时，系统会按

照以下步骤

进行处理。

 尝

试调用在运

算符*左侧的

2.0的__mul__方法

 2.0是浮

点数，__mul__方法没

能得到实现

 然后尝试调

用在*运算符

右侧的x（Variable）的特

殊方法



由于

x在运算符的

右侧，所以尝

试调用的是

__rmul__方法（而不是

__

mul__）

 但是Variable实例没

有实现__rmul__方法

这就是错误

发生的过程

。重点是，对于

*这种有两个

操作数项的

运算符，

要调

用的特殊方

法取决于操

作数项是右

项还是左项

。如果是乘法

运算，那么

对

左项调用__mul__方

法，对右项调

用__rmul__方法。

第 2阶

段　用自然的

代码表达 132

基

于以上分析

，我们知道只

要实现__rmul__方法

就可以解决

这个问题。

这

时，参数会按

照图211的方式

传给__rmul__方法。

2.0

* x

__rmul__(self, other)

图

211 向__rmul__方法传递

参数的示意

图

如图21

1所示

，在__rmul__(self, other)的参数中

，self对应自己，即

x，

other对应另一个

操作数项2.0。不

过乘法运算

的左右项即

使交换，结果

也不

会发生

改变。比如2.0

* x和

x * 2.0的结果相同

。因此，对乘法

，我们没有

必

要区分左右

项。对加法也

是如此，所以

针对+和*，只要

设置以下4个

特殊

方法即

可。

steps/step21.py

Variable.__add__

= add

Variable.__radd__ =

add

Variable.__mul__ = mul

Variable.__rmul__ = mul

现在可以

任意组合float和

int进行计算了

。下面是一个

使用示例。

steps/step21.py

x = Variable(np.array(2.0))

y = 3.0 *

x + 1.0

print(y)

运

行结果

variable(7.0)

步骤

21 运算符重载

（2）

133

这样就可以

将Variable实例与float及

int一起使用了

。最后还有一

个问

题需要

解决。

21.4

问题2：左

项为ndarray实例的

情况

还有一

个要解决的

问题是ndarray实例

为左项，Variable实例

为右项的

计

算。比如以下

代码。

x

= Variable(np.array([1.0]))

y =

np.array([2.0]) + x

在上面

的代码中，左

项是ndarray实例，右

项是Variable实例。这

时，

左项ndarray实例

的__add__方法被调

用。当然，这里

我们想让右

项Variable

实例的__radd__方

法被调用。为

此，我们需要

指定运算符

的优先级。具

体来说，

就是

向Variable实例添加

__array_priority__属性，并将其

值设置为大

的整

数值。实

际的代码如

下所示。

class Variable:

 __array_priority__

= 200

 ...

通过

向Variable类添加上

面的代码，我

们可以将Variable实

例的运算

符

优先级设置

为高于ndarray实例

的运算符优

先级。这样一

来，即使左项

是

ndarray实例，右项

的Variable实例的运

算符方法也

会被优先调

用。

以上就是

重载运算符

时需要注意

的地方。完成

本步骤操作

后，在使用

DeZero时

，我们就能通

过运算符*和

+将Variable实例和其

他类型的数

据连

接在一

起使用了。在

下一个步骤

，我们将添加

/和-等运算符

。

第 2阶段　用自

然的代码表

达 134

步骤 22

运算

符重载（3）

在上

一个步骤，我

们扩展了DeZero，使

其支持*和+这

两个运算符

。运

算符的种

类很多，本步

骤将实现对

表221中运算符

的支持。

表22-1 本

步骤中新增

加的运算符

特殊方法

示

例

__neg__(self) -self

__sub__(self,

other) self - other

__rsub__(self, other) other -

self

__truediv__(self, other) self

/ other

__rtruediv__(self, other)

other / self

__pow__(self,

other) self ** other

表221中的第

1个方法__neg__(self)对应

的是负数运

算符，它是只

有一

个操作

数项的运算

符（这种运算

符称为单目

运算符）。因此

，这个特殊方

法只

需要一

个参数。剩下

的是减法运

算、除法运算

和幂运算。正

如前面提到

的那样，

这些

运算对应的

运算符是有

两个操作数

项的运算符

（如a - b和a /

b等）。对

象

可能是右项

，也可能是左

项，因此特殊

方法有两个

。不过对于幂

运算，我

们只

考虑x **

3这样的

情况，也就是

左项是Variable实例

，右项是常数

（2或

3等int型数据

）。

步骤 22

运算符

重载（3）  135

除表22

1中

列出的运算

符之外，还有

其他类型的

Python运算符，如a // 

b和

a

% b等。此外，还有

a += 1和a

-= 2等赋值运

算符。本步骤

只选

择并实

现那些可能

会被频繁使

用的操作符

，至于其他运

算符，请读者

根据

需要自

行添加。本步

骤的内容有

些单调，大家

可以选择跳

过。

下面开始

编写代码。首

先复习一下

添加新运算

符的步骤。

1. 继

承Function类并实现

所需的函数

类（例：Mul类）

2.

使其

能作为Python函数

使用（例：mul函数

）

3. 为Variable类设置运

算符重载（例

：Variable.__mul__ =

mul）

我们将遵循

上面的步骤

添加新的运

算符。首先是

负数。

22.1 负数

负

数式子y = −x的导

数是 x

y = −1。因此，在

反向传播时

，要把从上

游

（输出方）传来

的导数乘以

−1之后传给下

游。基于这一

点，我们可以

把代

码编写

成下面这样

。

steps/step22.py

class Neg(Function):

def forward(self, x):

return -x

 def

backward(self, gy):

 return

-gy

def neg(x):

return Neg()(x)

Variable.__neg__ =

neg

上面的代码

实现了Neg类，并

实现了Python函数

neg。之后将neg赋给

特殊

方法__neg__，代

码就完成了

。然后，我们就

可以像下面

这样使用负

数运算符了

。

∂

∂

  第

2阶段　用自

然的代码表

达 136

steps/step22.py

x = Variable(np.array(2.0))

y

= -x # 求负数

print(y)

运

行结果

variable(-2.0)

下面

要实现的是

减法。

22.2 减法

减

法式子y =

x0 − x1的导

数为 x

y

0 = 1

和 x

y

1

= −1。因此

，在反向传

播

时，针对从上

游传来的导

数，把乘以1得

到的结果作

为x0的导数，把

乘以

−1得到的

结果作为x1的

导数。在此基

础上编写的

代码如下所

示。

steps/step22.py

class Sub(Function):

def forward(self, x0, x1):

y = x0 -

x1

 return y

def backward(self, gy):

return gy, -gy

def

sub(x0, x1):

 x1

= as_array(x1)

 return

Sub()(x0, x1)

Variable.__sub__ =

sub

如果x0和x1是

Variable实例，就可以

执行y = x0

- x1的计算

了。但是，

如果

x0不是Variable实例，就

无法正确执

行y =

2.0 - x这样的计

算。在这

种情

况下被调用

的是x的__rsub__方法

，参数以图22

1的

方式传递。

∂

∂

∂

∂

步

骤 22 运算符重

载（3）

137

2.0 - x

__rsub__(self, other)

图221

向__rsub__方法

传递参数的

示意图

如图

221所示，在调用

__rsub__(self, other)时，减号运算

符右项的x

传

给了self参数。考

虑到这一点

，__rsub__应按如下方

式实现。

steps/step22.py

def rsub(x0,

x1):

 x1 =

as_array(x1)

 return Sub()(x1,

x0) # 交换

x1和x0

Variable.__rsub__

= rsub

上面的代

码准备了函

数rsub(x0, x1)，函数内部

交换了x0和x1的

顺序，

然后调

用了Sub()(x1, x0)。最后函

数rsub被赋给特

殊方法__rsub__。

在加

法运算和乘

法运算中，左

项和右项交

换并不会影

响计算结果

，所以我

们不

必区分左右

项，但在减法

运算的情况

下，就要区分

左项和右项

了（x0

减去x1和x1减

去x0的结果是

不同的）。因此

，我们需要按

照前面的方

式

为右项单

独准备一个

函数rsub(x0, x1)。

到这里

就完成了减

法运算的实

现，现在我们

可以编写以

下代码了。

steps/step22.py

x = Variable(np.array(2.0))

y1 = 2.0 -

x

y2 = x

- 1.0

print(y1)

print(y2)

第

2阶段　用自然

的代码表达

138

运行结果

variable(0.0)

variable(1.0)

接

下来是除法

运算。

22.3

除法

除

法式子 y =

x

x

0

1

的导

数为 x

y

0

= x

1

1

和 x

y

1

= − x0

(x1)2

。基于

这个结果，

我

们编写如下

代码。

steps/step22.py

class

Div(Function):

 def forward(self,

x0, x1):

 y

= x0 / x1

return y

 def

backward(self, gy):

 x0,

x1 = self.inputs[0].data, self.inputs[1].data

gx0 = gy /

x1

 gx1 =

gy * (-x0 /

x1 ** 2)

return gx0, gx1

def

div(x0, x1):

 x1

= as_array(x1)

 return

Div()(x0, x1)

def rdiv(x0,

x1):

 x1 =

as_array(x1)

 return Div()(x1,

x0) # 交换x1和

x0

Variable.__truediv__

= div

Variable.__rtruediv__ =

rdiv

和减法运算

一样，在实现

除法运算时

，我们也需要

为右项和左

项分别实

现

不同的函数

。除此之外没

有什么特别

难的地方了

。最后是幂运

算。

∂

∂

∂

∂

步骤

22 运算

符重载（3）  139

22.4 幂运

算

幂运算用

式子y =

xc

表示，其

中x称为底，c称

为指数。由导

数公式可知

，

幂的导数为

x

y

= cxc−1。至于 y

c，在实践

中需要计算

它的情况并

不多（当

然也

可以计算它

），因此本书只

考虑计算底

x的导数的情

况。也就是说

，我

们将指数

c视为常数，不

去计算它的

导数。考虑到

这一点，可将

代码编写如

下。

steps/step22.py

class

Pow(Function):

 def __init__(self,

c):

 self.c =

c

 def forward(self,

x):

 y =

x ** self.c

return y

 def

backward(self, gy):

 x

= self.inputs[0].data

 c

= self.c

 gx

= c * x

** (c - 1)

* gy

 return

gx

def pow(x, c):

return Pow(c)(x)

Variable.__pow__ =

pow

上面的代

码在Pow类初始

化时设置指

数c。正向传播

forward(x)只接受一

个

变量，即底x。最

后一行代码

的意思是将

函数pow赋给特

殊方法__pow__。这样

，

我们就可以

使用**运算符

来进行幂运

算了。下面是

以上代码的

应用示例。

steps/step22.py

x = Variable(np.array(2.0))

y = x **

3

print(y)

运

行结果

variable(8.0)

∂

∂

∂

∂

第 2阶

段　用自然的

代码表达

140

到

这里就完成

了添加运算

符的工作。本

步骤虽然有

些枯燥，但DeZero

的

可用性得到

了大幅提升

。现在我们可

以用各种运

算符自由地

进行四则运

算了，

甚至还

能进行幂运

算，能实现的

计算也越来

越复杂。在下

一个步骤，我

们会

把现有

的成果整理

成一个Python的包

，然后验证当

前的DeZero所具备

的能力。

步骤

23 打包

141

步骤 23

打

包

我们此前

是把每个步

骤的代码各

自汇总成（或

者说装进）一

个文件，从

step01.py开

始，到上一个

步骤的step22.py结束

。现在DeZero已经具

有一定

的规

模了，为了能

够使用此前

的成果，在本

步骤中，我们

会把所有的

代码打

成一

个包。

使用Python开

发时，常常用

到模块、包和

库这几个术

语。下面笔者

来

简单介绍

一下它们。

模

块（module）

模块是Python文

件。模块特指

那些以方便

其他Python程序导

入和使用

而

创建的Python文件

。

包（package）

包是多个

模块的集合

。创建包之前

，要先创建一

个目录，并向

其中添加

模

块（Python文件）。

库（library）

库

是多个包的

集合。在文件

结构上，它由

一个或多个

目录组成。包

有时

也称为

库。

第 2阶段　用

自然的代码

表达 142

23.1 文件结

构

先来看看

文件结构的

设计。我们在

每个步骤都

编写了代码

文件：step01.

py、step02.py……为了使

用DeZero的这些step文

件，我们创建

了dezero目录。

最终

的文件结构

如下所示。

.

│

├──

│

 dezero

├── __init__.py

│   ├──

core_simple.py

│

├── ...

│

└── utils.py

│

├──

│

 steps

├── step01.py

│

├── ...

│

└── step60.py

设

计好结构之

后，就可以在

dezero目录下添加

模块了，由此

会创建出一

个名为dezero的包

。这个包就是

我们正在创

建的框架。今

后我们会把

代码添

加在

dezero目录下的文

件中。

23.2 将代码

移到核心类

接下来在dezero目

录下添加一

些文件。此处

的目标是将

上一步的step22.py

中

的代码移到

dezero/core_simple.py中（核心文件

）。文件名中之

所以有core

（核心

），是因为已经

实现的功能

都可以视为

DeZero的核心功能

。该文件最终

会被替换为

core.py，考虑到这一

点，我们暂时

将其命名为

core_simple.py。

下面从step22.py中复

制以下类到

核心文件中

。

 Config

 Variable

 Function

步骤 23 打包

143

 Add(Function)



Mul(Function)

 Neg(Function)



Sub(Function)

 Div(Function)



Pow(Function)

上

面的Add(Function)中的(Function)的

意思是该类

继承了Function类。

上

面列表中有

Config类、Variable类、Function类，还有

6个继承自Function

类

的函数类。接

下来是step22.py中的

Python函数。我们把

以下函数移

到核

心文件

中去。

 using_config



no_grad

 as_array



as_variable

 add



mul

 neg



sub

 rsub



div

 rdiv



pow

最前面

的两个函数

是 DeZero 的配置函

数，用于启用

或禁用反向

传

播。函数as_array和

as_variable用于将传来

的参数对象

转换为ndarray或

Variable。剩

下的是DeZero自身

使用的函数

。下面我们直

接将step22.py中

的类

和函数复制

到核心文件

中。

第 2阶段　用

自然的代码

表达 144

我们此

前还实现了

一些在 DeZero 中使

用的具体的

函数，如 Exp类和

Square类，exp函数和square函

数。但是，这些

内容不会纳

入核心文件

中。

我们稍后

会把它们添

加到dezero/functions.py中。

现在

可以从外部

的Python文件导入

dezero使用了，代码

如下所示。

import numpy

as np

from dezero.core_simple

import Variable

x =

Variable(np.array(1.0))

print(x)

运

行结果

variable(1.0)

如 上

所 示，编 写

from dezero.core_simple import Variable

即

可导入

Variable类。请

注意，这里用

的是dezero.core_simple。后面我

们会引入一

种

省略core_simple的机

制，直接编写

from dezero

import Variable即可。

使用from ...

import ...语

法可以直接

导入模块中

的类和函数

等。此

外，使用

import XXX

as A可以以名称

A导入一个名

为XXX的模块。例

如，使 用 import

dezero.core_simple as dz 就能

以名称

dz 导 入

dezero.core_simple模块。导入后

，我们可以通

过dz.Variable来访问

Variable类

。

23.3 运算符重载

现在step22.py中的大

部分代码已

经移动完成

了。接下来要

把进行运算

符重载的代

码移到dezero。为此

，我们要在dezero/core_simple.py（核

心文件）

中添

加以下函数

。

步骤

23 打包  145

dezero/core_simple.py

def setup_variable():

Variable.__add__ = add

Variable.__radd__ = add

Variable.__mul__ = mul

Variable.__rmul__ = mul

Variable.__neg__ = neg

Variable.__sub__ = sub

Variable.__rsub__ = rsub

Variable.__truediv__ = div

Variable.__rtruediv__ = rdiv

Variable.__pow__ = pow

setup_variable函

数用于重载

Variable的操作符。调

用这个函数

后，函

数内部

会设置 Variable的操

作符。那么最

好在哪里调

用这个函数

呢？

dezero/_ _init_

_.py文件就很

合适。

_ _init_ _.py文件是

导入模块时

执行的第1个

文件。拿现在

的情况来说

，

导入dezero包中的

模块后，首先

被调用的是

dezero/_ _init_ _.py中的代码。

这

里我们在dezero/_

_init_ _.py中

编写以下代

码。

dezero/__init__.py

from

dezero.core_simple import Variable

from

dezero.core_simple import Function

from

dezero.core_simple import using_config

from

dezero.core_simple import no_grad

from

dezero.core_simple import as_array

from

dezero.core_simple import as_variable

from

dezero.core_simple import setup_variable

setup_variable()

上面的代

码导入了setup_variable函

数并调用了

它。这样一来

，dezero

包的用户将

始终能够在

操作符重载

被预先设置

了的情况下

使用Variable。另

外，在

_ _init_

_.py的顶部有这

样一行代码

：from dezero.core_simple import

Variable。有了这条语

句，用户就可

以直接使用

dezero包中的Variable类。

示

例代码如下

所示。

第 2阶段

用自然的代

码表达 146

#

使用

dezero的用户的代

码

# from dezero.core_simple

import Variable

from dezero

import Variable

如上所示

，我们可以把

之前写的from dezero.core_simple

import Variable

替

换成from dezero

import Variable。有了dezero/_ _init_ _.py中

的这些导入

语句，对于剩

下的Function和using_config等，用

户都可以使

用这种简洁

版

的导入方

式。

23.4 实际的__init__.py文

件

从步骤23到

步骤32，本书都

会将dezero/core_simple.py作为DeZero的

核心文件来

使用。从步骤

33开始，我们会

使用dezero/core.py（替代dezero/

core_simple.py）。所

以，在dezero/_ _init_ _.py中，core_simple.py和core.py

会

交替导入。实

际代码如下

所示。

dezero/__init__.py

is_simple_core = True

if is_simple_core:

 from

dezero.core_simple import Variable

from dezero.core_simple import Function

from dezero.core_simple import using_config

from dezero.core_simple import no_grad

from dezero.core_simple import as_array

from dezero.core_simple import as_variable

from dezero.core_simple import setup_variable

else:

 from dezero.core

import Variable

 from

dezero.core import Function

...

 ...

setup_variable()

步骤 23 打

包

147

上面的代

码使用is_simple_core标识

来切换导入

语句。当is_simple_core

为 True

时

，从 core_simple.py 导 入；当

is_simple_core 为

False 时，从 core.

py导入。

请

读者根据章

节进度修改

is_simple_core标识。从现在

到步骤32为is_

simple_core=True，从

步骤33开始为

is_simple_core=False。

23.5

导入dezero

这样，我

们就得到了

dezero包。在本步骤

的step23.py代码文件

中，可

以编写

下面这段代

码。

steps/step23.py

if '__file__' in globals():

import os, sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy

as np

from dezero

import Variable

x =

Variable(np.array(1.0))

y = (x

+ 3) ** 2

y.backward()

print(y)

print(x.grad)

运行结果

variable(16.0)

8.0

上面的代码

首先通过if '__file__' in

globals():语

句检查了'__file__'

是

否被定义为

全局变量。如

果以python命令的

形式（如python steps23.py）

运行

代码，那么变

量__file__会被定义

。这时，代码将

获取当前文

件（step23.

第 2阶段　用

自然的代码

表达

148

py）的目录

路径，并将其

父目录添加

到模块的搜

索路径中。这

样无论从哪

里

运行Python命令

，dezero目录下的文

件都会被正

确导入。例如

，命令行中

既

可以运行python

steps/step23.py，也

可以运行cd steps; python step23.

py。本

书的后续章

节将省略添

加模块搜索

路径的代码

。

搜索路径的

代码是为了

导入手头正

在开发的dezero目

录而临时添

加的。

如果 DeZero

是

作为一个包

安装的（比如

通过 pip install dezero安

装），DeZero包

将被配置在

Python搜索路径中

。这意味着用

户不需要按

照前面代码

的方式手动

添加路径。另

外，在Python解释器

的交互模式

下

或在Google Colaboratory等环

境下运行代

码时，__file__变量没

有被

定义。考

虑到这一点

（为了让step文件

能够直接在

Google

Colaboratory

中运行），在把

父目录添加

到搜索路径

中时，增加了

if '__file__' in

globals():这行代码。

以

上就是step23.py中的

所有代码（没

有省略的代

码）。现在我们

完成了

DeZero框架

的原型，今后

将扩展dezero目录

下的文件（模

块）。

步骤

24 复杂

函数的求导

149

步骤

24

复杂函

数的求导

DeZero现

在支持比较

常见的运算

符（+、*、-、/和**）。因此，用

户可

以像编

写普通的Python程

序一样编写

代码。当你对

复杂的数学

表达式进行

编码时，就能

体会到这一

点的可贵之

处了。下面我

们尝试求出

几个复杂式

子

的导数。

本

步骤要探讨

的函数是优

化问题中经

常使用的测

试函数。优化

问题的

测试

函数是用来

评估各种优

化方法的函

数。换言之，它

们是用于基

准测试

（benchmark）的函

数。这样的函

数有好几种

。维基百科的

“用于优化的

测试

函数”（Test functions for optimization）页

面将这类函

数以图24

1的形

式总

结了出

来A。

在本步骤

中，我们将从

图241中选择3个

函数，尝试求

出它们的导

数。

由此我们

可以体会到

DeZero的强大。下面

先从简单的

函数Sphere开始。

A 图

为本书编写

时的版本，之

后可能会发

生改变。——译者

注

第 2阶段　用

自然的代码

表达 150

图241 优化

问题中使用

的基准测试

函数列表（摘

自维基百科

）

24.1

Sphere函数

Sphere函数可

用式子表示

为z = x2

+ y2

，它是一个

将输入变量

平方后

相加

的简单函数

。我们的任务

是求出它的

导数

x

z 和 y

z。下面

尝试计算

z = x2

+ y2在

(x, y) =

(1.0, 1.0)处的导数。代

码如下所示

。

∂

∂

∂

∂

步骤 24

复杂函

数的求导  151

steps/step24.py

import numpy as np

from dezero import Variable

def sphere(x, y):

z = x **

2 + y **

2

 return z

x = Variable(np.array(1.0))

y

= Variable(np.array(1.0))

z =

sphere(x, y)

z.backward()

print(x.grad,

y.grad)

运

行结果

2.0 2.0

上面

的代码将Sphere的

计算编写为

z = x **

2 + y **

2。计算结果是

x和y的导数都

为2.0。根据式子

x

z = 2x

和 y

z =

2y 可知，z = x2

+ y2在

(x, y)

= (1.0, 1.0)处

的导数为(2.0, 2.0)，这

与上述运行

结果一致。

24.2 matyas函

数

接下来是

matyas函数。它可用

式子表示为

z =

0.26(x2 + y2

)

− 0.48xy。

使用DeZero可将函

数按以下方

式实现。

steps/step24.py

def matyas(x, y):

z = 0.26 *

(x ** 2 +

y ** 2) -

0.48 * x *

y

 return z

x = Variable(np.array(1.0))

y

= Variable(np.array(1.0))

z =

matyas(x, y)

z.backward()

print(x.grad,

y.grad)

运行

结果

0.040000000000000036 0.040000000000000036

∂

∂

∂

∂

第 2阶段

用自然的代

码表达 152

如上

所示，这里也

可以直接把

式子转写为

代码。做到这

一点很容易

，因

为DeZero支持四

则运算的运

算符。我们不

妨来看看当

这些运算符

不可用时

matyas函

数的代码是

什么样子的

。

def

matyas(x, y):

z = sub(mul(0.26, add(pow(x,

2), pow(y, 2))), mul(0.48,

mul(x, y)))

 return

z

上面的代码

可读性较差

，由此我们可

以体会到能

够使用+和**等

运算符

是多

么可贵的一

件事。使用这

些运算符不

仅可以减少

打字量，还可

以用近

似于

普通式子的

写法来读写

代码。最后，我

们要挑战的

是复杂的Goldstein

Price函

数。

24.3 Goldstein-Price函数

GoldsteinPrice函数

可用以下式

子表示。

1+(x + y +

1)2  19

− 14x + 3x2

− 14y + 6xy

+ 3y2

30 +

(2x − 3y)

2

18 − 32x

+ 12x2 + 48y

− 36xy + 27y2

看起

来很复杂，但

有了DeZero，编码就

没有那么难

了。实际编写

的代

码如下

所示。

steps/step24.py

def goldstein(x, y):

z = (1

+ (x + y

+ 1)**2 * (19

- 14*x + 3*x**2

- 14*y + 6*x*y

+ 3*y**2)) * \

(30 + (2*x -

3*y)**2 * (18 -

32*x + 12*x**2 +

48*y - 36*x*y +

27*y**2))

 return z

对照式

子可以很快

完成编码。对

普通人来说

333333

，不使用运算

符是写不出

这

个函数的

代码的。下面

尝试计算GoldsteinPrice函

数的导数。

步

骤 24 复杂函数

的求导

153

steps/step24.py

x =

Variable(np.array(1.0))

y = Variable(np.array(1.0))

z = goldstein(x, y)

z.backward()

print(x.grad, y.grad)

运行

结果

-5376.0 8064.0

运行上

面的代码，可

得到x的导数

为−5376.0，y的导数为

8064.0。这些

都是正

确的结果。就

连Goldstein

Price函数这样

复杂的计算

，DeZero都能

求出它

的导数。大家

可以通过梯

度检验来验

证结果是否

正确。以上就

是第2

阶段的

内容。



在第2阶

段，DeZero已经有了

很大的进步

。在这个阶段

刚开始的时

候，

DeZero只能进行

简单的计算

，现在它已经

可以进行复

杂的计算了

（准确来

说，无

论计算图的

“连接”多么复

杂，它都可以

正确地进行

反向传播）。另

外，

通过重载

运算符，我们

可以像编写

普通的Python程序

一样编写代

码。从

能够自

动微分的角

度来看，DeZero可以

说是把普通

编程变成了

可微分编程

（differentiable programming）。

DeZero 的基础部分

到这里就完

成了。在下一

个阶段，我们

将扩展

DeZero来进

行更复杂的

计算。

  第

2阶段

用自然的代

码表达 154

专栏

：Define-by-Run

深度学习框

架可以分为

两大类：一类

是基于Define

andRun的框

架，另一

类是

基于Defineby

Run的框架

。本专栏将介

绍这两种方

式及其优缺

点。

DefineandRun（静态计算

图）

DefineandRun可以直译

为“定义计算

图，然后流转

数据”。用户定

义计算图，

然

后由框架对

计算图进行

转换，这样就

能流转数据

。这个处理流

程如图B

1所示

。

编译 计算图

的定义 流转

数据

图B1 Defineand

Run式的

框架的处理

流程

如图B1所

示，框架转换

了计算图的

定义。方便起

见，我们将这

种转换操作

称为编译。通

过编译，框架

将计算图加

载到内存中

，为数据流转

做好准备。这

里

比较重要

的一点是“计

算图的定义

”和“流转数据

”的处理是分

开的。查看下

面的

伪代码

可以更清楚

地了解这一

点。

步骤 24 复杂

函数的求导



155

# 虚拟的Define-and-Run式框

架的示例代

码

#

计算图的

定义

a = Variable('a')

b = Variable('b')

c

= a * b

d = c +

Constant(1)

# 计算图

的编译

f

= compile(d)

# 数据

流转

d = f(a=np.array(2), b=np.array(3))

示例代

码的前4行定

义了计算图

。需要注意的

是，这4行代码

并没有实际

进

行计算。所

以，编程的对

象是“符号”，而

不是“数值”。顺

带一提，这种

编程叫

作符

号式编程（symbolic programming）。

如

前面的代码

所示，在DefineandRun式的

框架中，用户

需要对使用

了符

号的抽

象计算过程

（而不是实际

的数据）进行

编码，而且这

些代码必须

用领域特

定

语言来编写

。这里所说的

领域特定语

言是一种由

框架自身规

则组成的语

言。拿

上面的

例子来说，用

户需要遵循

“在Constant中存储常

量”等规则。此

外，如果

想使

用if语句进行

切换操作，则

需要通过特

殊的操作来

实现，这也是

领域特定

语

言的规则。顺

带一提，TensorFlow框架

使用tf.cond操作实

现了if语句。下

面是一个具

体的例子。

import tensorflow as tf

flg = tf.placeholder(dtype=tf.bool)

x0

= tf.placeholder(dtype=tf.float32)

x1 =

tf.placeholder(dtype=tf.float32)

y = tf.cond(flg,

lambda: x0+x1, lambda: x0*x1)

如

代码所示，TensorFlow为

存储数据的

tf.placeholder（容器）创建了

计算

图。上面

的代码使用

tf.cond操作根据运

行时的flg的值

来切换处理

。换言之，

第 2阶

段　用自然的

代码表达 156

TensorFlow通

过tf.cond操作实现

了Python的if语句。

许

多DefineandRun式框架使

用领域特定

语言来定义

计算。领域特

定语言换个

说法就是运

行在Python上的“新

编程语言”（考

虑到它们

有

自己的if语句

和for语句等流

程控制指令

，称其为“新编

程语言”

比较

合适）。领域特

定语言也是

用来求导的

语言。在这样

的背景下，

近

来深度学习

框架也被称

为可微分编

程 。

以上是对

DefineandRun的简单介绍

。在深度学习

早期，大部分

框架可归

类

于Define

andRun。其中比较

有代表性的

框架有TensorFlow、Caffe、CNTK

等（TensorFlow后

来也采用了

Defineby

Run方式）。作为DefineandRun的

下一

代出场

的是Define

byRun，我们的

DeZero也采用了这

种方式。

Defineby

Run（动态

计算图）

DefinebyRun一词

的意思是计

算图是由数

据流定义的

。它的特点是

“数据

的流转

”和“计算图的

构建”同时进

行。

以 DeZero 为

例，当

用 户 流 转

数

据，即 进 行 普

通

的 数 值 计

算

时，

DeZero会在幕

后创建为计

算图准备的

链接（引用）。这

个链接就相

当

于DeZero中的计

算图，它的数

据结构是链

表（linked list）。使用链表

，

就可以在计

算结束后反

向回溯链接

。

使用DefinebyRun式的框

架，用户就可

以像使用NumPy编

写普通程序

一

样编写代

码。实际使用

DeZero，我们可编写

出以下代码

。

import numpy as

np

from dezero import

Variable

a = Variable(np.ones(10))

b = Variable(np.ones(10) *

2)

c = b

* a

d =

c + 1

print(d)

步骤 24 复杂函

数的求导

157

上

面的代码与

使用NumPy的普通

程序几乎完

全相同。唯一

不同的是，上

面

代码中NumPy的

数据被封装

在Variable类中。当代

码执行时，代

码中的值会

立

即被求出

。DeZero在幕后为计

算图创建了

连接。

Chainer 在 2015 年首

次提出

DefinebyRun 范

式

。此 后，它 被 PyTorch、

MXNet、DyNet和

TensorFlow（2.0版以后默认

采用该方式

）等许多框架

采用。

动态计

算图的优点

在使用动态

计算图框架

的情况下，我

们可以像使

用NumPy进行普通

编程一

样进

行数值计算

，这样就不用

学习框架专

用的领域特

定语言了。另

外，计算图也

无须通过编

译变为独有

的数据结构

。换言之，计算

图可以像普

通的Python程序

一

样被构建和

执行。因此，用

户可以使用

Python的if语句和for语

句构建计算

图。

实际使用

DeZero可编写出以

下代码。

x = Variable(np.array(3.0))

y = Variable(np.array(0.0))

while

True:

 y =

y + x

if y.data > 100:

break

y.backward()

如上

所示，用户可

以使用 while语句

和

if语句来进

行计算。计算

图（在

DeZero框架下

相当于计算

图的链接）在

幕后被创建

。上面的代码

使用了while

语句

和if语句，其他

的Python编程技术

，如闭包和递

归调用等也

可以继续在

DeZero框架下使用

。

静态计算图

（Define

andRun）的框架需要

在流转数据

之前定义计

算图。

因此在

数据流转的

过程中，计算

图的结构不

能改变。另外

，在使用静

态

计算图框架

时，程序员还

必须学会领

域特定语言

的特殊运算

语法，

比如相

当于if语句的

tf.cond。

  第

2阶段　用自

然的代码表

达 158

在调试时

我们也可以

看到动态计

算图的优点

。由于计算图

是作为Python程

序

运行的，所以

对它的调试

和对普通Python程

序的调试一

样。当然，我们

也可

以使用

pdb等Python调试器。在

使用静态计

算图框架时

，代码则被编

译器转换

为

只有框架才

能理解和执

行的表现形

式，Python（Python解释器）自

然无法理解

这种独特的

表现形式。静

态计算图之

所以难调试

，是因为“计算

图的定义”和

“数

据流转”是

分离的。在大

多数情况下

，问题（bug）是在“数

据流转”的过

程中被发

现

的，但问题多

由“计算图的

定义”引起。使

用静态计算

图时，问题的

出现场所

和

根源地往往

是分开的，这

就导致用户

很难找到问

题出现的原

因。

静态计算

图的优点

静

态计算图最

大的优点是

性能。优化计

算图是提高

性能的一种

方法。优化计

算图的具体

做法是改造

计算图的结

构和换用效

率高的运算

方式。下面看

一下图B

2

所示

的例子。

a

b

1

*

a

b

1

mul-add

+

图B

2 计

算图优化的

示例

步骤 24

复

杂函数的求

导  159

图B

2展示的

是a * b +

1的计算图

和优化后的

计算图。图中

使用了一种

可

以同时进

行乘法运算

和加法运算

的运算方式

（大多数硬件

有同时进行

加法运算和

乘法运算的

指令）。由此，两

个运算合并

为一个运算

，从而减少了

计算时间。

除

了上面这种

细粒度的优

化，我们还可

以考虑对计

算图的整体

进行优化。由

于在Defineand

Run式的框

架中，整个计

算图在数据

流转前就已

经存在了，所

以

对计算图

整体进行优

化是可行的

。如果计算图

中有通过for语

句不断重复

的运算，

我们

就可以将其

合并起来，使

运算变得更

为高效。

神经

网络的训练

通常采用“只

定义一次网

络，多次在该

网络中流转

数据”

的流程

。在这种流程

下，即使网络

的构建和优

化需要一定

的时间，这

些

时间也很可

能在反复流

转数据的阶

段节省下来

。

Defineand

Run式框架的另

一个优点是

通过编译，计

算图会转化

为其他的运

行形式。这样

我们就可以

在不借助Python的

情况下流转

数据。从Python中独

立

出来的最

大好处是消

除了Python带来的

额外开销，这

对计算资源

匮乏的边缘

计

算环境来

说尤为重要

。

此外，在多台

机器上执行

分布式学习

时，也有适用

于Define

andRun的情况。

尤

其是对计算

图本身进行

分割并将其

分布到多台

机器上的场

景，在分割之

前需要

用到

整个计算图

。这就体现出

Define

andRun式框架的优

点。

小结

上面

讨论了Define

andRun和DefinebyRun各

自的优缺点

，表B

1总结了

这

些内容。

第 2阶

段　用自然的

代码表达 160

表

B-1 比较静态计

算图和动态

计算图

Defineand

Run DefinebyRun

优点

·高性能

·易于

优化网络结

构

·易于进行

分布式学习

·可以通过Python控

制计算图

·易

于调试

·擅长

动态计算

缺

点 ·需要掌握

特有的语言

（规则）

·难以创

建动态计算

图

·难以调试

·低性能（有变

低的趋势）

如

表B1所示，两种

模式各有利

弊。简而言之

，在性能方面

，Defineand

Run

更有优势；而

在易用性方

面，DefinebyRun具有显著

优势。

此外，由

于静态计算

图和动态计

算图各有利

弊，所以许多

框架兼备这

两种模

式。例

如，PyTorch基本采用

动态计算图

的模式，但它

也有静态计

算图的模式

（详

见参考文

献[16]）。同样，Chainer框架

基本采用Defineby

Run，但

它也可以切

换为DefineandRun的模式

。TensorFlow从第2版开始

将名为Eager Execution

的动

态计算图模

式作为标配

，它同样可以

切换到静态

计算图的模

式。

最近，有人

尝试在编程

语言层面增

加自动微分

功能。其中一

个著名的例

子

是Swift for

TensorFlow（参考文

献[17]）。这是对Swift这

个通用编程

语言的扩展

，

具体来说是

修改Swift编译器

，向其中加入

自动微分机

制。编程语言

本身具备自

动微分功能

后，有望在性

能和可用性

两方面获得

优势。

 161

第 3 阶段

实现高阶导

数

我们的DeZero现

在已经能够

完美地运行

反向传播了

。无论多么复

杂的

计算，它

都能以正确

的逻辑进行

反向传播。有

了现在的DeZero，许

多需要

求导

的问题应该

就能解决了

。不过有些事

情现在DeZero还做

不到，比如计

算高阶导数

。

高阶导数指

的是对导数

求导数。具体

来说，就是重

复求导，从一

阶导数

到二

阶导数，从二

阶导数到三

阶导数，以此

类推。PyTorch和TensorFlow

等现

代深度学习

框架都可以

自动计算高

阶导数。准确

来说，它们可

以在反向

传

播的基础上

进一步进行

反向传播（这

个原理会在

本阶段阐明

）。

下面进入第

3阶段。这个阶

段的主要目

标是扩展DeZero，使

其能够计算

高阶导数，DeZero的

用途会由此

变得更加广

泛。我们继续

前进吧！

第 3阶

段　实现高阶

导数 162

步骤 25 计

算图的可视

化（1）

163

步骤 25

计算

图的可视化

（1）

现在的DeZero可以

帮助我们轻

松地将复杂

的式子转化

为代码。在步

骤

24中，我们已

经编写了一

个相当复杂

的函数，即Goldstein-Price函

数。如

此复杂

的计算背后

会产生什么

样的计算图

呢？想必大家

也想亲眼看

看计算图

的

全貌吧。为此

，本步骤将对

计算图进行

可视化操作

。

对计算图进

行可视化操

作后，在问题

发生的时候

，我们会更容

易找出问题

出现的原因

，有时还能发

现更好的计

算方法。让计

算图可视化

也是一种将

神经网络的

结构直观地

传达给第三

方的手段。

我

们可以从头

开始构建一

个可视化工

具，不过这就

偏离深度学

习的主题了

。

因此，本书选

择使用第三

方可视化工

具Graphviz。在本步骤

中，笔者主要

介

绍如何使

用Graphviz，在下一个

步骤，我们将

使用Graphviz可视化

计算图。

25.1 安装

Graphviz

Graphviz是一个图形

可视化的工

具（这里的“图

形”指的是像

计算图那

样

有节点和箭

头的数据结

构）。首先来介

绍Graphviz的安装方

法。

第 3阶段　实

现高阶导数

164

Graphviz支持的操作

系统有Windows、macOS和Linux。这

里介绍一下

它在macOS和Ubuntu（Linux的发

行版之一）上

的安装方法

。关于在其他

操作系统的

安装说明，请

参考Graphviz官网。

在

macOS上，我们可以

使用Homebrew或MacPorts来安

装Graphviz。

在使用Homebrew的

情况下，要打

开终端，运行

以下命令。



$ brew install graphviz

在Ubuntu上，在终端运行以下命令即可完成安装。

$



sudo apt install graphviz



安装完成后

，我们就可以

从终端使用

dot命令了。下面

实际运行一

下这

个命令

。



$ dot -V

dot - graphviz version 2.40.1 (20161225.0304)

如果像上面这样显示了Graphviz的版本，则说明已正确安装。下面介

绍如何使用dot命令。dot命令的使用方法如下所示。

$



dot sample.dot -T png

-o sample.png

上面这条命

令用于将文

件名从sample.dot转换

为sample.png。我们可

以

在-o选项后指

定要输出的

文件名，在-T选

项后指定要

输出的文件

扩展名。

上面

指定了png作为

扩展名，我们

也可以把扩

展名指定为

pdf、svg等。

上面的命

令指定文件

sample.dot作为第一个

参数。该文件

用DOT语言

记述

了要描绘的

图形的内容

。DOT语言是用于

描述图形的

语言，它的语

法

很简单。在

下一节，我们

将学习DOT语言

。

步骤 25 计算图

的可视化（1）

165

25.2 使

用DOT语言描述

图形

下面试

着用DOT语言描

述图形。打开

常用的编辑

器，输入以下

内容。

digraph g{

x

y

}

笔者来

介绍一下DOT语

言的语法。首

先编写digraph g{...}，这是

固定

写法，然

后在“...”的部分

记述想要画

的图形的信

息。上面的例

子中写了x和

y，

由此可以画

出两个节点

，每个节点之

间必须通过

换行隔开。

输

入以上内容

后，将其保存

为sample.dot，然后运行

以下命令。



$ dot sample.dot -T png -o sample.png

执行上述命令，我们会得到图25-1中的图形。

x y

图251 从dot文件转换而成的图形

这是DOT语言最简单的应用示例。

25.3 指定节点属性

我们还可以指定节点的颜色和形状。例如，可以将刚才使用的sample.

dot文件按如下方式修改。

  第 3阶段　实现高阶导数 166

digraph g {

1 [label="x", color=orange, style=filled]

2 [label="y", color=orange, style=filled]

}

每一行都包含一个节点的信息，这一点与之前一样。不同的是，这里每

一行之前都有一个“1”或“2”这样的数字。这些数字代表节点的ID，[...]

中记述该ID的节点的属性。例如，在label="x"的情况下，字符x会显示在

节点上；在color=orange的情况下，节点会被绘制为橙色；在style=filled

的情况下，节点会被填充。

节点ID可以是大于等于0的任何整数值。不过设置的节点ID不能与其他

节点ID重复。

和之前一样，在终端执行 dot sample.dot -T png -o sample.png命令，

会得到图25-2中的图形。

x y

图252 改变节点的颜色（参见彩图）

转换而成的是如图25-2所示的图形。接下来尝试添加一个浅蓝色的矩形

节点。首先在sample.dot中添加以下内容。

digraph g {

1 [label="x", color=orange, style=filled]

2 [label="y", color=orange, style=filled]

3 [label="Exp", color=lightblue, style=filled, shape=box]

}

步骤 25 计算图的可视化（1）  167

如上所示，我们向sample.dot中添加了一个新节点，其属性被设置为浅

蓝色（lightblue）的矩形（box）。通过这个文件，我们可以得到图25-3中的图形。

x y Exp

图253 圆形（椭圆形）和矩形节点的例子（参见彩图）

图25-3中增加了一个矩形的节点，这样我们就能绘制DeZero的变量和

函数了。接下来要做的是用箭头把它们连接起来。

本书在展示计算图时，用圆形（椭圆形）表示变量，用矩形表示函数。在

使用DOT语言进行可视化操作时，也采用同样的做法，用圆形绘制变量，

用矩形绘制函数。

25.4 连接节点

DOT语言使用“->”连接两个节点的ID。例如，1->2用来画出一条从

ID为1的节点到ID为2的节点的箭头。下面编写如下所示的dot文件。

digraph g {

1 [label="x", color=orange, style=filled]

2 [label="y", color=orange, style=filled]

3 [label="Exp", color=lightblue, style=filled, shape=box]

1 -> 3

3 -> 2

}

通过这个dot文件，我们可以得到图25-4中的图形。

  第 3阶段　实现高阶导数 168

x

y

Exp

图254 有箭头连接的节点（参见彩图）

图25-3中的节点用箭头连接了起来。DOT语言还有很多其他的功能，

不过对我们来说，掌握以上知识已经足够了。这样就做好了绘制DeZero计

算图的准备。在下一个步骤，我们将增加使用DOT语言输出DeZero计算图

的功能。

步骤 26 计算图的可视化（2）  169

步骤 26

计算图的可视化（2）

在上一个步骤，我们学习了DOT语言的语法。在本步骤，我们将基于

学到的知识将DeZero的计算图转换为DOT语言。具体来说，就是要实现一

个将DeZero的计算转换为DOT语言的函数。

26.1 可视化代码的使用示例

我们将在dezero/utils.py中实现一个名为get_dot_graph的函数来可视化

计算图。首先看一下该函数的使用示例。

import numpy as np

from dezero import Variable

from dezero.utils import get_dot_graph

x0 = Variable(np.array(1.0))

x1 = Variable(np.array(1.0))

y = x0 + x1 # 某种计算

# 命名变量

x0.name = 'x0'

x1.name = 'x1'

y.name = 'y'

txt = get_dot_graph(y, verbose=False)

print(txt)

# 保存为dot文件

with open('sample.dot', 'w') as o:

 o.write(txt)

  第 3阶段　实现高阶导数 170

运行结果

digraph g {

4423761088 [label="y", color=orange, style=filled]

4423742632 [label="Add", color=lightblue, style=filled, shape=box]

4403357456 -> 4423742632

4403358016 -> 4423742632

4423742632 -> 4423761088

4403357456 [label="x0", color=orange, style=filled]

4403358016 [label="x1", color=orange, style=filled]

}

上面的代码将作为最终输出的变量y传给了get_dot_graph函数。函数随

后把从输出变量y开始的计算过程以用DOT语言编写的字符串的形式返回（后

面会解释verbose参数）。另外，上面的代码在调用get_dot_graph函数之前，

通过x0.name='x0'和x1.name='x1'等设置了Variable实例的name属性。这几

行代码的目的是在可视化计算图时，在变量节点上绘制标签名。

有了上面输出的字符串后，就可以把它写到sample.dot之类的文件中。

这样一来，我们就可以在终端通过dot sample.dot -T png -o sample.png命

令将其转换为图像。得到的图像如图26-1所示。

y

x0 x1

Add

图261 计算图可视化的示例（参见彩图）

步骤 26 计算图的可视化（2）  171

以上就是计算图的可视化流程。综上所述，我们要做的是用DOT语言

编写从输出变量开始的计算过程。其实我们已经知道该怎么做了，因为在反

向传播中实现的逻辑基本可以直接使用。

反向传播从输出变量开始回溯所有节点（变量和函数）。活用这个机制，我

们就可以将计算图的节点转换为DOT语言。

26.2 从计算图转换为DOT语言

下面我们开始实现前面探讨的内容。在实现get_dot_graph函数之前，首

先要实现辅助函数_dot_var。函数名前面的_（下划线）表示我们打算只在本

地使用这个函数，即只用于get_dot_graph函数。下面是_dot_var函数的代码

和它的使用示例。

dezero/utils.py

def _dot_var(v, verbose=False):

 dot_var = '{} [label="{}", color=orange, style=filled]\n'

 name = '' if v.name is None else v.name

 if verbose and v.data is not None:

 if v.name is not None:

 name += ': '

 name += str(v.shape) + ' ' + str(v.dtype)

 return dot_var.format(id(v), name)

# 使用示例

x = Variable(np.random.randn(2, 3))

x.name = 'x'

print(_dot_var(x))

print(_dot_var(x, verbose=True))

运行结果

4423761088 [label="x", color=orange, style=filled]

4423761088 [label="x: (2, 3) float64", color=orange, style=filled]

  第 3阶段　实现高阶导数 172

前面的代码将一个Variable实例赋给_dot_var函数，函数返回以DOT

语言编写的表示实例信息的字符串。为了使指定的变量节点的ID唯一，这

里使用了Python内置的id函数。使用id函数可以得到对象的ID。对象的

ID是该对象特有的，因此，在用DOT语言时，我们可以将对象的ID用作

节点的ID。

上面的代码还使用了format方法来操作字符串。format方法将字符串{}

的部分替换成作为format参数传来的对象（字符串和整数等）的值。

_dot_var函数中有一个名为 verbose的参数。当 verbose为 True时，

_dot_var函数会将ndarray实例的形状和类型也作为标签输出。

下面实现一个能将DeZero的函数转换为DOT语言的工具函数。该函数

名为_dot_func，代码如下所示。

dezero/utils.py

def _dot_func(f):

 dot_func = '{} [label="{}", color=lightblue, style=filled, 

shape=box]\n'

 txt = dot_func.format(id(f), f.__class__.__name__)

 dot_edge = '{} -> {}\n'

 for x in f.inputs:

 txt += dot_edge.format(id(x), id(f))

 for y in f.outputs:

 txt += dot_edge.format(id(f), id(y())) # y是weakref

 return txt

# 使用示例

x0 = Variable(np.array(1.0))

x1 = Variable(np.array(1.0))

y = x0 + x1

txt = _dot_func(y.creator) 

print(txt)

步骤 26 计算图的可视化（2）  173

运行结果

4423742632 [label="Add", color=lightblue, style=filled, shape=box]

4403357456 -> 4423742632

4403358016 -> 4423742632

4423742632 -> 4423761088

_dot_func函数用DOT语言记述了DeZero的函数。此外，它用DOT语

言记述了函数与输入变量之间的连接，以及函数与输出变量之间的连接。回

顾一下前面的内容：DeZero函数继承自Function类，它拥有实例变量inputs

和outputs（图26-2）。

f

f.inputs f.outputs

图262 Function类的inputs和outputs

准备工作完成了，下面实现get_dot_graph函数。我们可以参考Variable

类的backward方法，编写出来的代码如下所示。

dezero/utils.py

def get_dot_graph(output, verbose=True):

 txt = ''

 funcs = []

 seen_set = set()

 def add_func(f):

 if f not in seen_set:

 funcs.append(f)

 # funcs.sort(key=lambda x: x.generation)

 seen_set.add(f). . . . . .

. . . . . .

  第 3阶段　实现高阶导数 174

 add_func(output.creator)

 txt += _dot_var(output, verbose)

 while funcs:

 func = funcs.pop()

 txt += _dot_func(func)

 for x in func.inputs:

 txt += _dot_var(x, verbose)

 if x.creator is not None:

 add_func(x.creator)

 return 'digraph g {\n' + txt + '}'

上面代码的逻辑与Variable类的backward方法基本相同（阴影部分是与

backward方法的实现不同的地方）。backward方法传播的是导数，但这里没有

传播导数，而是向txt添加用DOT语言编写的字符串。

另外，在实际的反向传播中，节点的遍历顺序也很重要。为此，我们赋

予了函数一个generation（辈分）整数值，并按照该值从大到小的顺序取出函

数（详见步骤15和步骤16）。但在get_dot_graph函数中，节点遍历的顺序并

不重要，所以我们注释掉了按generation的值排序的代码。

这里需要关注的是“存在哪些节点”“哪个节点与哪个节点相连”。也就是说，

节点的遍历顺序并不重要，所以我们不需要使用根据generation的值优

先取出某些节点的机制。

计算图可视化的代码到此就全部完成了。下面添加一个能使计算图的可

视化操作更为简单的函数。

26.3 从DOT语言转换为图像

get_dot_graph函数将计算图转换为DOT语言。之后要将DOT语言转换

成图像，需要（手动）执行dot命令。但是，每次都执行dot命令实在太麻烦了，

步骤 26 计算图的可视化（2）  175

所以，我们要实现一个能执行dot命令的函数。代码如下所示。

dezero/utils.py

import os

import subprocess

def plot_dot_graph(output, verbose=True, to_file='graph.png'):

 dot_graph = get_dot_graph(output, verbose)

 # ①将dot数据保存至文件

 tmp_dir = os.path.join(os.path.expanduser('~'), '.dezero')

 if not os.path.exists(tmp_dir): # 如果~/.dezero目录不存在，就创建该目录

 os.mkdir(tmp_dir)

 graph_path = os.path.join(tmp_dir, 'tmp_graph.dot')

 with open(graph_path, 'w') as f:

 f.write(dot_graph)

 # ②调用dot命令

 extension = os.path.splitext(to_file)[1][1:] # 扩展名（png、pdf等）

 cmd = 'dot {} -T {} -o {}'.format(graph_path, extension, to_file)

 subprocess.run(cmd, shell=True)

首先，①处调用前面实现的get_dot_graph函数来将计算图转换成DOT

语言（文本）。然后，将文本保存至文件。保存的目标目录为~/.dezero，文

件名为tmp_graph.dot（该文件只是临时使用，所以文件名包含tmp）。代码中

的os.path.expanduser('~')的作用是展开主目录的路径“~”。

②处指定保存的文件名，并调用dot命令。这里将plot_dot_graph函数

的to_file参数用作目标文件名。为了调用外部程序，这部分代码使用了

subprocess.run函数。

除了上面展示的代码，实际的plot_dot_graph函数中还添加了另外几

行代码，这些代码用于应对使用Jupyter Notebook开发的情况。具体来

说，这些代码的作用是当程序在Jupyter Notebook中运行时，能直接在

Jupyter Notebook的单元格中显示图像。

这样就实现了计算图可视化函数。这里实现的函数会添加到dezero/

  第 3阶段　实现高阶导数 176

utils.py中，以便将来在不同的地方使用。这么操作之后，我们通过from 

dezero.utils import plot_dot_graph即可导入该函数使用。

26.4 代码验证

现在试着将步骤24中实现的Goldstein-Price函数可视化。代码如下所示。

steps/step26.py

import numpy as np

from dezero import Variable

from dezero.utils import plot_dot_graph

def goldstein(x, y):

 z = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2)) * \

 (30 + (2*x - 3*y)**2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2))

 return z

x = Variable(np.array(1.0))

y = Variable(np.array(1.0))

z = goldstein(x, y)

z.backward()

x.name = 'x'

y.name = 'y'

z.name = 'z'

plot_dot_graph(z, verbose=False, to_file='goldstein.png')

运行上面的代码会得到文件goldstein.png。结果如图26-3所示，这是

一个各种变量和函数交织在一起的计算图。仔细观察会发现该计算图以输

入变量x和y为始，以输出变量z为终。这里多说两句，在使用DeZero实

现 Goldstein-Price 函数时，我们几乎将式子直接照搬在代码中，但其实

DeZero在幕后创建了图26-3这种错综复杂的计算图。

步骤 26 计算图的可视化（2）  177

z

Mul

Add

Mul

Add

Mul

Pow

y

Mul Mul Mul Pow Mul Mul Add

Sub

Mul

x

Pow Mul

Mul

Mul

Pow Mul

Add

Add

Mul Sub

Pow

Sub

Add

Mul

Add

Mul

Add

Sub

Add

Mul Sub

Pow

Add

图263 Goldstein-Price函数的计算图

到这里就完成了计算图的可视化。将来我们会根据需要使用这里实现的

函数。

  第 3阶段　实现高阶导数 178

步骤 27

泰勒展开的导数

下面使用DeZero解决一些具体问题。我们先来思考一下sin函数的导数。

它的导数当然可以通过解析方式求出。这里我们采用“正面进攻”的做法，

也就是使用DeZero直接实现sin函数，然后尝试使用泰勒展开式求出sin函

数的导数。

27.1 sin函数的实现

sin函数的导数能够以解析方式求出。若y =sin(x)，其导数就为 x

y = cos(x)。

因此，Sin类和sin函数可以按如下方式实现。

steps/step27.py

import numpy as np

from dezero import Function

class Sin(Function):

 def forward(self, x):

 y = np.sin(x)

 return y

 def backward(self, gy):

 x = self.inputs[0].data

 gx = gy * np.cos(x)

 return gx

def sin(x):

 return Sin()(x)

∂

∂

步骤 27 泰勒展开的导数  179

上面的代码利用NumPy内置的np.sin函数和np.cos函数轻松地完成了

实现。这样一来，我们就能在DeZero中使用sin函数进行计算了。下面是计

算在 x = 4

π 处y = sin(x)的导数的代码示例。

steps/step27.py

from dezero import Variable

x = Variable(np.array(np.pi/4))

y = sin(x)

y.backward()

print(y.data)

print(x.grad)

运行结果

0.7071067811865476

0.7071067811865476

计算得到的y的值和x的导数均为0.7071067811865476。这个值约等于

1

np.sqrt(2)（即 √1

2）。由sin( 4

π ) = cos( 4

π ) = √1

2 可知，计算结果是正确的。

27.2 泰勒展开的理论知识

下面进入正题。我们思考一下如何用另一种方法求出sin函数的导数。

所谓的另一种方法就是使用泰勒展开。泰勒展开是使用多项式逼近任意函数

的方法。式子如下所示。

f(x) = f(a) + f (a)(x − a) + 1

2! f (a)(x − a)

2 +

1

3! f (a)(x − a)

3 + ··· （27.1）

式子27.1就是f(x)在点a的泰勒展开。a是任意值，f(a)是f(x)在点a

的值。式子中的f ′

表示一阶导数，f ′′表示二阶导数，f ′′′表示三阶导数。式

子中的符号!表示阶乘（factorial），n!（n的阶乘）是从1到n的所有整数的乘积，

例如，5! = 5 × 4 × 3 × 2 × 1 = 120。

  第 3阶段　实现高阶导数 180

二阶导数是对普通导数进一步求导的结果。以物理学概念为例，位置的导

数（变化）是速度，速度的导数（变化）是加速度。在这个例子中，速度对

应于一阶导数，加速度对应于二阶导数。

利用泰勒展开，以点a为起点，f(x)可以表示为式子27.1。式子27.1中

的项包括一阶导数、二阶导数、三阶导数……如果在某一阶停止，得到的就

是f(x)的近似值。近似值中包含的项数越多，近似值的精度就越高。

a = 0时的泰勒展开也叫麦克劳林展开。将a = 0代入式子27.1，可得式

子27.2。

f(x) = f(0) + f (0)x +

1

2! f (0)x2 +

1

3! f (0)x3 + ··· （27.2）

从式子27.2可以看出，通过将a限制为a = 0，我们得到的数学式更加简洁。

现在将 f(x) = sin(x) 代入式子 27.2，此时 f ′

(x) = cos(x), f ′′(x) = −sin(x), 

f ′′′(x) = −cos(x), f ′′′(x) = sin(x), ···。另 外，由 于 sin(0) = 0，cos(0) = 1，

所以可推导出以下式子。

sin(x) = x

1! − x3

3! + x5

5! −··· =

∞



i=0

(−1)i x2i+1

(2i + 1)! （27.3）

从式子27.3可以看出，sin函数用x的多项式表示，多项式的项无限延续。

这里很重要的一点是，随着�的i的增大，近似值的精度会升高。另外，随

着i的增大，(−1)i x2i+1

(2i+1)! 的绝对值会越来越小，所以我们可以根据这个绝

对值来确定i的值（重复次数）。

27.3 泰勒展开的实现

下面根据式子27.3来实现sin函数。为了计算阶乘，我们需要使用Python

的math模块中的math.factorial函数。

步骤 27 泰勒展开的导数  181

steps/step27.py

import math

def my_sin(x, threshold=0.0001):

 y = 0

 for i in range(100000):

 c = (-1) ** i / math.factorial(2 * i + 1)

 t = c * x ** (2 * i + 1)

 y = y + t

 if abs(t.data) < threshold:

 break

 return y

上面的代码基于式子27.3实现，for语句中的t是第i次要添加的项。代

码中的threshold是阈值，当t的绝对值低于阈值时，程序退出for循环。代

码通过threshold控制近似精度（threshold越小，近似精度越高）。

接下来使用上面实现的my_sin函数进行计算。

steps/step27.py

x = Variable(np.array(np.pi/4))

y = my_sin(x)

y.backward()

print(y.data)

print(x.grad)

运行结果

0.7071064695751781

0.7071032148228457

这个结果与本步骤最开始实现的sin函数的计算结果基本相同。误差很小，

可以忽略。降低threshold的值可以进一步缩小误差。

从理论上来说，泰勒展开的阈值（threshold）越小，近似精度越高。但计

算机在计算的过程中会出现精度丢失和舍入误差等情况，所以结果并不一

定与理论值相符。

  第 3阶段　实现高阶导数 182

27.4 计算图的可视化

我们来看一下上面的代码运行时，会创建出什么样的计算图。这里使用

上一步实现的可视化函数，即dezero/utils.py中的plot_dot_graph函数。首

先是当threshold=0.0001时my_sin函数的计算图。结果如图27-1所示。

y

Add

Mul

Pow

x

Pow Pow Pow

Add

Mul

Add

Mul

Add

Mul

图271 threshold=0.0001时my_sin函数的计算图

图27-1是为了近似sin函数而创建的计算图。我们可以通过threshold的

值来控制计算图的复杂性，这一点很有趣。试着让threshold=1e-150（0.00…1，

整个数中共有150个0），结果如图27-2所示。

步骤 27 泰勒展开的导数  183

y

Add

Mul

Pow

x

Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow Pow

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

Add

Mul

图272 threshold=1e-150时my_sin函数的计算图

降低threshold的值会使for语句的循环次数变多，这样就会产生图27-2

这种复杂的计算图。这种计算图是我们使用Python的for语句和if语句创

建出来的。借助Python的控制语句，我们可以和平时一样编写代码，这体

现了DeZero的Define-by-Run的易用性。

  第 3阶段　实现高阶导数 184

步骤 28

函数优化

DeZero现在能够自动计算导数了。导数有多种用途，其中最重要的一

种用途就是函数优化。在本步骤，我们会尝试对具体的函数进行优化。

优化指的是对于给定的函数，找到使其取最小值（或最大值）的函数的参数（输

入）。神经网络的训练也是一个优化问题，它的目标是找到使损失函数的

输出值最小化的参数。因此，本步骤的内容可以直接应用于神经网络训练。

28.1 Rosenbrock函数

本步骤将处理Rosenbrock函数。该函数的式子如下，其形状如图28-1

所示。

y = 100(x1 − x2

0)

2 + (x0 − 1)2 (28.1)

观察图28-1，可以看到函数像是一个抛物线形状的绵延不断的山谷。

如果画出图 28-1 中“山”的等高线，就会发现线的形状很像香蕉，所以

Rosenbrock函数也叫“香蕉函数”。

本步骤的目标是找到使Rosenbrock函数的输出值最小的x0和x1。先说

答案，Rosenbrock函数的最小值在(x0, x1) = (1, 1)处。在本步骤，我们将使

用DeZero，看看它能否真的找到这个最小值。

步骤 28 函数优化  185

图281 Rosenbrock函数的形状（图片引自参考文献[19]）

Rosenbrock 函数的严格的定义是f(x0, x1) = b(x1 − x2

0)2 + (a − x0)2

，

其中a和b是常数。上面的例子是a = 1、b = 100时的Rosenbrock函数。

Rosenbrock函数常作为优化问题的基准函数来使用。当Rosenbrock函数

作为基准函数时，一般使用a = 1、b = 100作为a和b的值。

28.2 求导

首先求Rosenbrock函数在(x0, x1) = (0.0, 2.0)的导数 x

y

0 和 x

y

1 。使用

DeZero可按如下方式实现。

∂

∂

∂

∂

  第 3阶段　实现高阶导数 186

steps/step28.py

import numpy as np

from dezero import Variable

def rosenbrock(x0, x1):

 y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2

 return y

x0 = Variable(np.array(0.0))

x1 = Variable(np.array(2.0))

y = rosenbrock(x0, x1)

y.backward()

print(x0.grad, x1.grad)

运行结果

-2.0 400.0

如上所示，先把数值数据（ndarray实例）封装在Variable中，然后根据

式子编码。之后只要调用y.backward()，DeZero就会自动求出导数。

执行上面的代码，得到的x0的导数和x1的导数分别为-2.0和400.0。将

这两个导数以向量的形式汇总起来的(-2.0，400.0)称为梯度或梯度向量。梯

度展示了各点上函数的输出值增加得最快的方向。拿前面的例子来说，就是

在点(x0，x1)=(0.0，2.0)上，y的值增加得最快的方向是(-2.0，400.0)。这就

意味着梯度的反方向(2.0，-400.0)是y的值减少得最快的方向。

28.3 梯度下降法的实现

形状复杂的函数，其最大值可能不在梯度指示的方向上（或最小值不在

梯度的反方向上）。不过从局部来看，梯度表示函数的输出值最大的方向。

重复向梯度方向移动一定距离，然后再次求梯度的过程，可以帮助我们逐渐

接近目标位置（最大值或最小值）。这就是梯度下降法。如果从一个好的点开

始（给定一个好的初始值），使用梯度下降法就能高效地找到目标值。

下面使用梯度下降法来解决问题。这里的问题是找到Rosenbrock函数

步骤 28 函数优化  187

的最小值，因此我们要沿着梯度的反方向前进。考虑到这一点，代码可按下

面的方式编写。

steps/step28.py

x0 = Variable(np.array(0.0))

x1 = Variable(np.array(2.0))

lr = 0.001 # 学习率

iters = 1000 # 迭代次数

for i in range(iters):

 print(x0, x1)

 y = rosenbrock(x0, x1)

 x0.cleargrad()

 x1.cleargrad()

 y.backward()

 x0.data -= lr * x0.grad

 x1.data -= lr * x1.grad

上面的代码将迭代次数设置为iters。这里的iters是iterations的缩写。

与梯度相乘的值是事先设定好的。上面的代码设置了lr=0.001。这里的lr是

learning rate的首字母，意思是学习率。

上面代码中的for语句反复使用了Variable实例x0和x1来求导。由于这

会使导数值被相继加到x0.grad和x1.grad上，所以在计算新的导数时，

我们需要重置已经加过其他值的导数。因此，在进行反向传播之前，我们

要调用各变量的cleargrad方法来重置导数。

现在运行上面的代码。从输出信息可以看出(x0, x1)的值的更新过程。

在终端实际输出的结果如下所示。

  第 3阶段　实现高阶导数 188

运行结果

variable(0.) variable(2.)

variable(0.002) variable(1.6)

variable(0.005276) variable(1.2800008)

...

...

variable(0.68349178) variable(0.4656506)

我们可以看到从起点(0.0, 2.0)开始，最小值的位置是如何依次更新的。

将计算结果绘制在图上就是图28-2这样。

2.0

1.5

1.0

0.5

0.0

−0.5

2.5

−1.0

−2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

x0

图282 更新路径（圆点的轨迹表示更新过程，星号表示最小值的位置）

从图28-2可以看出我们正逐渐接近星号所指的目的地的位置，但尚未到

达目的地。所以，我们增加迭代次数，设置iters=10000，结果如图28-3所示。

x1

步骤 28 函数优化  189

2.5

2.0

1.5

1.0

0.5

0.0

−0.5

−1.0

−2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

x0

图283 iters=10000时的结果

如图28-3所示，这次离目的地更近了。此时(x0, x1)的值为(0.99449622, 

0.98900063)。如果再增加迭代次数，比如iters=50000，就会抵达(1.0, 1.0)。

本步骤的工作到此结束。在本步骤中，我们使用DeZero实现了梯度下降，

找到了Rosenbrock函数的最小值的位置，只是迭代次数实在太多了，有5万

次。其实梯度下降法并不擅长处理Rosenbrock这种类型的函数。下一个步

骤会介绍并实现另一种优化方法。

x1

  第 3阶段　实现高阶导数 190

步骤 29

使用牛顿法进行优化（手动计算）

在上一个步骤中，我们使用梯度下降法求得了Rosenbrock函数的最小值。

但在计算过程中，使用梯度下降法要经过近5万次的迭代才能找到梯度。从

这个例子可以看出梯度下降法有一个缺点，即在一般情况下，它的收敛速度

较慢。

有几种收敛速度较快的方法可用来替代梯度下降法，其中比较有名的是

牛顿法。在使用牛顿法进行优化时，我们能以更少的步数获取最优解。以上

一个步骤解决的问题为例，我们可以得到如图29-1所示的结果。

图291 采用梯度下降法的更新路径（左图）和采用牛顿法的更新路径（右图）

x0 x0

观察图29-1，可以看到梯度下降法在“山谷”里苦苦寻觅，缓慢地朝着

目的地前进，而牛顿法则跳过了“山谷”，一口气到达了目的地。它的迭代

x1

x1

步骤 29 使用牛顿法进行优化（手动计算）  191

次数仅有6次！梯度下降法需要迭代近5万次，而牛顿法只要6次即可找到

目标，二者相差悬殊。

对Rosenbrock函数进行优化时，梯度下降法和牛顿法在迭代次数上有很

大的差异。当然因初始值和学习率等设置值的不同，迭代次数也会有很大

的变化。在实际问题中，我们一般看不到这么巨大的差异。一般来说，如

果初始值与解足够接近，牛顿法会更快收敛。

29.1 使用牛顿法进行优化的理论知识

本步骤的目标是使用牛顿法实现优化。我们使用牛顿法替代梯度下降法，

验证它的确能更快收敛。另外，为了便于说明，我们使用只输入一个变量的

函数（Rosenbrock函数要输入两个变量）。

下面推导使用牛顿法进行优化的计算表达式。这里我们思考如何对

y = f(x)函数求最小值。要推导出计算表达式，首先需要通过泰勒展开将

y = f(x)变换成如下形式。

f(x) = f(a) + f (a)(x − a) + 1

2! f (a)(x − a)

2 +

1

3! f (a)(x − a)

3 + ··· (29.1)

通过泰勒展开，我们可以将f表示为以某一点a为起点的x的多项式（关

于泰勒展开，笔者已在步骤27中进行了介绍）。在多项式中，一阶导数、二

阶导数、三阶导数……各阶导数的项在不断增加，如果我们选择在某一阶结

束展开，就可以近似地表示f(x)。这里我们选择在二阶导数结束展开。

f(x)  f(a) + f (a)(x − a) + 1

2 f (a)(x − a)

2 (29.2)

式子29.2使用到二阶导数为止的项对f(x)这个函数进行了近似处理。如

果着眼于变量x，就可以看出这个表达式是x的二次函数。也就是说，“某个

函数”y = f(x)近似为x的二次函数。因此，这种近似叫作二次近似。图29-2

展示了我们所做的这些工作。

  第 3阶段　实现高阶导数 192

图292 以二阶泰勒展开进行近似的示例

如图29-2所示，二次近似函数是一条在点a处与y = f(x)相交的曲线。

让人欣慰的是二次函数的最小值可以通过解析的方式求得。为此，我们要做

的就是找到二次函数的导数为0的点。式子如下所示。

d

dx (f(a) + f (a)(x − a) + 1

2 f (a)(x − a)

2

)=0

f (a) + f (a)(x − a)=0

x = a − f (a)

f (a) （29.3）

从式子29.3可以看出，二次近似函数的最小值位于点 x = a − f (a)

f (a) 处。

换言之，我们只要将a的位置更新为 − f (a)

f (a) 即可，如图29-3所示。

步骤 29 使用牛顿法进行优化（手动计算）  193

图293 更新a的位置

将a的位置按照图29-3的方式进行更新，在更新后的a的位置重复同样

的操作。这就是使用牛顿法进行优化的操作。与梯度下降法比较来看的话，

牛顿法的特点就更明显了。我们来看下面的式子。

x ← x − αf (x) （29.4）

x ← x − f (x)

f (x) (29.5)

式子29.4为梯度下降法，式子29.5 A 为牛顿法。从式子可以看出，两种

方法都更新了x，但更新的方式不同。在梯度下降法中，系数α是手动设置

A 为了便于对比，式子29.5将式子29.3中的a改成了x。

  第 3阶段　实现高阶导数 194

的值，x的值通过沿着梯度的方向（一阶导数）前进α的方式来更新，而牛顿

法则通过二阶导数自动调整梯度下降法中的α。换言之，我们可以将牛顿法

看成 α = f

1

(x) 的方法。

这里探讨的是当函数的输入是标量时的牛顿法。这个理论可以很自然地扩

展到函数的输入是向量的情况。不同的是，在向量的情况下要将梯度作为

一阶导数，将黑塞矩阵作为二阶导数。详细内容请参考步骤36专栏。

综上所述，梯度下降法只利用一阶导数的信息，而牛顿法还利用了二阶

导数的信息。拿物理学中的概念来说就是梯度下降法只使用速度信息，而牛

顿法使用速度和加速度的信息。牛顿法有望通过增加的二阶导数的信息来提

高搜索效率，从而提高快速到达目的地的概率。

下面使用牛顿法尝试解决一个具体问题——对式子y = x4 − 2x2执行最

小化操作。如图29-4所示，这个函数的图形有两个“凹陷”的地方，取最小

值的地方有x = −1和x = 1两处。这里使用初始值x = 2，看看计算能否达

到其中一个取最小值的点x = 1。

10

8

6

4

2

0

−2

−2 −1 0 1 2

图294 y = x4 − 2x2的图形

x

y

步骤 29 使用牛顿法进行优化（手动计算）  195

29.2 使用牛顿法实现优化

下面来实现牛顿法。要使用牛顿法进行优化，只要实现式子29.5即可。

不过DeZero无法自动计算二阶导数，所以我们要手动对其进行计算，具体如下。

y = x4 − 2x2

y

x = 4x3 − 4x

2y

x2 = 12x2 − 4

基于以上结果，我们可以按如下方式使用牛顿法实现优化。

steps/step29.py

import numpy as np

from dezero import Variable

def f(x):

 y = x ** 4 - 2 * x ** 2

 return y

def gx2(x):

 return 12 * x ** 2 - 4

x = Variable(np.array(2.0))

iters = 10

for i in range(iters):

 print(i, x)

 y = f(x)

 x.cleargrad()

 y.backward()

 x.data -= x.grad / gx2(x.data)

与之前相同，一阶导数通过反向传播求得。二阶导数则通过手动编码求

得。之后根据牛顿法的更新公式来更新x。运行上面的代码，可以看到如下

所示的x值的更新过程。

∂

∂

∂

∂

  第 3阶段　实现高阶导数 196

0 variable(2.0)

1 variable(1.4545454545454546)

2 variable(1.1510467893775467)

3 variable(1.0253259289766978)

4 variable(1.0009084519430513)

5 variable(1.0000012353089454)

6 variable(1.000000000002289)

7 variable(1.0)

8 variable(1.0)

9 variable(1.0)

本题的答案（最小值）是1。从上面的结果可以看出，实际只迭代了7次

就找到了最小值。与之相比，梯度下降法需要迭代很多次才能接近最优解。

图29-5是两种方法的更新路径的比较图。

10

8

6

4

2

0

−2

10

8

6

4

2

0

−2

−2 −1 0 1 2 −2 −1 0 1 2

x x

图295 梯度下降法（左）的更新路径和牛顿法（右）的更新路径

如图29-5所示，梯度下降法的迭代次数较多。上图中梯度下降法的结果

是当学习率设置为0.01时的情况，此时要经过124次迭代，x = 1.0的绝对误

差才小于0.001。相比之下，牛顿法只需迭代7次。

以上就是牛顿法的理论知识和实现。本步骤实现了牛顿法，并使用它解

决了具体问题，取得了很好的效果。不过，在实现过程中，二阶导数的计算

是手动进行的（为了求得二阶导数，要手动写出式子，并手动将其编写为代码）。

接下来要做的就是将这个手动的过程自动化。

y

y

步骤 30 高阶导数（准备篇）  197

步骤 30

高阶导数（准备篇）

当前的DeZero虽然实现了自动微分，但只能计算一阶导数。这里我们来

扩展DeZero，使它能自动计算二阶导数，甚至是三阶导数、四阶导数等高阶导数。

要想使用DeZero求二阶导数，我们需要重新思考目前反向传播的实现

方式。DeZero的反向传播是基于Variable类和Function类实现的。我们先来

简单回顾一下目前Variable和Function这两个类的实现。由于内容较多，所

以笔者分3节来进行介绍。

30.1 确认工作①：Variable实例变量

我们先来回顾一下Variable类的实例变量。首先看一下初始化Variable

类的__init__方法。

class Variable:

 def __init__(self, data, name=None):

 if data is not None:

 if not isinstance(data, np.ndarray):

 raise TypeError('{} is not supported'.format(type(data)))

 self.data = data

 self.name = name

 self.grad = None

 self.creator = None

 self.generation = 0

 ...

  第 3阶段　实现高阶导数 198

如代码所示，Variable类有几个实例变量。这里重点关注一下其中的

data和grad。data和grad分别用于正向传播和反向传播的计算。需要注意的是，

data和grad都是ndarray实例的引用。为了强调这一点，笔者引入图30-1这

种图形表示方法。

data

Variable

grad

图301 Variable的新表示方法

如图30-1所示，我们将data和grad画成立方体的容器。图30-2是data

和grad引用了ndarray实例的图示。

data

x

grad data

x

grad

图302 Variable的新表示方法（数据引用）

图30-2中的左图是执行x = Variable(np.array(2.0))时的例子，右图是

执行x.backward()或x.grad = np.array(1.0)时的例子。笔者将使用这种图形

表示方法进行说明。

步骤 30 高阶导数（准备篇）  199

30.2 确认工作②：Function类

接下来是Function类。下面是Function类的__call__方法的代码。我们

重点看一下阴影部分。

class Function:

 def __call__(self, *inputs):

 inputs = [as_variable(x) for x in inputs]

 # ①正向传播的计算（主处理）

 xs = [x.data for x in inputs]

 ys = self.forward(*xs)

 if not isinstance(ys, tuple):

 ys = (ys,)

 outputs = [Variable(as_array(y)) for y in ys]

 if Config.enable_backprop:

 self.generation = max([x.generation for x in inputs])

 # ②创建连接

 for output in outputs:

 output.set_creator(self)

 self.inputs = inputs

 self.outputs = [weakref.ref(output) for output in outputs]

 return outputs if len(outputs) > 1 else outputs[0]

①处通过xs = [x.data for x in inputs]提取Variable的实例变量data，

并将其汇总在列表xs中，然后调用forward(*xs)进行具体的计算。

接下来是②的部分。这部分创建了Variable和Function之间的连接。从

变量到函数的连接是通过set_creator方法实现的。它的原理是让新创建的

Variable记住它的父函数（自己）。同时，通过将函数的输入变量和输出变量

赋给inputs和outputs的实例变量，保持从函数到变量的连接。

在变量和函数之间建立连接是为了之后反向传播导数。DeZero在计算发

生时动态地创建了这种连接。

  第 3阶段　实现高阶导数 200

DeZero 中的函数都继承自 Function类。继承了 Function的类需要在

forward方法中实现具体计算。下面是Sin类中进行sin函数计算的代码。

class Sin(Function):

 def forward(self, x):

 y = np.sin(x)

 return y

 def backward(self, gy):

 x = self.inputs[0].data

 gx = gy * np.cos(x)

 return gx

代码中的forward方法的参数和返回值都是ndarray实例。同样，backward

方法的参数和返回值也都是ndarray实例。使用Sin类可以进行以下计算。

def sin(x):

 return Sin()(x)

x = Variable(np.array(1.0))

y = sin(x)

上面的代码只进行了sin函数的正向传播。图30-3展示了此时变量和函

数的“活动”可视化后的样子。

data

x Sin

grad data

y

grad

forward

backward

图303 y=sin(x)的计算图（仅正向传播）

步骤 30 高阶导数（准备篇）  201

如图30-3所示，在正向传播中，具体的计算在Sin类的forward方法中进行。

由此便产生了变量和函数之间的连接。再提示一遍，这个连接是在Function

类的__call__方法中创建的。

30.3 确认工作③：Variable类的反向传播

最后来看看反向传播的逻辑。Variable类的backward方法实现了反向传播。

这里我们来看看如下所示的Variable类的backward方法，重点关注阴影部分。

class Variable:

 ...

 def backward(self, retain_grad=False):

 if self.grad is None:

 self.grad = np.ones_like(self.data)

 funcs = []

 seen_set = set()

 def add_func(f):

 if f not in seen_set:

 funcs.append(f)

 seen_set.add(f)

 funcs.sort(key=lambda x: x.generation)

 add_func(self.creator)

 while funcs:

 f = funcs.pop()

 # 反向传播的计算（主处理）

 gys = [output().grad for output in f.outputs] # ①

 gxs = f.backward(*gys) # ②

 if not isinstance(gxs, tuple):

 gxs = (gxs,)

 for x, gx in zip(f.inputs, gxs): # ③

 if x.grad is None:

 x.grad = gx

 else:

 x.grad = x.grad + gx

  第 3阶段　实现高阶导数 202

 if x.creator is not None:

 add_func(x.creator)

 if not retain_grad:

 for y in f.outputs:

 y().grad = None

①的部分将Variable的实例变量grad汇总在列表中。这里的实例变量

grad引用了ndarray实例。因此，元素为ndarray实例的列表传给了②处的

backward方法。③的部分把从输出端开始传播的导数（gxs）设置为函数的输

入变量（f.inputs）的grad。

根据以上内容，我们一起来看看下面这段代码。

x = Variable(np.array(1.0))

y = sin(x)

y.backward(retain_grad=True)

上面的代码进行了sin函数的计算（正向传播），然后进行反向传播。这

里使用了y.backward(retain_grad=True)使所有变量保留导数（这个函数是在

步骤18中为了改善性能引入的）。图30-4展示了此时变量和函数的“活动”

可视化后的样子。

步骤 30 高阶导数（准备篇）  203

data

x Sin

grad data

y

grad

forward

backward

data

x Sin

grad data

y

grad

forward

backward

图304 y=sin(x)的计算图的正向传播和反向传播

如图30-4所示，执行计算y=sin(x)时，计算图被创建，Variable的实例

变量data被赋值。通过反向传播，Sin类的backward方法被调用，Variable

的实例变量grad被赋值。

这就是目前DeZero的反向传播的实现方式。在下一个步骤中，我们将

修改当前的DeZero，以求解高阶导数。

  第 3阶段　实现高阶导数 204

步骤 31

高阶导数（理论篇）

在上一个步骤，我们回顾了DeZero目前的实现情况。主要内容可归纳

为以下几点。

 计算的连接是在Function类的__call__方法中创建的

 正向传播和反向传播的具体计算是在继承了Function的类的forward方

法和backward方法中进行的

这里要注意的是创建计算图连接的时机。连接是在进行正向传播的计算

时创建的，在反向传播时不会被创建。这就是问题的关键所在。

31.1 在反向传播时进行的计算

与正向传播一样，反向传播也会进行具体的计算。以上一个步骤的Sin

类为例，实现backward方法的代码如下所示。

class Sin(Function):

 ...

 def backward(self, gy):

 x = self.inputs[0].data

 gx = gy * np.cos(x)

 return gx

步骤 31 高阶导数（理论篇）  205

上面的代码进行了 gx=gy*np.cos(x)这个具体的计算。但是，当前的

DeZero不会为该计算创建计算图，因为该计算针对的是ndarray实例。如果

能对反向传播所进行的计算创建连接，会发生什么呢？答案是高阶导数会被

自动计算出来。在解释这个原理之前，我们先来看看图31-1中的计算图。

y x sin

图311 y=sin(x)的计算图

图31-1是y=sin(x)的计算图。调用y.backward()即可求出y对x的导数。

这是我们已经见过的例子。现在来看一下图31-2的计算图。

x cos gx

gy

*

图312 用于求y=sin(x)的导数的计算图（gx是y对x的导数）

图31-2中的计算图用于求sin函数的导数。该图以计算图的形式展示了

前面Sin类的反向传播的代码（gx = gy * np.cos(x)）。对于图31-2这样的计

算图，我们可以通过调用gx.backward()求出gx对x的导数。由于gx本来就

是y=sin(x)的导数，所以调用gx.backward()可以对x再次求导，也就是求x

的二阶导数。

如果理解不了上面的内容，你也可以把图31-2中的x当作时间，把gx当

作速度来进行思考。这样来看，图31-2就表示一个输入某时间后输出在该

时间的速度的计算图。此时通过反向传播求出的是速度对时间的导数，结

果对应的是加速度。

  第 3阶段　实现高阶导数 206

接下来我们要做的是将图31-2这种求导计算作为计算图创建出来。求导

计算换句话说就是通过反向传播进行计算。因此，如果能为反向传播进行的

计算建立连接，就可以解决这个问题。下面我们来思考一下具体的实现方法。

31.2 创建反向传播的计算图的方法

在DeZero中，连接是在正向传播的计算过程中被创建的。准确来说，

连接是在使用Variable实例进行普通计算（正向传播）的过程中被创建的。这

意味着如果在函数的backward方法中使用Variable实例代替ndarray实例进

行计算，就会创建该计算的连接。

为此，我们需要将导数（梯度）保存为Variable实例。具体来说，就是按

照图31-3的方式修改Variable。

data

x

grad

data

x

grad

data

gx

grad

图313 以前的Variable类（左图）和新的Variable类（右图）

如图31-3所示，此前Variable类的grad引用了ndarray实例。这里将其

改为引用Variable实例。以前面提到的y=sin(x)为例，修改好这一处后，我

们可以创建如图31-4所示的计算图。

步骤 31 高阶导数（理论篇）  207

data

y

grad

data

gy

grad

data

x

grad

data

gx

grad

sin

sin

的反向传播

图314 Sin类的正向传播和反向传播完成后的计算图（省略了反向传播的计算内容）

图31-4是Sin类的正向传播和反向传播完成后的计算图。我们首次为反

向传播的计算创建了计算图。由于表示导数的gy成了Variable实例，所以针

对使用gy完成的计算，DeZero也能创建连接。

在计算y=sin(x)时，如果使用y.forward()，那么只有x这样的终端变

量会保存导数，y是函数创建的变量，所以不保存导数。在图31-4中，y.grad

没有引用gy。

图31-4省略了Sin类中反向传播的计算内容。在实现Sin类的backward

方法时，我们将求导的代码写成gx = gy * cos(x)。假定所有的变量都已经

改为Variable实例，这时，我们可以创建出图31-5这样的计算图来展示图

31-4中省略的反向传播的计算。

  第 3阶段　实现高阶导数 208

data

y

grad data

x

grad

sin

data grad

cos

data

gy

grad

cos(x)

图315 实际创建的计算图

图31-5中的计算图是通过调用y.backward()创建而成的“新”的计算图，

即通过反向传播创建而成的计算图。如果创建了图31-5这样的计算图，我们

就可以通过对变量gx调用gx.backward()来求y对x的二阶导数。

以上是求高阶导数的思路，在下一个步骤，我们将通过代码实现它。

*

data

gx

grad

步骤 32 高阶导数（实现篇）  209

步骤 32

高阶导数（实现篇）

本步骤将对DeZero进行修改以实现高阶导数。我们要做的是按照上一

个步骤探讨的内容为反向传播进行的计算创建计算图。因此，反向传播时进

行的计算会用到Variable实例。

此前，我们已经在dezero/core_simple.py中实现了Variable类。这里我

们在dezero/core.py中实现一个新的Variable类来代替它。dezero/core_

simple.py中也实现了针对四则运算等的函数和运算符重载，这些也将在

dezero/core.py中实现。

32.1 新的DeZero

新DeZero中最重要的变化发生在Variable类的实例变量grad上。之前

grad引用了ndarray实例，在新的DeZero中，我们将其改为引用Variable实例。

Variable类的修改情况如下。

dezero/core.py

class Variable:

 ...

 def backward(self, retain_grad=False):

 if self.grad is None:

 # self.grad = np.ones_like(self.data)

 self.grad = Variable(np.ones_like(self.data))

 ...

  第 3阶段　实现高阶导数 210

如上所示，只修改了一处，即在自动微分的地方将self.grad改为Variable

实例。这样就完成了对Variable类的修改。

32.2 函数类的反向传播

剩下的工作就是修改backward方法（不修改Function类）。之前，我们已

经在dezero/core_simple.py文件中实现了以下DeZero的函数类。

 Add 

 Mul 

 Neg 

 Sub 

 Div 

 Pow

我们将修改这些类的backward方法，然后将它们添加到dezero/core.py中。

首先从Add类开始，不过Add类不需要做任何修改。这里我们看一下Add类的

实现，代码如下所示。

dezero/core.py

class Add(Function):

 def forward(self, x0, x1):

 y = x0 + x1

 return y

 def backward(self, gy):

 return gy, gy

Add类的反向传播只是将导数从输出端向输入端传递而已。反向传播中没

有计算任何内容，所以没有需要修改的代码。

下一个是Mul类。将Mul类的backward方法按图32-1进行修改。

步骤 32 高阶导数（实现篇）  211

图321 Mul类的backward方法的对比（左边是旧代码，右边是新代码）

class Mul(Function):

 ...

 def backward(self, gy):

 x0 = self.inputs[0].data

 x1 = self.inputs[1].data

 return gy * x1, gy * x0

class Mul(Function):

 ...

 def backward(self, gy):

 x0, x1 = self.inputs

 return gy * x1, gy * x0

如图32-1所示，之前我们需要从Variable实例中取出数据（ndarray实例），

而在新的Mul类中，Variable实例可以直接使用。

图32-1中需要大家注意的是进行反向传播的代码gy * x1。再次强调，

在新的DeZero中，gy和x1是Variable实例。我们已经在Variable类上实现

了*运算符的重载，因此在执行gy * x1的背后，Mul类的正向传播会被调用。

此时，Function.__call__()会被调用，该方法中会构建计算图。

反向传播的计算针对的是Variable实例，所以我们需要使用DeZero函数

对Variable实例进行计算。

之后按照同样的步骤对Sub类、Div类和Pow类修改backward方法即可。

修改方法与前面介绍的内容相同，本书就不再一一介绍了。

32.3 实现更有效的反向传播（增加模式控制代码）

我们在步骤18中引入了启用/禁用反向传播的模式。具体来说，当不需

要反向传播时，切换到“禁用反向传播模式”，以此来省略用于反向传播的

处理（如创建计算图和保存输入变量等）。这里对反向传播中进行的计算 3333333333

使用

同样的机制。也就是说，对于在反向传播中进行的计算，如果不想再次反向

  第 3阶段　实现高阶导数 212

传播了，即只进行一次反向传播，就要在“禁用反向传播模式”下进行反向

传播的计算。为了实现这个机制，我们需要在Variable类的backward方法中

添加以下代码。

dezero/core.py

def backward(self, retain_grad=False, create_graph=False ):

 ...

 while funcs:

 f = funcs.pop()

 gys = [output().grad for output in f.outputs]

 with using_config('enable_backprop', create_graph):

 gxs = f.backward(*gys) # 主要的backward处理

 if not isinstance(gxs, tuple):

 gxs = (gxs,)

 for x, gx in zip(f.inputs, gxs):

 if x.grad is None:

 x.grad = gx

 else:

 x.grad = x.grad + gx # 这个计算也是对象

 if x.creator is not None:

 add_func(x.creator)

 ...

首先增加 create_graph参数，并将默认值设置为 False。然后在 with 

using_config(...)中进行实际的反向传播处理(步骤18中已经解释过using_

config函数的用法，这里不再赘述)。这意味着当create_graph为False时，

反向传播中的计算是在禁用反向传播模式下进行的。

这部分内容有点复杂，笔者用具体的例子来补充说明。例如，在进行Mul类

的反向传播时，其backward方法执行gy * x1的计算。因为*运算符被重载，

所以代码Mul()(gy, x1)会被调用，这会触发父类Function.__call__()

被调用。Function.__call__方法会根据Config.enable_backprop的

值来启用或禁用反向传播。

步骤 32 高阶导数（实现篇）  213

为什么让create_graph=False作为默认设置呢？这是因为只需要一次反

向传播的情况占大多数。如果需要求二阶导数，将参数设置为True即可。在

这种情况下，反向传播的计算会创建出新的计算图，反向传播得以继续进行。

32.4 修改__init__.py

到这里就完成了新DeZero的核心功能。我们将在dezero/core.py中完成

目前所做的修改。从现在开始，我们使用dezero/core.py代替dezero/core_

simple.py。为此，需要将用于初始化操作的dezero/_ _init_ _.py修改成如下

内容。

dezero/__init__.py

# 从step23.py到step32.py使用simple_core

is_simple_core = False # True

if is_simple_core:

 from dezero.core_simple import Variable

 from dezero.core_simple import Function

 from dezero.core_simple import using_config

 from dezero.core_simple import no_grad

 from dezero.core_simple import as_array

 from dezero.core_simple import as_variable

 from dezero.core_simple import setup_variable

else:

 from dezero.core import Variable

 from dezero.core import Function

 from dezero.core import using_config

 from dezero.core import no_grad

 from dezero.core import as_array

 from dezero.core import as_variable

 from dezero.core import setup_variable

 ...

上面的代码将is_simple_core改为False，这样就可以从dezero/core.py

中导入支持高阶导数的core文件了。

  第 3阶段　实现高阶导数 214

core_simple.py中没有修改的类和函数也被复制到了dezero/core.py中，

如Function类和using_config函数等。

本步骤到此结束。在下一个步骤，我们将使用新的DeZero自动计算高

阶导数。

步骤 33 使用牛顿法进行优化（自动计算）  215

步骤 33

使用牛顿法进行优化（自动计算）

我们在前面通过手动计算求出了二阶导数。这里我们使用新的DeZero

来自动计算二阶导数。首先针对简单的计算求二阶导数。如果验证可行，之

后使用牛顿法进行优化。

33.1 求二阶导数

现在试着求二阶导数，这里以步骤29中介绍的y = x4 − 2x2为对象。在

使用DeZero的情况下，我们可以将代码编写成下面这样（下面的代码其实存

在一个问题）。

import numpy as np

from dezero import Variable

def f(x):

 y = x ** 4 - 2 * x ** 2

 return y

x = Variable(np.array(2.0))

y = f(x)

y.backward(create_graph=True)

print(x.grad)

gx = x.grad

gx.backward()

print(x.grad)

  第 3阶段　实现高阶导数 216

运行结果

variable(24.0)

variable(68.0)

首先通过y.backward(create_graph=True)进行第1次反向传播。代码中

指定create_graph=True，为反向传播的计算创建一个计算图。接着，程序对

反向传播的计算图再次进行反向传播。由于这里求的是x的二阶导数，所以

使用了gx=x.grad来取出y对x的导数。之后从这个gx导数再次进行反向传播，

这样就可以求出gx对x的导数，也就是二阶导数了。

执行上面的代码，得到的一阶导数是24.0，二阶导数是68.0。根据式子

y′

= 4x3 − 4x可知，x = 2时的一阶导数为24，这与代码的运行结果一致；

根据二阶导数的式子y′′ = 12x2 − 4可知，x = 2时的二阶导数为44。遗憾的是，

这与代码的运行结果不同。

68这个错误的运行结果是一阶导数（24）加上二阶导数（44）所得出的值。

也就是说，新的反向传播是在Variable的导数保留了上次结果的状态下进行的，

所以新的导数值中加上了上次的结果。解决这个问题的方法是在执行新的计

算之前重置Variable的导数。

回顾一下，DeZero中有一个反向传播的参数，即x.backward(retain_

grad=False)中的retain_grad。这个retain_grad是在步骤18中引入

的，当它为False（默认值）时，中间计算的变量的导数（梯度）会被自动重置。

此时只有终端变量，即用户赋值的变量持有导数。在上面的计算中，调用

x.backward()后，只有x会保存它的导数。

基于以上内容，我们再来求解前面的问题。代码如下所示。

x = Variable(np.array(2.0))

y = f(x)

y.backward(create_graph=True)

print(x.grad)

步骤 33 使用牛顿法进行优化（自动计算）  217

gx = x.grad

x.cleargrad()

gx.backward()

print(x.grad)

运行结果

variable(24.0)

variable(44.0)

与之前不同的地方是在调用gx.backward()之前添加了x.cleargrad()，

这将重置x的导数。由此便能正确进行反向传播了。实际运行上面的代码，

得到的二阶导数的结果为44.0，这与通过式子计算的结果一致。

33.2 使用牛顿法进行优化

下面使用牛顿法进行优化。回顾之前的内容，使用牛顿法进行优化的式

子如下所示。

x ← x − f (x)

f (x) （33.1）

如式子33.1所示，我们将使用函数f(x)的一阶导数和二阶导数来更新x。

这次试着使用DeZero来自动求解。

steps/step33.py

import numpy as np

from dezero import Variable

def f(x):

 y = x ** 4 - 2 * x ** 2

 return y

x = Variable(np.array(2.0))

iters = 10

for i in range(iters):

 print(i, x)

  第 3阶段　实现高阶导数 218

 y = f(x)

 x.cleargrad()

 y.backward(create_graph=True)

 gx = x.grad

 x.cleargrad()

 gx.backward()

 gx2 = x.grad

 x.data -= gx.data / gx2.data

上面的代码以步骤29中实现的代码为基础。之前的代码采用了手动计

算二阶导数的做法，而这次我们通过执行两次backward方法来实现自动微分。

运行上面的代码会输出以下x值的更新过程。

0 variable(2.0)

1 variable(1.4545454545454546)

2 variable(1.1510467893775467)

3 variable(1.0253259289766978)

4 variable(1.0009084519430513)

5 variable(1.0000012353089454)

6 variable(1.000000000002289)

7 variable(1.0)

8 variable(1.0)

9 variable(1.0)

从上面的结果可以看出，迭代7次就可以到达最小值1。这个结果与步

骤29中的结果相同。也就是说，我们实现了自动使用牛顿法进行优化的方法。

步骤 34 sin函数的高阶导数  219

步骤 34

sin函数的高阶导数

目前，我们已经实现了几个支持高阶导数的函数了。这些函数的实现

都在dezero/core.py中（具体来说是Add类、Mul类、Neg类、Sub类、Div类和

Pow类）。本步骤，我们将实现几个新的DeZero函数。

今后我们会把DeZero的函数添加到dezero/functions.py中。这样在其他

文件中就可以使用from dezero.functions import sin来导入DeZero

的函数了。

34.1 sin函数的实现

首先要实现的是支持高阶导数的新的Sin类。我们先来看一下式子，

y = sin(x)的导数为 x

y = cos(x)。因此，Sin类和sin函数可以通过如下代

码实现。

dezero/functions.py

import numpy as np

from dezero.core import Function

class Sin(Function):

 def forward(self, x):

 y = np.sin(x)

∂

∂

  第 3阶段　实现高阶导数 220

 return y

 def backward(self, gy):

 x, = self.inputs

 gx = gy * cos(x)

 return gx

def sin(x):

 return Sin()(x)

这里重点看一下backward方法的实现。特别要注意的是，backward方法

中的所有变量都是Variable实例（forward方法中的变量是ndarray实例）。因此，

代码中的cos(x)是DeZero的cos函数。这意味着要想实现Sin类，需要用到

Cos类和cos函数。

另外，backward方法的实现需要所有计算都使用DeZero函数。如果函

数不是DeZero函数，就得重新实现它。上面代码中的乘法运算gy * cos(x)

通过运算符重载调用了DeZero的mul函数。

dezero/functions.py中的Sin类的代码与上面的代码有一些不同。实际的

代码中增加了支持GPU的代码。另外，本书后面出现的函数的代码中也

省略了支持GPU的代码。步骤52中会实现对GPU的支持。

34.2 cos函数的实现

下面实现Cos类和cos函数。我们先看一下式子，y = cos(x)的导数为

y

x = − sin(x)。代码如下所示。

dezero/functions.py

class Cos(Function):

 def forward(self, x):

 y = np.cos(x)

∂

∂

步骤 34 sin函数的高阶导数  221

 return y

 def backward(self, gy):

 x, = self.inputs

 gx = gy * -sin(x)

 return gx

def cos(x):

 return Cos()(x)

需要注意的是backward方法中的代码，该方法中的具体计算用到了sin

函数。幸好我们刚刚实现了sin函数，这样就完成了DeZero的sin函数和cos

函数的实现。

34.3 sin函数的高阶导数

下面试着求sin函数的高阶导数。这次不仅要尝试求二阶导数，还要求

三阶导数和四阶导数。代码如下所示。

import numpy as np

from dezero import Variable

import dezero.functions as F

x = Variable(np.array(1.0))

y = F.sin(x)

y.backward(create_graph=True)

for i in range(3):

 gx = x.grad

 x.cleargrad()

 gx.backward(create_graph=True)

 print(x.grad) # n阶导数

运行结果

variable(-0.8414709848078965)

variable(-0.5403023058681398)

variable(0.8414709848078965)

  第 3阶段　实现高阶导数 222

上面的代码使用for语句来重复进行反向传播。这样就能求出二阶导数、

三阶导数等n阶导数了。for语句中的代码与之前的相同。具体来说，就是

使用gx = x.grad取出导数，然后从gx进行反向传播。在进行反向传播之前，

调用x.cleargrad()来重置导数。重复这个过程，就可以得到n阶导数。

上面的代码中用来执行导入操作的代码是import dezero.functions as 

F。由此我们就可以使用F.sin()和F.cos()这样的写法了。今后我们还

会在dezero/functions.py中增加各种函数，到时F.xxx()这种写法会非常

方便。

接下来在前面代码的基础上编写绘制图像的代码。修改后的代码如下所示。

steps/step34.py

import numpy as np

import matplotlib.pyplot as plt

from dezero import Variable

import dezero.functions as F

x = Variable(np.linspace(-7, 7, 200))

y = F.sin(x)

y.backward(create_graph=True)

logs = [y.data]

for i in range(3):

 logs.append(x.grad.data)

 gx = x.grad

 x.cleargrad()

 gx.backward(create_graph=True)

# 绘制图像

labels = ["y=sin(x)", "y'", "y''", "y'''"]

for i, v in enumerate(logs):

 plt.plot(x.data, logs[i], label=labels[i])

plt.legend(loc='lower right')

plt.show()

这段代码与前面代码的主要区别是输入的变量变成了x = Variable(np.

linspace(-7, 7, 200))。这里的np.linspace(-7, 7, 200)会创建一个数组，该

步骤 34 sin函数的高阶导数  223

数组包含200个−7到7均匀间隔的数。具体来说，这是一个一维数组，值为

[-7., -6.92964824, -6.85929648, ..., 7.]。上面的代码将这个一维数组封

装在了Variable中。

除输入变量变为一维数组外，求高阶导数的代码与之前的完全相同。当

输入多维数组时，此前实现的DeZero函数会对每个元素分别执行计算。因此，

200个元素可以一次（正向传播）完成计算。

输入多维数组时，很多NumPy的函数会分别计算每个元素。在正向传播

时，DeZero的函数使用NumPy的函数来计算ndarray实例。因此，如果

向DeZero函数输入多维数组，DeZero函数将逐元素进行计算。

执行上面的代码，结果如图34-1所示。

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−6 −4 −2 0 2 4 6

y = sin(x)

y′

y′′

y′′′

图341 y=sin(x)及其高阶导数的图像（标签y' 对应一阶导数，y''对应二阶导数，y'''

对应三阶导数。参见彩图）

y

x

  第 3阶段　实现高阶导数 224

图34-1是y=sin(x)和它的高阶导数的图像，其展示了波的相位发生了

偏移的函数。之所以这么说，是因为按照一阶导数、二阶导数、三阶导数

这种方式不断求导，函数也会不断发生变化，即y=sin(x)→y=cos(x)→y=-

sin(x)→y=-cos(x)。

本步骤到此结束。本步骤重新实现了DeZero的sin函数和cos函数。在

下一个步骤，我们将继续增加新的DeZero函数。

步骤 35 高阶导数的计算图  225

步骤 35

高阶导数的计算图

紧接着上一个步骤的内容，本步骤将继续增加DeZero的函数。这里要

增加的是tanh函数，tanh表示双曲正切，tanh函数可用式子35.1表示，其

图像如图35-1所示。

y = tanh(x) = ex − e−x

ex + e−x （35.1）

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−4 −2 0 2 4

x

图351 tanh函数的图像

y

  第 3阶段　实现高阶导数 226

如图35-1所示，tanh函数将输入转化为−1和1之间的值。首先求式子

35.1的导数 x

y 。

35.1 tanh函数的导数

为了求tanh函数的导数，我们需要使用以下导数公式。



f

g(

(

x

x

)

)

  = f (x)g(x) − f(x)g (x)

g(x)2 （35.2）

式子35.2是分数函数的导数公式。为了便于查看，这里用f ′

(x)表示

f(x)对x的导数。利用以自然常数（e）为底的指数函数的导数式子 e

x

x

= ex

和 e−

x

x

= −e−x

，可求得式子35.1表示的tanh函数的导数。

tanh(x)

x = (ex + e−x)(ex + e−x) − (ex − e−x)(ex − e−x)

(ex + e−x)2

= 1 − (ex − e−x)(ex − e−x)

(ex + e−x)2

= 1 −

 (ex − e−x)

(ex + e−x)



2

= 1 − tanh(x)

2

= 1 − y2

（35.3）

如式子35.3所示，利用分数函数的导数，通过简单的数学式变形，我们

就可以求出tanh函数的导数。最终结果是1 − y2

。

35.2 tanh函数的实现

当y = tanh(x)时，tanh函数的导数为 tanh(

x

x) = 1 − y2。我们编写如

下代码来实现Tanh类和tanh函数。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

步骤 35 高阶导数的计算图  227

dezero/functions.py

class Tanh(Function):

 def forward(self, x):

 y = np.tanh(x)

 return y

 def backward(self, gy):

 y = self.outputs[0]()

 gx = gy * (1 - y * y)

 return gx

def tanh(x):

 return Tanh()(x)

正向传播使用了NumPy的np.tanh方法，而反向传播通过gy*(1 - y * y)

（也可以写成gy * (1 - y ** 2)）实现。以上就是DeZero中tanh函数的实现。

为了便于将来使用，我们将这个tanh函数添加到dezero/functions.py中。

35.3 高阶导数的计算图可视化

在实现了DeZero的tanh函数之后，我们就可以用它来做一些有趣的实

验了。具体要做的是计算tanh函数的高阶导数，并将计算图可视化。我们一

起去看看随着阶数的增加，计算图会呈现什么样的变化吧。代码如下所示。

steps/step35.py

import numpy as np

from dezero import Variable

from dezero.utils import plot_dot_graph

import dezero.functions as F

x = Variable(np.array(1.0))

y = F.tanh(x)

x.name = 'x'

y.name = 'y'

y.backward(create_graph=True)

  第 3阶段　实现高阶导数 228

iters = 0

for i in range(iters):

 gx = x.grad

 x.cleargrad()

 gx.backward(create_graph=True)

# 绘制计算图

gx = x.grad

gx.name = 'gx' + str(iters+1)

plot_dot_graph(gx, verbose=False, to_file='tanh.png')

这段代码与我们之前看到的代码基本相同，都是通过在for语句中重

复进行反向传播来求高阶导数的。这里通过iters的值来指定迭代次数：当

iters=0时为一阶导数，iters=1时为二阶导数……依此类推，然后将计算图

可视化。

在进行计算图的可视化操作时，需要使用步骤26中实现的plot_dot_

graph函数。这个函数的实现在dezero/utils.py中。

接下来运行上面的代码。首先看一下iters=0时的计算图。结果如图35-2

所示。

步骤 35 高阶导数的计算图  229

gx1

Mul

Sub

Mul

y

Tanh

x

图352 y=tanh(x)的一阶导数的计算图

图35-2是求y=tanh(x)的一阶导数的计算图。可以看出图中使用了Tanh、

Mul和Sub这些 DeZero的函数。接下来改变iters的值，计算二阶导数、三

阶导数……由此会产生什么样的计算图呢？结果如图35-3所示。

  第 3阶段　实现高阶导数 230

四阶导数 三阶导数 二阶导数 五阶导数

图353 n阶导数（n = 2, 3, 4, 5）的计算图

如图35-3所示，随着阶数的增加，计算图的结构也开始变得复杂。通过

反向传播，新的计算图在前面计算的基础上被创建，节点的数量因此呈指数

增长，我们可以感受到计算图在不断变大。六阶导数和七阶导数的结果如图

35-4所示。

六阶导数 七阶导数

图354 n阶导数（n = 6, 7）的计算图

步骤 35 高阶导数的计算图  231

图35-4是相当复杂的计算图，这样复杂的计算图几乎不可能通过人力画

出来。DeZero虽然是我们创建的，但它创造出了我们实现不了的东西。从

这里我们可以感受到编程的乐趣。

最后以八阶导数为对象进行可视化操作来结束本步骤的内容。结果如图

35-5所示。

图355 八阶导数的计算图

图35-5是一个更为复杂的计算图。我们已经无法在有限的纸面上看清节

点的形状了。为了让大家感受到这个计算图有多复杂，下一页放大展示了图

35-5方框中的区域。本步骤到此结束。

  第 3阶段　实现高阶导数 232

步骤 35 高阶导数的计算图  233

  第 3阶段　实现高阶导数 234

步骤 36

DeZero的其他用途

前面我们使用了DeZero求高阶导数。其实我们只做了一件事，那就是

为反向传播的计算创建连接。其中的重点——为反向传播创建计算图正是

DeZero的一个新功能。求高阶导数只不过是DeZero的一个应用示例。本步

骤将探索DeZero还有哪些新的用途。

新DeZero可以对反向传播进行的计算再次进行反向传播。这个功能叫作

double backpropagation（后面将其称为double backprop），大多数现代深

度学习框架支持这个功能。

36.1 double backprop的用途

接下来看看double backprop除了求高阶导数，还有哪些用途。首先思

考下面这个问题。

问题：给定以下两个式子，求x = 2.0时的 x

z

（z对x的导数）。

y = x2 （36.1）

z =

 x

y 

3

+ y （36.2）

∂

∂

∂

∂

步骤 36 DeZero的其他用途  235

这个问题还是前面见过的求导的问题。与之前不同的是式子36.2中包含

导数。也就是说，我们需要对包含导数的式子进一步求导。这个问题也可以

通过double backprop计算。在详细解释之前，我们先手动计算 x

z 。式子可

按如下方式展开。

y

x = 2x

z =

 x

y 

3

+ y = 8x3 + x2

z

x = 24x2 + 2x

按照上面的方式展开式子后，将x = 2.0代入24x2 + 2x，得到答案100.0。

上式中的 y

x 不是数值，而是x的表达式。如果此时求出在x = 2.0时 x

y

的值，并将其代入 z =

 y

x



3

+ y，我们就会得到错误的结果。

基于以上内容，我们尝试用DeZero来求解这个问题。代码如下所示。

steps/step36.py

import numpy as np

from dezero import Variable

x = Variable(np.array(2.0))

y = x ** 2

y.backward(create_graph=True)

gx = x.grad

x.cleargrad()

z = gx ** 3 + y

z.backward()

print(x.grad)

运行结果

variable(100.)

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

  第 3阶段　实现高阶导数 236

代码中比较重要的地方是y.backward(create_graph=True)。这行代码的

作用是进行反向传播以求出导数。于是，一个新的计算图就被创建了出来（此

时，2*x的计算图是在用户看不见的地方创建的）。之后使用反向传播创建的

计算图进行新的计算，再次进行反向传播。这样就可以求得正确的导数。

上面代码中的gx=x.grad不仅仅是一个变量（值），还是一个计算图（式子）。

因此，我们可以对x.grad的计算图再次进行反向传播。

求出导数的式子，并使用该式子进行计算，然后再次求导的问题可以

用double backprop来解决。我们也可以在深度学习的研究中看到double 

backprop的这种用法。下面介绍几个例子。

36.2 深度学习研究中的应用示例

有很多与深度学习相关的研究使用了double backprop。比如参考文献[21]

的论文中优化了图36-1中的式子。

梯度

图361 WGANGP中优化的函数（式子引自参考文献[21]）

图36-1中需要注意的是在要优化的式子中引入了梯度（梯度是对张量的

各个元素的导数）这一点。该梯度可以通过第1次反向传播求出。之后使用

梯度计算函数L，为了优化函数L，再进行第2次反向传播。

double backprop就以这种方式应用于最新的研究。除了WGAN-GP，

MAML（参考文献[22]）和TRPO（参考文献[23]）等著名研究都使用了double 

步骤 36 DeZero的其他用途  237

backprop的功能。

TRPO使用double backprop来计算黑塞矩阵和向量的积。使用double 

backprop可以使计算更有效率。接下来的专栏部分将介绍黑塞矩阵和向

量的积。



以上就是第3阶段的内容。在这个阶段，我们修改DeZero实现了double 

backprop，由此能够求出高阶导数，并实现牛顿法。从下一个步骤开始，我

们将以实现神经网络为目标修改当前的DeZero。

  第 3阶段　实现高阶导数 238

专栏：牛顿法和 double backprop 的补充知识

本专栏将对第3阶段的内容进行补充说明。首先会介绍输入为向量时的牛顿

法，之后介绍牛顿法之外的其他方法，最后介绍double backprop的实际应用示例。

本专栏使用了大量的数学表达式，难度较高。如果读者觉得难以理解，可以跳过，

继续阅读后面的章节（本专栏的内容与后面的内容没有太大关联）。

多变量函数的牛顿法

我们在第3阶段实现了牛顿法，当时用牛顿法求出了式子y = x4 − 2x2的最

小值。从式子中可以看出，输入变量只有x。因此，准确来说，我们实现的是输

入变量为一个变量（标量）时的牛顿法。

现在来看看输入是多维数组时的牛顿法。这里考虑输入变量是向量x，函数

为y = f(x)的情况。假设x是向量，有n个元素，即x = (x1, x2, ··· , xn)。

关于符号的字体，本书采用的表示方法为如果变量是向量，则使用黑

斜体；如果变量是标量，则使用普通字体。

C.1是对y = f(x)应用牛顿法后的式子。

x←x−[∇2

f(x)]−1

∇f(x) （C.1）

首先来解释一下这些符号。式子C.1中的∇f(x)表示梯度。梯度是对x的各

元素的导数。∇f(x)的元素如下所示。

∇f(x) =





f

x1

f

x2

.

.

.

f

xn





（C.2）

∂

∂

∂

∂

∂

∂

步骤 36 DeZero的其他用途  239

另外，∇2

f(x)是黑塞矩阵，黑塞矩阵的式子如下所示。

∇2f(x) =





2f

x2

1

2f

x1 x2

··· 2f

x1 xn

2f

x2 x1

2f

x2

2

··· 2f

x2 xn

.

.

. .

.

. ... .

.

. 2f

xn x1

2f

xn x2

··· 2f

x2

n





（C.3）

如式子C.3所示，黑塞矩阵是对x的两个元素的导数。由于它是两个元素的

组合，所以被定义为矩阵的形式。

梯度∇f(x)也可以写成 x

f 。黑塞矩阵∇f(x)也可以写成 x∂

2f

xT 。

式子C.1利用梯度和黑塞矩阵更新x（式子C.1的[∇2

f(x)]−1 表示黑塞矩阵

∇2

f(x)的逆矩阵）。这时，x会在梯度方向上更新，并通过黑塞矩阵的逆矩阵调

整移动距离。利用黑塞矩阵这一二阶导数的信息，可以使输入变量更积极地前进，

更快到达目的地。遗憾的是，牛顿法很少在机器学习，特别是神经网络中使用。

牛顿法的问题

在机器学习等领域，牛顿法有一个很大的问题。这个问题就是当参数数量增

加时，牛顿法的黑塞矩阵，准确来说是黑塞矩阵的逆矩阵，其计算复杂度会变大。

具体来说，当参数数量为n时，需要数量级为n2的内存空间。另外，n × n逆矩

阵的计算需要数量级为n3的计算量。

神经网络的参数数量超过100万是很常见的事情。如果用牛顿法更新

100万个参数，则需要一个大小为100万 × 100万的黑塞矩阵，然而，

很少有内存能容纳如此庞大的矩阵。

由于牛顿法在很多情况下不是一个现实的方案，所以有人提出其他方法来

∂

∂

∂

∂ ∂

∂

∂ ∂

∂

∂ ∂

∂

∂

∂

∂ ∂

∂

∂ ∂

∂

∂ ∂

∂

∂

∂

∂

∂

∂

  第 3阶段　实现高阶导数 240

代替它，其中就有拟牛顿法。拟牛顿法是近似牛顿法中的黑塞矩阵逆矩阵的方

法的总称（拟牛顿法指的不是某个具体的方法）。人们已经提出了几种具体的方

法，其中最著名的是L-BFGS，它仅根据梯度来近似黑塞矩阵。PyTorch实现

了L-BFGS（参考文献[20]），我们可以尝试使用它。不过目前在深度学习领域，

SGD、Momentum、Adam等只使用梯度进行优化的算法才是主流，L-BFGS等

拟牛顿法并不常用。

double backprop的用途：黑塞矩阵和向量的积

最后笔者对double backprop的内容进行补充。double backprop可以用来计

算黑塞矩阵和向量的积。前面提到过，当元素数量很多时，计算黑塞矩阵的开销

是巨大的。但是，如果只需要黑塞矩阵和向量的积这一“结果”，我们则可以使

用double backprop来快速计算它。

假设有y = f(x)和v，∇2

f(x)是黑塞矩阵，求∇2

f(x)v，即黑塞矩阵∇2

f(x)

和向量v的积。为此，我们需要将式子变换成下面这样。

∇2

f(x)v=∇(vT∇f(x)) （C.4）

只要把左右两边的元素写出来，就可以看出该变换是成立的。这里将向量的

元素数量限制为2。展开式子后，结果如下所示。

∇2f(x)v =





2f

x2

1

2f

x1 x2

2f

x2 x1

2f

x2

2







v1

v2





=





2f

x2

1

v1 + 2f

x1 x2

v2

2f

x2 x1

v1 + 2f

x2

2

v2





∂

∂

∂

∂ ∂

∂

∂ ∂

∂

∂

∂

∂

∂

∂ ∂

∂

∂ ∂

∂

∂

步骤 36 DeZero的其他用途  241

∇(vT∇f(x)) = ∇(

 v1 v2





 x

f

1

x

f

2



)

= ∇( f

x1

v1 + f

x2

v2)

=





2f

x2

1

v1 + 2f

x1 x2

v2

2f

x2 x1

v1 + 2f

x2

2

v2





虽然这里将向量的元素数量限制为2，但其实我们很容易就能将其扩展到元

素数量为n的情况。由此可知，式子C.4成立。现在再来看一下式子C.4。式子C.4

的右项表示先求出向量v与梯度∇f(x)的积，即向量的内积，然后针对结果进一

步求梯度。于是，我们就不需要再创建黑塞矩阵，由此可以提高计算效率。

现在我们尝试用DeZero来求黑塞矩阵和向量的积。下面是一个使用元素数

量为2的向量进行计算的例子（这里提前使用了F.matmul函数来计算矩阵的乘积）。

import numpy as np

from dezero import Variable

import dezero.functions as F

x = Variable(np.array([1.0, 2.0]))

v = Variable(np.array([4.0, 5.0]))

def f(x):

 t = x ** 2

 y = F.sum(t)

 return y

y = f(x)

y.backward(create_graph=True)

gx = x.grad

x.cleargrad()

z = F.matmul(v, gx) 

z.backward()

print(x.grad)

运行结果

variable([ 8. 10.])

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂ ∂

∂

∂ ∂

∂

∂

  第 3阶段　实现高阶导数 242

上面的代码相当于式子∇(vT∇f(x))。vT∇f(x)的计算相当于z=F.matmul(v, 

gx)。使用z.backward()进一步求z的梯度，这样就能求出黑塞矩阵和向量的积了。

顺便说一下，上面输出的是正确的结果。以上就是本专栏的内容。

 243

第 4阶段

创建神经网络

前面我们主要处理的变量是标量，但在机器学习领域，张量（多维数组）

扮演着重要的角色。第4阶段的目标是将DeZero扩展到机器学习，尤其是神

经网络领域。为此，我们首先要使其能够用张量进行计算。

在机器学习中，计算导数的工作往往很复杂，不过DeZero已经具备了

自动微分的基础能力。因此，接下来我们要做的工作在技术上并不难。之后，

我们的主要任务是基于DeZero自动微分的能力，增加机器学习所需的功能。

完成这些工作后，我们将检验DeZero的实力，尝试用它解决一些机器学习

的问题。

在本阶段，我们花费大量时间打造的DeZero会在深度学习（神经网络）

领域开花结果。到这一阶段结束时，DeZero将成长为真正的深度学习框架。

下面让我们进入第4阶段！

  第 4阶段　创建神经网络 244

步骤 37 处理张量  245

步骤 37

处理张量

前面，我们处理的变量主要是标量。但在机器学习中，向量和矩阵等张

量才是主角。本步骤将讨论使用张量时需要注意的地方，并为扩展DeZero

做准备。此外，通过本步骤，我们还会得知张量可以直接用于此前已经实现

的DeZero函数。

37.1 对各元素进行计算

此前我们实现了多个DeZero函数，如add、mul、div和sin等。在实现

这些函数时，我们假设输入和输出都是标量。例如在实现sin函数时，我们

假设了以下情况。

import numpy as np

import dezero.functions as F

from dezero import Variable

x = Variable(np.array(1.0))

y = F.sin(x)

print(y)

运行结果

variable(0.84147098)

  第 4阶段　创建神经网络 246

上面例子中的x是作为单一值的标量（准确来说，是一个零维的ndarray

实例）。此前我们假定处理的是这类标量，并以此实现了DeZero。如果x是

张量（比如矩阵），又会发生什么呢？此时sin函数会应用到每个元素上。实

际运行的结果如下所示。

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.sin(x)

print(y)

运行结果

variable([[ 0.84147098 0.90929743 0.14112001]

 [-0.7568025 -0.95892427 -0.2794155 ]])

上面的代码对x的每个元素都应用了sin函数。因此，输入和输出的张

量的形状没有发生变化。具体来说，输入x的形状是(2, 3)，输出y的形状也

是(2, 3）。像这样，之前实现的DeZero函数会对每个元素进行计算。例如，

在加法运算的情况下，DeZero函数也会按照以下方式对每个元素进行计算。

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))

y = x + c

print(y)

运行结果

variable([[11 22 33]

 [44 55 66]])

上面的代码通过对x和c逐元素相加，得到了y的结果。因此，输出y的

形状与x和c的相同。

在上面的代码中，x的形状和c的形状应该是相同的。这样就能在张量的

元素之间建立一对一的关系。此外，NumPy还有一个叫广播的功能。该

功能的作用是当x和c的形状不同时，自动复制数据，并将其转换为形状

相同的张量。笔者将在步骤40中详细介绍广播功能。

步骤 37 处理张量  247

37.2 使用张量时的反向传播

这是本步骤的核心内容。此前在反向传播的实现中，我们一直是以标量

为对象的。那么对使用了张量的计算进行反向传播，会发生什么呢？其实在

使用当前已实现函数的情况下，即使对张量进行计算，反向传播的代码也能

正常工作，理由如下。

 我们以标量为对象实现了反向传播

 向目前实现的DeZero函数传入张量，函数会将每个张量的元素作为标

量进行计算

 如果将张量的每个元素作为标量进行计算，那么以标量为前提实现的

反向传播也会对张量的每个元素进行计算

从上面的推导过程可知，对逐元素进行计算的DeZero函数来说，即使

传入的是张量，反向传播也能正常工作。实际验证的结果如下。

steps/step37.py

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

c = variable(np.array([[10, 20, 30], [40, 50, 60]]))

t = x + c

y = F.sum(t)

上面的代码使用了用于求和的sum函数进行计算。sum函数会在步骤39

中实现，这里提前使用了它。sum函数会对传入的张量求其元素之和，然后

输出一个标量。上面代码中的x、c、t，其形状都是(2, 3)，只有最后的输出

y是标量。

机器学习的问题中通常会设置一个以张量为输入，但以标量为输出的函数

（损失函数）。上面的代码假定了机器学习问题的场景，最后进行了输出标

量的计算。

  第 4阶段　创建神经网络 248

前面代码中所做的计算可以用图371所示的计算图表示。

1 2 3

4 5 6

x

10 20 30

40 50 60

11 22 33

44 55 66 231

t + y sum

c

图371 使用张量的计算图

图371具体展示了每个变量的数据。从图中可以看出最后的输出是标量。

这里对这个最后的输出为标量的计算图进行反向传播。我们紧接着上面的代

码编写如下代码。

steps/step37.py

y.backward(retain_grad=True)

print(y.grad)

print(t.grad)

print(x.grad)

print(c.grad)

运行结果

variable(1)

variable([[1 1 1]

 [1 1 1]])

variable([[1 1 1]

 [1 1 1]])

variable([[1 1 1]

 [1 1 1]])

上面的代码调用了y.backward(retain_grad=True)求各变量的导数。代码

步骤 37 处理张量  249

中使用参数retain_grad=True保留了所有变量的导数，输出的结果也是正确的。

正如这段代码所展示的那样，当前的DeZero函数使用张量也能正确地进行

反向传播。

这里有一点很重要，即梯度的形状和数据的形状（正向传播过程中的数

据）必须一致。这意味着x.shape == x.grad.shape、c.shape == c.grad.shape、

t.shape == t.grad.shape。利用这个特性，我们有望实现那些不是逐元素计

算的函数，如sum和reshape等，这部分内容会在下一个步骤实现reshape函

数时介绍。

张 量 的 导 数 在 机 器 学 习 领 域 称 为 梯 度，Variable类 的 grad其实是

gradient（梯度）的缩写。从现在开始，本书将不再使用“张量的导数”这

一叫法，而是将其称为“梯度”。

以上就是本步骤的主要内容。最后通过式子来补充说明使用张量时的反

向传播。补充的内容有些难度，但与后续步骤的关联不大，所以跳过这部分

内容也没有问题。

37.3 使用张量时的反向传播（补充内容）

本节使用式子来说明使用张量时的反向传播。首先是事先准备。思考函

数y = F(x)，其中x和y是向量，假设这两个向量的元素数都是n。

这里只讨论向量的情况，但是本节得出的结论（理论）也适用于张量（n阶

张量）的情况。这是因为在使用张量进行计算的情况下，只要增加向量化

过程（将元素排成一列，变形为向量的处理）作为预处理即可。于是，这

里的向量理论就可以直接应用于张量了。

我们来看看y = F(x)的导数。y对x的导数可通过以下式子定义。

  第 4阶段　创建神经网络 250

y

x =





y1

x1

y1

x2

··· y1

xn

y2

x1

y2

x2

··· y2

xn

.

.

. .

.

. ... .

.

.

yn

x1

yn

x2

··· x

yn

n





由于y和x都是向量，所以导数为上面这种矩阵形式。这个矩阵也叫雅

可比矩阵。顺带一提，如果y不是向量而是标量，那么y对x的导数就是下

面这样。

y

x =

 x

y

1

y

x2

··· y

xn



这是一个1 × n的雅可比矩阵，我们可以将它看作一个行向量（=水平

向量）。

接下来思考复合函数。假设有复合函数y = F(x)，它由3个函数复合而

成，分别是a = A(x)，b = B(a)，y = C(b)。假设变量x、a、b都是向量，

它们的元素数为n，只有最终的输出y是标量。那么，基于链式法则，y对x

的导数可以表示如下。

y

x = y

b

b

a

a

x （37.1）

式子37.1是基于链式法则得出的结果，其中的 y

b 和 a

b 表示雅可比矩阵。

将它们作为矩阵的乘积进行计算（步骤41中会介绍矩阵的乘积），这就是式

子37.1表示的内容。

接下来思考式子37.1中矩阵的乘积的计算顺序。有两种计算方法，第一

种是图372那种从输入端到输出端的计算方式。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

步骤 37 处理张量  251

图372 沿输入端到输出端的方向添加括号（前向模式）

图372所示的这种沿输入端到输出端的方向添加括号的方法叫作自动微

分的前向模式（下文简称为前向模式）。这里要注意的一点是，中间的矩阵乘

积结果是矩阵。例如 a

b

x

a 的结果是一个n × n矩阵。

另一种方法是沿输出端到输入端的方向添加括号进行计算，具体如图

373所示。这就是反向模式（准确来说是自动微分的反向模式）。

图373 沿输出端到输入端的方向添加括号（反向模式）

图373展示了沿输出端到输入端的方向添加括号进行计算的方法。这时

由于y是标量，所以中间的矩阵乘积结果都是向量（行向量）。例如， y

b a

b

的结果是一个由n个元素组成的向量。

前向模式下传播的是n × n矩阵，而反向模式下传播的是有n个元素的向量。

另外，向量和矩阵的乘积的计算成本比矩阵和矩阵的乘积的计算成本更低。

基于这些原因可知反向模式，也就是反向传播在计算方面更加高效。

∂

∂

∂

∂

∂

∂

∂

∂

  第 4阶段　创建神经网络 252

如图373所示，反向模式（在式子上）由向量和雅可比矩阵的乘积组成。

以图373为例，首先求 y

b（向量）和 a

b（雅可比矩阵）的积，然后求 a

y（向量）

和 x

a（雅可比矩阵）的积。像这样，反向传播中会针对每个函数求向量和雅

可比矩阵的乘积。

需要注意的是，我们不必特意先求出雅可比矩阵再计算矩阵的乘积，只

要求出结果就可以进行反向传播了。举例来说，我们思考一下图373中的

a = A(x)逐元素进行计算的场景（比如a = sin(x)）。如果求这个函数的雅

可比矩阵，可得以下结果。





a1

x1

0 ··· 0

0 a2

x2

··· 0

.

.

. .

.

. ... .

.

.

0 ··· 0 an

xn





通过上式可知，在逐元素计算的情况下，函数的雅可比矩阵是对角矩阵

（对角矩阵是主对角线之外的元素皆为0的矩阵）。其原因是xi只影响ai

（i是

1 ∼ n的整数）。在雅可比矩阵是对角矩阵的情况下，向量和雅可比矩阵的乘

积如下所示。

y

a

a

x =

 a

y

1

y

a2

··· y

an







a1

x1

0 ··· 0

0 a2

x2

··· 0

.

.

. .

.

. ... .

.

.

0 ··· 0 an

xn





=

 a

y

1

a1

x1

y

a2

a2

x2

··· y

an

an

xn



∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

步骤 37 处理张量  253

从上面的式子可以看出，最终的结果可以通过求各元素的导数，然后将

导数乘以各个元素来求出。也就是说，在逐元素计算的情况下，我们也可以

通过拿导数乘以每个元素的方式来求出反向传播。

这种计算方式的重点是，我们不必特意先求出雅可比矩阵再计算矩阵的乘

积，只要求出结果即可。因此，如果有更高效的计算（实现）方法，我们

就可以使用这种计算方式。

以上就是通过式子介绍的张量版反向传播的内容。

  第 4阶段　创建神经网络 254

步骤 38

改变形状的函数

上一个步骤介绍了对张量进行计算时的反向传播。以下是我们学到的内容。

 对于逐元素进行计算的函数，如add函数和sin函数，我们可以假定输

入和输出是标量，以此为前提实现正向传播和反向传播

 在这种情况下，即使输入是张量，反向传播也是有效的

接下来我们要看的是不会3 3

逐元素进行计算的函数。首先，我们要实现

两个函数：一个是改变张量形状的reshape函数，另一个是执行矩阵转置的

transpose函数。这两个函数都会改变张量的形状。

38.1 reshape函数的实现

现在来实现变换张量形状的函数。在此之前，我们先确认一下NumPy

的reshape函数的用法。编写np.reshape(x, shape)这样的代码，可以将x转

换为shape的形状。下面是使用示例。

import numpy as np

x = np.array([[1, 2, 3], [4, 5, 6]])

y = np.reshape(x, (6,))

print(y)

步骤 38 改变形状的函数  255

运行结果

[1, 2, 3, 4, 5, 6]

上面的代码将x的形状由(2, 3)变换为(6,)。张量中的元素数量没有改变，

只有形状发生了变化。现在来实现DeZero版本的reshape函数。这里的问题

是如何实现它的反向传播。

针对不会3 3

逐元素进行计算的函数，以张量的形状作为切入点，会使反向传

播的实现变得清晰。具体来说，就是要确保变量的数据与梯度的形状一致。

假设有Variable实例x，这时反向传播的实现需要确保x.data.shape 

== x.grad.shape。

reshape函数只是对形状进行变换，也就是说，它不进行具体的计算。因

此在反向传播的过程中，reshape函数对从输出端传来的梯度不进行任何修改，

直接将其传给输入端。不过，如图381所示，梯度的形状会变得与输入的形

状相同。

x y

y.grad

reshape

1 2 3 1 2

4 5 6

3 4 5 6

reshape’

a b c

a b d e f c d e f

x.grad

gx gy

图381 reshape函数的正向传播和反向传播的计算图（执行反向传播的函数用reshape'

表示，并使用伪梯度(a, b, c, d, e, f)）

  第 4阶段　创建神经网络 256

在图381中，反向传播从输出端传播梯度。为了使x.data.shape和x.grad.

shape相等，我们对梯度进行转换。具体来说，就是将形状为(6,)的梯度

的形状转换为(2,3)的形状，也就是将它转换成输入变量的形状。这就是

reshape函数的反向传播。根据上述内容，我们来实现DeZero的reshape函数。

dezero/functions.py

class Reshape(Function):

 def __init__(self, shape):

 self.shape = shape

 def forward(self, x):

 self.x_shape = x.shape

 y = x.reshape(self.shape)

 return y

 def backward(self, gy):

 return reshape(gy, self.x_shape)

首先，在初始化reshape类的过程中，reshape类的初始化方法__init__

会接收要转换的形状，并将其保存为shape。然后，正向传播的forward方

法使用NumPy的reshape函数对形状进行转换。该方法使用self.x_shape = 

x.shape保存输入x的形状。于是，在反向传播的backward方法中，梯度的形

状会转换为输入的形状（self.x_shape）。

backward(gy) 的参数 gy 是 Variable 实 例。因 此，backward(gy) 必

须使用DeZero函数对Variable实例进行计算。这里用到了正在实现的

reshape函数。

接下来，按如下方式实现reshape函数。

dezero/functions.py

from dezero.core import as_variable

def reshape(x, shape):

步骤 38 改变形状的函数  257

 if x.shape == shape:

 return as_variable(x)

 return Reshape(shape)(x)

函数的参数x应为ndarray实例或Variable实例。如果x.shape == shape，

则函数直接返回x。不过，为了确保reshape函数返回Variable实例，这里

使用as_variable函数将x转换为Variable实例。另外，as_variable函数已

经在步骤21中实现了。如果x为ndarray实例，as_variable(x)会将x转换为

Variable实例并返回；如果x是Variable实例，as_variable(x)会直接返回x。

DeZero函数的输入是Variable实例或ndarray实例，输出是Variable

实例。如果函数继承自Function类（如Reshape），ndarray实例会在该

函数类的__call__方法中自动转换为Variable实例。

下面使用一下刚刚实现的reshape函数。

steps/step38.py

import numpy as np

from dezero import Variable

import dezero.functions as F

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.reshape(x, (6,))

y.backward(retain_grad=True)

print(x.grad)

运行结果

variable([[1 1 1]

 [1 1 1]])

上面的代码使用reshape函数来改变形状，然后调用y.backward(retain_

grad=True)来求x的梯度。此时，y的梯度被自动补全。补全的梯度具有与y

相同的形状（y.grid.shape == y.shape），是所有元素都为1的张量。下面我们

来看一下都有哪些数据在流转。结果如图382所示。

  第 4阶段　创建神经网络 258

x y

y.grad

reshape

1 2 3 1 2

4 5 6

3 4 5 6

1 1 1

1 1 1 1 1 1 1 1 1

x.grad

gy gx reshape

图382 使用reshape函数进行计算的例子

如图382所示，在正向传播的过程中，张量的形状由(2, 3)变为(6,)；

在反向传播的过程中，梯度的形状由(6,)变为(2, 3)，与正向传播的转换相反。

此时可知各变量的data和grad的形状是相同的。以上就是DeZero的reshape

函数的实现。下一节我们将研究如何使这个函数更加易用。

38.2 从Variable对象调用 reshape

我们的下一个目标是使DeZero中reshape函数的用法更接近NumPy中

reshape函数的用法。在NumPy中，reshape的用法如下所示。

x = np.random.rand(1, 2, 3)

y = x.reshape((2, 3)) # 传递元组

y = x.reshape([2, 3]) # 传递列表

y = x.reshape(2, 3) # 直接（展开后）传递参数

步骤 38 改变形状的函数  259

如上面的代码所示，reshape可作为ndarray实例的方法使用，我们也可

以向reshape传递可变长参数，如x.reshape(2, 3)。我们想办法在DeZero中

也实现这种用法。为此，要在Variable类中添加以下代码A。

dezero/core.py

import dezero

class Variable:

 ...

 def reshape(self, *shape):

 if len(shape) == 1 and isinstance(shape[0], (tuple, list)):

 shape = shape[0]

 return dezero.functions.reshape(self, shape)

上面的代码在Variable类中实现了reshape方法。该方法接收可变长参数，

然后调整传来的参数。下面调用刚刚实现的DeZero的reshape函数，代码如

下所示。

x = Variable(np.random.randn(1, 2, 3))

y = x.reshape((2, 3))

y = x.reshape(2, 3)

如上面的代码所示，我们可以将reshape函数作为Variable实例的方法

来调用。这样就能更轻松地改变Variable的形状了。到这里，reshape函数

的实现就全部结束了。

38.3 矩阵的转置

接下来实现进行矩阵转置的函数。矩阵的转置是对矩阵进行图383这种

变形处理。

A 这里为了避免循环导入，没有采用F.reshape的写法，而是采用了dezero.functions.reshape的写法。

  第 4阶段　创建神经网络 260

图383 矩阵转置的例子

如图383所示，转置改变了矩阵的形状。下面在DeZero中实现执行转

置操作的函数。

本节中实现的转置函数transpose只支持输入变量是矩阵（二阶张量）的

情况。实际的DeZero的transpose函数的实现更为通用，它支持轴数据

的替换。这部分内容将在本步骤的最后一节介绍。

我们可以使用NumPy的transpose函数执行转置操作，示例如下。

x = np.array([[1, 2, 3], [4, 5, 6]])

y = np.transpose(x)

print(y)

运行结果

[[1 4]

 [2 5]

 [3 6]]

上面的代码使x的形状由(2, 3)变为(3, 2)。张量的元素本身没有发生

改变，改变的是张量的形状。因此，它的反向传播只改变从输出端传播的梯

度的形状。形状的改变方式正好是正向传播的逆向变化。基于以上内容，我

们按如下方式实现DeZero的transpose函数。

dezero/functions.py

class Transpose(Function):

 def forward(self, x):

步骤 38 改变形状的函数  261

 y = np.transpose(x)

 return y

 def backward(self, gy):

 gx = transpose(gy)

 return gx

def transpose(x):

 return Transpose()(x)

上面的代码在正向传播的过程中使用np.transpose函数进行转置，在反

向传播的过程中使用正在实现的transpose函数对从输出端传来的梯度进行

转置。因此，反向传播中所做的转置是正向传播的逆转置。下面我们来实际

使用一下transpose函数。

steps/step38.py

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.transpose(x)

y.backward()

print(x.grad)

运行结果

variable([[1 1 1]

 [1 1 1]])

从上面的代码可以看出，transpose函数可以用于计算，也可以顺利实

现反向传播。接下来，为了能够从Variable变量调用transpose函数，我们

来添加以下代码。

dezero/core.py

class Variable:

 ...

 def transpose(self):

 return dezero.functions.transpose(self)

 @property

 def T(self):

 return dezero.functions.transpose(self)

  第 4阶段　创建神经网络 262

上面的代码添加了两个方法。第一个方法可以作为transpose方法使用，

第二个方法通过添加@property，可以让自己作为实例变量来使用。由此，

我们可以编写如下代码。

x = Variable(np.random.rand(2, 3))

y = x.transpose()

y = x.T

到这里我们就实现了执行转置操作的 transpose函数。本节实现的

transpose函数只支持矩阵，实际的DeZero的transpose函数在本节内容的基

础上增加了一些代码。下面笔者对此进行补充说明。

38.4 实际的transpose函数（补充内容）

NumPy的np.transpose函数有更为通用的用法，这个用法就是改变轴

的数据顺序。下面是一个实际的例子。

A, B, C, D = 1, 2, 3, 4

x = np.random.rand(A, B, C, D)

y = x.transpose(1, 0, 3, 2)

上面的代码中有形状为(A, B, C, D)的数据，代码中使用np.transpose

函数改变了形状的轴（为了方便大家理解，这里使用A等变量来表示形状的值）。

代码中的参数是变换后的轴的顺序。图384可以帮助我们理解这些内容。

(A, B, C, D)

transpose

形状：

(0, 1, 2, 3)

(B, A, D, C)

(1, 0, 3, 2) 索引：

图384 np.transpose函数的具体例子

步骤 38 改变形状的函数  263

如上图所示，如果指定了轴的顺序，数据的轴就会按照指示重新排序。

如果参数为None，轴就会以相反的顺序重新排序。默认参数是None。因此，

如果x是矩阵，那么x.transpose()可以使轴0和轴1的数据按照轴1、轴0的

顺序排列，这正是矩阵的转置操作。

DeZero的transpose函数也支持对轴的数据进行调换。它的反向传播

只执行轴的反向调换。这里没有展示相关代码。感兴趣的读者可以参考

dezero/functions.py中Transpose类的代码。

  第 4阶段　创建神经网络 264

步骤 39

求和的函数

本步骤将实现DeZero的求和函数——sum函数。我们先回顾一下加法的

导数，然后使用它来推导sum函数的导数，之后着手实现sum函数。

39.1 sum函数的反向传播

我们已经实现了进行加法运算的函数。当y = x0+ x1时，加法运算的导

数是 x

y

0 = 1， x

y

1 = 1。因此，在反向传播的过程中，从输出端传来的梯

度会直接传播到输入端。传播过程如图391所示。

∂

∂

∂

∂

步骤 39 求和的函数  265

x0

y +

x1

1

3

2

gx0

+’

gx1

1

1

1

x0.grad

x1.grad

y.grad

gy

图391 加法运算的正向传播和反向传播

从图391的计算图可以看出，执行加法运算之后，从变量y开始了反向

传播。此时，从输出端传播的梯度值1被复制成两份传给变量x0和x1。这就

是加法运算的反向传播。这种加法运算的反向传播对有两个元素的向量也同

样成立。下面请看图392。

图392 

x.grad y.grad

3

x y sum

1 2

1

gx sum’

1 1

gy

sum函数的计算图示例1（用sum'表示执行反向传播的函数）

  第 4阶段　创建神经网络 266

图392中的变量x是一个由两个元素组成的向量。对该向量应用sum函

数会输出标量。在反向传播的过程中，从输出端传播的值1被扩展为向量[1, 1]

（一维数组）后继续传播。

基于以上内容，我们可以推导出由两个以上元素组成的向量之和的反向

传播，关键点就是按向量的元素数量复制梯度。具体如图393所示。

x.grad y.grad

21

x y sum

1

gx sum’ gy

1 2 3 4 5 6

1 1 1 1 1 1

图393 sum函数的计算图示例2

如图393所示，梯度被复制，其形状与输入变量的形状相同。这就是

sum函数的反向传播。它同样适用于输入变量是二维以上数组的情况。下面

来实现sum函数。

39.2 sum函数的实现

在DeZero的sum函数的反向传播中，梯度的元素会复制为输入变量的形

状。然而反向传播需要对Variable实例进行计算，这意味着复制操作必须作

为DeZero函数运行。

将元素复制为指定形状的操作与NumPy的广播功能相同。这个操作将在

下一个步骤中通过broadcast_to(x, shape)函数实现。该函数用于将x

（Variable实例）复制为shape的形状。

步骤 39 求和的函数  267

这里预先使用broadcast_to函数。这样一来，我们就可以用如下代码实

现DeZero的Sum类和sum函数。

dezero/functions.py

class Sum(Function):

 def forward(self, x):

 self.x_shape = x.shape

 y = x.sum()

 return y

 def backward(self, gy):

 gx = broadcast_to(gy, self.x_shape)

 return gx

def sum(x):

 return Sum()(x)

在上面的代码中，反向传播的处理使用了将在下一个步骤实现的broadcast_

to函数。通过这个函数复制梯度gy的元素，使其与输入变量在形状上一致，

这样就实现了sum函数。下面我们尝试使用一下刚刚实现的sum函数。

steps/step39.py

import numpy as np

from dezero import Variable

import dezero.functions as F

x = Variable(np.array([1, 2, 3, 4, 5, 6]))

y = F.sum(x)

y.backward()

print(y)

print(x.grad)

运行结果

variable(21)

variable([1 1 1 1 1 1])

上面的代码正确地完成了加法运算并求出了梯度。sum函数也支持输入

变量不是向量的情况。下面是对二维数组（矩阵）进行计算的结果。

  第 4阶段　创建神经网络 268

steps/step39.py

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.sum(x)

y.backward()

print(y)

print(x.grad)

运行结果

variable(21)

variable([[1 1 1]

 [1 1 1]])

如上面的结果所示，x.grad的形状和x的形状相同，输出的值也正确。

通过以上代码，我们实现了“基础版”的sum函数。接下来，我们将扩展当前

的sum函数，实现真正的sum函数。

39.3 axis和keepdims

NumPy的np.sum函数的功能更加强大。首先，它能指定求和时的轴。

代码示例如下所示。

x = np.array([[1, 2, 3], [4, 5, 6]])

y = np.sum(x, axis=0)

print(y)

print(x.shape, ' -> ', y.shape)

运行结果

[5 7 9]

(2, 3) -> (3,)

代码中x的形状是(2, 3)，输出y的形状是(3,)。上面的代码在np.sum(x, 

axis=0)中指定了axis=0。这里的axis表示轴，也就是多维数组排列的方向。

请看图394。

步骤 39 求和的函数  269

1

axis=1

2 3

4 5 6

axis=0

图394 ndarray实例的axis（轴的索引）

图394是一个二维数组的例子。轴的索引如图394所示。我们可以在

np.sum函数中指定轴，沿着该轴的方向求和（图395）。

1

axis=1

2 3

4

6

15 5 6

5 7 9

axis=0 x.sum(axis=1)

x.sum(axis=0)

图395 对每个axis进行x.sum()计算的结果

axis参数可以指定为int、None或元组。如果将axis指定为None，函数

会计算所有元素的总和，然后输出一个值（标量）（默认参数是axis=None）。

如果将axis指定为元组，如(0, 2)，函数就会沿着该元组指定的轴求和（在

(0, 2)的情况下，函数会对两个轴进行计算）。

np.sum函数中还有keepdims参数，它是用于指定输入和输出是否应具有

相同维度（轴的数量）的标志位。下面是keepdims的使用示例。

  第 4阶段　创建神经网络 270

x = np.array([[1, 2, 3], [4, 5, 6]])

y = np.sum(x, keepdims=True)

print(y)

print(y.shape)

运行结果

[[21]]

(1, 1)

在上面的代码中，y的形状是(1, 1)。如果keepdims=False，那么y的形

状为()（标量）。从中可以看出，指定keepdims=True可以保留轴的数量。

前面介绍的axis和keepdims两个参数在实践中经常用到。因此，我们来

修改DeZero的sum函数，使其支持这两个参数。尽管axis和keepdims会使和

的计算变得复杂一些，但sum函数的反向传播的理论不变，即只复制梯度的元素，

使其与输入变量在形状上一致。修改后的Sum类和sum函数如下所示。

dezero/functions.py

from dezero import utils

class Sum(Function):

 def __init__(self, axis, keepdims):

 self.axis = axis

 self.keepdims = keepdims

 def forward(self, x):

 self.x_shape = x.shape

 y = x.sum( axis=self.axis, keepdims=self.keepdims )

 return y

 def backward(self, gy):

 gy = utils.reshape_sum_backward(gy, self.x_shape, self.axis,}

 self.keepdims)

 gx = broadcast_to(gy, self.x_shape)

 return gx

def sum(x, axis=None, keepdims=False ):

 return Sum( axis, keepdims )(x)

Sum类在初始化阶段接收axis和keepdims，将它们设置为属性，然后在

步骤 39 求和的函数  271

正向传播中使用这些属性计算总和。反向传播的实现则和之前一样，使用

broadcast_to函数。由此复制梯度的元素，使它的形状与输入变量的形状相同。

在反向传播的实现中，我们在 broadcast_to函数之前使用了 utils.

reshape_sum_backward函数。这个函数会对gy的形状稍加调整（因为使

用axis和keepdims求和时会出现改变梯度形状的情况）。这是与NumPy

相关的问题，不是核心内容，所以这里就不详细解释了。

这样就完成了DeZero的sum函数。我们再对sum函数进行改造，使其也

可以作为Variable的方法使用。为此，我们需要向Variable类中添加以下代码。

dezero/core.py

class Variable:

 ...

 def sum(self, axis=None, keepdims=False):

 return dezero.functions.sum(self, axis, keepdims)

下面是DeZero的sum函数的使用示例。

steps/step39.py

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.sum(x, axis=0)

y.backward()

print(y)

print(x.grad)

x = Variable(np.random.randn(2, 3, 4, 5))

y = x.sum(keepdims=True)

print(y.shape)

运行结果

variable([5 7 9])

variable([[1 1 1]

 [1 1 1]])

(1, 1, 1, 1)

以上就是本步骤的内容。

  第 4阶段　创建神经网络 272

步骤 40

进行广播的函数

上一个步骤实现了DeZero的sum函数。这个sum函数的反向传播中预先

使用了broadcast_to函数，本步骤将实现这个broadcast_to函数。另外，为

了在DeZero中实现与NumPy同样的广播功能，我们将对DeZero的一些函

数进行修改。

NumPy具备广播功能，NumPy的广播有时发生在DeZero的正向传播中。

不过当前的DeZero无法在广播发生时正确地进行反向传播。为了能正确

地处理广播，我们需要修改DeZero。

下面先使用NumPy的函数进行说明，然后实现DeZero函数。

40.1 broadcast_to函数和sum_to函数

首先看一下NumPy的np.broadcast_to(x, shape)。这个函数复制x（ndarray

实例）的元素，使结果的形状变为shape的形状。它的使用示例如下所示。

步骤 40 进行广播的函数  273

import numpy as np

x = np.array([1, 2, 3])

y = np.broadcast_to(x, (2, 3))

print(y)

运行结果

[[1 2 3]

 [1 2 3]]

如上面的代码所示，原本形状为(3,)的一维数组，在元素被复制后变为

(2, 3)。那么在进行广播（即复制元素）之后，反向传播会变成什么样呢？

在DeZero中，同一个变量（Variable实例）可以多次用在计算中。比如

y = x + x这样的计算，我们可以把x + x理解为“复制”x后再使用它的

意思。在反向传播中，梯度两次传播到x，梯度之间执行了加法运算。通

过这个原理可知，复制元素之后，只需求梯度的和即可。

在复制元素之后，反向传播会求梯度之和。以np.broadcast_to函数为例，

其反向传播如图401所示。

x y broadcast_to

gx gy sum_to

x.grad y.grad

1 2 3

2 2 2

1 2 3

1 2 3

1 1 1

1 1 1

图401 broadcast_to函数的反向传播

  第 4阶段　创建神经网络 274

如图401所示，broadcast_to函数的反向传播会求梯度的和，以使梯度

的形状变为输入x的形状。要实现这一点，只要使用一个叫sum_to(x, shape)

的函数即可。sum_to函数会求x的元素之和，并将结果的形状变为shape的形

状。有了这样的函数，就可以像图401这样在正向传播和反向传播之间建立

联系了。

sum_to(x, shape)函数用于求x的元素之和并将结果的形状转变为shape

的形状。不过NumPy中没有这样的函数。因此，DeZero在dezero/utils.py

中提供了一个NumPy版本的sum_to函数。使用该函数可以进行以下计算。

import numpy as np

from dezero.utils import sum_to

x = np.array([[1, 2, 3], [4, 5, 6]])

y = sum_to(x, (1, 3))

print(y)

y = sum_to(x, (2, 1))

print(y)

运行结果

[[5 7 9]]

[[ 6]

 [15]]

如上面的代码所示，sum_to(x, shape)函数会执行求和操作，并将结果

的形状变为shape的形状。它的作用与np.sum函数的作用相同，但参数不同。

下面探讨sum_to函数的反向传播。sum_to(x, shape)用于求x的元素之和，

使结果的形状变为shape的形状。它的反向传播可以直接使用broadcast_to

函数，具体如图402所示。

步骤 40 进行广播的函数  275

1 1 1

5 7 9

1 2 3

4 5 6

x y

gx gy broadcast_to

1 1 1

1 1 1

sum_to

x.grad y.grad

图402 sum_to函数的反向传播是broadcast_to函数

如图402所示，sum_to函数的反向传播使用broadcast_to函数来复制

梯度的元素，使结果的形状变为输入x的形状。以上就是NumPy版本的

broadcast_to函数和sum_to函数。下面我们会实现DeZero版本的broadcast_

to函数和sum_to函数。

40.2 DeZero的broadcast_to函数和 sum_to函数

DeZero的BroadcastTo类和broadcast_to函数如下所示。

dezero/functions.py

class BroadcastTo(Function):

 def __init__(self, shape):

 self.shape = shape

 def forward(self, x):

 self.x_shape = x.shape

 y = np.broadcast_to(x, self.shape)

 return y

 def backward(self, gy):

  第 4阶段　创建神经网络 276

 gx = sum_to(gy, self.x_shape)

 return gx

def broadcast_to(x, shape):

 if x.shape == shape:

 return as_variable(x)

 return BroadcastTo(shape)(x)

这里让我们把注意力放在反向传播的代码上。反向传播使用DeZero的

sum_to函数将结果的形状变为输入x的形状。接下来实现这个sum_to函数。

以下是SumTo类和sum_to函数的代码。

dezero/functions.py

from dezero import utils

class SumTo(Function):

 def __init__(self, shape):

 self.shape = shape

 def forward(self, x):

 self.x_shape = x.shape

 y = utils.sum_to(x, self.shape)

 return y

 def backward(self, gy):

 gx = broadcast_to(gy, self.x_shape)

 return gx

def sum_to(x, shape):

 if x.shape == shape:

 return as_variable(x)

 return SumTo(shape)(x)

需要注意反向传播的代码。反向传播复制梯度的元素使结果的形状变为

输入x的形状。在这个过程中用到了前面实现的DeZero的broadcast_to函数。

从代码中可以看出，broadcast_to函数和sum_to函数相互依赖。这样我们就

完成了DeZero的broadcast_to函数和sum_to函数。

步骤 40 进行广播的函数  277

40.3 支持广播

在本步骤实现sum_to函数是为了支持NumPy的广播。广播是NumPy

的一个功能，它使不同形状的多维数组之间的运算成为可能。下面是广播的

示例代码。

x0 = np.array([1, 2, 3])

x1 = np.array([10])

y = x0 + x1

print(y)

运行结果

array([11, 12, 13])

在上面的代码中，x0和x1的形状是不同的。在进行上面的计算时，元

素会被复制，以使x1与x0的形状相匹配。这里比较重要的一点是NumPy的

广播功能是在幕后进行的。DeZero也会实现这样的广播。我们来看下面的

代码。

x0 = Variable(np.array([1, 2, 3]))

x1 = Variable(np.array([10]))

y = x0 + x1 

print(y)

运行结果

variable([11, 12, 13])

上面的代码在正向传播时会进行广播，这是因为代码是基于ndarray实

例实现的。当然，如果在正向传播中进行了广播，那么在反向传播时就必须

进行广播的反向传播，但是目前的DeZero不会对广播的反向传播做任何处理。

NumPy的广播是在broadcast_to函数中进行的，broadcast_to函数的反

向传播对应的是sum_to函数。考虑到这一点，我们将DeZero的Add类修改成

下面这样。

  第 4阶段　创建神经网络 278

dezero/core.py

class Add(Function):

 def forward(self, x0, x1):

 self.x0_shape, self.x1_shape = x0.shape, x1.shape

 y = x0 + x1

 return y

 def backward(self, gy):

 gx0, gx1 = gy, gy

 if self.x0_shape != self.x1_shape:

 gx0 = dezero.functions.sum_to(gx0, self.x0_shape)

 gx1 = dezero.functions.sum_to(gx1, self.x1_shape)

 return gx0, gx1

如果在正向传播中进行了广播，就说明输入的x0和x1在形状上是不同的。

此时应进行广播的反向传播计算。为此需要求梯度gx0的和，使gx0变为x0

的形状，还要求梯度gx1的和，使gx1变为x1的形状。

以上修改是针对dezero/core.py中的Add类进行的。Mul、Sub、Div等所

有进行四则运算的类都要完成相同的修改。由此便可实现广播功能。经过以

上修改，我们可以编写以下代码。

steps/step40.py

import numpy as np

from dezero import Variable

x0 = Variable(np.array([1, 2, 3]))

x1 = Variable(np.array([10]))

y = x0 + x1

print(y)

y.backward()

print(x1.grad)

运行结果

variable([11 12 13])

variable([3])

步骤 40 进行广播的函数  279

上面的代码在x0 + x1时进行了广播。不过，这次广播的反向传播在

DeZero函数中被正确执行了。实际得到的x1的梯度是3，这是正确的结果。

通过以上操作，DeZero实现了广播功能。

  第 4阶段　创建神经网络 280

步骤 41

矩阵的乘积

本步骤的主题是向量的内积和矩阵的乘积。这里会先介绍这两种计算方

法，然后将它们实现为DeZero函数。完成本步骤后，我们就有了能够处理

张量的最低限度的函数集，由此可以开始解决实际问题了。

41.1 向量的内积和矩阵的乘积

下面介绍向量的内积和矩阵的乘积。首先是向量的内积。假设有向量

a = (a1, ··· , an)和向量b = (b1, ··· , bn)。向量的内积可以定义为式子41.1。

ab = a1b1 + a2b2 + ··· + anbn (41.1)

如式子41.1所示，把两个向量间相应元素的乘积相加，得到的就是向量

的内积。

这里对式子中符号的使用做一些规定。我们使用a、b这样的符号表示标量，

使用a、b这样的粗体符号表示向量和矩阵。

接下来是矩阵的乘积。矩阵乘积的计算方法如图411所示。

步骤 41 矩阵的乘积  281

图411 矩阵乘积的计算方法

如图411所示，矩阵乘积的计算方法是先分别求出左侧矩阵水平方向的

向量和右侧矩阵垂直方向的向量的内积，然后将结果存储在新矩阵的相应元

素中。例如，a的第1行和b的第1列的结果是新矩阵第1行第1列的元素，

a的第2行和b的第1列的结果是新矩阵第2行第1列的元素，以此类推。

下面使用NumPy来实现向量的内积和矩阵的乘积。为此，我们需要使

用np.dot函数。

import numpy as np

# 向量的内积

a = np.array([1, 2, 3])

b = np.array([4, 5, 6])

c = np.dot(a, b)

print(c)

# 矩阵的乘积

a = np.array([[1, 2], [3, 4]])

b = np.array([[5, 6], [7, 8]])

c = np.dot(a, b)

print(c)

运行结果

32

[[19, 22],

 [43, 50]]

  第 4阶段　创建神经网络 282

如代码所示，在计算向量的内积和矩阵的乘积时都可以使用np.dot函数。

如果np.dot(x, y)的两个参数都是一维数组，函数计算的就是向量的内积；

如果两个参数都是二维数组，函数计算的就是矩阵的乘积。

41.2 检查矩阵的形状

在使用矩阵和向量进行计算时，必须注意它们的形状。例如，在计算矩

阵的乘积时，形状的变化如图412所示。

形状：

图412 在计算矩阵乘积时，相应的维度（轴）的元素数量必须相同

图412展示了由3 × 2矩阵a和2 × 4矩阵b的乘积得到3 × 4矩阵c的

例子。如图所示，矩阵a和矩阵b的相应维度（轴）中的元素数量必须相同。

得到的矩阵c的行数是矩阵a的行数，列数是矩阵b的列数。

在矩阵的乘积等计算中，关注矩阵的形状，并观察其形状的变化是很重要

的。本书把这种专门确认形状的工作称为“形状检查”。

41.3 矩阵乘积的反向传播

下面介绍矩阵乘积的反向传播。矩阵乘积的反向传播有些复杂，这里先

直接推导，之后进行补充说明以帮助大家直观理解。DeZero的矩阵乘积计

算在MatMul类和matmul函数中实现。matmul是matrix multiply的缩写。

步骤 41 矩阵的乘积  283

下面以y = xW为例介绍矩阵乘积的反向传播。在该计算中，x、W

和y的形状分别为1 × D、D × H和1 × H。计算图如图413所示。

matmul 某种计算

标量

图413 矩阵乘积的正向传播（各变量上方标出了形状）

再次强调，我们处理的是最终会输出标量的计算。因此，假定计算最终

输出的标量是L（通过反向传播求L对每个变量的导数），此时，L对x的第i

个元素的导数 x

L

i 的式子如下所示。

L

xi

= 

j y

L

j

yj

xi

(41.2)

式子41.2中的 x

L

i 表示当xi发生（微小的）变化时L的变化程度。当xi

发生变化时，向量y的所有元素也会发生改变。y的每个元素的改变也会使

L最终发生变化。因此，从xi到L有多条链式法则的路径，其总和为 x

L

i 。

到式子41.2为止的推导过程还是很简单的。我们可以利用 y

x

j

i = Wij

A，

将其代入式子41.2，推导出式子41.3。

L

xi

= 

j y

L

j

yj

xi

= 

j

L

yj

Wij (41.3)

从式子41.3可知， x

L

i 可通过向量 L

y 和W的第i行向量的内积求出。

由此我们可以推导出以下式子。

A 展开y的第j个元素，有yj = x1W1j + x2W2j + ··· + xiWij + ··· + xHWHj。由此可知， y

x

j

i = Wij。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

  第 4阶段　创建神经网络 284

L

x = L

y

WT (41.4)

如式子41.4所示， L

x 可通过矩阵的乘积一次性求出。此时矩阵（和向量）

的形状的变化如图414所示。

形状：

图414 检查矩阵乘积的形状

从图414可以看出，矩阵的形状没有问题。这也证实了式子41.4在计算

矩阵时是成立的。我们也可以利用这个结论，也就是在矩阵乘法成立的基础上，

推导出反向传播的式子（实现）A。在介绍该方法时，我们会再次思考y = xW

这个矩阵乘积的计算。不过这次我们假设x的形状是N × D。换言之，x、

W和y的形状分别为N × D、D × H和N × H。此时反向传播的计算图如

图415所示。

A 这意味着我们可以通过矩阵检查来推导出矩阵乘积的反向传播的式子，但矩阵检查的方法并不总是

能正确推导出反向传播的式子。

∂

∂

∂

∂

∂

∂

步骤 41 矩阵的乘积  285

matmul

matmul’

图415 矩阵乘积的正向传播（上图）和反向传播（下图）

下面来推导 L

x 和 W

L 。关注矩阵的形状，构建矩阵乘积的式子。推导

出的式子如图416所示。

形状：

形状：

图416 矩阵乘积的反向传播

∂

∂

∂

∂

  第 4阶段　创建神经网络 286

与图414中的式子一样，图416中的式子也可以通过计算每个矩阵的元

素并比较两边的结果推导出来。另外，我们也可以确认式子通过了矩阵乘积

的形状检查。有了这个式子，就可以轻松实现执行矩阵乘积计算的DeZero

函数了。代码如下所示。

dezero/functions.py

class MatMul(Function):

 def forward(self, x, W):

 y = x.dot(W)

 return y

 def backward(self, gy):

 x, W = self.inputs

 gx = matmul(gy, W.T)

 gW = matmul(x.T, gy)

 return gx, gW

def matmul(x, W):

 return MatMul()(x, W)

上面的代码根据图416中的式子实现了DeZero的反向传播函数。另外，

在正向传播中，我们没有使用np.dot(x, W)，而是将计算实现为x.dot(W)。因此，

它也可以作为ndarray实例的方法来使用。

在上面代码的反向传播中使用的matmul函数正是我们现在实现的函数。

另外，用于转置的操作（W.T和x.T）会调用DeZero的transpose函数（该

函数已在步骤38中实现）。

我们可以像下面这样使用DeZero的matmul函数来进行计算，也可以求

出导数。

steps/step41.py

from dezero import Variable

import dezero.functions as F

步骤 41 矩阵的乘积  287

x = Variable(np.random.randn(2, 3))

W = Variable(np.random.randn(3, 4))

y = F.matmul(x, W)

y.backward()

print(x.grad.shape)

print(W.grad.shape)

运行结果

(2, 3)

(3, 4)

上面的代码随机创建NumPy的多维数组，并用它们来进行计算。上

述代码在运行时没有抛出任何错误。另外根据结果可知，x.grad.shape等于

x.shape，w.grad.shape等于W.shape。这样就实现了DeZero版本的矩阵乘积。

  第 4阶段　创建神经网络 288

步骤 42

线性回归

机器学习使用数据来解决问题。不是由人来思考问题的解决方案，而是

让计算机从收集的数据中找到（学习）问题的解决方案。机器学习的本质就

是从数据中寻找解决方案。从现在开始，我们将使用DeZero来挑战机器学

习问题。本步骤将实现机器学习中最基本的线性回归。

42.1 玩具数据集

在本步骤，我们将创建一个用于实验的小型数据集。这个小型数据集称

为玩具数据集（toy datasets）。考虑到重现性，我们用固定的随机种子创建数据，

具体代码如下。

import numpy as np

np.random.seed(0)

x = np.random.rand(100, 1)

y = 5 + 2 * x + np.random.rand(100, 1)

上面的代码创建了一个由变量x和y组成的数据集。这些数据点沿直线

分布，是在y上增加作为噪声的随机数得到的。图421展示了这些(x, y)数

据点的分布情况。

步骤 42 线性回归  289

7.5

7.0

6.5

6.0

5.5

0.2 0.4 0.6 0.8 1.0 0.0

x

图421 本步骤使用的数据集

如图421所示，虽然x和y之间呈线性关系，但数据中存在噪声。我们

的目标是创建根据x值预测y值的模型（式子）。

根据x值预测实数值y的做法叫作回归（regression）。另外，当预测模型呈

线性（直线）时，这种回归分析称为线性回归。

42.2 线性回归的理论知识

接下来的目标是找到拟合给定数据的函数。假设y和x之间的关系是线

性的，函数的式子就可以表示为y = Wx + b（其中W是标量）。y = Wx + b

这条直线如图422所示。

y

  第 4阶段　创建神经网络 290

残差

数据

预测值（模型）

图422 线性回归的示例

如图422所示，我们的目标是找到一条拟合数据的直线y = Wx + b。为此，

我们需要尽可能地减小数据和预测值之间的差，这个差叫作残差（residual）。

下面是表示预测值（模型）和数据之间的误差指标的式子。

L = N

1 N



i=1

(f(xi) − yi)

2 (42.1)

在式子42.1中，先求出这N个点中的每个点(xi, yi)的平方误差，然后

将它们加起来，之后乘以 N

1 求出平均数。这个式子叫作均方误差（mean 

squared error）。另外，在式子42.1中求平均数时乘的是 N

1 ，但在某些情况下，

会乘以 2

1

N 。但无论哪种情况，在用梯度下降法求解时，都可以通过调整学

习率的值来解决同样的问题。

评估模型好坏的函数叫作损失函数（loss function）。此时，我们可以说线

性回归使用均方误差作为损失函数。

步骤 42 线性回归  291

我们的目标是找到使式子42.1表示的损失函数的输出最小的W和b。这

就是函数优化问题。我们已经（在步骤28中）用梯度下降法解决了这样的问题。

此处同样使用梯度下降法来找到使式子42.1最小化的参数。

42.3 线性回归的实现

下面使用DeZero实现线性回归。这里将代码分为前后两部分。首先展

示代码的前半部分。

steps/step42.py

import numpy as np

from dezero import Variable

import dezero.functions as F

# 玩具数据集

np.random.seed(0)

x = np.random.rand(100, 1)

y = 5 + 2 * x + np.random.rand(100, 1)

x, y = Variable(x), Variable(y) # 可以省略

W = Variable(np.zeros((1, 1)))

b = Variable(np.zeros(1))

def predict(x):

 y = F.matmul(x, W) + b

 return y

上面代码中创建的参数W和b是Variable实例（W为大写字母）。至于二者

的形状，W为(1,1)，b为(1,)。

DeZero函数可以直接处理ndarray实例（这些实例会在DeZero内部被转

换为Variable实例）。因此，上面代码中的数据集x和y可以作为ndarray

实例处理，无须显式地转换为Variable实例。

  第 4阶段　创建神经网络 292

上面的代码还定义了predict函数，这个函数使用matmul函数进行计算。

我们可以使用矩阵的乘积一次性对多个数据（在上面的例子中是100个数据）

进行计算。这时，形状的变化如图423所示。

形状：

x

(100,1) (100,1) (1,1)

W = y

图423 矩阵乘积的形状的变化（这里没有加上b）

从图423可以看出，相应维度的元素数量是相同的。得到的结果y的形

状是(100, 1）。换言之，拥有100个数据的x中的所有数据都分别与W相乘了。

这样我们就能在一次计算中得到所有数据的预测值。这里x的数据维度是1，

即使维度为D，只要将W的形状设置为(D, 1)，依然能进行正确的计算。例如

当D=4时，矩阵乘积的计算如图424所示。

x

(100,4) (100,1) (4,1)

W = y

图424 矩阵乘积的形状的变化（当x数据的维度为4时）

如图424所示，让x.shape[1]和W.shape[0]相同后，矩阵乘积的计算就 ... ...

步骤 42 线性回归  293

能正确进行。在这种情况下，100个数据中的每一个数据都将与W进行向量

内积的计算。

上面代码中的y = F.matmul(x, W) + b在计算过程中会进行一次广播。

具体来说，b的形状是(1, )，在元素被复制成(100, 1)的形状后，程序

对每个元素进行加法运算。我们已在步骤40支持了广播。因此在广播的

情况下，反向传播也会正确进行。

接下来是代码的后半部分，如下所示。

steps/step42.py

def mean_squared_error(x0, x1):

 diff = x0 - x1

 return F.sum(diff ** 2) / len(diff)

lr = 0.1

iters = 100

for i in range(iters):

 y_pred = predict(x)

 loss = mean_squared_error(y, y_pred)

 W.cleargrad()

 b.cleargrad()

 loss.backward()

 W.data -= lr * W.grad.data

 b.data -= lr * b.grad.data

 print(W, b, loss)

上面的代码实现了求均方误差的函数mean_squared_error(x0, x1)。函数

内部只是使用DeZero函数对式子42.1进行了实现。下一步是通过梯度下降

法更新参数，相关实现已经在步骤28中完成了。这里需要注意的是，更新

参数的计算是像W.data -= lr * W.grad.data这样在实例变量data上进行的。

参数的更新只是简单地对数据进行更新，因此不需要创建计算图。

运行上面的代码，从结果可以看出，损失函数的输出值是逐渐减少的。

  第 4阶段　创建神经网络 294

最后得到的值是W = [[2.11807369]]，b = [5.46608905]。作为参考，这里给

出根据这些参数得到的图形，具体如图425所示。

7.5

7.0

6.5

6.0

5.5

0.2 0.4 0.6 0.8 1.0 0.0

x

图425 训练后的模型

如图 425 所示，我们已经得到了一个拟合数据的模型。我们使用

DeZero正确实现了线性回归。以上就是线性回归的实现。最后，笔者对

DeZero的mean_squared_error函数进行补充说明。

y

步骤 42 线性回归  295

42.4 DeZero的mean_squared_error函数（补充内容）

前面我们实现了求均方误差的函数。代码摘录如下。

steps/step42.py

def mean_squared_error(x0, x1):

 diff = x0 - x1

 y = F.sum(diff ** 2) / len(diff)

 return y

这个函数正确地进行了计算。此处是用DeZero函数进行计算的，所以

也能求导。不过，当前实现还有一些地方需要改进。为了方便说明，我们先

看一下图426的计算图。

x0

- **2

x1

sum y /

N

图426 mean_squared_error函数的计算图

图426是由上面的mean_squared_error函数产生的计算图。我们需要关

注的是中间的变量。这里有3个匿名变量。由于这些变量记录在计算图里，

所以只要计算图存在，它们就会一直保存在内存中。这些变量的数据（ndarray

实例）也将一直存在。

DeZero在求导时首先进行正向传播，然后进行反向传播。图426中的变

量（以及它们引用的数据）在正向传播和反向传播期间都保存在内存中。

  第 4阶段　创建神经网络 296

如果内存的使用量不存在问题，那么上面的实现方法也没有问题。不

过这种会被第三方使用的函数有更好的实现方法，即继承Function类进行实

现，也就是实现一个名为MeanSquaredError的DeZero函数类。实际的代码如

下所示。

dezero/functions.py

class MeanSquaredError(Function):

 def forward(self, x0, x1):

 diff = x0 - x1

 y = (diff ** 2).sum() / len(diff)

 return y

 def backward(self, gy):

 x0, x1 = self.inputs

 diff = x0 - x1

 gx0 = gy * diff * (2. / len(diff))

 gx1 = -gx0

 return gx0, gx1

def mean_squared_error(x0, x1):

 return MeanSquaredError()(x0, x1)

首先在正向传播中以ndarray实例为对象。这段代码与之前DeZero版

本的函数中实现的代码几乎相同。然后将反向传播的代码汇总到一起实现

backward方法。反向传播的实现具体来说就是通过式子求导后将其编写成代码。

此处不再赘述。

用新方法实现的mean_squared_error函数能得到与之前的版本相同的结果。

但从内存效率上来说，新的实现方法更好。这是为什么呢？我们看一下新的

mean_squared_error函数的计算图（图427）。

步骤 42 线性回归  297

mean_squared_error

x1

x0

y

图427 新的mean_squared_error函数的计算图

将图427与旧的计算图（图426）比较可知，新的计算图中没有中间变量。

中间的数据只用在MeanSquaredError类的forward方法中。准确来说，它们作

为ndarray实例使用，一旦离开forward方法的作用范围，就马上从内存中被

清除。

出于以上原因，我们使用新的方式实现了dezero/functions.py中的mean_

squared_error函数。为了便于参考，旧的实现方式被命名为mean_squared_

error_simple（在原名后附上了_simple），添加到dezero/functions.py中。以

上就是对DeZero的mean_squared_error函数的补充说明。

  第 4阶段　创建神经网络 298

步骤 43

神经网络

上一个步骤成功实现了线性回归，并让其正确运行。实现线性回归后，

我们就能轻松将其扩展到神经网络了。在本步骤中，我们会修改上一个步

骤的代码，使其“进化”为神经网络。首先把上一个步骤所做的修改实现为

DeZero的linear函数。

43.1 DeZero中的linear函数

上一个步骤以简单的数据集为对象实现了线性回归。线性回归中（除了

损失函数）只执行了矩阵乘积计算和加法运算。代码摘录如下。

y = F.matmul(x, W) + b

上面的代码用来求输入x和参数W之间的矩阵乘积，然后加上b的结

果。这种变换叫作线性变换（linear transformation）或仿射变换（affine 

transformation）。

严格来说，线性变换指的是y = F.matmul(x, W)，其中不包括b。在神

经网络领域，人们通常把包括b的运算称为线性变换（本书也沿用此叫法）。

另外，线性变换对应于神经网络中的全连接层，其中的参数W叫作权重

（weight），参数b叫作偏置（bias）。

步骤 43 神经网络  299

这里我们将上述线性变换实现为linear函数。上一个步骤也提到过，实

现方式有两种：一种是使用已经实现的DeZero函数；另一种是继承Function

类，实现一个名为Linear的新函数类。前面已经说过，后者的内存效率更高。

从图431中可以看出这一点。

图431 线性变换的两种实现方式

x

matmul +

W

t

b

y

x

linear W

b

y

图431左图的实现方式使用了DeZero的matmul函数和+（add函数）。使

用这种方式时，matmul函数的输出作为Variable实例记录在计算图中。也就

是说，在计算图存在期间，Variable实例和它内部的数据（ndarray实例）会

保存在内存中。

图431右图的实现方式是继承Function类后实现Linear类。由于在使用

这种方式的情况下，中间结果没有作为Variable实例存储在内存中，所以正

向传播中使用的数据在正向传播完成后会立即被删除。因此从内存效率的角

度考虑，要想让第三方使用DeZero，我们需要使用第二种实现方式。不过

针对第一种实现方式，有一个可以改善内存效率的技巧。下面笔者来介绍一

下这个技巧。

再次观察图431的左图。matmul函数的输出变量是t。这个变量t是

matmul函数的输出，也是+（add函数）的输入。现在思考一下这两个函数的

反向传播。首先，+的反向传播仅仅传播输出端的梯度。也就是说，+的反向

传播中不需要t的数据。另外，matmul的反向传播只需要输入变量x、W和b。

因此，matmul的反向传播也不需要t的数据。

  第 4阶段　创建神经网络 300

由此我们可以看出，整个反向传播的过程中都不需要变量t的数据。也

就是说，为了传播梯度，计算图中需要变量t，但其数据可以立即删除。基

于以上内容，我们按如下方式实现linear_simple函数。

dezero/functions.py

def linear_simple(x, W, b=None):

 t = matmul(x, W)

 if b is None:

 return t

 y = t + b

 t.data = None # 删除t的数据

 return y

想象一下参数x和W为Variable实例或ndarray实例的情况。如果这些参

数是ndarray实例，那么它们就会在matmul函数（准确来说是在Function类的

__call__方法）中转换为Variable实例。另外，函数也允许省略偏置b，如果

b=None，函数就只会计算矩阵的乘积并返回结果。

如果调用函数时提供了偏置参数，偏置就会被加到结果中。此时，作

为中间结果的t的数据在反向传播中就没有用了。因此，我们可以在计算完

y = t + b后使用t.data = None这行代码将其删除（引用计数变为0，t的数

据被Python解释器删除）。

在神经网络中，大部分内存被作为中间计算结果的张量（ndarray实例）所

占据。特别是在处理大的张量时，ndarray实例会非常大。因此，立即删

除不要的narray实例是理想的做法。在这个例子中，我们手动（通过t.data 

= None）删除了不需要的ndarray实例，但其实这项操作也可以自动化。

例如，Chainer中叫作Aggressive Buffer Release（参考文献[24]）的机制就

可以实现这一点。

以上就是改善内存使用的技巧。上面实现的linear_simple函数被添加

到dezero/functions.py中。另外，继承自Function类的Linear类和linear函

步骤 43 神经网络  301

数也在dezero/functions.py中实现。这些都是很简单的代码，有兴趣的读者

可以自行查看。

43.2 非线性数据集

上一个步骤使用了沿直线分布的数据集。这里通过以下代码创建一个更

复杂的数据集。

steps/step43.py

import numpy as np

np.random.seed(0)

x = np.random.rand(100, 1)

y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)

上面的代码使用sin函数创建数据。这些(x, y)数据点的分布情况如图

432所示。

2.0

1.0

1.5

0.5

−0.5

0.0

−1.0

0.2 0.4 0.6 0.8 1.0 0.0

x

图432 本步骤使用的数据集

y

  第 4阶段　创建神经网络 302

如图432所示，x和y不呈线性关系。对于这样的非线性数据集，当然

不能用线性回归来处理。这时就要用到神经网络了。

43.3 激活函数和神经网络

线性变换指对输入数据进行线性的变换。而神经网络则对线性变换的输

出进行非线性的变换。这种非线性变换叫作激活函数，典型的激活函数有

ReLU和sigmoid函数。

这里使用sigmoid函数作为激活函数。sigmoid函数的式子如式子43.1所

示，其图形如图433所示。

y = 1

1 + exp(−x) (43.1)

图433 sigmoid函数的图形

1.0

0.6

0.8

0.4

0.2

0.0

−2 0 2 4 −4

x

如图433所示，sigmoid函数是非线性函数。这种非线性变换应用于张

量的每个元素。下面使用DeZero实现sigmoid函数。代码如下所示。

y

步骤 43 神经网络  303

dezero/functions.py

def sigmoid_simple(x):

 x = as_variable(x)

 y = 1 / (1 + exp(-x))

 return y

上面的代码直接按照式子进行编码。只要注意使用DeZero的exp函数作

为指数函数，其他地方操作起来就没有什么难度了。下面使用这个sigmoid_

simple函数来实现神经网络。

前面的sigmoid函数的代码在内存效率方面存在问题。更好的实现方式是

实现继承于Function类的Sigmoid类。另外，在使用sigmoid函数的情

况下，以类为计算单位可以提高梯度计算的效率。Sigmoid类和sigmoid

函数的实现在dezero/functions.py中，感兴趣的读者可以查看。另外，关

于sigmoid函数的导数推导过程，请参阅《深度学习入门：基于Python的

理论与实现》的5.5.2节。

43.4 神经网络的实现

通常，神经网络以“线性变换→激活函数→线性变换→激活函数→线性

变换→……”的形式进行一系列的变换。例如，一个2层的神经网络可以用

以下代码实现（这里省略了创建参数的代码）。

W1, b1 = Variable(...), Variable(...)

W2, b2 = Variable(...), Variable(...)

def predict(x):

 y = F.linear(x, W1, b1) # 或者F.linear_simple(...)

 y = F.sigmoid(y) # 或者F.sigmoid_simple(y)

 y = F.linear(y, W2, b2)

 return y

上面的代码依次应用了线性变换和激活函数。这是用于神经网络推理

  第 4阶段　创建神经网络 304

（predict）的代码。当然，为了正确地进行推理，训练是必不可少的。神经网

络在训练过程中，会将损失函数加在推理处理的后面，然后找出使该损失函

数的输出最小的参数。这就是神经网络的训练过程。

在神经网络中，线性变换或基于激活函数的变换称为层（layer）。此外，具

有N个执行线性变换的带有参数的层，能够连续进行变换的网络叫作“N

层神经网络”。

接下来使用实际的数据集来训练神经网络。代码如下所示。

steps/step43.py

import numpy as np

from dezero import Variable

import dezero.functions as F

# 数据集

np.random.seed(0)

x = np.random.rand(100, 1)

y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)

# ①权重的初始化

I, H, O = 1, 10, 1

W1 = Variable(0.01 * np.random.randn(I, H))

b1 = Variable(np.zeros(H))

W2 = Variable(0.01 * np.random.randn(H, O))

b2 = Variable(np.zeros(O))

# ②神经网络的推理

def predict(x):

 y = F.linear(x, W1, b1)

 y = F.sigmoid(y)

 y = F.linear(y, W2, b2)

 return y

lr = 0.2

iters = 10000

# ③神经网络的训练

for i in range(iters):

 y_pred = predict(x)

 loss = F.mean_squared_error(y, y_pred)

步骤 43 神经网络  305

 W1.cleargrad()

 b1.cleargrad()

 W2.cleargrad()

 b2.cleargrad()

 loss.backward()

 W1.data -= lr * W1.grad.data

 b1.data -= lr * b1.grad.data

 W2.data -= lr * W2.grad.data

 b2.data -= lr * b2.grad.data

 if i % 1000 == 0: # 每隔1000次输出一次信息

 print(loss)

上面的代码首先在①处初始化权重。这里的I（=1）对应于输入层的维度，

H（=10）对应于隐藏层的维度，O（=1）对应于输出层的维度。根据此次处理的问题，

我们要将I和O的值设置为1。H是超参数，它可以被设置为大于等于1的任

何整数。另外，偏置被初始化为零向量（np.zeros(...)），权重被初始化为一

个小的随机值（0.01 * np.random.randn(...)）。

神经网络的权重的初始值需要设置为随机数。关于这样做的原因，请参阅

《深度学习入门：基于Python的理论与实现》的6.2.1节。

②处的代码进行神经网络的推理，③处的代码用来更新参数。除参数增

加了之外，③处的代码与上一个步骤的代码完全相同。

执行上面的代码后，神经网络就开始了训练。经过训练后的神经网络预

测出了图434这样的曲线。

  第 4阶段　创建神经网络 306

2.0

1.0

1.5

0.5

−0.5

0.0

−1.0

0.2 0.4 0.6 0.8 1.0 0.0

x

图434 训练后的神经网络

如图434所示，sin函数的曲线很好地拟合了数据。通过在线性回归的

实现中叠加激活函数和线性变换，神经网络也能正确地学习非线性关系。

当然，我们还可以通过这种方式来实现由更深的层组成的神经网络。不

过，随着层数的增加，参数的管理（重置参数的梯度和更新参数的工作）会

变得更加复杂。在下一个步骤，我们将创建一个简化参数管理的机制。

y

步骤 44 汇总参数的层  307

步骤 44

汇总参数的层

上一个步骤使用DeZero实现了神经网络。虽然简单，但它是一个真正

的神经网络。现在我们可以说DeZero是一个神经网络框架了，不过它在易

用性方面仍存在一些问题。接下来，我们将为DeZero增加更多神经网络的

功能。这样可以使神经网络以及深度学习以更简单、更直观的方式实现。

本步骤要解决的问题是参数的处理。在上一个步骤，在重置参数的梯度

（以及更新参数）时，我们不得不写一些相当枯燥的代码。后面我们将实现结

构更加复杂的网络，到时参数处理将更加复杂。

参数是通过梯度下降等优化方法进行更新的变量。以上一个步骤为例，用

于线性变换的权重和偏置就相当于参数。

本步骤将创建汇总参数的机制，为此我们将实现两个类：Parameter类

和Layer类。使用这两个类可以实现参数的自动化管理。

44.1 Parameter类的实现

首先从Parameter类开始说明。Parameter类的功能与Variable类的功能

完全相同。代码如下所示。

  第 4阶段　创建神经网络 308

dezero/core.py

class Parameter(Variable):

 pass

这就是Parameter类。从代码中可以看出，它只继承了Variable类。因此，

它具有与Variable类完全相同的功能。

以上Parameter类的代码在dezero/core.py中。在dezero/__init__.py

中添加代码from dezero.core import Parameter后，使用DeZero的

人就可以通过from dezero import Parameter来引入该类。

Parameter实例和Variable实例具有完全相同的功能。不过，我们可以把

这两个实例区分开来。下面是具体示例。

import numpy as np

from dezero import Variable, Parameter

x = Variable(np.array(1.0))

p = Parameter(np.array(2.0))

y = x * p

print(isinstance(p, Parameter))

print(isinstance(x, Parameter))

print(isinstance(y, Parameter))

运行结果

True

False

False

如上面的代码所示，Parameter实例和Variable实例可以组合在一起进行

计算。isinstance函数可以用来对它们加以区分。利用这一点，我们可以实

现只收集Parameter实例的功能。

步骤 44 汇总参数的层  309

44.2 Layer类的实现

接下来实现Layer类，它与DeZero的Function类相似，都是变换变量的

类，不过二者在持有参数这一点上不同。Layer类是持有参数并使用这些参

数进行变换的类。

Layer类是作为基类实现的，而具体的变换是在继承了Layer类的类中实

现的。例如，对于线性变换，我们将在继承了Layer类的Linear类中实现它。

下面看一下Layer类的实现。首先是初始化操作和__setattr__这一特殊

方法。

dezero/layers.py

from dezero.core import Parameter

class Layer:

 def __init__(self):

 self._params = set()

 def __setattr__(self, name, value):

 if isinstance(value, Parameter):

 self._params.add(name)

 super().__setattr__(name, value)

Layer类持有一个名为_params的实例变量。这个_params保存了Layer实

例所拥有的参数。

_params实例变量的类型是集合。集合与列表不同，它的元素是没有顺序的。

另外，集合不会持有ID相同的对象。

  第 4阶段　创建神经网络 310

__setattr__方法是在设置实例变量时被调用的特殊方法。如果定义了

__setattr__(self, name, value)，那么实例变量的名字会作为name参数、实

例变量的值会作为value参数传给该函数。通过重写这个方法，我们就可以

在添加实例变量时添加一些特殊处理。

这里添加只有当value是Parameter实例时才向self._params增加name

的处理A。这样我们就可以把Layer类的参数汇总到实例变量_params中。代

码如下所示。

layer = Layer()

layer.p1 = Parameter(np.array(1))

layer.p2 = Parameter(np.array(2))

layer.p3 = Variable(np.array(3))

layer.p4 = 'test'

print(layer._params)

print('-------------')

for name in layer._params:

 print(name, layer.__dict__[name])

运行结果

{'p2', 'p1'}

-------------

p2 variable(2)

p1 variable(1)

如上面的代码所示，设置layer的实例变量后，只有引用了Parameter实

例的变量名被添加到了layer._params中。此外，由于所有的实例变量都以

字典的形式存储在实例变量__dict__中，所以我们通过__dict__就可以单独

取出Parameter实例。

接下来向Layer类添加以下4个方法。

A 我们在self._params中增加的不是value而是name。这么做是因为在将参数保存到外部文件时，保

留name更加方便。这项工作将在步骤53中完成。

步骤 44 汇总参数的层  311

dezero/layers.py

import weakref

class Layer:

 ...

 

 def __call__(self, *inputs):

 outputs = self.forward(*inputs)

 if not isinstance(outputs, tuple):

 outputs = (outputs,)

 self.inputs = [weakref.ref(x) for x in inputs]

 self.outputs = [weakref.ref(y) for y in outputs]

 return outputs if len(outputs) > 1 else outputs[0]

 def forward(self, inputs):

 raise NotImplementedError()

 def params(self):

 for name in self._params:

 yield self.__dict__[name]

 def cleargrads(self):

 for param in self.params():

 param.cleargrad()

__call__方法接收输入并调用forward方法。forward方法由继承的类实现。

如果输出只有一个值，那么__call__方法将不返回元组，而是直接返回该值（这

个做法与Function类的实现相同）。另外，考虑到将来的需求，__call__方法

通过弱引用持有输入变量和输出变量。

params方法取出Layer实例所持有的Parameter实例。另外，cleargrads方

法重置所有参数的梯度。这个方法的名称是复数形式，即在cleargrad的后

面加上了s。这么做是为了显式地表明该函数会对Layer拥有的所有参数调用

cleargrad（单数形式）。

params方法使用yield返回值。yield的使用方法与return相同。区别

是return会结束处理并返回值，而yield是暂停处理并返回值。因此，

再次使用yield会恢复处理。以上面的代码为例，每次调用params方法时，

暂停的处理都会重新运行。组合使用yield和for语句，即可按顺序取出

参数。

  第 4阶段　创建神经网络 312

以上就是Layer类的实现。下面继承这个Layer类，实现线性变换等具体

的处理。

44.3 Linear类的实现

接下来实现进行线性变换的Linear类（实现的是作为层的Linear类，而

不是作为函数的Linear类）。这里首先给出简单的Linear类，然后展示改良

版的Linear类。我们来看以下代码。

import numpy as np

import dezero.functions as F

from dezero.core import Parameter

class Linear(Layer):

 def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):

 super().__init__()

 I, O = in_size, out_size

 W_data = np.random.randn(I, O).astype(dtype) * np.sqrt(1 / I)

 self.W = Parameter(W_data, name='W')

 if nobias:

 self.b = None

 else:

 self.b = Parameter(np.zeros(O, dtype=dtype), name='b')

 def forward(self, x):

 y = F.linear(x, self.W, self.b)

 return y

Linear类是在Layer类的基础上实现的。初始化方法__init__(self, in_

size, out_size, nobias)接收输入大小、输出大小和“是否使用偏置”的标志

位作为参数。当nobias为True时，省略偏置。

初始化权重和偏置的做法是向实例变量设置Parameter实例，如self.W = 

Parameter(...)和self.b = Parameter(...)。设置后，这两个参数的Parameter

实例变量名就会被添加到self._params中。

步骤 44 汇总参数的层  313

Linear类的权重初始值需要设置为随机数。上一个步骤将随机的初始值

的量级设置为0.01（0.01 * np.random.randn(...)）。这里将量级设置

为np.sqrt(1/in_size)。这是参考文献[25]中提出的设置初始值的方法。

另外，神经网络的计算也支持32位浮点数，因此，我们使用32位浮点数

作为参数数据的默认设置值。

之后，在forward方法中实现线性变换，只需调用DeZero的linear函数

即可。以上就是Linear类的实现。

前面提到了Linear类有一种更好的实现方法，即延迟创建权重W的时间。

具体做法是在forward方法中创建权重，这样就能自动确定Linear类的输入

大小（in_size）（无须用户指定）。下面是改进版的Linear类的实现。

dezero/layers.py

import numpy as np

import dezero.functions as F

from dezero.core import Parameter

class Linear(Layer):

 def __init__(self, out_size, nobias=False, dtype=np.float32, in_size=None):

 super().__init__()

 self.in_size = in_size

 self.out_size = out_size

 self.dtype = dtype

 self.W = Parameter(None, name='W')

 if self.in_size is not None: # 如果没有指定in_size，则延后处理

 self._init_W()

 if nobias:

 self.b = None

 else:

 self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')

 def _init_W(self):

 I, O = self.in_size, self.out_size

 W_data = np.random.randn(I, O).astype(self.dtype) * np.sqrt(1 / I)

 self.W.data = W_data

 def forward(self, x):

 # 在传播数据时初始化权重

  第 4阶段　创建神经网络 314

 if self.W.data is None:

 self.in_size = x.shape[1]

 self._init_W()

 y = F.linear(x, self.W, self.b)

 return y

这就是改进版的Linear类。这里需要注意的地方是我们不用指定__init__

方法的in_size。in_size参数的默认值为None，在None的情况下，self.W.data

的初始化会被推迟。具体来说，forward(self, x)方法根据输入x的大小创建

权重数据。现在我们只要按照layer = Linear(100)的方式指定输出大小即可。

以上就是Linear类的实现。

44.4 使用 Layer实现神经网络

现在使用Linear类来实现神经网络。这里再次尝试解决在上一个步骤中

已解决的问题，即sin函数的数据集的回归。下面代码中的阴影部分是与前

面代码不同的部分。

steps/step44.py

import numpy as np

from dezero import Variable

import dezero.functions as F

import dezero.layers as L # 作为L引入

# 数据集

np.random.seed(0)

x = np.random.rand(100, 1)

y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)

l1 = L.Linear(10) # 指定输出大小

l2 = L.Linear(1)}

def predict(x):

 y = l1(x)

 y = F.sigmoid(y)

 y = l2(y)

步骤 44 汇总参数的层  315

 return y

lr = 0.2

iters = 10000

for i in range(iters):

 y_pred = predict(x)

 loss = F.mean_squared_error(y, y_pred)

 l1.cleargrads()

 l2.cleargrads()

 loss.backward()

 for l in [l1, l2]:

 for p in l.params():

 p.data -= lr * p.grad.data

 if i % 1000 == 0:

 print(loss)

需要注意的是参数现在由Linear实例管理。这就使得重置参数梯度的处

理和更新参数的操作比之前更清晰了。

不过本步骤对Linear类是逐个进行处理的。今后如果进一步深化网络，

我们将很难一个个地处理Linear类。在下一个步骤中，我们会把多个Layer

合并成一个类来进行管理。

  第 4阶段　创建神经网络 316

步骤 45

汇总层的层

我们在上一个步骤中创建了Layer类。这个类具有管理参数的机制，所

以我们在使用Layer类时，不需要自行处理参数。不过，Layer实例本身是

需要管理的。例如，在实现一个10层的神经网络时，我们必须管理10个

Linear实例（这有点麻烦）。为了减轻工作负担，本步骤将扩展当前的Layer类。

45.1 扩展 Layer类

当前的Layer类可以持有多个Parameter。这里进一步扩展Layer类，使

其也可以持有其他的Layer。它们之间的关系如图451所示。

Layer

Parameter Parameter

Layer

Parameter

Layer

Parameter Parameter

图451 新的Layer类

如图451所示，Layer中可以容纳其他的Layer。也就是说，这是一个嵌

套结构。本步骤的目标是从图451最上层的Layer中取出所有的参数。为此

我们要将当前的Layer类修改成下面这样。

步骤 45 汇总层的层  317

dezero/layers.py

class Layer:

 def __init__(self):

 self._params = set()

 def __setattr__(self, name, value):

 if isinstance(value, (Parameter, Layer)): # ①再增加Layer

 self._params.add(name)

 super().__setattr__(name, value)

 def params(self):

 for name in self._params:

 obj = self.__dict__[name]

 if isinstance(obj, Layer): # ②从Layer取出参数

 yield from obj.params()

 else:

 yield obj

第1个变化是在设置实例变量时将Layer实例的名称也添加到_params。

这样，Parameter和Layer实例的名称就被添加到_params中。

第2个变化发生在取出参数的处理上。params方法从_params中取出name

（字符串），然后根据name将name对应的对象作为obj取出。如果obj是Layer

实例，则继续调用obj.params()。这样就能从Layer的Layer中递归取出参数了。

使用yield的函数叫作生成器。我们可以通过yield from来使用一个生

成器创建另一个新的生成器。yield from是Python 3.3中引入的功能。

这样就完成了新的Layer类。使用这个Layer类可以按如下方式实现神经

网络。

  第 4阶段　创建神经网络 318

import dezero.layers as L

import dezero.functions as F

from dezero import Layer

model = Layer()

model.l1 = L.Linear(5) # 只指定输出大小

model.l2 = L.Linear(3)

# 进行推理的函数

def predict(model, x):

 y = model.l1(x)

 y = F.sigmoid(y)

 y = model.l2(y)

 return y

# 访问所有参数

for p in model.params():

 print(p)

# 重置所有参数的梯度

model.cleargrads()

上面的代码使用model = Layer()创建了实例，然后向model增加了作为实

例变量的Linear实例。这样，执行推理的函数就可以实现为predict(model, x)。

这里重要的一点是我们能够通过model.params()访问model中存在的所有参数。

此外，model.cleargrads()可以重置所有参数的梯度。像这样，我们可以使

用Layer类来统一管理神经网络中使用的所有参数。

除了上面的方法，还有更便捷的方法可以使用Layer类。具体来说，就

是将模型定义为一个继承Layer类的“类”。代码如下所示。

class TwoLayerNet(Layer):

 def __init__(self, hidden_size, out_size):

 super().__init__()

 self.l1 = L.Linear(hidden_size)

 self.l2 = L.Linear(out_size)

 def forward(self, x):

 y = F.sigmoid(self.l1(x))

 y = self.l2(y)

 return y

步骤 45 汇总层的层  319

上面的代码定义了一个类名为TwoLayerNet的模型。该类继承于Layer，

并实现了__init__和forward方法。__init__方法创建了需要使用的Layer，

并使用self.l1 = ...进行了设置。而在forward方法中，我们编写了执行推

理的代码。这样就能将神经网络的代码整合到TwoLayerNet这一个类中。

这里展示的采用面向对象的方式定义模型的做法（也就是把模型以类为

单位进行整合）出自于Chainer框架。这种做法后来常用在 PyTorch和

TensorFlow等框架中。

45.2 Model类

模型或model这种词语已经在前面出现很多次了。模型这个词有“抽象

描述事物本质的结构或系统”的意思，机器学习中使用的模型就是如此。它

用式子抽象描述潜藏着复杂模式或规则的现象。神经网络也是通过式子表示

的函数，我们用模型指代它。

我们要为模型创建一个新的类Model。这个Model类与Layer类具有相同

的功能。我们还会增加一个用于可视化操作的方法，代码如下所示（这些代

码会添加到dezero/models.py中）。

dezero/models.py

from dezero import Layer

from dezero import utils

class Model(Layer):

 def plot(self, *inputs, to_file='model.png'):

 y = self.forward(*inputs)

 return utils.plot_dot_graph(y, verbose=True, to_file=to_file)

如代码所示，Model继承于Layer。因此，Model类的使用方法与我们此前看

到的Layer类的使用方法相同。例如，我们可以编写class TwoLayerNet(Model):

这样的代码。另外，Model类中增加了一个用于可视化操作的plot方法。该

  第 4阶段　创建神经网络 320

方法将 *inputs参数中传来的数据传给 forward方法进行计算，然后将创

建的计算图导出为图像文件。将utils.plot_dot_graph函数的参数设置为

verbose=True后，ndarray实例的形状和类型也会显示在计算图中。

最后，为了简化Model类的导入操作，在dezero/__init__.py中添加下面

一行代码。

from dezero.models import Model

于是，我们就可以写出下面这样的代码。

import numpy as np

from dezero import Variable, Model

import dezero.layers as L

import dezero.functions as F

class TwoLayerNet( Model ):

 def __init__(self, hidden_size, out_size):

 super().__init__()

 self.l1 = L.Linear(hidden_size)

 self.l2 = L.Linear(out_size)

 def forward(self, x):

 y = F.sigmoid(self.l1(x))

 y = self.l2(y)

 return y

x = Variable(np.random.randn(5, 10), name='x')

model = TwoLayerNet(100, 10)

model.plot(x)

如上面的代码所示，Model类的使用方法与我们此前看到的Layer类的使

用方法相同。而且，它有一个用于将计算图可视化的方法。执行上面的代码

可以得到图452这样的计算图。

步骤 45 汇总层的层  321

Linear

Sigmoid

Linear

图452 TwoLayerNet的计算图

45.3 使用Model来解决问题

下面使用Model类再次解决上一个步骤已经解决的问题，即sin函数的数

据集的回归。下面代码中的阴影部分是与上一个步骤中的代码不同的地方。

steps/step45.py

import numpy as np

from dezero import Variable, Model

import dezero.layers as L

import dezero.functions as F

# 创建数据集

np.random.seed(0)

x = np.random.rand(100, 1)

  第 4阶段　创建神经网络 322

y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)

# 设置超参数

lr = 0.2

max_iter = 10000

hidden_size = 10

# 定义模型

class TwoLayerNet(Model):

 def __init__(self, hidden_size, out_size):

 super().__init__()

 self.l1 = L.Linear(hidden_size)

 self.l2 = L.Linear(out_size)

 def forward(self, x):

 y = F.sigmoid(self.l1(x))

 y = self.l2(y)

 return y

model = TwoLayerNet(hidden_size, 1)

# 开始训练

for i in range(max_iter):

 y_pred = model(x)

 loss = F.mean_squared_error(y, y_pred)

 model.cleargrads()

 loss.backward()

 for p in model.params():

 p.data -= lr * p.grad.data

 if i % 1000 == 0:

 print(loss)

上面的代码将神经网络实现为继承于Model类的TwoLayerNet。这样，for

语句中的代码变得更加简洁。所有的参数都可以从Model中访问，重置参数

的梯度也由model.cleargrads()来完成。

现在我们从管理参数的泥潭中解脱出来了。今后不管要构建多么复杂的

网络，我们都可以让Model类（或Layer类）来管理其中使用的参数。本步骤

的主要任务已经完成了。在本步骤的最后，我们来实现一个更通用的神经网

络模型。

步骤 45 汇总层的层  323

45.4 MLP类

刚才我们实现了一个由两个全连接层组成的模型。定义模型的代码摘录

如下。

steps/step45.py

class TwoLayerNet(Model):

 def __init__(self, hidden_size, out_size):

 super().__init__()

 self.l1 = L.Linear(hidden_size)

 self.l2 = L.Linear(out_size)

 def forward(self, x):

 y = F.sigmoid(self.l1(x))

 y = self.l2(y)

 return y

上面的代码在1个类中实现了2层网络。考虑到今后的扩展，我们来实

现一个更通用的全连接层的网络。代码如下所示。

dezero/models.py

import dezero.functions as F

import dezero.layers as L

class MLP(Model):

 def __init__(self, fc_output_sizes, activation=F.sigmoid):

 super().__init__()

 self.activation = activation

 self.layers = []

 for i, out_size in enumerate(fc_output_sizes):

 layer = L.Linear(out_size)

 setattr(self, 'l' + str(i), layer)

 self.layers.append(layer)

 def forward(self, x):

 for l in self.layers[:-1]:

 x = self.activation(l(x))

 return self.layers[-1](x)

  第 4阶段　创建神经网络 324

下面简单介绍一下这段代码。首先在初始化操作中接收fc_output_sizes参数

和 activation参数。这里的 fc是 full connect（全连接）的缩写。fc_output_

sizes可以是元组或列表，用于指定全连接层的输出大小。例如，(10, 1)表

示创建2个Linear层，第1层的输出大小为10，第2层的输出大小为1。(10, 

10, 1)则表示再创建一个Linear层。另外，activation用来指定激活函数（默

认为F.sigmoid函数）。

这里以MLP为类名来实现模型。MLP是MultiLayer Perceptron的缩写，

意思是多层感知器。MLP是全连接层神经网络的别名。

MLP类是前面的TwoLayerNet类经过自然扩展后得到的类。需要注意，这

里使用setattr函数来设置实例变量。之所以使用该函数，是因为我们不能

使用self.l2 = ...这样的代码进行设置。另外，DeZero通过将层设置为模

型的实例变量来对层的参数进行管理。

MLP类的介绍到此结束。有了MLP类，我们可以轻松实现下面的N层网络。

model = MLP((10, 1)) # 2层

model = MLP((10, 20, 30, 40, 1)) # 5层

今后我们也将使用这个通用的MLP类。这里我们把MLP类的代码添加到

dezero/models.py中。以上就是本步骤的内容。

步骤 46 通过Optimizer更新参数  325

步骤 46

通过Optimizer更新参数

我们此前使用了梯度下降法来更新参数。在深度学习领域，除了梯度下

降法，人们还提出了各种优化方法。在本步骤，我们会把参数的更新工作（用

于更新的代码）模块化，并创建一个能轻松更换优化方法的机制。

46.1 Optimizer类

本节把进行参数更新的基础类实现为Optimizer（优化器）类。Optimizer

类是执行优化操作的基类。我们需要在继承了Optimizer的类中实现具体的

优化方法。Optimizer类的实现如下所示。

dezero/optimizers.py

class Optimizer:

 def __init__(self):

 self.target = None

 self.hooks = []

 def setup(self, target):

 self.target = target

 return self

 def update(self):

 # 将None之外的参数汇总到列表

 params = [p for p in self.target.params() if p.grad is not None]

  第 4阶段　创建神经网络 326

 # 预处理（可选）

 for f in self.hooks:

 f(params)

 # 更新参数

 for param in params:

 self.update_one(param)

 def update_one(self, param):

 raise NotImplementedError()

 def add_hook(self, f):

 self.hooks.append(f)

Optimizer类在初始化阶段初始化了两个实例变量，分别是target和hooks。

然后，通过 setup方法将作为类实例（Model或 Layer）的参数变量设置为

target实例变量。

Optimizer类的update方法对除grad实例变量为None的参数之外的其他参

数进行了更新。此外，具体的参数更新通过update_one方法进行。update_

one方法在继承Optimizer的类中通过重写来实现。

此外，Optimizer类还具有在更新参数之前对所有参数进行预处理的功能。

用户可使用add_hook方法来添加进行预处理的函数。这个机制可用于权重衰减、

梯度裁剪（参考文献[26]）等（实现示例在example/mnist.py等文件中）。

46.2 SGD类的实现

现在来实现使用梯度下降法更新参数的类。下面是具体代码。

dezero/optimizers.py

class SGD(Optimizer):

 def __init__(self, lr=0.01):

 super().__init__()

 self.lr = lr

 def update_one(self, param):

 param.data -= self.lr * param.grad.data

步骤 46 通过Optimizer更新参数  327

SGD类继承于Optimizer类，初始化方法__init__接收学习率。之后的update_

one方法中实现了更新参数的代码。这样就可以把参数更新交给SGD类来做

了。另外，SGD类的代码实现在dezero/optimizers.py中。我们可通过from 

dezero.optimizers import SGD从外部文件导入SGD。

SGD是Stochastic Gradient Descent的缩写，即随机梯度下降法。这里的

随机（Stochastic）是指从对象数据中随机选择数据，并对所选数据应用梯

度下降法。在深度学习领域，这种从原始数据中随机选择数据，并对这些

数据应用梯度下降法的做法很常见。

46.3 使用SGD类来解决问题

现在使用SGD类来解决上一个步骤中的问题。同样，下面代码的阴影部

分是与上一个步骤中的代码不同的地方。

steps/step46.py

import numpy as np

from dezero import Variable

from dezero import optimizers

import dezero.functions as F

from dezero.models import MLP

np.random.seed(0)

x = np.random.rand(100, 1)

y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)

lr = 0.2

max_iter = 10000

hidden_size = 10

model = MLP((hidden_size, 1))

optimizer = optimizers.SGD(lr)

optimizer.setup(model)

# 或者使用下一行统一进行设置

# optimizer = optimizers.SGD(lr).setup(model)

for i in range(max_iter):

 y_pred = model(x)

 loss = F.mean_squared_error(y, y_pred)

  第 4阶段　创建神经网络 328

 model.cleargrads()

 loss.backward()

 optimizer.update()

 if i % 1000 == 0:

 print(loss)

上面的代码使用了MLP类来创建模型（上一个步骤中使用的是TwoLayerNet），

然后由SGD类更新参数。更新参数的代码在SGD类中。因此，调用optimizer.

update()即可完成参数更新。

Optimizer类的setup方法将自身作为返回值返回。因此，我们可以编写

my_optimizer = SGD(...) .setup(...)这样的代码，在一行内完成

调用。

46.4 SGD以外的优化方法

基于梯度的优化方法多种多样。比较有代表性的方法有Momentum、

AdaGrad（参考文献[27]）、AdaDelta（参考文献[28]）和Adam（参考文献[29]）等。

引入Optimizer类是为了轻松切换这些不同的优化方法，这里我们通过继承基

类Optimizer来实现其他优化方法。本节将实现Momentum这一优化方法。

AdaGrad、AdaDelta和Adam等优化方法的代码在dezero/optimizers.py

中。本书不对这些方法进行讲解。感兴趣的读者请参考《深度学习入门：

基于Python的理论与实现》的6.1节。

Momentum方法的式子如下所示。

v ← αv − η

L

W (46.1)

W ← W + v (46.2)

∂

∂

步骤 46 通过Optimizer更新参数  329

其中，W是需要更新的权重参数， W

L 是梯度（损失函数L对W的梯度），

η是学习率。另外，v相当于物理学中的速度。式子46.1表示这样一个物理法则：

物体在梯度方向上受到一个力，这个力使物体加速。式子46.2表示物体以该

速度移动位置（参数）。

式子46.1中有一个αv。该项的作用是让物体在没有受任何力的作用时逐

渐减速（可将α设置为0.9这样的值）。

Momentum的代码如下所示，类名为MomentumSGD。

dezero/optimizers.py

import numpy as np

class MomentumSGD(Optimizer):

 def __init__(self, lr=0.01, momentum=0.9):

 super().__init__()

 self.lr = lr

 self.momentum = momentum

 self.vs = {}

 def update_one(self, param):

 v_key = id(param)

 if v_key not in self.vs:

 self.vs[v_key] = np.zeros_like(param.data)

 v = self.vs[v_key]

 v *= self.momentum

 v -= self.lr * param.grad.data

 param.data += v

上面的每个参数都拥有相当于速度的数据。因此，在上面的代码中字典

的实例变量被设置为self.vs。vs在初始化时是空的，但当update_one()第一

次被调用时，它会创建在形状上与参数相同的数据。之后，将式子46.1和式

子46.2用代码实现即可。

∂

∂

  第 4阶段　创建神经网络 330

dezero/optimizers.py中MomentumSGD类的代码与上面的代码有些不同。

dezero/optimizers.py中为了支持GPU，在np.ones_like方法之前根据

数据类型调用了CuPy的cupy.ones_like方法。步骤52中会实现对GPU

的支持。

以上就是Momentum的代码。现在我们可以把之前实现的训练代码

中的优化方法轻松切换到Momentum。只要将optimizer = SGD(lr)替换为

optimizer = MomentumSGD(lr)即可，不需要进行其他任何修改。这样我们就

可以轻松切换各种优化方法了。

步骤 47 softmax函数和交叉熵误差  331

步骤 47

softmax函数和交叉熵误差

此前我们使用神经网络解决了回归问题，现在开始我们要挑战新的问

题——多分类。顾名思义，多分类是将数据分类为多个值的问题。对未分类

的对象，我们推断它属于哪一个类别。本步骤为进行多分类做准备，下一个

步骤将使用DeZero实现多分类。

47.1 用于切片操作的函数

首先要增加一个工具函数，函数的名字是get_item。本节只展示该函数

的使用方法，对具体实现感兴趣的读者可以参考附录B。下面是get_item函

数的使用示例。

import numpy as np

from dezero import Variable

import dezero.functions as F

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

y = F.get_item(x, 1)

print(y)

运行结果

variable([4 5 6])

上面的代码使用get_item函数从Variable的多维数组中提取出一部分

  第 4阶段　创建神经网络 332

元素。这里从形状为(2, 3)的x中提取了第一行的元素。这个函数被实现为

DeZero函数，这意味着它的反向传播也能正确进行。我们可以试着紧跟上

面的代码写出如下代码。

y.backward()

print(x.grad)

运行结果

variable([[0. 0. 0.]

 [1. 1. 1.]])

上面的代码调用y.backward()进行反向传播（此时通过y.grad = Variable 

(np.ones_like(y.data))自动补充梯度）。切片所做的计算是将多维数组中的

一些数据原封不动地传递出去。因此，这个反向传播为多维数组中被提取的

部分设置梯度，并将其余部分设置为0。图471展示了这个过程。

x y

y.grad

get_item

get_item’

1 2 3

4 5 6

4 5 6

0 0 0

1 1 1 1 1 1

x.grad

gx gy

图471 get_item函数的正向传播和反向传播的示例

提取多维数组部分元素的操作叫作切片（slice）。在Python中，我们可以

通过编写x[1]或x[1:4]这样的代码对列表或元组执行切片操作。

步骤 47 softmax函数和交叉熵误差  333

我们也可以使用get_item函数多次提取同一组元素，代码如下所示。

x = Variable(np.array([[1, 2, 3], [4, 5, 6]]))

indices = np.array([0, 0, 1])

y = F.get_item(x, indices)

print(y)

运行结果

variable([[1 2 3]

 [1 2 3]

 [4 5 6]])

以上就是对DeZero的get_item函数的介绍。接下来进行设置，使get_

item函数也可以作为Variable的方法使用。代码如下所示。

Variable.__getitem__ = F.get_item

y = x[1]

print(y)

y = x[:,2]

print(y)

运行结果

variable([4 5 6])

variable([3 6])

上面用于设置的代码是Variable.__getitem__= get_item。用x[1]或x[:,2]

等写法编写的代码在运行时会调用get_item函数，该切片操作的反向传播也

能正确进行。这个特殊方法的设置在dezero/core.py的setup_variable函数

中被调用（setup_variable函数是DeZero在初始化时调用的函数）。这样就能

对Variable实例自由地进行切片操作了。下面开始本步骤的主要内容。

  第 4阶段　创建神经网络 334

47.2 softmax函数

当使用神经网络进行多分类时，我们可以直接使用之前在线性回归中使

用的神经网络。此前在MLP类中实现了神经网络，这意味着我们可以直接

使用它。例如，对于输入数据的维度是2，需要将数据分为3类的问题，我

们可以写出以下代码。

steps/step47.py

from dezero.models import MLP

model = MLP((10, 3))

上面的代码通过MLP((10, 3))创建了一个2层的全连接网络。第1个全

连接层的输出大小为10，第2个全连接层的输出大小为3。由此得到的model

会把输入数据变换为三维向量（有3个元素的向量）。输入一组数据后，代码

如下所示。

steps/step47.py

x = np.array([[0.2, -0.4]])

y = model(x)

print(y)

运行结果

variable([[-0.6150578 -0.42790162 0.31733288]])

上面代码中的x的形状是(1, 2)。该形状表示1个样本数据中有2个元

素（＝二维向量）。神经网络的输出的形状是(1, 3)，它意味着1个样本数据

会变换为3个元素（＝三维向量）。这个三维向量的每个元素都对应一个类别，

元素值最大的索引就是模型分类的类别。在上面的例子中，（在第0个、第1

个和第2个元素中）第2个元素的值最大，为0.31733288，这表示模型将数据

分到了第2个类中。

步骤 47 softmax函数和交叉熵误差  335

虽然上面的示例代码中只有一个输入数据，但模型也支持一次性处理多个数据。

如果x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 

0.3]])，那么4个输入数据将被合并为一组数据，y = model(x)会一次

性处理4个输入数据。具体来说，y.shape为(4, 3)，其中第i个输入数

据是x[i]，相应的输出是y[i]（i=0,1,2,3）。

这里展示的代码示例中，神经网络的输出是数值。这个数值也能够转换

为概率。进行这种转换的是softmax函数。softmax函数的式子如下所示。

pk = exp(yk)

 n

i=1 exp(yi) (47.1)

假设向softmax函数输入的数据yk共有n个（这个n是类的数量）。式子

47.1是求第k个输出pk的式子。softmax函数的分子是输入yk的指数函数，分

母是所有输入的指数函数之和，因此有0 ≤ pi ≤ 1和p1 + p2 + ··· + pn = 1。

换言之，(p1, p2,··· pn)可以解释为一种概率。

下面在DeZero中实现softmax函数。首先实现输入数据只有一个（一个

样本数据）时的softmax函数。代码如下所示。

steps/step47.py

from dezero import Variable, as_variable

import dezero.functions as F

def softmax1d(x):

 x = as_variable(x)

 y = F.exp(x)

 sum_y = F.sum(y)

 return y / sum_y

函数内部的实现仅仅是使用DeZero函数对式子47.1进行编码（假定

DeZero的Exp类和exp函数已经添加到functions.py中）。第一行的x = as_

variable(x)确保ndarray实例x被转换为Variable实例。

  第 4阶段　创建神经网络 336

在上面代码y/sum_y的计算中，由于y和sum_y的形状不同，所以通过广

播使二者形状匹配。我们已经在DeZero中支持了广播。因此，在使用广

播的情况下，反向传播也会正确进行。

下面我们来实际使用一下softmax1d函数。

steps/step47.py

x = Variable(np.array([[0.2, -0.4]]))

y = model(x)

p = softmax1d(y)

print(y)

print(p)

运行结果

variable([[-0.61505778 -0.42790161 0.31733289]])

variable([[0.21068638 0.25404893 0.53526469]])

结果p的每个元素都是0和1之间的值，它们的总和是1。这样就成功地

将神经网络的输出变换为概率了。

由于softmax函数的计算中有指数函数的计算，其值容易变大（或变小），

所以，在实现softmax函数时，我们通常会采取防止溢出的措施。本书不

对这部分内容进行介绍。关于如何更好地实现softmax函数，可以参阅《深

度学习入门：基于Python的理论与实现》的3.5.2节。

下面对softmax函数进行扩展，使其能够批量处理数据，比如图472中

的将softmax函数应用于每个样本数据的情况。

[[-0.615, -0.427, 0.317]

 [-0.763, -0.249, 0.185]

 [-0.520, -0.962, 0.578]

 [-0.942, -0.503, 0.175]]

[[0.210, 0.254, 0.535]

 [0.190, 0.318, 0.491]

 [0.215, 0.138, 0.646]

 [0.178, 0.276, 0.545]]

softmax

图472 对二维数据应用softmax函数的例子

步骤 47 softmax函数和交叉熵误差  337

我们要实现上图这种能应用于批量数据的softmax函数，其实现如下

所示A。

dezero/functions.py

def softmax_simple(x, axis=1):

 x = as_variable(x)

 y = exp(x)

 sum_y = sum(y, axis=axis, keepdims=True)

 return y / sum_y

假设参数x为二维数据，参数axis指定了softmax函数应用于哪个轴。如

果axis=1，则像图472那样使用softmax函数。求和时的参数keepdims=True，

这意味着对每一行都执行式子47.1的除法运算。

这里实现的softmax_simple函数只使用了DeZero函数。虽然它能输出

正确的结果，但也有可改进的地方。更好的实现方式是实现继承Function

类的 Softmax类，然后实现 softmax函数，这里不再赘述。代码在 dezero/

functions.py中，感兴趣的读者可以参考。

47.3 交叉熵误差

在线性回归中，我们使用均方误差作为损失函数，但在进行多分类时，

需要使用专用的损失函数。最常用的是交叉熵误差（cross entropy error）。

交叉熵误差的式子如下所示。

L = −

k

tk log pk (47.2)

式子中的tk表示训练数据的第k个维度的值。这个训练数据的元素值的

记录规则为，类别正确的元素值是1，其他为0。这种表示方法叫作onehot向量。

式子中的pk是使用了神经网络的softmax函数后的输出。

A 这段代码在dezero/functions.py中。由于exp函数和sum函数的代码都在dezero/functions.py中，所

以DeZero函数在调用exp时，只需使用exp()即可，无须使用F.exp()。

  第 4阶段　创建神经网络 338

交叉熵误差的式子 47.2 有更简化的表达形式。例如有 t = (0, 0, 1)，

p = (p0, p1, p2)，把它们代入式子47.2，得到L = −log p2。这意味着交叉熵

误差也可以通过提取正确类别编号的概率p来进行计算。因此，假设训练数

据中正确类别的编号为t，我们也可以通过下面的式子计算交叉熵误差。

L = −log p[t] (47.3)

式子中的p[t]表示只从向量p中提取第t个元素。这个切片操作是本步

骤开头向DeZero中添加的功能。

此处介绍的交叉熵误差针对的是数据只有一个的情况。如果有N个数据，

我们应计算每个数据的交叉熵误差，将它们相加，然后除以N，由此求出

平均交叉熵误差。

下面实现交叉熵误差。我们将softmax函数和交叉熵误差合二为一，实

现softmax_cross_entropy_simple(x, t)函数。代码如下所示。

dezero/functions.py

def softmax_cross_entropy_simple(x, t):

 x, t = as_variable(x), as_variable(t)

 N = x.shape[0]

 p = softmax(x) # 或者softmax_simple(x)

 p = clip(p, 1e-15, 1.0) # 为了防止log(0)，将p设为大于1e-15的值

 log_p = log(p) # 这个log是DeZero函数

 tlog_p = log_p[np.arange(N), t.data]

 y = -1 * sum(tlog_p) / N

 return y

上面代码中的输入参数x是使用神经网络的softmax函数之前的输出，t

是训练数据。假设训练数据是正确类别的编号（标签，非onehot向量）。

代码中p = softmax(x)的p元素是0和1之间的值。在下一步进行log计

算时，向log函数输入0会导致错误发生（准确来说是警告）。为了防止这种

步骤 47 softmax函数和交叉熵误差  339

情况出现，在输入为0的情况下，我们用一个较小的值1e-15来代替它。这

个替换是由clip函数完成的。clip函数的用法是clip(x, x_min, x_max)。调

用该函数时，如果 x的元素（Variable实例）小于 x_min，其值会被替换为

x_min；如果大于x_max，其值会被替换为x_max。这里就不介绍clip函数的

实现了（代码在dezero/functions.py中）。

另外，上面的代码通过np.range(N)创建了[0, 1, ..., N-1]的ndarray

实例。使用log_p[np.arange(N), t.data]可提取出对应于训练数据的模型输

出log_p[0, t.data[0]]、log_p[1, t.data[1]]……这是一个一维数组。

上面的softmax_cross_entropy_simple函数的实现比较简单。不过，

dezero/functions.py中的softmax_cross_entropy函数是更好的实现方

式。为了帮助大家理解，本步骤采用了简单的实现方式。

下面对进行多分类的神经网络使用具体数据来计算交叉熵误差。

x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]])

t = np.array([2, 0, 1, 0])

y = model(x)

loss = F.softmax_cross_entropy_simple(y, t)

# 或者F.softmax_cross_entropy(y, t)

print(loss)

运行结果

variable(1.4967442524053058)

上面的代码首先准备了输入数据x、训练数据t，训练数据中记录了正确

类别的编号，然后用y = model(x)来变换数据，用F.softmax_cross_entropy_

simple(y, t)来计算损失函数。现在我们已经做好实现多分类的准备了，下

一步将实际进行操作。

  第 4阶段　创建神经网络 340

步骤 48

多分类

上一个步骤实现了softmax函数和交叉熵误差，到这里我们就做好了实

现多分类的准备工作。本步骤将使用一个名为螺旋数据集的小型数据集来进

行多分类。首先了解一下这个螺旋数据集。

48.1 螺旋数据集

DeZero有一个模块（文件）叫dezero/datasets.py。该模块包含与数据集

有关的类和函数，它还内置了一些典型的机器学习的数据集。这里使用函数

get_spiral加载其中的螺旋数据集。下面是一个简单的使用示例。

import dezero

x, t = dezero.datasets.get_spiral(train=True)

print(x.shape)

print(t.shape)

print(x[10], t[10])

print(x[110], t[110])

运行结果

(300, 2)

(300,)

[0.05984409 0.0801167 ] 0

[-0.08959206 -0.04442143] 1

步骤 48 多分类  341

get_spiral函数从参数获取标志位train。如果train=True，则返回训练

数据；如果train=False，则返回测试数据。实际返回的值是x和t，x是输入

数据，t是训练数据（标签）。这里的x是形状为(300, 2)的ndarray实例，t是

形状为(300,)的ndarray实例。这里处理的是3类分类问题，t的元素值是0、1、

2其中之一。图481展示了呈螺旋状分布的数据集。

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00

x0

图481 呈螺旋状分布的数据集

图481使用了○、△和×这3种不同的符号来绘制每个类别的数据点。

从图中可以看出这是一个呈螺旋状分布的数据集。下面我们使用神经网络来

看看能否对这些数据正确地进行分类。

48.2 用于训练的代码

下面是进行多分类的代码。由于代码量很大，这里笔者把它分成前后两

部分来介绍，首先是代码的前半部分。

x1

  第 4阶段　创建神经网络 342

steps/step48.py

import math

import numpy as np

import dezero

from dezero import optimizers

import dezero.functions as F

from dezero.models import MLP

# ①设置超参数

max_epoch = 300

batch_size = 30

hidden_size = 10

lr = 1.0

# ②读入数据 / 创建模型和Optimizer

x, t = dezero.datasets.get_spiral(train=True)

model = MLP((hidden_size, 3))

optimizer = optimizers.SGD(lr).setup(model)

上面的代码与此前我们见过的代码基本相同。首先在代码①处设置超参

数。超参数是由人决定的参数，中间层的数量和学习率就属于此类。然后在

②处加载数据集并创建模型和Optimizer。

上面的代码设置max_epoch = 300。轮（epoch）是训练单位。使用完所有

事先准备的数据（“看过”所有数据）为1轮。代码中还有batch_size = 30，

它表示一次处理30个数据。

这里要处理的数据总共有300条，比前面示例中的数据都多。在实际的工

作中，要处理的数据通常会更多。在这种情况下，我们可以随机抽取一部

分数据进行处理，而不是一次性处理所有的数据。这种部分数据的集合叫

作小批量（mini batch）。

代码的后半部分如下所示。

steps/step48.py

data_size = len(x)

max_iter = math.ceil(data_size / batch_size) # 小数点向上取整

步骤 48 多分类  343

for epoch in range(max_epoch):

 # ③数据集索引重排

 index = np.random.permutation(data_size)

 sum_loss = 0

 for i in range(max_iter):

 # ④创建小批量数据

 batch_index = index[i * batch_size:(i + 1) * batch_size]

 batch_x = x[batch_index]

 batch_t = t[batch_index]

 # ⑤算出梯度 / 更新参数

 y = model(batch_x)

 loss = F.softmax_cross_entropy(y, batch_t)

 model.cleargrads()

 loss.backward()

 optimizer.update()

 sum_loss += float(loss.data) * len(batch_t)

 # ⑥输出每轮的训练情况

 avg_loss = sum_loss / data_size

 print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))

代码③处使用np.random.permutation函数随机重新排列数据集的索引。

如果调用np.random.permutation(N)，那么这个函数将输出一个从0到N − 1的

随机排列的整数列表。上面的代码在每轮训练时都调用index = np.random.

permutation(data_size)，重新创建随机排列的索引列表。

代码④处创建小批量数据。小批量数据的索引（batch_index）是从之前

创建的index中按顺序从头开始取出的。DeZero函数需要Variable或ndarray

实例作为输入。上面的例子中小批量数据的batch_x和batch_t都是ndarray

实例。当然，通过Variable(batch_x)显式地将它们转换为Variable后，计算

仍然会正确进行。

代码⑤处像往常一样求梯度，更新参数。代码⑥处记录每轮损失函数的

结果。以上就是用于训练螺旋数据集的代码。

现在运行上面的代码。从结果可知，损失（loss）正在稳步减少。结果如

图482所示。

  第 4阶段　创建神经网络 344

1.0

0.8

0.6

0.4

0.2

0 50

epoch

100 150 200 250 300

图482 损失的图像（横轴为轮，纵轴为每轮的平均损失）

如图482所示，随着训练的推进，损失逐渐减少。我们的神经网络看上去

正在沿着正确的方向进行训练。下面对训练后的神经网络生成了什么样的分离

区域，即决策边界（decision boundary）进行可视化操作。结果如图483所示。

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00

x0

图483 训练后神经网络的决策边界 loss x1

步骤 48 多分类  345

如图483所示，训练后的神经网络准确地识别出了“螺旋”模式。也就

是说，它能够学习到非线性的分离区域。这说明神经网络能够通过隐藏层表

示复杂的东西。通过增加更多的层来丰富表现力正是深度学习的一大特征。

  第 4阶段　创建神经网络 346

步骤 49

Dataset类和预处理

上一个步骤使用螺旋数据集进行了多分类。当时我们使用代码x, t = 

dezero.datasets.get_spiral()加载了数据。读取到的x和t是ndarray实例，

x的形状是(300, 2)，t的形状是(300,)。换言之，我们在一个ndarray实例

中保存了300条数据。

螺旋数据集是一个大约有300条数据的小数据集，所以我们能够把它当

作一个ndarray实例进行处理。但是当我们处理大型数据集时，比如一个由

100万个元素构成的数据集，这种数据形式就会出现问题。这是因为如果处

理的是一个巨大的ndarray实例，我们必须将所有的元素都保存到内存中。

为了解决此类问题，本步骤将创建一个数据集专用的Dataset类，之后在

Dataset类中提供数据预处理的机制。

49.1 Dataset类的实现

Dataset类是作为基类实现的。我们让用户实际使用的数据集类继承Dataset

类。Dataset类的代码如下所示。

dezero/datasets.py

import numpy as np

class Dataset:

步骤 49 Dataset类和预处理  347

 def __init__(self, train=True):

 self.train = train

 self.data = None

 self.label = None

 self.prepare()

 def __getitem__(self, index):

 assert np.isscalar(index) # 只支持index是整数（标量）的情况

 if self.label is None:

 return self.data[index], None

 else:

 return self.data[index], self.label[index]

 def __len__(self):

 return len(self.data)

 def prepare(self):

 pass

首先，初始化方法接收train参数。它是用于区分“训练”和“测试”的

标志位。另外，Dataset类由保存输入数据的实例变量data和保存标签的实

例变量label构成。之后调用的prepare方法用于准备数据，用户需要在继承

了Dataset的类中实现这个方法。

Dataset类中最重要的是__getitem__和__len__这两个方法。拥有这两个

方法（接口）是DeZero数据集的要求。只要固定了接口，我们就可以切换使

用各种数据集。

__getitem__是一个特殊的Python方法，它定义了通过方括号访问元素（如

x[0]和x[1]等）时的操作。Dataset类的__getitem__方法仅用来取出指定索

引处的数据。如果没有标签数据，它将返回输入数据self.data[index]和标

签数据None（这是无监督学习的情况）。另外，__len__方法在使用len函数时

被调用（如len(x)），它用于查看数据集的长度。

除了int类型，__getitem__方法原本还支持切片参数。切片是指x[1:3]

这样的操作。不过DeZero的Dataset类不支持切片操作，只支持int类

型的索引。

  第 4阶段　创建神经网络 348

以上就是Dataset类的代码。下面扩展Dataset类以实现螺旋数据集类。

代码如下所示，类名为Spiral。

dezero/datasets.py

class Spiral(Dataset):

 def prepare(self):

 self.data, self.label = get_spiral(self.train)

上面代码的prepare方法仅仅将数据设置为实例变量data和label。现在

我们可以像下面这样使用Spiral类来取出数据了，还可以得到数据的长度。

import dezero

train_set = dezero.datasets.Spiral(train=True)

print(train_set[0])

print(len(train_set))

运行结果

(array([-0.13981389, -0.00721657], dtype=float32), 1)

300

上面的代码通过train_set[0]访问数据，第0个输入数据和标签会以元

组的形式返回。

49.2 大型数据集的情况

对于螺旋数据集这样的小型数据集，我们可以直接将ndarray实例保存

在Dataset类的实例变量data和label中。但如果数据集很大，就不能使用这

种实现方式了。对于这种情况，我们可以采取以下实现方式。

class BigData(Dataset):

 def __getitem__(index):

 x = np.load('data/{}.npy'.format(index))

 t = np.load('label/{}.npy'.format(index))

步骤 49 Dataset类和预处理  349

 return x, t

 def __len__():

 return 1000000

上面的示例假定data目录和label目录内分别存储了100万个数据（标签）。

在这种情况下，我们不在BigData类初始化时加载数据，而是在访问数据时

加载数据。具体来说，就是在调用__getitem__(index)时加载data目录中的

数据（步骤53将介绍np.load函数）。这里再强调一次，DeZero的数据集类需

要满足的要求是实现__getitem__和__len__这两个方法，上面的BigData类也

满足了这个要求。因此，BigData类的使用方式与Spiral类的使用方式相同。

下面使用Spiral类编写训练代码。此时需要“连接”数据。

49.3 数据的连接

训练神经网络时会从数据集中取出一部分数据作为小批量数据。下面是

使用Spiral类取出小批量数据的代码。

train_set = dezero.datasets.Spiral()

batch_index = [0, 1, 2] # 取出第0个～第2个数据

batch = [train_set[i] for i in batch_index]

# batch = [(data_0, label_0), (data_1, label_1), (data_2, label_2)]

上面的代码首先通过索引操作提取多个数据（小批量数据）。代码中的

batch是由多个数据组成的列表。要作为DeZero的神经网络的输入，我们还

需将这些数据转换为ndarray实例。下面是用于转换的代码。

x = np.array([example[0] for example in batch])

t = np.array([example[1] for example in batch])

print(x.shape)

print(t.shape)

  第 4阶段　创建神经网络 350

运行结果

(3, 2)

(3,)

上面的代码从batch的每个元素中提取数据（或标签），并将它们转换（连

接）为一个ndarray实例。这样，这些数据就可以作为神经网络的输入使用了。

49.4 用于训练的代码

下面使用Spiral类进行训练。代码如下所示（这里省略了Python的导入

代码，阴影部分是与上一个步骤的代码不同的部分）。

steps/step49.py

max_epoch = 300

batch_size = 30

hidden_size = 10

lr = 1.0

train_set = dezero.datasets.Spiral()

model = MLP((hidden_size, 3))

optimizer = optimizers.SGD(lr).setup(model)

data_size = len(train_set)

max_iter = math.ceil(data_size / batch_size)

for epoch in range(max_epoch):

 index = np.random.permutation(data_size)

 sum_loss = 0

 for i in range(max_iter):

 # 取出小批量数据

 batch_index = index[i * batch_size:(i + 1) * batch_size]

 batch = [train_set[i] for i in batch_index]

 batch_x = np.array([example[0] for example in batch])

 batch_t = np.array([example[1] for example in batch])

 y = model(batch_x)

 loss = F.softmax_cross_entropy(y, batch_t)

 model.cleargrads()

 loss.backward()

步骤 49 Dataset类和预处理  351

 optimizer.update()

 sum_loss += float(loss.data) * len(batch_t)

 # Print loss every epoch

 avg_loss = sum_loss / data_size

 print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))

运行结果

epoch 1, loss 1.35

epoch 2, loss 1.06

epoch 3, loss 0.98

epoch 4, loss 0.90

...

与上一个步骤不同的地方是这里使用了Spiral类，创建小批量数据的代

码也得到了相应的修改，其余的代码与上一个步骤中的代码相同。运行上面

的代码，可以看到结果和之前一样，损失（loss）逐渐减少。

这样我们就使用Dataset类训练了神经网络。当改用其他数据集进行训

练时，我们会体会到使用Dataset类的好处。例如，当BigData类符合DeZero

数据集的要求时，我们只要将上面代码中的Spiral改为BigData，即可运行

新的代码。确定数据集的接口后，我们就能以相同的方式处理各种数据集了。

最后为现在的Dataset类添加一些预处理功能。

49.5 数据集的预处理

在向机器学习的模型输入数据之前，通常要对数据进行一定的处理，比

如从数据中减去某个值，或者改变数据的形状。常见的处理还有数据增强，

即通过旋转或翻转图像等方式来人为地增加数据。为了支持这些预处理（以

及数据增强），我们向Dataset类中添加以下实现预处理功能的代码。

  第 4阶段　创建神经网络 352

dezero/datasets.py

class Dataset:

 def __init__(self, train=True, transform=None, target_transform=None ):

 self.train = train

 self.transform = transform

 self.target_transform = target_transform

 if self.transform is None:

 self.transform = lambda x: x

 if self.target_transform is None:

 self.target_transform = lambda x: x

 self.data = None

 self.label = None

 self.prepare()

 def __getitem__(self, index):

 assert np.isscalar(index)

 if self.label is None:

 return self.transform(self.data[index]) , None

 else:

 return self.transform(self.data[index]),\ ,\

 self.target_transform(self.label[index])

 def __len__(self):

 return len(self.data)

 def prepare(self):

 pass

上面的代码在初始化阶段接收transform和target_transform等新的参数。

这些参数是可调用的对象（如Python的函数等）。transform对单个输入数据

进行转换处理，target_transform对单个标签进行转换处理。如果传来的参

数是None，预处理则会被设置为lambda x:x，lambda表达式可以直接返回参

数（即不做预处理）。有了设置预处理的功能后，我们可以写出以下代码。

def f(x):

 y = x / 2.0

 return y

train_set = dezero.datasets.Spiral(transform=f)

步骤 49 Dataset类和预处理  353

上面的示例代码针对输入数据执行了将其缩放为一半的预处理。用户可

以像这样对数据集添加任何预处理。DeZero在dezero/transforms.py中内置

了常用的预处理转换，比如数据正则化处理及与图像数据（PIL.Image实例）

相关的转换处理。下面是一个实际的使用示例。

from dezero import transforms

f = transforms.Normalize(mean=0.0, std=2.0)

train_set = dezero.datasets.Spiral(transforms=f)

如果输入是x，那么上面代码中的transform.Normalize(mean=0.0, std=2.0)

就会将x转换为(x - mean) / std。如果想连续进行多个转换，可以编写以下

代码。

f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0),

 transforms.AsType(np.float64)])

transform.Compose类按顺序从头开始处理列表中的转换。上面的代码

首先进行数据的正则化处理，然后将数据类型转换为np.float64。dezero/

transform.py中内置了许多有用的转换处理，代码都很简单，这里不再赘述，

感兴趣的读者可以查看。

  第 4阶段　创建神经网络 354

步骤 50

用于取出小批量数据的

DataLoader

上一个步骤创建了Dataset类，并建立了通过指定的接口访问数据集的

机制。本步骤将实现Dataset类中创建小批量数据的DataLoader（数据加载器）

类，让这个类来完成创建小批量数据和数据集重排等工作。由此，用户编写

的训练代码会变得更加简洁。这里笔者先介绍迭代器（iterator），然后介绍

如何实现DataLoader类。

50.1 什么是迭代器

顾名思义，迭代器可以重复地（迭代）提取元素。Python的迭代器提供

了从列表和元组等具有多个元素的数据类型中依次取出数据的功能，具体示

例如下。

>>> t = [1, 2, 3]

>>> x = iter(t)

>>> next(x)

1

>>> next(x)

2

>>> next(x)

3

>>> next(x)

Traceback (most recent call last):

 File "<stdin>", line 1, in <module>

StopIteration

步骤 50 用于取出小批量数据的 DataLoader  355

我们可以使用iter函数将列表转换成迭代器。上面的代码基于列表t创

建了迭代器x。next函数用于从迭代器中按顺序取出数据。在上面的例子中，

每次执行next函数，函数都会依次取出列表中的元素。在第4次执行next时，

由于下一个元素不存在，所以出现了StopIteration异常。

使用for语句从列表中取出元素时，其内部（用户看不见的地方）使用了迭

代器功能。假设有t = [1, 2, 3]，那么在运行for x in t: x时，列

表t会在语句内部转换为迭代器。

我们也可以创建一个Python迭代器。以下是自制的迭代器代码。

class MyIterator:

 def __init__(self, max_cnt):

 self.max_cnt = max_cnt

 self.cnt = 0

 def __iter__(self):

 return self

 def __next__(self):

 if self.cnt == self.max_cnt:

 raise StopIteration()

 self.cnt += 1

 return self.cnt

上面是MyIterator类的代码。为了使这个类作为Python的迭代器使用，

我们实现了特殊方法__iter__，它会返回自身（self）。然后实现了特殊方

法__next__，它将返回下一个元素。如果没有要返回的元素，则执行raise 

StopIteration()。这样，MyIterator的实例就可以作为迭代器使用了。下面

是它的使用示例。

  第 4阶段　创建神经网络 356

obj = MyIterator(5)

for x in obj:

 print(x)

运行结果

1

2

3

4

5

上面的代码使用for x in obj:语句取出了元素。下面利用迭代器的机制，

实现用于取出DeZero的小批量数据的DataLoader类。这个类从给定的数据

集中按顺序从头开始取出数据，并根据需要重排数据集。DataLoader的代码

如下所示。

dezero/dataloaders.py

import math

import random

import numpy as np

class DataLoader:

 def __init__(self, dataset, batch_size, shuffle=True):

 self.dataset = dataset

 self.batch_size = batch_size

 self.shuffle = shuffle

 self.data_size = len(dataset)

 self.max_iter = math.ceil(self.data_size / batch_size)

 self.reset()

 def reset(self):

 self.iteration = 0

 if self.shuffle:

 self.index = np.random.permutation(len(self.dataset))

 else:

 self.index = np.arange(len(self.dataset))

 def __iter__(self):

 return self

步骤 50 用于取出小批量数据的 DataLoader  357

 def __next__(self):

 if self.iteration >= self.max_iter:

 self.reset()

 raise StopIteration

 i, batch_size = self.iteration, self.batch_size

 batch_index = self.index[i * batch_size:(i + 1) * batch_size]

 batch = [self.dataset[i] for i in batch_index]

 x = np.array([example[0] for example in batch])

 t = np.array([example[1] for example in batch])

 self.iteration += 1

 return x, t

 def next(self):

 return self.__next__()

这个类在初始化时接收以下参数。

 dataset：具有Dataset接口的实例A

 batch_size: 小批量数据的大小

 shuffle：在每轮训练时是否对数据集进行重排

初始化方法在将参数设置为实例变量的参数之后调用了reset方法。reset

方法将iteration实例变量的次数设置为0，并根据需要重排数据的索引。

__next__方法取出小批量数据，并将其转换为ndarray实例。它的代码与

前面编写的代码相同，这里不再赘述。

dezero/dataloaders.py中的DataLoader类的代码还包括向GPU传输数据

的机制。上面的代码省略了支持GPU的代码。步骤52将介绍相关内容。

最后，在 dezero/__init__.py中添加导入语句 from dezero.dataloaders 

A“具有Dataset接口的实例”指的是实现了__getitem__和__len__方法的类的实例。

  第 4阶段　创建神经网络 358

import DataLoader。这样，用户就可以通过 from dezero import DataLoader

导入DataLoader（不必编写from dezero.dataloaders import DataLoader）。

50.2 使用 DataLoader

现在我们来用一下DataLoader类。使用这个DataLoader类可以轻松取出

小批量数据。下面是在神经网络训练的场景使用DataLoader的示例，代码如

下所示。

from dezero.datasets import Spiral

from dezero import DataLoader

batch_size = 10

max_epoch = 1

train_set = Spiral(train=True)

test_set = Spiral(train=False)

train_loader = DataLoader(train_set, batch_size)

test_loader = DataLoader(test_set, batch_size, shuffle=False)

for epoch in range(max_epoch):

 for x, t in train_loader:

 print(x.shape, t.shape) # x、t是训练数据

 break

 # 在每轮训练结束时取出测试数据

 for x, t in test_loader:

 print(x.shape, t.shape) # x、t是测试数据

 break

运行结果

(10, 2) (10,)

(10, 2) (10,)

上面的代码创建了两个DataLoader，分别用于训练和测试。由于用于训练

的DataLoader在每轮训练时要对数据进行重排，所以被设置为shuffle=True（默

认）；用于测试的DataLoader只用于精度评估，所以被设置为shuffle=False。

设置好之后，DataLoader就会进行小批量数据的取出和重排工作。

步骤 50 用于取出小批量数据的 DataLoader  359

接下来，使用DataLoader类训练螺旋数据集。不过在此之前，我们再

添加一个工具函数。

50.3 accuracy函数的实现

这里添加一个用于评估识别精度的函数accuracy。代码如下所示。

dezero/functions.py

def accuracy(y, t):

 y, t = as_variable(y), as_variable(t)

 pred = y.data.argmax(axis=1).reshape(t.shape)

 result = (pred == t.data)

 acc = result.mean()

 return Variable(as_array(acc))

accuracy函数用于计算参数y相对于t的“正确率”。其中，y是神经网络

的预测结果，t是正确答案的数据。这两个参数是Variable实例或ndarray实例。

函数内部首先求出神经网络的预测结果pred。为此需要找出神经网络预

测结果最大的索引，并进行reshape。然后将pred与正确答案的数据t进行比

较，结果是True/False张量（ndarray）。计算张量为True的数据所占的比例（求

平均值），得到的值就相当于正确率。

accuracy函数返回的是Variable实例，但函数内部的计算是针对ndarray

实例进行的。因此，不能对accuracy函数求导。

上面的最后一行代码return Variable(as_array(acc))使用了as_array函

数。这是因为acc（＝result.mean()）的数据类型是np.float64或np.float32。

使用as_array函数对acc进行转换，会得到ndarray实例（as_array函数已在

步骤9中实现）。

  第 4阶段　创建神经网络 360

下面是使用accuracy函数计算正确率（识别精度）的示例。

import numpy as np

import dezero.functions as F

y = np.array([[0.2, 0.8, 0], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])

t = np.array([1, 2, 0])

acc = F.accuracy(y, t)

print(acc)

运行结果

variable(0.6666666666666666)

上面代码中的y是神经网络对3个样本数据的预测结果（这是一个3类分

类），训练数据t是每个样本数据的正确答案的索引。通过accuracy函数计算

出的识别精度为0.66...。

50.4 螺旋数据集的训练代码

下面使用DataLoader类和accuracy函数来训练螺旋数据集。代码如下所

示（省略了导入部分）。

steps/step50.py

max_epoch = 300

batch_size = 30

hidden_size = 10

lr = 1.0

train_set = dezero.datasets.Spiral(train=True)

test_set = dezero.datasets.Spiral(train=False)

train_loader = DataLoader(train_set, batch_size)

test_loader = DataLoader(test_set, batch_size, shuffle=False)

model = MLP((hidden_size, 3))

optimizer = optimizers.SGD(lr).setup(model)

for epoch in range(max_epoch):

 sum_loss, sum_acc = 0, 0

步骤 50 用于取出小批量数据的 DataLoader  361

 for x, t in train_loader: # ①用于训练的小批量数据

 y = model(x)

 loss = F.softmax_cross_entropy(y, t)

 acc = F.accuracy(y, t) # ②训练数据的识别精度

 model.cleargrads()

 loss.backward()

 optimizer.update()

 sum_loss += float(loss.data) * len(t)

 sum_acc += float(acc.data) * len(t)

 print('epoch: {}'.format(epoch+1))

 print('train loss: {:.4f}, accuracy: {:.4f}'.format(

 sum_loss / len(train_set), sum_acc / len(train_set)))

 sum_loss, sum_acc = 0, 0

 with dezero.no_grad(): # ③无梯度模式

 for x, t in test_loader: # ④用于测试的小批量数据

 y = model(x)

 loss = F.softmax_cross_entropy(y, t)

 acc = F.accuracy(y, t) # ⑤测试数据的识别精度

 sum_loss += float(loss.data) * len(t)

 sum_acc += float(acc.data) * len(t)

 print('test loss: {:.4f}, accuracy: {:.4f}'.format(

 sum_loss / len(test_set), sum_acc / len(test_set)))

接下来笔者对上面代码中的①～⑤加以说明。①处使用DataLoader取出

小批量数据，②处使用accuracy函数计算识别精度，③处使用测试数据集对

每轮的模型进行评估。由于测试阶段不需要反向传播，所以我们将这段代

码置于with dezero.no_grad():的作用域内。这样可以省掉反向传播的处理，

节约相关资源（no_grad函数已在步骤18中引入）。

④处从用于测试的DataLoader中取出小批量的数据进行评估。最后的⑤

处使用accuracy函数计算识别精度。

现在运行上面的代码。图501是结果的可视化图形。

  第 4阶段　创建神经网络 362

图501 损失和识别精度的变化情况

如图 501 所示，随着轮数的增加，损失（loss）逐渐减少，识别精度

（accuracy）逐渐增加。这是训练正确进行的证据。图501还显示出训练（train）

和测试（test）之间的差异很小。据此可以说我们的模型没有过拟合。

过拟合是模型过度拟合了特定训练数据的状态。无法预测未知数据的状态，

或者说不能泛化的状态就是过拟合。由于神经网络可以创建表达能力较强

的模型，所以经常出现过拟合。

以上就是本步骤的内容。下一个步骤将使用MNIST数据集来代替螺旋

数据集。

步骤 51 MINST的训练  363

步骤 51

MINST的训练

在前面的步骤，我们已经建立了易于处理数据集的机制，这里简单回

顾一下。首先，我们通过Dataset类统一了数据集的处理（固定了接口）；然

后，让数据集的预处理可以在Dataset类中设置；最后，让小批量数据可通

过DataLoader类从Dataset中创建。这些类之间的关系如图511所示。

DataLoader next()

Dataset

Callable

小批量数据

图511 DeZero数据集的类图

图511中的Callable是执行预处理的对象（可调用对象）。图中各类之间

的关系是：Callable由Dataset持有，Dataset由DataLoader持有。用户（用户

编写的训练代码）从DataLoader请求数据，获得小批量数据。

本步骤将使用前面的数据集机制来训练另一个新的数据集。这个新的数

据集就是MNIST。我们首先来简单了解一下MNIST数据集。

  第 4阶段　创建神经网络 364

51.1 MNIST数据集

DeZero 在 dezero/datasets.py 中提供了 MNIST类，这个 MNIST类继承了

Dataset类。它的使用示例如下所示。

import dezero

train_set = dezero.datasets.MNIST(train=True, transform=None)

test_set = dezero.datasets.MNIST(train=False, transform=None)

print(len(train_set))

print(len(test_set))

运行结果

60000

10000

上面的代码分别获取了用于训练的数据和用于测试的数据。代码中通

过设置transform=None来（显式地）指定不对数据进行预处理。之后查看了用

于训练的数据（train_set）和用于测试的数据（test_set）的数据长度。结果

是train_set为60000，test_set为10000。也就是说，有60000个训练数据和

10000个测试数据。接下来运行以下代码。

x, t = train_set[0]

print(type(x), x.shape)

print(t)

运行结果

<class 'numpy.ndarray'> (1, 28, 28)

5

上面的代码从train_set中抽取了第0个样本数据。取出的MNIST数据

集的数据形式为(data, label)，即包含data（图像）和标签的元组。另外，

MNIST的输入数据的形状是(1, 28, 28)，这意味着图像数据是1个通道（灰

度）、28×28像素的数据。标签是作为正确答案的数字的索引（0～9）。下面

步骤 51 MINST的训练  365

尝试对数据执行可视化操作。

import matplotlib.pyplot as plt

# 数据示例

x, t = train_set[0] # 取出第0个(data, label)

plt.imshow(x.reshape(28, 28), cmap='gray')

plt.axis('off')

plt.show()

print('label:', t)

运行结果

5

运行上面的代码，我们会看到图512所示的图像。

图512 MINST的图像示例

下面使用神经网络来训练这个手写图像的数据。在训练之前，我们需要

对输入数据进行预处理。代码如下所示。

  第 4阶段　创建神经网络 366

def f(x):

 x = x.flatten()

 x = x.astype(np.float32)

 x /= 255.0

 return x

train_set = dezero.datasets.MNIST(train=True, transform=f)

test_set = dezero.datasets.MNIST(train=False, transform=f)

首先将输入数据排成一列。这样，输入数据的形状就从(1, 28, 28)转

换为(784,)。然后将数据的类型转换为np.float32（32位浮点数）。最后除以

255.0，将输入数据转换为0.0和1.0之间的数值。这些预处理在MNIST类中是

默认进行的。因此，编写dezero.datasets.MNIST(train=True)也会执行上述

预处理（dezero/datasets.py包含使用dezero/transfroms.py中的类进行预处

理的代码）。

51.2 训练MNIST

接下来训练MNIST。代码如下所示（省略了导入语句的代码）。

steps/step51.py

max_epoch = 5

batch_size = 100

hidden_size = 1000

train_set = dezero.datasets.MNIST(train=True)

test_set = dezero.datasets.MNIST(train=False)

train_loader = DataLoader(train_set, batch_size)

test_loader = DataLoader(test_set, batch_size, shuffle=False)

model = MLP((hidden_size, 10))

optimizer = optimizers.SGD().setup(model)

for epoch in range(max_epoch):

 sum_loss, sum_acc = 0, 0

 for x, t in train_loader:

步骤 51 MINST的训练  367

 y = model(x)

 loss = F.softmax_cross_entropy(y, t)

 acc = F.accuracy(y, t)

 model.cleargrads()

 loss.backward()

 optimizer.update()

 sum_loss += float(loss.data) * len(t)

 sum_acc += float(acc.data) * len(t)

 print('epoch: {}'.format(epoch+1))

 print('train loss: {:.4f}, accuracy: {:.4f}'.format(

 sum_loss / len(train_set), sum_acc / len(train_set)))

 sum_loss, sum_acc = 0, 0

 with dezero.no_grad():

 for x, t in test_loader:

 y = model(x)

 loss = F.softmax_cross_entropy(y, t)

 acc = F.accuracy(y, t)

 sum_loss += float(loss.data) * len(t)

 sum_acc += float(acc.data) * len(t)

 print('test loss: {:.4f}, accuracy: {:.4f}'.format(

 sum_loss / len(test_set), sum_acc / len(test_set)))

运行结果

epoch: 1

train loss: 1.9103, accuracy: 0.5553

test loss: 1.5413, accuracy: 0.6751

epoch: 2

train loss: 1.2765, accuracy: 0.7774

test loss: 1.0366, accuracy: 0.8035

epoch: 3

train loss: 0.9195, accuracy: 0.8218

test loss: 0.7891, accuracy: 0.8345

epoch: 4

train loss: 0.7363, accuracy: 0.8414

test loss: 0.6542, accuracy: 0.8558

epoch: 5

train loss: 0.6324, accuracy: 0.8542

test loss: 0.5739, accuracy: 0.8668

与上一个步骤相比，变化在于现在使用了MNIST数据集，以及修改了

超参数的值。仅做这些修改就能训练MNIST了。由此，测试数据集上的识

  第 4阶段　创建神经网络 368

别精度达到了约86%。虽然增加轮数可以提高精度，但似乎还有别的方法可

以从根本上改善精度。在本步骤的最后，我们会创建一个精度更高的模型。

51.3 改进模型

我们之前使用的神经网络的激活函数是sigmoid函数。在神经网络的历

史上，sigmoid函数很早就被人们使用了。近来，人们经常使用ReLU（Rectified 

Linear Unit）函数来代替它。ReLU函数在输入大于0时按原样输出，在输

入小于等于0时输出0。它的式子如下所示。

h(x) =  x

0 (

(x >

x  0)

0) (51.1)

正如我们所看到的那样，ReLU函数是一个非常简单的函数。在DeZero

中可以很轻松地实现ReLU函数，代码如下所示。

dezero/functions.py

class ReLU(Function):

 def forward(self, x):

 y = np.maximum(x, 0.0)

 return y

 def backward(self, gy):

 x, = self.inputs

 mask = x.data > 0

 gx = gy * mask

 return gx

def relu(x):

 return ReLU()(x)

正向传播通过np.maximum(x, 0.0)取出x的元素和0.0中更大的值。在反

向传播中，x中大于0.0的元素，其位置的梯度按原样通过，而其他位置的

梯度则被设置为0。因此，我们需要准备一个用来表示梯度是否通过的mask，

步骤 51 MINST的训练  369

并将其与梯度相乘。

ReLU函数所做的就是“让信号通过”和“不让信号通过”两个处理。在正

向传播中信号通过的元素的相应梯度会在反向传播中按原样通过，而在正

向传播中信号没有通过的元素的相应梯度在反向传播中也不会通过（值是0）。

下面使用ReLU函数创建新的神经网络。这里将本步骤的训练代码中创

建模型的部分修改如下。

steps/step51.py

# model = MLP((hidden_size, 10))

model = MLP((hidden_size, hidden_size, 10), activation=F.relu)

与之前的代码相比，层数有所增加，神经网络变为3层。层数增加了，

模型的表现力比之前更丰富了。另外，激活函数变为ReLU函数，神经网络

有望更高效地进行训练。我们将这个神经网络的优化方法从SGD改为Adam

后，对其进行训练。结果，在训练数据上的识别精度约为99%，在测试数据

上的识别精度约为98%。与之前的结果相比，精度得到了大幅改善。



第4阶段的内容到此结束。在这个阶段，我们改进了DeZero以支持神

经网络。对于基本的神经网络问题，使用现在的DeZero都可以轻松解决。

DeZero已经升级为神经网络的框架和深度学习的框架了。更重要的是，我

们到目前为止学到的知识也适用于PyTorch和Chainer等著名框架。

试着去读一下 Chainer 官方的 examples 中的 MNIST 训练代码吧（参

考文献[30]），你会发现大部分的代码与我们本步骤的代码相似。再去看看

PyTorch的MNIST代码（参考文献[31]），你也能马上理解它。尽管类名和

模块名不同，但也只是接口和名称不同，它们在本质上与DeZero基本相同。

我们现在掌握了可以灵活使用PyTorch和Chainer等真正框架的“活的知识”！

  第 4阶段　创建神经网络 370

专栏：深度学习框架

早期的深度学习框架之间有很大差异，而现在的框架已经十分成熟。

PyTorch、Chainer和TensorFlow等流行的框架大多朝着同一个方向发展。虽然

它们特点不同，接口迥异，但它们的核心设计理念有许多共同之处。具体来说，

有以下几点。

 可以创建DefinebyRun风格的计算图

 有一套函数、层等类的集合

 有一套更新参数的类（优化器）的集合

 可以将模型分为子类来实现

 有管理数据集的类

 除了CPU，还可以在GPU或专用的ASIC上运行

 有可以作为静态计算图来运行的模式，以提高性能（以及用于产品）

上述特征是所有现代深度学习框架的共同特征。本专栏将结合具体的例子，

详细探讨前3点。

严格来说，深度学习的框架不是工具或库。库和框架的区别在于由谁来

控制程序。库是工具函数和数据结构的集合，用户从库中适当地取出必

要的东西来使用。此时，程序的控制，即以怎样的顺序执行代码由用户

决定。框架则是基础。拿深度学习的框架来说，它是自动微分的基础，

用户在此基础上构建所需要的计算。此时，由框架来控制程序整体。总

而言之，库和框架在程序由谁来控制这一点上是有区别的。

DefinebyRun方式的自动微分

深度学习框架中最重要的功能是自动微分。有了自动微分，我们可以毫不费

力地求出导数，而且现代的框架使用的是DefinebyRun方式创建计算图。代码

步骤 51 MINST的训练  371

会立即被执行，计算图会在幕后被创建。因此，使用Python的语法创建计算图

是可行的，例如在使用PyTorch的情况下可以编写以下代码。

import torch

x = torch.randn(1, requires_grad=True)

y = x

for i in range(5):

 y = y * 2

y.backward()

print(x.grad)

运行结果

tensor([32.])

PyTorch中处理张量的类是Tensor（DeZero中相应的类是Variable）。上面

的例子使用torch.randn方法创建了一个用随机数初始化的张量（torch.Tensor

实例），然后使用for语句进行计算。此时计算被立即执行，计算的“连接”是在

幕后完成的。这就是DefinebyRun。之后使用y.backward()来求导。这种风格

与我们的DeZero相同。

除 了 DefinebyRun，PyTorch、Chainer 和 TensorFlow 的框架也有以

DefineandRun（静态计算图）方式运行的模式。DefinebyRun适用于研究和开

发中的试错阶段，DefineandRun更适用于注重性能的产品和边缘计算环境。

TensorFlow的版本1需要使用一种专有的领域特定语言来创建计算图。

不过从版本 2 开始，TensorFlow 增加了以 Define-by-Run 方式（在

TensorFlow 中叫作 Eager Execution）运行的模式，并且以它为标准

模式。

  第 4阶段　创建神经网络 372

层的类集

我们可以通过组合已准备好的Linear层和Sigmoid层等来建立神经网络模型。

因此，深度学习的实现往往可以通过简单地组合现有的层来完成，就像搭乐高积

木一样。当然，要做到这一点，需要框架提供一套层的类集，例如Chainer提供

了图D1所列的层。

图D1 Chainer提供的部分层的示例：Chainer提供两种被称为层的模块，它们分别

是link和function。图中展示的是link的一部分（图片摘自参考文献[32]）

如图D1所示，Chainer提供了各种层。用户可从中选择需要的层，并通过

连接它们来建立一个神经网络。深度学习的框架就是像这样提供了各种层。这里，

步骤 51 MINST的训练  373

关键的点在于这些层的类集是建立在自动微分的机制之上的。具体结构如图D2

所示。

自动微分

Linear Conv2d ...... Sigmoid

图D2 框架提供的各种层的类集以自动微分机制为基础

如图D2所示，深度学习框架以自动微分机制为基础，它们利用这种自动微

分机制提供了各种层。理解了该结构后，我们就可以站在更高的维度上看待框架

本身，而不会被各种框架的细节所迷惑。

优化器的类集

深度学习训练使用参数的梯度来依次更新参数。更新方法多种多样，现在人

们仍在提出新的方法。在此背景下，由独立模块负责参数更新是普遍的做法。例

如在TensorFlow中，这样的模块叫作优化器。TensorFlow提供了一套优化器，

如图D3所示。

  第 4阶段　创建神经网络 374

图D3 TensorFlow提供的优化器列表（摘自参考文献[33]）

如图D3所示，TensorFlow提供了各种优化器。有了这样的优化器的类集，

用户就可以在更高的层面上思考更新参数的任务了。此外，在不同的优化器之间

进行切换的操作也非常简单，便于试错。

小结

本专栏探讨的是深度学习框架的核心功能。总的来说，框架具有自动微分功

能，在此基础上实现了层的类集，还有用于更新参数的优化器类集。这些功能如

图D4所示。

自动微分

层的类集

Linear Conv2d ...... Sigmoid 优化器的类集

SGD AdaGrad Adam ......

图D4 深度学习框架的核心功能

步骤 51 MINST的训练  375

图D4中的3个功能是大多数框架具有的重要功能，其中作为框架支柱的是

自动微分。有了自动微分功能，在此基础上创建各种层的类集，也就搭好了框架

的骨架，再提供用于更新参数的优化器的类集，就能覆盖深度学习要做的大部分

工作了。

了解了图D4所示的3个核心功能的结构之后，我们就对框架有了整体印象，

能够更为简单地看待它们。理解这种结构也有助于我们使用PyTorch、Chainer

和TensorFlow等框架。

 377

第 5阶段

DeZero高级挑战

在第4阶段，我们向DeZero增加了机器学习，尤其是神经网络特有的功能。

具体来说是Layer、Optimizer和DataLoader等类。有了这些类，我们就可以

使用DeZero轻松地创建模型，高效地训练模型。DeZero已经具备了开发神

经网络的基本功能。

接下来我们要将DeZero提高到一个新的水平。具体来说就是增加一些

新的功能，比如在GPU上运行DeZero和将模型保存到外部文件等。此外，

DeZero还将支持在训练和测试时改变行为的层（Dropout等）。有了这些功能，

DeZero会成为一个优秀的深度学习框架。

在这个阶段的后半部分，我们会实现CNN（卷积神经网络）和RNN（循

环神经网络）。这些网络结构很复杂，看上去不容易实现，不过使用DeZero后，

我们会发现这些复杂的网络结构非常好处理。现在我们进入最后一个阶段。

  第 5阶段　DeZero高级挑战 378

步骤 52 支持 GPU  379

步骤52

支持GPU

深度学习所做的计算大多为矩阵的乘积。矩阵的乘积由乘法运算和加法

运算构成，可以并行计算，GPU比CPU更擅长这种计算。本步骤将创建在

GPU上运行DeZero的机制。

要在GPU上运行DeZero，从硬件方面来说，需要NVIDIA的GPU；从软

件方面来说，需要Python的库CuPy。如果手头的环境不符合这些要求，

也可以使用 Google Colaboratory 在云端的 GPU 上运行 DeZero（从 2020

年2月开始可免费使用）。附录C介绍了Google Colaboratory的相关内容，

感兴趣的读者可以查看。

52.1 CuPy的安装和使用方法

CuPy是用于在GPU上进行并行计算的库，可通过pip安装，安装命令

如下所示。

$



pip install cupy

下面开始使

用CuPy。它的优点

是拥有与NumPy相

同的API，所以我

们

掌握的NumPy知

识可以直接

用于CuPy。例如，我

们可以使用

CuPy编写下

面这

样的代码。

第

5阶段　DeZero高级挑

战 380

import

cupy as cp

x

= cp.arange(6).reshape(2, 3)

print(x)

y = x.sum(axis=1)

print(y)

运行结果

[[0 1 2]

[3 4 5]]

[

3 12]

上面的代码

导入了CuPy并进

行求和计算

。正如我们看

到的那样，这

段

代码几乎

与NumPy完全相同

，其实我们所

做的不过是

将np替换为cp，让

cp

进行必要的

计算而已，而

这个计算在

幕后使用的

是GPU。

因此，NumPy的代

码很容易转

换为GPU版本的

代码。只要将

NumPy

代码中的np（numpy）替

换为cp（cupy）即可。

虽

然CuPy与NumPy的许多

API是相同的，但

二者并不完

全兼容。

下面

让DeZero支持GPU。我们

要做的是将

DeZero中使用NumPy的代

码切换为使

用CuPy的代码。准

确来说，我们

要创建能够

切换二者的

机制。

要做到

这一点，我们

需要了解关

于CuPy的两件事

。第一件事是

在NumPy

和CuPy之间转

换多维数组

的方法。代码

示例如下。

import

numpy as np

import

cupy as cp

#

numpy -> cupy

n

= np.array([1,2,3])

c =

cp.asarray(n)

步

骤 52 支持

GPU  381

assert

type(c) == cp.ndarray

#

cupy -> numpy

c

= cp.array([1, 2, 3])

n = cp.asnumpy(c)

assert

type(n) == np.ndarray

如代

码所示，从

NumPy 转

换为 CuPy 需要使

用

cp.asarray函数，从

CuPy转

换为NumPy需要使

用cp.asnumpy函数。

使用

cp.asarray函数和cp.asnumpy函数

时，数据会从

PC的内存传输

到

GPU的显存上

（或者反方向

传输）。这个传

输处理往往

会成为深度

学习计

算的

瓶颈。因此，理

想的做法是

尽可能减少

数据传输。

第

二件事与函

数cp.get_array_module有关。该函

数根据数据

返回相应的

模块，具体的

用法如下所

示。

x =

np.array([1, 2, 3])

xp

= cp.get_array_module(x)

assert xp

== np

x =

cp.array([1, 2, 3])

xp

= cp.get_array_module(x)

assert xp

== cp

如上面的

代码所示，如

果x是一个NumPy或

CuPy的多维数组

，使用xp

= cp.get_array_module(x)就可以

返回该数组

的模块。有了

这个函数，即

使

不知道x是

NumPy的数据还是

CuPy的数据，也能

获得相应的

模块。利用它

可以编写同

时支持CuPy和NumPy的

代码，例如，通

过xp =

cp.get_array_

module(x)和y = xp.sin(x)，就可以

达到相应的

效果。

关于CuPy的

知识，了解这

些就足够了

。下面在DeZero中创

建在CuPy

和NumPy之间

切换的机制

。

第 5阶段　DeZero高级

挑战 382

52.2 cuda模块

在

DeZero中，我们把CuPy相

关的函数放

到dezero/cuda.py模块（文件

）

中。顺便说一

下，CUDA是NVIDIA提供的

面向GPU的开发

环境。首先，

dezero/cuda.py的

导入部分如

下所示。

dezero/cuda.py

import numpy

as np

gpu_enable =

True

try:

 import

cupy as cp

cupy = cp

except

ImportError:

 gpu_enable =

False

from dezero import

Variable

上面

的代码导入

了NumPy和CuPy。由于CuPy是

可选库，所以

我们还

需要

考虑它没有

被安装的情

况。因此，上面

使用了try语句

执行导入操

作，

如果发生

ImportError，我们就设置

gpu_enable

= False。这样即使环

境中没3

有3

安

装CuPy也不会出

现任何错误

。接下来在dezero/cuda.py中

添加以下3

个

函数。

dezero/cuda.py

def get_array_module(x):

if isinstance(x, Variable):

x = x.data

if not gpu_enable:

return np

 xp

= cp.get_array_module(x)

 return

xp

def as_numpy(x):

if isinstance(x, Variable):

x = x.data

if np.isscalar(x):

步骤 52

支

持 GPU  383

return np.array(x)

 elif

isinstance(x, np.ndarray):

 return

x

 return cp.asnumpy(x)

def as_cupy(x):

 if

isinstance(x, Variable):

 x

= x.data

 if

not gpu_enable:

 raise

Exception('CuPy cannot be loaded.

Install CuPy!')

 return

cp.asarray(x)

第一个函

数get_array_module(x)返回参数

x对应的模块

，其中x是Variable

或ndarray（numpy.ndarray或

cupy.ndarray）。它主要是对

cp.get_array_module

函数进行封

装，但也执行

了cupy没有被导

入时的处理

。具体来说，如

果gpu_

enable为False，它将总

是返回np（numpy）。

剩下

的两个函数

是用于将参

数转换为NumPy/CuPy多

维数组的函

数。

将参数转

换为NumPy的ndarray的函

数是as_numpy，将参数

转换为CuPy的

ndarray的

函数是as_cupy。以上

就是dezero/cuda.py中的所

有代码。这个

模

块（文件）中

的3个函数现

在可在DeZero的其

他类中使用

。

52.3 向Variable /

Layer / DataLoader类添加代

码

接下来向

DeZero的其他类添

加支持GPU的代

码。本节将向

Variable、

Layer和DataLoader类添加支

持GPU的代码。

首

先将Variable类现有

的__init__方法和backward方

法修改如下

。

dezero/core.py

...

try:

 import cupy

array_types = (np.ndarray, cupy.ndarray)

except ImportError:

第 5阶段　DeZero高级

挑战 384

array_types = (np.ndarray)

class

Variable:

 def __init__(self,

data, name=None):

 if

data is not None:

if not isinstance(data, array_types):

raise TypeError('{} is not

supported'.format(type(data)))

 ...

def backward(self, retain_grad=False, create_graph=False):

if self.grad is None:

xp = dezero.cuda.get_array_module(self.data)

self.grad = Variable(xp.ones_like(self.data))

...

__init__方法的

参数data要修改

为支持cupy.ndarray的情

况。为此，当

cupy 被

成

功 导 入 时

，执

行 array_types=(np.ndarray, cupy.ndarray)，将

array_types动态

修改为需要

检查的类型

。

backward方法修改自

动补全梯度

（self.grad）的地方。修改

后的方法会

根

据数据的

类型（self.data）创建numpy或

cupy的多维数组

。

此前Variable类在实

例变量data中持

有NumPy的多维数

组。下面我们

来实现用于

将数据移动

到CPU的to_cpu函数，还

有将数据移

动到GPU的

to_gpu函数

。代码如下所

示。

dezero/core.py

class Variable:

...

 def to_cpu(self):

if self.data is not

None:

 self.data =

dezero.cuda.as_numpy(self.data)

 def to_gpu(self):

if self.data is not

None:

 self.data =

dezero.cuda.as_cupy(self.data)

上面的代

码只是使用

了as_numpy函数或as_cupy函

数。这样就可

以将

Variable数据从

GPU传输到CPU，或者

从CPU传输到GPU了

。

步骤

52 支持 GPU

385

下

一个是Layer类。Layer是

保存参数的

类。这些参数

是继承了Variable

的

Parameter类。接下来添

加将Layer类的参

数传输到CPU或

GPU的函数。

代码

如下所示。

dezero/layers.py

class Layer:

...

 def to_cpu(self):

for param in self.params():

param.to_cpu()

 def to_gpu(self):

for param in self.params():

param.to_gpu()

最

后向DataLoader类中添

加以下阴影

部分的代码

。

dezero/dataloaders.py

...

import numpy as np

from dezero import cuda

class DataLoader:

 def

__init__(self, dataset, batch_size, shuffle=True,

gpu=False ):

 self.dataset

= dataset

 self.batch_size

= batch_size

 self.shuffle

= shuffle

 self.data_size

= len(dataset)

 self.max_iter

= math.ceil(self.data_size / batch_size)

self.gpu = gpu

self.reset()

 def __next__(self):

...

 xp =

cuda.cupy if self.gpu else

np

 x =

xp.array([example[0] for example in

batch])

 t =

xp.array([example[1] for example in

batch])

  第

5阶段　DeZero高级

挑战 386

self.iteration += 1

return x, t

def to_cpu(self):

 self.gpu

= False

 def

to_gpu(self):

 self.gpu =

True

DataLoader类的作

用是从数据

集创建小批

量数据，这个

小批量数据

是在

__next__方法中

创建的。此前

创建的小批

量数据是NumPy多

维数组。这里

根据实例变

量gpu的标志位

在CuPy和NumPy之间进

行切换，创建

相应的多

维

数组。

以上就

是针对DeZero的3个

类Variable、Layer、DataLoader所做的修

改。

52.4 函数的相

应修改

关于

DeZero的“GPU化”，剩下的

主要任务是

修改DeZero的函数

。这

些具体的

函数实际上

是在forward方法中

进行计算的

，比如Sin类，其代

码如

下所示

。

dezero/functions.py

class

Sin(Function):

 def forward(self,

x):

 y =

np.sin(x)

 return y

def backward(self, gy):

x, = self.inputs

gx = gy *

cos(x)

 return gx

代码中def forward(self, x):的参

数x应为NumPy的ndarray实

例。因

此，我们

可以对其使

用

np.sin(x)等 NumPy 的函数

进行计算。如

果想在

步骤

52

支持 GPU  387

GPU 上运行

，参数 x应为 CuPy

的

ndarray实例。此时用

cp.sin(x)来代替

np.sin(x)。换言

之，当x是NumPy时必

须使用np.sin；当x是

CuPy时必须

使用

cp.sin。基于以上内

容，我们将Sin类

修改如下。

dezero/functions.py

from dezero import cuda

class Sin(Function):

 def

forward(self, x):

 xp

= cuda.get_array_module(x)

 y

= xp.sin(x)

 return

y

 def backward(self,

gy):

 x, =

self.inputs

 gx =

gy * cos(x)

return gx

上

面的代码通

过xp =

cuda.get_array_module(x)提取x对应

的模块。代

码

中的xp可以是

cp，也可以是np。之

后使用xp进行

计算。现在的

Sin类在

CPU/GPU（NumPy/CuPy）上都能

正确运行。

这

里只展示了

Sin类的代码，但

上面的修改

需要应用于

dezero/functions.

py中所有对应

的地方。对应

的地方指的

是np.xxx()这种以np.开

头的

代码。我

们要对这些

地方执行与

上面相同的

修改。此外，还

要对dezero/

optimizers.py和dezero/layers.py执行

同样的修改

。

最后修改DeZero的

四则运算的

代码。具体来

说，就是对dezero/core.py

按

如下内容进

行修改。

dezero/core.py

def as_array(x,

array_module=np):

 if np.isscalar(x):

return array_module.array(x)

 return

x

  第

5阶

段　DeZero高级挑战

388

def add(x0,

x1):

 x1 =

as_array(x1, dezero.cuda.get_array_module(x0.data))

 return

Add()(x0, x1)

def mul(x0,

x1):

 x1 =

as_array(x1, dezero.cuda.get_array_module(x0.data))

 return

Mul()(x0, x1)

# 对sub、rsub、div、rdiv进行同样

的修改

...

首先

向as_array函数添加

新的参数array_module。这

个array_module的

值为numpy或

cupy，函数将数据

转换为array_module指定

的模块的ndarray。

然

后使用这个

新的as_array函数修

改add和mul等四则

运算函数。在

add函数

中使用

as_array函数，是为了

能够运行x + 1这

样的代码（x是

Variable实例）。

即使x.data是

CuPy数据，x

+ 1这样的

代码也能正

确运行。

52.5 在GPU上

训练MNIST

经过这

些修改之后

，我们可以在

GPU上运行DeZero了。这

里尝试在

GPU上

运行MNIST的训练

代码。代码如

下所示。

steps/step52.py

import

time

import dezero

import

dezero.functions as F

from

dezero import optimizers

from

dezero import DataLoader

from

dezero.models import MLP

max_epoch

= 5

batch_size =

100

train_set = dezero.datasets.MNIST(train=True)

train_loader = DataLoader(train_set, batch_size)

model = MLP((1000, 10))

optimizer = optimizers.SGD().setup(model)

步骤

52

支持 GPU  389

# GPU mode

if

dezero.cuda.gpu_enable:

 train_loader.to_gpu()

model.to_gpu()

for epoch in

range(max_epoch):

 start =

time.time()

 sum_loss =

0

 for x,

t in train_loader:

y = model(x)

loss = F.softmax_cross_entropy(y, t)

model.cleargrads()

 loss.backward()

optimizer.update()

 sum_loss +=

float(loss.data) * len(t)

elapsed_time = time.time() -

start

 print('epoch: {},

loss: {:.4f}, time: {:.4f}[sec]'.format(

epoch + 1, sum_loss

/ len(train_set), elapsed_time))

在GPU可用

的环境下，上

面的代码会

将DataLoader和模型的

数据传输

到

GPU。这样后续处

理就会使用

CuPy的函数了。

下

面在GPU上实际

运行上面的

代码。结果显

示损失像以

前一样顺利

减

少，而且运

行速度也比

在CPU上要快。作

为参考，图521展

示了在Google

Colaboratory中运

行上面代码

的结果。

  第

5阶

段　DeZero高级挑战

390

图521

在Google Colaboratory中的运

行结果

如图

521所示，在使用

GPU时，每轮计算

可以在1.5秒左

右完成。这

个

结果取决于

Google Colaboratory的执行环境

（如分配的GPU）。而

在使用

CPU时，每

轮计算大约

需要8秒，所用

时间是使用

GPU的5倍左右。到

这里，

DeZero就支持

GPU了。

步骤 53 模型

的保存和加

载

391

步骤53

模型

的保存和加

载

在本步骤

中，我们会实

现将模型的

参数保存到

外部文件的

功能，还会实

现加载已保

存的参数的

功能。有了这

些功能，我们

就可以将训

练过程中的

模

型保存为

“快照”，也可以

加载训练好

的参数，只进

行推理。

DeZero 的参

数实现为 Parameter类

（它继承自

Variable）。Parameter的

数据则作为

ndarray实例被保存

在实例变量

data中，所以，这里

我们要把

ndarray实

例保存到外

部文件。正好

NumPy提供了一些

用于保存（和

加载）

ndarray的函数

。我们首先看

一下这些函

数的用法。

如

果在

GPU 上运行

DeZero，我 们 需

要 使

用 CuPy 的

ndarray（cupy.

ndarray）来代替

NumPy的ndarray。此时要把

CuPy的张量换成

NumPy

的张量，然后

把它们保存

到外部文件

。因此，在将数

据保存到外

部文件时，

我

们只考虑NumPy的

情况。

53.1 NumPy的save函数

和load函数

NumPy有np.save函

数和np.load函数。使

用这些函数

可以保存和

加载

ndarray实例。代

码如下所示

。

第 5阶段　DeZero高级

挑战

392

import numpy as

np

x = np.array([1,

2, 3])

np.save('test.npy', x)

x = np.load('test.npy')

print(x)

运行结

果

[1 2 3]

首先是np.save函

数。通过这个

函数，我们可

以将ndarray实例保

存到外

部文

件中。然后使

用np.load函数来加

载数据。这样

就可以保存

和加载单个

ndarray实例了。

上面

的代码将文

件保存为test.npy。如

上面的例子

所示，文件的

扩展名

必须

是.npy。如果没有

为文件加.npy这

个扩展名，那

么.npy将被自动

添

加到文件

名的末尾。

接

下来保存和

加载多个3 3

ndarray实

例。为此我们

需要使用np.savez函

数

和np.load函数。下

面是使用示

例。

x1 = np.array([1,

2, 3])

x2 =

np.array([4, 5, 6])

np.savez('test.npz',

x1=x1, x2=x2)

arrays =

np.load('test.npz')

x1 = arrays['x1']

x2 = arrays['x2']

print(x1)

print(x2)

运行结果

[1 2 3]

[4 5 6]

步骤

53 模型的

保存和加载

393

上面的代码

使用np.savez('test.npz',

x1=x1, x2=x2)保存了

多个ndarray

实例。我

们可以通过

x1=x1和x2=x2将实例作

为“关键字参

数”传给函数

，这

样在加载

实例时，数据

就可以通过

arrays['x1']和arrays['x2']取出。另外

，

在使用np.savez函数

时，要保存的

文件的扩展

名必须是.npz。

接

下来对Python字典

应用上面的

操作。下面是

实际的代码

示例。

x1 =

np.array([1, 2, 3])

x2

= np.array([4, 5, 6])

data = {'x1':x1, 'x2':x2}

np.savez('test.npz', **data)

arrays =

np.load('test.npz')

x1 = arrays['x1']

x2 = arrays['x2']

print(x1)

print(x2)

运行结

果

[1 2

3]

[4 5 6]

上面的代

码使用np.savez('test.npz',**data)保存

了字典。在传

递参数时，

使

用**data这种带两

个星号的方

式，可以展开

字典并将其

作为“关键字

参数”

传给函

数。

以上就是

NumPy中用于执行

保存和加载

操作的函数

的使用方法

，接下

来我们

会使用这里

介绍的函数

来实现将DeZero的

参数保存到

外部文件的

功能。

首先要

做的是将存

在于Layer类中的

Parameter“扁平地”取出

。

函 数

np.savez_compressed 与 np.savez 的

作

用 相 同。该 函

数

不 但 与

np.savez的

用法相同，还

会压缩并保

存文件。既然

如此，我们就

来使用

这个

np.savez_compressed函数吧。

  第

5阶

段　DeZero高级挑战

394

53.2 Layer类参数的扁

平化

首先回

顾一下Layer类的

层次结构。层

次结构是一

个嵌套的结

构，Layer

中还有别

的Layer。具体示例

如下所示。

layer =

Layer()

l1 = Layer()

l1.p1 = Parameter(np.array(1))

layer.l1

= l1

layer.p2 =

Parameter(np.array(2))

layer.p3 = Parameter(np.array(3))

上

面的layer中包含

另一个层l1。图

531是这个层次

结构的可视

化图形。

layer

p2

p3

l1

p1

图531 Layer类

的层次结构

现在考虑从

图531所示的层

次结构中将

Parameter作为一个扁

平的、非

嵌套

的字典取出

。为此要在Layer类

中添加一个

名为_flatten_params的方法

。

我们先来看

看这个方法

的用法。

params_dict =

{}

layer._flatten_params(params_dict)

print(params_dict)

步骤

53

模型的保存

和加载  395

运行

结果

{'p2': variable(2), 'l1/p1': variable(1),

'p3': variable(3)}

上面的

代码准备了

字典params_dict =

{}，并将其

作为参数传

给函数，

即layer._flatten_params(params_dict)。然

后，layer中包含的

参数被“扁平

地”

取出。实际

上，l1层中的p1参

数是用l1/p1这个

键存储的。下

面是_flatten_

params方法的

代码。

dezero/layers.py

class Layer:

...

 def _flatten_params(self,

params_dict, parent_key=""):

 for

name in self._params:

obj = self.__dict__[name]

key = parent_key +

'/' + name if

parent_key else name

if isinstance(obj, Layer):

obj._flatten_params(params_dict, key)

 else:

params_dict[key] = obj

这个方

法接收的参

数是字典params_dict和

文本parent_key。顺带提

一下，

Layer类的实

例变量_params保存

的是Parameter的实例

变量名称或

者Layer的

实例变

量名称。因此

我们需要通

过obj = self.__dict__[name]将实际的

对象

取出。之

后，如果取出

的obj是Layer，则调用

该obj的_flatten_params方法。

通

过（递归）调用

，我们就能以

扁平的结构

取出Parameter。

53.3 Layer类的save函

数和load函数

到

这里，我们就

完成了将Layer类

的参数保存

到外部文件

的准备。下面

添加新方法

save_weights和load_weights。

  第

5阶段　DeZero高

级挑战 396

dezero/layers.py

import os

class Layer:

...

 def save_weights(self,

path):

 self.to_cpu()

params_dict = {}

self._flatten_params(params_dict)

 array_dict =

{key: param.data for key,

param in params_dict.items()

if param is not

None}

 try:

np.savez_compressed(path, **array_dict)

 except

(Exception, KeyboardInterrupt) as e:

if os.path.exists(path):

 os.remove(path)

raise

 def load_weights(self,

path):

 npz =

np.load(path)

 params_dict =

{}

 self._flatten_params(params_dict)

for key, param in

params_dict.items():

 param.data =

npz[key]

save_weights方法

首先通过self.to_cpu()确

保数据在主

内存中（数据

是

NumPy的ndarray），然后创

建保存ndarray实例

的值的字典

（array_dict），之

后使用np.savez_compressed函

数将数据保

存为外部文

件。load_weights方法

使用

np.load函数来加载

数据，并将相

应键的数据

设置为参数

。

上面的代码

在保存文件

时使用了try语

句，该语句用

于处理由用

户发起的

键

盘中断（例如

用户按下了

Ctrl+C键）。在这种情

况（在保存过

程中收到了

中断信号的

情况）下，正在

保存的文件

会被删除，由

此可以防止

创建（和加载

）

不完整状态

的文件。

下面

以MNIST的训练为

例保存和加

载参数。代码

如下所示。

步

骤 53 模型的保

存和加载

397

steps/step53.py

import os

import dezero

import dezero.functions

as F

from dezero

import optimizers

from dezero

import DataLoader

from dezero.models

import MLP

max_epoch =

3

batch_size = 100

train_set = dezero.datasets.MNIST(train=True)

train_loader

= DataLoader(train_set, batch_size)

model

= MLP((1000, 10))

optimizer

= optimizers.SGD().setup(model)

# 加

载参数

if os.path.exists('my_mlp.npz'):

 model.load_weights('my_mlp.npz')

for epoch in range(max_epoch):

sum_loss = 0

for x, t in

train_loader:

 y =

model(x)

 loss =

F.softmax_cross_entropy(y, t)

 model.cleargrads()

loss.backward()

 optimizer.update()

sum_loss += float(loss.data) *

len(t)

 print('epoch: {},

loss: {:.4f}'.format(

 epoch

+ 1, sum_loss /

len(train_set)))

model.save_weights('my_mlp.npz')

首次

运行上面的

代码时，my_mlp.npz文件

并不存在，所

以和往常一

样，

我们让模

型的参数在

随机初始化

的状态下开

始训练，最后

通过model.save_

weights('my_mlp.npz')保存模

型的参数。

再

次运行上面

的代码时，my_mlp.npz文

件已经存在

，该文件会被

加载。

由此，之

前训练得到

的参数会赋

给模型。到这

里，我们就实

现了保存和

加载

模型参

数的功能。

第

5阶段　DeZero高级挑

战 398

步骤54

Dropout和测

试模式

过拟

合是神经网

络中常见的

问题。过拟合

发生的主要

原因有以下

几点。

1.

训练数

据少

2. 模型的

表现力太强

针对第一个

原因，我们可

以采取的措

施有增加数

据或使用数

据增强（data

augmentation）。

针 对

第 二

个 原 因

，可 以

采 取 的

措 施

有 权 重

衰 减（weight

decay）、

Dropout（参考文

献[34]）和批量正

则化（batch normalization）（参考文

献[35]）。

尤其是Dropout这

种方法简单

又有效，在实

践中经常被

使用。因此，本

步骤

会将Dropout添

加到DeZero中。此外

，在训练和测

试时需要改

变Dropout

的处理，所

以本步骤还

会创建区分

训练阶段和

测试阶段的

机制。

54.1 什么是

Dropout

Dropout是一种通过

随机删除（禁

用）神经元进

行训练的方

法。模型在训

练时随机选

择隐藏层的

神经元，并删

除选中的神

经元。如图541所

示，被删

除的

神经元不传

递任何信号

。

步骤

54 Dropout和测试

模式  399

图541 Dropout在训

练时的行为

在使用Dropout训练

时，每当有数

据流过，它都

会随机选择

要删除的神

经元。假设有

一个由10个神

经元组成的

层，我们想通

过该层后面

的Dropout

层随机删

除60%的神经元

。这时可以编

写如下代码

。

import numpy as np

dropout_ratio = 0.6

x

= np.ones(10)

mask =

np.random.rand(10) > dropout_ratio

y

= x * mask

代码中的mask是

元素为True或False的

数组。mask的创建

方法为首先

用np.random.rand(10)生成10个0 ∼ 1的

随机值，然后

将这些值与

dropout_

ratio（=0.6）进行比较，只

有大于dropout_ratio的元

素会转换为

True，其余的

元素

变为False。这个例

子创建的是

False所占比例为

60%的mask。

创建mask后，运

行y = x

* mask。这会使与

mask的False相对应的

x的元

素变为

0（即删除它们

）。结果导致平

均每次只有

4个神经元的

输出被传递

到

下一层。在

训练期间，每

次传递数据

时，Dropout层都会做

上面的处理

。

第 5阶段　DeZero高级

挑战

400

在机器

学习中，我们

经常使用集

成学习（ensemble learning)。集成

学习是

单独

训练多个模

型，在推理时

对多个模型

的输出取平

均值的方法

。以神

经网络

为例，我们可

以准备5个具

有相同（或类

似）结构的网

络，并单独

训

练每个网络

，然后在测试

时将5个输出

的平均值作

为答案。经过

实验发

现，这

种做法可以

使神经网络

的识别精度

提高几个百

分点。集成学

习与

Dropout相似。之

所以这么说

，是因为Dropout在训

练时会随机

删除神经元

，

这可以解释

为每次都在

训练不同的

模型。换言之

，我们可以把

Dropout当

作一种在

一个网络上

达到与集成

学习相同效

果的伪实现

。

上面的代码

是Dropout在训练时

所做的处理

。而在测试时

，模型需要使

用所有的神

经元，“模仿”训

练时集成学

习的行为。这

可以通过使

用所有的

神

经元计算输

出，然后“弱化

”其输出来实

现。弱化的比

例是在训练

时幸存

下来

的神经元的

比例。具体代

码如下所示

。

# 训练时

mask

= np.random.rand(*x.shape) > dropout_ratio

y = x *

mask

# 测试

时

scale

= 1 - dropout_ratio

y = x *

scale

上面的代

码在测试时

的转换比例

为y = x*scale。用具体数

字来说，就是

平均有40%的神

经元在训练

时存活。考虑

到这一点，测

试时要使用

所有的神

经

元进行计算

，然后将输出

乘以0.4。这样就

可以使训练

和测试在缩

放幅度

上保

持一致。

以上

是“常规的Dropout”。之

所以说它是

“常规的”，是因

为Dropout

还有其他

的实现方式

，这种实现方

式叫作Inverted

Dropout（反向

Dropout）。

接下来笔者

将介绍Inverted Dropout。为了

加以区分，今

后我们将此

前介绍

的Dropout称

为Direct

Dropout（直接Dropout）。

步骤

54 Dropout和测试模式

401

54.2 Inverted Dropout

Inverted

Dropout在训练时进

行缩放处理

。回忆一下，之

前在测试时

乘

以scale以实现

缩放。现在为

了避免在测

试时做相应

的处理，我们

要在训练时

先将神经元

的值乘以 scale

1

，这

样就不必在

测试时做任

何缩放转换

。Inverted 

Dropout的代码如下

所示。

#

训练时

scale = 1 -

dropout_ratio

mask = np.random.rand(*x.shape)

> dropout_ratio

y =

x * mask /

scale

# 测试时

y

= x

Inverted Dropout的工

作原理与Direct

Dropout的

工作原理相

同。不过

Inverted Dropout有一

个优点，那就

是它在测试

时不做任何

处理，所以它

在

测试时的

处理速度（略

）快。如果只使

用推理处理

，这倒是一个

理想的特性

。

Inverted Dropout还支持在训

练时动态地

改变dropout_ratio。例如我

们

可以在第

1次流转数据

时设置dropout_ratio =

0.43455，在下

一次流转数

据

时设置dropout_ratio = 0.56245。而

Direct

Dropout需要固定dropout_ratio

进

行训练。如果

在训练过程

中改变了数

值，它就会与

测试时的行

为不一致。

出

于这些优点

，许多深度学

习框架采用

了Inverted Dropout方式，DeZero

也采

用了该方式

。

54.3 增加测试模

式

在使用Dropout的

情况下，我们

需要区分训

练阶段和测

试阶段。为此

，

可以继续使

用在步骤18创

建的禁用反

向传播模式

（with dezero.no_grad():）。

首先，在dezero/core.py的Config类

附近添加以

下阴影部分

的代码。

第 5阶

段　DeZero高级挑战

402

dezero/core.py

class Config:

 enable_backprop

= True

 train

= True

@contextlib.contextmanager

def

using_config(name, value):

 old_value

= getattr(Config, name)

setattr(Config, name, value)

yield

 setattr(Config, name,

old_value)

def test_mode():

return using_config('train', False)

上面的代码

在Config类中添加

了类变量train。这

个变量的默

认值是

True。另外

在 dezero/__init__.py中有一行

代码是 from dezero.core

import 

Config，由此

其他文件可

以引用dezero.Config.train。

之后

增加了test_mode函数

。通过与with语句

相结合，我们

能够只在代

码块内将Config.train切

换为False。由于用

户也要直接

使用这个函

数，所以

dezero/__init__.py中增

加了代码from dezero.core import test_mode。这

样

用户可以

通过from dezero import test_mode导入test_mode函

数。

54.4 Dropout的实现

最

后来实现Dropout，具

体代码如下

A。

dezero/functions.py

def dropout(x, dropout_ratio=0.5):

x = as_variable(x)

if dezero.Config.train:

A 我们还可以

通过定义继

承Function类的Dropout类的

方式实现DeZero的

Dropout。虽然这种实

现方

式的处

理效率更高

，但由于Dropout的处

理很简单，而

且在推理过

程中不做任

何处理，所以

本书没

有对

这种实现方

式进行详细

介绍。

步骤 54

Dropout和

测试模式  403

xp = cuda.get_array_module(x)

mask = xp.random.rand(*x.shape) >

dropout_ratio

 scale =

xp.array(1.0 - dropout_ratio).astype(x.dtype)

y = x *

mask / scale

return y

 else:

return x

代

码中的x是Variable实

例或ndarray实例。考

虑到它也可

能是CuPy的

ndarray实例

，我们使用xp

= cuda.get_array_module(x)来

获得实际的

包。剩

余的代

码都是介绍

过的内容。下

面是dropout的使用

示例。

steps/step54.py

import numpy as np

from dezero import test_mode

import dezero.functions as F

x = np.ones(5)

print(x)

# 训练时

y = F.dropout(x)

print(y)

# 测试时

with

test_mode():

 y =

F.dropout(x)

 print(y)

运行

结果

[1. 1. 1. 1.

1.]

variable([0. 2. 2.

0. 0.])

variable([1. 1.

1. 1. 1.])

我们可

以按照上面

的方式使用

F.dropout函数。此外，我

们也实现了

指定训练阶

段和测试阶

段的机制。今

后在发生过

拟合时，请积

极地使用

Dropout吧

。

  第

5阶段　DeZero高级

挑战 404

步骤55

CNN的

机制（1）

下面用

几个步骤的

篇幅来探讨

CNN 的内容。CNN 是

Convolutional 

Neural Network的

缩写，意为卷

积神经网络

。它是一种在

图像识别、语

音识

别和自

然语言处理

等领域使用

的神经网络

。尤其是在图

像识别领域

，大多数

深度

学习方法基

于CNN。接下来笔

者将介绍CNN的

机制，特别是

用于图像

的

CNN的机制。

本书

从CNN的实现出

发，仅对其机

制进行说明

。这里不会解

释CNN为什

么擅

长图像识别

，为什么可以

提取图像的

特征值。相关

内容请参考

本书前

作《深

度学习入门

：基于Python的理论

与实现》等资

料。

55.1 CNN的网络结

构

CNN和我们之

前见过的神

经网络相同

，也是由各层

组合而成的

。不过

在CNN中出

现了新的层

——卷积层（convolution layer）和池

化层（pooling

layer）。后面会

解释卷积层

和池化层的

具体内容，这

里我们先看

看CNN是由

什么

样的层组合

而成的。图551是

一个CNN模型的

示例。

步骤

55 CNN的

机制（1）  405

Conv ReLU

Linear ReLU

Linear

Pool Conv ReLU

Pool input

ReLU Linear

output

图551 CNN模型

的示例（本计

算图以层为

单位绘制，只

展示输入变

量和输出变

量。卷积

层用

Conv表示，池化层

用Pool表示）

如 图

551

所 示，CNN 中新添

加了 Conv

层 和 Pool 层

。CNN

中各层的

连

接顺序是Conv→ReLU→（Pool）（Pool层

有时会被省

略）。我们可以

认为是

Conv→ReLU→（Pool）替代

了此前的Linear→ReLU。另

外，图551的输出

附近

的层则

使用了之前

的组合Linear→ReLU。以上

是CNN的常见配

置。

55.2 卷积运算

CNN中使用了卷

积层。卷积层

中执行的处

理是卷积运

算，它相当于

图

像处理中

的过滤操作

。下面笔者以

图55

2为例对卷

积操作进行

说明。

1

输入数

据

2

0 1

3

2

0

3

3 0

1 2

2 3

0 1

2

过滤器

0

0 1

1

2

1 0 2

15

输

出数据

16

6

15

图552 卷

积运算的例

子（卷积运算

用⊛表示）

如图

552所示，卷积操

作对输入数

据应用过滤

器。在这个例

子中，输入

数

据拥有垂直

方向和水平

方向这两个

维度，过滤器

也同样拥有

垂直方向和

水

平方向两

个维度。如果

按照(height,

width)的顺序

来描述数据

的形状，那么

第 5阶段

DeZero高级

挑战 406

在这个

例子中输入

大小为(4, 4)，过滤

器大小为(3,

3)，输

出大小为(2, 2)。

此

时的卷积运

算会按照图

553所示的顺序

进行。

1

1*2 + 2*0

+ 3*1 + 0*0

+ 1*1 + 2*2

+ 3*1 + 0*0

+ 1*2 = 15

2

0 1

3

2

0

3

3

0 1 2

2

3 0 1

2

0

0 1

1

2

1 0 2

15

1 2

0

1

3

2

0

3

3 0 1

2

2 3 0

1

2 0

0

1

1

2

1

0 2

15 16

1 2

0 1

3

2

0

3

3 0 1 2

2 3 0 1

2 0

0 1

1

2

1 0

2

15 16

6

1 2

0 1

3

2

0

3

3 0 1 2

2 3 0 1

2 0

0 1

1

2

1 0

2

15 16

6

15

图553 卷积

运算的计算

顺序

卷积运

算一边以一

定的间隔移

动过滤器窗

口，一边使用

输入数据。如

图

553所示，最终

求出的是过

滤器和输入

的相应元素

相乘，然后各

项相加的结

果。

步骤

55 CNN的机

制（1）  407

这个结果

会存储在相

应的位置。通

过对所有位

置执行这一

过程，可得到

卷积

运算的

输出。另外，这

里所说的过

滤器在一些

资料中用卷

积核来表示

，本书

把过滤

器与卷积核

看作同一个

术语。

仔细观

察图55

3中的过

滤器，我们会

发现它在水

平和垂直两

个方向上移

动。由于这是

一个在两个

维度上移动

的过滤器，所

以是二维卷

积层。以此

类

推，如果过滤

器只在一个

方向移动，就

是一维卷积

层；如果过滤

器在三

个方

向移动，就是

三维卷积层

。对于图像，我

们主要处理

的是二维卷

积层。

在DeZero中，二

维卷积层是

以Conv2d这个名字

来实现的。

在

全连接层的

神经网络中

，除权重参数

之外还有偏

置。卷积层也

有偏置。

包括

偏置在内的

卷积运算的

处理流程如

图554所示。

1

输入

数据

2

0 1

3

2

0

3

3 0 1 2

2 3 0 1

2

过滤器

0

0 1

1

2

1 0

2

15

3

偏置

16

+ 6 15

18 19

9 18

图554 卷积

运算的偏置

如图554所示，偏

置的加法运

算针对的是

应用了过滤

器之后的数

据。请

注意，这

里只有一个

偏置（在这个

例子中，应用

过滤器之后

的数据有4个

，

偏置只有1个

）。在应用过滤

器后，该值会

加到所有元

素中。

接下来

介绍卷积层

中的填充（padding）和

步幅（stride）这两个

术语。

55.3

填充

在

进行卷积层

的主要处理

之前，有时要

在输入数据

的周围填充

固定的数

据

（例如0）。这个处

理叫作填充

。例如，在图555所

示的例子中

，针对大小为

(4,

第 5阶段

DeZero高级

挑战 408

4)的输入

数据使用了

幅度为1的填

充。

0

0

0 0 0

0

0

0

0

0

7 12

4

15

10

16

2

10

10 6 15

6

8 10 4

3

1

输入数据

（padding:1）

2

0 1

3

2

0

3

3 0

1 2

2 3

0 1

2

过滤器

输出

数据

0

0 1

1

2

1 0

2

0

0

0

0

0

0 0

0 0 0

图55

5 卷积

运算的填充

处理

如图555所

示，大小为(4,

4)的

输入数据经

过填充后变

为(6,6)的形状。

然

后应用大小

为(3,3)的过滤器

，生成了大小

为(4,4)的输出数

据。这个例子

中的填充被

设置为1，但其

实它也可以

被设置为2或

3等任意整数

。我们也可

以

为垂直方向

和水平方向

单独设置填

充。

使用填充

的主要目的

是调整输出

的大小。例如

，对大小为(4,

4)的

输入

数据应

用大小为(3, 3)的

过滤器后，输

出大小变为

(2, 2)，相当于输出

大小比输入

大小缩小了

2个元素。对重

复进行卷积

运算的深度

网络来说，

这

是一个问题

，因为如果每

次进行卷积

运算时都会

缩小空间，那

么在某个

时

间点就不能

再进行卷积

运算了。为了

避免出现这

种情况，需要

使用填充。

在

前面的例子

中，通过将填

充的幅度设

置为1，输入大

小变成了(6, 6)。

此

时再经过过

滤器，输出大

小将变成(4, 4)，大

小不变。

55.4 步幅

应用过滤器

的位置之间

的间隔称为

步幅。前面例

子中的步幅

都是1，如

果把

步幅设置为

2，如图556所示，应

用过滤器的

窗口的间隔

就会变为两

个

元素。

步骤

55

CNN的机制（1）  409

步幅

：2

1 2

0 1

3

2

3 0

1

1 2

0

1

3

2

3

0 1 2 0

0 1

1

2

1 0 2

15

0

3

2

1

2

0 1

3

2

3 0 1

1 2

0 1

3

2

3 0

1

0

3

2

2 3 0 2

3 0 1

1

2

0 1

3

2

3 0 1

1 2

0 1

3

2

3 0

1 2 0

0

1

1

2

1

0 2

15 11

0

3

2

1

2

0 1

3

2

3 0 1

1 2

0 1

3

2

3 0

1

0

3

2

2 3 0 2

3 0 1

图55

6 步幅为2的

卷积运算的

例子

在图556的

例子中，步幅

为2的过滤器

应用于输入

大小为(7,

7)的数

据。

将步幅设

置为2，输出大

小则为(3, 3)。从例

子可以看出

，步幅指定了

应用

过滤器

的间隔。我们

还可以分别

设置垂直方

向和水平方

向的步幅。

55.5 输

出大小的计

算方法

从前

面的例子可

以看出，步幅

越大，输出大

小越小。另外

，填充越大，

输

出大小越大

。也就是说，输

出大小受步

幅和填充的

影响。除步幅

和填充之

外

，如果给定了

输入数据和

过滤器的大

小，那么输出

的大小就是

唯一的。其

  第

5阶段

DeZero高级挑

战 410

计算方法

如下所示。

steps/step55.py

def get_conv_outsize(input_size, kernel_size, stride,

pad):

 return (input_size

+ pad * 2

- kernel_size) // stride

+ 1

函

数的所有参

数都应为int类

型。input_size是输入数

据的大小，kernel_

size是

过滤器的大

小，stride是步幅的

大小，pad是填充

的大小。

上面

代码中的//是

除法运算。计

算结果不能

被整除时，小

数部分会被

舍去。

下面实

际使用这里

实现的get_conv_outsize函数

。结果如下所

示。

steps/step55.py

H,

W = 4, 4

# input_shape

KH, KW

= 3, 3 #

kernel_shape

SH, SW =

1, 1 # stride（垂直方向

的步幅，水平

方向的步幅

）

PH, PW = 1,

1 # padding（垂直方向的

填充，水平方

向的填充）

OH

= get_conv_outsize(H, KH, SH,

PH)

OW = get_conv_outsize(W,

KW, SW, PW)

print(OH,

OW)

运

行结果

4 4

上面

的代码成功

计算出了输

出大小。考虑

到将来也要

用这个get_conv_

outsize函数

，我们把它添

加到dezero/utils.py中。

介绍

完卷积运算

的基本内容

后，本步骤也

就结束了。在

下一个步骤

中，

我们将探

讨CNN机制的其

他主题（通道

和池化）。

步骤

56 CNN的机制（2）  411

步骤

56

CNN的机制（2）

上一

个步骤介绍

了在垂直方

向和水平方

向上排列的

二维数据（二

阶张量）

的卷

积运算。不过

对图像来说

，除在垂直方

向和水平方

向之外，数据

还在通

道方

向上排列，因

此DeZero要能处理

这种三维数

据（三阶张量

）。本步骤,我们

将使用与上

一个步骤相

同的做法来

研究三阶张

量的卷积运

算，之后会探

讨池化。

56.1 三阶

张量

我们先

看一个卷积

运算的例子

，图56

1是对通道

数为3的数据

进行卷积

运

算的情形。

4 2

1 2

4

2

5

3 0 6

5

3

0

1

1

输

入数据

2

0

1

3

2

0

3

3 0 1

2

2 3 0

1

过滤

器

4 0

2

0

2

0

1 3

2

0

2 0

0 1

1

2

1 0

2

63

输出数据

55

18

51

图561 对三阶张

量进行卷积

运算的例子



第 5阶段　DeZero高级

挑战 412

图561中的

卷积运算的

计算步骤与

二阶张量的

相同。除了在

更深的方向

上增加数据

，过滤器的工

作方式和计

算方法都没

有发生变化

。这里需要注

意

的是，输入

数据的通道

数和过滤器

的通道数相

同。图561的输入

数据和过滤

器的通道数

都是3。另外，我

们可以将过

滤器在垂直

方向和水平

方向上的大

小设置为任

意数字。在上

面的例子中

，过滤器的大

小是(3,

3)，我们也

可以

把它设

置为其他的

值，如(1, 2)或(2,1)。

与上

一个步骤一

样，图56

1中的过

滤器在水平

方向和垂直

方向这两个

维

度上移动

，因此，该过滤

器被分类到

二维卷积层

中（尽管是针

对三阶张量

进行的卷积

运算）。在许多

深度学习框

架中，该处理

被实现为Conv2d或

Convolution2d类。

56.2 结合方块

进行思考

如

果把三阶张

量看作方块

，就容易理解

它的卷积运

算了。如图562所

示，

笔者结合

三维的方块

来介绍三阶

张量。

W

(C, H, W) (C,

KH, KW) (1, OH,

OW)

H

C

形状

KW

OW

OH KH

C

图

562 结合方块来

思考卷积运

算

图562中数据

的书写顺序

是(channel, height, width)。例如，通道

数为

C、高度为

H、宽度为W的数

据的形状可

写为(C, H, W)。过滤器

的情况也一

样，

按照(channel,

height, width)的顺

序书写即可

。如果通道数

为C、过滤器的

步骤 56 CNN的机制

（2）

413

高度为KH A，宽度

为KW，则写为(C, KH,

KW)。

输

出被称为“特

征图”。在图562的

例子中，输出

以一个特征

图的形式

显

示出来。假设

我们想让通

道方向上有

多个特征图

。要达到这样

的效果，需

要

用到多个过

滤器（权重）。具

体如图563所示

。

W

(OC,

OH, OW) (OC, C,

KH, KW) (C, H,

W)

H

C

形状

KW

OC

OW

OH

KH

C

OC

个

图563 基

于多个过滤

器的卷积运

算的例子

图

56

3中的各张量

分别应用了

OC B 个过滤器。于

是有OC个特征

图会被

创建

出来。这OC个特

征图汇总在

一起就是(OC,

OH, OW)形

状的方块。

从

图563可以看出

，关于卷积运

算的过滤器

，其数量也是

我们需要考

虑的

要素。为

此，我们要把

过滤器的权

重数据设为

一个四阶张

量，将其数据

形

状规定为

(output_channel, input_channel, height,

width)。如果

有20个通

道数为3、大小

为(5,5)的过滤器

，那么该过滤

器的形状为

(20, 

3,

5, 5)。

A KH是Kernel

Height的首字母

缩写，KW是Kernel Width的首

字母缩写。

B OC是

Output

Channel的首字母缩

写。

......

第 5阶段　DeZero高

级挑战 414

卷积

运算中（和全

连接层相同

）也存在偏置

。图564在图563的基

础上

添加了

偏置的加法

处理。

W

(C, H, (OC

W) , C, KH,

(OC KW) , OH,

OW) (OC, 1, 1)

H

C

+

+

(OC, OH, OW)

KW

OW

OH

KH

C

OC

OW

OH

OC

1

1

OC

图56

4 卷积

运算的处理

流程（又添加

了偏置项）

如

图564所示，每个

通道只有一

个偏置。这里

，偏置的形状

是(OC,

1, 1)，

应用过滤

器后的输出

的形状是(OC, OH,

OW)。由

于形状不同

，偏置在被广

播

处理之后

加到结果中

。以上就是包

含了偏置加

法运算的卷

积运算。

56.3 小批

量处理

在神

经网络的训

练过程中，输

入数据会被

合并起来进

行处理（这种

处理

方式叫

作小批量处

理）。卷积运算

也会进行小

批量处理。因

此，流经各层

的

数据会被

视为四阶张

量。图565是对由

N个数据组成

的小批量数

据进行卷积

运算的过程

。

......

步骤 56 CNN的机制

（2）

415

N个数据 N个数

据 N个数据

W

(OC, C, KH,

KW) (OC, 1, 1)

H

C

+

+

KW

OW

OH

KH

C

OC

OW

OH

OC

1

1

OC

(N, C, H, W)

(N, OC, OH, OW)

(N, OC, OH, OW)

图

565 卷积运算的

处理流程（小

批量处理）

图

56

5的数据在前

面增加了用

于小批量的

维度。因此，数

据格式变为

(batch_size, channel, height, width)。小批量处理

对这个四阶

张量中的每

个样本数据

（单独）进行相

同的卷积运

算。

到这里就

介绍完CNN中的

卷积层的计

算了。接下来

讨论CNN中的池

化层。

56.4 池化层

池化处理是

缩小垂直方

向和水平方

向空间的操

作。图566的例子

展示

了步幅

为2的2 × 2的最大

池化的处理

过程。最大池

化是取最大

值的运算，

2

× 2表

示池化目标

区域的大小

。如图所示，此

处从2 × 2区域取

出值最大的

元素。另外，在

这个例子中

，由于步幅被

设置为2，所以

2

× 2窗口的移动

间隔是两个

元素。一般来

说，池化窗口

的大小和步

幅应设置为

相同的值，例

如3 × 3池化的步

幅为3，4

× 4池化的

步幅为4，以此

类推。 ......

第 5阶段

DeZero高级挑战 416

1

2

0 1

1

2

0

3

3

0 1 2

2

4 0 1

2

1 2

0 1

1

2

0

3

3 0 1 2

2 4 0 1

2 3

1 2

0 1

1

2

0

3

3 0

1 2

2 4

0 1

2 3

4

1 2

0

1

1

2

0

3

3 0 1

2

2 4 0

1

2 3

4

2

图

566 最大池化的

处理步骤

除

最大池化之

外，还有平均

池化。最大池

化是从对象

区域中取出

最大值的

计

算，平均池化

则是计算对

象区域的平

均值。由于在

图像识别领

域主要使

用

的是最大池

化，所以本书

在提到池化

层时指的是

最大池化。

以

上就是对池

化层的介绍

。池化层具有

以下特点。

没

有学习参数

与卷积层不

同，池化层没

有任何学习

参数。这是因

为池化只取

对象区域

中

的最大值（或

平均值）。

通道

数量不发生

变化

池化计

算不改变输

入数据和输

出数据的通

道数量。如图

56

7所示，计算

是

按通道独立

进行的。

步骤

56 CNN的机制（2）

417

4 4

3

64 5

2 34

2

4 2

4

2 1 2

4

2

5

3 0

6 5

3

0

1

1

输入

数据

2

0 1

1

2

0

3

3 0

1 2

2 4

0 1

输出数

据

图56

7 池化的

通道数量

对

微小的位置

变化具有鲁

棒性

对于输

入数据中的

微小差异，池

化的结果是

相同的。因此

，它对输入数

据的微小差

异具有鲁棒

性（健壮性）。例

如图56

8中的3 × 3池

化的情景，从

中我们可以

看出池化可

以吸收输入

数据的差异

。右图中的输

入数据在水

平方

向上移

动了一个元

素，但输出是

相同的（结果

与数据有关

，有时候也会

出现

输出不

同的情况）。

1

3

2

3

2

1

9

74

6 8

1

2

0 9

0

2

3 0 1

7

3

1

2

2 1

0

3

2

2 4 0

6 0 1

1

0

2 1

1

2

2 4 0

1 8 1

9

74

6 8

1

2

0 9

0

2

3 0 1

7

3

1

2

2 1

2 4

0

6 0 1

1 0

2 1

2 4 0 1

8

图

568 当输入数据

有微小差异

时的比较

以

上就是对CNN机

制的介绍。前

面主要探讨

了卷积层和

池化层。在下

一个步骤，我

们将把这两

个处理实现

为DeZero函数。

  第

5阶

段　DeZero高级挑战

418

步骤57

conv2d函数和

pooling函数

步骤55和

步骤56介绍了

卷积层和池

化层。如果自

己从头实现

卷积运

算，编

写出的代码

将会包含多

层for语句的嵌

套。这样的代

码很烦琐，而

且

在NumPy中使用

for语句会使处

理速度变慢

。所以这里我

们不使用for语

句，

而是使用

im2col这个工具函

数来实现目

标。im2col是image

to column的缩写

，

意思是从图

像到列。

在DeZero中

，神经网络的

转换处理是

以函数的形

式实现的，而

具有参数

的

层则继承于

Layer类来管理参

数。这里将卷

积层所做的

处理以conv2d（或

conv2d_simple）函

数的形式实

现，然后实现

继承于Layer类的

Conv2d层。

另外，由于

池化层没有

参数，所以我

们只实现pooling函

数。

57.1

使用im2col展开

im2col是一个用于

展开输入数

据的函数，它

将输入数据

展开为卷积

核易

于处理

的形式。如图

571所示，它从作

为输入数据

的三阶张量

中取出了应

用

卷积核的

区域（准确来

说是从包括

批量大小在

内的四阶张

量中取出应

用卷积

核的

区域）。

步骤 57 conv2d函

数和pooling函数

419

OH*OW C*KH*KW

4

7

8

输

入数据

W

H

C

KW

KH

OH*OW

C

个

reshape

图

571 展开卷积核

的应用区域

如图571所示，首

先取出应用

卷积核的区

域，然后将取

出的区域变

为一

列，最终

将其转换为

矩阵（二阶张

量）。这就是im2col函

数所做的处

理。

Chainer的im2col执行了

图571中第一阶

段的处理（不

包括reshape部分

的

处理）。这是因

为一旦取出

了应用卷积

核的区域，后

面就可以通

过张量

积A 进

行必要的计

算。由于我们

使用了矩阵

的乘积，所以

本书需要完

成到

reshape部分的

处理。另外，DeZero中

使用的im2col函数

有一个名为

to_

matrix的标志位，只

有当这个标

志位为True时，图

57

1中的reshape部

分才

会被一并处

理。 A

通过im2col将输

入数据展开

后，将卷积层

的卷积核（过

滤器）扩展为

一列，

然后计

算两个矩阵

的乘积，具体

如图572所示。

A 简

单来说，张量

积是矩阵乘

积的扩展。它

可以在任意

的张量之间

指定张量的

轴，进行乘积

累加运算。

我

们可以使用

NumPy的np.tensordot和np.einsum来计算

张量积。 ......

....

...

...

...

...

...

第 5阶

段　DeZero高级挑战

420

矩阵的乘积

im2col

reshape @

输入数据

输

出数据（二阶

张量） 输出数

据

卷积核

图

57

2 基于输入数

据和卷积核

的矩阵乘积

的计算

如图

572所示，首先计

算矩阵的乘

积。这个矩阵

乘积的输出

也是矩阵（二

阶张量）。最后

，输出转换为

三阶张量（准

确来说是包

含批量大小

在内的四阶

张量）。以上就

是卷积层的

实现流程。

在

进行卷积运

算时使用im2col展

开输入数据

后，元素的数

量大多会大

于

原来的数

量。因此，使用

了im2col的实现需

要大量的内

存空间。不过

由

于可以使

用矩阵的乘

积进行计算

，所以矩阵库

优化过的函

数能发挥很

大的

作用。

57.2 conv2d函

数的实现

本

书把DeZero的im2col函数

当作黑盒使

用（不关注其

内部实现）。im2col

函

数是对Variable实例

的输入进行

计算的DeZero函数

，其导数可以

通过

backward求得。

......

步

骤 57

conv2d函数和pooling函

数  421

由于

CNN 的函

数代码较多

，所以我们不

将 CNN 相关的代

码保存在

dezero/functions.py 中

，而 是 保

存 在

dezero/functions_conv.py 中。

DeZero的im2col函数也

在dezero/functions_conv.py中。另外，dezero/

functions.py 中

导入了 dezero/functions_conv.py 中实

现的

DeZero 函数。

这

样用户就能

从dezero/functions.py导入所有

的函数。

现在

来看DeZero的im2col函数

，它有以下参

数。表57

1是对其

参数的

说明

。

im2col(x, kernel_size,

stride=1, pad=0, to_matrix=True)

表57-1

im2col函数的参

数

参数 类型

说明

x

Variable或ndarray 输入

数据

kernel_size int或(int,

int) 卷积

核大小

stride int或(int,

int) 步

幅

pad int或(int,

int) 填充

to_matrix bool

是

否变形为矩

阵

kernel_size参数可以

是int或(int, int)（元组）。如

果传来的值

是(int,

int)，那么第一

个元素对应

于高度，第二

个元素对应

于宽度；如果

只传int，

那么高

度和宽度是

同一个值。参

数stride和pad的类型

也一样。最后

的参数

to_matrix是标

志位，如果为

True，则指示函数

在取出应用

卷积核的区

域后

将其变

为矩阵（这样

就能以矩阵

的乘积进行

计算了）。

下面

来实际使用

这个im2col函数。

steps/step57.py

import numpy

as np

import dezero.functions

as F

x1 =

np.random.rand(1, 3, 7, 7)

col1 = F.im2col(x1, kernel_size=5,

stride=1, pad=0, to_matrix=True)

print(col1.shape)

第

5阶段　DeZero高级挑

战 422

x2 = np.random.rand(10, 3,

7, 7) # 10个数据

kernel_size = (5, 5)

stride = (1, 1)

pad = (0, 0)

col2 = F.im2col(x2, kernel_size,

stride, pad, to_matrix=True)

print(col2.shape)

运

行结果

(9, 75)

(90,

75)

上面

的代码展示

了两个例子

。第一个是形

状为(1, 3, 7,

7)的数据

，即

数据的批

量大小为1，通

道数为3，高为

7，宽为7。第二个

例子是将第

一个

例子的

批量大小增

加到10的情况

。分别对两个

数据集应用

im2col函数后，

第二

维的元素数

都变为75。这与

卷积核的元

素数（通道数

为3，大小为(5,

5)）

一

致。另外，批量

大小为1的im2col的

结果大小为

(9, 75)。第二个例子

的批

量大小

为10，所以结果

大小为(90,

75)，是第

一个例子的

10倍。

接下来使

用im2col函数来实

现进行卷积

运算的DeZero的函

数。在此之前

，

首先实现工

具函数pair(x)。

dezero/utils.py

def pair(x):

 if

isinstance(x, int):

 return

(x, x)

 elif

isinstance(x, tuple):

 assert

len(x) == 2

return x

 else:

raise ValueError

如果

参数x是int，函数

pair(x)将返回(x, x)。如果

x是有两个元

素的元组，

pair(x)则

将其按原样

返回。在使用

这个函数的

情况下，不管

输入是int还是

(int, int)，我们都可以

得到具有两

个元素的元

组。示例如下

所示。

步骤 57

conv2d函

数和pooling函数  423

from

dezero.utils import pair

print(pair(1))

print(pair((1, 2)))

运

行结果

(1,

1)

(1, 2)

下面

实现进行卷

积运算的函

数conv2d_simple（将以下代

码添加到dezero/

functions_conv.py，而

不是dezero/functions.py中）。

dezero/functions_conv.py

from dezero.utils

import pair, get_conv_outsize

def

conv2d_simple(x, W, b=None, stride=1,

pad=0):

 x, W

= as_variable(x), as_variable(W)

Weight = W #

为了

避免W（Width和Weight）冲突

N, C, H,

W = x.shape

OC, C, KH, KW

= Weight.shape

 SH,

SW = pair(stride)

PH, PW = pair(pad)

OH = get_conv_outsize(H, KH,

SH, PH)

 OW

= get_conv_outsize(W, KW, SW,

PW)

 col =

im2col(x, (KH, KW), stride,

pad, to_matrix=True) # ①

Weight = Weight.reshape(OC, -1).transpose()

# ②

 t

= linear(col, Weight, b)

# ③

 y

= t.reshape(N, OH, OW,

OC).transpose(0, 3, 1, 2)

# ④

 return

y

上面代码中

重要的部分

用阴影表示

。①处使用im2col展开

输入数据，②处

将卷积核（Weight）像

图572那样并排

展开为一列

。通过在Weight.reshape(OC,

-1)中指

定1可以做到

这一点，这是

reshape函数的一个

便利的功能

。如果

reshape函数的

参数被指定

为1，在保持多

维数组元素

数量不变的

情况下，元

素

会被降维合

并到一起。例

如，形状为(10, 3, 5, 5)的

数组的元素

数是750，

对这个

数组使用reshape(10, -1)，它

将变换为形

状是(10, 75)的数组

。

第 5阶段　DeZero高级

挑战 424

③处计算

矩阵的乘积

，这一行使用

了用于线性

变换的linear函数

进行

包含偏

置的计算。最

后的④处将输

出变换为合

适的形状。在

变换时使用

了

DeZero的transpose函数。步

骤38中曾经介

绍过transpose函数可

以用来交

换

张量的轴的

顺序，这里我

们以图57

3的形

式改变轴的

顺序。

(N, OH, OW,

OC)

transpose

形状：

(0,

1, 2, 3)

(N,

OC, OH, OW)

(0,

3, 1, 2) 索

引：

图573 通过transpose函

数交换轴的

顺序

以上就

是conv2d_simple函数的实

现。由于我们

在实现卷积

运算时使用

的

是此前实

现的DeZero函数，所

以它的反向

传播也可以

正确进行。例

如，我

们可以

按以下方式

使用conv2d_simple函数。

steps/step57.py

N,

C, H, W =

1, 5, 15, 15

OC, (KH, KW) =

8, (3, 3)

x

= Variable(np.random.randn(N, C, H,

W))

W = np.random.randn(OC,

C, KH, KW)

y

= F.conv2d_simple(x, W, b=None,

stride=1, pad=1)

y.backward()

print(y.shape)

print(x.grad.shape)

运

行结果

(1, 8,

15, 15)

(1, 5,

15, 15)

上面

的代码成功

地执行了卷

积运算。这里

展示的卷积

运算采用了

普通的

实现

方式（因此函

数也被命名

为conv2d_simple），更好的实

现方式是实

现继承

Function类的

Conv2d类。Conv2d类和conv2d函数

的代码在dezero/functions_

conv.py中

。感兴趣的读

者可以查阅

。

步骤 57

conv2d函数和

pooling函数  425

A

Conv2d类在正

向传播阶段

使用im2col方法，并

通过张量积

进行计算。另

外，

通过im2col展开

的二阶张量

（这里称之为

col）在使用后会

立即从内存

中

删除（因为

col非常大，会占

用很多内存

)。之后的反向

传播阶段通

过转置

卷积

A

来进行计算

。

57.3 Conv2d层的实现

接

下来实现作

为层的Conv2d类（不

是函数）。代码

如下所示。

dezero/layers.py

class Conv2d(Layer):

def __init__(self, out_channels, kernel_size,

stride=1,

 pad=0, nobias=False,

dtype=np.float32, in_channels=None):

 super().__init__()

self.in_channels = in_channels

self.out_channels = out_channels

self.kernel_size = kernel_size

self.stride = stride

self.pad = pad

self.dtype = dtype

self.W = Parameter(None, name='W')

if in_channels is not

None:

 self._init_W()

if nobias:

 self.b

= None

 else:

self.b = Parameter(np.zeros(out_channels, dtype=dtype),

name='b')

 def _init_W(self,

xp=np):

 C, OC

= self.in_channels, self.out_channels

KH, KW = pair(self.kernel_size)

scale = np.sqrt(1 /

(C * KH *

KW))

 W_data =

xp.random.randn(OC, C, KH, KW).astype(self.dtype)

* scale

 self.W.data

= W_data

A 转

置卷积也称

为逆卷积，它

进行的是卷

积的逆变换

。

第 5阶段　DeZero高级

挑战

426

 def forward(self,

x):

 if self.W.data

is None:

 self.in_channels

= x.shape[1]

 xp

= cuda.get_array_module(x)

 self._init_W(xp)

y = F.conv2d_simple(x, self.W,

self.b, self.stride, self.pad)

# 或者 y =

F.conv2d(x, self.W, self.b, self.stride,

self.pad)

 return y

上

面的代码首

先继承了Layer类

，然后实现了

Conv2d类。初始化时

接收

表572中的

参数。

表57-2

Conv2d类的

初始化参数

参数 类型 说

明

out_channels

Int 输出数据

的通道数

kernel_size int或

(int,

int) 卷积核大小

stride int或(int, int)

步幅

pad int或(int, int)

填

充

nobias bool 是否使用

偏置

dtype numpy.dtype 权重的

初始化数据

类型

in_channels

int或None 输入

数据的通道

数

表572中需要

注意的是in_channels的

默认值是None。如

果它是None，

in_channels的值

将从forward(x)中的x的

形状中获得

，同时权重数

据也会被

初

始化。具体做

法与全连接

层的Linear层相同

。

主处理用到

了刚刚实现

的函数conv2d_simple（或conv2d）。以

上是Conv2d

层的实

现。

57.4 pooling函数的实

现

最后实现

pooling函数。与conv2d_simple函数

一样，这个函

数也使用im2col

展

开输入数据

。不过与卷积

层不同的是

，在池化时各

通道方向是

独立的。也

就

是说，池化的

应用区域按

通道单独展

开。具体如图

574所示。

步骤 57

conv2d函

数和pooling函数  427

1

2 0 1

0

0 4 2

7

2 0 1

3

0 2 4

6

5 4 3

1

2 0 4

1

0 3 2

3

0 2 3

3

0 4 2

4

2 0 1

1

0 3 1

6

2 4 5

1

2 0 1 0

0 4 2 7

2 0 1

3

0 2 4 6

5 4 3 1

2 0 4

1

0 3 2 3

0 2 3 3

0 4 2

4

2 0 1 1

0 3 1 6

2 4 5

7

2 1 2

4

2

5

0 0

6 5

3

0

1

1

输

入数据

2

0 1

3

2

0

4

1 0

4 2

3 2

0 1

W

H

C

KH*KW

C*KH*KW

OH*OW*C

OH*OW

reshape im2col

图57

4 对

输入数据展

开池化应用

区域（2 × 2池化的

例子）

如果以

这种方式展

开数据，之后

只要针对展

开后的矩阵

求出其中每

一行

的最大

值，将结果变

形为合适的

形状即可。这

个过程如图

575所示。

max

7 4

2 4

4 6

4 3

4 6

3 4

输出数

据

7 2 1

2

4

2

5

0 0 6 5

3

0

1

1

输入数据

展开 reshape 2

0

1

3

2

0

4

1 0 4

2

3 2 0

1

2

4

7

4

6

4

3

3

4

4

3

6

1 2 0

1

0 0 4

2

7 2 0

1

3 0 2

4

6 5 4

3

1 2 0

4

1 0 3

2

3 0 2

3

3 0 4

2

4 2 0

1

1 0 3

1

6 2 4

5

图575 pooling函数

的实现流程

（池化应用区

域中数值最

大的元素用

阴影表示）

以

上就是pooling函数

的实现流程

。代码如下所

示。

  第

5阶段　DeZero高

级挑战 428

dezero/functions_conv.py

def pooling_simple(x, kernel_size, stride=1,

pad=0):

 x =

as_variable(x)

 N, C,

H, W = x.shape

KH, KW = pair(kernel_size)

PH, PW = pair(pad)

SH, SW = pair(stride)

OH = get_conv_outsize(H, KH,

SH, PH)

 OW

= get_conv_outsize(W, KW, SW,

PW)

 col =

im2col(x, kernel_size, stride, pad,

to_matrix=True) # ①展开

col

= col.reshape(-1, KH *

KW)}

 y =

col.max(axis=1) # ②最大值

y = y.reshape(N, OH,

OW, C).transpose(0, 3, 1,

2) # ③变换

return

y

pooling函数（准确来

说是pooling_simple函数）的

实现分3步进

行：①处

展开输

入数据，②处计

算每一行的

最大值，③处变

换为合适的

输出大小。

在

计算最大值

时用到了DeZero的

max函数。这个max函

数指定的参

数可以

与NumPy的

np.max的参数相同

。上面的代码

通过指定axis参

数来计算

每

个指定的轴

上的最大值

。

以上就是pooling函

数的代码。从

代码中可以

看出，只要将

输入数据展

开为易于进

行池化的形

式，后续的实

现就会变得

非常简单。

步

骤

58 具有代表

性的CNN（VGG16）  429

步骤58

具

有代表性的

CNN（VGG16）

我们在上一

个步骤实现

了Conv2d层和pooling函数

。在本步骤，我

们将

使用它

们来实现一

个著名的模

型VGG16，同时使用

训练后的权

重进行推理

。

58.1 VGG16的实现

VGG（参考

文献[36]）是在2014年

的ILSVRC比赛中获

得亚军的模

型。在

参考文

献[36]中的文章

中，作者通过

改变模型中

使用的层数

等方式提出

了几种

变体

，这里我们将

实现其中一

个名为VGG16的模

型，其网络构

成如图581所示

。

图581

VGG16的网络构

成（图中省略

了激活函数

ReLU） 3x3 Conv 64

3x3 Conv 64 3x3

Conv 128 3x3 Conv

128 3x3 Conv 256

3x3 Conv 256 3x3

Conv 256

3x3 Conv

512

3x3 Conv 512

3x3 Conv 512

Linear

4096

Linear 1000

Linear

4096

pool/2

pool/2

pool/2

pool/2

3x3 Conv 512

3x3 Conv 512

3x3

Conv 512

pool/2

dropout

dropout

  第

5阶段　DeZero高级

挑战 430

图58

1中的

“3 × 3 conv

64”表示卷积核

大小为3 × 3，输出

通道数为

64。另

外，“pool/2”表示2

× 2的池

化，“linear 4096”表示输出

大小为4096

的全

连接层。VGG16有以

下几个特点

。

 使用3 × 3的卷积

层（填充为1

× 1）

 卷

积层的通道

数量（基本上

）在每次池化

后变为原来

的2倍（64→128→

256→512）

 在全连

接层使用Dropout



使

用ReLU作为激活

函数

现在参

照图581来实现

VGG16。代码如下所

示。

dezero/models.py

import dezero.functions as F

import dezero.layers as L

class VGG16(Model):

 def

__init__(self):

 super().__init__()

# ①只指定输

出的通道数

self.conv1_1 =

L.Conv2d(64, kernel_size=3, stride=1, pad=1)

self.conv1_2 = L.Conv2d(64, kernel_size=3,

stride=1, pad=1)

 self.conv2_1

= L.Conv2d(128, kernel_size=3, stride=1,

pad=1)

 self.conv2_2 =

L.Conv2d(128, kernel_size=3, stride=1, pad=1)

self.conv3_1 = L.Conv2d(256, kernel_size=3,

stride=1, pad=1)

 self.conv3_2

= L.Conv2d(256, kernel_size=3, stride=1,

pad=1)

 self.conv3_3 =

L.Conv2d(256, kernel_size=3, stride=1, pad=1)

self.conv4_1 = L.Conv2d(512, kernel_size=3,

stride=1, pad=1)

 self.conv4_2

= L.Conv2d(512, kernel_size=3, stride=1,

pad=1)

 self.conv4_3 =

L.Conv2d(512, kernel_size=3, stride=1, pad=1)

self.conv5_1 = L.Conv2d(512, kernel_size=3,

stride=1, pad=1)

 self.conv5_2

= L.Conv2d(512, kernel_size=3, stride=1,

pad=1)

 self.conv5_3 =

L.Conv2d(512, kernel_size=3, stride=1, pad=1)

self.fc6 = L.Linear(4096) #

②只指定输出

的大小

 self.fc7 =

L.Linear(4096)

 self.fc8 =

L.Linear(1000)

 def forward(self,

x):

 x =

F.relu(self.conv1_1(x))

 x =

F.relu(self.conv1_2(x))

步骤

58 具有代表性

的CNN（VGG16）

431

 x =

F.pooling(x, 2, 2)

x = F.relu(self.conv2_1(x))

x = F.relu(self.conv2_2(x))

x = F.pooling(x, 2,

2)

 x =

F.relu(self.conv3_1(x))

 x =

F.relu(self.conv3_2(x))

 x =

F.relu(self.conv3_3(x))

 x =

F.pooling(x, 2, 2)

x = F.relu(self.conv4_1(x))

x = F.relu(self.conv4_2(x))

x = F.relu(self.conv4_3(x))

x = F.pooling(x, 2,

2)

 x =

F.relu(self.conv5_1(x))

 x =

F.relu(self.conv5_2(x))

 x =

F.relu(self.conv5_3(x))

 x =

F.pooling(x, 2, 2)

x = F.reshape(x, (x.shape[0],

-1))} # ③变形

x = F.dropout(F.relu(self.fc6(x)))

x = F.dropout(F.relu(self.fc7(x)))

x = self.fc8(x)

return x

代码

很长，但结构

很简单。初始

化方法创建

需要的层，forward方

法使

用层和

函数来进行

处理。接下来

补充说明上

面代码中标

记的3处。

首先

①处在创建卷

积层时没有

指定输入数

据的通道数

。输入数据的

通道

数是从

正向传播的

数据流中获

得的，同时权

重参数也会

被初始化。②处

的

L.Linear(4096)同样只指

定了输出大

小。输入的大

小是由实际

流入的数据

自

动确定的

，所以②处只指

定输出大小

即可。

最后③处

为了从卷积

层切换到全

连接层对数

据进行了变

形。卷积层处

理

的是四阶

张量，全连接

层处理的是

二阶张量。因

此，在向全连

接层传播数

据

之前，要使

用reshape函数将数

据变形为二

阶张量。以上

就是VGG16类的实

现。

58.2

已训练的

权重数据

VGG16是

在大型数据

集ImageNet上训练的

，训练后的权

重数据已开

放

下载。这里

向刚才实现

的VGG16类中添加

用于加载已

训练的权重

数据的函数

。

第 5阶段　DeZero高级

挑战 432

VGG16模型基

于Creative Commons Attribution许可协议

开放下载。另

外，

为了使DeZero能

够读取模型

原始的权重

数据而对该

模型施加了

微小修改

的

权重文件可

从GitHub上获得。

向

VGG16类添加的代

码如下所示

。

dezero/models.py

from

dezero import utils

class

VGG16(Model):

 WEIGHTS_PATH =

'https://github.com/koki0702/dezero-models/' \

 'releases/download/v0.1/vgg16.npz'

def __init__(self, pretrained=False ):

...

if pretrained:

 weights_path

= utils.get_file(VGG16.WEIGHTS_PATH)

 self.load_weights(weights_path)

上面的代码

在VGG16类的初始

化方法中添

加了参数pretrained=False。如

果参数为True，则

从指定位置

下载并读取

权重文件（DeZero专

用的转换过

的

权重文件

）。加载权重文

件是步骤53中

增加的功能

。

dezero/utils.py中有一个get_file函

数。该函数从

指定的URL下载

文件，

然后返

回下载文件

（在PC上）的绝对

路径。如果下

载的文件已

经在缓存目

录中，它会返

回该文件的

绝对路径。DeZero的

缓存目录是

~/.dezero。

以上就是VGG16类

的实现。VGG16类的

代码在dezero/models.py中。下

面

是使用已

训练的VGG16的实

例代码。

步骤

58 具有代表性

的CNN（VGG16）

433

import numpy as

np

from dezero.models import

VGG16

model = VGG16(pretrained=True)

x = np.random.randn(1, 3,

224, 224).astype(np.float32) # 虚拟数据

model.plot(x)

为了实现可

视化操作，上

面的代码还

创建了VGG16的计

算图。结果如

图582所示。

第 5阶

段　DeZero高级挑战

434

Linear

Div

Mul

(1, 4096)

bool

ReLU

Linear

Div

Mul

(1, 4096) bool

ReLU

Linear

Reshape

Pooling

ReLU

Conv2d

ReLU

Conv2d

ReLU

Conv2d

Pooling

ReLU

Conv2d

ReLU

Conv2d

ReLU

Conv2d

Pooling

ReLU

Conv2d

ReLU

Conv2d

ReLU

Conv2d

Pooling

ReLU

Conv2d

ReLU

Conv2d

Pooling

ReLU

Conv2d

ReLU

Conv2d

图582

VGG16的计算图

步骤 58 具有代

表性的CNN（VGG16）

435

58.3 使用

已训练的VGG16

接

下来使用已

训练的VGG16进行

图像识别。首

先要做的是

加载样本图

像。

import dezero

from PIL

import Image

url =

'https://github.com/oreilly-japan/deep-learning-from-scratch-3/' \

 'raw/images/zebra.jpg'

img_path = dezero.utils.get_file(url)

img

= Image.open(img_path)

img.show()

上面的代

码使用前面

介绍的dezero.utils.get_file函数

下载图像文

件，

然后使用

PIL包读取已下

载的图像。执

行上述代码

，会显示图583中

的图像。

图583

使

用PIL读取的样

本图像

PIL（Python Image Library）是一

个图像处理

库，针对图像

提供了读取

、

保存、转换等

功能。我们可

以通过pip install pillow来安

装PIL。

第 5阶段　DeZero高

级挑战 436

前面

代码中的img = Image.open(img_path)用

于读取图像

，img的类型是PIL.

Image。但

是DeZero处理的数

据是ndarray类型，因

此我们需要

使用执行转

换

处理的函

数。我们在DeZero的

VGG16类中准备一

个静态方法

preprocess。该

方法可按

以下方式使

用。

from models

import VGG16

x =

VGG16.preprocess(img)

print(type(x), x.shape)

运行结果

<class

'numpy.ndarray'> (3, 224, 224)

preprocess是静态方法

，它可以从类

调用，无须通

过实例。它接

收的参

数是

PIL.Image类型的数据

。内部所做的

处理是将数

据调整为高

224、宽224

的大小，然

后将其转换

为ndarray实例。这里

的大小(224, 224)是VGG16

的

输入图像的

大小。VGG16.preprocess方法也

做了一些其

他的预处理

，如

按照BGR的顺

序排列颜色

通道、减去固

定值等。这些

是用 VGG16训练

ImageNet时

要做的预处

理。

在使用已

训练的权重

数据推断未

知数据时，需

要做与训练

模型时相同

的预

处理，否

则输入到模

型的数据将

是不同的，模

型无法正确

进行识别。

准

备工作已经

完成。下面使

用已训练的

VGG16进行分类。以

下是step/

step58.py中的所

有代码。

steps/step58.py

import numpy as

np

from PIL import

Image

import dezero

from

dezero.models import VGG16

步骤

58

具有代表性

的CNN（VGG16）  437

url

= 'https://github.com/oreilly-japan/deep-learning-from-scratch-3/' \

'raw/images/zebra.jpg'

img_path = dezero.utils.get_file(url)

img = Image.open(img_path)

x

= VGG16.preprocess(img)

x =

x[np.newaxis] # 增加用于

小批量处理

的轴

model

= VGG16(pretrained=True)

with dezero.test_mode():

y = model(x)

predict_id

= np.argmax(y.data)

model.plot(x, to_file='vgg.pdf')

# 计算图

的可视化

labels =

dezero.datasets.ImageNet.labels() # ImageNet的

标签

print(labels[predict_id])

运行结

果

zebra

上面的代

码首先读取

图像，进行预

处理，然后在

数据的前面

添加用于小

批量的轴。这

会使x的形状

由(3, 224,

224)变为(1, 3, 224, 224)。之后

将

数据传给

VGG16，让它进行推

理。输出层（1000个

类别）中数值

最大的索引

就是模型分

类的结果。

另

外，dezero/datasets.py中还准备

了ImageNet的标签（键

为对象ID、

值为

标签名的字

典）。使用它可

以从对象ID中

取出标签名

称。结果zebra（斑马

）

表明图像被

正确识别了

出来。到这里

就完成了VGG16的

实现。

除VGG16之外

，dezero/models.py中还有其他

著名的模型

，如ResNet（参

考文献

[37])和SqueezeNet（参考文献

[38]）。感兴趣的读

者可以参考

。

第 5阶段　DeZero高级

挑战 438

步骤59

使

用RNN处理时间

序列数据

我

们此前见到

的神经网络

是具有前馈

（feed forward）结构的网络

。前馈

指信号

向一个方向

前进，它的特

点是输出只

取决于输入

。而RNN（循环神经

网络）是具有

如图591所示的

循环结构的

网络。

图591

RNN的结

构

RNN x h

图591中的循

环结构使RNN的

输出前馈到

自身。所以，RNN网

络拥有

“状态

”。也就是说，当

数据输入到

RNN时，状态被更

新，输出由状

态决定。

本步

骤的主题是

RNN。RNN

在计算上比

前馈网络更

复杂。但有了

DeZero，如此复杂的

计算也能简

单地实现。在

本步骤，笔者

将结合RNN的

实

现来介绍RNN的

原理。

59.1 RNN层的实

现

首先使用

式子来介绍

RNN。我们以输入

为时间序列

数据xt、输出为

隐

藏状态ht的

RNN为例进行思

考。这里的t指

的是时间序

列数据的时

间t（或

步骤 59

使

用RNN处理时间

序列数据  439

第

t个）。另外，由于

RNN的状态被称

为隐藏状态

（hidden

state），所以在式

子

中用h来表示

RNN的状态。下面

是RNN的正向传

播的式子。

ht =

tanh(ht−1Wh+xtWx+b) （59.1）

先

来看一下式

子59.1中的符号

。RNN有两个权重

：一个是权重

Wx，

用于将输入

x转换为隐藏

状态h；另一个

是权重Wh，用于

将前一个时

刻的

RNN的输出

转换为下一

个时刻的输

出。另外还有

偏置b。这里的

ht−1和xt

是行向量

。

式子59.1中首先

进行矩阵乘

积的计算，然

后使用tanh函数

（双曲正切函

数）

对它们的

和进行转换

。其结果是时

刻t的输出ht。这

个ht既用在其

他层，

也用在

下一个时刻

的RNN层（自身）。

下

面来实现DeZero的

RNN层。按照之前

的方法进行

操作，RNN层继承

于

Layer类，它的forward方

法中是正向

传播的处理

。下面是RNN层的

代码（将此

代

码添加到dezero/layers.py中

）。

dezero/layers.py

class RNN(Layer):

def __init__(self, hidden_size, in_size=None):

super().__init__()

 self.x2h =

Linear(hidden_size, in_size=in_size)

 self.h2h

= Linear(hidden_size, in_size=in_size, nobias=True

)

 self.h =

None

 def reset_state(self):

self.h = None

def forward(self, x):

if self.h is None:

h_new = F.tanh(self.x2h(x))

else:

 h_new =

F.tanh(self.x2h(x) + self.h2h(self.h))

self.h = h_new

return h_new

第 5阶段　DeZero高级

挑战 440

首先，初

始化方法__init__接

收hidden_size和in_size。如果in_size为

None，则意味着只

指定了隐藏

层的大小。在

这种情况下

，输入大小会

自动从

正向

传播时传来

的数据中获

得。__init__方法创建

以下两个线

性层。

 x2h：将输入

x转换为隐藏

状态h的全连

接层

 h2h：将上一

个隐藏状态

转换为下一

个隐藏状态

的全连接层

之后forward方法的

实现中根据

self.h（隐藏状态）是

否存在来切

换处

理。第1次

self.h ==

None，所以只能从

输入x计算隐

藏状态。从第

2次之后，

forward方法

使用以前的

隐藏状态（self.h）计

算新的隐藏

状态。另外，RNN层

中准备了一

个叫reset_state的方法

，这是用于重

置隐藏状态

的方法。

如式

子59.1所示，RNN的偏

置只有一个

。因此，我们只

使用x2h的偏置

，

省略h2h（Linear层）的偏

置（上面的代

码将h2h初始化

为nobias=True）。

现在向上

面的RNN层传入

实际的数据

。尝试运行下

面的代码。

import numpy as

np

import dezero.layers as

L

rnn = L.RNN(10)

# 只

指定隐藏层

的大小

x =

np.random.rand(1, 1)

h =

rnn(x)

print(h.shape)

运行

结果

(1,

10)

上面的

代码创建的

是虚拟的数

据，即形状为

(1, 1)的x。这表示批

量大

小为1（即

有一个数据

），数据的维度

为1。将这个输

入x传给rnn，可得

隐藏

状态h。此

时的计算图

如图592所示。

步

骤 59

使用RNN处理

时间序列数

据  441

x

h

x2h

tanh

图59

2 第一次

传x时的计算

图（x2h是Linear层）

接下

来继续传数

据。假设紧接

着前面的代

码运行y =

rnn(np.random.rand 

(1, 1))，此时

的计算图如

图59

3所示。

x

h

x2h

x

x2h

tanh h

tanh h2h +

图59

3 处

理完第2个输

入数据后的

计算图

图593是

在图59

2的计算

图的基础上

“成长”起来的

计算图。正是

RNN

的隐藏状态

使得这种“成

长”成为可能

。通过使用之

前保存的隐

藏状态，

RNN的计

算图与之前

的计算图建

立了连接。

如

图59

3所示，RNN创建

了包含所有

输入数据的

计算图。因此

，RNN可

以学习输

入数据之间

的“关系”。在图

593中出现了2个

x2h实例，但它们

是同一个Linear实

例，而且使用

了相同的权

重。

第 5阶段　DeZero高

级挑战 442

59.2 RNN模型

的实现

下面

使用RNN层来实

现神经网络

（模型）。这里要

用到将RNN层的

隐藏状

态转

换为输出的

Linear层。代码如下

所示，类名为

SimpleRNN。

steps/step59.py

from dezero import

Model

import dezero.functions as

F

import dezero.layers as

L

class SimpleRNN(Model):

def __init__(self, hidden_size, out_size):

super().__init__()

 self.rnn =

L.RNN(hidden_size)

 self.fc =

L.Linear(out_size)

 def reset_state(self):

self.rnn.reset_state()

 def forward(self,

x):

 h =

self.rnn(x)

 y =

self.fc(h)

 return y

上面的代码

向Linear层添加了

实例变量fc。这

个Linear层接收RNN层

的隐藏状态

并计算输出

。另外，上面的

模型使用reset_state方

法重置RNN

层的

隐藏状态。现

在试着用这

个模型进行

训练。这里使

用均方误差

（mean_

squared_error函数）作为损

失函数。求梯

度的代码如

下所示。

seq_data

= [np.random.randn(1, 1) for

_ in range(1000)] #

虚拟

的时间序列

数据

xs = seq_data[0:-1]

ts = seq_data[1:] #

xs的下一

个时间步的

数据

model = SimpleRNN(10,

1)

loss, cnt =

0, 0

for x,

t in zip(xs, ts):

y = model(x)

loss += F.mean_squared_error(y, t)

步骤 59 使

用RNN处理时间

序列数据

443

 cnt +=

1

 if cnt

== 2:

 model.cleargrads()

loss.backward()

 break

首

先创建虚拟

的时间序列

数据seq_data。我们想

训练的是基

于该时间序

列数据预测

下一个时间

步的数据的

模型。为此需

要保存训练

数据，即输入

数

据的下一

个时间步的

数据。

然后是

关键的反向

传播，上面的

代码只是一

个示例，它在

第2个输入数

据到来时进

行反向传播

。传第2个输入

数据时的计

算图如图594所

示。

创建了如

图

594 所示的计

算图后，我们

就可以通过

loss.backward()

求出每个参

数的梯度。这

种在由一系

列的输入数

据组成的计

算图上进行

的

反向传播

叫作基于时

间的反向传

播（Backpropagation Through Time，

BPTT），它表示回

溯时间进行

反向传播。

RNN可

以学到输入

数据的排列

方式的模式

。数据排列的

“顺序”对应于

时

间序列数

据的“时间”。因

此，Backpropagation Through Time

中用到了

Time（时间）一词。

  第

5阶段

DeZero高级挑

战 444

x

h

y t

x2h

loss

fc

mean_squared_error

y t

fc

mean_squared_error

x

x2h

tanh h tanh h2h

+

+

图594

应用损

失函数后的

计算图

图594是

有两个输入

数据时的计

算图。当然，输

入数据可以

是10个、

100个，甚至

是任何数量

。计算图将根

据输入数据

的数量变长

。不过为了更

好地执行反

向传播，计算

图到一定的

长度后需要

“截断”，这就是

Truncated

BPTT（truncate的意思是“截

断”或“切断”）。在

上面的例子

中，截断发生

在两个输入

数据处。

在进

行Truncated BPTT时，需要注

意保留RNN的隐

藏状态。我们

可

以思考一

下对图59

4的计

算图进行反

向传播后，传

入下一个输

入数据的情

况。

在此情况

下，RNN的隐藏状

态需要从前

一个隐藏状

态开始。具体

如图595所示。

步

骤

59 使用RNN处理

时间序列数

据  445

h

y t

loss

fc

mean_squared_error

y t

fc

mean_squared_error

x

x2h

h tanh h2h +

h

x

x2h

tanh

h2h +

+

从上一个

隐藏状态开

始

图595 在下一

次迭代中创

建的计算图

图595中的第一

个隐藏状态

从上一个隐

藏状态开始

。对于该隐藏

状态的

变量

，我们需要切

断计算上的

连接。这样一

来，梯度将不

再向上一次

训练的

计算

图传播（这就

是“截断的BPTT”）。

59.3 切

断连接的方

法

接下来在

Variable类中添加用

于切断连接

的方法。Variable类位

于

dezero/core.py中。我们在

Variable类中添加下

面的unchain方法。

dezero/core.py

class

Variable:

 ...

def unchain(self):

 self.creator

= None

unchain方

法只是将创

造者self.creator设为None。这

样就切断了

与作为

创造

者的函数之

间的连接。

随

后我们再增

加用于切断

连接的方法

。这个方法是

unchain_backward。

  第

5阶段　DeZero高级

挑战 446

当这个

方法被调用

时，它会被调

用的变量反

向回溯计算

图，并调用图

中出现

的所

有变量的unchain方

法。代码如下

所示。

dezero/core.py

class Variable:

...

 def unchain_backward(self):

if self.creator is not

None:

 funcs =

[self.creator]

 while funcs:

f = funcs.pop()

for x in f.inputs:

if x.creator is not

None:

 funcs.append(x.creator)

x.unchain()

代码实

现中反向回

溯变量和函

数，并调用变

量的unchain方法。这

个逻

辑与Variable类

的backward方法的逻

辑相同。但由

于这里不用

考虑回溯变

量

的顺序（变

量的“辈分”），所

以这个实现

比backward方法的代

码更简单。

59.4 正

弦波的预测

基于以上实

现，我们来尝

试训练RNN。这里

将包含噪声

的正弦波作

为

数据集使

用。我们可以

使用dezero/datasets.py中的SinCurve类

加载数据集

，

代码如下所

示。

import numpy as np

import dezero

import matplotlib.pyplot

as plt

train_set =

dezero.datasets.SinCurve(train=True)

print(len(train_set))

print(train_set[0])

print(train_set[1])

print(train_set[2])

步骤 59 使用

RNN处理时间序

列数据

447

# 绘制

图形

xs

= [example[0] for example

in train_set]

ts =

[example[1] for example in

train_set]

plt.plot(np.arange(len(xs)), xs, label='xs')

plt.plot(np.arange(len(ts)), ts, label='ts')

plt.show()

运行结

果

999

(array([-0.03458701]), array([0.01726473]))

(array([0.01726473]), array([0.04656735]))

(array([0.04656735]), array([0.03284844]))

上面的代

码输出了train_set的

第0个、第1个和

第2个数据。这

些数据

都是

元组，元组的

第1个元素是

输入数据，第

2个元素是训

练数据（标签

）。另外，

上面的

代码还可以

绘制出图596这

样的图形。

1.0

0.5

0.0

−0.5

−1.0

0 200 400

600 800 1000

图

59

6 正弦波数据

集的plot图

从图

596可以看出这

是一个有噪

声的正弦波

。图中实际上

绘制了xs和

第

5阶段　DeZero高级挑

战 448

ts这两组数

据，但我们只

能看到一条

曲线。其实ts是

xs的下一个时

间步的

数据

，因此，图596中绘

制的两条曲

线几乎是相

互重叠的。

正

弦波数据集

的训练数据

是输入数据

的未来一个

时间步的数

据。以上面的

代码为例，二

者的关系是

xs[1:]

== ts[:-1]。这样的数据

集可以用于

预测时间序

列数据的问

题，即基于过

去的数据预

测下一个数

据的问题。

下

面用RNN来训练

正弦波数据

集，代码如下

所示（省略了

导入部分）。

steps/step59.py

# 设

置超参数

max_epoch =

100

hidden_size = 100

bptt_length = 30 #

BPTT的

长度

train_set = dezero.datasets.SinCurve(train=True)

seqlen = len(train_set)

model

= SimpleRNN(hidden_size, 1)

optimizer

= dezero.optimizers.Adam().setup(model)

# 训练开

始

for epoch in range(max_epoch):

model.reset_state()

 loss, count

= 0, 0

for x, t in

train_set:

 x =

x.reshape(1, 1) # ①形状转换

为(1,

1)

 y =

model(x)

 loss +=

F.mean_squared_error(y, t)

 count

+= 1

 #

②调整Truncated BPTT的时

机

 if

count % bptt_length ==

0 or count ==

seqlen: 

 model.cleargrads()

loss.backward()

 loss.unchain_backward() #

③切断连接

optimizer.update()

 avg_loss

= float(loss.data) / count

print('| epoch %d |

loss %f' % (epoch

+ 1, avg_loss))

步骤

59 使用RNN处

理时间序列

数据  449

下面只

对以上代码

补充3点内容

。首先①处将x的

形状转换为

(1, 1)。

DeZero神经网络的

输入数据必

须是二阶张

量或四阶张

量（在使用CNN时

）。

因此，即使输

入数据只有

一个，也必须

把它转换为

(1,

1)。

②处判断调用

backward方法的时机

——要么在数据

流转了30次之

后，要

么在到

达数据集的

终点（末尾）时

。最后③处通过

unchain_backward方法切断

RNN隐

藏状态的连

接。

通过调用

loss.backward_unchain()，从loss开始回溯

，切断所有出

现

的变量的

连接。因此，RNN的

隐藏状态的

连接也被切

断。

运行上面

的代码可知

，损失（loss）在稳步

下降。下面我

们来使用一

下训

练后的

模型。这次将

新的（无噪音

的）余弦波作

为输入数据

，并尝试预测

下

一步的数

值。代码如下

所示，结果如

图597所示。

steps/step59.py

import

matplotlib.pyplot as plt

xs

= np.cos(np.linspace(0, 4 *

np.pi, 1000))

model.reset_state() #

重置

模型

pred_list = []

with dezero.no_grad():

 for

x in xs:

x = np.array(x).reshape(1, 1)

y = model(x)

pred_list.append(float(y.data))

plt.plot(np.arange(len(xs)), xs, label='y=cos(x)')

plt.plot(np.arange(len(xs)), pred_list, label='predict')

plt.xlabel('x')

plt.ylabel('y')

plt.legend()

plt.show()

第 5阶段

DeZero高级挑战 450

x

y=cos(x)

predict

图

597

模型对新数

据的预测结

果（y=cos(x)）

从图597可以

看出，预测结

果还不错。不

过在目前的

实现中，数据

是

一个个处

理的（因为批

量大小是1），所

以处理时间

较长。如果增

大批量大小

，

每轮的处理

时间将会变

短。下一个步

骤中我们会

修改代码，使

数据可以作

为

小批量数

据一次性得

到处理，还会

使用LSTM层实现

更好的模型

。

y

步骤

60 LSTM与数据

加载器  451

步骤

60

LSTM与数据加载

器

上一个步

骤使用RNN进行

了时间序列

数据（正弦波

）的训练，本步

骤将

对上一

个步骤的代

码做两项改

进。

第一项改

进是编写用

于时间序列

数据的数据

加载器。上一

个步骤对一

个

数据（批量

大小为1的数

据）进行了模

型的正向传

播。本步骤将

使用针对时

间

序列数据

的数据加载

器，对由多个

数据组成的

小批量数据

进行正向传

播。

第二项改

进是使用LSTM层

来代替RNN层。LSTM层

的识别精度

更高。

最后在

完成这两项

改进之后，我

们再次尝试

训练正弦波

。

60.1 用于时间序

列数据的数

据加载器

上

一个步骤按

顺序依次取

出时间序列

数据（批量大

小为1）。本步骤

将把

多个数

据整理成小

批量数据进

行训练。为此

我们要创建

专用的数据

加载器。

为了

将时间序列

数据合并为

小批量数据

，在传递数据

时，我们可以

“偏

移”每个小

批量数据的

起始位置。假

设时间序列

数据由1000个数

据组成，我

们

要创建的是

大小为2的小

批量数据。在

这种情况下

，第一个样本

数据是从

时

间序列数据

的开头（第0个

）依次取出的

。第二个样本

数据则以第

500个数

据作为

起始位置，并

从该处依次

取出数据（起

始位置偏移

500）。

基于以上内

容，下面实现

用于时间序

列数据的数

据加载器。代

码如下所示

。

第 5阶段　DeZero高级

挑战 452

dezero/dataloaders.py

class SeqDataLoader(DataLoader):

def __init__(self, dataset, batch_size,

gpu=False):

 super().__init__(dataset=dataset, batch_size=batch_size,

shuffle=False,

 gpu=gpu)

def __next__(self):

 if

self.iteration >= self.max_iter:

self.reset()

 raise StopIteration

jump = self.data_size //

self.batch_size

 batch_index =

[(i * jump +

self.iteration) % self.data_size for

i in

 range(self.batch_size)]

batch = [self.dataset[i] for

i in batch_index]

xp = cuda.cupy if

self.gpu else np

x = xp.array([example[0] for

example in batch])

t = xp.array([example[1] for

example in batch])

self.iteration += 1

return x, t

首先修

改的是初始

化的部分。由

于数据重排

会改变数据

的顺序，所以

在

时间序列

数据的情况

下，设置shuffle=False。

在__next__方

法中，我们编

写了取出下

一个小批量

数据的代码

。重要的

部分

用阴影标出

。首先求偏移

量jump，然后将用

于取出每个

样本数据的

索引

的起始

位置设置为

batch_index，最后从数据

集self.dataset中取出数

据。

以上是用

于实现时间

序列数据的

数据加载器

的代码。下面

是这个

SeqDataLoader类的

使用示例。

train_set =

dezero.datasets.SinCurve(train=True)

dataloader = SeqDataLoader(train_set,

batch_size=3)

x, t =

next(dataloader)

print(x)

print('---------------')

print(t)

步

骤 60 LSTM与数据加

载器

453

运行结

果

[[-0.04725922]

[ 0.83577416]

 [-0.83650972]]

---------------

[[ 0.04529467]

[ 0.83116588]

 [-0.88256346]]

60.2 LSTM层的实现

接下来是第

二项改进。这

里我们会实

现LSTM层，并用它

来代替RNN层。

下

面用式子来

表示LSTM所做的

计算。

ft

= σ(xtWx

(f) +

ht−1Wh

(f) + b(f)

)

it = σ(xtWx

(i) + ht−1Wh

(i)

+ b(i)

)

ot

= σ(xtWx

(o) +

ht−1Wh

(o) + b(o)

)

ut = tanh(xtWx

(u) + ht−1Wh

(u)

+ b(u)

)

(60.1)

ct = ft

ct−1 + it

ut (60.2)

ht =

ot  tanh(ct)

(60.3)

上面的

式子是LSTM所做

的计算。LSTM除了

使用隐藏状

态h，还使用

记

忆单元c。式子

60.2和式子60.3中的

⊙是哈达玛积

的符号，表示

每对相应

元

素的乘积。对

式子的介绍

到此为止，下

面我们在DeZero中

实现上面的

式子。

本书只

介绍LSTM的主要

内容，更详细

的说明请参

考本书前作

《深度学习

进

阶：自然语言

处理》第6章的

内容。

下面在

DeZero中实现式子

60.1、式子60.2和式子

60.3。代码如下所

示。

第 5阶段　DeZero高

级挑战 454

dezero/layers.py

class LSTM(Layer):

def __init__(self, hidden_size, in_size=None):

super().__init__()

 H, I

= hidden_size, in_size

self.x2f = Linear(H, in_size=I)

self.x2i = Linear(H, in_size=I)

self.x2o = Linear(H, in_size=I)

self.x2u = Linear(H, in_size=I)

self.h2f = Linear(H, in_size=H,

nobias=self)

 self.h2i =

Linear(H, in_size=H, nobias=self)

self.h2o = Linear(H, in_size=H,

nobias=self)

 self.h2u =

Linear(H, in_size=H, nobias=self)

self.reset_state()

 def reset_state(self):

self.h = None

self.c = None

def forward(self, x):

if self.h is None:

f = F.sigmoid(self.x2f(x))

i = F.sigmoid(self.x2i(x))

o = F.sigmoid(self.x2o(x))

u = F.tanh(self.x2u(x))

else:

 f =

F.sigmoid(self.x2f(x) + self.h2f(self.h))

i = F.sigmoid(self.x2i(x) +

self.h2i(self.h))

 o =

F.sigmoid(self.x2o(x) + self.h2o(self.h))

u = F.tanh(self.x2u(x) +

self.h2u(self.h))

 if self.c

is None:

 c_new

= (i * u)

else:

 c_new =

(f * self.c) +

(i * u)

h_new = o *

F.tanh(c_new)

 self.h, self.c

= h_new, c_new

return h_new

上面

的代码虽然

有点多，但主

要的工作是

将LSTM的式子转

换为代码。

有

了DeZero，即使是LSTM的

复杂式子也

可以轻松实

现。最后再次

尝试训

练上

一个步骤的

正弦波。训练

代码如下所

示。

步骤 60 LSTM与数

据加载器

455

steps/step60.py

import numpy

as np

import dezero

from dezero import Model

from dezero import SeqDataLoader

import dezero.functions as F

import dezero.layers as L

max_epoch = 100

batch_size

= 30

hidden_size =

100

bptt_length = 30

train_set = dezero.datasets.SinCurve(train=True)

#

①使

用时间序列

数据的数据

加载器

dataloader = SeqDataLoader(train_set,

batch_size=batch_size)

seqlen = len(train_set)

class BetterRNN(Model):

 def

__init__(self, hidden_size, out_size):

super().__init__()

 self.rnn =

L.LSTM(hidden_size) # ②使用

LSTM

self.fc = L.Linear(out_size)

def reset_state(self):

 self.rnn.reset_state()

def forward(self, x):

y = self.rnn(x)

y = self.fc(y)

return y

model =

BetterRNN(hidden_size, 1)

optimizer =

dezero.optimizers.Adam().setup(model)

for epoch in

range(max_epoch):

 model.reset_state()

loss, count = 0,

0

 for x,

t in dataloader:

y = model(x)

loss += F.mean_squared_error(y, t)

count += 1

if count % bptt_length

== 0 or count

== seqlen:

 #

dezero.utils.plot_dot_graph(loss) # 绘制计算图



第 5阶段　DeZero高级

挑战 456

model.cleargrads()

 loss.backward()

loss.unchain_backward()

 optimizer.update()

avg_loss = float(loss.data) /

count

 print('| epoch

%d | loss %f'

% (epoch + 1,

avg_loss))

只有两

处与上一个

步骤不同。第

一处是使用

SeqDataLoader类创建数据

加载器；第二

处是使用LSTM层

设计模型。以

这种方式进

行训练，训练

速度会

比上

一个步骤的

更快。现在使

用训练好的

模型来对新

的数据（无噪

音的余弦波

）

进行预测。结

果如图60

1所示

。

x

y=cos(x)

predict

图601 使用了LSTM层

的模型的预

测结果

图60

1表

明预测结果

良好，比上一

个步骤的结

果（图597）的精度

更高。

我们已

经成功实现

了LSTM这种复杂

的层，并完成

了时间序列

数据处理这

一复

杂任务

。最后一起来

看一下由前

面的代码创

建的计算图

，计算图如图

60

2所示。

y

步骤 60

LSTM与

数据加载器

457

图602

使用LSTM模型

训练时间序

列数据时创

建的计算图

如图 602 所示，这

里创建的是

一个相当复

杂的计算图

。如果没有

DeZero这

样的框架，我

们很难创建

如此复杂的

计算图。DeZero的灵

活性使

得这

样一个复杂

的计算图的

创建工作变

得异常简单

。不管将来碰

到多么复杂

的计算，DeZero都可

以轻松解决

。



本书的60个步

骤到此就全

部结束了。到

达此处意味

着我们已经

实现了

创建

深度学习框

架的宏伟目

标。感谢大家

跟随我的脚

步走完了这

个漫长的旅

第 5阶段

DeZero高级

挑战 458

程。作为

本书的作者

，我很开心大

家能花这么

多时间读完

这么多页的

内容。

回顾这

段旅程，DeZero从最

开始的一只

小小的“箱子

”起步，一点点

地

扩展，在我

们解决各种

问题，进行各

种实验的同

时，DeZero也在不断

成长。

经过一

点点的积累

，DeZero已经成长为

一个优秀的

深度学习框

架。现在的

DeZero已

经具备了许

多现代框架

应该具备的

功能。

虽然本

书到此结束

，但我们还有

很多工作要

做。请大家继

续走下去，自

由地走下去

。无论是使用

从本书获得

的知识来创

建自己的原

创框架，还是

进

一步扩展

DeZero，或是改用PyTorch或

TensorFlow等框架，请尽

情享受新

的

旅程。后面的

专栏探讨了

DeZero未来的发展

方向，感兴趣

的读者可以

参考。

步骤 60

LSTM与

数据加载器

459

专栏：走向未

来

本专栏会

介绍几个未

来针对DeZero可做

的工作，其中

总结了笔者

想到的一

些

内容，如将来

如何扩展DeZero，作

为OSS（开源软件

）如何进行开

发，需要哪

些

材料等。此外

，本专栏还列

举了正文中

没有提到的

DeZero开发过程中

的故事（如

图

标的创作）等

。

增加函数和

层

本书实现

了许多DeZero的函

数和层。当然

，还有一些函

数和层尚未

实现。例如，

进

行张量积计

算的tensorDot函数和

用于批量正

则化的batchNorm函数

等。另外，

与其

他深度学习

框架比较一

下，我们也会

发现还有哪

些函数和层

尚未实现。例

如，

通过阅读

PyTorch的文档，可以

整理出DeZero中缺

少的函数。

提

高内存的使

用效率

提高

深度学习框

架的内存效

率是一个重

要的课题，尤

其是在大型

网络中，

由于

网络会使用

大量的内存

，所以深度学

习框架经常

会出现物理

内存不足的

问

题。在内存

的使用效率

方面，我们还

可以对DeZero做一

些改进。最重

要的改进

是

让DeZero保留所有

正向传播计

算的结果（数

据的ndarray实例），也

就是预想

反

向传播的计

算要用到这

些结果，在DeZero中

保留所有中

间计算的结

果。但是

有些

函数不需要

保留中间计

算的结果。例

如tanh函数就能

在没有正向

传播输入

的

情况下计算

反向传播，因

此，在这种情

况下，正向传

播的输入数

据应被立即

删除。考虑到

这一点，我们

可以设计一

种机制，根据

函数来决定

要保留的数

据。

Chainer和PyTorch其实已

经实现了这

一机制。感兴

趣的读者可

以参考Chainer

的Aggressive Buffer Release（参

考文献[24]）等。

第

5阶段　DeZero高级挑

战 460

静态计算

图与ONNX

DeZero 采 用

DefinebyRun（动

态 计

算 图）的

方 式 创

建 计

算 图，不 提

供

DefineandRun（静态计算图

）的方式。静态

计算图适用

于对性能有

要求的场景

，

而且静态计

算图在经过

编译（转换）后

，可以在非Python的

环境中运行

。考虑到

这些

，或许有些用

户希望DeZero也能

够运行静态

计算图。

另外

，在深度学习

领域，还有一

种叫作ONNX（参考

文献[40]）的数据

格式。

ONNX是用来

表示深度学

习模型的格

式，许多框架

支持该格式

。ONNX的优势

在于

训练好的模

型可以轻松

移植到其他

框架。如果DeZero也

支持ONNX，它就

可

以与各种框

架联动，通用

性会更强。

发

布到PyPI

软件开

发结束之后

，为了让用户

使用，我们需

要将代码汇

总成包发布

。在

Python领域，常用

的软件库是

PyPI（Python

Package Index）。软件包发布

到

PyPI后，用户就

可以使用pip install

...命

令来安装软

件包，这样任

何人都

可以

轻松使用它

。

DeZero已经发布到

PyPI了。网上有很

多介绍如何

将代码发布

到PyPI的文章，

大

家可自行参

考。笔者也欢

迎大家基于

本书的DeZero开发

自己的原创

框架，并

发布

到网上供世

人使用。有机

会请试一试

。

准备文档

在

发布框架（或

库）时，提供文

档会方便用

户使用。许多

有名的框架

提供了

关于

如何使用其

函数、类等（API）的

文档。

Python 提供了

docstring（文档字符串

）方案。docstring是为Python函

数

或类等编

写的说明文

字（注释），需要

以固定格式

编写在代码

中。

DeZero的实际代

码中也有写

好的docstring。例如dezero/cuda.py中

as_

cupy函数的代码

，具体如下所

示。

步骤 60 LSTM与数

据加载器

461

def as_cupy(x):

"""Convert to



cupy.ndarray



.

 Args:

x (



numpy.ndarray



or



cupy.ndarray



): Arbitrary object that

can be

 converted

to



cupy.ndarray



.

 Returns:



cupy.ndarray



: Converted array.

"""

 if isinstance(x,

Variable):

 x =

x.data

 if not

gpu_enable:

 raise Exception('CuPy

cannot be loaded. Install

CuPy!')

 return cp.asarray(x)

上面代码中

的注释部分

包含了函数

的基本信息

、参数类型、返

回值类型等

的

说明。说明

风格有NumPy风格

和Google风格等几

种比较有名

的风格。DeZero

采用

了Google风格。上面

的说明能帮

助读者理解

函数。当然，这

些说明也可

以

用母语来

写（考虑到本

书可能会被

翻译为多种

语言，所以笔

者在DeZero中用英

语

写了docstring）。另外

，写好docstring后，可以

通过Sphinx（参考文

献[39]）工

具等将

其输出为HTML、PDF等

形式。在使用

Sphinx的情况下，我

们可以不费

吹灰之力创

建出一个专

用页面。

制作

图标

创建OSS时

，我们也可以

考虑为它制

作一个图标

。拥有一个有

吸引力的图

标有助于得

到用户的认

可。当然，图标

可以由开发

者自己制作

，但要想使设

计更

有吸引

力，不妨考虑

请专业人士

来制作。DeZero的图

标也是在众

包网站上以

竞

标的形式

请人制作的

。非常感谢设

计者的完美

设计。

增加实

现示例（examples）

前面

介绍的是创

建DeZero的过程，而

真正有趣的

是使用创建

的DeZero来

第 5阶段

DeZero高级挑战 462

实

现有名的研

究成果或自

己设计的新

模型。我们可

以考虑增加

使用DeZero的

实现

实例。用DeZero实现

GAN（参考文献[41]）、VAE（参

考文献[42]）和Style 

Transfer（参

考文献[43]）等著

名的研究成

果是展示如

何使用DeZero的一

个好办

法。通

过这样的工

作，我们还可

以发现DeZero缺失

的功能。另外

，在dezero/

examples中有一些

使用DeZero的实现

示例（预计会

继续增加），感

兴趣的读者

可

以参考。

附

录A

in-place运算

（步骤

14的补充内容

）

本附录内容

是对步骤14的

补充，这里笔

者会对步骤

14中所说的“在

导

数加法计

算时不使用

+=”的原因进行

说明。

A.1 问题确

认

首先对这

个问题进行

梳理。在步骤

14中，为了能够

重复使用同

一个变量，

我

们对Variable类的backward方

法做了如下

修改。

class Variable:

 ...

def backward(self):

 if

self.grad is None:

self.grad = np.ones_like(self.data)

funcs = [self.creator]

while funcs:

 f

= funcs.pop()

 gys

= [output.grad for output

in f.outputs]

 gxs

= f.backward(gys)

 for

x, gx in zip(f.inputs,

gxs): 

 if

x.grad is None:

x.grad = gx

else:

 x.grad =

x.grad + gx

if x.creator is not

None:

 funcs.append(x.creator)

附录A in-place运

算（步骤 14的补

充内容） 464

阴影

部分是修改

的地方。简单

来说，第一次

传播导数（梯

度）时，进行

的

是赋值操作

，即x.grad = gx，之后进行

的是加法运

算，即x.grad

= x.grad 

+

gx。其中的

gx是ndarray实例。步骤

14提到将加法

运算的代码

改写为x.grad 

+= gx在某

些情况下会

出现问题，这

里说明其原

因。

A.2 关于复制

和覆盖

首先

作为预备知

识了解一下

ndarray实例的“复制

”和“覆盖”。先看

看

下面的代

码。

>>> import numpy as

np

>>> x =

np.array(1)

>>> id(x)

4370746224

>>> x += x

# 覆盖

>>> id(x)

4370746224

>>> x =

x + x #

复制

（新创建）

>>> id(x)

4377585368

从id(x)的

结果可以看

出，x（ndarray实例）要么

在内存中被

覆盖，要么

被

重新创建。在

使用加法赋

值运算符+=时

，x的对象ID不变

，也就是说内

存

位置相同

，所以只有值

被改写了。这

种不执行复

制操作，直接

覆盖内存中

的

值的运算

叫作in

place运算。

而

x = x

+ x导致对象的

ID不同。这说明

新的ndarray实例在

内存的另

一

个位置被创

建。从内存使

用的角度来

看，使用inplace运算

更好（如果in

place运

算没有问题

）。

函数id返回Python对

象的ID。id函数的

返回值取决

于实际运行

的时间和

环

境。

附录A

in-place运算

（步骤 14的补充

内容）  465

A.3 DeZero的反向

传播

在DeZero的反

向传播中，导

数是作为ndarray实

例传播的。这

里我们思

考

一下将第2次

及之后的反

向传播的导

数的代码以

in

place运算的方式

改为

x.grad += gx的情况

。将step/step14.py中的代码

以in

place运算的方

式进行

替换

后运行以下

代码。

x =

Variable(np.array(3))

y = add(x,

x)

y.backward()

print('y.grad: {}({})'.format(y.grad,

id(y.grad)))

print('x.grad: {}({})'.format(x.grad, id(x.grad)))

运行结

果

y.grad: 2 (4427494384)

x.grad: 2 (4427494384)

上面代码

的运行结果

是x的导数和

y的导数都是

2，ndarray的ID相同。

也就

是说，二者引

用的是同一

个ndarray。这里的问

题是y的导数

不正确，y

的导

数应该是1。

这

个问题发生

的原因是inplace运

算覆盖了这

个值。由于y.grad和

x.grad

引用的是同

一个值，所以

y.grad变为了错误

的结果。我们

把代码改为

x.grad 

= x.grad

+ gx。再次运行这

段代码，结果

如下所示。

运

行结果

y.grad:

1 (4755624944)

x.grad: 2

(4755710960)

这次

y和x引用的是

不同的ndarray，而且

引用的值是

正确的值，这

就解

决了前

面所说的问

题。以上就是

步骤14没有使

用+=（inplace运算）的原

因。

附录B

实现

get_item函数

（步骤47的

补充内容）

步

骤47只介绍了

DeZero的函数get_item的使

用方法，这里

笔者将介绍

其实现。首先

看一下GetItem类和

get_item函数的代码

。

dezero/functions.py

class GetItem(Function):

def __init__(self, slices):

self.slices = slices

def forward(self, x):

y = x[self.slices]

return y

 def

backward(self, gy):

 x,

= self.inputs

 f

= GetItemGrad(self.slices, x.shape)

return f(gy)

def get_item(x,

slices):

 return GetItem(slices)(x)

上面的代码

在初始化阶

段接受进行

切片操作的

参数slices。之后的

forward(x)方法只是通

过x[self.slices]取出元素

。

DeZero的forward(x)中的x是ndarray实

例，而backward(gy)中的gy

是

Variable实例。在实现

反向传播的

过程中，需要

使用DeZero的函数

对

Variable实例进行

计算。

附录B 实

现 get_item函数（步骤

47的补充内容

）

467

另外，DeZero中没有

与切片操作

相对应的反

向传播的计

算。为此，笔

者

另外准备了

名为GetItemGrad的新的

DeZero函数类。换言

之，我们要通

过

GetItemGrad的正向传

播来实现GetItem的

反向传播的

处理。

接下来

是GetItemGrad类的代码

，如下所示。

dezero/functions.py

class GetItemGrad(Function):

def __init__(self, slices, in_shape):

self.slices = slices

self.in_shape = in_shape

def forward(self, gy):

gx = np.zeros(self.in_shape)

np.add.at(gx, self.slices, gy)

return gx

 def

backward(self, ggx):

 return

get_item(ggx, self.slices)

首

先在初始化

阶段接收执

行切片操作

的参数（slices）和输

入数据的形

状（in_shape）。然后，在主

计算（forward）中准备

元素为零的

多维数组作

为

输入的梯

度，之后执行

np.add.at(gx,

self.slices, gy)。这行代码针

对gx在

self.slices指定的

位置上加上

了gy。我们可以

从下面的代

码示例清楚

地了

解到np.add.at函

数的用法。

>>> import numpy as

np

>>> a =

np.zeros((2, 3)) 

>>>

a

array([[0., 0., 0.],

[0., 0., 0.]])

>>>

b = np.ones((3,))

>>>

b

array([1., 1., 1.])

>>> slices = 1

>>> np.add.at(a, slices, b)

>>> a

array([[0., 0.,

0.],

 [1., 1.,

1.]])

  附

录B

实现 get_item函数

（步骤 47的补充

内容） 468

如果通

过多维数组

的切片操作

一次提取出

多个元素，那

么在反向传

播中就

需要

加上相应的

梯度。因此，上

面的代码通

过np.add.at函数进行

了加法

计算

。

接着需要实

现与np.add.at函数对

应的反向传

播，有趣的是

，我们刚刚

实

现的get_item函数就

是。到这里，get_item函

数就全部完

成了。

附录C

在

Google Colaboratory上运行

Google Colaboratory（以下

简称“Google Colab”）是一个

在云端运行

的

Jupyter

Notebook环境。只要

有浏览器，任

何人都可以

使用它。它不

仅支持

CPU，还支

持GPU。

本附录将

介绍如何使

用Google Colab运行DeZero。作为

示例，这里运

行步骤52中的

代码（MNIST的训练

代码）。首先访

问以下链接

。

https://colab.research.google.com/github/oreillyjapan/deeplearning

fromscratch3/blob/master/examples/mnist_colab_gpu.ipynb

在浏览器中

打开上面的

链接，会显示

图C

1中的界面

。图C1中Notebook

的内容

被分为一个

个单元格。单

元格可以是

文本，也可以

是用 Python

或用其

他语言编写

的代码。在运

行单元格中

的代码时，要

先点击单元

格

来选择它

，然后点击代

码左边的播

放按钮。我们

也可以使用

键盘快捷键

“command+return”或“Ctrl+Enter”来运行代

码。

图C1中显示

的数据是本

书GitHub仓库中的

examples/mnist_colab_gpu.

ipynb。GitHub上的ipynb文件可

以从Google Colab打开。

附

录C 在 Google Colaboratory上运行

470

图C1 Google Colab的界面

A

A

下

面的内容以

图C1的Notebook为基础

。建议读者一

边在Google

Colab

上运行

单元格一边

阅读本部分

的内容。另外

，在首次运行

单元格时，会

看

到警告。选

择“直接运行

”，单元格就会

继续运行下

去。

这个Notebook首先

会安装DeZero。由于

DeZero已发布到PyPI中

，所

以我们可

以通过pip install dezero命令

来安装它。

A

图

中代码请参

考随书下载

的代码文件

。——译者注

附录

C 在 Google

Colaboratory上运行  471

安

装完成后，使

用GPU运行DeZero。

如果

是True，则说明GPU处

于可用的状

态；如果是False，则

说明我们

需

要在Google Colab中对GPU进

行设置。设置

方法如下所

示。



从菜单的

“运行时”中选

择“改变运行

时类型”

 从“硬

件加速器”下

拉菜单选择

“GPU”

下面使用DeZero训

练MNIST。首先在CPU上

运行。

附录C 在

Google Colaboratory上运行

472

接下

来使用GPU进行

计算。

作为参

考，比较一下

DeZero在CPU和GPU上的速

度。结果如下

所示。

以上就

是对Google

Colab的介绍

。除了本附录

中展示的例

子，它还可以

运行DeZero的其他

示例代码。当

然，我们也可

以运行自己

用DeZero编写的

原

创代码。大家

不妨将Google Colab灵活

用在各个地

方。

后

记

感谢

大家陪伴我

走过创建DeZero的

旅程。希望通

过本书，大家

能对深

度学

习框架和深

度学习本身

有新的认识

。如果能达到

这样的效果

，作为作者，

我

会非常高兴

。在本书的最

后，我想简单

介绍一下本

书的创作过

程。

本书的写

作开始于2018年

10月左右，最终

这本书用了

大约一年半

的时

间完成

。刚开始写作

时，我想着花

大半年的时

间就能写完

，不过大家也

知道

我每次

都会预测错

。这可能给周

围的人添了

麻烦，借此机

会向他们表

示歉意。

本书

的主题是创

建一个深度

学习的“迷你

框架”。之所以

选择这样一

个主题，

主要

有3个原因（也

是我决定写

本书的理由

）。

第1个原因是

现代深度学

习框架已经

过了过渡期

，各技术社区

在最大程

度

上确定了共

通的功能。我

认为现在是

写本书的最

佳时机。

第2个

原因是还没

有任何能在

实现层面上

帮助大家理

解框架内容

的图书

或资

料。就我自身

而言，我从Chainer和

PyTorch等的代码（框

架内的代码

）

中学到了很

多。我希望能

以正确且有

趣的方式来

介绍这些技

术，这是一件

有

价值的事

情。

第3个原因

是Chainer的代码很

优美。Chainer设计得

很超前，实现

了

DefinebyRun这个引领

时代的方案

。我被它的思

想和代码所

吸引，这也是

我编写本书

的巨大动力

。不过Chainer的内容

（对初学者而

言）是庞大且

复杂的。

因此

，我的目标是

在Chainer的基础上

创建一个具

有现代功能

并且尽可能

简

单的框架

。

  后

记 474

考虑到

上述情况，本

书的方向顺

利地确定下

来。不过我对

本书的构成

，

即如何帮助

读者更好地

理解，如何传

达其中的“乐

趣”没有什么

好的想法。

当

时有一段摸

索和试错的

时期。大约在

开始写作三

个月之后，我

意识到只能

以渐进的方

式一边开发

一边讲解。从

那时起，我花

了很多时间

来思考合适

的

结构（按步

骤创建的结

构），由此就出

现了本书中

的60个步骤。

对

了，当时我买

了DeAgostini出版集团

出品的周刊

Robi，制作了用双

腿行走的机

器人Robi（这个系

列共有70卷）。我

一边读周刊

，一边思考如

何

创建框架

，这让我有了

一段无比难

忘的经历（Robi的

制作过程很

有趣，我很

好

地放松了心

情）。

于是本书

的结构确定

了下来，写作

过程开始有

了节奏感。这

使得我在

写

本书时感到

非常愉快，尤

其是我对开

发自创的框

架DeZero感到特别

兴奋。

我记得

自己当时废

寝忘食地编

写代码，思考

设计，还研究

了其他代码

。我再

次从开

发软件的工

作中体会到

了乐趣。

现在

这本书终于

写完了，我松

了一口气，但

也渴望继续

写一些内容

。虽

然本书到

步骤60就结束

了，但随着离

目标越来越

近，我的脑海

里也涌现出

超越这个目

标的更加有

趣的想法。例

如，我曾想增

加一个步骤

，用DeZero

实现更高

级的模型（GAN、VAE、DQN和

BERT等）。我甚至想

过继续扩展

下去，将本书

的内容扩展

到步骤100。

但本

书内容已经

足够多了，也

圆满实现了

创建框架的

目标。更重要

的是，

DeZero的发展

超出了最初

的预期，所以

我打算把步

骤100留作我自

己（或读者）

的

“课后作业”。最

后再次感谢

花时间阅读

本书的读者

，谢谢你们。

谢

辞

在本书的

写作过程中

，我得到了许

多人的帮助

，在此向他们

表示感谢。

Preferred Networks公

司的得居诚

也和我就Chainer的

内容进行了

各种讨论，

斋

藤俊太就本

书的结构和

内容给了我

建议。此外，编

写本书的想

法源自我与

西川彻、冈之

原大辅、奥田

辽介的交流

。我非常感谢

能有这样的

机会编写本

书。

此外，本书

采用了公开

审阅的方法

进行校对。在

公开审阅的

过程中，手

后

记  475

稿在网上

公开，任何人

都可以阅读

和评论。我最

终收到了来

自100多人的

2000多

条评论。在此

，我向参与审

阅的人表示

衷心的感谢

。正是因为有

你

们的帮助

，我才能进一

步打磨本书

。当然，如果本

书有任何不

足或错误，都

是我的责任

，与各位审稿

人无关。

本书

依旧由O’Reilly

Japan公司

的宫川直树

、岩佐未央、小

柳彩良负责

编辑。本书的

设计（主要是

排版和封面

设计）由Top Studio公司

的武藤健志

、

增子萌负责

。承蒙各位帮

助，本书才得

以面世。最后

我要感谢一

直陪在我身

边并给予我

支持的家人

，谢谢你们。

后

记 476

审稿

齐藤

三千雄

佐藤

亮介 小泽辽

增宫雄一 山

崎祐太

平田

恭嘉

石原祥

太郎 中村雄

一郎 加藤皓

也 石崎一明

远藤嵩良

高

桥英树 熊谷

直也 铃木淳

哉 饭田啄巳

长滨直智

清

水敦 高原裕

笹川大河 后

藤雅弘

山内

健太

野口宗

之 高嶋航大

长野将吾 日

隈雅也

浦优

太

山本雄大

竹中诚 丰田

胜之 坂口生

有

柏田祐树

山下耕太郎

稻田高明

森

长诚 石川智

贵

芝田将 竹

之内俊昭

安

藤巧 今井健

男 宫林优

村

井龙马

三桥

晟 金亲智 藤

波靖 利弘俊

策

桦山绘里

松桥慎也 为

安圭介 柳英

生 水谷宗隆

荻岛真治

古

木友子 堀直

人 滨田祐介

志田刚

关阳

介

冲山智 田

渊大将 清水

俊树 稻留隆

之

深泽骏介

吉村哲 三田

雅 石川雄太

郎 佐野弘实

小内伸之介

斋藤航贵

小

林久美子 新

里伦子 铃木

慎太郎

渡边

启太

荻野甫

大泷启介 本

川哲哉 柴田

敦也

加藤贵

大

高野刚 家

城博隆 锻岛

康裕 长坂瑛

河田孝允

西

平政隆 高濑

健司 轻部俊

和 今村功一

安藤朋昭

尾

形胜弥 大前

谦友 村松将

尚 都筑俊介

齐藤友诚

刘

超 堀井良威

kk2170 ryo

制作

武藤健

志

增子萌

编

辑

宫川直树

岩佐未央

小

柳彩良

参考

文献

第 1阶段

：自动微分

[1] Todd Young, Martin

J. Mohlenkamp. Introduction to

Numerical 

Methods and

Matlab Programming for Engineers[M].

Athens: Ohio 

University,

2019.

[2] Wengert, Robert

Edwin. A simple automatic

derivative evaluation 

program[J].

Communications of the ACM

, 1964, 7(8): 463

464.

[3] Automatic Reverse

Mode Differentiation: Lecture Notes.

[4] Automatic differentiation in

pytorch.

[5] CS231n: Convolutional

Neural Networks for Visual

Recognition.

[6] Baydin, Atilim

Gunes, et al. Automatic

differentiation in machine

learning: a survey[J]. Journal

of machine learning research,

2018, 

18(153).

[7]

Maclaurin, Dougal. Modeling, inference

and optimization with

composable differentiable procedures[D]. Cambridge

: Harvard 

University,

2016.

[8] unittest—ユニットテストフレームワーク.

[9]

Travis CI官

网.

参考文献

478

第 2阶段：用自

然的代码表

达

[10]

Hertz, Matthew, and Emery

D. Berger. Quantifying the

performance 

of garbage

collection vs. explicit memory

management[J]. Proceedings 

of

the 20th annual ACM

SIGPLAN conference on Object

oriented 

programming, systems,

languages, and applications, 2005.

[11] PyPI. Memory Profiler.

[12] Python Document. contextlib.

[13] Wikipedia. Test functions

for optimization.

[14] Christopher

Olah. Neural Networks, Types,

and Functional Programming.

[15]

Yann LeCun. Differentiable Programming.

[16] PyTorch Document, TORCHSCRIPT.

[17] Swift for TensorFlow.

第 3阶段：实

现高阶导数

[18] Graphviz

Graph Visualization Software.

[19]

Wikipedia, Rosenbrock function.

[20]

PyTorch Document, torch.optim.LBFGS.

[21]

Gulrajani, Ishaan, et al.

Improved training of wasserstein

gans[J]. 

Advances in

neural information processing systems.

2017.

[22] Finn, Chelsea,

Pieter Abbeel, Sergey Levine.

Modelagnostic meta

learning for fast adaptation

of deep networks[J]. JMLR,

2017.

[23] Schulman, John,

et al. Trust region

policy optimization[J]. International

conference on machine learning,

2015, 37: 1889–1897.

第

4阶段：创建

神经网络

[24] Seiya Tokui.

Aggressive Buffer Release.

[25]

LeCun, Yann A., et

al. Efficient backprop[J]. Neural

networks: Tricks 

参

考文献

479

of the trade,

2012.

[26] Pascanu, Razvan,

Tomas Mikolov, and Yoshua

Bengio. On the

difficulty of training recurrent

neural networks[J]. International

conference on machine learning.

2013, 28:1310–1318.

[27] Duchi,

John, Elad Hazan, and

Yoram Singer. Adaptive subgradient

methods for online learning

and stochastic optimization[J]. Journal

of 

Machine Learning

Research 2011, 12: 2121–2159.

[28] Zeiler, Matthew D.

ADADELTA: an adaptive learning

rate method[J]. 

arXiv

preprint arXiv:1212.5701, 2012.

[29]

Loshchilov, Ilya, and Frank

Hutter. Fixing weight decay

regularization 

in adam[J].

arXiv preprint arXiv:1711.05101, 2017.

[30] Chainer MNIST Example.

[31] PyTorch MNIST Example.

[32] Chainer Document. Link

and Chains.

[33] TensorFlow

API Document. Module: tf.keras.optimizers.

第 5阶

段：DeZero高级挑战

[34] Srivastava, Nitish,

et al. Dropout: a

simple way to prevent

neural 

networks from

overfitting[J]. The journal of

machine learning research

2014: 1929–1958.

[35] Ioffe,

Sergey, Christian Szegedy. Batch

normalization: Accelerating 

deep

network training by reducing

internal covariate shift[J]. arXiv

preprint arXiv:1502.03167, 2015.

[36]

Simonyan, Karen, Andrew Zisserman.

Very deep convolutional

networks for largescale

image recognition[J]. arXiv preprint

arXiv:1409.1556, 2014.

[37] He,

Kaiming, et al. Deep

residual learning for image

recognition[R]. 

Proceedings of

the IEEE conference on

computer vision and pattern

参考文献 480

recognition. 2016.

[38] Iandola,

Forrest N., et al.

SqueezeNet: AlexNetlevel accuracy

with 

50x fewer

parameters and <0.5 MB

model size[J]. arXiv preprint

arXiv:1602.07360, 2016.

[39] SPHINX

documentation.

[40] ONNX官

网.

[41]

Goodfellow, Ian, et al.

Generative adversarial nets[J]. Advances

in 

neural information

processing systems. 2014.

[42]

Kingma, Diederik P., Max

Welling. Autoencoding variational

bayes[J]. 

arXiv preprint

arXiv:1312.6114 , 2013.

[43]

Gatys, Leon A., Alexander

S. Ecker, and Matthias

Bethge. Image style

transfer using convolutional neural

networks[R]. Proceedings of the

IEEE conference on computer

vision and pattern recognition.

2016.

①

②

③

图52 从输出

端的导数开

始依次进行

计算的流程

x y

图252 改变节点

的颜色

x

y Exp

图253

圆

形（椭圆形）和

矩形节点的

例子

x

Exp

y

图254 有箭

头连接的节

点

x0

x1

Add

y

图26

1 计算图

可视化的示

例

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00

−6

−4 − 6 4

2 0 2

y

= sin(x)

y′

y′′

y′′′

图341 y=sin(x)及其高

阶导数的图

像（标签y'

对应

一阶导数，y''对

应二阶导数

，y'''

对应三阶导

数。）

y

x
