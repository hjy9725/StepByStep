note


通过两阶段

训练3D VAE，

对视频进行

压缩编码


第一阶段：

在较低分辨率

和较少帧数的

视频上进行

训练，学习

压缩和重建

视频的基本

能力
第二阶段：

在更长的视频

上训练，提高

模型处理长

视频的能力，

同时保持帧

与帧之间的

连续性
为了提高

文本和视频

之间的对齐度，

提出了一个

带有专家自

适应层归一化

(LayerNorm)的

专家transformer，

以促进两种

模态之间的

深度融合，

基于3d full 

attention

进行时空

注意力建模

训练
文章目录


note
一、模型介绍


二、Cogvideox模型


0. 模型概览


1. VAE编码器


（1）3D VAE

编码器结构


（2）3D VAE的

两阶段训练


2. 2. Expert transformer


（1）Expert transformer block


（2）3D full attention


三、微调数据和

微调模式


1. cogvideox的

数据处理pipeline


2. 微调数据

的准备


四、评测效果


（1）自动化评估


（2）人工评估：


五、论文总结


Reference


一、模型介绍


模型名	CogVideoX-2B	

CogVideoX-5B


模型介绍	

入门级模型, 

兼顾兼容性。

运行、二次开发

成本低。	

视频生成质量

更高,视觉

效果更好的

更大尺寸模型。


推理精度	FP16*(推荐), 

BF16, FP32, 

FP8* (E4M3,E5M2), 

INT8, 不支持INT4	

BF16(推荐), FP16, 

FP32, FP8*(E4M3, E5M2), 

INT8, 不支持INT4
单GPU显存消耗	FP16: 18GB using SAT / 12.5GB* using diffusers
INT8: 7.8GB* using diffusers	BF16: 26GB using SAT / 20.7GB* using diffusers
INT8: 11.4GB* using diffusers
多GPU推理显存消耗	FP16: 10GB* using diffusers	BF16: 15GB* using diffusers
推理速度	FP16: ~90*s	BF16: ~180*s
微调精度	FP16	FP16
微调显存消耗(每卡)	47 GB (bs=1, LORA)
61 GB (bs=2, LORA)
62GB (bs=1, SFT)	63 GB (bs=1, LORA)
80 GB (bs=2, LORA)
75GB (bs=1, SFT)
提示词语言	English*	English*
提示词长度上限	226 Tokens	226 Tokens
视频长度	6秒	6秒
帧率	8帧/秒	8帧/秒
视频分辨率	720*480, 不支持其他分辨率(含微调)	720*480, 不支持其他分辨率(含微调)
位置编码	3d_sincos_pos_embed	3d_rope_pos_embed
CogVideoX的核心技术特点如下：

针对内容连贯性问题，智谱AI自主研发了一套高效的三维变分自编码器结构（3D VAE）。该结构能够将原始视频数据压缩至原始大小的2%，显著降低了视频扩散生成模型的训练成本和难度。结合3D RoPE位置编码模块，该技术有效提升了在时间维度上对帧间关系的捕捉能力，从而建立了视频中的长期依赖关系。
在可控性方面，智谱AI打造了一款端到端的视频理解模型，该模型能够为大量视频数据生成精确且内容相关的描述。这一创新增强了模型对文本的理解和对指令的遵循能力，确保生成的视频更加符合用户的输入需求，并能够处理超长且复杂的prompt指令。
我们的模型采纳了一种将文本、时间、空间三维一体融合的transformer架构。该架构摒弃了传统的cross attention模块，创新性地设计了Expert Block以实现文本与视频两种不同模态空间的对齐，并通过Full Attention机制优化模态间的交互效果。
注意：c端主要特点如下
快速生成：仅需30秒即可完成6秒视频的生成。
高效的指令遵循能力：即使是复杂的prompt，清影也能准确理解并执行。
内容连贯性：生成的视频能够较好地还原物理世界中的运动过程。
画面调度灵活性：例如，镜头能够流畅地跟随画面中的三只狗狗移动，效果宛如专业摄影师的跟拍。

二、Cogvideox模型
0. 模型概览
通过两阶段训练3D VAE，对视频进行压缩编码
第一阶段：在较低分辨率和较少帧数的视频上进行训练，学习压缩和重建视频的基本能力
第二阶段：在更长的视频上训练，提高模型处理长视频的能力，同时保持帧与帧之间的连续性
为了提高文本和视频之间的对齐度，提出了一个带有专家自适应层归一化(LayerNorm)的专家transformer，以促进两种模态之间的深度融合，基于3d full attention进行时空注意力建模训练（如下图）

训练方法：

图像-文、视频-文混合训练
渐进式训练：低分辨率训练 -› 高分辨率训练 -› 高质量高分辨率视频（占比20%）训练
1. VAE编码器
和2d vae的区别：conv2d->conv3d
连续性更强，压缩倍数更高
视频时序因果关系，单张图片压缩

（1）3D VAE编码器结构
下图内容：
(a) CogVideoX 中 3D VAE 的结构。它由一个编码器、一个解码器和一个潜在空间正则化器组成，实现了从像素到潜在空间的4×8×8压缩。

图 2 (a) 显示了所提出的 3D VAE 的结构。它由一个编码器、一个解码器和一个潜在空间正则化器组成。高斯潜在空间受 Kullback-Leibler (KL) 正则化器的约束。编码器和解码器由四个对称排列的阶段组成，分别通过resnet块堆叠阶段的交错执行2×下采样和上采样。
前两轮下采样和上采样涉及空间和时间维度，而最后一轮仅在空间上采样。这使得 3D VAE 能够在时间维度上实现 4 倍的压缩和空间维度上的 8×8 压缩。我们总共实现了从像素到潜在空间的4×8×8压缩。

(b) 时间因果卷积的上下文并行实现（上图右侧）：

我们采用时间因果卷积（Yu et al., 2023），它将所有填充放在卷积空间的开头，如图 2 (b) 所示。确保了未来的信息不会影响当前或过去的预测。
因为处理具有大量帧的视频会引入过多的 GPU 内存使用，我们在时间维度上应用上下文并行技术进行 3D 卷积，以在多个设备之间分配计算。如图 2 (b) 所示，由于卷积的因果性质，每个rank简单地将长度为 k-1 的段发送到下一个rank，其中 k 表示时间内核大小。这导致通信开销相对较低。
vae训练：时间维度上做context parallel
时间维度上的并行计算：通过时间因果卷积，每个计算单元（rank）只需要处理一部分数据，然后将处理结果传递给下一个计算单元。具体来说，每个单元处理长度为 k-1 的数据段，其中 k 是卷积核的时间维度大小。这种设计意味着每个单元在处理时只需要考虑本单元的数据和前一个单元传递的数据，从而减少了数据传输和通信的开销。
（2）3D VAE的两阶段训练
第一阶段训练：在这一阶段，3D VAE在较低分辨率和较少帧数的视频上进行训练，以节省计算资源。较高分辨率的编码能够自然地泛化到更低分辨率，而帧数的扩展则不那么顺畅。因此，第一阶段主要是为了学习压缩和重建视频的基本能力。
第二阶段训练：在第二阶段，使用上下文并行技术在更长的视频上进行微调（finetuning）。这一阶段的目的是提高模型处理长视频的能力，同时保持帧与帧之间的连续性，避免生成的视频中出现闪烁现象。
具体的loss函数如下， α , β \alpha, \betaα,β 和 γ \gammaγ 是用于平衡不同损失项的权重参数。

L V A E = α ⋅ L R + β ⋅ L L + γ ⋅ L K L L_{V A E}=\alpha \cdot L_R+\beta \cdot L_L+\gamma \cdot L_{K L}
L 
VAE
​
 =α⋅L 
R
​
 +β⋅L 
L
​
 +γ⋅L 
KL
​
 

L2损失（Mean Squared Error, MSE）：计算原始帧和重建帧之间的均方误差。
L R = 1 N ∑ i = 1 N ∥ x i − x ^ i ∥ 2 2 L_R=\frac{1}{N} \sum_{i=1}^N\left\|x_i-\hat{x}_i\right\|_2^2
L 
R
​
 = 
N
1
​
  
i=1
∑
N
​
 ∥x 
i
​
 − 
x
^
  
i
​
 ∥ 
2
2
​
 
LPIPS感知损失（Perceptual Loss）：使用LPIPS度量计算两个视频帧序列之间的感知差异。
L L = LPIPS ⁡ ( x , x ^ ) L_L=\operatorname{LPIPS}(x, \hat{x})
L 
L
​
 =LPIPS(x, 
x
^
 )
GAN损失: 使用3D discriminator产生的损失，可能包括生成器和判别器的损失。 L K L = L_{K L}=L 
KL
​
 = − E q ( z ∣ x ) [ log ⁡ p ( z ) ] -\mathbb{E}_{q(z \mid x)}[\log p(z)]−E 
q(z∣x)
​
 [logp(z)]
其中GAN loss：在生成对抗网络（GANs）和其他一些生成模型中，判别器的作用是区分生成的样本和真实样本，从而为生成模型提供反馈。对于3D VAE中使用的GAN损失，它通常涉及到训练一个3D判别器，这个判别器专门设计来评估生成的视频帧或帧序列是否看起来真实。以下是3D VAE中可能使用的GAN损失的一个示例公式：
L G A N = − E x ~ ∼ P x ~ [ log ⁡ D ( x ~ ) ] + E x ∼ P data  [ log ⁡ ( 1 − D ( x ) ) ] L_{G A N}=-\mathbb{E}_{\tilde{x} \sim P_{\tilde{x}}}[\log D(\tilde{x})]+\mathbb{E}_{x \sim P_{\text {data }}}[\log (1-D(x))]
L 
GAN
​
 =−E 
x
~
 ∼P 
x
~
 
​
 
​
 [logD( 
x
~
 )]+E 
x∼P 
data 
​
 
​
 [log(1−D(x))]

其中:

D DD 是判别器。
x ~ \tilde{x} 
x
~
  是从潜在空间 P x ~ P_{\tilde{x}}P 
x
~
 
​
  中采样并经过解码器重建的假视频帧。
x xx 是来自真实数据分布 P data  P_{\text {data }}P 
data 
​
  的真实视频帧。
第一项鼓励判别器正确识别假视频帧。
第二项鼓励判别器将真实视频帧识别为真实。
这种损失函数有助于训练过程中生成器的优化，使得生成的视频不仅在像素级别上与真实视频相似，而且在视觉上也更加逼真。

2. 2. Expert transformer
（1）Expert transformer block
Patchify：3d vae将视频编码为T X H x W X C维度的tensor（T表示帧维度），然后沿着spatial 空间维度进行patchify切分为长度为T * (H/p) * (W/p)的序列Zvison。注意为了图像和视频的联合训练，这里不在时间维度T上进行patchify。
3D-ROPE：视频tensor都能用3d坐标(x,y,t)表示，在这三个坐标分别使用1d-rope，分别占用隐藏层channel维度的3/8、3/8、2/8，最后沿着channel维度拼接得到3d-rope emb。
消融实验：a图表明3d-rope比正弦位置编码更快loss收敛、b图两种方式相差不大
c图表明expert AdaLN + MLP没提高，所以最后使用expert AdaLN

问题：为啥没使用时间和空间分离的attention（容易造成帧之间的不一致，以往这么做是为了减少计算复杂度）


更好的语义理解：Text+Video full attention
让文本也参与 self attention 计算和 FFN
文本和视频对齐：Text Expert AdaLN + Vision Expert AdaLN
（2）3D full attention

方式：mixed-duration training and Frame Pack
Frame pack：


三、微调数据和微调模式
1. cogvideox的数据处理pipeline
视频caption需要能够详细描述视频中的内容，之前开源的视频理解模型视频描述能力较差
Version1：利用图像重标注模型对每帧分别标注，结合短视频caption信息利用大语言模型summary得到视频描述
Version2：利用version 1得到的数据微调CogVLM2-Video，得到一个端到端的视频描迖模型

注：与训练对应，推理时输入详细的prompt，才能最大限度激发模型能力

训练数据：

使用cogvlm-video构造视频字幕数据（其中也借助到了Panda70M构造视频字幕数据）
使用纪录片等视频数据，基于 video-llama 训练了多个过滤器，用于自动筛选低质量视频
备注：

Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers https://arxiv.org/abs/2402.19479
2. 微调数据的准备
文生视频-微调数据：文本-视频对数据
数据集格式应该如下：
.
├── labels
│   ├── 1.txt
│   ├── 2.txt
│   ├── ...
└── videos
    ├── 1.mp4
    ├── 2.mp4
    ├── ...
1
2
3
4
5
6
7
8
9
注：

每个 txt 与视频同名，为视频的标签。视频与标签应该一一对应。
一般不使用一个视频对应多个标签。
如果为风格微调，准备至少50条风格相似的视频和标签，以利于拟合。
当前开源的2b和5b支持全参微调、Lora微调：
（1）如果是全参微调，更新3d full attention参数（vae和T5参数不会更新）
（2）如果是Lora微调，会更新3d full attention的四坨低秩矩阵参数：


四、评测效果
（1）自动化评估
指标：人类动作、场景一致、动态程度、多对象、风格匹配、动态质量、细粒度动态信息即GPT4o-MT Score


注：

使用来自VBench的几个指标(Huang et al.， 2024)：人类行为、场景、动态度、多个对象和外观风格，但VBench有的指标不会采用，比如颜色度量旨在测量生成的视频中跨帧特定颜色对应的对象的存在，通过计算概率来评估模型的质量。然而，该指标可能会误导表现出更大变化的视频生成模型，因此在我们的评估中不包括它。
对于更长的生成视频，一些模型可能会产生帧之间变化最小的视频，以获得更高的分数，但这些视频缺乏丰富的内容。因此，评估视频动态的指标变得更加重要。
使用了两种视频评估工具：来自 Deviil (Liao et al., 2024) 的动态质量和来自 ChronoMagic (Yuan et al., 2024) 的 GPT4o-MTScore，它们更关注视频的动态特性。动态质量是通过将各种质量指标与动态分数的集成来定义的。这种方法减轻了视频动态和视频质量之间的负相关引起的偏差，从而对视频质量进行更彻底的评估。
例如，ChronoMagic 引入了 GPT4o-MTScore，这是一种旨在测量延时视频的变质幅度的指标，例如描述物理、生物和气象变化的指标。该指标是通过定期从生成的视频中提取帧并使用 GPT-4o (OpenAI, 2024) 对变化程度进行评分来获得的，提供了对视频动态的细粒度评估。这种方法确保了对内容随时间的可变性进行更准确的评估，抵消了评分中静态帧序列的潜在偏差。
（2）人工评估：


人工评估的指标：

感官质量:这一部分主要关注视频的感知质量，包括主体一致性、不同帧的连续性和稳定性。
指令遵循：这部分侧重于生成的视频是否与指令对齐，包括主题、数量、元素和细节的准确性。
物理模拟：这部分侧重于模型是否可以遵守物理世界的客观规律，例如照明效果、不同对象之间的交互以及流体动力学的真实性。
画面质量：这一部分主要关注可以从单帧图像中评估的指标，包括美学质量、清晰度和保真度
五、论文总结
论文试图解决的问题：CogVideoX 旨在解决的问题是生成与文本提示相符的连贯、长时间且具有显著运动特征的视频。这包括提高文本-视频对齐、确保视频的连贯性以及处理视频数据的复杂性。
解决方案的关键：CogVideoX 的关键解决方案包括使用3D变分自编码器（VAE）来压缩视频数据，专家变换器（expert transformer）来改善文本和视频之间的对齐，以及采用分阶段训练技术来提高视频的连贯性和质量。
实验设计：实验设计包括使用不同的训练技术，如混合持续时间训练和分辨率逐步训练。使用VBench (Huang et al., 2024) 中的自动化度量评估和人类评估来测试模型性能。
定量评估的数据集：使用VBench (Huang et al., 2024) 进行定量评估，它是一套为自动评估生成视频质量设计的工具。CogVideoX-2B的模型权重已在GitHub上开源。
实验结果：实验结果表明CogVideoX在多个自动化指标和人类评估中表现出最先进的性能，这支持了其作为高质量文本到视频生成模型的假设。
论文贡献：论文的贡献包括提出了CogVideoX模型，该模型使用3D VAE和专家变换器生成高质量的视频；开发了有效的文本-视频数据处理流程；通过分阶段训练技术提高了模型性能；并在GitHub上开源了模型权重。
下一步工作：论文提到未来的工作将集中在改进CogVideoX捕捉复杂动态的能力，并确保视频生成的更高质量。此外，研究者们还在探索视频生成模型的扩展规律，并计划训练更大、更强大的模型以生成更长、更高质量的视频，推动文本到视频生成领域的边界。
Reference
[1] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer
[2] VBench: Comprehensive Benchmark Suite for Video Generative Models. CVPR 2024
[3] https://github.com/THUDM/CogVideo/tree/main/sat
[4] 图生视频模型：https://huggingface.co/THUDM/CogVideoX-5b-I2V/blob/main/README_zh.md
[5] 相关开源-迪士尼数据：https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset
[6] 5b模型部署推理：https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space
[7] https://huggingface.co/spaces/Vchitect/VBench_Leaderboard