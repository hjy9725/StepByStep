9.1.2 贪心搜索的改进 贪心搜索在每个生成步骤中均选择最高概率的词元，这可能会由于忽略在某 些步骤中概率不是最高、但是整体生成概率更高的句子而造成局部最优。为了解 决这个问题，可以进一步采用以下的改进策略。 • 束搜索. 在解码过程中，束搜索（Beam Search）[236] 会保留前 𝑛 个具有最高 概率的句子，并最终选取整体概率最高的生成回复。这里的 𝑛 被称为束大小（Beam Size）。当 𝑛 = 1，束搜索就退化为贪心搜索。如图 9.3 所示（𝑛 = 2），第一步保留了 概率最高的两个词“coffee”和“water”作为候选；第二步基于“coffee”和“water” 均进行扩展，得到模型在两个上下文内容下的概率分布，最后选择联合概率最高 的两个句子“coffee then”和“coffee and”作为候选。在下面的生成步骤中，将会继 续基于这两个候选去进行扩展，每次都选择联合概率最高的两个句子。最后，当 两个束的句子均生成结束后，选择整体生成概率最高的候选句子作为最终的输出。 在实践中，束的数量通常设定在 3 到 6 的范围内，设置过大的束会显著增加运算 开销，并可能会导致性能下降。 • 长度惩罚. 由于束搜索中需要比较不同长度候选句子的概率，往往需要引入 201 9.1 解码策略 I am sleepy. I start a pot of T=1.3 T=1.0 T=0.7 图 9.4 温度设置为 1.3、1.0 和 0.7 时的下一个词的概率分布变化 长度惩罚（Length Penalty）（亦称为长度归一化）技术。如果没有长度惩罚，传统 的束搜索会倾向于生成较短的句子，因为每生成一个单词，都会乘以一个小于 1 的概率，使得句子的生成概率逐渐变小。因此，可以在生成概率的计算中引入长 度惩罚，通过将句子概率除以其长度的指数幂 𝛼，对于句子概率进行归一化处理， 从而鼓励模型生成更长的句子。在实践中，𝛼 通常设置为 0.6 到 0.7 之间的数值。 • 重复惩罚. 为了缓解贪心搜索重复生成的问题，可以使用 𝑛-元惩罚（𝑛-gram Penalty）来强制避免生成重复的连续 𝑛 个词元，实践中 𝑛 通常设置为 3 到 5 之间 的整数。进一步地，研究人员还提出了相对“温和”的惩罚机制来降低生成重复词 元的概率，而不是“一刀切”地完全避免某些短语的生成，如出现惩罚（Presence Penalty）和频率惩罚（Frequency Penalty）。具体地，出现惩罚在生成过程中会将已 经生成词元的 logits（公式 5.11）减去惩罚项 𝛼 来降低该词元之后生成的概率。频 率惩罚相较于出现惩罚，会记录每个词元生成的数目，然后减去出现次数乘以惩 罚项 𝛼，因此如果一个词元生成得越多，惩罚也就越大。在实践中，𝛼 的取值范围 通常在 0.1 到 1 之间。这些重复惩罚方法不止适用于贪心搜索，对于随机采样也均 适用。 