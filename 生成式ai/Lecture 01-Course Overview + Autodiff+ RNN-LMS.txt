








有五个家庭作业

但我说四个模块

那是因

为第五个家庭作业实际

上是

呃

这就像家庭作业

零

我们是

我们只是

呃

我

一会儿再谈这个

所以呃

这是一种特殊的

考试日

期定了

3月27日星期三有一

次随堂考试

为了家庭作

业

你有

呃

家庭作业提交

的六种弹性日

在你可以

的地方

嗯，提交晚了，没有

惩罚

呃

你知道的

迟交三

天后接受提交，不得延期

扩展真的很像紧急情况

如果你只是

你知道流鼻

涕

那是个好日子

类似的

事情

如果你在医院

然后

就像不用担心一样

就像

你会

你会得到延期的

嗯

嗯

如果他们看到其他类

别的教学大纲

掉进桶里

嗯背诵

呃

会有

呃

差不多

每个作业都有一个

这将

是在星期五

在上课的时

间和地点

这些是可选的

但更多的互动会议

读数

将是

呃

在线PDF技术将使用

包括所有你以前可能都

用过的熟悉的东西

嗯

学

术诚信

呃

本课程大力鼓

励合作

关键是你需要记

录你的合作

嗯

您的解决

方案应该始终独立编写

嗯也就是说

呃

哎呦

所以

这门课的合作策略

如果

你拿了

呃

十三零一十六

零一

嗯

本课程的协作策

略与该课程不同

嗯

所以

如果你选了这门课

请仔

细阅读这些协作策略

呃

不要以为它们完全一样

嗯

所以我认为这门课的

政策是鼓励合作

呃

以一

些谨慎的方式

比我们在

那门课上做得更多

嗯

然

后办公时间被贴在谷歌

日历上

嗯是的

所以我鼓

励你问很多问题

举手打

断

或者事后再问

在广场

上

嗯

本课程的简介是机

器学习课程的介绍

所以

最初我们只是

你知道这

里列出的机器学习课程

介绍的通常列表

包括七

十五

但后来我们说

啊

但

深度学习课程也有很大

的先驱者

呃

它本身就是

机器学习课程介绍

它没

有任何先决条件

嗯所以

嗯

如果你拿了

你知道的

十一四五六八十五

八七

八十五

这也是这门课的

一个很好的预科

嗯

但我

们不想让你误以为

深度

学习是一种预演

这不是

预演

嗯

所以Pytorch和深度学习

并不是提前假设的

嗯

这

在你们之间造成了一点

分裂

因为这取决于你在

哪个学期上的预科

您可

能大量接触过Pytorch中的深度

学习，也可能没有接触过

好吧如果你碰巧

例如

以

我去年秋天的机器学习

课程介绍为例

您已经实

现了一个转换器语言模

型

是的但是如果你选修

了不同的机器学习课程

你当然没有

那也没关系

嗯所以家庭作业零

呃

我

之前提到的是让每个人

都跟上速度

呃圆周率手

电筒和舒适

尤其是如何

使用它

嗯和嗯

然后我们

也会花

呃

只是其中的一

些

这些早期的讲座经历

了一些

深度学习中的一

些基础话题

那将是对

呃

对你们中的一些人来说

对你们中的其他人来说

是新的

所以我们会让每

个人都在同一水平上

在

最初的几周里

好啦

所以

呃

这是一张五份家庭作

业的图片

呃

家庭作业零

是一种独特的，因为它可

能是你的背景

也可能不

会

呃

然后是家庭作业一

二三四你们要跳进去

做

一些有趣的事情

呃

建造

呃

不仅仅是

呃

你知道的

有点啊

一种老式的变形

金刚语言模型

但是你是

如何建造一个最先进的

据说有滑动窗的张力

嗯

用于图像生成

我们会好

好想想

您实际上如何为

适配器构建一个gan或扩散

模型

我们将考虑如何使

用一个相当大的语言模

型

然后在上面放一个适

配器

嗯和嗯

用于多模态

地基模型

嗯

取决于成功

我们实际上还没有写这

部作品

但我们希望让您

使用文本到图像模型

实

际上自己造一个

虽然我

还没有概念证明

任何人

都可以在笔记本电脑或

小型GPU上做到这一点

所以

我们要想办法

这是否真

的是

呃

即使适度合理

呃

还有这个呃

额外家庭作

业

作业六二三

这只适用

于注册为10-6的学生

二十三

呃

你们要分析一下最近

的研究

呃那你就会知道

阅读一些文献，并创建一

个小视频演示文稿

所以

这口井的目标

这个项目

有许多不同的目标

呃

探

索一些你感兴趣的生成

建模技术

加深了你对现

实世界应用程序的理解

你们三人一组

嗯

没有关

于这个主题的教科书

但

你会被指示

呃

到当前的

研究论文

所以，嗯，所以这

基本上是

我觉得

这门课

的一个学习目标是让你

达到这样的地步，你实际

上可以

有点舒服

舒适地

阅读在这个领域迅速发

表的研究论文

万一你在

找东西

呃

就像讲座下面

有一个链接

一个现在写

着幻灯片的

呃

如果你点

击它

你会发现这些幻灯

片

嗯

有一个办公时间链

接，上面有谷歌日历

办公

时间还没到

不过，我们很

快就会弄好的

我们只是

在做一些最后一分钟的

安排

作业将出现在此课

程页面上

呃

就在作业清

单下面

我们一般也会在

广场上宣布

嗯好吧

然后

呢

呃

关于家庭作业零的

一个独特之处

因为呃

这

是一个不寻常的任务

嗯

那个

这项任务的独特政

策是

基本上会批准任何

和所有的延期请求

你应

该按时完成作业

如果你

坐在这里

你应该

你应该

按时完成作业

您没有理

由需要延期请求

这真的

是让

我们有一个公平的

政策

因为八天后有人

会

出现在我的办公室

我可

以在截止日期后参加你

的人工智能课程吗

我们

也许能容纳那个人

嗯好

吧

我会

我会

呃

有很多

我

想我们已经谈了很多

对

课程不同的学习目标进

行划分的不同方法

呃

问

题

什么路线与预定的路

线

但当然就像贝塔大脑

这周很有趣

所以可能有

更少的重叠

所以这两门

课程会有重叠

呃

但就像

在完全不同的话题

对呀

所以嗯

我觉得安德烈最

近把扩散模型

呃

但这些

可能不会出现在高级深

度学习中

呃

但在深度学

习中，会有一些类似于

我

们只需要能够谈论的核

心模型

就像变形金刚和

CNN

呃

那个课程

这门课必然

会讲到

然后呃

高级深度

学习也

呃

谈论一些呃

我

们将要讨论的生成模型

嗯

但可能呃

它会有

它会

减少对一些类似的关注

例如，它可能会谈论甘斯

嗯

但它可能会更少地描

述这两种生成模型

从语

言和视觉

和其他模式，并

试图将它们联系在一起

所以

这门课存在的大部

分原因

你能找到这门课

的所有主题

在机器学习

部门现在提供的其他课

程中

呃

但你可能需要上

大约五门不同的课程才

能掌握所有的主题

嗯

所

以

我们把这门课放在一

起的部分原因是因为我

们意识到

当我们的博士

生

例如

有时间在机器学

习部门上五门不同的课

程

不是每个人都这么做

的

还有呃

所以这是为了

把分散在许多地方的许

多话题聚集在一起

现在

有很多不同的课程

这是

个好问题

谢谢你

你最喜

欢的聊天是什么

所以我

想我还没有最喜欢的聊

天

英镑提示

但我确实一

直在开发一些最喜欢的

鹦鹉图像

独角鲸和墨西

哥蝾螈

哇塞

结果是非常

惊人的

这堂课花了多少

时间

与三零一或六零相

比

所以这堂课的目的是

呃

相对耗时

呃

三一六一

点赞

这是一个12个单元的

课程

应该是每周12个小时

呃我觉得呃

有时新的课

程

呃

存在一边的空气

比

预期的要花更多的时间

嗯，我还没有教一门新课

程，我们就像

所有的家庭

作业都太简单了

我们真

的应该把它提升到一个

整体的水平

嗯，所以我想

这是说

嗯

我想我们也变

得更好了

就像

我其实是

想拉你们

你花了多少时

间在事情上，把事情拨回

来

呃

如果我们过火了

嗯

，所以嗯，所以实际上有一

些适应性内置

呃

已经进

入课程能够做到这一点

是啊，是啊

是啊，是啊

我有

一个关于第十个二十三

个项目的问题

所以是的

不太清楚

这是额外的家

庭作业

这不是一个项目

所以这个项目是为这门

课的每个人准备的

四百

二十三

六百二十三

每个

人都在做一个项目

作业

六二三是

是啊，是啊

做了

什么是好的

所以基本上

你希望我们中的任何一

个做这个项目的人

呃

使

用现成的生成模型

或者

从零开始建造一些东西

所以嗯

我认为使用现成

的生成模型

或者从零开

始构建一些东西都是进

行项目的好方法

但不管

你往哪个方向走

你得建

造一些东西

这不仅仅是

一门你喜欢的课程

使用

生成模型

嗯

做一些像我

们现在这样的事情

我们

从根本上专注于打开引

擎盖

实际上就像

你如何

建造这些东西等等

但是

有很多事情

你做不到的

事

除非你用的是一个预

先训练好的生成模型

因

为训练可能要花60万美元

你没有那么多现金

我猜

我不知道

所以说

呃

是啊

，是啊

是啊，是啊

所以关于

元素，看起来就像建筑或

路易

有点像你

就像一个

更高的层次

像骗局或

我

们如何做最好的问题

所

以我们更关注

有点如何

或者如何使用LLMS

我觉得这

里的斜度肯定是

你如何

建造它们

不是很清楚你

怎么用它们

虽然我们会

在某种程度上谈论

嗯

就

像在上下文学习中

例如

并试图理解为什么这实

际上是有效的

所以这就

像是对所有成功的赞美

但我不知道

我在服役中

没有看到这一点

像数据

工程

像你的指导

一个很

好的问题

所以嗯

数据工

程就像超级超级重要说

构建大型语言模型

然而

，它并没有明确地出现在

教学大纲中

所以嗯

我们

不一定要去

所以说

我不

想这么说

我们绝对不会

谈论这件事

但我们不会

把它作为这门课的重点

有一个十一

LTI秋季提供的

六十七个大型语言模型

课程

这实际上花了很多

时间思考数据工程

在本

课程中，我们强调了一些

数据工程的讨论

但是还

有一个很棒的课程啊

这

个秋季提供的LMS课程就像

一个很好的补充

我不确

定这门课程在多大程度

上是开放的和可用的

但

你可以回头看看，也可以

在接下来的秋天接受

当

我认为它会再次提供

我

向上压

他们的研究里没

有人

哦耶

所以重用

就像

这样，为一个项目建立自

己的研究是完全可以的

它只需要被清楚地记录

下来所以你仍然需要去

做一些其他的事情

除了

你现有的研究

就像你现

有的研究就像你对待它

一样

就像别人以前的作

品一样

好啦

所以嗯所以

剩下的时间

呃

我真正想

做的是

回去好好想想

一

些基本的话题，希望能

呃

对于你们中那些通常

我

知道

有一个特定的队列

介绍ML学生

谁真的没有做

过深度学习

嗯所以嗯

对

你们来说

我特别想说的

是

圆周率火炬是如何工

作的

就像

什么是

你是如

何建造像圆周率火炬这

样的东西的它对你有什

么作用

它是怎么做到的

嗯然后嗯

我也想说一下

只是最基本的井

其实不

是这样的

其实呢

没那么

基本

呃好吧

我将谈谈两

个最基本的

所以呃

我们

会的

呃

在剩下的时间里

今天

反向传播权

所以呃

你们中有多少人以前见

过反向传播

好啦

如果你

不举手

呃

下课后我们应

该谈谈

嗯

所以呃

反向传

播基本上是通过计算图

的两次传递

在这个计算

图中有一个向前的传递

嗯

这个想法是，你基本上

是在写下一些算法

去计

算一些函数

所以y等于x的

某个f

你可以把这个函数

表示为一个有向无环图

其中每个节点都在计算

一些中间量

你按拓扑顺

序访问这些节点

呃

比如

说从下到上

然后你得到

顶部的输出

然后你做这

个从顶部开始的向后计

算

在你计算的每个节点

上，以相反的拓扑顺序工

作

呃

用你已经计算出来

的偏导数来做一个偏导

数

这就是反向传播的基

本思想

然后呃

通常

呃

当

我们实际实现一个通用

的反向传播算法时

就像

反向模式下的自动差分

嗯

不仅仅是一个神经网

络的特定反向传播实例

化

我们做的其实是

这个

替代版本

所以在这里你

会看到

嗯，我是说，我们计

算

所以如果我们有一些

我们关心的节点VJ

还有一

些中间节点

呃你一你二

你们三个

我们所做的是

通过链式法则直接计算

就

涉及这些中间项的链

式法则

我声称这实际上

不是实现这些东西的好

方法

您真正应该做的是

，每次访问一个节点

您实

际上应该增加d

dvj

所以每次

你访问一个u节点

您应该

将d y dvj递增为d y

d u

我是对的

这两

个是完全等价的算法

你

最终得到的答案是一样

的

但是第二种方法使您

能够编写更干净的代码

这允许您对您的反向传

播算法进行通用

所以基

本上Back prop是关于重用计算

向

前传球和偏导数

你在后

传计算

并允许您计算梯

度

这样你就可以训练一

个机器学习系统

好啦

所

以嗯

所以在这里

呃

这只

是一个神经网络的小图

片

这是他们来的最基本

的

我们有一些输入

X

呃

我

们在这里用线性层打它

乘以某个矩阵得到

然后

我们把它通过元素的sigmoid得

到z

然后我们再做一个线

性层

软麦克斯

然后说一

些交叉熵损失

好啦

所以

这里我们有一张计算图

的实际图片

如果你要实

施

嗯，在介绍ML课程中

反向

传播算法

您的代码可能

如下所示

但就像麻木而

不是数学

基本上就像

你

知道的

你有点喜欢

你只

是在进行正向计算

然后

你要通过这个向后的计

算

这段代码没有真正的

结构

你知道的

也许你会

在上面加评论

但我没有

很难解析

你真的看不出

你会如何重用这里的任

何东西

所以基于模块的

自动区分说你知道

这就

像一个时间代码，你扔掉

我们想要一些可重复使

用的东西

所以

嗯

已经有

了

有这整个

不同的集合

呃

神经网络

支持自动区

分或自动差异的库

然后

嗯

从历史上看，有两种不

同的类型

有静态神经网

络包

呃

要求您提前指定

实际的完整计算图

张量

流就是这样开始的

提亚

娜是另一个典型的例子

然后是这些动态神经网

络库，可以让你

在运行时

指定

其中包括像火炬这

样的东西

Pytorch

迪内特

最终Tensorflow出

现了，意识到每个人都应

该这样做

把它添加到他

们的图书馆里

所以当我

说到基于D的模块时

我在

这里真正说的是什么

这

是支持任意计算图的动

态方法吗

然后嗯

关键思

想是组件化

神经网络的

分层计算

每一层只是整

合了多个实值节点

在计

算图中

像向量值节点或

模块

模块有两个动作

正

向计算

它从某个输入,通

过某个可微函数f得到一

个输出

这就是事情的这

一面

这是通过f得到b

然后

向后计算得到一个梯度

不只是任何梯度

但是如

果你的损失函数或目标

函数是j

那么呃

我们有呃

j相对于

存储在g子中的变

量b

模块的向后计算应该

接受GB

B和A

或者目标函数相

对于变量的梯度

A

所以呃

基于模块的广告的想法

是你可以有一堆小模块

为你做这些的人

这是一

个乙状结肠模块

所以它

的正向函数会把A和应用

段

乙状结肠元

向后的会

做这种元素的计算，1-b

B和gb得

到g

然后是一个SoftMax模块

所以

你知道会有类似的结构

但是内部计算是不同的

就在这里，我们正在计算

前进中的SoftMax

向后涉及到这

种对角化或从向量产生

对角矩阵

呃

然后做一些

简单的矩阵向量乘法得

到实际的输出

嗯然后你

知道

线性模块和交叉熵

模块的类似结构

所以一

旦你实现了这些东西

您

可以回到您的过程实现

你可以写你的正向计算

即线性向前和乙状结肠

向前

然后是线性向前，然

后是软最大向前，交叉熵

向前

然后你可以做同样

的事情

向后交叉熵

向后

Softmax

线性向后

乙状结肠向后

线性向后大

现在您重用

了一些代码

烦人的是

你

还是得写这个代码

会很

好的

如果你不需要写代

码的话

因为反向传播是

一种通用算法

它在任何

计算图上都是一样的

为

什么我们每次都要把这

个写出来

好啦

所以实现

模块的那种面向对象的

方式将与我们以前所拥

有的完全一样

除了现在

我刚刚把实际的函数更

改为方法到类方法

对呀

所以像乙状结肠类这样

的每一个类都有

它是对

模块类的扩展

都有一个

前锋和一个

它有一个向

前和向后的方法

然后SoftMax有

一个向前和向后的方法

对，同样的事情一直下去

在某个时候

必须有人实

际实现每个模块的前向

和后向

通常当你实现它

的时候

您实际上并没有

用Python实现

您实际上将其实

现为CUDA内核

呃

所以它在GPU上

运行得很快

但是一旦你

实现了它

一旦每个人都

能重用那个模块

这个想

法是这样的

就像神经网

络模块

然后呃

你构建不

同的层

所以你有一个线

性层和乙状结肠

线性软

最大值和交叉熵

然后你

定义你的正向计算，说

好

啦

我们将把我们的投入

是X

我们的张量y

我们的参

数α和β

我们将把我们的线

性层调用

向前应用于x和

alpha

然后我们的乙状结肠层

被应用到一个

在一层中

的输出

然后这被应用于

z和beta，以此类推

最后我们返

回目标函数j的张量

所以

说

嗯

这其中的美妙之处

在于

嗯

在向后

我们实际

上并没有详细说明如何

通过你的神经网络反向

传播

相反，我们只是倒着

调用磁带

这是一个特殊

的功能

运行我们刚才反

向进行的正向计算

基本

上以正确的顺序调用每

个模块的向后方法

那么

这是如何工作的呢

所以

关键是

你有一些全球磁

带

在这里，我想让你想象

一个图灵机

对呀

就像那

种带子

就像为你储存信

息一样

但是这盘带子

它

也是作为堆栈实现的

所

以你可以把东西推到上

面然后把它们弹下来

然

后呃

这些模块中的每一

个

所以它初始化

所以你

通过存储一个输出张量

来初始化任何模块

嗯和

输出梯度

所以输出张量

就像实际的

你知道的

从

模块中出来的

输出梯度

类似于目标函数相对于

b的梯度

好啦

所以这种方

法向前应用

你会注意到

我们刚刚定义的这些模

块

然而，在这里，我实际上

称之为应用向前方法

所

以这是一个非常谨慎的

选择

因为

申请

正向方法

所做的不仅仅是该模块

的正向计算

也就是说，它

首先接受的不是张量列

表

但是其他模块的列表

它做的第一件事就是抓

住所有的张量

就像数学

有点

像Numpy数组

喜欢来自模

块的对象，并将它们馈送

到该类的正向方法中

这

给了我们一些张量，我们

存储作为我们的向上

最

关键的是在这里的12行

呃

我们像自己一样把这个

模块推到磁带上

所以这

记录了我们刚刚做的向

前计算

所以这是有趣的

部分

呃

当我们向后调用

磁带时

嗯

我们实际上可

以反向运行那盘磁带

并

以适当的顺序调用所有

向后的方法

这是倒着放

的带子

所以当被称为磁

带的堆栈

呃里面还是有

元素的

我们弹出添加到

其中的最新内容，并将其

称为模块

向后应用方法

毫不奇怪

向后应用只是

调用向后方法来获得适

当的渐变

然后，这是美丽

的部分

它在做

我们在版

本B中谈到的增量

它说每

个模块都是这个模块的

输入

我们把刚才计算的

梯度

并增加这些模块的

输出梯度

所以嗯

这种基

于a的模块实现方式非常

接近于pi

torch正在做的事情

呃

所以呃

所以这里我用了

同样的神经网络

我在这

里为我们定义了

呃

布局

不同层的init方法

然后向前

的方法穿过这些单独的

层

你会注意到交叉熵的

损失

我们故意避开这个

神经网络

因为如果你这

样定义神经网络

然后可

以在训练时间或测试时

间使用

而损失真的只是

训练时间的事情

所以我

们把它放在模块外面

与

之前的幻灯片如此不同

但几乎无关紧要的是

然

后当我们想采取一步随

机梯度下降

呃

我们只要

定义损失函数

优化器

这

就像随机梯度下降

我们

计算

啊

这里少了一条线

对不起

我应该说模型等

于神经

我真的很需要这

堂课

神经网络

然后我们

说模型接受我们的输入

然后我们的损失函数从

模型中获得预测

和地面

真相Y得到我们的损失

然

后我们在优化器中归零

梯度

然后呃

我们称丢失

的圆点为向后

这就是倒

着放带子的原因

呃

最后

，优化器的点步骤是采取

梯度

与梯度方向相反的

一步

好啦

所以圆周率火

炬只是一个自动微分的

例子，你可能已经注意到

了

虽然这里我们不叫线

性向前一点

在这里我们

称它为自点

x的线性之一

但线性的实际上是一个

类

它是一个真实的物体

那么为什么我们不调用

类的方法呢

这就是处理

类或对象的方法

您在它

们上调用方法

事实证明

，高炬在句法上有糖

嗯

有

多少人知道下划线

调用

下划线下划线方法

哦很

好

你们中的一些人

嗯好

吧

所以呃

双下划线特殊

方法

所有双下划线都允

许您定义处理对象时会

发生什么

好像它是一个

函数

好啦

所以

换句话说

运行x的线性等价于运行

线性点下划线

在pi torch中调用

下划线x

这几乎和沿着x向

前线性运行是一样的

但

是有一些特殊的簿记记

录记录磁带

例如

你们会

注意到在我的自动微分

理论中

我把所有这些张

量看作是平等的

投入

产

出

参数

它们只是计算图

中的张量

嗯在Pytorch

但你会注

意到我们定义了线性层

但我们从来没有把参数

传递到线性层

那么他们

在哪里

Pytorch

为了使代码更干

净，将参数存储在实际的

模块类中

还有呃

这很方

便，因为这样

嗯

它还可以

将这些变量标记为参数

因为当您有一个优化器

类时

它想知道你真正想

要的渐变是什么

就像

例

如

我们可能不希望这里

有关于x的梯度

也就是说

我们的投入

如果我们在

做图像分类

那是我们的

输入图像

嗯

所以说

通过

标记什么是参数，什么不

是参数

Pytorch就是用它来

减少

实际需要保留的内存量

呃

根据您实际计算和存

储的渐变

手持火炬

深度

学习变得很容易

嗯，所以

我其实

最近和一个学生

谈话

呃是谁花了我一点

时间

呃呃

了解学生关心

的是什么

因为他一直在

解释

你知道他选了一门

课，他所做的就是

你知道

用Pytorch建立深度学习模型

我

就像

那太好了

所以你

你

懂深度学习

他就像

哦不

我不知道Pytorch是怎么工作的

可以进去用圆周率手电

筒

也不明白反向传播是

如何工作的

因为你不需

要真正了解反向传播是

如何工作的

你仍然可以

让pytorch为你做一些事情

嗯

我

认为了解反向传播是如

何工作的是有价值的

你

们所有人都将接受

你会

如果你不知道

呃

什么背

道具

好啦

也就是说，我们

现在可以考虑一些复杂

的深度模型

我们也不用

太担心

呃说

做复杂的矩

阵演算来计算它们的梯

度是什么

所以我我想

你

知道的

今天做一些世代

然后嗯

我们实话实说吧

生成数据的最简单方法

是使用Engram语言模型

这些东

西已经存在很长时间了

呃

所以你的目标是用人

类语言生成逼真的句子

这个想法只是以最后n减

去一个单词为条件

抽取

第n个单词

对呀

所以如果

你想

你知道如果你有像

蝙蝠这样的东西

现在你

要做的是

你想生成下一

个单词

所以你拿一个多

面模具

这是给蝙蝠的

呃

对不起

球棒做了，你滚了

，死了

它出现在其他一些

单词上，比如噪音，所以这

是你的下一个样本

现在

你拿起另一个死亡

这是

制造噪音的模具你滚动

它它就会出现在

这是你

的下一个样品

然后你的

噪音滚出来，你得到了骑

士

所以这是一个从Engram语言

模型中采样的句子

嗯所

以有了Engram语言模型

我们现

在可以定义序列上的概

率分布

我们早就可以这

么做了

虽然只是用概率

的链式法则

就像序列w 1到

w 6的概率是由链式法则决

定的

W的一次概率

给定1

W 2的

概率

以此类推，一直往下

走

嗯所以

Engram语言模型说得

很好

我们实际上可以定

义一个长度序列上的概

率分布

T

嗯

做一些独立性

假设

啊

他们上来了

我不

敢相信他们真的

这是整

个课程中唯一会出现这

些的地方

嗯

所以我们在

这里做的独立假设是

假

设w 4只依赖于w 3

它有条件地

独立于前面给出的所有

其他单词

好啦

呃所以呃

现在这个序列的概率所

涉及的信息

所以这是呃

n等于两个n克模型

这是一

个n等于三克的模型

我们

现在以前面的两个词为

条件

所以嗯

你可以了解

这些概率

嗯

你可以把这

些概率想象成一个大的

概率表

这些不同的多面

模具中的每一个

嗯，你知

道

取决于你是什么

呃

你

知道的

每一个看起来都

有点不同

对呀

制造噪声

对噪声和污染有很高的

概率

是啊，是啊

制造噪声

污染

呃

呃

但制造噪音斑

马的概率非常低或为零

所以你可以通过从数据

中计数来了解这些概率

所以如果你有一个数据

集合

呃

你可以去找

呃说

如果你想学习这个特定

的概率表条件对序列的

影响

牛吃东西

然后您可

以查看数据集中的所有

内容

所有你看到牛吃东

西的地方

然后数一下后

面的单词跟着正确的次

数

十一次中有四次是舒

适的玉米

十一次中有三

次吃草

嘿十一个中的两

个

现在你得到了前两个

单词下一个单词的概率

分布

好啦

然后你就可以

呃

在你学会从你的

呃

像

这样的Engram语言模型

如果你

想

你可以训练它

说莎士

比亚全集

你会看到一些

非常非常糟糕的莎士比

亚

你可以在右边看到

所

以嗯

这就是两千年初的

情况，对吧

呃

这些是模型

他们是我们最好的模特

嗯

他们是稍微好一点的

但他们真的很慢，他们没

有得到更多这样的你

所

以每一个机器翻译和自

动语音识别系统都使用

Engram语言模型

好啦

于是出现

了递归神经网络语言模

型

他们把事情做得更好

一点

这里的主干是典型

的递归神经网络

我们说

我们有一系列的输入

X和

呃

我们将有一系列的输

出

注意x 1本身是一个向量

对呀

这是一个输入向量

然后这是另一个向量

以

此类推

呃

每一个都是一

个向量

好啦

下面是实际

计算的工作原理

嗯，所以

我们要采取英寸

假设我

们想在这里计算H2

所以要

计算H2

我们先在这里取x 2

我

们把它乘以大矩阵w x

h

我们

再加上

呃

某个h乘以一个

不同的矩阵w h h

然后我们加

上一个偏置向量

好啦

到

目前为止我们得到了x2和

h1的线性组合

现在我们通

过一些非线性激活函数

书法H，这给了我们H两个

所

以现在呃

给定h

2

我们可以

计算出y 2

所以y2只是h2的线性

函数

现在我们每走一步

H 3是x

3和H 2的函数，依此类推

向

下

好啦

所以嗯

让我们回

到概率的链式法则

我喜

欢这条规则

你知道的

它

适用于任何概率分布

它

适用于输入的任何顺序

其实呢

我们会

我们会解

释为什么这会有用

当我

们考虑变形金刚语言模

型时

然后呢

呃

对于一个

递归神经网络语言模型

呃

我们实际上要用概率

链式法则

稍微扭一扭

我

们要说

嗯

我不喜欢的一

点是

其中的每一个都是

不同的概率分布

因为这

件事的条件是五个字

而

这个只有三个条件

好啦

所以为了解决这个问题

，我们可以说

如果我们先

把前面的单词转换成一

个固定长度的向量

然后

我们定义下一个单词的

概率分布概率

以固定长

度向量为条件

这使得我

们可以使用概率链式法

则

在给定所有前面的单

词的情况下定义下一个

单词的概率

我们只需要

函数fθ

从任意长的向量序

列中得到一个固定长度

的向量

所以RNN语言模型

毫

不奇怪

用rnn定义fθ

一个复杂

的函数来获得

说

像H5这样

的矢量

这是所有向量的

固定长度表示，直到时间

第五步

就像H3是一个固定

长度的表示

通过所有向

量的方式，相同的长度

第

三步

rnn语言模型说OK

我们要

接受某种特殊的开始符

号

就会得到零氢

然后我

们得到一个在给定h-1的情

况下的概率分布

我们从

一个单词中取样，现在我

们从刚才取样的单词中

取样

虽然我们反馈，我们

得到一个矢量

这是h 0的rnn部

分

然后呢

不管这是什么

现在我们有一个关于2的

概率分布

对不起

我们对

蝙蝠这个词进行了采样

现在我们继续沿着这条

路走下去

我们得到了

嗯

呃

下一个单词的概率分

布

考虑到前面所有的话

RNN给了我们向量表示

我们

可以继续取样单词

就像

我们从Engram语言模型中做的

那样

但现在我们的概率

分布来自神经网络

好啦

唯一的问题是我们没有

说

我们如何实际创建发

行版

给定h t的w t的p

Ht是一个向

量

如何得到单词上的概

率分布

拿一个新的图层

然后用柔和的声音

适用

于线性层的大软最大值

应用于HT

给了我们一个概

率分布

我们应该说这个

线性层应该有多长

以便

SoftMax具有适当的词汇量

但仅

此而已

现在我们有一个

合适大小的单词的概率

分布

它是由这个神经网

络定义的

这是基于之前

的所有单词

好啦

所以嗯

我们可以把所有这些概

率相乘

这就给了我们整

个序列的概率

我们可以

从这个模型中取样

就像

我们从Engram模型中取样一样

我们现在要做的就是定

义这些概率

这些不同侧

面的染料的重量

基于一

个轻度复杂的神经网络

好啦

所以呃

事实证明，如

果你这样做

呃

你回去

嗯

，样品要好得多

其实他们

是

他们是

它们要好得多

，你可能需要一秒钟才能

真正弄清楚

呃

哪一个是

正确的标签

我让你考虑

一下

这是我们的第一张

照片

文本的生成模型

嗯

文本的两种生成模型

一

个非常简单的Engram语言模型

做得很糟糕

一个递归神

经网络语言模型做得更

好

使我们能够得到一些

可以稍微过去的东西

被

冒充为真正的莎士比亚

文本

现在的问题变成了

嗯

呃

如果我们能训练一

个简单的递归神经语言

模型

只要莎士比亚全集

，然后得到一个看起来像

这样的东西

如果我们有

一个稍微好一点的模型

和更多的数据，我们能做

什么

所以我们会接受这

个想法

下次再继续

呃同

时

嗯我们会

呃

我们会让

你做零号作业

现实地

我

其实还没写完作业

但我

明天早上会把它拿出来

的

对你们来说

呃

他们已

经做了几个月的Pytorch

嗯

如果

你没有做重量和偏见

像

一堆这样的作业

只是让

你熟悉重量和偏见

这是

一种很好的绘图和记录

的方法

所以希望里面有

新的东西给你

我们下期

再见
