所以欢迎大家来到人工

智能的第一次迭代

呃

十

四二十三斜线十六二十

三嗯

很高兴你们都能来

我们真的很期待这个学

期

尤其是看到

不只是

你

知道的

你们这学期要学

的东西

但你们用的东西

我们正在学习做的事情

因为我觉得作为一名教

练

会很有趣的见证

你想

出的项目

你们在这门课

程中将要学习的有趣的

方向

嗯，所以我真的很兴

奋这个学期

我想用

呃

一

点点

呃

像一个大的

非常

非常大的图景

什么是生

成性的问题

当我在想这

个的时候

我有点恍然大

悟

你知道吗，通常当我教

课程介绍的时候

我喜欢

从

艾对

我和我说

哎呦

你

知道的

有这个

有这些很

好的子球

感知

推理与控

制

运动操纵

规划

通信

创

造力

学习

但就像这些和

生成人工智能有什么关

系

我是说

老实说

感觉你

知道

这些事情中的大多

数实际上与生成性没有

任何关系

爱

好啦

有谁不

同意

但你可以选择一个

次级目标，在这里不同意

我的观点

我是说

这个

这

与我们这学期的课程无

关

是啊，是啊

感知似乎很

重要

所以它不是每次都

产生完全相同的东西

你

可能

如果你正在生成一

些东西

想要别的条件

为

了控制我们

那是不同行

为的形态

用于控制

你可

以捕捉不同行为的不同

模式

是呀

这个我真的想

了很多

我想你是对的

虽

然有一个多模态的东西

，我没有想到

是啊，是啊

其

他想法

创造力是伟大的

我觉得这有点

呃

你知道

的

我们开始看到的地方

呃

这些技术的采用

只是

艺术家和其他喜欢

他们

将自己的创造力注入模

型的方式

这本身可能不

仅仅是随机的

以一种我

们希望是创造性的方式

是啊，是啊

人类是不同的

这是个很好的问题

我们

所说的智能机器到底是

什么意思

这是不是像智

能

像人一样

还是说

呃

像

不同于人类的东西

我不

知道

我不知道

我不知道

但我们绝对不是说

呃

我

喜欢进入这个想法

就像

超人的智慧

就像嗯

所以

我认为所有这些都有一

些有趣的角度

你可以开

始从生成的角度来看待

艾对

所以交流是关于理

解的

语言的生成与生成

大型语言模型在这两个

方面都很出色

即使他们

他们被训练产生

学习我

们传统上一直认为是参

数估计的任务

然而，生成

人工智能向我们展示了

这种新的做事方式

在上

下文学习中被称为

在这

里，学习这个词几乎有点

用词不当

从我们通常的

做法来看

用它

呃

因为学

习实际上是在测试时的

推理过程中发生的

嗯

根

本没有参数估计

大型语

言模型也擅长某些推理

任务

如果你知道思想链

提示

这是一个很好的例

子

嗯

它们已经被用来

禁

足了

用具身代理进行规

划

这我甚至不知道

我只

是谷歌了一下

你知道的

基础模式和规划

显然有

人已经做了LLM计划

嗯所以

呃

创造力是显而易见的

文本到图像模型、文本和

音乐模型浮现在脑海中

嗯

在感知领域

多模态基

础模型正在学习回答关

于图像的问题

以及图像

中的文本

对呀

就像如果

呃

如果文本实际上表示

为图像中的像素

嗯也是

人们使用扩散模型作为

零点分类器，如

以一个被

训练生成图像的模型为

例

并用它对标签进行图

像分类

这是以前从未见

过的控制

情感与操纵

嗯

是的

其实呢

有一种技术

叫做白日梦者

学习经验

的生成模型来强化

在那

里我们实际上产生了你

会模拟的那种东西

所以

这是一个很好的例子

在

那里你知道你可以学会

拥有一只四足动物

那个

呃

仅仅从一个小时的真

实世界经验中就能学到

好啦

那么生成人工智能

和这些目标有什么关系

呢

我觉得它有很多

而且

越来越多

呃

随着近年来

它变得越来越突出

我觉

得

呃在增长

然而，我承认

仍有许多未回答的问题

我们到底是什么意思

不

仅仅是智能机器

但是但

是你知道

实现这些目标

和评估是不在这里的事

情

但这是个很难解决的

问题

好啦

所以呃

生成示

例

爱

我想你们见过很多

这样的东西

这就像GPT

例如

写一个关于存在无限多

个素数的证明

用莎士比

亚的风格来做

通过双方

的对话来玩

争论证据

对

呀

嗯，就像

证明的最后一

部分

你知道这是简单的

朋友

等着瞧吧

我们将把

素数相乘

那是关键

从2到

p的所有素数组合

我们要

做这个新Q

我们会发现

但

我们还必须包括一件事

，以使证据更加滴水不漏

我们在后面加一个

现在

证据就在我们眼前

是的

，它还解释了如何将它们

相乘并添加一个

这是经

典的欧几里得证明

所以

呃也

呃

图像编辑是一种

完全

这为人们创造了一

种全新的图像编辑方式

嗯

你知道的

您可以取出

图像的一部分，并让模型

将其填充回来

你可以去

掉颜色，让模型生成不同

的颜色

呃

呃

你们周末都

在做一些文本到图像的

生成

创建一个小数据集

嗯

这里是呃

尤其是你知

道

呃

一个由稳定扩散产

生的人脸图像的令人惊

讶的好例子

嗯嗯

音乐世

代

呃

我们的音频坏了

否

则我们会听到一些

我想

说的是非常平庸的音乐

是由一个模特创作的

嗯

这就像一个非常未解决

的问题，极其困难

但是音

乐天才就在外面

这是一

个

这是一个基于变压器

的解码器模型

呃

在这些

类似于离散的音频表示

上

它可以得到多层不同

的仪器

似乎在听起来像

音乐的东西中走到了一

起

嗯和嗯

呃

代码生成

呃

这可能是一个你已经与

嗯

这就像经典的例子

好

吧，现在经典

这就像在2023年

我想上课

我不知道经典

这个词是否适用

但是但

是英镑

在那种

嗯那个

他

们开始看的那种报纸

GPT 4和

它能做什么，基本上试图

做广告

向世界开放

我就

像一个

你知道一个四五

页的聊天广告

英镑真的

嗯所以这是他们在看的

地方

它是如何产生

Lotech的Tici代

码

如果你最近没用过tii

基

本上你知道

日志技术可

能是有史以来最难调试

的编程语言

当您击中错

误时

因为它的错误信息

是如此不透明

Tixie把它乘以

十

就像在ltech里面

一点语言

所以在迪克西画独角兽

是非常困难的

然后呃

他

们让GBD 4做这件事

在我训练

的一个月里

事实上，情况

正在逐渐好转

嗯和嗯

我

想我不知道我的痴迷是

什么

这基本上就像我用

的聊天一样

gpt for用于生成lte

我

可以自己编码

我只是不

能写ltech

你知道它可以把看

起来像ltech的东西变成迟到

的东西

他们也可以

做一

些事情，比如编写Pytorch代码

嗯

所以嗯

最近

我们开始看

到潜在扩散模型的应用

生成多个相关帧

如果你

把这些放在一起

你得到

视频和

嗯

他们有点有趣

如何在这些潜在变量之

间进行时间对齐的不同

技术

对呀

所以呃

有一个

有张报纸

保留您自己的

相关性，实际上包括时间

创建的相关噪声

所以在

这里使用的噪音

扩散模

型

这种去噪声图像实际

上是随着时间的推移而

相关的

所以正如你所知

，你会得到一些相机旋转

或什么的

就像这个一样

用于生成图像的实际噪

声是相关的

其他方面

这

可能涉及到某种卷积和

时间上的注意力层

呃

在

时间上交错着相似的层

跨越不同的空间维度

所

以也越来越需要丰富的

理解

你如何利用这些不

同的技术并扩大它们的

规模

所以呃

这是一个用

于训练大型语言模型的

开源数据集的示例

这叫

堆

就像一个点

两万亿代

币

虽然我想如果你去掉

重复

比那个少一点

嗯

但

你实际上可以看到

嗯

这

些数据是从哪里来的

而

且它是相当多样化的

对

所以嗯

这里好像有一大

块

那是酒吧中心

所以很

多医学文献

有一堆免费

法律和档案馆的

更多的

技术论文

就像我们读到

的各种

嗯有嗯开放的网

络文本

呃

这是呃

一些网

络爬行数据的大规模收

集

以及代码

比如GitHub和一些

呃

洛杉矶技术来源也是

DMF

然后这里还有其他的东

西

喜欢字幕

那实际上是

在看电视上的字幕

这样

它就可以接触到对话和

其他类似的东西

所以弄

清楚要包括哪些数据组

合

呃

当你训练一个生成

模型的时候

我们这个时

代面临的巨大挑战

还有

很多

呃

有很多这样的谣

言

然后呢

我不会的

我不

太想说是新闻

这表明G B的

原因之一

T

四个一直保持

在

许多大型语言模型的

不同排名是OpenAI拥有所有这

些

呃

这些类型的

嗯，不同

的未披露数据合同

呃

使

其能够

呃

不仅仅是利用

我们

这是相当大的

但不

仅仅是使用像这样的小

型开源数据集

但是要结

合许多其他数据源

呃

也

许其他型号没有机会

也

是

对生成人工智能的理

解有很多不同的方面

我

们如何正确地训练这些

模型

如果我们只是训练

他们

呃

概率上

呃

他们不

一定总是按照我们想要

的方式行事

所以弄清楚

你怎么能在训练循环中

注入某种奖励模型

这样

你就可以把它引导到

一

种特殊的学习方式可能

非常重要

在生成人工智

能中也会出现很多不同

的问题

使用内存

你知道

有一次

呃

我被告知在哪

里

呃

你知道

有这样的专

家

呃

在电子游戏行业

呃

请注意，这是现在

这些人

是为像雅达利这样的东

西写游戏的人

就像最初

的任天堂

他们最大的限

制之一是

你如何将这个

游戏的所有信息打包到

这个

你知道这个小小的

墨盒

它只有

你知道它有

几千字节的内存

他们想

尽了办法

我们怎么能把

我们需要你的一切都塞

进这一点点记忆中

似乎

我们现在已经回到了机

器学习的那个点

在那里

我们建立了有这么多参

数的模型

我们又回到了

试图找出

我到底要怎么

把它塞进TPU

我有三个两个

G的内存

或者那个特斯拉

一个只有80G内存的100个

对，呃

理解模型本身

也有一些

系统

这些GPU如何工作的方

面

和不同类型的记忆层

以及不同内存访问的速

度

对于让这些模型高效

运行变得非常重要

还有

其他事情，比如

如何在多

台不同的机器上分发模

型

嗯，火车的费用已经增

加了

呃

戏剧性地

所以当

你思考什么是生成性的

时候

我认为另一个有趣

的方面是

语言与视觉的

融合

所以我这里有一个

语言建模的时间线和另

一个时间线

呃

用于图像

生成

在这两个时间线上

呃

二十七年有一个重要

的时刻

呃

那是

呃

的发明

呃

变压器

二十七年后在

语言建模方面发生了什

么

那是呃

大多数语言模

型切换到使用变形金刚

说Rnand

同样地

呃

在计算机视

觉中

呃

很多很多的模型

呃

CNN的

然后一个变压器出

现了

花了更长的时间

说

二二一

当视觉变压器为

计算机视觉社区问世时

也开始切换到变压器

但

现在有一种感觉

我很确

定如果我被逼

呃

我们可

以用一个模型来教整个

课程

变压器

我们不需要

提到任何其他

任何其他

模式

至少今年不会

所以

嗯所以

但我认为重要的

是

我们来谈论跨越这两

者的技术

呃

什么这两个

呃

过去看起来非常不同

的领域或模式

以前看起

来非常不同的技术

所以

有一段时间

你知道的

什

么时候啊

当CNN在这里做一

切的时候

所有的事情都

是和这里的RNN一起做的

为

了深入了解什么是有效

的

像语言这样的一种形

式，并将其转化为视觉有

时是困难的

但现在这种

翻译发生得更快了

使我

们能够做更多的事情

以

多种方式

所以对于这样

的课程

我想这其实是一

种偶然

我们能够利用这

种时间线的合并

因为我

们可以加快速度

我们来

看看这些不同的技术

因

为他们之间有很多共同

点

好啦

所以嗯

让我在这

里暂停一下

呃

在我继续

讲下去之前，你有什么问

题要问？

一点都不像不像

但不是像这个伟大的问

题

我想我们没有时间

呃

三个D形

我很想去那里

但

我的意思是

看我们的日

程安排

我们很有话题

我

想我们不会得到三个D形

状

所以说

我觉得呃

我忘

了课程编号

但我想你可

以

你可以去拿16824什么的

它

在课程中表现出来

十六

八二十四

有没有人知道

听起来像吗

否

否

有人在

点头

好啦

也许我错了

但

是在里有一个课程

有这

个的

如果你四处打听

所

以我认为这

呃

这个问题

很可能是浪费时间

我是

说

你们都坐在这里意味

着

你们都有理由想学习

元爱和我也许可以跳过

这个

但我情不自禁

但是

而且而且真的

我希望你

们都能离开

今天用一个

很好的比喻

所以当别人

问你

你为什么在人工智

能班

比如你的父母或者

你有一个真正的

你知道

的

快速回答他们

所以回

想一下汽车被发明的日

子

内燃机是一种新事物

还有呃

当时有个叫亨利

福特的家伙

他有这样的

想法每个美国人都应该

有一辆车看看他给我们

带来了什么

像现在这样

的男人

当你开始考虑汽

车

就像从车里出来的一

些好东西

但也有一些不

太好的事情

汽车数量上

升

这是

呃

这就像一个

我

不知道你有没有在新闻

里看到

但最近它得到了

很多关注

这是行人死亡

人数

在一九八一年和二

十二年之间

所以呃

那只

是在美国

实际上它正在

欧洲流行

嗯

所以也许你

知道我们在汽车上有这

些问题

对呀

你知道他们

在耗油

它们是极其危险

的金属大块

就像飞来飞

去

嗯

不顾行人

也许我们

可以解决这一切

如果我

们有这么漂亮的自动驾

驶电动汽车车队

对呀

我

的意思是这绝对是美丽

的

你知道我们怎样才能

很好地到达那里

我们必

须真正了解这辆车的内

部工作原理

因为你得打

开引擎盖拆下引擎

也许

放一个FRC

然后把引擎放在

然后你还得把方向盘移

开

因为方向盘是问题的

一部分

对呀

就像你让人

摸方向盘

这只是个坏主

意

对呀

然而，如果他们只

是这些就像

你知道自动

驾驶电动汽车

一切都会

迎刃而解

所以我们需要

的是深入了解这些东西

内部工作原理的工程师

你知道我真的认为在这

种情况下你知道什么都

不会出错

我的意思是它

只是

嗯

好啦

但关键是，你

知道深刻理解这些东西

是至关重要的

因为我想

你知道

也许生成人工智

能不像下一个内燃机

不

是那种程度

但我觉得有

很多

事情进展顺利的潜

力

事情很有可能出错

所

以你们都需要对这些东

西的工作原理有深刻的

理解

所以当有人让你从

A点到B点

你说实际上我认

为这实际上是完全错误

的方向

我们应该从A点到

C点

这样我们就不会以另

一轮

你知道的

又大又重

的金属块污染了一切

所

以嗯

从根本上说

生成AI呃

里面是什么

这只是概率

建模对吧

它有可能在时

间t进行下一次观测

再加

上之前所有的观测结果

然后嗯

然而，我们将花很

多时间思考

你如何定义

概率分布

好啦

所以嗯

这

些天因为我们有深度学

习

呃

你知道的

一般来说

，我只是想模拟每一个可

能的变量相互作用

永远

正确

我只想让每个变量

都以它之前的所有其他

变量为条件

人们过去用

一种不同的方式做事，叫

做独立假设

你可能从来

没有听说过这些

嗯

这是

我在学习机器学习时学

到的东西

呃

但我们摆脱

了独立的假设

条件独立

性假设

我们用像RNN语言模

型这样的东西来取代它

，它只是以一切为条件

嗯

，没有条件独立的假设

我

们这学期要去哪

嗯

我们

将讨论文本的第一个生

成模型

然后我们会考虑

图像的生成模型

从那里

我们会考虑

呃

你实际上

是如何适应的

呃

大型

比

如真正的大型模型你如

何以高效的方式做到这

一点

我们将转而考虑多

模态

呃

基础模型

我不敢

说

只是一种多模态生成

模型

因为我觉得这有点

让人困惑

就像

你学习多

模态模型实际上是为了

什么

是生成文本还是图

像还是两者兼而有之

或

者两者都不是

嗯所以所

以在这个意义上

好吧，它

不一定那个空间里的所

有东西都是生成的

所以

我们要稍微作弊一下

那

么呃

我们要讨论的其他

事情是

你实际上是如何

扩大规模的

有很多有趣

的问题

不仅仅是系统问

题

呃

但这也是一种算法

问题

这种与系统局限性

的联系

嗯，所以这不会是

一个系统的课程

嗯

没有

任何系统可以做到这一

点

呃

但如果这是你真正

感兴趣的事

有一些非常

酷的机器学习系统课程

如雨后春笋般出现在csd和

lti

我可以告诉你那些

嗯

我

们还将考虑这类模型会

出什么问题

嗯然后

呃

然

后呢

在时间允许的情况

下

我们会我们会考虑一

些高级的话题

呃

三维建

模目前不在该列表中

呃

但谁知道也许我们会走

运

嗯好吧

所以我想通过

几个

呃

教学大纲要点

教

学大纲是

呃

在课程网页

和

实际的课程政策是必

读的

嗯

这是呃

很可能是

一个大球场

所以呃

课程

政策往往变得更加重要

球场越大

嗯

所以说

所以

请务必熟悉这里的

让我

指出他们的几件事

嗯

这

门课的评分就像

40%的家庭

作业

百分之十

呃

低赌注

测验

一中期中考试两成

然后二十个

5%的项目和5%的

参与

所以课程的大致组

织方式是

呃

第一个说

五

分之三的课程有点

呃

家

庭作业和测验为主

对呀

所以

呃

大约有四个模块

每个人都会有一个家庭

作业和一个与之相关的

小测验

然后前四个小测

验就会进入

呃

这就是考

试的内容

或许我们可以

再考一次

然后我们放弃

你的最低测验

后面五分

之二的课程

呃会专注于

项目

嗯和嗯

所以你会

你

们一组一起做

让我看看

啊

然后你会注意到我提

到了呃

有五个家庭作业

但我说四个模块

那是因

为第五个家庭作业实际

上是

呃

这就像家庭作业

零

我们是

我们只是

呃

我

一会儿再谈这个

所以呃

这是一种特殊的

考试日

期定了

3月27日星期三有一

次随堂考试

为了家庭作

业

你有

呃

家庭作业提交

的六种弹性日

在你可以

的地方

嗯，提交晚了，没有

惩罚

呃

你知道的

迟交三

天后接受提交，不得延期

扩展真的很像紧急情况

如果你只是

你知道流鼻

涕

那是个好日子

类似的

事情

如果你在医院

然后

就像不用担心一样

就像

你会

你会得到延期的

嗯

嗯

如果他们看到其他类

别的教学大纲

掉进桶里

嗯背诵

呃

会有

呃

差不多

每个作业都有一个

这将

是在星期五

在上课的时

间和地点

这些是可选的

但更多的互动会议

读数

将是

呃

在线PDF技术将使用

包括所有你以前可能都

用过的熟悉的东西

嗯

学

术诚信

呃

本课程大力鼓

励合作

关键是你需要记

录你的合作

嗯

您的解决

方案应该始终独立编写

嗯也就是说

呃

哎呦

所以

这门课的合作策略

如果

你拿了

呃

十三零一十六

零一

嗯

本课程的协作策

略与该课程不同

嗯

所以

如果你选了这门课

请仔

细阅读这些协作策略

呃

不要以为它们完全一样

嗯

所以我认为这门课的

政策是鼓励合作

呃

以一

些谨慎的方式

比我们在

那门课上做得更多

嗯

然

后办公时间被贴在谷歌

日历上

嗯是的

所以我鼓

励你问很多问题

举手打

断

或者事后再问

在广场

上

嗯

本课程的简介是机

器学习课程的介绍

所以

最初我们只是

你知道这

里列出的机器学习课程

介绍的通常列表

包括七

十五

但后来我们说

啊

但

深度学习课程也有很大

的先驱者

呃

它本身就是

机器学习课程介绍

它没

有任何先决条件

嗯所以

嗯

如果你拿了

你知道的

十一四五六八十五

八七

八十五

这也是这门课的

一个很好的预科

嗯

但我

们不想让你误以为

深度

学习是一种预演

这不是

预演

嗯

所以Pytorch和深度学习

并不是提前假设的

嗯

这

在你们之间造成了一点

分裂

因为这取决于你在

哪个学期上的预科

您可

能大量接触过Pytorch中的深度

学习，也可能没有接触过

好吧如果你碰巧

例如

以

我去年秋天的机器学习

课程介绍为例

您已经实

现了一个转换器语言模

型

是的但是如果你选修

了不同的机器学习课程

你当然没有

那也没关系

嗯所以家庭作业零

呃

我

之前提到的是让每个人

都跟上速度

呃圆周率手

电筒和舒适

尤其是如何

使用它

嗯和嗯

然后我们

也会花

呃

只是其中的一

些

这些早期的讲座经历

了一些

深度学习中的一

些基础话题

那将是对

呃

对你们中的一些人来说

对你们中的其他人来说

是新的

所以我们会让每

个人都在同一水平上

在

最初的几周里

好啦

所以

呃

这是一张五份家庭作

业的图片

呃

家庭作业零

是一种独特的，因为它可

能是你的背景

也可能不

会

呃

然后是家庭作业一

二三四你们要跳进去

做

一些有趣的事情

呃

建造

呃

不仅仅是

呃

你知道的

有点啊

一种老式的变形

金刚语言模型

但是你是

如何建造一个最先进的

据说有滑动窗的张力

嗯

用于图像生成

我们会好

好想想

您实际上如何为

适配器构建一个gan或扩散

模型

我们将考虑如何使

用一个相当大的语言模

型

然后在上面放一个适

配器

嗯和嗯

用于多模态

地基模型

嗯

取决于成功

我们实际上还没有写这

部作品

但我们希望让您

使用文本到图像模型

实

际上自己造一个

虽然我

还没有概念证明

任何人

都可以在笔记本电脑或

小型GPU上做到这一点

所以

我们要想办法

这是否真

的是

呃

即使适度合理

呃

还有这个呃

额外家庭作

业

作业六二三

这只适用

于注册为10-6的学生

二十三

呃

你们要分析一下最近

的研究

呃那你就会知道

阅读一些文献，并创建一

个小视频演示文稿

所以

这口井的目标

这个项目

有许多不同的目标

呃

探

索一些你感兴趣的生成

建模技术

加深了你对现

实世界应用程序的理解

你们三人一组

嗯

没有关

于这个主题的教科书

但

你会被指示

呃

到当前的

研究论文

所以，嗯，所以这

基本上是

我觉得

这门课

的一个学习目标是让你

达到这样的地步，你实际

上可以

有点舒服

舒适地

阅读在这个领域迅速发

表的研究论文

万一你在

找东西

呃

就像讲座下面

有一个链接

一个现在写

着幻灯片的

呃

如果你点

击它

你会发现这些幻灯

片

嗯

有一个办公时间链

接，上面有谷歌日历

办公

时间还没到

不过，我们很

快就会弄好的

我们只是

在做一些最后一分钟的

安排

作业将出现在此课

程页面上

呃

就在作业清

单下面

我们一般也会在

广场上宣布

嗯好吧

然后

呢

呃

关于家庭作业零的

一个独特之处

因为呃

这

是一个不寻常的任务

嗯

那个

这项任务的独特政

策是

基本上会批准任何

和所有的延期请求

你应

该按时完成作业

如果你

坐在这里

你应该

你应该

按时完成作业

您没有理

由需要延期请求

这真的

是让

我们有一个公平的

政策

因为八天后有人

会

出现在我的办公室

我可

以在截止日期后参加你

的人工智能课程吗

我们

也许能容纳那个人

嗯好

吧

我会

我会

呃

有很多

我

想我们已经谈了很多

对

课程不同的学习目标进

行划分的不同方法

呃

问

题

什么路线与预定的路

线

但当然就像贝塔大脑

这周很有趣

所以可能有

更少的重叠

所以这两门

课程会有重叠

呃

但就像

在完全不同的话题

对呀

所以嗯

我觉得安德烈最

近把扩散模型

呃

但这些

可能不会出现在高级深

度学习中

呃

但在深度学

习中，会有一些类似于

我

们只需要能够谈论的核

心模型

就像变形金刚和

CNN

呃

那个课程

这门课必然

会讲到

然后呃

高级深度

学习也

呃

谈论一些呃

我

们将要讨论的生成模型

嗯

但可能呃

它会有

它会

减少对一些类似的关注

例如，它可能会谈论甘斯

嗯

但它可能会更少地描

述这两种生成模型

从语

言和视觉

和其他模式，并

试图将它们联系在一起

所以

这门课存在的大部

分原因

你能找到这门课

的所有主题

在机器学习

部门现在提供的其他课

程中

呃

但你可能需要上

大约五门不同的课程才

能掌握所有的主题

嗯

所

以

我们把这门课放在一

起的部分原因是因为我

们意识到

当我们的博士

生

例如

有时间在机器学

习部门上五门不同的课

程

不是每个人都这么做

的

还有呃

所以这是为了

把分散在许多地方的许

多话题聚集在一起

现在

有很多不同的课程

这是

个好问题

谢谢你

你最喜

欢的聊天是什么

所以我

想我还没有最喜欢的聊

天

英镑提示

但我确实一

直在开发一些最喜欢的

鹦鹉图像

独角鲸和墨西

哥蝾螈

哇塞

结果是非常

惊人的

这堂课花了多少

时间

与三零一或六零相

比

所以这堂课的目的是

呃

相对耗时

呃

三一六一

点赞

这是一个12个单元的

课程

应该是每周12个小时

呃我觉得呃

有时新的课

程

呃

存在一边的空气

比

预期的要花更多的时间

嗯，我还没有教一门新课

程，我们就像

所有的家庭

作业都太简单了

我们真

的应该把它提升到一个

整体的水平

嗯，所以我想

这是说

嗯

我想我们也变

得更好了

就像

我其实是

想拉你们

你花了多少时

间在事情上，把事情拨回

来

呃

如果我们过火了

嗯

，所以嗯，所以实际上有一

些适应性内置

呃

已经进

入课程能够做到这一点

是啊，是啊

是啊，是啊

我有

一个关于第十个二十三

个项目的问题

所以是的

不太清楚

这是额外的家

庭作业

这不是一个项目

所以这个项目是为这门

课的每个人准备的

四百

二十三

六百二十三

每个

人都在做一个项目

作业

六二三是

是啊，是啊

做了

什么是好的

所以基本上

你希望我们中的任何一

个做这个项目的人

呃

使

用现成的生成模型

或者

从零开始建造一些东西

所以嗯

我认为使用现成

的生成模型

或者从零开

始构建一些东西都是进

行项目的好方法

但不管

你往哪个方向走

你得建

造一些东西

这不仅仅是

一门你喜欢的课程

使用

生成模型

嗯

做一些像我

们现在这样的事情

我们

从根本上专注于打开引

擎盖

实际上就像

你如何

建造这些东西等等

但是

有很多事情

你做不到的

事

除非你用的是一个预

先训练好的生成模型

因

为训练可能要花60万美元

你没有那么多现金

我猜

我不知道

所以说

呃

是啊

，是啊

是啊，是啊

所以关于

元素，看起来就像建筑或

路易

有点像你

就像一个

更高的层次

像骗局或

我

们如何做最好的问题

所

以我们更关注

有点如何

或者如何使用LLMS

我觉得这

里的斜度肯定是

你如何

建造它们

不是很清楚你

怎么用它们

虽然我们会

在某种程度上谈论

嗯

就

像在上下文学习中

例如

并试图理解为什么这实

际上是有效的

所以这就

像是对所有成功的赞美

但我不知道

我在服役中

没有看到这一点

像数据

工程

像你的指导

一个很

好的问题

所以嗯

数据工

程就像超级超级重要说

构建大型语言模型

然而

，它并没有明确地出现在

教学大纲中

所以嗯

我们

不一定要去

所以说

我不

想这么说

我们绝对不会

谈论这件事

但我们不会

把它作为这门课的重点

有一个十一

LTI秋季提供的

六十七个大型语言模型

课程

这实际上花了很多

时间思考数据工程

在本

课程中，我们强调了一些

数据工程的讨论

但是还

有一个很棒的课程啊

这

个秋季提供的LMS课程就像

一个很好的补充

我不确

定这门课程在多大程度

上是开放的和可用的

但

你可以回头看看，也可以

在接下来的秋天接受

当

我认为它会再次提供

我

向上压

他们的研究里没

有人

哦耶

所以重用

就像

这样，为一个项目建立自

己的研究是完全可以的

它只需要被清楚地记录

下来所以你仍然需要去

做一些其他的事情

除了

你现有的研究

就像你现

有的研究就像你对待它

一样

就像别人以前的作

品一样

好啦

所以嗯所以

剩下的时间

呃

我真正想

做的是

回去好好想想

一

些基本的话题，希望能

呃

对于你们中那些通常

我

知道

有一个特定的队列

介绍ML学生

谁真的没有做

过深度学习

嗯所以嗯

对

你们来说

我特别想说的

是

圆周率火炬是如何工

作的

就像

什么是

你是如

何建造像圆周率火炬这

样的东西的它对你有什

么作用

它是怎么做到的

嗯然后嗯

我也想说一下

只是最基本的井

其实不

是这样的

其实呢

没那么

基本

呃好吧

我将谈谈两

个最基本的

所以呃

我们

会的

呃

在剩下的时间里

今天

反向传播权

所以呃

你们中有多少人以前见

过反向传播

好啦

如果你

不举手

呃

下课后我们应

该谈谈

嗯

所以呃

反向传

播基本上是通过计算图

的两次传递

在这个计算

图中有一个向前的传递

嗯

这个想法是，你基本上

是在写下一些算法

去计

算一些函数

所以y等于x的

某个f

你可以把这个函数

表示为一个有向无环图

其中每个节点都在计算

一些中间量

你按拓扑顺

序访问这些节点

呃

比如

说从下到上

然后你得到

顶部的输出

然后你做这

个从顶部开始的向后计

算

在你计算的每个节点

上，以相反的拓扑顺序工

作

呃

用你已经计算出来

的偏导数来做一个偏导

数

这就是反向传播的基

本思想

然后呃

通常

呃

当

我们实际实现一个通用

的反向传播算法时

就像

反向模式下的自动差分

嗯

不仅仅是一个神经网

络的特定反向传播实例

化

我们做的其实是

这个

替代版本

所以在这里你

会看到

嗯，我是说，我们计

算

所以如果我们有一些

我们关心的节点VJ

还有一

些中间节点

呃你一你二

你们三个

我们所做的是

通过链式法则直接计算

就

涉及这些中间项的链

式法则

我声称这实际上

不是实现这些东西的好

方法

您真正应该做的是

，每次访问一个节点

您实

际上应该增加d

dvj

所以每次

你访问一个u节点

您应该

将d y dvj递增为d y

d u

我是对的

这两

个是完全等价的算法

你

最终得到的答案是一样

的

但是第二种方法使您

能够编写更干净的代码

这允许您对您的反向传

播算法进行通用

所以基

本上Back prop是关于重用计算

向

前传球和偏导数

你在后

传计算

并允许您计算梯

度

这样你就可以训练一

个机器学习系统

好啦

所

以嗯

所以在这里

呃

这只

是一个神经网络的小图

片

这是他们来的最基本

的

我们有一些输入

X

呃

我

们在这里用线性层打它

乘以某个矩阵得到

然后

我们把它通过元素的sigmoid得

到z

然后我们再做一个线

性层

软麦克斯

然后说一

些交叉熵损失

好啦

所以

这里我们有一张计算图

的实际图片

如果你要实

施

嗯，在介绍ML课程中

反向

传播算法

您的代码可能

如下所示

但就像麻木而

不是数学

基本上就像

你

知道的

你有点喜欢

你只

是在进行正向计算

然后

你要通过这个向后的计

算

这段代码没有真正的

结构

你知道的

也许你会

在上面加评论

但我没有

很难解析

你真的看不出

你会如何重用这里的任

何东西

所以基于模块的

自动区分说你知道

这就

像一个时间代码，你扔掉

我们想要一些可重复使

用的东西

所以

嗯

已经有

了

有这整个

不同的集合

呃

神经网络

支持自动区

分或自动差异的库

然后

嗯

从历史上看，有两种不

同的类型

有静态神经网

络包

呃

要求您提前指定

实际的完整计算图

张量

流就是这样开始的

提亚

娜是另一个典型的例子

然后是这些动态神经网

络库，可以让你

在运行时

指定

其中包括像火炬这

样的东西

Pytorch

迪内特

最终Tensorflow出

现了，意识到每个人都应

该这样做

把它添加到他

们的图书馆里

所以当我

说到基于D的模块时

我在

这里真正说的是什么

这

是支持任意计算图的动

态方法吗

然后嗯

关键思

想是组件化

神经网络的

分层计算

每一层只是整

合了多个实值节点

在计

算图中

像向量值节点或

模块

模块有两个动作

正

向计算

它从某个输入,通

过某个可微函数f得到一

个输出

这就是事情的这

一面

这是通过f得到b

然后

向后计算得到一个梯度

不只是任何梯度

但是如

果你的损失函数或目标

函数是j

那么呃

我们有呃

j相对于

存储在g子中的变

量b

模块的向后计算应该

接受GB

B和A

或者目标函数相

对于变量的梯度

A

所以呃

基于模块的广告的想法

是你可以有一堆小模块

为你做这些的人

这是一

个乙状结肠模块

所以它

的正向函数会把A和应用

段

乙状结肠元

向后的会

做这种元素的计算，1-b

B和gb得

到g

然后是一个SoftMax模块

所以

你知道会有类似的结构

但是内部计算是不同的

就在这里，我们正在计算

前进中的SoftMax

向后涉及到这

种对角化或从向量产生

对角矩阵

呃

然后做一些

简单的矩阵向量乘法得

到实际的输出

嗯然后你

知道

线性模块和交叉熵

模块的类似结构

所以一

旦你实现了这些东西

您

可以回到您的过程实现

你可以写你的正向计算

即线性向前和乙状结肠

向前

然后是线性向前，然

后是软最大向前，交叉熵

向前

然后你可以做同样

的事情

向后交叉熵

向后

Softmax

线性向后

乙状结肠向后

线性向后大

现在您重用

了一些代码

烦人的是

你

还是得写这个代码

会很

好的

如果你不需要写代

码的话

因为反向传播是

一种通用算法

它在任何

计算图上都是一样的

为

什么我们每次都要把这

个写出来

好啦

所以实现

模块的那种面向对象的

方式将与我们以前所拥

有的完全一样

除了现在

我刚刚把实际的函数更

改为方法到类方法

对呀

所以像乙状结肠类这样

的每一个类都有

它是对

模块类的扩展

都有一个

前锋和一个

它有一个向

前和向后的方法

然后SoftMax有

一个向前和向后的方法

对，同样的事情一直下去

在某个时候

必须有人实

际实现每个模块的前向

和后向

通常当你实现它

的时候

您实际上并没有

用Python实现

您实际上将其实

现为CUDA内核

呃

所以它在GPU上

运行得很快

但是一旦你

实现了它

一旦每个人都

能重用那个模块

这个想

法是这样的

就像神经网

络模块

然后呃

你构建不

同的层

所以你有一个线

性层和乙状结肠

线性软

最大值和交叉熵

然后你

定义你的正向计算，说

好

啦

我们将把我们的投入

是X

我们的张量y

我们的参

数α和β

我们将把我们的线

性层调用

向前应用于x和

alpha

然后我们的乙状结肠层

被应用到一个

在一层中

的输出

然后这被应用于

z和beta，以此类推

最后我们返

回目标函数j的张量

所以

说

嗯

这其中的美妙之处

在于

嗯

在向后

我们实际

上并没有详细说明如何

通过你的神经网络反向

传播

相反，我们只是倒着

调用磁带

这是一个特殊

的功能

运行我们刚才反

向进行的正向计算

基本

上以正确的顺序调用每

个模块的向后方法

那么

这是如何工作的呢

所以

关键是

你有一些全球磁

带

在这里，我想让你想象

一个图灵机

对呀

就像那

种带子

就像为你储存信

息一样

但是这盘带子

它

也是作为堆栈实现的

所

以你可以把东西推到上

面然后把它们弹下来

然

后呃

这些模块中的每一

个

所以它初始化

所以你

通过存储一个输出张量

来初始化任何模块

嗯和

输出梯度

所以输出张量

就像实际的

你知道的

从

模块中出来的

输出梯度

类似于目标函数相对于

b的梯度

好啦

所以这种方

法向前应用

你会注意到

我们刚刚定义的这些模

块

然而，在这里，我实际上

称之为应用向前方法

所

以这是一个非常谨慎的

选择

因为

申请

正向方法

所做的不仅仅是该模块

的正向计算

也就是说，它

首先接受的不是张量列

表

但是其他模块的列表

它做的第一件事就是抓

住所有的张量

就像数学

有点

像Numpy数组

喜欢来自模

块的对象，并将它们馈送

到该类的正向方法中

这

给了我们一些张量，我们

存储作为我们的向上

最

关键的是在这里的12行

呃

我们像自己一样把这个

模块推到磁带上

所以这

记录了我们刚刚做的向

前计算

所以这是有趣的

部分

呃

当我们向后调用

磁带时

嗯

我们实际上可

以反向运行那盘磁带

并

以适当的顺序调用所有

向后的方法

这是倒着放

的带子

所以当被称为磁

带的堆栈

呃里面还是有

元素的

我们弹出添加到

其中的最新内容，并将其

称为模块

向后应用方法

毫不奇怪

向后应用只是

调用向后方法来获得适

当的渐变

然后，这是美丽

的部分

它在做

我们在版

本B中谈到的增量

它说每

个模块都是这个模块的

输入

我们把刚才计算的

梯度

并增加这些模块的

输出梯度

所以嗯

这种基

于a的模块实现方式非常

接近于pi

torch正在做的事情

呃

所以呃

所以这里我用了

同样的神经网络

我在这

里为我们定义了

呃

布局

不同层的init方法

然后向前

的方法穿过这些单独的

层

你会注意到交叉熵的

损失

我们故意避开这个

神经网络

因为如果你这

样定义神经网络

然后可

以在训练时间或测试时

间使用

而损失真的只是

训练时间的事情

所以我

们把它放在模块外面

与

之前的幻灯片如此不同

但几乎无关紧要的是

然

后当我们想采取一步随

机梯度下降

呃

我们只要

定义损失函数

优化器

这

就像随机梯度下降

我们

计算

啊

这里少了一条线

对不起

我应该说模型等

于神经

我真的很需要这

堂课

神经网络

然后我们

说模型接受我们的输入

然后我们的损失函数从

模型中获得预测

和地面

真相Y得到我们的损失

然

后我们在优化器中归零

梯度

然后呃

我们称丢失

的圆点为向后

这就是倒

着放带子的原因

呃

最后

，优化器的点步骤是采取

梯度

与梯度方向相反的

一步

好啦

所以圆周率火

炬只是一个自动微分的

例子，你可能已经注意到

了

虽然这里我们不叫线

性向前一点

在这里我们

称它为自点

x的线性之一

但线性的实际上是一个

类

它是一个真实的物体

那么为什么我们不调用

类的方法呢

这就是处理

类或对象的方法

您在它

们上调用方法

事实证明

，高炬在句法上有糖

嗯

有

多少人知道下划线

调用

下划线下划线方法

哦很

好

你们中的一些人

嗯好

吧

所以呃

双下划线特殊

方法

所有双下划线都允

许您定义处理对象时会

发生什么

好像它是一个

函数

好啦

所以

换句话说

运行x的线性等价于运行

线性点下划线

在pi torch中调用

下划线x

这几乎和沿着x向

前线性运行是一样的

但

是有一些特殊的簿记记

录记录磁带

例如

你们会

注意到在我的自动微分

理论中

我把所有这些张

量看作是平等的

投入

产

出

参数

它们只是计算图

中的张量

嗯在Pytorch

但你会注

意到我们定义了线性层

但我们从来没有把参数

传递到线性层

那么他们

在哪里

Pytorch

为了使代码更干

净，将参数存储在实际的

模块类中

还有呃

这很方

便，因为这样

嗯

它还可以

将这些变量标记为参数

因为当您有一个优化器

类时

它想知道你真正想

要的渐变是什么

就像

例

如

我们可能不希望这里

有关于x的梯度

也就是说

我们的投入

如果我们在

做图像分类

那是我们的

输入图像

嗯

所以说

通过

标记什么是参数，什么不

是参数

Pytorch就是用它来

减少

实际需要保留的内存量

呃

根据您实际计算和存

储的渐变

手持火炬

深度

学习变得很容易

嗯，所以

我其实

最近和一个学生

谈话

呃是谁花了我一点

时间

呃呃

了解学生关心

的是什么

因为他一直在

解释

你知道他选了一门

课，他所做的就是

你知道

用Pytorch建立深度学习模型

我

就像

那太好了

所以你

你

懂深度学习

他就像

哦不

我不知道Pytorch是怎么工作的

可以进去用圆周率手电

筒

也不明白反向传播是

如何工作的

因为你不需

要真正了解反向传播是

如何工作的

你仍然可以

让pytorch为你做一些事情

嗯

我

认为了解反向传播是如

何工作的是有价值的

你

们所有人都将接受

你会

如果你不知道

呃

什么背

道具

好啦

也就是说，我们

现在可以考虑一些复杂

的深度模型

我们也不用

太担心

呃说

做复杂的矩

阵演算来计算它们的梯

度是什么

所以我我想

你

知道的

今天做一些世代

然后嗯

我们实话实说吧

生成数据的最简单方法

是使用Engram语言模型

这些东

西已经存在很长时间了

呃

所以你的目标是用人

类语言生成逼真的句子

这个想法只是以最后n减

去一个单词为条件

抽取

第n个单词

对呀

所以如果

你想

你知道如果你有像

蝙蝠这样的东西

现在你

要做的是

你想生成下一

个单词

所以你拿一个多

面模具

这是给蝙蝠的

呃

对不起

球棒做了，你滚了

，死了

它出现在其他一些

单词上，比如噪音，所以这

是你的下一个样本

现在

你拿起另一个死亡

这是

制造噪音的模具你滚动

它它就会出现在

这是你

的下一个样品

然后你的

噪音滚出来，你得到了骑

士

所以这是一个从Engram语言

模型中采样的句子

嗯所

以有了Engram语言模型

我们现

在可以定义序列上的概

率分布

我们早就可以这

么做了

虽然只是用概率

的链式法则

就像序列w 1到

w 6的概率是由链式法则决

定的

W的一次概率

给定1

W 2的

概率

以此类推，一直往下

走

嗯所以

Engram语言模型说得

很好

我们实际上可以定

义一个长度序列上的概

率分布

T

嗯

做一些独立性

假设

啊

他们上来了

我不

敢相信他们真的

这是整

个课程中唯一会出现这

些的地方

嗯

所以我们在

这里做的独立假设是

假

设w 4只依赖于w 3

它有条件地

独立于前面给出的所有

其他单词

好啦

呃所以呃

现在这个序列的概率所

涉及的信息

所以这是呃

n等于两个n克模型

这是一

个n等于三克的模型

我们

现在以前面的两个词为

条件

所以嗯

你可以了解

这些概率

嗯

你可以把这

些概率想象成一个大的

概率表

这些不同的多面

模具中的每一个

嗯，你知

道

取决于你是什么

呃

你

知道的

每一个看起来都

有点不同

对呀

制造噪声

对噪声和污染有很高的

概率

是啊，是啊

制造噪声

污染

呃

呃

但制造噪音斑

马的概率非常低或为零

所以你可以通过从数据

中计数来了解这些概率

所以如果你有一个数据

集合

呃

你可以去找

呃说

如果你想学习这个特定

的概率表条件对序列的

影响

牛吃东西

然后您可

以查看数据集中的所有

内容

所有你看到牛吃东

西的地方

然后数一下后

面的单词跟着正确的次

数

十一次中有四次是舒

适的玉米

十一次中有三

次吃草

嘿十一个中的两

个

现在你得到了前两个

单词下一个单词的概率

分布

好啦

然后你就可以

呃

在你学会从你的

呃

像

这样的Engram语言模型

如果你

想

你可以训练它

说莎士

比亚全集

你会看到一些

非常非常糟糕的莎士比

亚

你可以在右边看到

所

以嗯

这就是两千年初的

情况，对吧

呃

这些是模型

他们是我们最好的模特

嗯

他们是稍微好一点的

但他们真的很慢，他们没

有得到更多这样的你

所

以每一个机器翻译和自

动语音识别系统都使用

Engram语言模型

好啦

于是出现

了递归神经网络语言模

型

他们把事情做得更好

一点

这里的主干是典型

的递归神经网络

我们说

我们有一系列的输入

X和

呃

我们将有一系列的输

出

注意x 1本身是一个向量

对呀

这是一个输入向量

然后这是另一个向量

以

此类推

呃

每一个都是一

个向量

好啦

下面是实际

计算的工作原理

嗯，所以

我们要采取英寸

假设我

们想在这里计算H2

所以要

计算H2

我们先在这里取x 2

我

们把它乘以大矩阵w x

h

我们

再加上

呃

某个h乘以一个

不同的矩阵w h h

然后我们加

上一个偏置向量

好啦

到

目前为止我们得到了x2和

h1的线性组合

现在我们通

过一些非线性激活函数

书法H，这给了我们H两个

所

以现在呃

给定h

2

我们可以

计算出y 2

所以y2只是h2的线性

函数

现在我们每走一步

H 3是x

3和H 2的函数，依此类推

向

下

好啦

所以嗯

让我们回

到概率的链式法则

我喜

欢这条规则

你知道的

它

适用于任何概率分布

它

适用于输入的任何顺序

其实呢

我们会

我们会解

释为什么这会有用

当我

们考虑变形金刚语言模

型时

然后呢

呃

对于一个

递归神经网络语言模型

呃

我们实际上要用概率

链式法则

稍微扭一扭

我

们要说

嗯

我不喜欢的一

点是

其中的每一个都是

不同的概率分布

因为这

件事的条件是五个字

而

这个只有三个条件

好啦

所以为了解决这个问题

，我们可以说

如果我们先

把前面的单词转换成一

个固定长度的向量

然后

我们定义下一个单词的

概率分布概率

以固定长

度向量为条件

这使得我

们可以使用概率链式法

则

在给定所有前面的单

词的情况下定义下一个

单词的概率

我们只需要

函数fθ

从任意长的向量序

列中得到一个固定长度

的向量

所以RNN语言模型

毫

不奇怪

用rnn定义fθ

一个复杂

的函数来获得

说

像H5这样

的矢量

这是所有向量的

固定长度表示，直到时间

第五步

就像H3是一个固定

长度的表示

通过所有向

量的方式，相同的长度

第

三步

rnn语言模型说OK

我们要

接受某种特殊的开始符

号

就会得到零氢

然后我

们得到一个在给定h-1的情

况下的概率分布

我们从

一个单词中取样，现在我

们从刚才取样的单词中

取样

虽然我们反馈，我们

得到一个矢量

这是h 0的rnn部

分

然后呢

不管这是什么

现在我们有一个关于2的

概率分布

对不起

我们对

蝙蝠这个词进行了采样

现在我们继续沿着这条

路走下去

我们得到了

嗯

呃

下一个单词的概率分

布

考虑到前面所有的话

RNN给了我们向量表示

我们

可以继续取样单词

就像

我们从Engram语言模型中做的

那样

但现在我们的概率

分布来自神经网络

好啦

唯一的问题是我们没有

说

我们如何实际创建发

行版

给定h t的w t的p

Ht是一个向

量

如何得到单词上的概

率分布

拿一个新的图层

然后用柔和的声音

适用

于线性层的大软最大值

应用于HT

给了我们一个概

率分布

我们应该说这个

线性层应该有多长

以便

SoftMax具有适当的词汇量

但仅

此而已

现在我们有一个

合适大小的单词的概率

分布

它是由这个神经网

络定义的

这是基于之前

的所有单词

好啦

所以嗯

我们可以把所有这些概

率相乘

这就给了我们整

个序列的概率

我们可以

从这个模型中取样

就像

我们从Engram模型中取样一样

我们现在要做的就是定

义这些概率

这些不同侧

面的染料的重量

基于一

个轻度复杂的神经网络

好啦

所以呃

事实证明，如

果你这样做

呃

你回去

嗯

，样品要好得多

其实他们

是

他们是

它们要好得多

，你可能需要一秒钟才能

真正弄清楚

呃

哪一个是

正确的标签

我让你考虑

一下

这是我们的第一张

照片

文本的生成模型

嗯

文本的两种生成模型

一

个非常简单的Engram语言模型

做得很糟糕

一个递归神

经网络语言模型做得更

好

使我们能够得到一些

可以稍微过去的东西

被

冒充为真正的莎士比亚

文本

现在的问题变成了

嗯

呃

如果我们能训练一

个简单的递归神经语言

模型

只要莎士比亚全集

，然后得到一个看起来像

这样的东西

如果我们有

一个稍微好一点的模型

和更多的数据，我们能做

什么

所以我们会接受这

个想法

下次再继续

呃同

时

嗯我们会

呃

我们会让

你做零号作业

现实地

我

其实还没写完作业

但我

明天早上会把它拿出来

的

对你们来说

呃

他们已

经做了几个月的Pytorch

嗯

如果

你没有做重量和偏见

像

一堆这样的作业

只是让

你熟悉重量和偏见

这是

一种很好的绘图和记录

的方法

所以希望里面有

新的东西给你

我们下期

再见
