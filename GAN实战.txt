目录

版权信息

版 权

版权声明

内容提要

序

致谢

前 言

作者简介

关于封面

资源与支持

第一部分 生成对抗网络（GAN）与生成模型导论

第1章 GAN简介

1.1 什么是GAN

1.2 GAN是如何工作的

1.3 GAN实战

1.3.1 GAN的训练

1.3.2 达到平衡

1.4 为什么要学GAN

1.5 小结

第2章 自编码器生成模型入门

2.1 生成模型简介

2.2 自编码器如何用于高级场景

2.3 什么是GAN的自编码器

2.4 自编码器的构成

2.5 自编码器的使用

2.6 无监督学习

2.6.1 吐故纳新

2.6.2 使用自编码器生成

2.6.3 变分自编码器

2.7 代码就是生命

2.8 为什么要尝试使用GAN

2.9 小结

第3章 你的第一个GAN模型：生成手写数字

3.1 GAN的基础：对抗训练

3.1.1 代价函数

3.1.2 训练过程

3.2 生成器和鉴别器

3.2.1 对抗的目标

3.2.2 混淆矩阵

3.3 GAN训练算法

3.4 教程：生成手写数字

3.4.1 导入模块并指定模型输入维度

3.4.2 构造生成器

3.4.3 构造鉴别器

3.4.4 搭建整个模型

3.4.5 训练

3.4.6 输出样本图像

3.4.7 运行模型

3.4.8 检查结果

3.5 结论

3.6 小结

第4章 深度卷积生成对抗网络（DCGAN）

4.1 卷积神经网络

4.1.1 卷积滤波器

4.1.2 参数共享

4.1.3 卷积神经网络可视化

4.2 DCGAN简史

4.3 批归一化

4.3.1 理解归一化

4.3.2 计算批归一化

4.4 教程：用DCGAN生成手写数字

4.4.1 导入模块并指定模型输入维度

4.4.2 构造生成器

4.4.3 构造鉴别器

4.4.4 构建并运行DCGAN

4.4.5 模型输出

4.5 结论

4.6 小结

第二部分 GAN的前沿主题

第5章 训练与普遍挑战：为成功而GAN

5.1 评估

5.1.1 评估框架

5.1.2 IS

5.1.3 FID

5.2 训练中的挑战

5.2.1 增加网络深度

5.2.2 游戏设置

5.2.3 最小-最大GAN

5.2.4 非饱和GAN

5.2.5 何时停止训练

5.2.6 WGAN

5.3 总结游戏设置

5.4 训练技巧

5.4.1 输入的归一化

5.4.2 批归一化

5.4.3 梯度惩罚

5.4.4 对鉴别器进行更多的训练

5.4.5 避免稀疏梯度

5.4.6 平滑和带噪声的标签

5.5 小结

第6章 渐进式增长生成对抗网络（PGGAN）

6.1 潜在空间插值

6.2 它们发展如此之快

6.2.1 高分辨率层的渐进增长和平滑

6.2.2 示例实现

6.2.3 小批量标准偏差

6.2.4 均衡学习率

6.2.5 生成器中的像素级特征归一化

6.3 主要创新点总结

6.4 TensorFlow Hub库及其实践

6.5 PGGAN的实际应用

6.6 小结

第7章 半监督生成对抗网络（SGAN）

7.1 SGAN简介

7.1.1 什么是SGAN

7.1.2 结构

7.1.3 训练过程

7.1.4 训练目标

7.2 教程：SGAN的实现

7.2.1 架构图

7.2.2 实现

7.2.3 设置

7.2.4 数据集

7.2.5 生成器

7.2.6 鉴别器

7.2.7 搭建整个模型

7.2.8 训练

7.3 与全监督分类器的对比

7.4 结论

7.5 小结

第8章 条件生成对抗网络（CGAN）

8.1 动机

8.2 什么是CGAN

8.2.1 CGAN的生成器

8.2.2 CGAN的鉴别器

8.2.3 汇总表

8.2.4 架构图

8.3 教程：CGAN的实现

8.3.1 实现

8.3.2 设置

8.3.3 CGAN的生成器

8.3.4 CGAN的鉴别器

8.3.5 搭建整个模型

8.3.6 训练

8.3.7 输出样本图像

8.3.8 训练模型

8.3.9 检查输出：生成目标数据

8.4 结论

8.5 小结

第9章 循环一致性生成对抗网络（CycleGAN）

9.1 图像到图像的转换

9.2 循环一致性损失：再GAN一次

9.3 对抗损失

9.4 恒等损失

9.5 架构

9.5.1 CycleGAN架构：构建网络

9.5.2 生成器架构

9.5.3 鉴别器架构

9.6 GAN的面向对象设计

9.7 教程：CycleGAN

9.7.1 构建网络

9.7.2 构建生成器

9.7.3 构建鉴别器

9.7.4 训练CycleGAN

9.7.5 运行CycleGAN

9.8 扩展、增强和应用

9.8.1 增强CycleGAN

9.8.2 应用

9.9 小结

第三部分 何去何从

第10章 对抗样本

10.1 对抗样本的背景

10.2 谎言，该死的谎言及分布

10.3 训练的使用与滥用

10.4 信号与噪声

10.5 柳暗花明又一村

10.6 GAN的对手

10.7 结论

10.8 小结

第11章 GAN的实际应用

11.1 医学领域的GAN

11.1.1 利用GAN提高诊断准确率

11.1.2 方法

11.1.3 结果

11.2 时尚领域的GAN

11.2.1 利用GAN设计服装

11.2.2 方法

11.2.3 创造新单品以符合个人偏好

11.2.4 修改现有单品以更符合个人偏好

11.3 结论

11.4 小结

第12章 展望未来

12.1 伦理问题

12.2 GAN的创新

12.2.1 相对生成对抗网络（RGAN）

12.2.2 自注意力生成对抗网络（SAGAN）

12.2.3 BigGAN

12.3 拓展阅读

12.4 回顾与总结

12.5 小结

版权信息

书名：GAN实战

ISBN：978-7-115-55084-2

本书由人民邮电出版社发行数字版。版权所有，侵权必究。

您购买的人民邮电出版社电子书仅供您个人使用，未经授权，不

得以任何方式复制和传播本书内容。

我们愿意相信读者具有这样的良知和觉悟，与我们共同保护知识

产权。

如果购买者有侵权行为，我们可能对该用户实施包括但不限于关

闭该帐号等维权措施，并可能追究法律责任。

版 权

著 [英] 雅各布•朗格尔（Jakub Langr）

[美] 弗拉基米尔•博克（Vladimir Bok）

译 罗家佳

责任编辑 吴晋瑜

人民邮电出版社出版发行 北京市丰台区成寿寺路11号

邮编 100164 电子邮件 315@ptpress.com.cn

网址 http://www.ptpress.com.cn

读者服务热线：(010)81055410

反盗版热线：(010)81055315

版权声明

Original English language edition，entitled

GANs In

Action by Jakub Langr

and Vladimir Bok published

by Manning

Publications, USA. Copyright© 2019

by Manning Publications.

Simplified Chinese-language edition copyright©

2021 by

POST & TELECOM PRESS

Co.，LTD. All rights reserved.

本书中文简体字版由Manning Publications Co.授权人民邮电出

版社有限公司独家出版。未经出版者书面许可，不得以任何方式复制

或抄袭本书内容。

版权所有，侵权必究。

内容提要

本书主要介绍构建和训练生成对抗网络（GAN）的方法。全书共12

章，先介绍生成模型以及GAN的工作原理，并概述它们的潜在用途，然

后探索GAN的基础结构（生成器和鉴别器），引导读者搭建一个简单的

对抗系统。

本书给出了大量的示例，教读者学习针对不同的场景训练不同的

GAN，进而完成生成高分辨率图像、实现图像到图像的转换、生成对抗

样本以及目标数据等任务，让所构建的系统变得智能、有效和快速。

本书适合有一定Python经验且有基于深度学习的图像处理基础的

数据专业人员阅读。

感谢那些认为笑话比数学更像是惩罚的人。

——Jakub Langr

感谢Michael Reitano，是他帮我成为更好的作家！感谢Simone

Reitano，是他助我成为更好的自己！

——Vladimir Bok

序

我在2015年首次接触到GAN的时候，便立刻爱上了它。我确信，它

就是我在机器学习其他内容中总是错过的能自我批评的机器学习系

统。人们会不断地制订可行的计划，然后分辨出“莽撞地冲向一扇

门”并不是最好的主意。其他机器学习系统做不到这样，但是GAN可

以。GAN真的很有意义——要进入更高层次的人工智能阶段，我们就应

该利用好自动学习的表达和机器学习的反馈循环。毕竟数据非常昂

贵，计算却越来越便宜。

GAN让我为之着迷的另外一点在于它的增长曲线，尽管这一点是我

在后来才意识到的。机器学习的其他内容都没有这么“新颖”。大多

数计算机视觉技术是在1998年之前发明的，而GAN在2014年才开始发挥

作用。从2014年到我撰写本文时，GAN一直保持着不间断的指数增长。

到目前为止，我们已经取得了包括生成猫咪表情包在内的很多成

就。第一篇GAN论文的引用次数是原版TensorFlow论文的2.5倍以上，

而且GAN还常被麦肯锡咨询公司（McKinsey & Company）和主流媒体讨

论，由此可见其影响远远不止于技术层面。

这是一个充满可能性的迷人新世界，能和诸位读者一起分享，我

感到既荣幸又兴奋。本书的筹划已近两年，希望广大读者见到它会和

我们一样兴奋，非常期待你们的反馈！

——Jakub Langr

用著名科幻小说家Arthur C. Clarke的话来说，“任何足够先进

的技术，都与魔法无异。”——这在我早年探索计算机科学领域那些

“不可能的问题”时激励了我。然而经过在机器学习领域多年的学习

和工作，我发现自己对机器智能的进步已经不敏感了。2011年，当IBM

的新型智能机器人Watson在美国老牌智力问答节目Jeopardy中战胜人

类对手时，我印象深刻；然而在2016年，当谷歌的AlphaGo在围棋中又

一次战胜人类时（从计算方面上讲应是更令人惊异的成就），我几乎

没有什么特别的感觉。这项成就让人感觉有些平凡—— 甚至是预料之

中的——魔法消失了。

此时，GAN登场了。

我最初接触GAN是在微软研究院的一个研究项目中。那是2017年，

我和团队成员厌倦了一遍遍地听Despacito（当时非常流行的西班牙语

歌曲），开始试着用频谱（声音数据的视觉编码）对音乐进行生成建

模。显而易见，GAN的数据合成能力远远优于其他技术，其他算法产生

的频谱图只不过是白噪声，而GAN的输出简直和我们平时听的音乐别无

二致。看到机器在目标明确的领域取得成功是一回事（如智力问答和

围棋），而目睹一种算法独立地创造出新颖而又真实的东西可真是另

外一回事了！

希望当读者阅读本书时，不仅能感受到我对GAN的热情，更能由此

重新发现AI的魔力。Jakub和我力求使这一前沿领域变得更加普及和全

面。我们希望读者会发现本书有趣又丰富——而其中的幽默又恰到好

处。

——Vladimir Bok

致谢

本书的出版离不开曼宁出版社编辑团队的支持和指导，感谢项目

编辑Christina Taylor的辛勤工作和奉献精神。感谢John Hyaduck和

Kostas Passadis合作，他们的深刻见解帮我们把这本书做到最好。

还要感谢曼宁出版社的工作人员在多方协商、推广和促使本书的

出版落地等关键方面所做的幕后工作，他们是Brian Sawyer、

Christopher Kaufmann、Aleksandar Dragosavljevic、 Rebecca

Rinehart、Melissa Ice、Candace Gillhoolley和许多其他工作人

员。

诚挚感谢所有对本书的初稿提供了宝贵的反馈意见的读者。

感谢我在培生的前团队成员，时至今日，他们依然是我伟大的导

师和朋友——感谢Andy、Kostas、Andreas、Dario、Marek和Hubert。

2013年，他们给了我第一个数据科学方向的实习机会，从而改变了我

的生活和职业生涯。

言语不足以表达我对所有前辈的感激之情，尤其是对Pavan Kumar

博士的感激，他是一位出色的朋友、室友和同事。

还要感谢来自Filtered.com、牛津大学、ICP以及商业咨询公司

Mudano研发团队的朋友和同事。

还有太多的人在本书面世过程中给予我们帮助，囿于字数，不能

一一列出，在此一并感谢他们。感谢我的朋友和家人对我长久以来的

支持！

最后，我要吐槽一下Carminia路的狐狸——到底是什么让它们在

凌晨两点发出那种折磨人的喧嚣？what does the fox

say?[1]

——Jakub Langr

感谢James McCaffrey、Roland Fernandez、Sayan Pathak以及微

软研究院AI-611的其他职员，是他们使我有幸受到机器学习和人工智

能领域领军人物的指导。感谢AI-611的队友Tim Balbekov和Rishav

Mukherji和我一起踏上征程，感谢导师Nebojsa Jojic和Po-Sen Huang

的悉心指导。

还要感谢我的大学导师Krzysztof Gajos教授，感谢他在我本科就

读期间就允许我参加他的研究生研讨会，那是我第一次接触到计算机

科学研究领域。

特别感谢在Intent同事们对我的支持和鼓励，感谢他们对我的包

容，彼时因为忙于研究和写作，我经常在深夜才回复他们的邮件。

深深感谢Kimberly Pope多年前对我这位年轻高中生的信任，并赠

予一份改变人生的奖学金，这是我永远无法偿还的“债务”。

最后感谢家人和朋友一如既往地支持我。

——Vladimir Bok

[1] 源自伊尔维萨克兄弟为了宣传新一季脱口秀节目而制作的歌曲

The Fox。这里是一个诙谐的梗。——译者注

前 言

本书旨在引导对生成对抗网络（GAN）有兴趣的人从头开始学习。

本书从最简单的例子开始，介绍一些最具创新性的GAN的实现和技术细

节，进而对这些研究进展做出直观的解释，并完整地呈现所涉及的一

切内容（不包括最基本的数学和原理），让最前沿的研究变得触手可

及。

本书的最终目标是提供必要的知识和工具，让你不仅能全面了解

对GAN迄今为止取得的成就，还能有能力自由选择开发新的应用。生成

对抗这一模式充满潜力，等着像你这样怀有进取心、想在学术研究和

实际应用中做出点成就的人去挖掘！欢迎你加入我们的GAN之旅。

读者对象

本书适合已经有一些机器学习和神经网络相关经验的读者阅读。

下面列出了理想情况下读者应该提前知悉的内容。尽管本书尽力让内

容显得通俗易懂，但你至少应该对下面70%的知识有信心。

能够运行书中的Python程序，不需要精通Python，但至少应该有

两年的Python工作经验（最好有全职的数据科学家或软件工程师

工作背景）。

了解面向对象的编程、如何使用对象以及如何找出它们的属性和

方法；理解典型的Python对象（如Pandas DataFrame）和非典型

对象（如Keras层）。

了解机器学习理论的基础知识，如训练集和测试集的分割、过拟

合、权重和超参数，以及监督学习、无监督学习和强化学习。熟

悉准确率和均方误差等指标。

了解基本的统计学和微积分的知识，如概率、密度函数、概率分

布、微分和简单的优化。

了解基本的线性代数知识，如矩阵、高维空间，还应该了解主成

分分析的概念。

了解深度学习的基础知识，如前馈网络、权重和偏置、激活函

数、正则化、随机梯度下降和反向传播。

需要基本熟悉或自学基于Python 的机器学习库——Keras。

以上要求不是危言耸听，而是为了确保你能充分利用本书所讲的

内容。当然，不管怎样，你都可以学习，但是之前了解的内容越少，

需要你在线检索学习的内容就越多。如果你感觉上面的要求不足为

惧，那就开始学习吧！

代码说明

本书有许多包含源代码的示例，放在有编号的列表中或者内嵌在

普通文本中。这两种情况下源代码的格式都是以等宽字体样式显示

的。有时也用加粗样式突出显示代码，表示与先前所示代码不同（有

所更改），例如向现有代码行中添加新功能。

本书对大部分源代码做了格式化，以适应本书页面的排版。此

外，若正文中有对代码的解释说明，则通常会删除源代码中的注释。

为了突出重要概念，清单都给出了注释。本书示例的代码可以从异步

社区本书详情页的“配套资源”处下载。

本书使用数据科学教育的标准用具——Jupyter Notebook，所以

你应先掌握这个工具的用法。这对于中级Python学习者来说应该不

难。访问GPU或者使其所有功能正常运行有时候会遇到困难，特别是在

Windows系统，所以有些章节提供了谷歌Colaboratory（缩写为

Colab）Notebook。这是谷歌的免费平台，并预先封装了必要的数据科

学工具以及有限时长内免费的GPU。你可以直接在浏览器中运行这些代

码，也可以将其他章节的代码上传到Colab——它们是兼容的。

其他在线资源

GAN是一个拥有良好（尽管是零散的）资源的活跃领域。有学术倾

向的人可以在arXiv官方网站找到相关的最新论文。arXiv是康奈尔大

学拥有和运营的学术论文电子预印本在线存储库。

本书作者都是 Medium 写作平台的活跃撰稿人（特别是以技术为

核心的出版物 Towards Data Science和Hacker

Noon），你可以在那

里找到他们写的最新内容。

本书内容结构

本书力求在理论和实践之间取得平衡。全书分为如下三部分。

第一部分生成对抗网络（GAN）与生成模型导论 这部分介绍生

成学习和GAN的基本概念，并实现几种GAN最典型的变体。

第1章介绍了生成对抗网络（GAN），并在高层面上对其工作原理

进行阐述。通过学习本章内容，你会了解到GAN是由两个独立的神

经网络（生成器和鉴别器）组成的——它们通过动态竞争进行训

练。掌握本章的知识可为理解本书其余部分的内容奠定基础。

第2章讨论了自编码器，它在许多方面被视为GAN的前身。鉴于生

成学习的新颖性，增加这一章内容有助于将GAN置于更广阔的背景

下。本章给出了第一个代码教程，用于构建一个自编码器来生成

手写数字——我们在后面几章的GAN教程中也会探索相同的任务。

如果你已经熟悉自编码器的相关内容或想直接研究GAN，可以跳过

这一章。

第3章深入研究了GAN和对抗性学习背后的理论。本章解释了GAN和

传统神经网络的主要区别，即讨论了它们的代价函数和训练过程

上的差异。在本章结尾的代码教程中，我们将应用所学知识在

Keras中实现GAN，并训练它生成手写数字。

第4章介绍了卷积神经网络和批归一化。本章实现了一种先进的

GAN结构，它使用卷积网络作为其生成器和鉴别器，并利用批归一

化处理来稳定训练过程。

第二部分 GAN的前沿主题

在第一部分的基础上，这部分深入研究了GAN的基础理论，并实现

了一系列高级的GAN架构。

第5章讨论了许多训练GAN的理论和实践障碍以及克服这些障碍的

方法。本章根据相关的学术论文和演讲全面概述了训练GAN的首选

做法，还涵盖了用于评估GAN性能的选项以及需要担心这个问题的

原因。

第6章探索了渐进式增长生成对抗网络（PGGAN），一种生成器和

鉴别器的前沿训练方法。PGGAN通过在训练过程中添加新的层获得

了非常好的图像质量和分辨率。本章给出了真实的代码示例，并

使用TensorFlow Hub（TFHub）从理论上和实践上来解释它的作用

原理。

第7章继续探索基于核心GAN模型的创新。你会了解到通过半监督

学习仅使用一小部分带标签的训练样本就可提高分类准确率的巨

大实际意义。使用本章实现了半监督生成对抗网络（SGAN），并

解释了它是如何使用标签将鉴别器转换成一个健壮的多类分类器

的。

第8章展示了另一种在训练中使用标签的GAN架构。通过在训练生

成器和鉴别器时使用标签或者其他条件信息，条件生成对抗网络

（CGAN）解决了生成器的主要缺点之一——无法明确指定要合成

的样本。本章最后实现了一个CGAN，用于直接查看目标数据的生

成。

第 9 章讨论了最有趣的 GAN

架构之一：循环一致性生成对抗网

络（CycleGAN）。此技术可以将一幅图像转换成另一幅图像，例

如将马的图像转换成斑马的图像。本章介绍了CycleGAN的架构并

解释其主要组成和创新点，并在教程中用CycleGAN把苹果转换成

了橙子（也可以将橙子转换成苹果）。

第三部分 何去何从

这一部分讨论了如何应用GAN和对抗学习以及在何处应用它们。

第10章介绍了对抗样本。对抗样本是一种故意欺骗机器学习模型

使其犯错的技术。本章从理论和实际层面讨论了它们的重要性，

并探讨了它们和GAN的关系。

第11章介绍了GAN的实际应用，探索了如何将前几章介绍的技术用

于医学和时尚领域的实际用例：在医学领域，如何使用GAN来扩充

小数据集以提高分类准确率；在时尚领域，GAN是如何推动个性化

定制发展的。

第12章总结了迄今为止GAN的主要收获，讨论了GAN的相关道德伦

理考量，并介绍了一些新兴的GAN技术。

作者简介

Jakub Langr是将GAN用于创意和广告应用程序的一家初创公司的

联合创始人。Jakub毕业于牛津大学，自 2013 年以来一直从事数据科

学工作，近期在一家公司担任数据科学技术负责人，在Mudano担任研

发数据科学家。他为英国伯明翰大学和许多私营公司设计并教授数据

科学课程，还在牛津大学担任客座讲师。他还是英国皇家统计协会的

研究员，常在各种国际会议上应邀演讲。Jakub将他从本书中获得的所

有收益捐赠给了非营利组织——英国心脏基金会。

Vladimir Bok在微软研究院进行音乐风格转换的独立研究项目时

认识到了GAN的巨大潜力。他曾在美国创业孵化器Y Combinator支持的

初创公司做数据科学应用，并在微软领导研究跨功能计划。最近，

Vladimir一直在一家总部位于纽约的初创公司负责数据科学项目，该

公司为在线旅行和电子商务品牌（其中包括《财富》500 强企业）提

供机器学习服务。Vladimir以优异的成绩毕业于哈佛大学并获得了计

算机学士学位。他从本书中获得的所有收益捐赠给了非营利组织——

Girls Who Code。

关于封面

本书封面上的人物形象被冠以“伦敦资产阶级”的标题，意味着

这是来自伦敦的一位资产阶级女性。该插画最初发行于1787年，出现

在Jacques Grasset de Saint-Sauveur（1757—1810）的各国服饰集

锦中。每张插图都是手工绘制和着色的。Grasset de Saint-Sauveur

的藏品种类丰富，生动地再现了200年前世界上各个城镇和地区间的文

化差异。当时的区域差异性较大，人们说着不同的语言和方言，无论

是在城市还是乡村，人们都很容易通过衣着来辨别各自的居所、职业

和地位。

后来，人们的衣着发生了变化，曾经丰富的区域多样性逐渐消

失。现在，人们很难通过衣着区分不同地方的居民，更不必说不同的

国家、地区或城镇了。或许我们已经将多样的文化转变为更加多样的

个人生活，当然还有更加多样和快节奏的科技生活。

在很难将一本计算机图书和另一本计算机图书区分开的当今时

代，曼宁出版社选择如此栩栩如生的插画作为封面，恰好与计算机行

业的创造性和主动性相吻合。

资源与支持

本书由异步社区出品，社区（https://www.epubit.com/）为您提

供相关资源和后续服务。

配套资源

本书为读者提供源代码。要获得以上配套资源，请在异步社区本

书页面中单击 ，跳转到下载界面，按提示进行操作即可。注

意：为保证购书读者的权益，该操作会给出相关提示，要求输入提取

码进行验证。

提交勘误

作者和编辑尽最大努力来确保书中内容的准确性，但难免会存在

疏漏。欢迎读者将发现的问题反馈给我们，帮助我们提升图书的质

量。

如果读者发现错误，请登录异步社区，按书名搜索，进入本书页

面，单击“提交勘误”，输入勘误信息，单击“提交”按钮即可。本

书的作者和编辑会对读者提交的勘误进行审核，确认并接受后，将赠

予读者异步社区的100积分（积分可用于在异步社区兑换优惠券、样书

或奖品）。

与我们联系

我们的联系邮箱是contact@epubit.com.cn。

如果读者对本书有任何疑问或建议，请您发邮件给我们，并请在

邮件标题中注明本书书名，以便我们更高效地做出反馈。

如果读者有兴趣出版图书、录制教学视频，或者参与图书翻译、

技术审校等工作，可以发邮件给我们；有意出版图书的作者也可以到

异步社区在线投稿（直接访问

www.epubit.com/selfpublish/submission即可）。

如果您来自学校、培训机构或企业，想批量购买本书或异步社区

出版的其他图书，也可以发邮件给我们。

如果读者在网上发现有针对异步社区出品图书的各种形式的盗版

行为，包括对图书全部或部分内容的非授权传播，请将怀疑有侵权行

为的链接发邮件给我们。这一举动是对作者权益的保护，也是我们持

续为广大读者提供有价值的内容的动力之源。

关于异步社区和异步图书

“异步社区”是人民邮电出版社旗下IT专业图书社区，致力于出

版精品IT图书和相关学习产品，为作译者提供优质出版服务。异步社

区创办于2015年8月，提供大量精品IT图书和电子书，以及高品质技术

文章和视频课程。更多详情请访问异步社区官网https://www.

epubit.com。

“异步图书”是由异步社区编辑团队策划出版的精品IT专业图书

的品牌，依托于人民邮电出版社近40年的计算机图书出版积累和专业

编辑团队，相关图书在封面上印有异步图书的LOGO。异步图书的出版

领域包括软件开发、大数据、人工智能、测试、前端、网络技术等。

异步社区

微信服务号

第一部分 生成对抗网络（GAN）与生成

模型导论

第一部分将带你步入生成对抗网络（Generative Adversarial

Network，GAN）的世界，欣赏几种最为典型的GAN变体的实现。

第1章介绍GAN的基础知识，让你在此基础上对GAN的工作原理有一

个直观的认识。

第2章介绍自编码器的相关内容，让你更全面地了解生成模型。自

编码器是GAN在理论和实践上最为重要的前身，且至今仍在广泛使

用。

继第1章的讲解，第3章将深入探讨GAN和对抗性学习背后的理论，

在此基础上引导你搭建并训练第一个功能完整的GAN。

第4章介绍DCGAN的相关内容。DCGAN在初始GAN的基础上，创新性

地使用卷积神经网络，以提高生成图像的质量。

第1章 GAN简介

本章主要内容

生成对抗网络简介

这类机器学习算法的特别之处

本书所涉及的令人兴奋的GAN应用

机器能否思考——这一想法比计算机本身出现得还要早。数学

家、逻辑学家以及计算机科学家艾伦•图灵（Alan Turing），或许是

凭借在破译密码机方面做出的贡献而广为人知的，但他于1950年撰写

的《计算机器与智能》（Computing machinery and intelligence）

这篇论文也足以使他的名字永载后世。

在这篇文章中，图灵提出了一种叫作模仿游戏的测试。今天，人

们更习惯称之为图灵测试。在这个假设的测试场景中，一个不知情的

观察者与位于一扇紧闭的门后的两个“对应者”交谈：一个是人类，

另一个是计算机。图灵认为，如果观察者不能分辨出哪个是人、哪个

是机器，那么必须承认通过该测试的计算机是智能的。

试过与自动聊天机器人或智能语音助手进行对话的人都知道，要

通过这个看似简单的测试，计算机还有很长的路要走。然而，在其他

类型的任务中，计算机不仅能与人类的表现相媲美，在一些领域甚至

超越了人类——即使是一些在最近还被认为最智能的算法都无法企及

的领域，例如，计算机在人脸识别中已经能做到超越人类的精确度，

还能不需人类经验即可掌握围棋技能。[1]

机器学习算法非常擅长识别已有数据中的模式，并将这种洞察能

力用于诸如分类（为样本分配正确类别）和回归（根据一系列的输入

进行数值估算）等任务中。但是当被要求生成新数据时，计算机就显

得力不从心了。算法可以击败国际象棋大师，估计股价走势，并能分

辨信用卡交易是否可能是欺诈，但相比之下，任何与Amazon的Alexa或

Apple的Siri进行闲聊的尝试却往往注定失败。事实上，人类最基本和

最重要的能力——例如一次愉快的对话或者一件原创作品的制作——

都足以使最先进的超级计算机束手无策。

但这一切在2014年发生了改变，当时还是加拿大蒙特利尔大学博

士生的伊恩•古德费勒（Ian Goodfellow）发明了GAN，这项技术使得

计算机能够利用两个独立的神经网络生成真实的数据。GAN并不是第一

个用于生成数据的计算机程序，但其出色的效果和多功能性使得它脱

颖而出。GAN已经取得了惊人的成果，而这些成果长期以来被认为是几

乎不可能通过人工系统实现的，比如，生成栩栩如生的伪图像，将涂

鸦手绘变成照片风格的图像，或者将视频片段中奔跑的骏马变成黑白

相间的斑马——利用GAN，这些都不需要大量精心标注的训练数据即可

实现。

由于GAN的出现，机器数据生成技术取得了长足的进步，图1.1所

示的人脸合成就是一个很好的例子。就在GAN被发明出来的2014年，机

器生成的最好效果只是一张模糊的脸——这在当时已被认为是突破性

的成就了。仅仅3年后的2017年，GAN技术的进步就让计算机能够生成

以假乱真的高分辨率人脸了！在本书中，我们将深入探究让这一切成

为可能的算法——GAN。

（来源：The Malicious Use of

Artificial Intelligence: Forecasting,

Prevention, and Mitigation, by

Miles Brundage et al.,

2018.）

图1.1 人脸生成的进展

1.1 什么是GAN

GAN是一类由两个同时训练的模型组成的机器学习技术：一个是生

成器，训练其生成伪数据；另一个是鉴别器，训练其从真实数据中识

别伪数据。

生成（generative）一词预示着模型的总目标——生成新数据。

GAN通过学习生成的数据取决于所选择的训练集，例如，如果我们想用

GAN合成一幅看起来像达•芬奇作品的画作，就得用达•芬奇的作品作为

训练集。

对抗（adversarial）一词则是指构成GAN框架的两个动态博弈、

竞争的模型：生成器和鉴别器。生成器的目标是生成与训练集中的真

实数据无法区分的伪数据——在刚才的示例中，这就意味着能够创作

出和达•芬奇画作一样的绘画作品。鉴别器的目标是能辨别出哪些是来

自训练集的真实数据，哪些是来自生成器的伪数据。也就是说，鉴别

器充当着艺术品鉴定专家的角色，评估被认为是达•芬奇画作的作品的

真实性。这两个网络不断地“斗智斗勇”，试图互相欺骗：生成器生

成的伪数据越逼真，鉴别器辨别真伪的能力就要越强。

网络（network）一词表示最常用于生成器和鉴别器的一类机器

学习模型：神经网络。依据GAN实现的复杂程度，这些网络包括从最简

单的前馈神经网络（第3章）到卷积神经网络（第4章）以及更为复杂

的变体（如第9章的U-Net）。

1.2 GAN是如何工作的

支撑GAN的数学理论是较为复杂的（我们将在后面几章中集中探

讨，特别是第3章和第5章），幸运的是，我们有许多现实世界的示例

可以做类比，这样能使GAN更容易理解。前面我们讨论了一个艺术品伪

造者（生成器）试图愚弄艺术品鉴定专家（鉴别器）的示例。伪造者

制作的假画越逼真，鉴定专家就必须具有越强的辨别真伪的能力。反

过来也是成立的：鉴定专家越善于判断某幅画是否是真的，伪造者就

越要改进造假技术，以免被当场识破。

还有一个比喻经常用来形容GAN（Ian Goodfellow经常喜欢用的示

例），假币制造者（生成器）和试图逮捕他的侦探（鉴别器）——假

钞看起来越真实，就需要越好的侦探才能辨别出它们，反之亦然。

用更专业的术语来说，生成器的目标是生成能最大程度有效捕捉

训练集特征的样本，以至于生成出的样本与训练数据别无二致。生成

器可以看作一个反向的对象识别模型——对象识别算法学习图像中的

模式，以期能够识别图像的内容。生成器不是去识别这些模式，而是

要学会从头开始学习创建它们，实际上，生成器的输入通常不过是一

个随机数向量。

生成器通过从鉴别器的分类结果中接收反馈来不断学习。鉴别器

的目标是判断一个特定的样本是真的（来自训练集）还是假的（由生

成器生成）。因此，每当鉴别器“上当受骗”将假的图像错判为真实

图像时，生成器就会知道自己做得很好；相反，每当鉴别器正确地将

生成器生成的假图像辨别出来时，生成器就会收到需要继续改进的反

馈。

鉴别器也会不断地改善，像其他分类器一样，它会从预测标签与

真实标签（真或假）之间的偏差中学习。所以随着生成器能更好地生

成更逼真的数据，鉴别器也能更好地辨别真假数据，两个网络都在同

时不断地改进着。

表1.1总结了GAN的两个子网络的关键信息。

表1.1 生成器和鉴别器的关键信息

生成器 鉴别器

生成器 鉴别器

输

入

一个随机数向量

鉴别器的输入有两个来源：来自训练集的真实样

本和来自生成器的伪样本

输

出

尽可能令人信服的伪样本 预测输入样本是真实的概率

目

标

生成与训练集中数据别无

二致的伪数据

区分来自生成器的伪样本和来自训练集的真实样

本

1.3 GAN实战

现在，你已经对GAN及其组成网络有了一个大致的了解，接下来看

一下系统的实际运行情况。假定我们的目标是教GAN生成逼真的手写数

字（第3章将实现这样一个模型，并在第4章中对其进行扩展）。GAN的

核心结构如图1.2所示。

让我们看看其中的细节。

（1）训练数据集——包含真实样本的数据集，是我们希望生成

器能以近乎完美的质量去学习模仿的数据集。在这个示例中，数据集

由手写数字的图像组成。该数据集用作鉴别器网络的输入 。

图1.2 两个GAN子网及其输入、输出和交互

（2）随机噪声向量——生成器网络的初始输入 。此输入是一

个由随机数组成的向量，生成器将其用作合成伪样本的起点。

（3）生成器网络——生成器接收随机数向量 作为输入并输出

伪样本 。它的目标是生成和训练数据集中的真实样本别无二致的伪

样本。

（4）鉴别器网络——鉴别器接收来自训练集的真实样本 或生

成器生成的伪样本 作为输入。对每个样本，鉴别器会进行判定并输

出其为真实的概率。

（5）迭代训练/调优——对于每个鉴别器的预测，我们会衡量它

效果有多好——就像对常规的分类器一样——并用结果反向传播去迭

代优化鉴别器网络和生成器网络。

更新鉴别器的权重和偏置，以最大化其分类的精确度（最大化正

确预测的概率： 为真， 为假）。

更新生成器的权重和偏置，以最大化鉴别器将 误判为真的概

率。

1.3.1 GAN的训练

了解各种GAN组件的用途可能像是在看搜索引擎的快照，除非我们

看到它们是如何运作的，否则将无法完全理解。这就是本节的主要内

容。我们首先介绍GAN的训练算法，其次演示训练过程，以便你可以清

楚地看到实际的架构图。

GAN训练算法

对于每次训练迭代，执行如下操作。

（1）训练鉴别器。

a. 从训练集中随机抽取真实样本 。

b. 获取一个新的随机噪声向量 ，用生成器网络合成一

个伪样本 。

c. 用鉴别器网络对 和 进行分类。

d. 计算分类误差并反向传播总误差以更新鉴别器的可训

练参数，寻求最小化分类误差。

（2）训练生成器。

a. 获取一个新的随机噪声向量 ，用生成器网络合成一

个伪样本 。

b. 用鉴别器网络对 进行分类。

c. 计算分类误差并反向传播以更新生成器的可训练参

数，寻求最大化鉴别器误差。

结束

GAN训练过程可视化

GAN的训练算法如图1.3所示，其中的字母表示GAN训练算法中的步

骤。

图1.3 GAN训练算法有两个主要部分。训练鉴别器和训练生成器两部分，在训练

过程的对应阶段中描绘了同一个GAN网络在不同时间点的状态

子程序图示说明

（1）训练鉴别器。

a. 从训练集中随机抽取真实样本 。

b. 获取一个新的随机噪声向量 ，用生成器网络合成一

个伪样本 。

c. 用鉴别器网络对 和 进行分类。

d. 计算分类误差并反向传播总误差以更新鉴别器的权重

和偏置，寻求最小化分类误差。

（2）训练生成器。

a. 获取一个新的随机噪声向量 ，用生成器网络合成一

个伪样本 。

b. 用鉴别器网络对 进行分类。

c. 计算分类误差并反向传播以更新生成器的权重和偏

差，寻求最大化鉴别器误差。

1.3.2 达到平衡

你可能想知道GAN训练循环何时停止，更准确地说，如何知道GAN

何时能被完全训练好，以便确定适当的训练迭代次数？对于一般的神

经网络，我们通常有一个明确的目标去实现以及用来衡量效果。例

如，当训练一个分类器时，我们度量在训练集和验证集上的分类误

差，一旦发现验证集误差开始变坏，就停止进程（为了避免过度拟

合）。在GAN结构中，鉴别器网络和生成器网络有两个互为竞争对手的

目标：一个网络越好，另一个就越差。那么，我们如何决定何时停止

进程呢？

熟悉博弈论的人可能会意识到这是一个零和博弈问题，即一方的

收益等于另一方的损失。当一方提高一定程度时，另一方会恶化同样

的程度。零和博弈都有一个纳什均衡点，那就是任何一方无论怎么努

力都不能改善他们的处境或者结果。

当满足以下条件时，GAN达到纳什均衡。

（1）生成器生成的伪样本与训练集中的真实数据别无二致。

（2）鉴别器所能做的只是随机猜测一个特定的样本是真的还是假

的（也就是说，猜测一个示例为真的概率是50%）。

注意

纳什均衡是以美国经济学家、数学家John Forbes Nash的名字命

名的，他的生平事迹和职业生涯被收录在一本名为《美丽心灵》（A

Beautiful Mind）的传记中，并被翻拍成同名电影。

让我们来解释为何会出现这种情况。当每一个伪样本 与来自训

练集的真实样本无法区分时，鉴别器用任何手段都无法区分它们。因

为鉴别器接收到的样本有一半是真的，一半是假的，所以它所能做的

最有用的事情就是抛硬币，以50%的概率把每个样本分为真和假。

同样，生成器也处于这样一个点上，它不能从进一步的调优中获

得任何提高了。因为生成器生成的样本早已和真实样本无法区分了，

以至于对随机噪声向量 转换为伪样本 的过程做出哪怕一丁点儿

改变，也可能给鉴别器提供从真实样本中辨别出伪样本的机会，从而

使生成器变得更糟。

当达到纳什均衡时，GAN就被认为是收敛的。这是一个棘手的问

题，在实践中，由于在非凸博弈中实现收敛所涉及的巨大复杂性，几

乎不可能达到GAN的纳什均衡（在后续的章节中，特别是第5章中，有

更多关于收敛的内容）。实际上，GAN的收敛仍是GAN研究中最重要的

开放性问题之一。

幸而这并没有妨碍到GAN的研究，也没有妨碍生成对抗学习的许多

创新应用。即使在缺乏严格数学保证的情况下，GAN也取得了引人瞩目

的实证结果——本书涵盖了一部分最具影响力的工作，下一节先介绍

其中一些示例。

1.4 为什么要学GAN

自发明以来，GAN一直被学术界和工业界的专家们誉为“深度学习

中最重要的创新之一”。Facebook的人工智能研究主管Yann LeCun甚

至表示，GAN及其变体是“过去20年来深度学习中最酷的想法”。[2]

这种兴奋是合情合理的。机器学习领域的其他进展可能在科研人

员中人尽皆知，但对于门外汉来说，可能疑惑多于兴奋，GAN激起了从

研究人员到大众的极大兴趣——包括《纽约时报》、BBC、《科学美国

人》以及许多其他知名媒体机构，甚至可能就是GAN的某项成果驱使你

来购买这本书的呢。（对吧？）

最值得关注的可能是GAN创作超现实主义意象的能力。图1.4所示

的人脸都不是真人的，都是假的，这展示了 GAN 合成足以和真实照片

以假乱真图像的能力。这些人脸是用渐进式增长生成对抗网络生成

的，相关内容参见第6章。

（来源：Progressive Growing of GAN

for Improved Quality, Stability

and Variation，by Tero Karras

et al., 2017.）

图1.4 这些逼真但虚假的脸是由在高分辨率名人肖像照片集上训练过的渐进GAN

生成的

GAN另一个引人瞩目的成就是图像到图像的转换（image-to￾image translation）。与把句子从汉语翻译成西班牙语的方式类似，

GAN可以将图像从一种风格转换为另一种风格。如图1.5所示，GAN可以

把马的图像转换为斑马的图像，把一张照片变成莫奈的画作，而这几

乎不需要任何监督，也不需要任何标签。使这一切成为可能GAN的变体

是循环一致性生成对抗网络（CycleGAN），相关内容参见第9章。

更实用些的GAN应用同样令人着迷。在线零售的巨头亚马逊

（Amazon）尝试利用GAN提供时尚建议：通过分析无数的搭配，系统能

学会生成符合给定的任意风格的新产品。[3] 在医学研究中，GAN通

过合成样本增强数据集，以提高诊断准确率。[4] 在掌握了训练GAN及

其变体的细节之后，我们将在第11章详细地探讨这两个应用。

（来源：Unpaired Image-to-Image Translation Using

Cycle-Consistent

Adversarial Networks, by Jun-Yan

Zhu et al., 2017.)

图1.5 通过使用名为CycleGAN的GAN变体，可以将莫奈的画作变成照片，或将图

片中的斑马变成马；反之亦然

GAN也被视为实现通用人工智能[5]的重要基石。它是一种能够匹

敌人类认知能力的人工系统，能获取几乎任何领域的专业知识——从

走路所需的运动技能到语言表达技能，甚至于写诗所需的创作技能。

然而，拥有生成新数据和新图像的能力使得GAN有时也会很危险。

关于假新闻的传播及其危险性已经是老生常谈，GAN生成可信假视频的

能力也令人不安。在2018年一篇关于GAN的文章的结尾处——这篇文章

的标题很贴切“如何成为一个人工智能”——《纽约时报》记者 Cade

Metz和Keith Collins谈到了令人担忧的前景：GAN可能被用来制造和

传播易使人轻信的错误信息，比如虚假的世界各国领导人发表声明的

视频片段。《麻省理工学院科技评论》旧金山分社社长Martin Giles

也表达了他的担忧，他在2018年发表的《GAN之父：赋予机器想象力的

人》一文中提到，在技术娴熟的黑客手中，GAN可能会以前所未有的规

模被用来探索和利用系统漏洞。这些忧虑促使我们讨论GAN的应用在道

德伦理上的考量（第12章）。

GAN可以为世界带来许多好处，但是任何技术创新都是一把双刃

剑。对此，我们必须怀有一种哲学意识：“除掉”一种技术是不可能

的，所以确保像你这样的人了解这项技术的迅速崛起及其巨大的潜力

是很重要的。

本书也仅能触及应用GAN可实现功能的一些皮毛，但是，我们希望

这本书能够为你提供必要的理论知识和实践技能，使你能够继续从各

个方面探索自己最感兴趣的领域。

事不宜迟，让我们开始吧！

1.5 小结

（1）GAN 是一种利用两个神经网络之间的动态竞争来合成真实数

据样本的深度学习技术，例如能合成具有照片级真实感的虚假图像。

构成一个完整GAN的两个网络如下：

生成器，其目标是通过生成与训练数据集别无二致的数据来欺骗

鉴别器；

鉴别器，其目标是正确区分来自训练数据集的真实数据和由生成

器生成的伪数据。

（2）目前，GAN在许多不同的领域都有着广泛的应用，如时尚、

医药和网络安全等。

[1] 见Surpassing Human-Level Face

Verification Performance

on LFW with GaussianFace,

by Chaochao Lu and

Xiaoou Tang,

2014. 另见《纽约时报》文章GoogleTang, 2014Level Fs

Chinese Go

Master in Win for

A.I.,by Paul Mozur, 2017.

[2] 见GoogleMozur, 2017Level Fs

Chinese Go Master in

Win f,

by Cade Metz, Wired,

2017.

[3] Amazon Has Developed

an AI Fashion Designer,

by Will

Knight, MIT Technology Review,

2017.

[4] Synthetic Data Augmentation

Using GAN for Improved

Liver

Lesion Classification, by Maayan

Frid-Adar, et al., 2018.

[5] OpenAI Founder: Short-Term

AGI Is a Serious

Possibility,

by Tony Peng, Synced,

2018. 另见A Path to

Unsupervised

Learning Through Adversarial Networks,

by Soumith Chintala,f

Code, 2016.

第2章 自编码器生成模型入门

本章主要内容

将数据编码到一个潜在空间（降维）以及后续的维数扩展

理解在变分自编码器环境下生成建模的挑战性

用Keras和自编码器生成手写数字

了解自编码器的局限性以及GAN的应用动机

将本章献给我的祖母Aurelie Langrova，她在我们写完这一章时

去世了。我很想念她！

——Jakub

你可能想知道为什么选择在书中插入这一章。主要原因有以下3

个。

（1）生成模型对大多数人来说是一个全新的领域。大多数人一

开始接触到的往往都是机器学习中的分类任务——也许因为它们更为

直观；而生成模型试图生成看起来很逼真的新样本，所以人们对它了

解甚少。考虑到自编码器（最接近GAN的前身）丰富的资源和研究，本

书决定在探讨GAN之前增加一章，在一个更简单的环境介绍生成模型。

如果你想直奔令人兴奋的新话题，请略过这部分内容。

（2）生成模型非常具有挑战性。由于生成模型代表性不足，大

多数人不知道典型的生成模型的结构是什么样子的，也不知道面临何

种挑战。尽管自编码器在许多方面与最常用的模型相近（例如，有一

个明确的目标函数），但它们仍然展现出许多GAN也面临的挑战，如评

估生成样本质量有多困难。我们将在第5章更深入地讨论这一问题。

（3）生成模型是当前文献中的研究热点。正如我们将在本章中

讨论的，自编码器本身有它自己的用途。自编码器也是一个活跃的研

究领域，甚至在某些领域是最前沿的并且被许多GAN模型显式地采用。

其他GAN架构也隐式地受其启发或者模型包含自编码器这种思维——比

如我们将在第9章中介绍的CycleGAN。

2.1 生成模型简介

你应该对“深度学习如何获取图像中的原始像素并将其转化为类

别的预测”这种操作并不陌生。例如，可以取包含图像像素的3个矩阵

（每个颜色通道各1个）在一个转换系统中传递，最后得到一个数字。

如果想反过来做，该怎么办呢？

从要生成内容的描述指令开始，最后在转换系统的另一端得到图

像。这是最简单、最非正式的生成模型，本书中的内容会更有深度。

更正式一点，取一个特定的描述指令 ——简单地假设它是介于

0和9之间的数字——并尝试得到一个生成的样本 。理想情况下，

应该和另一个真实的样本 看起来一样真实。描述指令 是潜在空间

（latent space）中的某个激励，我们不会总是得到相同的输出 。

这个潜在空间是一个习得的表征——希望它按人类思考方式对人们有

意义（“解离”）。不同的模型将学习相同数据的不同潜在表征。

第1章中的随机噪声向量通常被称为来自潜在空间的样本。潜在

空间是数据点的一种更简单的隐式表示，它用 表示，简单来说就是维

度更低的，例如，一个由100个数字组成的向量或数组，而不是将要使

用的768维的样本。在许多方面，一个好的数据点的潜在表示会对该

空间中相似的事物进行分组。在本章中，你从图2.3中可以理解在自编

码器的环境中潜在的含义，从图2.6和图2.7中可以看到潜在空间是如

何影响生成样本的。但在此之前，我们先来看看自编码器的功能描

述。

2.2 自编码器如何用于高级场景

顾名思义，自编码器可以帮助我们对数据进行自动编码，它由两

部分构成：编码器和解码器。为了便于说明，我们考虑这样一个用

例：压缩。

想象一下，你正在给你的祖父母写信，讲述自己作为机器学习工

程师的工作经历。他们的认知水平有限，而你只有一页纸的篇幅解释

自己所做的一切，还要让他们能够理解。

再想象一下，你的祖父母还有严重的健忘，根本记不得你做了什

么。这已经感觉很难了不是吗？可能难在你必须解释所有术语。例

如，他们仍可以阅读并理解你信中的基本内容，比如你的宠物猫做了

什么，但是可能对机器学习工程师的概念比较陌生。换句话说，他们

从潜在空间 到输出 的转换已经被随机初始化了。你必须先在他们的

头脑中重新训练这些“心理结构”才能解释清楚。通过传入概念 并查

看他们是否能成功地以有意义的方式重现概念 ，来训练他们的“自

编码器”。这样就可以测量误差，这称为重建损失 。

我们每天都在不知不觉地压缩数据（或者说是信息），这样就不

会花很长时间解释已知的概念。自编码在人类的交流中随处可见，但

这取决于具体情境：向祖父母解释的东西不必向我们的同事解释，比

如机器学习模型是什么。因此一些人的潜在空间比另一些人更合适，

这取决于具体情境。我们可以直接转到他们的“自编码器”已经理解

的简单表示。

我们可以压缩，因为将某些重复出现的概念简化为已经得到认同

的抽象概念是很有用的，例如一个职位名称。自编码器可以系统地自

动发现这些高效信息模式，对其进行定义，并将它们用作快捷方式来

提高信息吞吐量。结果就是我们只需要传输 即可——它通常是低维度

的，从而节省了带宽。

从信息论的角度来看，这就是在不牺牲太多理解的情况下尝试通

过“信息瓶颈”（信件或口头交流）传递尽可能多的信息。

你几乎可以把这想象成一个只有你和你的家人理解的秘密快捷方

式，但是已经针对你们经常讨论的话题进行了优化。[1]为简单起见并

把重点放在压缩上，我们选择忽略词语是一个显式模型的事实，尽管

大多数单词的背后还具有情境相关的巨大复杂性。

定义

潜在空间是数据的隐式表示。自编码器不是在未压缩的版本中表

达单词或图像（例如机器学习工程师，或图像的JPEG编解码器），而

是根据对数据的理解来对其进行压缩和聚类。

2.3 什么是GAN的自编码器

自编码器与GAN的一个关键区分点是：我们用一个损失函数对整个

自编码器网络进行端到端的训练，而GAN的生成器和鉴别器分别有损失

函数。现在看看自编码器和GAN所处的位置，如图2.1所示，两者都是

生成模型，且都是人工智能（AI）和机器学习（ML）的子集。

图2.1 在AI图景中的GAN和自编码器。不同的研究人员对此可能有不同的结论，

我们将这一点留给学术界人士去讨论

在这种情况下，对于自编码器（或其变分形式，VAE），我们有一

个试图优化的已明确写出的函数（一个代价函数）；但在GAN中没有像

均方误差、准确率或ROC曲线下面积这样明确的指标进行优化。[2]GAN

有两个不能写在一个函数中的相互竞争的目标。

2.4 自编码器的构成

在查看自编码器的结构时，我们将以图像为例，但此结构也适用

于其他情况（例如语言，像前文给祖父母写信的示例）。与机器学习

中的许多进展一样，自编码器的高级理念很直观，并遵循以下简单步

骤，如图2.2所示。

图2.2 在前文提到的信件示例中使用自编码器遵循以下步骤。（1）压缩关于机

器学习工程师的所有知识。（2）将其组合到潜在空间（给祖母的信）中。当利用

对单词的理解作为解码器（步骤3）重建一个含义有损的版本时，你就得到了一个

与原始输入（即你的想法）在同一空间的（祖母的头脑中）想法的表示

（1）编码器网络：取一个表示 （如一个图像），然后用学过的

编码器（通常是一个单层或多层的神经网络）将维数从 减小到 。

（2）潜在空间 ：在训练时，试图建立具有一定意义的潜在空

间。潜在空间通常是有较小维度的表示，起着中间步骤的作用。在这

种数据的表示中，自编码器试图“组织其思想”。

（3）解码器网络：用解码器将原始对象重建到原始的维度，这通

常由一个神经网络完成。它是编码器的镜像，对应着从 到 的步骤。

我们可以应用解码的逆过程，从潜在空间的256个像素长的向量中得到

784个像素长的重构向量（28×28大小的图像）。

下面给出一个自编码器训练过程的示例。

（1）将图像 通过自编码器输入。

（2）得到 ，即重建的图像。

（3）评估重建损失，即 和 之间的差异：

使用图像 和 的像素之间的距离（如MAE）完成；

给了一个显式的目标函数 ，以通过梯度下降的形式进行

优化。

因此我们的任务就是找到解码器和编码器的参数，这些参数将最

小化我们用梯度下降法更新的重构损失。

现在你可能想知道为什么这很有用或者很重要，那就继续看看

吧！

2.5 自编码器的使用

尽管自编码器很简单，但是有很多理由值得我们关注它。

（1）首先，我们可以自由地压缩！这是因为图2.2中的中间步骤

（2）在潜在空间的维度上变成了一个智能缩减的图像或对象。从理论

上讲，这可能比原始输入小几个数量级，而且显然不是无损的，但是

如果愿意的话，我们可以随意利用这种副作用。

（2）仍使用潜在空间，我们可以联想到许多实际应用，如单类

分类器（one-class classifier）——一种异常检测算法，可以在缩

减的可更快搜索的潜在空间中查看项目，以检查和目标类别的相似

性。这就可以用于搜索（信息检索）或者异常检测（比较潜在空间中

的接近度）。

（3）另一个用例是黑白图像的数据去噪或彩色化。[3]如果有旧

的/有噪声的一张照片或者一段视频——例如第二次世界大战时期的影

像，那么可以减少它们的噪点并重新着色。因此自编码器与GAN的相似

之处在于，GAN在这类应用程序中也表现很出色。

（4）有些GAN的架构，例如BEGAN[4]，将自编码器用作其架构的

一部分以帮助稳定训练——后面就会发现这至关重要。

（5）训练自编码器不需要带标签的数据。我们将在下一节说明到

为什么无监督学习如此重要。这让我们更轻松，因为不需要我们去寻

找标签就可以自训练。

（6）最后但同样重要的是，可以用自编码器生成新图像。自编码

器已应用于生成从数字到人脸到卧室的任何事物，但通常图像的分辨

率越高，性能就越差，因为输出往往看起来很模糊。但是对于MINST数

据集和其他低分辨率图像来说，自编码器的效果很好。

定义

Modified National Institute of

Standards and

Technology（MNIST）数据集是一个手写数字的数据集。维基百科对这

一在计算机视觉领域的文献中非常流行的数据集有一个很好的概述。

这些都可以做到，因为我们找到了已拥有数据的新表示。这种表

示很有用，可以提取出压缩信息的核心信息；基于隐式表达，它也很

容易操作或生成新的数据。

2.6 无监督学习

在第1章中，我们讨论了无监督学习（尽管没怎么用这个术语）。

在本节中，我们将进一步探讨这一概念。

定义

无监督学习（unsupervised learning）是一种从数据本身学习

而不需要关于这些数据含义的附加标签的机器学习。例如，聚类是无

监督的，因为只是试图揭示数据的底层表示；异常检测是有监督的，

因为需要人工标记的异常情况。

从本章中，我们可以了解无监督机器学习为何与众不同，可以使

用任何数据而不必为特定目的对其进行标记。我们可以使用任何互联

网上的数据而不必为关心的每一种表示标记每个样本，例如，这张图

片中有只狗吗？有辆车吗？

在监督学习中，如果数据没有针对确切任务的标签，那么别的

（几乎）所有标签都可能是没用的。如果你有一个可以对谷歌街景中

的汽车分类的分类器，想对动物进行分类但是没有这些动物图像的标

签，这种情况下使用相同数据集训练一个动物分类器基本上是不可能

的。即使这些样本中经常出现动物，也需要标注者重新标注同样的谷

歌街景数据集中的动物。

本质上，我们需要在了解具体用例之前就考虑到数据的应用，这

很困难！但是对于许多压缩类型的任务，你总是有带标记的数据，即

数据本身。François Chollet、Google的研究科学家、深度学习框架

Keras的发明者等研究人员把这种类型的机器学习称为自监督。在本

书的大部分内容中，唯一的标签将是样本本身或者数据集中的任何其

他样本。

由于训练数据也充当了标签，从一个关键的角度来看，训练许多

这样的算法会容易得多——毕竟现在有更多的数据可以处理，不需要

等待数周时间，也不需要为足够的带标签的数据支付数百万美元。

2.6.1 吐故纳新

自编码器本身其实是相当老的想法——至少将机器学习视为一个

领域时就已存在。但是，鉴于当今每个人都在研究更深奥的东西，因

此人们成功地将深度学习应用于编码器和解码器的一部分中，一点儿

也不会让人感到惊讶。

自编码器由两个神经网络组成：编码器和解码器。在本书示例

中，两者都具有激活函数[5]，且只为每个函数使用一个中间层。这意

味着每个网络中有两个权重矩阵——对于编码器网络，一个从输入到

中间层，一个从中间层到潜在空间；对于解码器网络，又有一个从潜

在空间到不同的中间层和一个从中间层到输出的权重矩阵。如果每个

网络都只有一个权重矩阵，那么过程将类似于一种名为主成分分析

（Principal Component Analysis，PCA）的成熟分析技术。如果你掌

握了相关的线性代数知识，对这些就会对此非常熟悉。

注意

在如何求解上存在一些技术差异，例如PCA是数值确定的，而自编

码器通常是使用随机优化进行训练的。解的最终形式也有所不同，但

本书不会讲解其中一种是如何得出正交基的以及它们是如何从根本上

处于相同的向量空间——如果你碰巧知道其中的原理，就更好理解

了。

2.6.2 使用自编码器生成

我们在本章章首提到可以使用自编码器生成数据。可能有些人已

经考虑到潜在空间的使用以及是否可以将其重新用于其他用途。完全

可以！（如果你也是这样认为，请给自己点一个赞！）

让我们一起来看一下如何生成数据。回到给祖父母写信的例子并

稍加改变，使用自编码器作为生成模型可能开始有意义。例如，假设

你对一种职业的认知变成了编码器的网络输入，将写在纸上的职业的

词语看作潜在空间的输入，把祖父母头脑中关于职业的认知看作输

出。

在这种情况下，潜在空间解码（一个写在纸上的单词结合祖父母

阅读和理解的能力）成为一种在他们头脑中产生认知的生成模型。文

字作为一种激励或某种潜在向量，而输出的认知和原始输入处于同一

个高维空间——祖父母的想法和你的想法一样复杂，尽管略有不同。

现在把重点放回图像域——在一组图像上训练自编码器，调整编

码器和解码器的参数，以找到适合两个网络的参数。我们还可以了解

样本在潜在空间中的表示方式。为了生成样本，我们切断编码器部

分，仅使用潜在空间和解码器。生成过程如图2.3所示。

图2.3 从训练中知道样本放在潜在空间中的位置，所以可以轻松生成与模型所见

相似的样本。即使不能，也可以轻松在潜在空间中进行迭代或网格搜索来确定模

型可以生成的表示类型

（图片改编自Mat Leonard在Github上的简单自编码器项目）

2.6.3 变分自编码器

变分自编码器和“常规”编码器有何区别？这一切都与神奇的潜

在空间有关。在变分自编码器的情况下，我们将潜在空间表示为一个

习得的平均值和标准差的分布，而不仅仅是一组数字。通常选择多元

高斯模型，但它是什么或者为什么选择这个分布而不是另一种的原因

现在并不重要。如果想重新了解一下这个分布是什么样子的，请参阅

图2.5。

如果你熟悉统计学，那么可能已经意识到变分自编码器是一种基

于贝叶斯机器学习的技术。这意味着必须学习分布，所以就增加了更

多的限制。换句话说，常规自编码器尝试将潜在空间作为数字数组来

学习变分自编码器——以贝叶斯为例，尝试找到定义分布的正确参

数。

然后从潜在分布中采样得到一些数字，把它们输入到解码器，就

返回一个类似原始数据集中的样本，不过它是由模型创建的新样本。

2.7 代码就是生命

本书使用了流行的深度学习的高级API——Keras，强烈建议你熟

练掌握它。网上有许多提供优质免费资源的社区，如Towards Data

Science。如果想从书本中学习有关知识，推荐另一本曼宁出版社不错

的书——Deep Learning with Python，这是Keras的发明者和编写者

François Chollet写的。

Keras是用于TensorFlow、Microsoft Cognitive

Toolkit（CNTK）和Theano这几个深度学习框架的高级API。它易于使

用并允许更高级别的抽象，可让用户专注于概念和思路，而不必记录

使用的每个标准块的乘法、偏置、激活函数、池化[6]以及担心变量作

用域等。

为了展示Keras的威力和用它简化编写神经网络的过程，这里看一

下以最简单的形式写的变分自编码器的示例。[7] 在本教程中使用

Keras函数式API来编写函数导向的深度学习代码，随着学习更加深入

和复杂，我们将在后续章节介绍顺序式API。

此示例的目的是基于潜在空间生成手写数字。我们将创建一个对

象——generator或decoder，给定一个是潜在空间向量的输入，

可以用predict()的方法生成新的手写数字的样本。我们当然要使用

MINST数据集，因为我们可不想让任何人思考是否还有其他数据集可用

（图2.4）。

（来源：Artificial Intelligence Memes for

Artificial Intelligence

Teens on Facebook．）

图2.4 关于计算机视觉研究人员的想法，说得够多了……

首先导入所有依赖项。清单2.1所示的代码已经在Keras的最新版

本2.2.4和TensorFlow 1.12.0版本中进行了验证。

清单2.1 标准导入

from keras.layers import Input,

Dense, Lambda

from keras.models import Model

from keras import backend

as K

from keras import objectives

from keras.datasets import mnist

import numpy as np

下一步设置全局变量和超参数，如清单2.2所示。图像的原始尺寸

为28×28，这是标准尺寸，然后将MINST数据集中的图像展平，获得

784（28×28）的向量。

还有一个包含256个节点的中间层。但是我们要尝试其他的大小

——这就是为什么它是一个超参数！

清单2.2 设置超参数

batch_size = 100

original_dim = 28*28 ←---

MNIST图像的高×宽

latent_dim = 2

intermediate_dim = 256

nb_epoch = 5 ←---

epoch数

epsilon_std = 1.0

如清单2.3所示，开始构建编码器，使用Keras的函数式API实现。

注意

函数式API使用Python中的lambda函数返回另一个函数的构造，

该函数接收另一个输入，生成最终结果。

简单的用法是先简单地声明每个层，将前面的层作为常规参数之

后的第二组参数，例如层h将x作为输入。最后当编译模型并指出模型

的起点 和终点（[z_mean, z_log_varand z]）时，Keras将会

明白起始输入和最终列表输出是如何连接在一起的。 是潜在空间，在

本示例中是一个由均值和方差定义的正态分布。

现在定义编码器。[8]

清单2.3 构建编码器

x = Input(shape=(original_dim,), name="input")

←--- 编码器的输入

h = Dense(intermediate_dim, activation='relu',

name="encoding")(x)

←--- 中间层

z_mean = Dense(latent_dim, name="mean")(h)

←--- 定义潜在空间的均

值

z_log_var = Dense(latent_dim, name="log-variance")(h)

←--- 定

义潜在空间的log variance

z = Lambda(sampling, output_shape=(latent_dim,))([z_mean,

z_log_var]) ←--- 注意，output_shape不是一定要用TensorFlow后端

encoder = Model(x, [z_mean,

z_log_var, z], name="encoder") ←---

将编码器定义为一个Keras模型

接下来是棘手的部分，从潜在空间采样后将这些信息传递给解码

器。但是请仔细考虑一下z_mean和z_log_var是如何连接的：它们

都是由有两个节点的全连接层与层h连接的，这是正态分布的定义特

征——均值和方差。清单2.4实现了前述的采样函数。

清单2.4 创建采样辅助函数

def sampling(args):

z_mean, z_log_var = args

epsilon = K.random_normal(shape=(batch_size, latent_dim),

mean=0.)

return z_mean + K.exp(z_log_var

/ 2) * epsilon

换句话说，学习的是均值（μ）和方差（σ）。这个整体实现

中，我们有一个 通过采样函数连接z_mean和z_log_var，这样既可

以训练，又可以接着有效采样，以在最后得到一些看上去很优雅的数

字。在生成过程中，我们将根据这些学习到的参数从此分布中采样，

然后把这些值输入解码器获得输出。对分布或者概率密度函数有些陌

生的读者，可以参考图2.5所示的单峰二维高斯分布的示例。

（a）

（b）

（c）

（d）

（e）

（f）

图2.5 二维高斯分布概率密度函数。它们是不相关的二维正态分布，具有不同的

方差（a）方差0.5，（b）方差1，（c）方差2。（d）、（e）和（f）的分布与

（a）、（b）和（c）的完全相同，但以![{z}]

(http://private.codecogs.com/gif.latex?{z})轴最大值为0.7绘制。直观地

讲，这只是一个函数，它表示每一点发生的可能性。因此（a）和（d）更加集

中，而（c）和（f）使远离原点的值有可能出现，但每个给定值的可能性都不大

现在我们已经了解了定义潜在空间的内容以及这些分布的外观，

接下来编写解码器，如清单2.5所示。首先将之前的层写为变量，以便

稍后生成时重新使用。

清单2.5 编写解码器

input_decoder = Input(shape=(latent_dim,), name="decoder_input")

←--- 解码器的输入

decoder_h = Dense(intermediate_dim, activation='relu',

←---

潜在空间转化为中间维度

name="decoder_h")(input_decoder)

x_decoded = Dense(original_dim, activation='sigmoid',

name="flat_decoded")(decoder_h) ←--- 得到原始维度的平均值

decoder = Model(input_decoder, x_decoded,

name="decoder") ←---

将解码器定义为一个Keras模型

现在把编码器和解码器组合为一个VAE模型，如清单2.6所示。

清单2.6 组合模型

output_combined = decoder(encoder(x)[2]) ←---

抓取输出需要获取第

三个元素，即采样z

vae = Model(x, output_combined)

←--- 连接输入和总输出

vae.summary() ←--- 输出模型的整体结构

接下来定义损失函数，这样自编码器就可以训练了，如清单2.7所

示。

清单2.7 定义损失函数

def vae_loss(x, x_decoded_mean, z_log_var,

z_mean,

original_dim=original_dim):

xent_loss = original_dim *

objectives.binary_crossentropy(x,

x_decoded_mean)

kl_loss = - 0.5

* K.sum(

1 + z_log_var -

K.square(z_mean) - K.exp(z_log_var),

axis=-1)

return xent_loss + kl_loss

vae.compile(optimizer='rmsprop', loss=vae_loss) ←--- 编译模型

可以看到，二元交叉熵和KL散度加在一起组成整体损失。KL散度

衡量分布之间的差异：假设有图2.5中的两个分布，然后测量它们重叠

的体积。二元交叉熵是二分类常见的损失函数之一：这里简单定义为

将x的每个像素灰度值与x_decoded_mean进行比较。如果你在读完

下面的定义后仍感困惑，请参考第5章中关于测量分布之间差异的更多

详细信息。

定义

对熟悉信息论的人来说，KL散度（Kullback–Leibler

divergence, KL divergence）又名相对熵，是两个分布的交叉熵与

其自身熵的差异。对此不熟悉的读者可以这样理解，假设绘制出两个

分布，二者不重叠的任何地方的面积都将与KL散度成比例。

接下来我们定义模型，从x开始到x_decoded_mean结束。该模

型使用RMSprop编译，也可以使用Adam或者随机梯度下降

（Stochastic Gradient Descent，SGD）。与任何深度学习系统一

样，我们使用反向传播误差来定位参数空间。我们总是使用某种类型

的梯度下降，但一般来说，除了Adam、SGD和RMSprop这3种方法，人们

很少尝试其他的方法。

定义

随机梯度下降是一种优化技术。通过计算某权重对误差的贡献，

我们可以更新权重值。如果预测100%正确，则不会更新，这就是模型

训练的过程。建议参考如Deep Learning with Python的相关图书深入

学习SGD。

我们使用拆分训练集/测试集和输入归一化的标准操作对模型进行

训练，如清单2.8所示。

清单2.8 拆分训练集/测试集

(x_train, y_train), (x_test, y_test)

= mnist.load_data()

x_train = x_train.astype('float32') /

255.

x_test = x_test.astype('float32') /

255.

x_train = x_train.reshape((len(x_train),

np.prod(x_train.shape[1:])))

x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

我们对数据进行归一化操作，并将训练集和测试集中的样本从

28×28的矩阵调整大小为784位长的数组。

接下来，应用fit函数，使用shuffle获得一个真实的无序数据

集。在训练过程中，用验证集数据加以监测：

vae.fit(x_train, x_train,

shuffle=True,

nb_epoch=nb_epoch,

batch_size=batch_size,

validation_data=(x_test, x_test),verbose=1)

完整版代码提供了对潜在空间的有趣可视化，请查看配套资源中

本章的Jupyter Notebook或者Google Colaboratory Notebook。训练

完成后，我们甚至可以在二维平面上可视化潜在空间的值，如图2.6所

示。

图2.6 测试集中所有点到潜在空间及其类的二维投影。图上显示的是二维潜在空

间。我们映射出这些生成样本的类别，并根据右侧的图例相应地为它们着色。可

以看到同一类样本倾向于整齐地分组在一起，这说明是一个很好的表示

我们还可以用固定增量网格化枚举潜在空间的所有值，以查看生

成的输出。例如，在两个维度上以0.15的线性增量从0.05增加到

0.95，就得到了图2.7所示的可视化结果。这种情况下使用的是二元高

斯函数，因此有两个轴可以迭代。有关此可视化的代码，参见配套资

源中本章的Jupyter Notebook或者Google Colaboratory Notebook。

图2.7 网格化枚举潜在空间子集的值，并将每个潜在空间值输入以生成样本，最

后组成了本图。由此可以了解到随着 的变化最终生成的图像变化了多少

2.8 为什么要尝试使用GAN

看起来本书到这里似乎就可以结束了，毕竟我们已经生成了MINST

数据集的图像—— 这将是后续几个程序的测试用例。请允许我们解释

一下为什么还要介绍后续章节的动机。

为了理解面临的挑战，假设我们有一个简单的一维双峰分布，如

图2.8所示（和之前一样，只需要将其视为一个简单的数学函数。它介

于0和1之间，表示任何给定点的概率。该函数的值越高，在该精确点

上采样的点就越多）。

假设从这个真实分布中抽取了一堆样本，但是不知道底层的模

型。现在试图推断产生这些样本的分布是什么，但是由于某种原因，

我们假设真实分布是简单的高斯分布，而且只需要估计均值和方差即

可。由于没有正确指定模型（在这种情况下对这些样本的模态做出了

错误的假设），因此陷入了巨大的麻烦。例如，如果应用最大似然估

计（maximum likelihood estimation）这一传统统计技术来估计此分

布，则为单峰分布——在某种程度上，这就是VAE在努力做的——我们

得到了错误的估计。由于没有正确指定模型，[9] 它将估计出一个大

约是两个分布平均值的正态分布，称为点估计（point estimate）。

最大似然是一种不会预测出也无法计算出两个不同分布的技术。因此

为了使误差最小，它会在点估计值周围建立一个“厚尾”的正态分

布。这看似微不足道，但请牢记，我们正尝试在非常高维的空间中确

定模型，所以这并不容易！

图2.8 最大似然、点估计和真实分布。灰色（理论上）的分布是双峰而不是单

峰。但如果假设是单峰，就会导致灾难性错误，而且会导致模式崩溃（第5章）。

尤其在使用KL散度时，这经常会发生，例如VAE或早期GAN

定义

双峰（bimodal）是指有两个峰或者两个模式。这个概念在第5章

中会很有用。在此处，我们令总体分布由均值为0和5的两个正态分布

组成。

有趣的是，点估计也将是错误的，甚至生成的样本在真实值中从

未出现过。查看样本（黑色十字图标）会发现在我们估计为均值的地

方没有实际样本出现。这会产生大麻烦。将它与自编码器联系起来，

看看在图2.6中我们如何在以原点为中心的潜在空间中学习二维正态分

布？但是如果将人脸放入训练数据中会怎样呢？这将不再有一个容易

估计的中心，因为两个数据分布会有比想象中更多的模式。结果是，

即使在分布的中心附近，VAE也会产生两个数据集的奇怪混合，因为它

会试图以某种方式分离两个数据集。

到目前为止，我们仅讨论了统计上的错误假设的影响。要将这一

方面与自编码器生成的图像联系起来，应该考虑高斯潜在空间 允许我

们做什么。VAE使用高斯分布来构建它所见的数据的表示。但是因为高

斯分布有99.7%的概率质量落在中间的3个标准差内，VAE也会选择“安

全中间地带”。VAE在某种程度上是在试图直接提出基于高斯分布的基

础模型，但现实可能非常复杂，所以VAE不能像GAN那样进行扩展——

GAN可以提取“场景”。

由图2.9可以看到，当VAE选择“安全中间地带”时会发生什么。

在CelebA数据集上——其特征是经过对齐和裁剪的人脸，VAE很好地模

拟出了始终存在的面部特征（如眼睛或嘴巴），却在背景部分出错

了。

（来源：VAE-TensorFlow by Zhenliang He,

GitHub）

图2.9 这些由VAE生成的假人脸边缘非常模糊并融合到了背景中。因为CelebA数

据集的图像居中且对齐，使眼睛和嘴巴周围的特征保持一致，但是背景往往会有

所不同。VAE选择稳妥的方法，通过选择“安全”像素值使背景模糊，这样可以最

大限度地减少损失，但不能生成质量良好的图像

此外，GAN对真实数据的分布具有隐式且难以分析的理解。正如你

将在第5章中发现的那样，VAE属于直接估计极大似然模型的那一族。

希望本节能使你理解目标数据的分布以及在训练过程中如何体现

分布的影响。这些假设将在第10章中得到进一步研究：模型假设了如

何填充分布，这就产生了一个致命问题——对抗样本能利用它使机器

学习模型失败。

2.9 小结

（1）高级自编码器由编码器、潜在空间和解码器组成。使用常用

目标函数来训练自编码器，该目标函数可以测量再现数据和原始数据

之间的距离。

（2）自编码器有许多应用，也可以用作生成模型。在实践中这往

往不是其主要用途，因为其他方法（尤其是GAN）对生成任务更为擅

长。

（3）可以使用Keras编写简单的变分自编码器生成手写数字。

（4）VAE的局限性促使我们的研究向GAN迈进。

[1] 实际上，著名的欧洲金融家族Rothschilds在他们的交流中就这

样做了，这也是他们在金融领域如此成功的原因。

[2] 代价函数（也称为损失函数或目标函数）是试图优化/最小化的

函数。例如，在统计学中，这可能是均方根误差（RMSE）。均方根误

差（RMSE）是一个数学函数，通过获取样本的真实值和预测值之间差

的平方根来给出误差。在统计学中，我们通常希望通过假阳性和假阴

性的几种组合来评估分类器，ROC曲线下面积（AUC）可以帮助我们做

到这一点。因为这个概念超出了本书的范围，更多详细信息可以查阅

维基百科上的解释。

[3] 有关黑白图像着色的更多信息, 请参阅Emil Wallner在GitHub上

的项目Coloring Greyscale Images.

[4] BEGAN是边界均衡生成对抗网络（Boundary Equilibrium

Generative Adversarial Network）的缩写。这种有趣的GAN架构率先

使用自编码器作为网络其架构的一部分。

[5] 通过激活函数将前一层计算的任何输出馈入，再传递给下一层。

人们通常会选择一个线性整流函数（Rectified Linear Unit，ReLU）

又称修正线性单元，定义为max(0, x)。本书不会过多阐述激活函数。

[6] 池化（pooling block）是在层上的操作，它将多个输入合并为

更少的输入，例如有4个数字的矩阵将其最大值作为一个数字输入。这

是计算机视觉中一种降低复杂度的常见操作。

[7] 为简单起见，作者已经对此例进行了修改。

[8] 这个想法受到本书论坛用户Branko Blagojevic的启发，笔者在

此表示感谢。

[9] 参见 Pattern Recognition

and Machine Learning一书，作者

Christopher Bishop (Springer, 2011)。

第3章 你的第一个GAN模型：生成手写

数字

本章主要内容

探索GAN与对抗训练背后的理论

了解GAN与传统神经网络的区别

Keras中实现GAN并训练它，以生成手写数字

本章将探讨GAN背后的基础理论，然后介绍一些如果深入研究这个

领域可能会遇到的常用数学表示。这些描述要么是你在更侧重于理论

的出版物中看到的，要么是在关于这个主题的许多学术论文中看到

的。本章也为后续章节提供了背景知识，特别是第5章。

但从严格的实用角度来看，你不必为这些形式担心，就像不需要

知道发动机如何运转就可以驾车一样。用如Keras和TensorFlow这样的

机器学习库提取出基础数学知识，并巧妙地把它们打包成可导入的代

码行。

这将是本书中反复出现的主题，在机器学习和深度学习中也是如

此。如果你愿意直接进入实践部分，也可以粗略浏览理论部分并跳到

代码教程部分（3.4节）。

3.1 GAN的基础：对抗训练

形式上，生成器和鉴别器由可微函数表示如神经网络，它们都有

自己的代价函数。这两个网络是利用鉴别器的损失进行反向传播训

练。鉴别器努力使真实样本输入和伪样本输入带来的损失最小化，而

生成器努力使它生成的伪样本造成的鉴别器损失最大化。

图3.1总结了这一动态过程。它是第1章中GAN结构图的一个更通用

的版本——第一次解释了什么是GAN以及它们是如何工作的。与第1章

中手写数字的示例不同，在图3.1中，训练数据集理论上可以是任何东

西，具有普遍性。

图3.1 在这个GAN结构图中，生成器和鉴别器都利用鉴别器损失进行训练。鉴别

器努力使损失最小化，生成器则努力使它产生的伪样本对应的损失最大化

训练数据集决定了生成器要学习模拟的样本类型，例如，目标是

生成猫的逼真图像，我们就会给GAN提供一组猫的图像。

用更专业的术语来说，生成器的目标是生成符合训练数据集数据

分布的样本。[1] 对计算机来说，图像只是矩阵：灰度图像是二维

的，彩色图像是三维的。当在屏幕上呈现时，这些矩阵中的像素值将

显示为图像线条、边缘、轮廓等的所有视觉元素。这些值在数据集中

的每个图像上遵循复杂的分布，如果没有分布规律，图像将不过是些

随机噪声。目标识别模型学习图像中的模式以识别图像的内容，生成

器所做的可以认为是相反的过程：它学习合成这些模式，而不是识别

这些模式。

3.1.1 代价函数

遵循标准的表示形式，用 表示生成器的代价函数，用 表示

鉴别器的代价函数。两个网络的训练参数（权重和偏置）用希腊字母

表示： 表示生成器， 表示鉴别器。

GAN在两个关键方面不同于传统的神经网络。第一，代价函数 ，

传统神经网络的代价函数仅根据其自身可训练的参数定义，数学表示

为 。相比之下，GAN由两个网络组成，其代价函数依赖于两个网络

的参数。也就是说，生成器的代价函数是 ( , )，而鉴别器的

成本函数是 ( , )。[2]

第二，在训练过程中，传统的神经网络可以调整它的所有参数

θ。在GAN中，每个网络只能调整自己的权重和偏置。也就是说，在训

练过程中，生成器只能调整 ，鉴别器只能调整 。因此，每个网

络只控制了决定损失的部分参量。

为了使上述内容不那么抽象，考虑下面这个类比。想象一下我们

正在选择下班开车回家的路线，如果交通不堵塞，最快的选择是高速

公路，但在交通高峰期，优选是走一条小路。尽管小路更长更曲折，

但当高速公路上交通堵塞时，走小路可能会更快地回家。

让我们把它当作一道数学题—— 作为代价函数，并定义为回家

所需的时间。我们的目标是尽量减小 。为简单起见，假设离开办公

室的时间是固定的，既不能提前离开，也不能为了避开高峰时间而晚

走。所以唯一能改变的参数是路线θ。

如果我们所拥有的是路上唯一的车，代价将类似于一个常规的神

经网络：它将只取决于路线，且优化 )完全在我们的能力范围内。

然而，一旦将其他驾驶员引入方程式，情况就会变得更加复杂。突然

之间，我们回家的时间不仅取决于自己的决定，还取决于其他驾驶员

的行路方案，即 (θ 我们, θ

其他驾驶员)。就像生成器网络和鉴别器

网络一样，“代价函数”将取决于各种因素的相互作用，其中一些因

素在我们的掌控之下，而另一些因素则不在。

3.1.2 训练过程

上面所描述的两个差异对GAN的训练过程有着深远的影响。传统神

经网络的训练是一个优化问题，通过寻找一组参数来最小化代价函

数，移动到参数空间中的任何相邻点都会增加代价。这可能是参数空

间中的局部或全局最小值，由寻求最小化使用的代价函数所决定。最

小化代价函数的优化过程如图3.2所示。

（来源：Adversarial Machine Learning, by

Ian Goodfellow, ICLR

Keynote, 2019.）

图3.2 碗形网格表示参数空间θ1和θ2中的损失 。黑色点线表示通过优化使

参数空间中的损失最小化

因为生成器和鉴别器只能调整自己的参数而不能相互调整对方的

参数，所以GAN训练可以用一个博弈过程来更好地描述，而非优化。

[3] 该博弈中的对手是GAN所包含的两个网络。

回顾第1章，当两个网络达到纳什均衡时GAN训练结束，在纳什均

衡点上，双方都不能通过改变策略来改善自己的情况。从数学角度来

说，发生在这样的情况下——生成器的可训练参数 对应的生成器的

代价函数 ( , )最小化；同时，对应该网络参数

下的鉴别

器的代价函数 ( , )也得到最小化。[4]

图3.3说明了二者零

和博弈的建立和达到纳什均衡的过程。

图3.3 玩家1（左）试图通过调整 来最小化 。玩家2（中间）试图通过调整

来（最大化 ）最小化 。鞍形网格（右）显示了参数空间 中的组合

损失。虚线表示在鞍形中心收敛到纳什均衡

回到我们的类比，对于我们和可能在路上遇到的所有其他驾驶员

来说，当每一条回家的路线所花费的时间都完全相同时，纳什均衡将

会发生。任何更快的路线都会被交通拥堵量的成比例增长所抵消，从

而减缓了每个人的速度。而这种状态在现实生活中几乎是无法实现

的，即便使用像谷歌地图这样提供实时流量更新的工具，也不可能完

美地评估出回家的最佳路径。

这同样适用于训练GAN网络时的高维、非凸情况。即使是像MNIST

数据集中的那些小到只有28×28像素的灰度图像，也有28×28=784

维。如果它们被着色（RGB），它们的维数将增加到3倍变成2352。在

训练数据集中的所有图像上捕获这种分布非常困难，特别是当最好的

学习方法是从对手（鉴别器）那里学习时。

成功地训练GAN需要反复试验，尽管有最优方法，但它是一门科学

的同时也是一门艺术。第5章详细讨论了GAN的收敛问题。现在大可放

心，情况并没有听起来那么糟。正如在第1章中预告的那样，也正如本

书将展示的那样，无论是近似生成分布的巨大复杂性，还是对GAN收敛

条件理解的缺乏，都没有阻碍GAN的实际可用性和生成真实数据样本的

能力。

3.2 生成器和鉴别器

现在通过引入更多的表示概括所学的内容。生成器 接收随机噪

声向量 并生成一个伪样本 。数学上来说， 。鉴别器

的

输入要么是真实样本 ，要么是伪样本 ；对于每个输入，它输出一个

介于0和1之间的值，表示输入是真实样本的概率。图 3.4用刚才介绍

的术语和符号描述了GAN架构。

图3.4 生成器网络 将随机向量 转换为伪样本

： 。鉴别器网络 对

输入样本是否真实进行分类并输出。对于真实样本 ，鉴别器力求输出尽可能接近

1的值；对于伪样本 ，鉴别器力求输出尽可能接近0的值。相反，生成器希望

尽可能接近1，这表明鉴别器被欺骗，将伪样本分类为真实样本

3.2.1 对抗的目标

鉴别器的目标是尽可能精确。对于真实样本 ， )力求尽可能

接近1（正的标签）；对于伪样本 ， 力求尽可能接近0（负的标

签）。

生成器的目标正好相反，它试图通过生成与训练数据集中的真实

数据别无二致的伪样本 来欺骗鉴别器。从数学角度讲，即生成器试

图生成假样本 ，使得 尽可能接近1。

3.2.2 混淆矩阵

鉴别器的分类可以用混淆矩阵来表示，混淆矩阵是二元分类中所

有可能结果的表格表示（表3.1）。鉴别器的分类结果如下：

（1）真阳性（true positive）——真实样本正确分类为真

；

（2）假阴性（false negative）——真实样本错误分类为假

；

（3）真阴性（true negative）——伪样本正确分类为假

；

（4）假阳性（false positive）——伪样本错误分类为真

。

表3.1 鉴别器结果的混淆矩阵

输入

鉴别器输出

接近1（真） 接近0（假）

真

真阳性 假阴性

假阳性 真阴性

假

使用混淆矩阵的术语，鉴别器试图最大化真阳性和真阴性分类，

这等同于最小化假阳性和假阴性分类。相反，生成器的目标是最大化

鉴别器的假阳性分类，这样生成器才能成功地欺骗鉴别器，使其相信

伪样本是真的。生成器不关心鉴别器对真实样本的分类效果如何，只

关心对伪样本的分类。

3.3 GAN训练算法

回顾一下第1章中的GAN训练算法，并使用本章介绍的符号将其规

范化。与第1章中的算法不同，这里介绍的算法使用小批量（mini￾batch）而不是一次使用一个样本。

GAN训练算法

对于每次训练迭代，执行

（1）训练鉴别器。

a. 取随机的小批量的真实样本 。

b. 取随机的小批量的随机噪声 ，并生成一小批量伪样

本： 。

c. 计算 和 的分类损失，并反向传播总误差以

更新 来最小化分类损失。

（2）训练生成器。

a. 取随机的小批量的随机噪声 生成一小批量伪样本：

。

b. 用鉴别器网络对 进行分类。

c. 计算 的分类损失，并反向传播总误差以更新

来最大化分类损失。

结束

注意，在步骤1中训练鉴别器时，生成器的参数保持不变；同样，

在步骤2中，在训练生成器时保持鉴别器的参数不变。之所以只允许更

新被训练网络的权重和偏置，是因为要将所有更改隔离到仅受该网络

控制的参数中。这可以确保每个网络都能获得如何进行更新的相关信

号，而不受其他网络更新的干扰。你可以把这想象成两个对手在轮流

比赛。

当然，你还可以想象这样一种场景，如果每个玩家只不过是在撤

销对方的进度，那么即使是回合制游戏，也不能保证产生有用的结

果。（前面有没有说过GAN训练起来非常棘手？）第5章还将讨论最大

限度地提高成功机会的技术。

理论就是这些，现在把学到的付诸实践，实现我们的第一个GAN

吧！

3.4 教程：生成手写数字

本节将实现一个GAN，它将学习生成外观逼真的手写数字，用的是

带有TensorFlow后端的Python神经网络库Keras。图3.5显示了将实现

的GAN的高级架构。

本教程中使用的大部分代码，特别是训练循环中使用的样板，都

是Erik Linder Norén创建的开源Github存储库Keras-GAN改编而来

的。存储库还包括几个高级的GAN变体，其中一些将在本书后面介绍。

在代码和网络架构方面，我们对其进行了很大的修改和简化并重命名

了变量，使它们与本书中使用的表示方法一致。

Jupyter Notebook版的完整实现，包括对训练进度的可视化，可

在配套资源的第3章文件夹中找到。代码用Python 3.6.0、Keras

2.1.6和TensorFlow 1.8.0版本测试过。

图3.5 在训练迭代过程中，生成器学习将输入的随机噪声转换为看起来像训练数

据集（MNIST手写数字数据集）中的图像；同时，鉴别器学习区分由生成器生成的

伪图像和来自训练数据集的真实图像

3.4.1 导入模块并指定模型输入维度

首先导入运行模型所需的所有包和库，如清单3.1所示。注意：此

处还直接从keras.datasets导入了MNIST手写数字数据集。

清单3.1 Import statements

%matplotlib inline

import matplotlib.pyplot as plt

import numpy as np

from keras.datasets import mnist

from keras.layers import Dense,

Flatten, Reshape

from keras.layers.advanced_activations import LeakyReLU

from keras.models import Sequential

from keras.optimizers import Adam

然后指定模型和数据集的输入维度，如清单3.2所示。MNIST中的

每个图像都是28×28像素的单通道图像（灰度图）。变量z_dim设置

了噪声向量 的大小。

清单3.2 模型输入维度

img_rows = 28

img_cols = 28

channels = 1

img_shape = (img_rows, img_cols,

channels) ←--- 输入图片的维度

z_dim = 100 ←---

噪声向量的大小用作生成器的输入

接下来实现生成器和鉴别器网络。

3.4.2 构造生成器

简而言之，生成器是一个只有一个隐藏层的神经网络。如清单3.3

所示，生成器以 为输入，生成28×28×1的图像。在隐藏层中使用

LeakyReLU激活函数。与将任何负输入映射到0的常规ReLU函数不同，

LeakyReLU函数允许存在一个小的正梯度，这样可以防止梯度在训练

过程中消失，从而产生更好的训练效果。

在输出层使用tanh激活函数，它将输出值缩放到范围[–1, 1]。

之所以使用tanh（与sigmoid不同，sigmoid会输出更为典型的0到1范

围内的值），是因为它有助于生成更清晰的图像。

清单3.3 生成器

def build_generator(img_shape, z_dim):

model = Sequential()

model.add(Dense(128, input_dim=z_dim)) ←--- 全连接层

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Dense(28 * 28 *

1, activation='tanh')) ←--- 带

tanh激活函数的输出层

model.add(Reshape(img_shape)) ←--- 生成器的输出改变为图像尺

寸

return model

3.4.3 构造鉴别器

鉴别器接收28×28×1的图像，并输出表示输入是否被视为真而不

是假的概率。鉴别器由一个两层神经网络表示，其隐藏层有128个隐藏

单元及激活函数为LeakyReLU。

为简单起见，我们构造的鉴别器网络看起来与生成器几乎相同，

但并非必须如此。实际上，在大多数GAN的实现中，生成器和鉴别器网

络体系结构的大小和复杂性都相差很大。

注意，与生成器不同的是，清单3.4中鉴别器的输出层应用了

sigmoid激活函数。这确保了输出值将介于0和1之间，可以将其解释

为生成器将输入认定为真的概率。

清单3.4 鉴别器

def build_discriminator(img_shape):

model = Sequential()

model.add(Flatten(input_shape=img_shape)) ←--- 输入图像展

平

model.add(Dense(128)) ←--- 全连接层

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Dense(1, activation='sigmoid')) ←--- 带sigmoid

激活函数的输出层

return model

3.4.4 搭建整个模型

在清单3.5中构建并编译先前实现的生成器模型和鉴别器模型。注

意：在用于训练生成器的组合模型中，通过将

discriminator.trainable设置为False来固定鉴别器参数。还要

注意的是，组合模型（其中鉴别器设置为不可训练）仅用于训练生成

器。鉴别器将用单独编译的模型训练。（当回顾训练循环时，这一点

会变得很明显。）

使用二元交叉熵作为在训练中寻求最小化的损失函数。二元交叉

熵（binary cross-entropy）用于度量二分类预测计算的概率和实际

概率之间的差异；交叉熵损失越大，预测离真值就越远。

优化每个网络使用的是Adam优化算法。该算法名字源于adaptive

moment estimation，这是一种先进的基于梯度下降的优化算法，对

其工作原理的阐释超出了本书的范围，但可以说Adam凭借其通常优异

的性能已经成为大多数GAN的首选优化器。

清单3.5 构建并编译GAN

def build_gan(generator, discriminator):

model = Sequential()

model.add(generator) ←--- 生成器模型和鉴别器模型结合到一起

model.add(discriminator)

return model

discriminator = build_discriminator(img_shape) ←---

构建并编译

鉴别器

discriminator.compile(loss='binary_crossentropy',

optimizer=Adam(),

metrics=['accuracy'])

generator = build_generator(img_shape, z_dim)

←--- 构建生成器

discriminator.trainable = False ←---

训练生成器时保持鉴别器的参数固

定

gan = build_gan(generator, discriminator)

←--- 构建并编译鉴别器

固定的GAN模型，以训练生成器

gan.compile(loss='binary_crossentropy', optimizer=Adam())

3.4.5 训练

清单3.6实现了GAN训练算法。首先，取随机小批量的MNIST图像为

真实样本，从随机噪声向量 中生成小批量伪样本，然后在保持生成器

参数不变的情况下，利用这些伪样本训练鉴别器网络。其次，生成一

小批伪样本，使用这些图像训练生成器网络，同时保持鉴别器的参数

不变。算法在每次迭代中都重复这个过程。

我们使用独热编码（one-hot-encoded）标签：1代表真实图像，0

代表伪图像。 从标准正态分布（平均值为0、标准差为1的钟形曲线）

中取样得到。训练鉴别器使得假标签分配给伪图像，真标签分配给真

图像。对生成器进行训练时，生成器要使鉴别器能将真实的标签分配

给它生成的伪样本。

注意：训练数据集中的真实图像被重新缩放到了−1到1。如前例

所示，生成器在输出层使用tanh激活函数，因此伪样本同样将在范围

(−1，1)内。相应地，就得将鉴别器的所有输入重新缩放到同一范

围。

清单3.6 GAN训练循环

losses = []

accuracies = []

iteration_checkpoints = []

def train(iterations, batch_size, sample_interval):

(X_train, _), (_, _)

= mnist.load_data() ←--- 加载MINST数

据集

X_train = X_train /

127.5 - 1.0 ←---

灰度像素值[0, 255]缩放

到[−1,1]

X_train = np.expand_dims(X_train, axis=3)

real = np.ones((batch_size, 1))

←--- 真实图像的标签都是1

fake = np.zeros((batch_size, 1))

←--- 伪图像的标签都是0

for iteration in range(iterations):

idx = np.random.randint(0, X_train.shape[0],

batch_size)

←--- 随机噪声采样

imgs = X_train[idx]

z = np.random.normal(0, 1,

(batch_size, 100)) ←---

获取随机的一批真实图像

gen_imgs = generator.predict(z)

d_loss_real = discriminator.train_on_batch(imgs, real)

←--- 图像像素缩放到[0,1]

d_loss_fake = discriminator.train_on_batch(gen_imgs,

fake)

d_loss, accuracy = 0.5

* np.add(d_loss_real,

d_loss_fake)

z = np.random.normal(0, 1,

(batch_size, 100)) ←---

生成一批伪图像

gen_imgs = generator.predict(z)

g_loss = gan.train_on_batch(z, real)

←--- 训练鉴别器

if (iteration + 1)

% sample_interval == 0:

losses.append((d_loss, g_loss))

accuracies.append(100.0 * accuracy) ←---

生成一

批伪图像训练生成器

iteration_checkpoints.append(iteration + 1)

print("%d [D loss: %f,

acc.: %.2f%%] [G loss:

%f]"

% ←--- 输出训练过程

(iteration + 1, d_loss,

100.0 * accuracy,

g_loss))

sample_images(generator) ←--- 输出生成图像的采样

3.4.6 输出样本图像

在生成器训练代码中，你可能注意到调用了 sample_images()

函数。该函数在每次sample_interval迭代中调用，并输出由生成

器在给定迭代中合成的含有4×4幅合成图像的网格，如清单3.7所示。

运行模型后，你可以使用这些图像检查临时和最终的输出情况。

清单3.7 显示合成图像

def sample_images(generator, image_grid_rows=4,

image_grid_columns=4):

z = np.random.normal(0, 1,

(image_grid_rows *

image_grid_columns, z_dim)) ←--- 样本随机噪声

gen_imgs = generator.predict(z) ←---

从随机噪声生成图像

gen_imgs = 0.5 *

gen_imgs + 0.5 ←---

将图像像素值重缩放至[0,

1]内

fig, axs = plt.subplots(image_grid_rows,

←--- 设置图像网格

image_grid_columns,

figsize=(4, 4),

sharey=True,

sharex=True)

cnt = 0

for i in range(image_grid_rows):

for j in range(image_grid_columns):

axs[i, j].imshow(gen_imgs[cnt, :, :,

0],

cmap='gray') ←--- 输出一个图像网格

axs[i, j].axis('off')

cnt += 1

3.4.7 运行模型

这是最后一步，如清单3.8所示，设置训练超参数——迭代次数和

批量大小，然后训练模型。目前没有一种行之有效的方法来确定正确

的迭代次数或正确的批量大小，只能观察训练进度，通过反复试验来

确定。

也就是说，对这些数有一些重要的实际限制：每个小批量必须足

够小，以适合内存器处理（典型使用的批量大小是2的幂：32、64、

128、256和512）。迭代次数也有一个实际的限制：拥有的迭代次数越

多，训练过程花费的时间就越长。像GAN这样复杂的深度学习模型，即

使有了强大的计算能力，训练时长也很容易变得难以控制。

为了确定合适的迭代次数，你需要监控训练损失，并在损失达到

平稳状态（这意味着我们从进一步的训练中得到的改进增量很少，甚

至没有）的次数附近设置迭代次数。（因为这是一个生成模型，像有

监督的学习算法一样，也需要担心过拟合问题。）

清单3.8 运行模型

iterations = 20000 ←---

设置训练超参数

batch_size = 128

sample_interval = 1000

train(iterations, batch_size, sample_interval) ←---

训练GAN直

到指定迭代次数

3.4.8 检查结果

经过训练迭代后由生成器生成的样本图像，按照时间先后排列，

如图3.6所示。可以看到，生成器起初只会产生随机噪声。在训练迭代

的过程中，它越来越擅长模拟训练数据的特性，每次鉴别器判断生成

的图像为假或判断生成的图像为真时，生成器都会稍有改进。生成器

经过充分训练后可以合成的图像样本，如图3.7所示。

图3.6 起初看上去不过是随机噪声，生成器逐渐学习模拟训练数据集的特征——

本例中是手写数字的图像

为了进行比较，我们给出从MNIST数据集中随机选择的真实图像样

本，如图3.8所示。

图3.7 虽远非完美，但简单的双层生成器学会了生成逼真的数字，如数字9和1

图3.8 MNIST数据集中用于训练GAN的真实手写数字样本。尽管生成器在模拟训练

数据方面取得了惊人的进展，但它生成的数字与真实的由人类书写的数字之间的

区别仍然很明显

3.5 结论

尽管GAN产生的图像还远不完美，但是其中许多图像很容易被识别

为真实的数字。考虑到生成器和鉴别器只用了简单的双层网络结构，

这已经是一个惊人的成就了！我们将在第4章介绍如何在生成器和鉴别

器中使用更加复杂和强大的神经网络结构——卷积神经网络，以提高

生成图像的质量。

3.6 小结

（1）GAN是由两个网络组成的：生成器 和鉴别器 。它们各

自有自己的损失函数： ( , )和

( , )。

（2）在训练过程中，生成器和鉴别器只能调整自己的参数，即

和 。

（3）两个网络通过一个类似博弈的动态过程同时训练：生成器试

图最大化鉴别器的假阳性分类（将生成的图像分类为真图像），而鉴

别器试图最小化它的假阳性和假阴性分类。

[1] Generative Adversarial Networks,

Network, by Ian J.

Goodfellow et al., 2014.

[2] NIPS 2016 Tutorial:

Generative Adversarial Networks, by

Ian Goodfellow, 2016.

[3] Generative Adversarial Networks,

Network, by Ian J.

Goodfellow et al., 2014.

[4] 同上。

第4章 深度卷积生成对抗网络

（DCGAN）

本章主要内容

理解卷积神经网络背后的关键概念

批归一化的使用

实现深度卷积生成对抗网络，一种先进的GAN架构

我们在第3章实现了一个GAN，其生成器和鉴别器是具有单个隐藏

层的简单前馈神经网络。尽管很简单，但GAN的生成器充分训练后得到

的手写数字图像的真实性有些还是很具说服力的。即使是那些无法被

识别为人类手写数字的字符，也具有许多手写符号的特征，例如可辨

认的线条边缘和形状，特别是与用作生成器原始输入的随机噪声相

比，更是如此。

想象一下，如果使用更强大的网络架构可以实现什么？本章中的

生成器和鉴别器都将使用卷积神经网络（CNN，或ConvNet），而不再

是简单的双层前馈网络。这种GAN架构称为深度卷积生成对抗网络

（Deep Convolutional GAN，DCGAN）。

在深入探讨DCGAN实现的细节之前，我们先在本章介绍ConvNet的

关键概念，回顾开发DCGAN背后的历史，并介绍使DCGAN这样复杂的架

构在实践中变为可行的关键性突破之一：批归一化（batch

normalization）。

4.1 卷积神经网络

你应接触过卷积网络的相关知识，如果不熟悉，也不用担心，因

为本节将复习本章和本书其余部分需要了解的所有关键概念。

4.1.1 卷积滤波器

常规前馈神经网络的神经元排列在平面的全连接层中，而ConvNet

中的层排列在三维（宽×高×深）中。卷积是通过在输入层上滑动一

个或多个滤波器（filter）来执行的。每个滤波器都有一个相对较小

的感受野（宽×高），但它贯穿输入图像的全部深度。

每个滤波器在输入图像上滑动每一步，都会输出一个激活值：它

是输入值和过滤器值之间的点积。此过程将为每个滤波器生成一个二

维的激活图（activation map）。将每个滤波器生成的激活图堆叠在

一起可以形成一个三维输出层，其输出深度等于所用滤波器的数量。

4.1.2 参数共享

重要的是，给定滤波器参数被其所有输入值共享，这具有直观和

实用的优点。直观地讲，参数共享能够有效地学习视觉特征和形状

（如线条和边缘），无论它们在输入图像中位于何处。从实际的角度

来看，参数共享可以大大减少可训练参数的数量。这降低了过拟合的

风险，并允许该技术在不增加可训练参数的情况下扩展到更高分辨率

的图像中，而在同样情况下，传统全连接网络却要指数爆炸一样增加

可训练参数才可以做到。

4.1.3 卷积神经网络可视化

如果这样解释听起来有些令人困惑，那么通过可视化这些概念可

以使它们不那么抽象。图示能使大多数人更容易理解所有内容。图4.1

展示了单个卷积操作，图4.2说明了输入层和输出层都是卷积层情况下

的卷积操作。

图4.1描述了二维输入上单个滤波器的卷积运算。实际上，输入图

像通常是三维的而且几个滤波器堆叠在一起使用。但基本机制是不变

的：不管输入体积的深度如何，每个滤波器每一步产生一个值。使用

的滤波器数量决定了输出图像的深度，因为它们生成的激活图相互叠

加，如图4.2所示。

（来源: A Guide to

Convolution Arithmetic for Deep

Learning, by

Vincent Dumoulin and Francesco

Visin, 2016.）

图4.1 一个3×3的卷积滤波器在一个5×5的输入上滑动——从左到右，从上到

下。过滤器滑动步长为2，因此一共滑动4次，得到一个2×2的激活图。注意，每

次滑动，整个过滤器会生成一个激活值

（来源：Convolutional Neural Network, by

Nameer Hirschkind et al.,

Brilliant.org, retrieved November 1,

2018.）

图4.2 在激活图（特征图）、整体输入和输出图像中展示的单次卷积步骤的激活

值注意：卷积滤波器会贯穿输入图像的整个深度，输出图像的深度是通过激活图

堆叠来确定的。

注意

如果想深入了解卷积网络及其基本概念，建议你阅读François

Chollet所著的Deep Learning with Python（曼宁出版社，2017年）

一书中的相关章节，其中提供了深度学习中所有关键概念和技术优秀

而详实的介绍，包括ConvNet。如果你对学术有更高的要求，可以参考

Andrej Karpathy在斯坦福大学“视觉识别卷积神经网络”课程中的精

彩讲义。

4.2 DCGAN简史

DCGAN由Alec Radford、Luke Metz和Soumith Chintala于2016年

提出，自问世以来，便成了GAN领域最重要的早期创新之一。[1] 这并

不是研究人员第一次在GAN中使用ConvNet的尝试，却是第一次成功将

ConvNet直接整合到完整的GAN模型中。

ConvNet的使用加剧了困扰GAN训练的许多困难，包括训练不稳定

和梯度饱和。的确，这些挑战是如此艰巨，以至于有些研究人员求助

于其他方法，如拉普拉斯生成对抗网络（LAPGAN）——它使用拉普拉

斯金字塔式的级联卷积网络，也就是说，在每一层使用GAN框架对单独

的卷积神经网络进行训练。[2] 由于被更优越的方法所取代，LAPGAN

在很大程度上已经被扔进了历史的垃圾筒，因此了解它的内部原理并

不重要。

尽管LAPGAN笨拙复杂且计算烦琐，但在其发布时仍提供了当时质

量最高的图像，与原始GAN相比改进了4倍（LAPGAN有40%，原始GAN有

10%的生成图像被人工评估者误认为是真实的）。因此，LAPGAN展示了

将GAN与ConvNet结合的巨大潜力。

在DCGAN中，Radford和他的合作者引入了一些技术和优化方法，

使ConvNet可以扩展到完整的GAN框架，而无须修改底层的GAN架构，也

不需要将GAN简化为更复杂的模型框架的子结构（如LAPGAN）。

Radford等人引入的关键技术之一就是使用了批归一化——通过归一化

应用它的每一层的输入来帮助稳定训练过程。下面仔细看看什么是批

归一化以及它又是怎么起作用的。

4.3 批归一化

批归一化是由谷歌科学家Sergey Ioffe和Christian Szegedy于

2015年提出的。[3] 这一想法既简单又具有开创性。就像对网络输入

进行归一化一样，他们建议在每个小批量训练数据通过网络时，对每

个层的输入进行归一化。

4.3.1 理解归一化

本节的内容有助于提醒我们什么是归一化以及为什么先要对输入

特征值进行归一化。归一化（normalization）是数据的缩放，使它

具有零均值和单位方差。这是通过取每个数据点 减去平均值µ，然后

除以标准偏差得到的，如式4.1所示。

式4.1

归一化有几个优点。最重要的一点或许是使得具有巨大尺度差异

的特征之间的比较变得更容易，进而使训练过程对特征的尺度不那么

敏感。下面考虑一个（虚构的）例子，假设我们尝试基于两个特征来

预测一个家庭的每月支出：家庭的年收入和家庭成员数。一般而言，

一个家庭的收入越多，家庭成员越多，支出就越多。

但是这两个特征的尺度截然不同：年收入增加10美元可能不会影

响一个家庭的支出，但增加10个成员可能会严重影响任何一个家庭的

预算。归一化通过将每个特征值缩放到一个标准化的尺度上解决了这

个问题，这样一来每个数据点都不表示为其实际值，而是以一个相对

的“分数”表示给定数据点与平均值的标准偏差。

批归一化背后所体现的理念是，在处理具有多层的深度神经网络

时，仅规范化输入可能还远远不够。当输入值经过一层又一层网络

时，它们将被每一层中的可训练参数进行缩放。当参数通过反向传播

得到调整时，每一层输入的分布在随后的训练迭代中都容易发生变

化，从而影响学习过程的稳定性。在学术界，这个问题称为协变量偏

移（covariate shift）。批归一化通过按每个小批量的均值和方差缩

放每个小批量中的值来解决该问题。

4.3.2 计算批归一化

批归一化的计算方式与之前介绍的简单归一化方程在几个方面有

所不同。我们将在本节一一进行介绍。

令 为小批量 的平均值， 为小批量

的方差（均方误差）。归

一化值 的计算如式4.2所示。

式4.2

增加ε 项是为了保持数值稳定性，主要是为了避免被零除，一般

设置为一较小的正常数，例如0.001。

在批归一化中不直接使用这些归一化值，而是将它们乘以γ 并加

上β后，再作为输入传递到下一层，如式4.3所示。

式4.3

重要的是，γ 和β 项是可训练的参数，就像权重和偏置一样在

网络训练期间进行调整。这样做有助于将中间的输入值标准化，使其

均值在0附近（但非0）。方差也不是1。γ 和β 是可训练的，因此网

络可以学习哪些值最有效。

幸运的是，我们不必操心这些。Keras中的函数

Keras.layers.BatchNormalization可以处理所有小批量计算

并在后台进行更新。

批归一化限制了更新前一层中的参数对当前层接收的输入分布可

能的影响。这减少了跨层参数之间不必要的相互依赖，从而有助于加

快网络训练并增强鲁棒性，特别是在网络参数初始化方面。

批归一化已被证明对包括DCGAN在内的许多深度学习架构是否可行

至关重要，我们将在4.4节看到它的作用。

4.4 教程：用DCGAN生成手写数字

我们将在本节回顾第3章中的生成MNIST手写数字。这次将使用

DCGAN架构，并将生成器和鉴别器都换成卷积网络，如图4.3所示。除

此更改外，其余网络结构保持不变。在本教程的最后，我们将比较两

个GAN（传统GAN与DCGAN）生成的手写数字的质量，以展示更高级的网

络结构带来的改进。

图4.3 本章教程的总体模型架构与第3章中实现的GAN相同。唯一的区别（在此概

要图中不能体现）是生成器和鉴别器网络的内部表示形式，将在本教程的后面加

以详细介绍

与第3章一样，本教程中的许多代码均改编自Erik Linder Norén

在Github仓库Keras- GAN中建立的开源GAN模型，我们在实现细节和网

络架构方面进行了大量的修改和改进。本书的配套资源提供了完整实

现的 Jupyter Notebook实现，包括训练进度的可视化等信息。代码是

用Python 3.6.0、Keras 2.1.6和Tensor Flow

1.8.0版本测试的。为

了加快训练时间，我们建议在GPU上运行模型。

4.4.1 导入模块并指定模型输入维度

首先导入训练和运行模型所需的所有包、模块以及库。直接从

keras.datasets导入MNIST手写数字数据集，如清单4.1所示。

清单4.1 导入声明

%matplotlib inline

import matplotlib.pyplot as plt

import numpy as np

from keras.datasets import mnist

from keras.layers import (

Activation, BatchNormalization, Dense, Dropout,

Flatten,

Reshape)

from keras.layers.advanced_activations import LeakyReLU

from keras.layers.convolutional import Conv2D,

Conv2DTranspose

from keras.models import Sequential

from keras.optimizers import Adam

指定模型输入维度：图像尺寸和噪声向量 的长度，如清单4.2所

示。

清单4.2 模型输入维度

img_rows = 28

img_cols = 28

channels = 1

img_shape = (img_rows, img_cols,

channels) ←--- 输入图像的维度

z_dim = 100 ←---

用于输入生成器的噪声向量的大小

4.4.2 构造生成器

ConvNet传统上用于图像分类任务，图像以尺寸——高度×宽度

×彩色通道数作为输入，并通过一系列卷积层输出一个维数为1 ×

的类别得分向量， 是类别标签数。要使用ConvNet结构生成图像，则

是上述过程的逆过程：并非获取图像再将其处理为向量，而是获取向

量并调整其大小以使之变为图像。

这一过程的关键是转置卷积（transposed convolution）。我们

通常使用卷积减小输入的宽度和高度，同时增加其深度。转置卷积与

其相反，用于增加宽度和高度，同时减小深度，如图4.4的生成器网络

图所示。

图4.4 生成器将随机噪声向量作为输入并生成28×28×1的图像。这一过程通过

多层转置卷积实现，在卷积层之间应用批归一化来稳定训练过程（图像未按比例

绘制）

生成器从噪声向量 开始，使用一个全连接层将向量重塑为具有

小的宽×高和大的深度的三维隐藏层。使用转置卷积对输入进行逐步

重塑，以使其宽×高增大而深度减小，直到具有想要合成的图像大小

28×28×1。

在每个转置卷积层之后，应用批归一化和LeakyReLU激活函数；

在最后一层不应用批归一化，并且使用tanh激活函数代替ReLU。

综合所有步骤如下。

（1）取一个随机噪声向量 ，通过全连接层将其重塑为7×7×256

张量。

（2）使用转置卷积，将7×7×256张量转换为14×14×128张量。

（3）应用批归一化和LeakyReLU激活函数。

（4）使用转置卷积，将14×14×128张量转换为14×14×64张

量。注意：宽度和高度尺寸保持不变。可以通过将

Conv2DTranspose中的stride参数设置为1来实现。

（5）应用批归一化和LeakyReLU激活函数。

（6）使用转置卷积，将14×14×64张量转换为输出图像大小

28×28×1。

（7）应用tanh激活函数。

在Keras中实现生成器网络的代码如清单4.3所示。

清单4.3 DCGAN生成器

def build_generator(z_dim):

model = Sequential()

model.add(Dense(256 * 7 *

7, input_dim=z_dim))←--- 通过全连接

层将输入重新调整大小为7×7×256的张量

model.add(Reshape((7, 7, 256)))

model.add(Conv2DTranspose(128, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层从大小为7×7×256的张量到14×14×128的

张量

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活

model.add(Conv2DTranspose(64, kernel_size=3, strides=1,

padding='same')) ←--- 转置卷积层从大小为14×14×128的张量到14×14×64

的张量

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Conv2DTranspose(1, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层从大小为14×14×64的张量到28×28×1的张

量

model.add(Activation('tanh')) ←--- 带tanh激活函数的输出层

return model

4.4.3 构造鉴别器

鉴别器是一种我们熟悉的ConvNet，它接收图像并输出预测向量：

在这种情况下，它是一种二值分类，表明输入的图像是否被认为是真

实的而不是假的。图 4.5 所示的是我们将要实现的鉴别器网络。

图4.5 鉴别器将28×28×1图像作为输入经过多个卷积层，使用sigmoid激活函

数σ输出/输入图像是真实的概率。在卷积层之间应用批归一化来稳定训练过程

（图像未按比例绘制）

鉴别器的输入是28×28×1的图像。应用卷积可以对图像进行变

换，使其宽×高逐渐变小，深度逐渐变深。在所有卷积层中使用

LeakyReLU激活函数；批归一化用于除第一层以外的所有卷积层；输

出使用全连接层和sigmoid激活函数。

综合所有步骤如下。

（1）使用卷积层将28×28×1的输入图像转换为14×14×32的张

量。

（2）应用LeakyReLU激活函数。

（3）使用卷积层将14×14×32的张量转换为7×7×64的张量。

（4）应用批归一化和LeakyReLU激活函数。

（5）使用卷积层将7×7×64的张量转换为3×3×128的张量。

（6）应用批归一化和LeakyReLU激活函数。

（7）将3×3×128张量展成大小为3×3×128=1152的向量。

（8）使用全连接层，输入sigmoid激活函数计算输入图像是否真

实的概率。

用Keras实现鉴别器，代码如清单4.4所示。

清单4.4 DCGAN鉴别器

def build_discriminator(img_shape):

model = Sequential()

model.add(

Conv2D(32, ←--- 卷积层，从大小为28 ×

28× 1的张量到

14×14×32的张量

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，从大小为14×14×32的张量到7×7×64的张量

Conv2D(64,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，从大小为7×7×64的张量到3×3×128的张量

Conv2D(128,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Flatten())

model.add(Dense(1, activation='sigmoid')) ←--- 带sigmoid激

活函数的输出层

return model

4.4.4 构建并运行DCGAN

除了生成器和鉴别器的网络结构，DCGAN的其他设置和实现与第3

章中简单GAN的网络相同。这体现了GAN架构的通用性。构建并编译

DCGAN的代码如清单4.5所示，对模型进行训练的代码如清单4.6所示。

清单4.5 构建并编译DCGAN

def build_gan(generator, discriminator):

model = Sequential()

model.add(generator) ←--- 生成器和鉴别器结合为一个模型

model.add(discriminator)

return model

discriminator = build_discriminator(img_shape) ←---

构建并编译

鉴别器

discriminator.compile(loss='binary_crossentropy',

optimizer=Adam(),

metrics=['accuracy'])

generator = build_generator(z_dim) ←---

构建生成器

discriminator.trainable = False ←---

生成器训练时鉴别器参数保持不变

gan = build_gan(generator, discriminator)

gan.compile(loss='binary_crossentropy', optimizer=Adam()) ←---

构建并编译鉴别器固定的GAN模型来训练生成器

清单4.6 训练DCGAN

losses = []

accuracies = []

iteration_checkpoints = []

def train(iterations, batch_size, sample_interval):

(X_train, _), (_, _)

= mnist.load_data() ←--- 加载MNIST数

据集

X_train = X_train /

127.5 - 1.0 ←---

灰度像素值从[0, 255]缩

放到[1, 1]

X_train = np.expand_dims(X_train, axis=3)

real = np.ones((batch_size, 1))

←--- 真实图像标签为1

fake = np.zeros((batch_size, 1))

←--- 伪图像标签为0

for iteration in range(iterations):

idx = np.random.randint(0, X_train.shape[0],

batch_size)

←--- 获取一批真实图像

imgs = X_train[idx]

z = np.random.normal(0, 1,

(batch_size, 100)) ←---

生成一批伪图像

gen_imgs = generator.predict(z)

d_loss_real = discriminator.train_on_batch(imgs, real)

←--- 训练鉴别器

d_loss_fake = discriminator.train_on_batch(gen_imgs,

fake)

d_loss, accuracy = 0.5

* np.add(d_loss_real,

d_loss_fake)

z = np.random.normal(0, 1,

(batch_size, 100)) ←---

生成一批伪图像

gen_imgs = generator.predict(z)

g_loss = gan.train_on_batch(z, real)

←--- 训练生成器

if (iteration + 1)

% sample_interval == 0:

losses.append((d_loss, g_loss)) ←---

accuracies.append(100.0 * accuracy)

iteration_checkpoints.append(iteration + 1) ←--

- 保存损失和准确率以便训练后绘图

print("%d [D loss: %f,

acc.: %.2f%%] [G loss:

%f]"

% ←--- 输出训练过程

(iteration + 1, d_loss,

100.0 * accuracy,

g_loss))

sample_images(generator) ←--- 输出生成图像的采样

为完整起见，清单4.7包含了sample_images()函数，它在指定的

训练迭代中输出一个4×4的图像网格。

清单4.7 显示生成图像

def sample_images(generator, image_grid_rows=4,

image_grid_columns=4):

z = np.random.normal(0, 1,

(image_grid_rows *

image_grid_columns, z_dim)) ←--- 随机噪声采样

gen_imgs = generator.predict(z) ←---

从随机噪声生成图像

gen_imgs = 0.5 *

gen_imgs + 0.5 ←---

图像像素缩放到[0,1]

fig, axs = plt.subplots(image_grid_rows,

←--- 设置图像网格

image_grid_columns,

figsize=(4, 4),

sharey=True,

sharex=True)

cnt = 0

for i in range(image_grid_rows):

for j in range(image_grid_columns):

axs[i, j].imshow(gen_imgs[cnt, :, :,

0],

cmap='gray') ←--- 输出一个图像网格

axs[i, j].axis('off')

cnt += 1

接下来，使用清单4.8所示的代码运行模型。

清单4.8 运行模型

iterations = 20000 ←---

设置超参数

batch_size = 128

sample_interval = 1000

train(iterations, batch_size, sample_interval) ←---

训练DCGAN

直到指定的迭代次数

4.4.5 模型输出

充分训练后的DCGAN的生成器生成的手写数字如图4.6所示。为便

于同时比较，图4.7显示了第3章中GAN生成的数字样本，图4.8显示了

MNIST数据集中真实的手写数字样本。

图4.6 充分训练的DCGAN生成的手写数字

图4.7 第3章中实现的GAN生成的手写数字

图4.8 从用于训练DCGAN的MNIST数据集中随机选择的真实手写数字。与第3章中

简单GAN生成的图像不同，充分训练的DCGAN生成的许多手写数字与训练数据几乎

没有区别

可以看出，为实现DCGAN而做的所有额外工作都得到了丰厚的回报

——网络经过充分训练后产生的许多手写数字图和人手所写的真实图

像别无二致。

4.5 结论

DCGAN展示了GAN框架的通用性。理论上来说，鉴别器和生成器可

以用任何可微函数表示，甚至可以用多层卷积网络这样复杂的函数表

示。但是DCGAN也表明，要使更复杂的实现在实践中真正起作用，还存

在很大的障碍。没有批归一化等突破性技术，DCGAN将无法正确训练。

我们将在第5章探讨使GAN训练变得具有挑战性的一些理论和实践

方面的局限性，以及克服这些局限性的方法。

4.6 小结

（1）卷积神经网络（ConvNet）使用一个或多个在输入图像上滑

动的卷积滤波器。在输入图像上滑动的每一步，滤波器都会使用一组

参数来产生一个激活值。来自所有滤波器的所有激活值共同生成输出

层。

（2）批归一化是指在将每一层的输出作为输入传递到下一层之

前，对其进行归一化，以减小神经网络中协变量偏移（训练期间各层

之间输入值分布的变化）。

（3）深度卷积生成对抗网络（DCGAN）以卷积神经网络为生成器

和鉴别器。本章的代码教程中实现了这种架构，它在图像处理任务

（如手写数字生成）中实现了优异的性能。

[1] Unsupervised Representation Learning

with Deep

Convolutional Generative Adversarial Networks,

by Alec

Radford et al., 2015.

[2] Deep Generative Image

Models Using a Laplacian

Pyramid

of Adversarial Networks, by

Emily Denton et al.,

2015.

[3] Batch Normalization: Accelerating

Deep Network Training

by Reducing Internal Covariate

Shift, by Sergey Ioffe

and

Christian Szegedy, 2015.

第二部分 GAN的前沿主题

第二部分探讨了GAN中的一些前沿主题。以第一部分的基本概念为

基础，这部分旨在加深对GAN的理论解释，并扩展实现GAN的实用工具

包。

第5章涉及训练GAN的诸多理论，并介绍了实践上的障碍以及克服

这些障碍的方法。

第6章介绍一种开创性的训练方法——渐进式增长

GAN（Progressive GAN），它使GAN以前所未有的分辨率合成高质

量图像。

第7章介绍GAN在半监督学习（只用一小部分带标签样本训练分类

器的方法）中的应用，这在实践中具有重要意义。

第8章介绍条件GAN（conditional GAN），这是一种在训练生成器

和鉴别器时，使用标签（或其他条件信息）来引导生成目标数据

的技术。

第9章探讨CycleGAN，这是一种用于图像到图像转换的通用技术，

可以将一个图像（如苹果）转换为另一个图像（如橙子）。

第5章 训练与普遍挑战：为成功而GAN

本章主要内容

评估GAN时遇到的挑战

最小-最大（Min-Max）GAN、非饱和（Non-Saturating）GAN以及

沃瑟斯坦（Wasserstein）GAN（即WGAN）

使用要诀和技巧实现最优训练

注意

阅读本章时请记住，GAN难以训练又难以评估是众所周知的。与其

他任何前沿领域一样，关于什么是最优方法的观点也在不断变化。

像“如何训练你的DRAGAN”这样的论文既证明了机器学习研究人

员的恶搞能力越来越强，也体现了很好地训练生成对抗网络是非常困

难的。专注于改进GAN 训练的arXiv论文多达几十篇，并且在顶级学术

会议上（包括NIPS[1]，著名的机器学习会议之一），针对GAN训练的

各个方面举办了无数场研讨会。

但是，GAN的训练是一个不断发展的挑战，因此许多资源（包括论

文和会议发表的）现已需要进行一定程度的更新。本章提供了最全

面、最新的训练技术概述，其中有很多涉及数学理论的内容，但是没

有超出必要要求。

作为本书第二部分的第1章，本章内容相当丰富。建议你尝试一些

带有多个参数的模型后再回到本章，那样不仅对GAN每个部分的功能有

了深刻的理解，还切身体验了训练它们所面临的挑战。

本章不但教读者如何去训练，而且提供了至少在未来几年内都有

用的参考。这里总结了人们的经验、博客文章以及相关论文中的训练

技巧和窍门。（如果你以后不会选择学术道路，现在可以拿出涂鸦笔

在脚注上乱涂乱画了。）我们将本章视为一个短暂的学术中场休息，

它将清晰地描绘出现在和未来GAN所有令人惊叹的发展。

我们希望以一些基本工具武装你的头脑，以帮助你理解今后可能

出现的新论文。在许多书中，训练技巧会以利弊清单的形式出现，这

样并不能使你有全面而高层次的理解。由于GAN是一个新兴领域，学术

文献尚未就某些方面给出结论，就更不可能有简单的技巧清单。GAN领

域的发展如此之快，我们更希望授人以渔而不是授人以鱼——授予你

找到正确方法的能力，而不是简单提供很快就过时的信息。

解释完本章目的后，你需要再次明确GAN所处的位置。图5.1在第2

章的图上进行了扩展并展示了模型的分类，让你可以了解到有哪些其

他生成技术以及它们之间的不同与相似之处。

（来源：Generative Adversarial Network (GAN),

by Ian Goodfellow,

NIPS 2016 tutorial.）

图5.1 GAN应置于何处

图5.1有如下两个关键要点。

（1）所有这些生成模型最终都源自最大似然（maximum

likelihood），至少隐式地是这样。 （2）第2章介绍的变分自编码器

位于树的显式部分。还记得有一个明确的损失函数（重建损失）吗？

有了GAN就没有这个函数了，不过现在有两个相互竞争的损失函数，稍

后我们将更深入地讨论。但这样系统就没有了单一的解析解。

如果你了解一些图中其他技术，那就太好了。图中展示的关键在

于训练正在从显式和易驾驭的方法转向隐式的方法。如果没有显式的

损失函数（尽管在3.2.1节中含蓄地提到了两种不同的损失），如何去

评估GAN呢？如果正在进行并行的大规模的实验，那又该怎么办呢？

为了消除可能的混乱，我们在此说明图5.1中的技术并非都来自深

度学习，当然你也不需要了解除VAE和GAN之外的技术。

5.1 评估

回顾一下第1章中伪造达•芬奇画作的类比。假设一个伪造者（生

成器）正在试图模仿达•芬奇，想使这幅伪造的画被展览接收。伪造者

要与艺术评论家（鉴别器）竞争，后者试图只接收真正的作品进入展

览。如果你是那位伪造者，目的是伪造这位伟大艺术家的“遗失的作

品”，以对达•芬奇风格的完美模仿欺骗艺术评论家，要如何评价自己

的做得有多好呢？

GAN试图解决伪造者与艺术评论家之间永无止境的竞争问题。考虑

到生成器通常比鉴别器更受关注，考虑它的评估时应该格外仔细。但

是要如何量化一个伟大画家的风格，或者说如何模仿他呢？以及如何

才能量化生成作品的整体质量？

5.1.1 评估框架

首选解决方案是拥有达•芬奇用他的风格画出的所有可能的作

品，然后看看用GAN生成的图像会不会在这个收藏集中。可以将此过程

视为最大化最大似然的非近似版本。事实上，我们已经知道一个图像

是否在该集合中，因此不涉及任何可能性。这种解决方案在实际中是

永远不可能实现的。

次优的解决方案是评估图像并指出对应要检查的地方，然后合计

所有错误或伪造的画的数量。这样局限性很大，而且最终总是需要人

类评论家来审查作品。从根本上讲，这种解决方案尽管可能是次优

的，但也是不可大规模使用的。

要用一种统计方法来评估生成样本的质量，这样可以扩大评估规

模并可在实验中使用。如果没有一个易于计算的度量标准，也就无法

监控进展。想象一下，循环中每次超参数初始化时，测量或者反向传

播都需要人为调整——这对于评估不同的实验尤其是一个问题。GAN对

超参数非常敏感，从而让这个问题更为致命。因此，如果没有统计指

标会非常困难，每次要评估训练质量时都必须人为核对。

那为什么不能直接用已经了解的方法（如最大似然）作为指标

呢？最大似然是统计的方法，可以度量一些模糊的期望，而且不管怎

样都能从中得到隐式的结果。尽管如此，最大似然还是很难使用，因

为要对基础分布及其可能性进行正确的估计，这可能意味着要处理数

十亿图像。[2] 即使只有一个好的样本——我们从训练集中获得的有

效内容，也有理由想要得到超越最大似然的效果。

最大似然还存在什么问题呢？它可是许多机器学习研究中固定使

用的指标。最大似然具有许多期望性质，但正如上文所说，将其用于

GAN的评估并不容易。

此外，实际中，最大似然近似容易过度泛化，因此生成的样本由

于太过多样化而显得不真实。[3] 使用最大似然可能会生成在现实世

界中永远不会出现的样本，例如有多个头的狗或者一只长着几十只眼

睛却没有身体的长颈鹿。因为不希望GAN的“暴力生成”给任何人带来

噩梦，所以应该使用损失函数和/或评估方法，淘汰“过于泛化”的样

本。

考虑过度泛化的另一种方法是从伪数据和真实数据的概率分布开

始，看看距离函数（一种测量真假图像分布之间距离的方法）在概率

质量为0的情况下的作用。如果这些样本之间没有太大的差异，那么因

这些样本过度泛化而造成的额外损失可能很小，例如，除了几个关键

问题（如多个头），这些模式都接近真实数据。根据真实数据生成的

不应该有一头牛有多个头的样本，但是一个过度泛化的度量标准允许

创建这样的样本。

这就是为什么研究人员认为需要不同的评估原则，即使我们切实

在做的事一直是使可能性最大化，只是以不同的方式对其进行衡量而

已。稍后讲到的KL散度和JS散度也是基于最大似然的，因此这里可以

将它们视为可互换的。

我们必须评估样本，但是又不能简单地使用最大似然。接下来将

讨论用于统计评估生成样本质量的两个最常用且公认的度量标准：

Inception Score（IS）和Fréchet Inception Distance（FID）。

这两个指标的优点在于，它们已被广泛证实与至少某些期望性质（如

图像的视觉吸引力或真实感）高度相关。IS完全是基于“样本应该是

可识别的”这一理念设计的，但也被证明与人类对真实图像构成的直

觉有关，这一点已经过Amazon Mechanical Turk[4]的验证。

5.1.2 IS

显然，我们需要一种好的统计评估方法。让我们从理想的评估方

法所要达到的高层次要求开始。

（1）生成的样本要看起来像真实的可分辨的东西，如桶或奶牛。

样本不但要看起来逼真，而且要是数据集中的物品。此外，分类器确

信它看到的就是它识别成的物品。幸运的是，我们已经有了计算机视

觉分类器，它能够将图像分类为某个特定类别，并具有一定的可信

度。IS本身就是以其中的一个分类器——以Inception网络命名的。

（2）生成的样本是多种多样的，并且理想情况下应该包含原始数

据集中表示的所有类别。这一点也很重要，生成的样本应该是训练数

据集的代表。如果生成MNIST数据集的GAN始终不能生成数字8，则说明

它不是一个好的生成模型。不应该存在类间模式崩溃（interclass

mode collapse）[5]。

虽然我们可能会对生成模型还有更多的要求，但这是一个良好的

开端。

IS最初出现在2016年的一篇论文中，该论文对IS进行了全面的验

证，证实它确实与人类对高质量样品构成的直觉有关。[6] 此后，这

个指标在GAN研究界流行起来。

我们已经解释了为什么要使用这一指标。现在深入了解一下技术

细节，计算IS的过程很简单：

（1）采用真实分布和生成分布之间的Kullback-Leibler（KL）差

异。[7]

（2）对第1步的结果求指数。

举个例子：辅助分类器生成对抗网络（Auxiliary Classifier

GAN，ACGAN）[8] 的失效模式，我们试图从ImageNet数据集中生成雏

菊样本。当在以下ACGAN失效模式下运行Inception网络时，会看到图

5.2所示的内容。你得到的结果可能会与此有些差异，这取决于操作系

统、TensorFlow版本和实施细节。

（来源：Odena, 2017, https://arxiv.org/pdf/1610.09585.pdf.）

图5.2 ACGAN的失效模式，右边的分数表示softmax的输出

这里要注意的重点是，Inception分类器不确定它在看什么，尤其

是在前3个类别中。人们会觉得它可能是一朵花，但就连我们也不确

定。对预测的总体信心（指数）也很低（分数总和应为1.00）。这是

一个IS较低的例子，它符合本节开始的两个要求。因此这一探索指标

的过程是成功的，因为它符合我们的直觉。

5.1.3 FID

下一个要解决的问题是样本多样性的缺乏。通常，GAN只能学习到

每个类别中的一小部分图像。2017年，有人提出了一个新的解决方

案：Fréchet Inception Distance（FID）。[9] FID通过提高对噪声

的鲁棒性并检测类内（intraclass）样本遗漏来改进IS。

这是很重要的，如果接受IS基准，那么仅生成一种类型的图像。

从技术上来说，这已经满足了能生成此类别的要求。但是，假如想创

建猫咪生成算法，这实际上并不是我们想要的（比如，有多个品种的

猫的样本）。此外，我们还希望GAN可以从不同角度输出代表猫的样

本，并且通常是明显不同的图像。

我们同样不希望GAN只是简单地记住图像。幸运的是，这个问题容

易检测——查看像素空间中图像之间的距离即可，如图 5.3 所示。

FID的技术实现也很复杂，但背后的高级想法是我们正在寻找一个样本

的生成分布，使得为了确保生成分布与真实分布相似而必须进行的修

改最小化。

FID是通过Inception网络运行图像来计算的。在实际应用中，我

们比较的是中间表示（特征图或层）而不是最终输出（换句话说，是

嵌入它们）。更具体地说，计算嵌入均值的距离、方差和两个分布

（真实分布和生成分布）的协方差。

（来源：Do GAN Actually Learn

the Distribution? An Empirical

Study,by Sanjeev Arora and

Yi Zhang, 2017.）

图5.3 GAN主要通过记忆样本获取模式，这也产生了不良结果；表明GAN并未学到

很多有用的信息，很可能不会泛化。证据就在图中：前两排是重复的样本；最后

一排是中间那排在训练集中最近邻的样本。请注意，由于GAN设置的分辨率较低，

显示这些样本的分辨率非常低

为了从图像中抽象出来，如果我们有一个包含一些容易理解的分

类器的域，就可以使用它们的预测来衡量特定样本看起来是否真实。

总而言之，FID是一种从人类评估者那里抽象出来的方法，它可以根据

分布进行统计推理，甚至可以用于诸如图像的真实感这样难以量化的

事物。

这一指标实在是太新了，因此我们仍然有必要拭目以待，看看以

后的论文会不会发现它的缺陷。但考虑到已经有很多知名的学者开始

使用这个指标，本章便将其包括在内了。[10]

5.2 训练中的挑战

训练GAN可能很复杂，我们将在本节介绍最优方法，但仅给出高级

的、易于理解的解释，而不会深究那些证明定理或数学依据，因为这

些细节超出了本书的范围。你可以寻找参考资料并自行尝试，这些资

料通常会提供一些代码示例来帮助入门。

下面列举了一些训练中的主要问题。

（1）模式崩溃。在模式崩溃中，某些模式（如某些类）在生成

的样本中没有很好地表示出来。即使真实数据分布中有样本分布在这

一部分，该模式也崩溃了。例如，MNIST数据集没有生成数字8。注

意，即使网络已经收敛，模式崩溃也可能发生。我们在解释IS时已经

讨论了类间模式崩溃，在解释FID时讨论了类内模式崩溃。

（2）收敛速度慢。这是GAN在GAN和无监督场景下的一个大问

题。在这种情况下，收敛速度和可用的计算资源是主要的限制；而在

监督学习中，可用的标记数据通常是首要障碍。有些人认为，未来的

人工智能竞赛的决定因素将会是计算力，而不是数据。此外，每个人

都希望不出几天的训练就能快速得到模型。

（3）过度泛化。我们特别讨论一下不应该得到支持（不应该存

在）的模式（潜在数据样本），例如，一头牛有多个身体但只有一个

头，或者只有一个身体但有多个头。当GAN过度泛化并学习到基于真实

数据不应该存在的事物时，这种情况就会发生。

注意，模式崩溃和过度泛化有时可以通过重新初始化算法来简单

地解决，但这样的算法是脆弱的。上面列出的问题大致提出了两个关

键指标：速度和质量。即便是两个指标，它们也是相似的，许多训练

最终都专注于更快地消除真实分布和生成分布之间的差距。

那么该如何解决呢？在GAN训练方面，下列几种技术可用于改善训

练过程。

（1）增加网络深度。

（2）更改网络设置。

原始论文提出的最小-最大（Min-Max）设计和停止判则。

原始论文提出的非饱和（Non-Saturating）设计和停止判则

[11]。

最近的改进——沃瑟斯坦生成对抗网络（Wasserstein GAN）。

（3）其他一些的训练技巧。

归一化输入。

梯度惩罚。

多训练鉴别器。

避免稀疏梯度。

使用平滑和带噪声的标签。

5.2.1 增加网络深度

与许多机器学习算法一样，最简单的使学习更稳定的方法是降低

复杂性。如果可以从简单的算法开始并逐步地增加复杂性，则训练过

程可以更稳定，收敛更快，并且还有潜在的其他好处（第6章）。

我们可以使用简单的生成器和鉴别器来快速实现稳定性，然后在

训练时增加复杂性，正如在最令人兴奋的GAN论文中所解释的那样。

[12] 论文中提到，NVIDIA的研究者逐步扩大这两个网络，在每个训练

周期结束时将生成器的输出大小加倍，鉴别器的输入也加倍。我们从

两个简单的网络开始训练，直到获得良好的性能。

这样可确保不是从一个比初始输入大几个数量级的巨大参数空间

开始，而是从生成一个4像素×4像素的图像开始，在将输出大小加倍

之前操控这个参数空间。重复此操作，直到获得大小为1024像素

×1024像素的图像。

看看效果多么令人印象深刻——图5.4中的两幅图都是生成的。现

在，我们已经超越了自编码器生成的64像素×64像素的模糊图像。

（来源：Karras et al., 2017,

https://arxiv.org/abs/1710.10196.）

图5.4 GAN生成的全高清图像，可以把这当作第6章的预告

这种方法具有以下优点：稳定、训练速度快，最重要的是生成样

本的质量好、尺寸大。我们希望越来越多的论文可以使用这种新的范

例。在实验中一定要用这种方法的原因还包括这种技术几乎可以应用

于任何类型的GAN。

5.2.2 游戏设置

思考GAN的双方博弈本质的一种方法是，想象正在玩围棋游戏或其

他可能在任何时候结束的棋盘游戏，比如国际象棋（实际上，这是借

鉴了DeepMind团队创造AlphaGo的方法，并将其分为策略网络和价值网

络）。作为一名玩家，你不仅需要了解游戏的目的和两个玩家都想达

成的目标，还需要了解你离胜利有多近。因此，要有规则并且有距离

（胜利）指标，如丢失的棋子数量。

但正如并非每一个棋盘游戏的胜利指标都适用于其他游戏一样，

某些GAN胜利指标——距离或散度——往往用于特定的游戏设置。我们

有必要分别研究每个损失函数（胜利指标）和玩家动态（游戏设

置）。

这里引入一些描述GAN问题的数学表示，这些方程式很重要，但不

会用到任何不必要的复杂数学。之所以介绍它们，是想给你提供一个

高层次的解释，并提供一些工具去理解许多 GAN 的研究人员似乎仍然

无法区分的东西。（好吧，也许他们应该在头脑中也训练一个鉴别

器。）

5.2.3 最小-最大GAN

正如在本书前面所解释的那样，我们可以从游戏理论的角度思考

GAN的设置，这种假设下有两个玩家都在试图超越对手。2014年的原始

论文也提到游戏有两个版本。原则上，更易理解且理论上更有根据的

方法正是现在所描述的：仅将GAN问题视为一个最小-最大（min￾max）博弈。式5.1描述了鉴别器的损失函数。

式5.1

s代表对 （真实数据分布）或 （潜在空间）的期望， 代表鉴

别器的函数（将图像映射到概率）， 代表生成器的函数（将潜在向

量映射到图像）。任何二元分类问题都应该熟悉这第一个方程。如果

给一定的自由并摆脱复杂性，则可以把这个方程改写为

这说明鉴别器正在尝试最大程度地减少将真实样本误认为是伪样

本（第一部分）或将伪样本误认为是真实样本（第二部分）的可能

性。

现在我们将注意力转向式5.2中生成器的损失函数。

式5.2

因为只有两方起作用且它们彼此竞争，所以有理由认为生成器的

损失对鉴别器来说是负损失。

将它们放在一起：有两个损失函数，而且一个是另一个的负值。

这样对抗性就很明显了，生成器试图比鉴别器更聪明。至于鉴别器，

请记住它是一个二元分类器，只输出一个数字（而不是两类），因此

它会因其可信度或缺乏可信度而受到惩罚。剩下的只是一些花哨的数

学运算，这些运算证明了一些很好的性质，例如JS散度的渐近一致

性。

前面已经解释了通常不用最大似然的原因。我们使用如KL散度和

JS散度以及最近的推土机距离（也称为Wasserstein距离）来代替。所

有这些散度均有助于理解真实分布与生成分布之间的差异。JS散度度

量了两个概率分布的相似度，它是基于KL散度的变体，解决了KL散度

非对称的问题。一般来说，JS散度是对称的。

定义

JS散度（JSD）是KL散度的对称版本。若 ，则

有 。

KL散度和JS散度通常被视为GAN最终在试图最小化的东西。这两种

都是距离指标，有助于理解高维空间中两种分布的差异。一些精巧的

证明将这些散度与GAN的min-max版本联系起来了，但是对于本书而

言，这些问题太学术化了。如果你没有理解这段话，也不要担心，这

只是统计学家的事情。

最小-最大GAN（MM-GAN）用于给出良好的理论解释，除此之外通

常不在实际中使用。它是理解GAN的一个简洁的理论框架：既是一个博

弈论的概念（源于两个网络/玩家之间的竞争本质），又是一个信息论

的概念。除此之外，MM-GAN通常没有任何优势，接下来的两种结构更

为典型。

5.2.4 非饱和GAN

在实际应用中，我们经常发现MM-GAN会带来很多问题，例如鉴别

器收敛缓慢。GAN的原始论文提出了一种替代方法：非饱和GAN（Non￾Saturating GAN，NS-GAN）。在这一版本中，没有让两个损失函数成

为彼此的直接竞争对手，而是使两个损失函数相互独立，如式5.3所

示，但在方向上与原始公式（式5.2）一致。

同样让我们专注于一个一般性的解释：这两个损失函数不再直接

相互抵消。但是在式5.3中，可以看到生成器正在试图最小化方程式

5.4中鉴别器第二项的相反项，它试图让它所生成的样本不被发现（是

假的）。

式5.3

式5.4

从直观上看，鉴别器与之前完全相同——式5.1和式5.4相同，但

是式5.2的等价形式已经变了。使用NS-GAN的主要原因是在MM-GAN的情

况下，梯度很容易饱和到接近0，这种情况下，反向传播的权重更新为

0或很小，从而导致收敛缓慢。也许图示能更清晰地展示这一点（图

5.5）。

（来源: Understanding Generative Adversarial

Networks,by Daniel

Seita, 2017.）

图5.5 这些假设的关系在理论上应该是什么样子的。 轴是生成器的损失函

数，而 是鉴别器对生成样本可能性的“猜测”。可以看到，

Minimax（MM）保持平坦状态的时间过长，给生成器的信息太少——梯度消失了

可以看到在0.0附近，最大似然和MM-GAN的梯度都接近于0，这是

很多早期训练发生的地方；而NS-GAN在0.0附近的梯度要高得多，所以

训练在一开始就进行得更快。

对NS变体为什么会收敛到纳什均衡，没有很好的理论解释。实际

上，由于NS-GAN是启发性的，因此使用这种形式不再提供过去获得的

简洁的数学保证，如图5.6所示。由于GAN问题的复杂性，即使用NS￾GAN训练，也有可能完全不收敛，尽管经验证明它表现得比MM-GAN更

好。

图5.6 请默哀

但是这种可怕的牺牲带来了性能的显著提升。NS方法的优点不但

在于初始训练更快，而且由于生成器学习得更快，鉴别器也学习得更

快。这正是我们所期待的，（几乎）所有人在计算和时间上的预算都

很紧，当然学习得越快越好。有人认为，在使用固定的计算力时，NS￾GAN仍没有被超越，甚至Wasserstein GAN也不能说是一个比它更好的

架构。[13]

5.2.5 何时停止训练

严格地说，NS-GAN不再与JS散度渐近一致，且具有理论上更加难

以解释的平衡状态。

第一点很重要，因为JS散度是一个有意义的工具，它可以解释为

什么隐式生成分布应该完全收敛到真实分布，并从原则上给出了停止

训练的标准。但是在实际中，这几乎毫无意义，因为永远无法验证真

实分布和生成分布何时收敛。人们一般通过每隔几次迭代查看生成的

样本来决定何时停止。最近，有些人开始考虑通过FID、IS或不太流行

的分段Wasserstein距离来定义停止标准。

第二点也很重要，因为不稳定显然会导致训练出现问题。一个更

重要的问题是知道何时停止。GAN问题的两个原始公式从来没有给出在

实际中完成训练的明确条件。原则上总是说一旦达到纳什均衡训练就

完成了，但实践中这又很难验证，因为高维性使得均衡难以证明。

如果绘制生成器和鉴别器的损失函数，它们通常会随处乱跳。这

很容易解释，因为它们互相竞争，如果一个变得更好，另一个损失就

会更大。因此，仅通过观察两个损失函数还不能知道何时真正完成了

训练。

NS-GAN的拥护者们声称，NS-GAN仍然比Wasserstein GAN快得多。

因此，NS-GAN或许可以通过更快地运行来克服这些限制。

5.2.6 WGAN

最近，GAN训练的新发展迅速在学术界流行起来：Wasserstein

GAN（WGAN）。[14] 现在几乎所有主要学术论文和从业人员都会提到

它。归根结底，WGAN之所以重要，有以下3个原因。

（1）它显著改进了损失函数，使得损失函数现在可以解释并提供

了更清晰的停止标准。

（2）根据经验，WGAN往往可以得到更好的结果。

（3）与许多对GAN的研究不同，WGAN从损失开始就有明确的理论

支持，还表明我们试图估计的KL散度在理论上和实践上最终都是不合

理的，并在此基础上提出了一种较好的损失函数来解决这一问题。

第一点的重要性在5.2.5节中已经相当明显。由于生成器和鉴别器

之间的竞争性质，没有一个明确的停止训练的时间点。WGAN使用推土

机距离（earth mover’s distance）作为损失函数，该函数与生成

样本的视觉质量明显相关。第二点和第三点的好处是显而易见的——

我们当然希望拥有更高质量的样本和更好的理论基础。

这个魔法是怎么实现的？让我们先来详细看看鉴别器或者说批评

家（critic）的Wasserstein损失，如式5.5所示。

式5.5

该方程式与之前看到的类似（作为方程式5.1的高级简化），但有

一些重要的区别。公式中有函数 ，它充当鉴别器。鉴别器试图估计

推土机距离，并在 函数的不同（有效）参数下，寻求真实分布（第

一项）与生成分布（第二项）之间的最大差异。现在只是简单地测量

差异。鉴别器试图让生成器的处境变得最困难——通过查看在共享空

间中使用 的不同投影，来最大化生成器必须移动的概率量。

式5.6展示了生成器，它现在必须包括推土机距离。

式5.6

在此公式中，我们试图最小化真实分布的期望与生成分布的期望

之间的距离。介绍WGAN的论文很复杂，但它的要点是 是满足技术约

束的函数。

注意

满足的技术约束为1-Lipschitz：对于所有 ， ：

。

生成器试图解决的问题与之前类似，在这里更详细地介绍一下：

（1）从真实分布 或生成分布［ ，其中 ～

( )］中

提取 ；

（2）生成样本是从 （潜在空间）中采样，通过 进行变换以在

同一空间中获得样本 ，然后使用 进行评估；

（3）试图最小化损失函数或距离函数，本例中是推土机距离。实

际的数是用推土机距离计算出来的，我们稍后会加以解释。

设置也很好，因为有一个更容易理解的损失（例如，没有对

数）。我们还有更多可调的训练，因为在WGAN中必须设置一个裁剪常

数（clipping constant），其作用类似于标准机器学习中的学习率。

这为我们提供了一个额外的参数来调参，但是如果GAN架构对它非常敏

感的话，就可能是一把双刃剑。在不深入研究数学的情况下，WGAN有

如下两个实际意义。

（1）有更清晰的停止标准，因为该GAN已被后来许多论文所验

证，它们显示了鉴别器损失与感知质量之间的相关性。可以简单地测

量Wasserstein距离，这有助于告知何时停止训练。

（2）可以训练WGAN直至收敛。这是相关的，因为有meta-review

论文[15]表明，使用JS损失和真实分布中生成器之间的差异来衡量训

练进度通常是没有意义的。[16] 换言之，在国际象棋中有时需要输掉

几个回合，暂时表现变差，以便在几次迭代中学习并最终做得更好。

这听起来像魔术，部分是因为WGAN使用的距离指标与迄今为止遇

到的任何指标都不同。这个距离指标被称为推土机距离或

Wasserstein距离，其背后的想法很聪明。这一次不会再用数学来折

磨你了，让我们来谈谈这个想法。

有两种高维分布：真实的数据分布（我们从未完全看到）和来自

生成器的样本分布（假的）。试想一下，即使是32×32

RGB（×3×256像素值）图像的样本空间会有多巨大。现在把这两个分

布的概率质量都想象成两组山丘。我们将在第10章对此进行更详细的

讨论。图5.7可以作为参考，但基于与第2章相同的思想。

图5.7 图（a）与第2章中的类似。为了更加清楚，图（b）提供了从相同分布中

得出数据的高斯分布的另一个视图，在顶部仅显示第一个分布的切片，在右侧仅

显示第二个分布的切片。图（a）是该数据的概率密度抽象，其中 轴表示该点被

采样的概率。即使其中一个只是另一个的抽象，如何比较两者？即使我们告诉你

（如何比较），你如何确保它们是一样的？如果这一分布具有3072个可能的维度

怎么办？而在这个例子中，仅只有两个维度而已！我们正在建立如何比较如图

（b）中两个像沙堆一样的分布的方法，但随着分布变得越来越复杂，正确匹配也

越来越困难

想象一下，我们必须把代表概率质量的所有土从假分布中移走，

以使它看起来与真实分布完全相同，或者至少与我们所看到的类似。

就好比你的邻居有一个超酷的沙堡，而你有很多沙子，并试图做出完

全一样的沙堡。需要花费多少工夫才能做得惟妙惟肖？嘿，没关系，

我们都经历过这样的事，有时候你只是希望自己的沙堡能更酷更闪亮

一点。

使用Wasserstein距离的近似形式，我们可以评估离生成看起来像

来自真实分布的样本有多近。为什么是近似？因为从来没有看到过真

实的数据分布，所以很难计算出确切的推土机距离。

推土机距离具有比JS散度或KL散度更好的性能，并且已经在WGAN

的基础上有了重要的贡献，同时也验证了它的卓越性能。[17] 尽管在

某些情况下，WGAN并没有完全胜过其他所有GAN，但它至少在任何情况

下都表现得一样好（应该指出，有些人可能不同意这种解释）。

总的来说，WGAN（或其梯度惩罚版本，WGAN-GP）得到了广泛的

使用，并已经成为GAN的研究和实际应用中的行业标准——尽管NS-GAN

不应那么快被遗忘。如果你看到一篇新论文没有将WGAN作为比较的基

准之一并且没有充分理由时，那就要当心了！

5.3 总结游戏设置

我们已经介绍了GAN结构的3个核心版本：最小-最大GAN、非饱和

GAN和WGAN。每篇论文的开头都会提到其中一个版本，你至少应该知道

论文是使用原始的版本（更容易解释，但在实践中效果不佳），还是

非饱和版本（失去了很多数学上的保证，但效果要好得多），还是更

新的Wasserstein版本（既具有理论基础，又具有出色的性能）。

表5.1列出了本书用到的NS-GAN、WGAN甚至改进的WGAN-GP的公

式。为完整起见，我们将WGAN-GP也列入表中，因为这3个都是学术和

行业的首选。

表5.1 损失函数总结a

名

称

公 式 注 释

名

称

公 式 注 释

NS-GAN

这是原始公式之

一，除非作为基

础模块或比较，

通常不在实践中

使用。这是一个

和上文介绍的

NS-GAN等价的

公式，只是没有

常数，它们实际

上是等价的b

WGAN

这是有着简化损

失的WGAN，它

似乎为GAN创造

了一个新的范

例，上文把这个

方程更详细地解

释为方程5.5

名

称

公 式 注 释

WGAN￾GP

c

这是具有梯度惩

罚（GP）GAN的

例子。WGAN￾GP通常有最好

的结果。在本章

中还没有详细讨

论WGAN-GP；为

了完整起见，我

们将其包含在这

里

a. 来源：Collection of Generative

Models in TensorFlow,

by Hwalsuk Lee.

b. 我们倾向于在书面代码中使用常数，而在论文中使用更简洁的

数学公式。

c. 这是一个带有梯度惩罚的WGAN版本，通常在新的学术论文中使

用（Gulrajani et al., 2017.）。

5.4 训练技巧

我们现在要将注意力从有充分根据的学术成果转向学者或从业人

员刚刚“摸索出来”的领域。这些仅仅是技巧，通常只需要尝试一

下，看看它们是否对你有用。本节中的技巧列表受Soumith Chintala

在2016年发布的帖子“How to Train a

GAN: Tips and Tricks

to

Make GAN Work”的启发，但此后发生了一些变化。

发生变化的一个例子是一些架构上的建议，例如DCGAN是一切的基

准。目前大多数人都是从WGAN开始的；将来，自注意力（Self￾Attention）GAN（第12章）可能会成为焦点。此外，有些事情仍然是

正确且公认的，例如使用Adam优化器而不是随机梯度下降。[18] 我们

鼓励你查看上文提到的帖子，因为它的创建是GAN历史上的重要时刻。

5.4.1 输入的归一化

根据几乎所有机器学习资源，包括Chintala的清单，将图像归一

化在–1和1之间通常情况下是一个好主意。之所以进行归一化操作，

是因为计算更容易处理，机器学习的其余情况也是如此。考虑到对输

入的这种限制，最好使用tanh激活函数来限制生成器的最终输出。

5.4.2 批归一化

我们就批归一化在第4章中进行了详细讨论，为完整起见，故将其

包括在此。对批归一化的看法变化为：最初批归一化被认为是一种非

常成功的技术；但最近研究表明，它有时会产生不好的结果，特别是

在生成器中，但在鉴别器中，大多对提升结果有帮助。[19]

5.4.3 梯度惩罚

此训练技巧基于Chintala列表中的第10点，根据直觉，如果梯度

的范数过高，就会出现问题。即使在今天，如第12章中所述，BigGAN

之类的网络也在这一领域进行创新。[20]

但技术问题仍然存在，简单的加权裁剪可能会产生其他深度学习

中已知的梯度消失或爆炸问题。我们可以限制鉴别器输出相对于其输

入的梯度范数。换言之，如果稍微改变输入内容，更新后的权重也不

应该会有太大变化。深度学习充满了这样的魔法。这在WGAN结构中尤

其重要，也可以应用在其他地方。[21] 许多论文都以某种形式使用了

这一技巧。[22]

在这里，我们可以简单地使用你喜欢的深度学习框架的原生实现

来惩罚梯度，而不必关注过多的实现细节。更聪明的方法最近已经由

研究人员发表并在ICML 2018上发布，但尚未得到广泛的学术认可。

[23] 为了使GAN更稳定，人们正在进行大量研究，例如Jacobian

clamping，这也是有待于在新的研究中复现的——所以需要等待，看

看有哪些方法会成功。

5.4.4 对鉴别器进行更多的训练

最近，对鉴别器进行更多的训练是一种成功的方法。在Chintala

的原始列表中，这种方法被标记为效果不确定，因此请谨慎使用。这

里主要有两种方法：

（1）在生成器有机会生成任何样本之前对鉴别器进行预训练；

（2）每个训练周期更多次地更新鉴别器，通常，鉴别器权重更新

5次，生成器才更新1次。

用深度学习研究员兼教师Jeremy Howard的话说，之所以对鉴别器

进行更多的训练有用，是因为这是“外行指导外行”，这首先需要你

不断地注入关于真实数据的信息。

5.4.5 避免稀疏梯度

从直觉上讲，稀疏梯度（如ReLU或MaxPool生成的梯度）会增加训

练难度是有道理的。原因如下。

（1）直觉，尤其是平均池化，这样说可能会令人困惑，但请这样

思考：如果使用标准的最大池化，那么将会失去除了卷积的感受野中

的最大值以外的所有值，这使得在DCGAN的情况下使用转置卷积来恢

复信息变得更加困难。使用平均池化至少可以知道平均值是多少，但

它仍然不是完美——我们仍在丢失信息，但至少比以前少了，因为平

均值比简单的最大值更具代表性。

（2）如果使用ReLU激活，则会导致另一个问题——信息损失。解

决此问题的一种方法是应用此操作时考虑丢失了多少信息，因为稍后

可能需要恢复它。回想一下，ReLU( )只是max(0, )，这意味着对于

所有负值来说所有信息都会永远丢失。如果能确保把负数区域的信息

留到以后处理并标记这些信息是不同的，就可以保留所有这些信息。

幸运的是，对于这两种问题，都有简单的解决方案：可以使用

LeakyReLU激活函数（例如对于 而言是0.1 ，对于 而言是

），平均池化也可以解决很多这些问题。还有其他激活函数（例如

sigmoid、ELU和tanh），但是人们最常用的是LeakyReLU激活函数。

注意

LeakyReLU函数的值可以是任何实数，通常为 。

总的来说，我们试图最小化信息损失，并使信息流尽可能地合乎

逻辑，而不是要求GAN以某种奇怪的方式反向传播错误，在这种情况

下，它还必须学习映射。

5.4.6 平滑和带噪声的标签

研究人员使用多种方法来给标签添加噪声或使其平滑。Ian

Goodfellow推荐使用单边标签平滑（例如，以0和0.9作为二元标

签），但一般来说，增加噪声或裁剪似乎是个好主意。

5.5 小结

（1）通过学习本章内容，你应能了解“评估对于生成模型而言是

一个如此困难的主题”的原因，以及能够在有明确停止标准的情况下

训练GAN的方法。

（2）各种评估技术超越了对分布的简单统计评估，并提供了与样

本视觉质量相关的更有用的信息。

（3）训练分为3种结构设置：博弈论的最小-最大GAN、启发性的

非饱和GAN和最新的以及理论上有据可查的沃瑟斯坦GAN。

（4）能够使训练更快的技巧如下。

归一化输入，这是机器学习中的标准做法。

使用梯度惩罚令训练更具稳定性。

预训练鉴别器可以提供良好的生成器，这样会为生成的样本设置

更高的标准。

避免稀疏梯度，因为它们会丢失太多信息。

使用平滑和带有噪声的标签，而不是典型的二分类标签。

[1] NIPS 2016举办了一个GAN训练研讨会，与会者都是该领域的重要

研究人员，本章就是以此次会议为基础编写的。NIPS最近将其缩写改

为NeurIPS。

[2] 解决维数问题更好的处理方法参见第10章。

[3] How (Not) to

Train your Generative Model:

Scheduled

Sampling, Likelihood, Adversary?, by

Ferenc Huszár, 2015.

[4] Amazon Mechanical Turk

是一种服务，它允许按小时计算地购

买人们的时间来完成预先指定的任务。它有点像按需自由职业者或

Task Rabbit，不过是以在线形式实现的。

[5] An Introduction to

Image Synthesis with Generative

Adversarial Nets, by He

Huang et al., 2018.

[6] Improved Techniques for

Training GANs, by Tim

Salimans

et al., 2016.

[7] KL散度的相关内容参见第2章。

[8] Conditional Image Synthesis

with Auxiliary Classifier

GANs, by Augustus Odena

et al., 2017.

[9] GANs Trained by

a Two Time-Scale Update

Rule Converge to

a Local Nash Equilibrium,

by Martin Heusel et

al., 2017.

[10] Is Generator Conditioning

Causally Related to GAN

Performance?, by Augustus Odena

et al., 2018, talk

at UCL,

February 10, 2018.

[11] Generative Adversarial Networks,

Network, by Ian

Goodfellow et al., 2014.

[12] Progressive Growing of

GANs for Improved Quality,

Stability, and Variation, by

Tero Karras et al.,

2017.

[13] Are GANs Created

Equal? A Large-Scale Study,

by Mario

Lucic et al., 2017.

[14] Wasserstein GAN ,

by Martin Arjovsky et

al., 2017.

[15] meta-review是综述类文献的综述，它有助于研究人员汇集来自

几篇论文的发现。

[16] Many Paths to

Equilibrium: GANs Do Not

Need to Decrease

a Divergence at Every

Step, by William Fedus

et al., 2018.

[17] Improved Training of

Wasserstein GANs, by Ishaan

Gulrajani et al., 2017.

[18] 为什么Adam比SGD更好? 因为Adam是SGD的延伸，实际应用往往

效果更好。Adam将几种训练技巧和SGD组合成一个易于使用的程序包。

[19] Tutorial on Generative

Adversarial Networks—GANs in

the Wild, by Soumith

Chintala, 2017.

[20] Large-Scale GAN Training

for High-Fidelity Natural

Image Synthesis, by Andrew

Brock et al., 2019.

[21] 尽管作者在这里借用了强化学习的概念，称鉴别器为“批评

家”，但那篇论文的大部分灵感都来自于它。

[22] Least Squares Generative

Adversarial Networks, by

Xudong Mao et al.,

2016. 另见BEGAN: Boundary Equilibrium

Generative Adversarial Networks, by

David Berthelot et al.,

2017.

[23] Odena et al.,

2018, http://arxiv.org/abs/1802.08768.

第6章 渐进式增长生成对抗网络

（PGGAN）

本章主要内容

在整个训练过程中渐进式增长鉴别器网络和生成器网络

让训练更稳定，输出更多样化，质量和分辨率更高

使用TFHub —— 一个用于模型和TensorFlow代码的中央仓库

在本章中，我们提供了一个实践教程，使用TensorFlow和

TensorFlow Hub（TFHub）构建渐进式增长生成对抗网络

（Progressive GAN，PGGAN或ProGAN）——一种能够生成全高清的具

有照片级真实感图像的前沿技术。这项技术在顶级机器学习会议ICLR

2018上提出时引起了轰动，以至于谷歌立即将其整合为TensorFlow

Hub中的几个模型之一。这项技术被深度学习的鼻祖之一Yoshua

Bengio称赞为“好得令人难以置信”，在其发布后，立即成为学术报

告和实验项目的最爱。

建议使用TensorFlow 1.7或更高版本来完成本章。本书使用的版

本是1.8+。

建议使用不高于0.4.0版本的TensorFlow Hub，以后的版本因与

TensorFlow 1.x的兼容性问题无法导入。阅读本章后，你将能够实现

PGGAN的所有关键改进。本章涉及4个创新点，它们分别为高分辨率层

的渐进式增长和平滑、小批量标准偏差、均衡学习率和像素级特征归

一化。

本章将给出如下两个主要例子。

（1）PGGAN的关键创新部分的代码，具体来说，就是平滑地增大

高分辨率层以及前面列出的其他3个创新点。PGGAN其余部分的实现所

需篇幅实在太长，无法在本书中展示。

（2）谷歌在TFHub上提供了一个预训练好的且易于下载的实现。

TFHub是一个用于机器学习模型的新的集中式仓库，类似于Docker Hub

或Conda以及PyPI。此复现能够进行潜在空间插值以控制生成样本的特

征。这会简要涉及生成器潜在空间中的种子向量，以便获得想要的图

片（第2章和第4章）。

这里使用TFHub而不是像其他章那样从头开始实现PGGAN，原因有

如下3个。

（1）尤其是对于从业人员，我们希望确保你（至少从一章里）了

解到可以加快工作流程的软件工程最佳实践。想尝试快速用GAN解决问

题吗？使用TFHub上的其中一种实现即可。与最初编写本章时相比，

TFHub现在有更多的实现，包括许多参考实现（如第12章中的BigGAN和

第5章中的NS-GAN）。希望你能接触到易于使用的最先进的例子，因为

这就是机器学习的发展方式——尽可能地使机器学习自动化，这样我

们就可以专注于最重要的事情：产生影响。谷歌的Cloud AutoML和亚

马逊的SageMaker是这种趋势的主要例子，甚至Facebook最近都推出了

PyTorch Hub，所以两种主要机器学习框架现在都有一个仓库了。

（2）NVIDIA研究人员花了一到两个月的时间来运行最初的

PGGAN。任何人想独自运行它都是不切实际的，特别是在进行实验或出

现错误情况下。[1] TFHub也提供了一个完全可训练的PGGAN，因此，

如果想利用做计算的日子来做其他事，你也可以从头训练！

（3）我们仍然想在这里展示PGGAN最重要的创新。但是要很好地

解释这些内容（包括代码），即使是用Keras编写的，也无法将所有实

现细节都放在一章中，因为太过庞大了。TFHub使我们可以跳过无关紧

要的样板代码，而专注于实现重要的想法。

6.1 潜在空间插值

第2章中有一个较低分辨率的空间（潜在空间），可以为输出提

供随机初始值。对于第4章中的DCGAN以及PGGAN，初始训练的潜在空间

具有语义上有意义的性质。这意味着可以找到向量偏移量，例如，将

眼镜引入人脸图像，相同的偏移量会在新的图像中引入眼镜。还可以

选择两个随机向量，然后在它们之间每次移动相等的增量，从而逐渐

平滑地获得与第二个向量匹配的图像。

上述方法称为插值，如图6.1所示。正如BigGAN论文的作者所

说，从一个向量到另一个向量的有意义的转换表明GAN已经学习到了一

些底层结构。

图6.1 我们可以进行潜在空间插值，因为发送给生成器的潜在向量会产生一致的

结果，这种结果在某些方面是可以预测的。如果考虑潜在向量的变化，不仅生成

过程是可预测的，输出也不是参差不齐的，对微小的变化也不会做出剧烈的反

应。例如，想要一幅由两张脸混合生成的图像，在两个向量的平均值附近搜索即

可

6.2 它们发展如此之快

在前面的章节中，我们已经了解到使用GAN可以轻松实现哪些结

果，难以实现哪些结果，还对模式崩溃（只展示了总体分布的几个样

本）和缺乏收敛性（导致结果质量较差的原因之一）有了一定的了

解。

最近，芬兰NVIDIA的一个团队发表了一篇论文，这篇论文成功击

败了之前的许多前沿论文，这就是Tero Karras等人所撰写的

Progressive Growing of GAN

for Improved Quality, Stability,

and Variation。该论文有4个基本的创新点，让我们依次来看看。

6.2.1 高分辨率层的渐进增长和平滑

在深入研究PGGAN的作用之前，我们先来看一个简单的类比！想象

一下，俯瞰某处山区：山区有许多山谷，那里有漂亮的小溪和村庄，

非常宜居。但是其间也会有许多山坡，它们崎岖不平，而且由于天气

原因，通常不宜居住。我们用山谷和山坡类比损失函数，希望沿着山

坡进入更好的山谷，以最小化损失。

我们可以把训练想象成将登山者放到这处山区的任意地方，让他

顺着山坡往下的路进入山谷——这就是随机梯度下降所做的（第10

章）。但是，假设从一处非常复杂的山脉开始，登山者不知道该往哪

个方向走。他周围的地势是崎岖不平的，以至于他很难弄清楚哪里是

有宜居地的最宜人、最低的山谷。假如我们拉远画面并降低山脉的复

杂度，使登山者对这一特定区域有一个高层次的了解。

随着登山者越来越接近山谷，我们再通过放大地形增加复杂性。

这样不再只看到粗糙/像素化的构造，而是可以看到更精细的细节。这

种方法的优势在于，当登山者沿着斜坡下山时，他可以很容易地进行

一些小的优化以使旅行更加轻松，例如，他可以沿着一条干涸的小溪

行走以更快地到达山谷。这就是渐进式增长（progressive

growing）：随着登山者的行进，提高地形的分辨率。

然而，如果你玩过一款沙箱类游戏，或者带着3D眼镜在谷歌地图

上快速移动，就会知道快速增加周围地形的分辨率是惊心动魄且不愉

快的——所有物体突然映入眼帘。因此，随着登山者越来越接近目

标，我们渐进式地、平滑地并慢慢地引入更多的复杂性。

用专业术语来说，就是训练过程正在从几个低分辨率的卷积层发

展到多个高分辨率的层。先训练早期的层，再引入更高分辨率的层

——高分辨率层中的损失空间很难应对。从简单的（如经过几步训练

得到的4×4）开始，最后到更复杂的（如经过多个时期训练的

1024×1024），如图6.2所示。

这种情况下的问题是，即便一次增加一个层（例如，从4×4到

8×8），也会给训练带来巨大的影响。PGGAN所做的就是平滑地增加这

些层，如图6.3所示，以给系统适应更高的分辨率的时间。

但不是立即跳到该分辨率，而是在通过参数α（介于0和1之间，

从0到1线性缩放）平滑地增加高分辨率的新层。α 会影响旧的但扩大

规模的层和新生成的更大的层的利用程度。虚线下方的D部分，只是简

单地缩小至1/2，再平滑地注入训练过的层以用于鉴别，如图6.3（b）

所示。如果我们对这一新层有信心，保持在32×32（图6.3(c)），然

后在恰当地训练好32×32分辨率的层之后，就准备再次增长。

图6.2 能看到如何从平滑的山脉开始，通过放大逐渐增加复杂性吗？实际上这就

是添加额外层对损失函数的影响。这很方便，因为山区（损失函数）在平坦的情

况下更容易导航。可以这样认为：当结构更复杂时（b），损失函数凹凸不平且难

以导航（d），因为参数太多（尤其是在早期层中）会产生巨大的影响——通常会

增加问题的维数。但是，如果最初删除部分复杂度（a），就可以在早期获得更容

易导航的损失函数（c），并且只有我们确信自己处于损失空间近似正确的部分，

才会增加复杂性。只有这样，才能从（a）和（c）转变为（b）和（d）

图6.3 训练了足够迭代次数的16×16分辨率后（a），在生成器（G）中引入了另

一个转置卷积，在鉴别器（D）中引入了另一个卷积，使G和D之间的“接口”为

32×32。生成32×32层有两条路径：（1−α）乘以简单地以最近邻插值增加尺度

的层，这没有任何经过训练的参数，比较直接；（α）乘以额外的转置卷积的输

出层，这需要训练，但最后会表现得更好。二者相连，以形成新的32×32的生成

图像，α 从0到1线性缩放，当α 达到1时，从16×16开始的最近邻插值将完全为

零。这种平滑的过渡机制极大地稳定了PGGAN架构

6.2.2 示例实现

针对以上详细介绍的所有创新，我们将在本节分别给出可以运行

的版本，以便讨论代码。你可以尝试将此作为练习，把这些创新构建

到一个GAN网络中，或通过使用现有的架构来实现。如果准备好了，请

赶紧动手加载值得信赖的机器学习库：

import tensorflow as tf

import keras as K

渐进式平滑增长的代码如清单6.1所示。

清单6.1 渐进式平滑增长

def upscale_layer(layer, upscale_factor):

'''

Upscales layer (tensor) by

the factor (int) where

the tensor is [group,

height, width, channels]

'''

height = layer.get_shape()[1]

width = layer.get_shape()[2]

size = (upscale_factor *

height, upscale_factor * width)

upscaled_layer = tf.image.resize_nearest_neighbor(layer,

size)

return upscaled_layer

def smoothly_merge_last_layer(list_of_layers, alpha):

'''

Smoothly merges in a

layer based on a

threshold value alpha.

This function assumes: that

all layers are already

in RGB.

This is the function

for the Generator.

:list_of_layers : items should

be tensors ordered by

resolution

:alpha : float \in

(0,1)

'''

last_fully_trained_layer = list_of_layers[-2] ←---

如果使

用的是Tensor Flow而不是Keras，要记得scope

last_layer_upscaled = upscale_layer(last_fully_trained_layer,

2) ←--- 现在有了最初训练过的层

larger_native_layer = list_of_layers[-1] ←---

新加的层还没

有完全训练

assert larger_native_layer.get_shape() ==

last_layer_upscaled.get_shape() ←--- 这确保可以运行合并代码

new_layer = (1-alpha) *

upscaled_layer + larger_native_layer

* alpha ←--- 此代码块应该利用广播（broadcasting）功能

return new_layer

我们已经了解了在不增加复杂度的情况下实现渐进式平滑增长的

底层细节，希望你能体会到这个想法的普遍性。虽然Karras等人绝对

不是最先提出在训练过程中增加模型复杂性的，但PGGAN似乎是迄今为

止最有发展前途的方法，它确实是振奋人心的创新。截至2019年6月，

他们撰写的论文被引用多达730余次。在这样的背景下，我们继续来看

第二个重大创新点。

6.2.3 小批量标准偏差

Karras等人在论文中引入的下一个创新点是小批量标准偏差

（mini-batch standard deviation）。在深入探讨之前，我们先回顾

模式崩溃的问题——当GAN学习如何创建一些好的样本或者只是稍微置

换一下它们时，这种情况就会发生。

人们通常希望能够生成真实数据集中所有人的脸，而不是只能生

成某个女人的一张照片。为此，Karras等人创造了一种方法，使鉴别

器可以辨别所获取的样本是否足够多样。这种方法本质上是为鉴别器

计算了一个额外的标量统计量。此统计量是生成器生成的或来自真实

数据的小批量中所有像素的标准偏差。

这是一个非常简单而优雅的解决方案：现在鉴别器需要学习的

是，如果正在评估的批量中图像的标准偏差很低，则该图像很可能是

伪造的——因为真实数据所具有的偏差较大。[2] 生成器别无选择，

只能增加生成样本的偏差，才有机会欺骗鉴别器。

上述内容理解起来很直观，技术实现起来也非常简单，因为它只

应用在鉴别器中。考虑到我们还想最小化可训练参数的数量，只添加

一个额外的数字似乎就够了。该数字作为特征图附加到鉴别器上——

维度或tf.shape列表中的最后一个数字。

具体步骤如下，程序如清单6.2所示。

（1）[4D→3D] 计算批次中所有图像和所有通道（高度、宽度和

颜色）的标准偏差，然后得到关于每个像素和每个通道的标准偏差的

一个图像。

（2）[3D→2D] 对所有通道的标准差取均值，得到像素的一个特

征图或标准差矩阵，但是颜色通道折叠了。

（3）[2D→标量/0D] 对前一个矩阵内所有像素的标准偏差取均

值，以获得一个标量值。

清单6.2 小批量标准偏差

def minibatch_std_layer(layer, group_size=4):

'''

Will calculate minibatch standard

deviation for a layer.

Will do so under

a prespecified tf-scope with

Keras.

Assumes layer is a

float32 data type. Else

needs

validation/casting.

NOTE: there is a

more efficient way to

do this in Keras,

but

just for

clarity and alignment with

major implementations (for

understanding)

this was done more

explicitly. Try this as

an exercise.

'''

group_size = K.backend.minimum(group_size, tf.shape(layer)

[0]) ←--- 如果使用的是TensorFlow而不是Keras，那么scope一个小批量组

必须能够被group_size整除（或<=）

shape = list(K.int_shape(input)) ←---

获取一些形状信息，以便

快速调用和确保默认值。从tf.shape中得到输入，因为pre-image维度通常在图形执行

之前转换为None

shape[0] = tf.shape(input)[0]

minibatch = K.backend.reshape(layer,

(group_size, -1, shape[1], shape[2],

shape[3])) ←---

改变形状，以便在小批量水平上操作。在这段代码中，假设层是[Group (G), Mini￾batch (M),Width

(W), Height (H), Channel

(C)]，注意：不同的实现有的使

用了特定的Theano顺序

minibatch -= tf.reduce_mean(minibatch, axis=0,

keepdims=True)

←--- 将均值集中于组[M,W,H,C]

minibatch = tf.reduce_mean(K.backend.square(minibatch), axis

= 0) ←--- 计算组[M,W,H,C]的方差

minibatch = K.backend.square(minibatch +

1e8) ←--- 计算组

[M,W,H,C]的标准偏差

minibatch = tf.reduce_mean(minibatch, axis=[1,2,4],

keepdims=True) ←--- 取特征图和像素的平均值[M,1,1,1]

minibatch = K.backend.tile(minibatch,

[group_size, 1, shape[2], shape[3]])

←--- 转换标量

值以适应组和像素

return K.backend.concatenate([layer, minibatch], axis=1)

←---

附加一个特征图

6.2.4 均衡学习率

均衡学习率（equalized learning rate）可能是一种谁都不清

楚的深度学习黑魔法。尽管研究人员确实在PGGAN论文中给出了简短的

解释，但他们在口头报告中回避了这个话题，这表明这可能只是一个

“看起来可行”的技巧。在深度学习中，这种情况时有发生。

此外，关于均衡学习率的许多细微差别，需要你对RMSProp或Adam

的实现以及权重初始化有充分的理解。所以，即便你不理解，也不用

担心，因为这可能对谁都没有意义。

但是如果你非常好奇，大概的解释是这样的：需要确保将所有权

重 归一化（ ）在一定范围内，这样 =

/ 就需要一个常数 ，

这个常数 对于每一层都是不同的，具体取决于权重矩阵的形状。这也

确保了如果任何参数需要采取更大的操作来达到最优（因为它们往往

变化更大），那么使用相关参数可以很容易做到。

RMSProp等人使用简单的标准正态初始化，然后在运行时缩放每层

的权重。有些人可能以为Adam已经做到了这一点。是的，Adam允许不

同参数的学习率不同，但它有一个陷阱。Adam通过参数的估计标准偏

差来调整反向传播的梯度，从而确保该参数的大小与更新无关。Adam

在不同的方向上有不同的学习率，但并不总是考虑动态范围——在给

定的小批量中，维度或特征的变化有多大。有人指出这似乎解决了一

个类似于权重初始化的问题。[3]

如果你还是不明白，请不要担心，可以参考我们强烈推荐的两个

出色资源：Andrew Karpathy在2016年的一次计算机科学讲座中介绍了

权重初始化[4]；一篇关于Adam工作原理的文章。[5] 均衡学习率的代

码见清单6.3。

清单6.3 均衡学习率

def equalize_learning_rate(shape, gain, fan_in=None):

'''

This adjusts the weights

of every layer by

the constant from

He's initializer so that

we adjust for the

variance in the

dynamic

range in different features

shape : shape of

tensor (layer): these are

the dimensions

of each layer.

For example, [4,4,48,3]. In

this case, [kernel_size,

kernel_size,

number_of_filters, feature_maps]. But this

will depend

slightly on your implementation.

gain : typically sqrt(2)

fan_in : adjustment for

the number of incoming

connections

as per Xavier's /

He's initialization

'''

if fan_in is None:

fan_in = np.prod(shape[:-1]) ←---

默认

为所有形状维数减去特征图维数的乘积，这给出了每个神经元传入连接的数量

std = gain /

K.sqrt(fan_in) ←--- 此处使用了初始化常量[6]

wscale = K.constant(std, name='wscale',

dtype=np.float32)

←--- 在调整之外创建一个常量

adjusted_weights = K.get_value('layer', shape=shape,

←---

获取权重使用广播机制应用调整

initializer=tf.initializers.random_normal()) * wscale

return adjusted_weights

如果你仍然感到困惑，请放心，无论是学术界还是工业界，这些

初始化技巧和复杂的学习率调整都很少有区别。同样，将权重值限制

在1和1之间，就在大多数次重新运行中取得了更好的效果，但并不意

味着此技巧适用于其他情况。所以让我们来看看更成熟的技术。

6.2.5 生成器中的像素级特征归一化

我们从为什么想要归一化特征的动机——训练的稳定性开始说

起。根据经验，来自NVIDIA的作者发现，训练发散的早期迹象之一是

特征的爆炸式增长。BigGAN的发明者在第12章中也有类似的观察结

果。所以Karras等人引入了一种技术来解决这个问题。从更广泛的意

义上说，这是训练GAN的常见方式：在训练中观察到一个特定的问题，

于是引入了防止该问题发生的机制。

注意，大多数网络都使用了某种形式的归一化。通常使用的是批

归一化或其虚拟版本。表6.1概述了本书迄今为止在GAN中使用的归一

化技术。第4章和第5章提到了这些内容，并介绍了其他的GAN和梯度惩

罚。为了使批归一化及其虚拟版本能够等效工作，我们必须拥有大量

的小批处理，以便求各个样本的平均值。

表6.1 GAN中使用的归一化技术

方法 提出者及提出时间

生成器归

一化

鉴别器归

一化

方法 提出者及提出时间

生成器归

一化

鉴别器归

一化

DCGAN

Radford等人, 2015,

https://arxiv.org/abs/1511.06434.pdf

批归一化 批归一化

改进的

GAN

Salimans等人, 2016,

https://arxiv.org/pdf/1606.03498.pdf

虚拟批归一

化

虚拟批归一

化

WGAN

Arjovsky等人, 2017,

https://arxiv.org/pdf/1701.07875.pdf

— 批归一化

WGAN-GP

Gulrajani等人, 2017,

http://arxiv.org/abs/1704.00028

批归一化 层归一化

基于“所有主要的实现使用了归一化”这一事实，我们可以断定

它显然很重要，但是为什么不直接使用标准的批归一化呢？这是因为

在想要达到的分辨率下，批归一化过于占用内存。我们必须想出使用

少量样本（适用于使用并行网络图的GPU显存内存）仍然可以正常工作

的一些办法。至此，我们了解了像素级特征归一化的需求从何而来以

及为什么要进行像素级特征归一化。

如果从算法角度说，像素级归一化将在输入被送到下一层之前在

每一层获得激活幅度。

像素级特征归一化

对每个特征图，执行

（1）在位置 上获取该特征图 的像素值。

（2）为每个 构造一个向量，其中

a． = [ 的（0，0）值，

的（0，0）值，...，

的（0，0）值]。

b． = [ 的（0，1）值，

的（0，1）值，...，

的（0，1）值]。

…

c． = [ 的

值， 的 值，…， 的

值）]。

（3）将步骤2中定义的每个向量 归一化得到单位标准值，称

之为 。

（4）将原来的张量形状传递到下一层。

结束

像素级特征归一化的过程如图6.4所示。步骤3的准确描述如式6.1

所示。

图6.4 将图像中的所有点（步骤1）映射到一组向量（步骤2），然后对其进行归

一化，以使它们都在同一范围内（通常在高维空间中介于0和1之间），这就是步

骤3

式6.1

式6.1将图6.4步骤2中构造的每个向量归一化（除以根号下的表

达式）。该表达式只是特定 像素每个平方值的平均值。有一件事

可能会让你吃惊，那就是增加了一个小的噪声项（ε）。这只是确保

不被零除的一种方法。Alex Krizhevsky等人在2012年的论文ImageNet

Clas-sification with Deep Convolutional

Neural Networks中对整

个过程进行了更详细的说明。像素级特征归一化的代码见清单6.4。

最后要注意的是，像素级特征归一化这一技巧仅用于生成器，因

为两个网络都使用时，激活幅度的爆炸会导致军备竞赛（arms race）

[7]。

清单6.4 像素级特征归一化

def pixelwise_feat_norm(inputs, **kwargs):

'''

Uses pixelwise feature normalization

as proposed by

Krizhevsky et al. 2012.

Returns the input normalized

:inputs : Keras /

TF Layers

'''

normalization_constant = K.backend.sqrt(K.backend.mean(

inputs**2, axis=-1, keepdims=True) +

1.0e-8)

return inputs / normalization_constant

6.3 主要创新点总结

我们已经就“如何改善GAN训练”提出了4个聪明的想法，如果不

让它们在训练中发挥作用，就很难分辨出它们各自的效果，好在PGGAN

论文的作者们给出了一个有用的表格，以帮助你理解这一点（图

6.5）。

图6.5 各项技术对提高得分的贡献。可以看到，均衡学习率起到了很大作用，像

素级归一化也增强了这一点；尽管作者没有说明如果仅进行像素级归一化但不引

入均衡学习率，那么该技术的有效性如何。我们把这个表格仅作为一个例子来说

明可以从这些变化中得到的改进大致有多少，后面会有更详细的讨论

PGGAN论文的作者使用了分段Wasserstein距离（Sliced

Wasserstein distance，SWD）——其值越小越好。回顾一下第5章，

Wasserstein（推土机）距离越小意味着结果越好，可以用为了使两个

分布相似而必须移动的概率质量来量化。SWD意味着真实数据和生成的

样本的patches会最小化此距离。PGGAN解释了这种技术的细节，但是

正如作者在ICLR的演讲中所说的那样，已经有了更好的度量标准，如

Fréchet inception distance（FID），在第5章中已经深入介绍过。

图6.5的一个关键结论是，小批处理并不能很好地工作。因为在百

万像素分辨率下，我们没有足够的虚拟内存来将许多图像加载到GPU。

我们必须使用更小的小批处理（总的来说这可能会更差），并且必须

进一步减小小批处理的大小，这会使训练变得困难。

6.4 TensorFlow Hub库及其实践

谷歌最近宣布，作为TensorFlow Extended（TFX）平台的一部

分，以及实现从软件工程到机器学习领域的最优方法，他们创建了一

个名为TensorFlow Hub或TFHub的中心模型和代码库。使用TFHub非常

容易，特别是调用他们已经储存在其中的模型。

导入hub模块并调用正确的URL后，TensorFlow会自行下载并导入

模型，然后就可以开始使用了。这些模型在用于下载模型的同一个URL

中有很好的文档记录，只需将它们输入浏览器中即可。实际上，要获

得经过预训练的PGGAN，输入一个import语句和一行代码即可。

清单6.5给出的是一个完整的代码示例，该代码根据

latent_vector中指定的随机种子生成一张人脸。[8] 输出如图6.6

所示。

清单6.5 使用TFHub

import matplotlib.pyplot as plt

import tensorflow as tf

import tensorflow_hub as hub

with tf.Graph().as_default():

module = hub.Module("https://tfhub.dev/google/progan-128/1")

←--- 从TFHub导入PGGAN

latent_dim = 512 ←---

运行时采样的潜在维度

latent_vector = tf.random_normal([1, latent_dim],

seed=1337)

←--- 改变种子得到不同人脸

interpolated_images = module(latent_vector) ←---

使用该模块

从潜在空间生成图像

with tf.Session() as session:

←--- 运行TensorFlow session

得到(1, 128, 128, 3)的图像

session.run(tf.global_variables_initializer())

image_out = session.run(interpolated_images)

plt.imshow(image_out.reshape(128,128,3))

plt.show()

图6.6 清单6.5的输出。尝试更改latent_vector定义中的种子以获得不同的输

出。警告：即使这个随机种子参数定义输出时是一致的，但我们发现有时在重新

运行时会得到不同的结果，这取决于TensorFlow的版本。该图像是用1.9.0-rc1

获得的

希望这足以让你开始使用PGGAN！你可以随意修改并扩展代码。注

意，PGGAN的TFHub版本未使用完整的1024×1024分辨率，而是

128×128。这可能是因为运行完整版本的计算成本很昂贵，并且在计

算机视觉领域中模型的大小可能会迅速增大。

6.5 PGGAN的实际应用

人们对PGGAN的实际应用和通用性很感兴趣是可以理解的。下面介

绍的一个很好的例子是来自在英国伦敦一家医疗科技初创公司Kheiron

Medical Technologies。最近，他们发表了一篇论文，充分证明了

PGGAN的通用性和实际应用性。[9]

利用庞大的医学乳腺X线数据集[10]，这些研究人员成功生成了

1280×1024逼真的全数字化乳腺X线摄影（Full-Field Digital

Mammography，FFDM），如图6.7所示。这在以下两个层面上都是非凡

的成就。

图6.7 FFDM的渐进式增长。不仅显示了这些乳腺X线照片的分辨率逐渐提高（图

e），还展示了一些训练统计数据（图a～图d），说明训练这些GAN对谁来说都是

一件麻烦事

（1）它显示了该技术的可推广性。考虑一下乳腺X线照片与人脸

图像有何不同，特别是结构上。前者对组织结构是否合理的标准非常

高，然而PGGAN以迄今为止最高的分辨率成功生成了样本，这足以骗过

专业医务人员。

（2）它展示了如何将这些技术应用于多个领域和多种用途。例

如，我们可以以半监督方式使用这个新数据集（第7章），也可以将合

成数据集开源，用于医学研究，而不必担心受通用数据保护条例

（GDPR）或其他法律影响，因为这些X片不属于任何人。

图6.8显示了这些乳腺X线照片的真实感。这些都是随机挑选的

（不是只挑最好的），然后与数据集中与它最接近的图像之一加以比

较。

图6.8 在比较真实数据集和生成数据集时，这些数据看起来非常真实，并且很接

近训练集中的样本。在随后的工作（MammoGAN）中，Kheiron表明，这些图像骗

过了受过训练和认证的放射线医师。[11] 这是一个好兆头，尤其是在高分辨率的

情况下。当然，原则上我们更希望用统计方法来衡量生成样本的质量。正如我们

从第5章了解到的那样，这对于普通图像已经够困难了，更不用说对于任意的GAN

了

GAN不仅可以用于对抗乳腺癌或生成人脸，在其他应用中也有涉

及。截至2018年7月底，已经有62篇GAN的医学应用论文发表。[12] 我

们强烈建议你研读这些论文，当然，并非所有论文使用了PGGAN。一般

来说，GAN在许多研究领域都能实现巨大的飞跃，但它的应用往往不那

么直观。我们希望它们能更容易理解，以便更多的研究人员能使用它

们。

本章介绍的技术都代表了解决GAN问题的一类一般方法 —— 使用

了一个渐进复杂的模型。我们希望这一范例能在GAN中有所应用。对

TensorFlow Hub也是如此，因为它对于TensorFlow就像PyPI/Conda对

于Python一样重要 —— 大多数Python程序员每周都会用到这些软件

库！

我们希望这项新的PGGAN技术能让你了解到GAN可以做什么以及人

们为什么对这篇论文如此感兴趣，也希望PGGAN可以生成的不只是猫咪

表情包。[13] 我们会在第7章给出一些工具，以便你可以开始自己的

研究。

6.6 小结

（1）借助最先进的PGGAN技术，我们可以实现百万像素的合成图

像。

（2）PGGAN技术具有4个关键的训练创新。

渐进式和平滑地增大高分辨率的层。

小批处理标准偏差，以强制所生成的样本多样化。

均衡学习率，确保在每个方向上采取适当大小的学习步骤。

像素特征向量归一化，确保生成器和鉴别器在竞争中不会失控。

（3）实现了一个使用新发布的TensorFlow Hub的实践教程，并使

用PGGAN的低分辨率版本来生成图像！

（4）了解了GAN如何有助于对抗癌症。

[1] Progressive Growing of

GANs for Improved Quality,

Stability, and Variation, by

Tero Karras, 2018.

[2] 有些人可能会反驳说，如果采样的真实数据包含大量非常相似的

图片，也会发生这种情况。从技术上来说，这是正确的，但在实际中

这种情况很容易修复，请记住这里所说的“相似性”可是非常高的，

以至于简单的最近邻聚类的一次传递就可以揭示它。

[3] Progressive Growing of

GANs.md, by Alexander Jung,

2017.

[4] Lecture 5: Training

Neural Networks, Part I,by

Fei-Fei

Li et al. 2016.

[5] Delving Deep into

Rectifiers: Surpassing Human-Level

Performance on ImageNet Classification,

by Kaiming He et

al..

[6] Delving Deep into

Rectifiers: Surpassing Human-Level

Performance on ImageNet Classification,by

Kaiming He et al..

[7] 这里借喻预防式的对抗。——编辑注

[8] 此样本是用TFHub生成的，基于http://mng.bz/nvEa提供的Colab

示例。

[9] High-Resolution Mammogram Synthesis

Using Progressive

Generative Adversarial Networks, by

Dimitrios Korkinof et

al., 2018.

[10] 用于乳腺癌筛查的X射线扫描。

[11] MammoGAN: High-Resolution Synthesis

of Realistic

Mammograms, by Dimitrios Korkinof

et al., 2019.

[12] GANs for Medical

Image Analysis,by Salome Kazeminia

et

al., 2018.

[13] Gene Kogan’s Twitter

image, 2018.

第7章 半监督生成对抗网络（SGAN）

本章主要内容

基于原始GAN模型蓬勃发展的创新领域

半监督学习及其巨大的实践意义

半监督生成对抗网络（SGAN）

SGAN模型的实现

到现在为止，我们不仅了解了GAN定义和功能，还运行了两个最典

型的实现：原始GAN和奠定大多数高级GAN变体（包括第6章介绍的

PGGAN）基础的DCGAN。

但和很多领域一样，当你认为开始真正掌握它的时候，你会发现

这个领域比最初想象的要大得多，也复杂得多，看似很透彻的理解也

不过是冰山一角。

GAN也不例外，自发明以来，它一直是一个活跃的研究领域，每年

都有无数的变化。被很贴切地命名为“The GAN Zoo”的非官方列表，

试图囊括所有已有命名的GAN变体（每种GAN实现有不同的名称，由创

造它们的研究人员命名）——在编写本书时已经增至300余个。原始

GAN论文被引用了9000余次（截至2019年7月），成为近年来深度学习

领域被引用最多的研究论文之一，由此可以推测研究人员发明的GAN变

体的真实数量可能远远不止于此[1]，如图7.1所示。

（来源: The GAN Zoo,

Avinash Hindupur, 2017.）

图7.1 此图大致描述了从2014年GAN的发明到2018年的前几个月，研究人员发布

的GAN数目的逐月累积统计。生成对抗学习领域自诞生以来呈指数级增长，而且这

种兴趣和流行的增长简直看不到尽头

值得注意的是，并非GAN变体都与原始GAN截然不同。它们中的许

多与原始GAN模型在很大程度上非常相似（如第4章中的DCGAN），甚至

许多复杂的创新，如Wasserstein GAN（第5章），也主要关注于如何

提高原始GAN模型或其类似模型的性能和稳定性。

本章和接下来的两章将重点讨论与原始GAN模型非常不同的GAN变

体模型——不同之处不仅表现在模型实现的架构和基础数学描述上，

还表现在动机和目标上。特别要说明的是，我们将介绍3种GAN模型：

SGAN（本章）、CGAN（第8章）和CycleGAN（第9章）。

通过本章给出的概念和具体示例，你会了解到每一个GAN变体的目

标、动机、模型架构以及它们的网络如何训练和工作。这些主题将通

过概念和具体例子进行介绍。我们将提供教程，提供这些模型的完整

工作实现，以便你可以亲自上手体验。

事不宜迟，让我们开始吧！

7.1 SGAN简介

半监督学习（semi-supervised learning）是GAN在实际应用中

最有前途的领域之一。与监督学习（数据集中的每个样本有一个标

签）和无监督学习（不使用任何标签）不同，半监督学习只为训练数

据集的一小部分提供类别标签。通过内化数据中的隐藏结构，半监督

学习努力从标注数据点的小子集中归纳，以有效地对从未见过的新样

本进行分类。要使半监督学习有效，标签数据和无标签数据必须来自

相同的基本分布。

缺少标签数据集是机器学习研究和实际应用中的主要瓶颈之一。

尽管无标签数据非常丰富（互联网实际上就是无标签图像、视频和文

本的无限来源），但为它们分配类别标签通常非常昂贵、不切实际且

耗时。在 ImageNet 中手工标注 320

万张图像用了两年半的时间。

ImageNet是一个标签图像的数据库，在过去的十年中对于图像处理和

计算机视觉取得的许多进步均有帮助。[2]

深度学习先驱、美国斯坦福大学教授、百度前首席科学家Andrew

Ng认为，训练需要大量标签数据是监督学习的致命弱点。目前，工业

中的人工智能应用绝大多数使用监督学习。[3] 缺乏大型标签数据集

的一个领域是医学，医学上获取数据（如来自临床试验的结果）通常

需要耗费大量的精力和开支，更别说会面临道德伦理和隐私等更严重

的问题了。[4] 因此，提高算法从越来越少的标注样本中学习的能力

具有巨大的实际意义。

有趣的是，半监督学习可能也是最接近人类学习方式的机器学习

方式之一。小学生学习阅读和书写时，老师不必带他们出门旅行，让

他们在路上看到成千上万个字母和数字的样本以后，再根据需要纠正

他们——就像监督学习算法的运作方式一样。相反，只需要一组样本

可供孩子学习字母和数字，然后不管何种字体、大小、角度、照明条

件和许多其他条件下，他们能够识别出来。半监督学习旨在按照这种

有效的方式教会机器。

作为可用于训练的附加信息的来源，生成模型已被证明有助于提

高半监督模型的准确性。不出所料，GAN是最有前途的。2016年，Tim

Salimans、Ian Goodfellow和他们在OpenAI的同事仅使用2000个带标

签的样本就在街景房屋号码数据集（Street View House Numbers，

SVHN）上获得了近94%的准确率。[5] 相比之下，当时在SVHN训练集中

对所有73257张图像使用带标签的最佳全监督算法的准确率约为

98.40%。[6] 换句话说，半监督GAN的总体准确率与全监督的基准测试

非常接近，而训练时使用的标签不到3%。

下面我们来看看Salimans和他的同事是如何在如此短的时间内取

得如此大的成就的。

7.1.1 什么是SGAN

半监督生成对抗网络（Semi-Supervised GAN，SGAN）是一种生

成对抗网络，其鉴别器是多分类器。这里的鉴别器不只是区分两个类

（真和假），而是学会区分 类，其中 是训练数据集中的类数，

生成器生成的伪样本增加了一个类。

例如，MNIST手写数字数据集有10个标签（每个数字一个标签，从

0到9），因此在此数据集上训练的SGAN鉴别器将预测10+1=11个类。在

我们的实现中，SGAN鉴别器的输出将表示为10个类别的概率（之和为

1.0）加上另一个表示图像是真还是假的概率的向量。

将鉴别器从二分类器转变为多分类器看似是一个微不足道的变

化，但其含义比乍看之下更为深远。我们从一个图7.2所示的SGAN架构

开始解释。

图7.2 此SGAN中，生成器输入随机噪声向量 并生成伪样本 。鉴别器接收3种

数据输入：来自生成器的伪数据、真实的无标签数据样本 和真实的标签数据样本

，其中 是给定样本对应的标签；然后鉴别器输出分类，以区分伪样本与真实

样本区，并为真实样本确定正确的类别。注意，标签数据比无标签数据少得多。

实际情况中，这一对比甚至比本图所显示的更明显，标签数据仅占训练数据的一

小部分（通常低至1%～2%）

如图7.2所示，与传统GAN相比，区分多个类的任务不仅影响了鉴

别器本身，还增加了SGAN架构、训练过程和训练目标的复杂性。

7.1.2 结构

SGAN生成器的目的与原始GAN相同：接收一个随机数向量并生成伪

样本，力求使伪样本与训练数据集别无二致。

但是，SGAN鉴别器与原始GAN实现有很大不同。它接收3种输入：

生成器生成的伪样本 、训练数据集中无标签的真实样本 和有标

签的真实样本 。其中 表示给定样本 的标签。

SGAN鉴别器的目标不是二分类，而是在输入样本为真的情况下，

将其正确分类到相应的类中，或将样本作为假的（可以认为是特殊的

附加类）排除。

有关这两个SGAN子网络的要点见表7.1。

表7.1 SGAN的生成器网络和鉴别器网络

生成器 鉴别器 生成器 鉴别器

输

入

一个随机数向量

鉴别器接收3种输入；

训练数据集中无标签的真实样

本

训练数据集中有标签的真实样

本

生成器生成的伪样本

输

出

尽可能令人相信的伪样本

表示输入样本属于 个真实类别中的

某一个或属于伪样本类别的可能性

目

标

生成与训练数据集数据别无二致的伪样

本，以欺骗鉴别器，使之将伪样本分到

真实类别

学会将正确的类别标签分配给真实的

样本，同时将来自生成器的所有样本

判别为假

7.1.3 训练过程

回想一下，常规GAN通过计算 )和 的损失并反向传播总损

失来更新鉴别器的可训练参数，以使损失最小，从而训练鉴别器。生

成器通过反向传播鉴别器损失 并寻求使其最大化来进行训练，以

便让鉴别器将合成的伪样本错误地分类为真。

为了训练SGAN，除了 )和 ，我们还必须计算有监督训练样

本的损失： 。这些损失与SGAN鉴别器必须达到的双重目标相对

应：区分真伪样本；学习将真实样本正确分类。用原论文中的术语来

说，双重目标对应于两种损失：有监督损失（supervised loss）和

无监督损失（unsupervised loss）。[7]

7.1.4 训练目标

到目前为止，你看到的GAN变体都是生成模型。它们的目标是生成

逼真的数据样本。正因如此，人们最感兴趣的一直是生成器。鉴别器

网络的主要目的是帮助生成器提高生成图像的质量。在训练结束时，

我们通常会忽略鉴别器，仅使用训练好的生成器来创建逼真的合成数

据。

在SGAN中主要关心的反而是鉴别器。训练过程的目标是使该网络

成为仅使用一小部分标签数据的半监督分类器，其准确率尽可能接近

全监督的分类器（其训练数据集中的每个样本都有标签）。生成器的

目标是通过提供附加信息（它生成的伪数据）来帮助鉴别器学习数据

中的相关模式，从而提高其分类准确率。训练结束时，生成器将被丢

弃，而训练有素的鉴别器将被用作分类器。

至此，我们已经介绍了是什么推动了SGAN的诞生以及它是如何工

作的，接下来通过模型的实现来了解它的实际应用。

7.2 教程：SGAN的实现

本教程实现了一个SGAN模型。该模型仅使用100个训练样本即可对

MNIST数据集中的手写数字进行分类。在教程的最后，我们将模型的分

类准确率与其对应的全监督模型进行了比较，看看半监督学习所取得

的进步。

7.2.1 架构图

本教程中实现的SGAN模型的高级示意如图7.3所示，它比本章开头

介绍的一般概念图要复杂一些。关键在于（实现）细节。

为了解决区分真实标签的多分类问题，鉴别器使用了softmax函

数，该函数给出了在给定数量的类别（本例中为10类）上的概率分

布。给一个给定类别标签分配的概率越高，鉴别器就越确信该样本属

于这一给定的类。为了计算分类误差，我们使用了交叉熵损失，以测

量输出概率与目标独热编码标签之间的差异。

图7.3 本章教程中实现的SGAN的高级示意。生成器将随机噪声转换为伪样本；鉴

别器输入有标签的真实图像 、无标签的真实图像 和生成器生成的伪图像

。为了区分真实样本和伪样本，鉴别器使用了sigmoid函数；为了区分真实标

签的分类，鉴别器使用了softmax函数

为了输出样本是真还是假的概率，鉴别器使用了sigmoid激活函

数，并通过反向传播二元交叉熵损失来训练其参数，这与第3章和第4

章中实现的GAN相同。

7.2.2 实现

你可能会注意到，本书许多SGAN实现都是从第4章的DCGAN模型改

编而来的。这并不是出于懒惰（嗯，也许是有一点……），而是为了

更好地了解SGAN所需的不同修改，且不会干扰到网络无关部分中的实

现细节。

本书Github仓库（https://Github.com/ GAN-in-Action/GAN-in￾action）中的第7章文件夹提供了完整实现的Jupyter Notebook，其中

包括训练进度的可视化等信息。代码是用Python3.6.0、Keras2.1.6和

TensorFlow1.8.0版本测试的。为了加快训练时间，我们建议你在GPU

上运行模型。

7.2.3 设置

首先导入运行模型需要的所有模块和库，如清单7.1所示。

清单7.1 导入声明

%matplotlib inline

import matplotlib.pyplot as plt

import numpy as np

from keras import backend

as K

from keras.datasets import mnist

from keras.layers import (Activation,

BatchNormalization,

Concatenate, Dense,

Dropout, Flatten, Input, Lambda,

Reshape)

from keras.layers.advanced_activations import LeakyReLU

from keras.layers.convolutional import Conv2D,

Conv2DTranspose

from keras.models import Model,

Sequential

from keras.optimizers import Adam

from keras.utils import to_categorical

指定输入图像的大小、噪声向量 的大小以及半监督分类的真实类

别的数量（鉴别器将学习识别每个数字对应的类），如清单7.2所示。

清单7.2 模型输入维度

img_rows = 28

img_cols = 28

channels = 1

img_shape = (img_rows, img_cols,

channels) ←--- 输入图像的维度

z_dim = 100 ←---

噪声向量的大小，用作生成器的输入

num_classes = 10 ←---

数据集中类别的数量

7.2.4 数据集

尽管MNIST训练数据集里有50000个有标签的训练图像，但我们仅

将其中的一小部分（由num_labeled参数决定）用于训练，并假设

其余图像都是无标签的。我们这样来实现这一点：取批量有标签数据

时仅从前num_labeled个图像采样，而在取批量无标签数据时从其

余（50000 – num_labeled）个图像中采样。

Dataset对象（清单7.3）提供了返回所有num_labeled训练样

本及其标签的函数，以及能返回MNIST数据集中所有10000个带标签的

测试图像的函数。训练后，我们将使用测试集来评估模型的分类在多

大程度上可以推广到以前未见过的样本。

清单7.3 用于训练和测试的数据集

class Dataset:

def __init__(self, num_labeled):

self.num_labeled = num_labeled ←---

训练中使用的有标签

图像的数量

(self.x_train, self.y_train), (self.x_test, ←---

加

载MINST数据集

self.y_test) =

mnist.load_data()

def preprocess_imgs(x):

x = (x.astype(np.float32) -

127.5) / 127.5 ←---

灰度像素值从[0, 255]缩放到[–1, 1]

x = np.expand_dims(x, axis=3)

←--- 将图像尺寸扩

展到宽×高×通道数

return x

def preprocess_labels(y):

return y.reshape(-1, 1)

self.x_train = preprocess_imgs(self.x_train) ←---

训练

self.y_train = preprocess_labels(self.y_train)

self.x_test = preprocess_imgs(self.x_test) ←---

测试

self.y_test = preprocess_labels(self.y_test)

def batch_labeled(self, batch_size):

idx = np.random.randint(0, self.num_labeled,

batch_size) ←--- 获取随机批量的有标签图像及其标签

imgs = self.x_train[idx]

labels = self.y_train[idx]

return imgs, labels

def batch_unlabeled(self, batch_size):

idx = np.random.randint(self.num_labeled,

self.x_train.shape[0], ←--- 获取随机批量的无标签图像

batch_size)

imgs = self.x_train[idx]

return imgs

def training_set(self):

x_train = self.x_train[range(self.num_labeled)]

y_train = self.y_train[range(self.num_labeled)]

return x_train, y_train

def test_set(self):

return self.x_test, self.y_test

在本教程中，我们假设只有100个有标签的MNIST图像用于训练：

num_labeled = 100 ←---

要使用的有标签样本的数量（其余将作为无标签样本

使用）

dataset = Dataset(num_labeled)

7.2.5 生成器

SGAN的生成器网络与第4章中DCGAN的相同，如清单7.4所示。生成

器使用转置卷积层将输入的随机噪声向量转换为28×28×1图像。

清单7.4 SGAN生成器

def build_generator(z_dim):

model = Sequential()

model.add(Dense(256 * 7 *

7, input_dim=z_dim)) ←--- 通过一

个全连接层改变输入为一个7×7×256的张量

model.add(Reshape((7, 7, 256)))

model.add(Conv2DTranspose(128, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层，张量从7×7×256变为14×14×128

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活

model.add(Conv2DTranspose(64, kernel_size=3, strides=1,

padding='same')) ←--- 转置卷积层，张量从14×14×128变为14×14×64

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活

model.add(Conv2DTranspose(1, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层，张量从14×14×64变为28×28×1

model.add(Activation('tanh')) ←--- 带tanh激活函数的输出层

return model

7.2.6 鉴别器

鉴别器是SGAN模型中最复杂的部分，它有如下双重目标。

（1）区分真实样本和伪样本。为此，SGAN鉴别器使用了sigmoid

函数，输出用于二元分类的概率。

（2）对于真实样本，还要对其标签准确分类。为此，SGAN鉴别器

使用了softmax函数，输出概率向量——每个目标类别对应一个。

1．核心鉴别器网络

我们先来定义核心鉴别器网络。清单7.5中的模型与第4章中实现

的基于ConvNet的鉴别器相似。实际上，直到3×3×128卷积层，它的

批归一化和LeakyReLU激活与之前的一直是完全相同的。

在该层之后添加了一个Dropout，这是一种正则化技术，通过在

训练过程中随机丢弃神经元及其与网络的连接来防止过拟合。[8] 这

就迫使剩余的神经元减少它们之间的相互依赖，并得到对基础数据更

一般的表示形式。随机丢弃的神经元比例由比例参数指定，在本实现

中将其设置为0.5，即model.add(Dropout(0.5))。由于SGAN分类

任务的复杂性增加，我们使用了Dropout，以提高模型从只有100个有

标签的样本中归纳的能力。

清单7.5 SGAN鉴别器

def build_discriminator_net(img_shape):

model = Sequential()

model.add( ←--- 卷积层，张量从14×14×64变为14×14×32

Conv2D(32,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，张量从14×14×32变为7×7×64

Conv2D(64,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，张量从7×7×64变为3×3×128

Conv2D(128,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Dropout(0.5)) ←--- Dropout

model.add(Flatten()) ←--- 将张量展平

model.add(Dense(num_classes)) ←--- 与num_classes神经元完全

连接的层

return model

注意，Dropout层是在批归一化之后添加的。出于这两种技术之间

的相互作用，这种方法已显示出优越的性能。[9]

另外，请注意前面的网络以一个具有10个神经元的全连接层结

束。接下来，我们需要定义从这些神经元计算出的两个鉴别器输出：

一个用于有监督的多分类（使用softmax），另一个用于无监督的二

分类（使用sigmoid）。

2．有监督的鉴别器

清单 7.6 的代码采用了之前实现的核心鉴别器网络，并用于构建

鉴别器模型的有监督部分。

清单7.6 SGAN有监督的鉴别器

def build_discriminator_supervised(discriminator_net):

model = Sequential()

model.add(discriminator_net)

model.add(Activation('softmax')) ←--- softmax激活函数输出

真实类别的预测概率分布

return model

3．无监督的鉴别器

清单7.7在核心鉴别器网络的顶上实现了模型的无监督部分。注

意，predict(x)这个函数将10个神经元（来自核心鉴别器网络）的

输出转换成一个二分类的真假预测。

清单7.7 SGAN鉴别器无监督部分

def build_discriminator_unsupervised(discriminator_net):

model = Sequential()

model.add(discriminator_net)

def predict(x):

prediction = 1.0 -

(1.0 / ←--- 将真实类别的分布转换为二

元真-假概率

(K.sum(K.exp(x), axis=-1,

keepdims=True) + 1.0))

return prediction

model.add(Lambda(predict)) ←--- 之前定义的真-假输出神经元

return model

7.2.7 搭建整个模型

接下来，我们将构建并编译鉴别器模型和生成器模型（清单

7.8）。注意，有监督损失和无监督损失分别使用

categorical_crossentropy和binary_crossentropy损失函

数。

清单7.8 构建模型

def build_gan(generator, discriminator):

model = Sequential()

model.add(generator) ←--- 合并生成器模型和鉴别器模型

model.add(discriminator)

return model

discriminator_net = build_discriminator_net(img_shape) ←---

核心鉴别器网络：这些层在有监督和无监督训练中共享

discriminator_supervised =

build_discriminator_supervised(discriminator_net) ←---

discriminator_supervised.compile(loss='categorical_crossentropy',

metrics=['accuracy'],

optimizer=Adam()) ←---

构建并编译有监督训练鉴别器

discriminator_unsupervised = build_discriminator_unsupervised(

←---

discriminator_net)

discriminator_unsupervised.compile(loss='binary_crossentropy',

optimizer=Adam()) ←--

- 构建并编译无监督训练鉴别器

generator = build_generator(z_dim) ←---

构建生成器

discriminator_unsupervised.trainable = False ←---

生成器训练时，

鉴别器参数保持不变

gan = build_gan(generator, discriminator_unsupervised)

←---

gan.compile(loss='binary_crossentropy', optimizer=Adam()) ←---

构建并编译鉴别器固定的GAN模型，以训练生成器。注意：鉴别器使用无监督版本

7.2.8 训练

以下伪代码概述了SGAN的训练算法。

SGAN训练算法

对每次训练迭代，执行以下操作。

（1）训练鉴别器（有监督）。

a．随机取小批量有标签的真实样本 。

b．计算给定小批量的 并反向传播多分类损失更

新 ，以使损失最小化。

（2）训练鉴别器（无监督）。

a．随机取小批量无标签的真实样本 。

b．计算给定小批量的 )并反向传播二元分类损失更

新 ，以使损失最小化。

c．随机取小批量的随机噪声 生成一小批量伪样本：

。

d．计算给定小批量的 并反向传播二元分类损失更

新 以使损失最小化。

（3）训练生成器。

a．随机取小批量的随机噪声 生成一小批量伪样本：

。

b．计算给定小批量的 并反向传播二元分类损失更

新 以使损失最大化。

结束

清单7.9实现了SGAN的训练算法。

清单7.9 SGAN的训练算法

supervised_losses = []

iteration_checkpoints = []

def train(iterations, batch_size, sample_interval):

real = np.ones((batch_size, 1))

←--- 真实图像的标签：全为1

fake = np.zeros((batch_size, 1))

←--- 伪图像的标签：全为0

for iteration in range(iterations):

imgs, labels = dataset.batch_labeled(batch_size)

←--

- 获取有标签样本

labels = to_categorical(labels, num_classes=num_classes)

←--- 独热编码标签

imgs_unlabeled = dataset.batch_unlabeled(batch_size)

←--- 获取无标签样本

z = np.random.normal(0, 1,

(batch_size, z_dim)) ←---

生成一批伪图像

gen_imgs = generator.predict(z)

d_loss_supervised,

accuracy =

discriminator_supervised.train_on_batch(imgs, labels) ←--- 训

练有标签的真实样本

d_loss_real = discriminator_unsupervised.train_on_batch(

←--- 训练无标签的真实样本

imgs_unlabeled, real)

d_loss_fake =

discriminator_unsupervised.train_on_batch(gen_imgs, fake) ←---

训练伪样本

d_loss_unsupervised = 0.5 *

np.add(d_loss_real,

d_loss_fake)

z = np.random.normal(0, 1,

(batch_size, z_dim)) ←---

生成一批伪样本

gen_imgs = generator.predict(z)

g_loss = gan.train_on_batch(z, np.ones((batch_size,

1)))

←--- 训练生成器

if (iteration + 1)

% sample_interval == 0:

supervised_losses.append(d_loss_supervised) ←--

-

iteration_checkpoints.append(iteration + 1) ←--

- 保存鉴别器的有监督分类损失，以便绘制损失曲线

print( ←--- 输出训练过程

"%d [D loss supervised:

%.4f, acc.: %.2f%%] [D

loss" +

" unsupervised: %.4f] [G

loss: %f]"

% (iteration + 1,

d_loss_supervised, 100 *

accuracy,

(d_loss_unsupervised, g_loss))

1．训练模型

之所以使用较小的批量，是因为只有100个有标签的训练样本。我

们通过反复试验确定迭代次数：不断增加次数，直到鉴别器的有监督

损失趋于平稳，但不要超过稳定点太远（以降低过拟合的风险）。训

练模型的代码如清单7.10所示。

清单7.10 训练模型

iterations = 8000 ←---

设置超参数

batch_size = 32

sample_interval = 800

train(iterations, batch_size, sample_interval) ←---

按照指定的

迭代次数训练SGAN

2．模型训练和测试准确率

现在，让我们看看SGAN作为分类器的表现吧！在训练过程中，

SGAN达到了100%的有监督准确率。尽管这看似很好，但请记住只有100

个有标签的样本用于有监督训练——也许模型只是记住了训练数据

集。分类器能在多大程度上泛化到训练集中未见过的数据上才是重要

的，如清单7.11所示。

清单7.11 测试准确率

x, y = dataset.test_set()

y = to_categorical(y, num_classes=num_classes)

_, accuracy = discriminator_supervised.evaluate(x,

y) ←--- 在

测试集上计算分类准确率

print("Test Accuracy: %.2f%%" %

(100 * accuracy))

请尽情欢呼吧！SGAN能够准确分类测试集中大约89%的样本。为了

解这有多了不起，我们对比一下SGAN和全监督分类器的性能。

7.3 与全监督分类器的对比

为了使比较尽可能公平，我们让全监督分类器使用与训练有监督

鉴别器相同的网络结构，如清单7.12所示。这样做的意图在于，这将

能突显出半监督学习GAN对分类器泛化能力的提高。

清单7.12 全监督分类器

mnist_classifier = build_discriminator_supervised(

build_discriminator_net(img_shape))

←--- 有着与SGAN鉴别器相同网络结构的全监督分类器

mnist_classifier.compile(loss='categorical_crossentropy',

metrics=['accuracy'],

optimizer=Adam())

我们用和训练SGAN相同的100个样本来训练全监督分类器。为简洁

起见，此处不显示训练代码和输出训练和测试准确率的代码。你可以

在Github仓库中第7章文件夹下的SGAN Jupyter Notebook中找到相关

代码。

与SGAN的鉴别器一样，全监督分类器在训练数据集上达到了100%

的准确率。但是在测试集上它只能正确分类大约70%的样本，比SGAN差

了约20个百分点。换句话说，SGAN将训练准确率提高了近30个百分

点！

随着训练数据的增加，全监督分类器的泛化能力显著提高。使用

相同的设置和训练，使用10000个有标签样本（是最初使用样本的100

倍）训练的全监督分类器，可以达到约98%的准确率，不过这不是半

监督学习。

7.4 结论

在本章中，我们通过教鉴别器输出真实样本的类别标签，来探索

如何把GAN用于半监督学习。你可以看到，经过SGAN训练的分类器从少

量训练样本中泛化的能力明显优于全监督分类器。

从GAN创新的角度来看，SGAN的主要特点是在鉴别器训练中使用标

签。你可能想知道标签是否也可以用于生成器训练，那就应该问问第

8 章中的 GAN 变体（条件GAN）了。

7.5 小结

（1）半监督生成对抗网络（SGAN）的鉴别器可用来：区分真实样

本与伪样本；给真实样本分配正确的类别标签。

（2）SGAN 的目的是将鉴别器训练成一个分类器，使之可以从尽

可能少的有标签样本中获得更高的分类精度，从而减少分类任务对大

量标注数据的依赖性。

（3）在本章的实现中，我们将 softmax 和多元交叉熵损失用于

分配真实标签的有监督任务，将sigmoid和二元交叉熵用于区分真实

样本和伪样本。

（4）我们证明了SGAN对没见过的测试集数据的分类准确率远远优

于在相同数量的有标签样本上训练的全监督分类器。

[1] 引自微软学术（MA）搜索引擎的一份跟踪报告，另见 Top 20

Research Papers on Machine

Learning and Deep Learning,

by

Thuy T. Pham, 2017.

[2] The Data That

Transformed AI Research—and Possibly

the

World, by Dave Gershgorn,

2017.

[3] What Artificial Intelligence

Can and Can’t Do

Right

Now, by Andrew Ng,

2016.

[4] What AI Can

and Can’t Do (Yet)

for Your Business, by

Michael Chui et al.,

2018.

[5] Improved Techniques for

Training GANs, by Ian

Goodfellow

et al., 2016.

[6] Densely Connected Convolutional

Networks, by Gao Huang

et al., 2016.

[7] Improved Techniques for

Training GANs, by Tim

Salimans

et al., 2016.

[8] Improving Neural Networks

by Preventing Co-Adaptation of

Feature Detectors,by Geoffrey E.

Hinton et al., 2012.

另见

Dropout: A Simple Way

to Prevent Neural Networks

from

Overfitting,by Nitish Srivastava et

al., 2014, Journal of

Machine Learning Research 15,

1929–1958.

[9] Understanding the Disharmony

between Dropout and Batch

Normalization by Variance Shift,

by Xiang Li et

al., 2018.

第8章 条件生成对抗网络（CGAN）

本章主要内容

使用标签训练生成器和鉴别器

教GAN生成与指定标签匹配的样本

训练条件生成对抗网络（CGAN）生成手写数字

我们在第7章介绍了SGAN，由此引入了在GAN训练中使用标签的想

法。GAN使用标签将鉴别器训练为功能强大的半监督分类器。本章将介

绍条件生成对抗网络（Conditional GAN，CGAN）——它使用标签来训

练生成器和鉴别器。正是由于这项创新，CGAN才能够引导生成器合成

我们想要的伪样本。

8.1 动机

无论是简单的手写数字还是逼真的人脸，GAN都可以生成其样本图

像。虽然可以通过选择训练数据集来控制GAN模拟的样本的域，但我们

不能指定所GAN生成样本的任何特征。比如，在第4章中实现的DCGAN可

以合成逼真的手写数字，但我们无法控制它是否会在任何给定时间生

成数字7而不是9。

对于MNIST这样的简单数据集，样本只可能属于10个类中的一个，

这种担心可能影响甚微。如果目标是生成数字9，则可以继续生成样本

直至得到想要的数字。但在更复杂的数据生成任务上，可能的答案范

围太大，这种蛮力解决的方案不太实用。以生成人脸的任务为例，尽

管第6章中的PGGAN生成的图像令人印象深刻，但无法控制要生成什么

样的人脸——无法指导生成器合成男性面孔还是女性面孔，更不用说

年龄或面部表情等其他特征了。

决定将生成何种数据的能力为众多应用打开了大门。举个有点刻

意的例子，假设有一个侦探正在破解一桩神秘谋杀案，根据目击者描

述凶手是一个有着红色长发和绿色眼睛的中年妇女。如果不雇用素描

画家（一次只能画一幅素描），而是将描述性特征输入计算机程序并

使之输出一系列符合条件的人脸图像，证人可以指出里面最像罪犯的

那一个，这样将大大加快破案过程。

你可能会想到许多其他的实际应用，对于这些应用，能生成符合

选择标准的图像的能力将颠覆行业规则：在医学研究中，可以指导新

药的开发；在电影制作和计算机图像合成（CGI）中，可以用最少的人

工动画输入来创建想要的精确场景……这样的例子不胜枚举。

CGAN是最早使目标数据生成成为可能的GAN创新之一，可以说是最

具影响力的一种。接下来，我们将介绍CGAN的工作方式以及如何用

MNIST数据集实现它的小规模版本。

8.2 什么是CGAN

CGAN是由加拿大蒙特利尔大学的博士生Mehdi Mirza和Flickr的AI

架构师Simon Osindero于2014年推出的一个生成对抗网络，其生成器

和鉴别器在训练过程中受到一些附加信息的制约。[1] 从理论上讲，

这里所谓的“附加信息”可以是任何东西，例如一个类标签、一组标

签甚至是书面描述。为简单起见，我们在解释CGAN的工作原理时将标

签用作条件信息。

在CGAN训练期间，生成器学习为训练数据集中的每个标签生成逼

真的样本，而鉴别器则学习区分真的样本-标签对与假的样本-标签

对。半监督GAN的鉴别器除了区分真实样本与伪样本，还为每个真实样

本分配正确的标签；而CGAN中的鉴别器不会学习识别哪个样本是哪个

类。它只学习接受真实的且样本-标签匹配正确的对，拒绝不匹配的对

和样本为假的对。

例如，无论样本（手写数字3）是真还是假，CGAN鉴别器都应拒绝

该样本-标签对( ，4)，因为样本与标签4不匹配。CGAN鉴别器还应该

拒绝所有图像为假的样本-标签对，即使标签与图像匹配。

因此，为了欺骗鉴别器，CGAN生成器仅生成逼真的数据是不够

的，生成的样本还需要与标签相匹配。在对生成器进行充分训练之

后，就可以通过传递所需的标签来指定希望CGAN合成的样本。

8.2.1 CGAN的生成器

为规范起见，我们把条件标签称为 ，生成器使用噪声向量 和标

签 合成一个伪样本 （读作“给定 或以

为条件时的

”）。这个伪样本的目的是（在鉴别器的眼中）看起来尽可能接近给

定标签的真实样本。CGAN的生成器的架构如图8.1所示。

图8.1 CGAN生成器： 。生成器输入随机噪声向量 和标签

，生

成一个与标签匹配的逼真伪样本

8.2.2 CGAN的鉴别器

鉴别器接收带标签的真实样本 ，以及带有（用于生成自己

的）标签的伪样本 。在真实样本-标签对上，鉴别器学习如何

识别真实数据以及如何识别匹配对。在生成器生成的样本中，鉴别器

学习识别伪样本-标签对，以将它们与真实样本-标签对区分开来。

鉴别器输出表明输入是真实的匹配对的概率。它的目标是学会接

收所有真实样本-标签对，并拒绝所有伪样本和所有与标签不匹配的样

本，如图8.2所示。

图8.2 CGAN的鉴别器输入真实样本和它们的标签 ，以及伪样本和用于合成

它们的标签 。鉴别器输出一个表示输入真伪的概率（σ 由sigmoid激活

函数计算）

8.2.3 汇总表

表8.1总结了CGAN的两个子网络的输入、输出和目标。

表8.1 CGAN的生成器和鉴别器网络

生成器 鉴别器

输

入

一个随机数向量和一个标签：

鉴别器的输入：

训练数据集中带标签的真实样本

为与给定标签匹配，生成器生成的带标

签的伪样本

输

出

力求与标签匹配的伪样本： 表示输入样本是否为匹配的真实样本-标签对

的概率

目

标

生成与标签匹配的逼真的伪样

本

区分来自生成器的伪样本-标签对和来自训练

数据集的真实样本-标签对

8.2.4 架构图

CGAN的高层次架构图示意如图8.3所示。注意，对于每个伪样本，

相同的标签 同时被传递给生成器和鉴别器。另外，通过在带有不匹配

标签的真实样本上训练鉴别器来拒绝不匹配的对；它识别不匹配对的

能力是被训练成只接收真实匹配对时的副产品。

图8.3 CGAN生成器使用随机噪声向量 和标签 （

个可能的标签之一）作为输

入，并力图生成既逼真又能匹配标签 的伪样本

注意

读者可能已经注意到了，本书对于每种GAN变体，几乎都会提供一

个汇总了鉴别器网络和生成器网络的输入、输出和目标的表格，以及

网络架构图。这不是偶然的，实际上，这些章节的主要目标之一是为

读者提供一种思考的模板——或者说可重用的框架。当遇到与原始GAN

不同的GAN实现时，读者可以按框架来探索。最好的第一步通常是分析

生成器和鉴别器网络以及整个模型架构。

CGAN鉴别器接收生成器生成的假的带标签的样本 和真的

带标签的样本 ，并学会分辨给定样本-标签对的真假。

理论足够了，现在是时候将所学的知识付诸实践，实现自己的

CGAN模型了！

8.3 教程：CGAN的实现

我们将实现一个CGAN模型，使之学习生成想要的手写数字。在教

程最后，我们将为每类数字生成图像样本，以了解模型学习生成目标

数据的能力。

8.3.1 实现

本实现受到Keras-GAN模型的开源Github仓库中CGAN的启发（和在

第3和第4章中使用的相同）。[2] 特别要说明，我们用了开源模型中

利用Embedding层将样本和标签合并到联合隐藏表示中的方法（稍后

会对此进行更多介绍）。

但是CGAN模型的其他部分与Keras-GAN仓库中的有所不同。我们对

嵌入实现进行了重构，使其更具可读性，还添加了详细的解释性注

释。还有一个关键点是在CGAN中用了卷积神经网络，以生成更为逼真

的样本——回顾第3章中的GAN和第4章中的DCGAN生成的图像之间的区

别！

配套资料第8章文件夹下的Jupyter Notebook有完整的代码实现，

包括训练进度等可视化信息。代码是用Python3.6.0、Keras2.1.6和

TensorFlow1.8.0版本测试的。要缩短训练时间，读者应在GPU上运行

模型。

8.3.2 设置

首先导入模型需要的所有模块和库，如清单8.1所示。

清单8.1 导入声明

%matplotlib inline

import matplotlib.pyplot as plt

import numpy as np

from keras.datasets import mnist

from keras.layers import (

Activation, BatchNormalization, Concatenate, Dense,

Embedding, Flatten, Input, Multiply,

Reshape)

from keras.layers.advanced_activations import LeakyReLU

from keras.layers.convolutional import Conv2D,

Conv2DTranspose

from keras.models import Model,

Sequential

from keras.optimizers import Adam

指定输入图像的大小，噪声向量 的大小以及数据集中类的数量，

如清单8.2所示。

清单8.2 模型的输入维度

img_rows = 28

img_cols = 28

channels = 1

img_shape = (img_rows, img_cols,

channels) ←--- 输入图像维度

z_dim = 100 ←---

噪声向量的大小，用作生成器的输入

num_classes = 10 ←---

数据集中的类别数

8.3.3 CGAN的生成器

我们在本节实现CGAN的生成器。读者应该已经从第4章和第7章熟

悉了关于生成器的大部分内容。对于CGAN的修改围绕输入处理进行，

这里我们使用嵌入和逐元素乘法（element-wise multiplication）将

随机噪声向量 和标签 组合成一个联合表示，具体如下。

（1）使用Keras的Embedding层将标签 （0到9的整数）转换为

大小为z_dim（随机噪声向量的长度）的稠密向量。

（2）使用Keras的Multiply层将标签与噪声向量 嵌入联合表示

中。顾名思义，该层将两个等长向量的对应项相乘，并输出作为结果

乘积的单个向量。

（3）将得到的向量作为输入，保留CGAN生成器网络的其余部分以

合成图像。

图8.4以标签7为例说明了这个过程。

图8.4 将条件标签（在此例中为7）和随机噪声向量 合并为一个联合表示的过程

首先，将标签嵌入与 大小相同的向量中；其次，将嵌入标签和

的对应元素相乘（符号 表示按元素相乘）；最后，将所得的联合表

示用作CGAN生成器网络的输入。

清单8.3展示了Python / Keras代码中的所有内容。

清单8.3 CGAN的生成器

def build_generator(z_dim):

model = Sequential()

model.add(Dense(256 * 7 *

7, input_dim=z_dim)) ←--- 通过

全连接层将输入变为7×7×256的张量

model.add(Reshape((7, 7, 256)))

model.add(Conv2DTranspose(128, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层，张量从7×7×256变为14×14×128

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活

model.add(Conv2DTranspose(64, kernel_size=3, strides=1,

padding='same')) ←--- 转置卷积层，张量从14×14×128变为14×14×64

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活

model.add(Conv2DTranspose(1, kernel_size=3, strides=2,

padding='same')) ←--- 转置卷积层，张量从14×14×64变为28×28×1

model.add(Activation('tanh')) ←--- 带tanh激活的输出层

return model

def build_cgan_generator(z_dim):

z = Input(shape=(z_dim, ))

←--- 随机噪声z

label = Input(shape=(1, ),

dtype='int32') ←--- 条件标签：G

应该生成的指定数字，整数0～9

label_embedding = Embedding(num_classes, z_dim,

input_length=1)(label) ←--- 标签嵌入：将标签转化为大小为z_dim的稠密

向量；生成形状为(z_dim, batch_size, 1)的三维张量

label_embedding = Flatten()(label_embedding) ←---

将嵌入

的三维张量展平成形状为(batch_size，z_dim)的二维张量

joined_representation = Multiply()([z, label_embedding])

←--- 向量z和嵌入标签的元素级乘积

generator = build_generator(z_dim)

conditioned_img = generator(joined_representation) ←---

为给定的标签生成图像

return Model([z, label], conditioned_img)

8.3.4 CGAN的鉴别器

接下来，我们实现CGAN的鉴别器。就像在8.3.3节中所做的一样，

除了要处理输入图像及其标签的那部分内容，网络架构应该看起来很

熟悉。这里也使用Keras的Embedding层将输入标签转换为稠密向

量。但与生成器输入是平面向量不同，鉴别器输入是三维图像。这需

要用到特定的处理方法，步骤如下。

（1）取一个标签（0到9的整数），使用Keras的Embedding层将

标签变成大小为28 × 28 ×

1 = 784（扁平化图像的长度）的稠密向

量。

（2）将嵌入标签调整为图像尺寸（28 × 28 ×

1）。

（3）将重塑后的嵌入标签连接到对应图像上，生成形状（28 ×

28 × 2）的联合表示。可以将其视为在顶部“贴有”嵌入标签的图

像。

（4）将图像-标签的联合表示输入CGAN的鉴别器网络中。注意，

为了训练正常进行，必须将模型输入尺寸调整为（28 × 28 ×

2）来

对应新的输入形状。

以标签7为例展示该过程，如图8.5所示。

图8.5 将标签（此例中为7）和输入图像组合成一个联合表示的过程

综上所述，首先，将标签嵌入具有扁平化图像大小的向量中（28

× 28 × 1

= 784）；其次，将嵌入标签调整为与输入图像形状相同

的张量（28 × 28 ×

1）；再次，将重塑后的嵌入标签和对应的图像

连接到一起。最后，将这个联合表示输入CGAN的鉴别器网络中。

除了预处理，与第4章相比，我们还需要对鉴别器网络进行一些其

他的调整。（与第7章一样，基于DCGAN的实现会更容易看到变为CGAN

模型要特定更改的部分，而不会受不相关部分实现细节的干扰。）首

先，将模式输入尺寸调整为（28 × 28 ×

2），以对应新的输入形

状；其次，将第一个卷积层的深度从32增加到64，之所以发生这一变

化，是因为嵌入了联合标签，有更多的信息要编码。这种网络架构在

实验中确实产生了更好的结果。

输出层使用sigmoid激活函数来得到输入图像-标签对是真的而不

是假的概率（此处没有变化）。清单8.4实现了CGAN的鉴别器。

清单8.4 CGAN的鉴别器

def build_discriminator(img_shape):

model = Sequential()

model.add( ←--- 卷积层，从28×28×2变为14×14×64的张量

Conv2D(64,

kernel_size=3,

strides=2,

input_shape=(img_shape[0], img_shape[1],

img_shape[2] + 1),

padding='same'))

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，从14×14×64变为7×7×64的张量

Conv2D(64,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add( ←--- 卷积层，从7×7×64变为3×3×128的张量

Conv2D(128,

kernel_size=3,

strides=2,

input_shape=img_shape,

padding='same'))

model.add(BatchNormalization()) ←--- 批归一化

model.add(LeakyReLU(alpha=0.01)) ←--- LeakyReLU激活函数

model.add(Flatten()) ←--- 带sigmoid激活函数的输出层

model.add(Dense(1, activation='sigmoid'))

return model

def build_cgan_discriminator(img_shape):

img = Input(shape=img_shape) ←---

输入图像

label = Input(shape=(1, ),

dtype='int32') ←--- 为输入图像

加标签

label_embedding = Embedding(num_classes, ←---

标签嵌入：将

标签转化为大小为z_dim的稠密向量;生成形状为(batch_size, 1, 28×28×1)的三维

张量

np.prod(img_shape),

input_length=1)(label)

label_embedding = Flatten()(label_embedding) ←---

将嵌入

的三维张量展平成形状为(batch_size, 28×28×1)的二维张量

label_embedding = Reshape(img_shape)(label_embedding) ←--

- 将嵌入标签调整为和输入图像一样的维度

concatenated = Concatenate(axis=-1)([img, label_embedding])

←--- 将图像与其嵌入标签连接

discriminator = build_discriminator(img_shape)

classification = discriminator(concatenated) ←---

对图像-

标签对进行分类

return Model([img, label], classification)

8.3.5 搭建整个模型

接下来，我们构建并编译CGAN的鉴别器和生成器模型，如清单8.5

所示。注意，在用于训练生成器的组合模型中，相同的输入标签将同

时传递到生成器（生成样本）和鉴别器（进行预测）。

清单8.5 构建并编译CGAN的鉴别器和生成器模型

def build_cgan(generator, discriminator):

z = Input(shape=(z_dim, ))

←--- 随机噪声向量z

label = Input(shape=(1, ))

←--- 图像标签

img = generator([z, label])

←--- 为指定标签生成图像

classification = discriminator([img, label])

model = Model([z, label],

classification) ←--- 结合生成器

- >鉴别器模型G([z, label]) =

x*D(x*) = 类别

return model

discriminator = build_cgan_discriminator(img_shape) ←---

构建

并编译鉴别器

discriminator.compile(loss='binary_crossentropy',

optimizer=Adam(),

metrics=['accuracy'])

generator = build_cgan_generator(z_dim) ←---

构建生成器

discriminator.trainable = False ←---

生成器训练时鉴别器参数保持不变

cgan = build_cgan(generator, discriminator)

←--- 构建并编译鉴别

器固定的CGAN模型来训练生成器（鉴别器权重固定）

cgan.compile(loss='binary_crossentropy', optimizer=Adam())

8.3.6 训练

对于CGAN训练算法，每次迭代的细节如下。

CGAN训练算法

对每次训练迭代，执行如下操作。

（1）训练鉴别器。

a．随机取小批量有标签的真实样本及其标签 。

b．计算给定小批量的 并反向传播二分类损失更

新 ，以使损失最小化。

c．随机取小批量的随机噪声向量和类别标签 并生成

小批量伪样本： 。

d．计算小批量的 并反向传播二分类损失更新

，以使损失最小化。

（2）训练生成器。

a．随机取小批量的随机噪声和类别标签 生成小批量

伪样本： 。

b．计算给定小批量的 并反向传播二分类损失

更新 ，以使损失最大化。

结束

清单8.6实现了CGAN的训练算法。

清单8.6 CGAN的训练算法

accuracies = []

losses = []

def train(iterations, batch_size, sample_interval):

(X_train, y_train), (_, _)

= mnist.load_data() ←--- 加载

MINST数据集

X_train = X_train /

127.5 - 1. ←---

灰度像素值从[0, 255]缩

放到[1, 1]

X_train = np.expand_dims(X_train, axis=3)

real = np.ones((batch_size, 1))

←--- 真实图像的标签：全为1

fake = np.zeros((batch_size, 1))

←--- 伪图像的标签：全为0

for iteration in range(iterations):

idx = np.random.randint(0, X_train.shape[0],

batch_size)

←--- 生成一批量伪样本及其标签

imgs, labels = X_train[idx],

y_train[idx]

z = np.random.normal(0, 1,

(batch_size, z_dim)) ←---

生成一批伪图像

gen_imgs = generator.predict([z, labels])

d_loss_real = discriminator.train_on_batch([imgs,

labels], real) ←--- 训练鉴别器

d_loss_fake = discriminator.train_on_batch([gen_imgs,

labels], fake)

d_loss = 0.5 *

np.add(d_loss_real, d_loss_fake)

z = np.random.normal(0, 1,

(batch_size, z_dim)) ←---

生成一批噪声向量

labels = np.random.randint(0, num_classes,

batch_size).reshape(-1, 1) ←--- 得到一批随机标签

g_loss = cgan.train_on_batch([z, labels],

real) ←---

训练生成器

if (iteration + 1)

% sample_interval == 0:

print("%d [D loss: %f,

acc.: %.2f%%] [G loss:

%f]"

% ←--- 输出训练过程

(iteration + 1, d_loss[0],

100 * d_loss[1],

g_loss))

losses.append((d_loss[0], g_loss)) ←--- 保存鉴

别器的有监督分类损失以便绘制损失曲线

accuracies.append(100 * d_loss[1])

sample_images() ←--- 输出生成图像样本

8.3.7 输出样本图像

你可能觉得在第3章和第4章中见过下一个函数。它被用来检查生

成器生成的图像的质量是如何随着训练的进行而提高的。清单8.7中的

函数确实与此相似，但存在一些重要差异。

首先，它生成的不是随机手写数字组成的4×4的网格，而是2×5

的数字网格，第一行是1到5，第二行是6到9。这可以用来检查CGAN在

学习生成特定数字方面的表现。其次，使用set_title()方法显示

每个样本的标签。

清单8.7 显示生成的图像

def sample_images(image_grid_rows=2, image_grid_columns=5):

z = np.random.normal(0, 1,

(image_grid_rows *

image_grid_columns, z_dim)) ←--- 随机噪声采样

labels = np.arange(0, 10).reshape(-1,

1) ←--- 得到图像标签

0-9

gen_imgs = generator.predict([z, labels])

←--- 从随机噪声

生成图像

gen_imgs = 0.5 *

gen_imgs + 0.5 ←---

图像像素值缩放到[0, 1]

fig, axs = plt.subplots(image_grid_rows,

←--- 设置图像网格

image_grid_columns,

figsize=(10, 4),

sharey=True,

sharex=True)

cnt = 0

for i in range(image_grid_rows):

for j in range(image_grid_columns):

axs[i, j].imshow(gen_imgs[cnt, :, :,

0],

cmap='gray') ←--- 输出图像网格

axs[i, j].axis('off')

axs[i, j].set_title("Digit: %d" %

labels[cnt])

cnt += 1

图8.6显示了此函数的输出示例，说明了在训练过程中CGAN生成的

数字的改进。

图8.6 从随机噪声开始，GCAN学习为训练数据集中的每个标签生成逼真的数字

8.3.8 训练模型

最后让我们运行一下刚刚实现的模型：

iterations = 12000 ←---

设置超参数

batch_size = 32

sample_interval = 1000

train(iterations, batch_size, sample_interval) ←---

训练CGAN直

到指定迭代次数

8.3.9 检查输出：生成目标数据

图8.7显示了充分训练后的CGAN的生成器生成的手写数字。在每一

行，命令生成器合成一个从0到9的不同数字。

图8.7 每行显示为匹配给定数字（0到9）而生成的图像样本。CGAN生成器成功学

会了生成数据集中的每个类

注意，每个数字以不同的书写方式呈现，这证明CGAN不仅能生成

与训练数据集中的每个标签匹配的样本，还能捕捉到训练数据的多样

性。

8.4 结论

本章介绍了如何使用标签来指导生成器和鉴别器的训练，使GAN能

生成想要的伪样本。与DCGAN一样，CGAN是最具影响力的早期GAN变体

之一，催生了无数新的研究方向。

其中最具影响力和发展前景的也许是使用条件对抗网络作为图像

到图像转换（image- to-image translation）问题的通用解决方案。

这是一类试图将图像从一种模态转换为另一种模态的问题。图像到图

像转换的应用范围包括黑白照片着色、白天-黑夜场景转换以及从地图

视图合成卫星视图。

基于CGAN最成功的早期实现之一是pix2pix，它使用成对图像（一

个作为输入，另一个作为标签）来学习将图像从一个域转换到另一个

域。回想一下，无论在理论上还是在实践中，用于训练CGAN的条件信

息可能远不止为更复杂的用例和场景提供标签。例如，就着色任务

言，成对图像将是黑白照片（输入）和同一照片的彩色版本（标

签）。读者会在第9章中看到这些示例。

这里没有详细介绍pix2pix，因为它发布仅一年后就被另一种GAN

变体所取代，后者不但在图像到图像的转换任务上优于pix2pix的性

能，而且不需要成对图像。循环一致性对抗网络（Cycle-Consistent

Adversarial Network，CycleGAN）只需要代表两个域的两组图像（例

如一组黑白照片和一组彩色照片）。第8章会介绍有关此出色GAN变体

的内容。

8.5 小结

（1）CGAN是GAN的一种变体，在训练过程中，生成器和鉴别器都

以附加数据（如类别标签）为条件。

（2）附加信息约束生成器去合成特定类型的输出，鉴别器则仅接

收与给定附加信息匹配的真实样本。

（3）本章中的教程实现了一个CGAN，它用MNIST数据集的类别标

签作为条件信息来生成想要的逼真的手写数字。

（4）嵌入把整数映射到具有期望尺寸的稠密向量中。我们使用嵌

入层从随机噪声向量和标签（用于CGAN生成器训练）以及从输入图像

和标签（用于CGAN鉴别器训练）创建联合隐藏表示。

[1] Conditional Generative Adversarial

Nets, by Mehdi Mirza

and Simon Osindero, 2014.

[2] Erik Linder-Norén’s Keras-GAN

GitHub repository, 2017.

第9章 循环一致性生成对抗网络

（CycleGAN）

本章主要内容

通过在整个图像上设置条件来扩展条件GAN

探索最强大、最复杂的GAN架构之一：循环一致性生成对抗网络

（CycleGAN）

介绍面向对象的GAN设计及其4个主要组件的架构

用CycleGAN将苹果转换为橙子

这是一项具有普遍吸引力的技术突破，似乎谁都喜欢把苹果变作

橙子，我们在本章中将学会如何实现它！但这不是一件容易的事，所

以需要至少两个鉴别器和两个生成器来实现。这显然使架构变得复杂

了，因此本章不得不花更多的时间去讨论，但至少这是一个开始以面

向对象的编程（OOP）方式思考的好时机。

9.1 图像到图像的转换

第8章的结尾涉及了GAN应用的一个有趣领域，即图像到图像的转

换。在此应用中，GAN在视频、静态图像甚至风格迁移方面都取得了巨

大的成功。事实上，由于GAN几乎可以实现一种新的用途，它已经走在

了许多此类应用的最前沿。因其视觉特性，较为成功的GAN变体通常会

在YouTube和Twitter上亮相，如果读者还没有看过这些视频，建议搜

索pix2pix、CycleGAN或vid2vid来看一下。

这种类型的转换意味着生成器的输入是一幅图片，因为我们需要

生成器（转换器）从这个图像开始——换句话说，是将图像从一个域

映射到另一个域。在此之前，生成的潜在向量通常是某种程度上有些

难以理解的向量。现在，我们把它换成输入图像。

一种很好的思考图像到图像转换的方式是把它作为特殊情况下的

条件GAN。在这种情况下，我们是以完整的图像（而不只是一个类）为

条件，而这一图像通常与输出图像的维数相同），然后将其作为一个

标签提供给网络（第8章）。如图9.1所示，这一领域最早的著名例子

之一是来自加州大学伯克利分校的图像转换作品。

（来源：Image-to-Image Translation with Conditional

Adversarial

Networks, by Phillip Isola.）

图9.1 条件GAN为图像转换提供了一个强大的框架，该框架应用到许多领域都表

现得很好

如图9.1所示，我们可以进行以下任意一种情况的映射。

从语义标签（例如，原彩色图中，在汽车应该出现的地方画为蓝

色，在道路应该在的地方上画为紫色）到真实街景图。

从卫星图像到类似于Google地图的视图。

从白天图像到夜晚图像。

从黑白照片到彩色照片。

从轮廓到合成的时尚单品。

这个想法显然是强大且通用的，但问题是需要成对的数据。从第

8 章可知，CGAN需要标签。因为这种情况是用另一个图像作为标签，

除非映射到对应的图像（除了是在另一个域，其他一模一样），否则

映射就没有意义。

因此，夜晚图像需要从与白天图像完全相同的位置拍摄。时尚单

品的轮廓需要与训练集中另一个域的全彩色/合成的单品精确匹配。换

句话说，在训练期间，GAN需要访问原始域中单品的对应标签。

例如，在黑白图像转换的例子中，通常首先要获取大量彩色图像

并对所有图像应用黑白滤波器，这样以后将未修改的图像用作一个

域，将经过黑白滤波器的图像用作另一个域。这确保了在两个域中有

对应的图像。然后我们就可以随意使用训练好的GAN了，但如果没有生

成这些“完美”配对的简便方法，那就不走运了！

9.2 循环一致性损失：再GAN一次

加州大学伯克利分校团队认为，实际上并不需要完美的配对。[1]

相反，只需要完成一个循环：从一个域转换到另一个域，然后再转换

回来。例如从一个公园的夏天图像（域A）转到一个冬天的图像（域

B），然后又回到夏天（域A）。现在我们基本上创建了一个循环，理

想情况下原始图像 和重构的图像（ ）是相同的。如果不同，则可以

测量它们像素级别的损失，这样就获得了CycleGAN的第一个损失：循

环一致性损失（cycle-consistency loss），如图9.2所示。

（来源：Jun-Yan Zhu et al.,

2017.）

图9.2 损失是双向的，因此不仅可以从夏季生成冬季的图像，还从冬季生成夏季

的图像。如果G是从A到B的生成器，F是从B到A的生成器，则

一个常见的类比是反译的过程，将中文句子翻译成英文，然后再

翻译回中文时应返回相同的句子。如果不同，就可以通过第一句话和

第三句话的差别来衡量循环一致性损失。

为了能够使用循环一致性损失，我们需要有两个生成器：一个生

成器，从A转换为B，称为 ，有时简称为 ；另一个生成器，从B转

换为A，称为 ，简洁起见以 表示。从技术上讲，有两种损失——

前向循环一致性损失和后向循环一致性损失，由于它们表示

和 ，你可能会认为它们基本相同，但其

实是差1的（off by one）。

9.3 对抗损失

除了循环一致性损失，还有对抗损失。生成器 的生成每个转

换都有对应的鉴别器 ， 有鉴别器 。应该这样想，在转换到域

A时始终在测试图片是否看起来真实，因此使用 ；反之亦然。

这和简单架构的想法是一样的，但是现在由于这两个损失，我们

有了两个鉴别器。不仅要确保从苹果到橙子的转换看起来真实，还要

确保从估计的橙子到重构的苹果的转换看起来也真实。回想一下，对

抗损失确保图像看起来真实，因此它仍然是CycleGAN起作用的关键所

在。由此，我们把对抗损失列为第二位。循环中的第一个鉴别器尤为

重要——否则只会得到帮助GAN记忆重建内容的噪声。[2]

9.4 恒等损失

恒等损失（identity loss）的思路很简单：强制CycleGAN保留

图片的整体颜色结构（或温度）。为此，我们引入一个正则化项，以

使图像的色调与原始图像保持一致。这是一种确保即使在图像上应用

了许多过滤器仍然可以恢复原始图像的方法。

这是通过将已经在域A中的图像输入从B生成A的生成器（ ）做

到的，因为CycleGAN应该知道它们已经在正确的域中。换句话说，就

是惩罚对图像不必要的修改：如果输入一个斑马图并试图“斑马化”

图像，则会返回与输入相同的斑马图，因为没什么需要改变的。[3]图

9.3展示了恒等损失的影响。

即使恒等损失不是CycleGAN工作所必需的，出于完整性考虑，也

应将其包括在内。本章的实现和CycleGAN作者的最新实现都包含了

它，因为根据经验这种调整通常会带来更好的结果，并且强制实施了

看起来合理的约束。但即使是CycleGAN论文本身也只是简单地提到这

一点——似乎是事后证明，因此这里不再赘述。

输入 没有恒等损失 有恒等损失

图9.3 一个图像胜过千言万语——此图展示了恒等损失的影响：在没有恒等损失

时有明显的色调改变，而且似乎找不到发生这种变化的原因，因此应该惩罚这种

行为。即使图为黑白的，也能够看出区别

表9.1总结了本章中提到的损失。

表9.1 损失

计算 测量 确保

计算 测量 确保

对抗

损失

（这就是第5章中介绍的NS-GAN）

损失包括两个方面：一

是给定图像是真实图像

而不是转换图像的可能

性；二是生成器可能欺

骗鉴别器的部分。注

意，该公式仅适用于

，等价的 进入最

终损失

转换后的

图像看起

来真实、

清晰，与

真实图像

别无二致

循环

一致

性损

失：

前向

传播

和 间的差异

（定义为 ）

原始域图像 与两次转

换后的图像 的差异

原始图像

和两次转

换后的图

是相同

的。如果

失败，可

能没有连

贯的映射

A-B-A

计算 测量 确保

循环

一致

性损

失：

后向

传播

原始域图像 与两次转

换后的图像 的差异

原始图像

和两次转

换后的图

是相同

的。如果

失败，可

能没有连

贯的映射

B-A-B

总损

失

所有4个损失结合在一

起：对抗损失×2（因

为有两个生成器），加

上循环一致性损失（前

向和后向合为一项）

整个转换

是逼真且

有意义的

（提供匹

配的图

片）

恒等

损失

B中图像与 )之间

的差异，反之亦然

CycleGAN

只修改图

像需要改

变的部分

a. 有些人可能不熟悉这个符号，它代表两个项之间的L1范数。为

简单起见，我们可以将其视为每个像素与重构图像上对应像素之间的

绝对差异。

9.5 架构

CycleGAN直接基于CGAN架构构建，本质上是两个CGAN结合在一

起，或者正如CycleGAN作者指出的那样，是一个自编码器。回顾第2

章，有一个输入图像 和重建图像 ，这是通过潜在空间 重建的结果

（图9.4）。

为了将图9.4转化为CycleGAN的示意图，我们令 是域A中的图像，

是域B中的图像， 是重构域 A 中的图像。但是，在CycleGAN中处理

的是等维的潜在空间（步骤2），它恰好是CycleGAN必须找到的另一个

有意义的域（B）。即便使用自编码器，潜在空间也只是另一个域，尽

管这不太容易解释。

图9.4 在这幅来自第2章的自编码器的图中，我们使用了这样的类比：将人类概

念压缩（步骤1）成更紧凑的书面形式的字母（步骤2），然后把这一概念扩展成

其他人头脑中相同概念的（不完美）理解（步骤3）

与第2章相比，主要的新概念是引入了对抗损失。这些与许多其他

自编码器和GAN的混合模型本身就是一个活跃的研究领域！因此对于感

兴趣的研究人员来说，这也是一个不错的领域。但是现在，我们将这

两个映射视为两个自编码器： 和 。我们采用自编码器的

基本思想（包括一种用循环一致性损失代替的显式损失函数），并在

此基础上增加鉴别器。这两个鉴别器，每一步使用一个，确保两个转

换（包括到潜在空间的）在各自的域中看起来像真实图像。

9.5.1 CycleGAN架构：构建网络

在进入CycleGAN的实现之前，我们先简要地看一下图9.5 所示的

整体简化实现，其中有两个流程——上半部分，流程A-B-A从域A中的

图像开始；下半部分，流程B-A-B从域B中的图像开始。

（来源：Understanding and Implementing CycleGAN

in TensorFlow, by

Hardik Bansal and Archit

Rathore, 2017.）

图9.5 CycleGAN的简化架构中，输入图像要么进入鉴别器进行评估，要么转换到

一个域，并由另一个鉴别器评估后再转换回来

图像遵循两条路径：其一，输入图像进入鉴别器，以判断是否真

实；其二，输入图像进入生成器，转换为B，然后由鉴别器B评估，看

它在域B中看起来是否真实，并最终转换回A来测量循环一致性损失。

下半部分基本是上半部分图像的一个差1（off-by-one）循环，

并且遵循所有相同的基本步骤。我们将使用apple2orange数据集，也

可以使用其他的，比如著名的horse2zebra数据集，对代码稍作修改并

使用提供的bash脚本下载数据，即可轻松使用。

为了更清楚地总结图9.5，我们在表9.2中给出了所有4个主要的网

络。

表9.2 网络

输入 输出 目标

生成

器：

从A到B

真实的A或从B到A的转换 转换到域B

生成在域B中逼真的图

像

生成

器：

从B到A

真实的B或从B到A的转换 转换到域A

生成在域A中逼真的图

像

鉴别器

A

域A中的图像，转换的或者是真

实的

图像是真实的概

率

不被从B到A的生成器

欺骗

鉴别器

B

域B中的图像，转换的或者是真

实的

图像是真实的概

率

不被从A到B的生成器

欺骗

9.5.2 生成器架构

图 9.6 显示了生成器的架构。通过使用代码中的变量名重新创建

了这个关系图，并包含了形状。这是一个U-Net架构的例子，其特征图

在网络结构中的形状像U。

图9.6 生成器的架构。生成器本身具有收缩路径（d0至d3）和扩展路径（u1至

u4）。收缩路径和扩展路径有时分别称为编码器和解码器

这里有几点需要注意：在编码器中使用标准卷积层；在以上这些

层中创建跳跃连接（skip connection），以使信息更容易通过网络

传播。在图中，这分别由d0至d4和u1至u4之间的轮廓和颜色深浅表

示。可以看到，解码器中有一半的块来自那些跳过连接（注意，特性

映射的数量是原来的两倍！）[4]；解码器使用带最终卷积层的反卷积

层将图像放大到原始图像的大小。

自编码器也是单独用于生成器架构的有用的教学工具，因为生成

器具有编码器-解码器架构。

（1）编码器——图9.4中的步骤1：这些是降低每个特征图（层

或切片）分辨率的卷积层。此为收缩路径（d0至d4）；

（2）解码器——图9.4中的步骤3：这些是将图像放大至

128×128的反卷积层（转置的卷积）；此为扩展路径（u1至u4）。

还要明确一点，这里的自编码器模型在两个方面很有用：第一，

整个CycleGAN架构可以看作训练了两个自编码器。[5] 第二，U-Net本

身具有称为编码器和解码器的部分。

读者可能还会对缩小和随后的放大感到困惑，但这只是为了将图

像压缩为最有意义的表示，同时能重新添加所有细节。这与自编码器

的原理相同，只是现在有了一条记住细微差别的路径。U-Net架构在

多个领域中已经得到了经验证明——它在各种分割任务上的表现更

好。其关键思想是，在下采样过程中，我们可以专注于大区域的分类

和理解，同时高分辨率的跳跃连接保留了可以精确分割的细节。

在实现CycleGAN时，我们将使用具有跳跃连接的U-Net架构，它更

具可读性，如图9.6所示。但许多CycleGAN实现使用ResNet架构，读者

也可自己进行尝试。

注意

ResNet的主要优点是它使用较少的参数，并在中间引入称为

transformer的步骤，它用残差连接来代替编码器-解码器跳跃连接。

根据我们的测试，至少在apple2orange数据集上结果保持不变。

我们没有显式定义transformer，而是用了从卷积层到反卷积层的跳跃

连接。我们在代码部分会再次提及这些相似之处，现在读者只要记住

这一点就好。

9.5.3 鉴别器架构

CycleGAN的鉴别器基于PatchGAN架构——我们在代码部分会深入

讨论技术细节。有一点可能令人困惑，那就是没有得到单一的浮点数

作为鉴别器的输出，而是得到一组单通道值。我们可以把这些值看作

一组小鉴别器，然后将其平均。

最终，这使CycleGAN的设计是全卷积的，意味着它扩展到更高的

分辨率会相对容易。在视频游戏和现实场景互相转换的例子中，得益

于全卷积设计，CycleGAN作者仅做了少量修改就得到了CycleGAN的升

级版。除此之外，鉴别器应该只是之前见过的鉴别器的相对简单的实

现，只不过现在有两个。

9.6 GAN的面向对象设计

我们一直在TensorFlow中和面向对象的编程（Object-Oriented

Programming，OOP）中使用对象，但是通常会用更加函数化的方式处

理架构，因为它们一般比较简单。CycleGAN的架构很复杂，因此需要

一种允许我们继续访问已定义的原始属性和方法的结构。于是我们在

教程中将把CycleGAN编写为一个它自己的Python类，并用方法构建生

成器和鉴别器，然后进行训练。

9.7 教程：CycleGAN

在本教程中，我们将使用Keras-GAN实现，而且使用基于

TensorFlow后端的Keras。[6] 我们在Keras 2.2.4和TensorFlow

1.12.0的版本进行了测试，并从hash 46fcdb9384b3bc9399c6

51b2b43640aa54098e64安装了Keras_contrib。这次必须使用不同

的数据集（尽管在第2章中开了个玩笑，但我们确实知道其他数据

集）。但出于教学目的，我们将继续使用较简单的apple2orange数据

集。直接完成所有常规导入，如清单9.1所示。

清单9.1 导入

from __future__ import print_function,

division

import scipy

from keras.datasets import mnist

from keras_contrib.layers.normalization import

InstanceNormalization

from keras.layers import Input,

Dense, Reshape, Flatten, Dropout,

Concatenate

from keras.layers import BatchNormalization,

Activation,

ZeroPadding2D

from keras.layers.advanced_activations import LeakyReLU

from keras.layers.convolutional import UpSampling2D,

Conv2D

from keras.models import Sequential,

Model

from keras.optimizers import Adam

import datetime

import matplotlib.pyplot as plt

import sys

from data_loader import DataLoader

import numpy as np

import os

按照承诺，我们将使用面向对象的编程风格。在清单9.2中，创建

一个包含所有初始化参数的CycleGAN类，同时包含data_loader。

data_loader在本书GitHub仓库中有定义，用来加载预处理的数据。

清单9.2 启动CycleGAN类

class CycleGAN():

def __init__(self):

self.img_rows = 128 ←---

self.img_cols = 128

self.channels = 3 ←---

输入形状

self.img_shape = (self.img_rows, self.img_cols,

self.channels)

self.dataset_name = 'apple2orange' ←---

配置

data_loader

self.data_loader =

DataLoader(dataset_name=self.dataset_name, ←--- 使用

data_loader对象导入预处理的数据集

img_res=

(self.img_rows, self.img_cols))

patch = int(self.img_rows /

2**4) ←--- 计算D的输出大小

（PatchGAN）

self.disc_patch = (patch, patch,

1)

self.gf = 32 ←---

G的第一层的滤波器的数量

self.df = 64 ←---

D的第一层的滤波器的数量

self.lambda_cycle = 10.0 ←---

循环一致性损失的权重

self.lambda_id = 0.9 *

self.lambda_cycle ←--- 恒等损失

的权重

optimizer = Adam(0.0002, 0.5)

两个新参数是lambda_cycle和lambda_id。第二个超参数影

响恒等损失。CycleGAN作者自己也注意到这个值会影响变化的剧烈程

度，尤其是在训练的早期。[7] 设置一个较低的值会导致不必要的变

化，例如完全反转颜色。基于多次重新运行apple2orange的训练过

程，我们选择了现在的值——这一过程通常是理论指导的炼丹术！

第一个超参数lambda_cycle控制实施循环一致性损失的严格程

度。将此值设置得较高，可确保原始图像和重建图像尽可能地接近。

9.7.1 构建网络

现在我们有了基本参数，可以构建基本网络了，如清单9.3所示。

我们将从高层次的视角开始向下探索，这包括以下内容。

（1）构建两个鉴别器 和 并编译。

（2）构建两个生成器。

a．实例化 和 。

b．为两个方向的图像输入创建占位符。

c．将它们都连接到另一个域中的图像。

d．为原始域中的重构图像创建占位符。

e．为两个方向创建恒等损失约束。

f．现在鉴别器的参数暂时不可训练。

g．编译两个生成器。

清单9.3 构建网络

self.d_A = self.build_discriminator() ←---

self.d_B = self.build_discriminator()

self.d_A.compile(loss='mse',

optimizer=optimizer,

metrics=['accuracy'])

self.d_B.compile(loss='mse',

optimizer=optimizer,

metrics=['accuracy']) ←--- 构建并编译鉴别器

self.g_AB = self.build_generator() ←---

self.g_BA = self.build_generator() ←---

从这里开始构造生成器的计算

图前两行代码构建生成器

img_A = Input(shape=self.img_shape) ←---

img_B = Input(shape=self.img_shape) ←---

从两个域中输入图像

fake_B = self.g_AB(img_A) ←---

fake_A = self.g_BA(img_B) ←---

转换图像到另一个域中

reconstr_A = self.g_BA(fake_B) ←---

reconstr_B = self.g_AB(fake_A) ←---

转换回原来的域

img_A_id = self.g_BA(img_A) ←---

img_B_id = self.g_AB(img_B) ←---

图像的恒等映射

self.d_A.trainable = False ←---

self.d_B.trainable = False ←---

组合模型只训练生成器

valid_A = self.d_A(fake_A) ←---

valid_B = self.d_B(fake_B) ←---

鉴别器决定转换图像的正确性

self.combined = Model(inputs=[img_A, img_B],

←---

outputs=[valid_A, valid_B,

reconstr_A, reconstr_B,

img_A_id, img_B_id])

self.combined.compile(loss=['mse', 'mse',

'mae', 'mae',

'mae', 'mae'],

loss_weights=[1, 1, ←--- 组合模型训练

生成器来欺骗鉴别器

self.lambda_cycle,

self.lambda_cycle,

self.lambda_id,

self.lambda_id],

optimizer=optimizer)

在上述代码中，还要澄清的一点是：combined模型的输出共有6

个。这是因为总是要获得正确性（从鉴别器中）、重建损失和恒等损

失——分别用于A-B-A和B-A-B循环。前两个是平方误差，其余的是平

均绝对误差。各损失的相对权重受前面提到的lambda因子的影响。

9.7.2 构建生成器

接下来我们构建生成器，这里用了9.5.2节中描述的跳跃连接，即

使用U-Net架构。与某些实现用的ResNet架构相比，U-Net架构更易于

编写。在生成器函数中，我们首先定义辅助函数，具体步骤如下。

（1）定义conv2d()函数。

a．标准2D卷积层。

b．Leaky ReLU激活函数。

c．实例归一化[8]。

（2）将deconv2d()函数定义为执行以下操作的转置卷积（又称

反卷积）[9]层。

a．对input_layer进行上采样。

b．如果有设置dropout率，可能会应用dropout。

c．始终应用Instance Normalization。

d．更重要的是，在输出层与来自图9.6下采样部分的对应维

度的层之间创建跳跃连接。

注意

在步骤（2）的d中使用一个简单的UpSampling2D，它没有可学

习参数而是使用最近邻法插值。

然后创建实际的生成器，具体步骤如下。

（3）取输入（128×128×3）并将其分配给d0。

（4）通过卷积层d1，到达64×64×32层。

（5）取d1（64×64×32）应用conv2d得到32×32×64（d2）。

（6）取d2（32×32×64）应用conv2d得到

16×16×128（d3）。

（7）取d3（16×16×128）应用conv2d得到8×8×256（d4）。

（8）u1：对d4上采样，并在d3和u1之间创建一个跳跃连接。

（9）u2：对u1上采样，并在d2和u2之间创建一个跳跃连接。

（10）u3：对u2上采样，并在d1和u3之间创建一个跳跃连接。

（11）u4：使用常规的上采样获得128×128×64的图像。

（12）使用常规的2D卷积除去多余的特征图，获得

128×128×3（高度×宽度×颜色通道）的图像。

构建生成器的代码如清单9.4所示。

清单9.4 构建鉴别器

def build_generator(self):

"""U-Net Generator"""

def conv2d(layer_input, filters, f_size=4):

"""Layers used during downsampling"""

d = Conv2D(filters, kernel_size=f_size,

strides=2, padding='same')(layer_input)

d = LeakyReLU(alpha=0.2)(d)

d = InstanceNormalization()(d)

return d

def deconv2d(layer_input, skip_input, filters,

f_size=4,

dropout_rate=0):

"""Layers used during upsampling"""

u = UpSampling2D(size=2)(layer_input)

u = Conv2D(filters, kernel_size=f_size,

strides=1,

padding='same', activation='relu')(u)

if dropout_rate:

u = Dropout(dropout_rate)(u)

u = InstanceNormalization()(u)

u = Concatenate()([u, skip_input])

return u

d0 = Input(shape=self.img_shape) ←---

输入图像

d1 = conv2d(d0, self.gf)

←---

d2 = conv2d(d1, self.gf

* 2)

d3 = conv2d(d2, self.gf

* 4)

d4 = conv2d(d3, self.gf

* 8) ←--- 下采样

u1 = deconv2d(d4, d3,

self.gf * 4) ←---

u2 = deconv2d(u1, d2,

self.gf * 2)

u3 = deconv2d(u2, d1,

self.gf) ←--- 上采样

u4 = UpSampling2D(size=2)(u3)

output_img = Conv2D(self.channels, kernel_size=4,

strides=1, padding='same',

activation='tanh')(u4)

return Model(d0, output_img)

9.7.3 构建鉴别器

现在我们来构建鉴别器，这里使用了一个辅助函数来创建由2D卷

积、LeakyReLU和InstanceNormalization（可选）组成的层。

按照以下方式应用这些层，如清单9.5所示。

（1）取输入图像（128×128×3）并指定给d1（64×64×64）。

（2）取d1（64×64×64）并指定给d2（32×32×128）。

（3）取d2（32×32×128）并指定给d3（16×16×256）。

（4）取d3（16×16×256）并指定给d4（8×8×512）。

（5）取d4（8×8×512）并通过conv2d将其展平为8×8×1。

清单9.5 构建鉴别器

def build_discriminator(self):

def d_layer(layer_input, filters, f_size=4,

normalization=True):

"""Discriminator layer"""

d = Conv2D(filters, kernel_size=f_size,

strides=2, padding='same')(layer_input)

d = LeakyReLU(alpha=0.2)(d)

if normalization:

d = InstanceNormalization()(d)

return d

img = Input(shape=self.img_shape)

d1 = d_layer(img, self.df,

normalization=False)

d2 = d_layer(d1, self.df

* 2)

d3 = d_layer(d2, self.df

* 4)

d4 = d_layer(d3, self.df

* 8)

validity = Conv2D(1, kernel_size=4,

strides=1,

padding='same')(d4)

return Model(img, validity)

9.7.4 训练CycleGAN

构建完所有网络后，现在我们来实现创建训练循环的方法。对于

CycleGAN训练算法，每个训练迭代的细节如下。

CycleGAN训练算法

对每次训练迭代，执行如下操作。

（1）训练鉴别器。

a．从每个域中抽取一小批随机图像 和

。

b．使用生成器 将 转换为域B，反之使用 。

c．计算 ( , 1)和

( ( ), 0)，分别得

到A中真实图像和从B转换的图像的损失，然后将这两个损失相加；

中的1和0用作标签。

d．计算 ( , 1)和

( ( ), 0)，分别获

得B中的真实图像和从A转换的图像的损失，然后将这两个损失相加；

中的1和0用作标签。

e．将步骤c和d的损失相加，得到鉴别器总损失。

（2）训练生成器。

a．使用组合模型执行如下操作。

输入来自域 和域 的图像。

输出以下项。

A的正确性： 。

B的正确性： 。

重构的A： 。

重构的B： 。

A的恒等映射： 。

B的恒等映射： 。

b．使用循环一致性损失、恒等损失和对抗性损失更新两

个生成器的参数。

标量的均方误差（MSE）（鉴别器概率）。

图像的均方绝对误差（MAE）（重建的或恒等映射的）。

结束

清单9.6实现了此CycleGAN的训练算法。

清单9.6 CycleGAN的训练算法

def train(self, epochs, batch_size=1,

sample_interval=50):

start_time = datetime.datetime.now()

valid = np.ones((batch_size,) +

self.disc_patch) ←--- 对

抗损失的ground truths

fake = np.zeros((batch_size,) +

self.disc_patch)

for epoch in range(epochs):

for batch_i, (imgs_A, imgs_B)

in enumerate(

self.data_loader.load_batch(batch_size)):

fake_B = self.g_AB.predict(imgs_A) ←---

fake_A = self.g_BA.predict(imgs_B) ←---

开始训练

鉴别器将图像转换到另外的域

dA_loss_real = self.d_A.train_on_batch(imgs_A,

valid) ←---

dA_loss_fake = self.d_A.train_on_batch(fake_A,

fake)

dA_loss = 0.5 *

np.add(dA_loss_real, dA_loss_fake)

dB_loss_real = self.d_B.train_on_batch(imgs_B,

valid)

dB_loss_fake = self.d_B.train_on_batch(fake_B,

fake)

dB_loss = 0.5 *

np.add(dB_loss_real, dB_loss_fake)

←--- 训练鉴别器（原图=真/转换=假）

d_loss = 0.5 *

np.add(dA_loss, dB_loss) ←---

鉴别器总损失

g_loss = self.combined.train_on_batch([imgs_A,

imgs_B], ←--- 训练生成器



[valid, valid,



imgs_A, imgs_B,



imgs_A, imgs_B])

if batch_i % sample_interval

== 0: ←--- 如果是

保存间隔=>保存生成的图像样本

self.sample_images(epoch, batch_i) ←--- 该

函数与之前遇到的类似

9.7.5 运行CycleGAN

我们完成了所有复杂的代码，现在可以实例化一个CycleGAN对

象，并从采样图像中看一看结果（图9.7）：

gan = CycleGAN()

gan.train(epochs=100, batch_size=64, sample_interval=10)

图9.7 苹果变成了橙子，橙子变成了苹果——这些就是出现在Jupyter

Notebook中的结果

（根据随机种子、TensorFlow和Keras的实现以及超参数的不同，结果可能会略

有不同）

9.8 扩展、增强和应用

由于CycleGAN的结果着实惊人，许多研究人员对这项技术进行了

改进。在本节中，我们将详细介绍CycleGAN的扩展，然后讨论它的一

些应用。

9.8.1 增强CycleGAN

“增强CycleGAN：从未配对数据中学习多对多映射”是对标准

CycleGAN的巧妙扩展，它在两次转换过程都注入了潜在空间信息。在

斯德哥尔摩召开的 ICML 2018 大会上，增强CycleGAN利用额外的变量

来驱动生成过程。[10] 与CGAN使用潜在空间的方式相同，我们可以在

CycleGAN设置中使用它，可以得到比CycleGAN已经做到的更好的结

果。

例如，如果在域A中有鞋子的轮廓，则可以在域B中生成样本，其

中相同类型的鞋子是蓝色的。在传统的CycleGAN情况下，鞋子总是蓝

色的，但现在有了可以利用的潜在变量，它可以是橙色的、黄色的，

抑或是可以选择的任何颜色。

这也是一个用于思考原始CycleGAN局限性的有用框架：我们没有

获得任何额外的种子参数（例如额外的潜在向量 ），因此无法控制或

更改另一端的结果。如果从一个特定的手提包轮廓中得到的图像是橙

色的，则它将始终是橙色的。如图9.8所示，增强CycleGAN使我们能够

更好地控制结果。

（来源：Augmented CycleGAN: Learning Many-to-Many

Mappings from

Unpaired Data, by Amjad

Almahairi et al., 2018.）

图9.8 在增强CycleGAN的信息流中，潜在向量 和 将与图像输入一起作

为生成器的种子，有效地将问题归结为两个CGAN的连接，从而使得我们能够控制

生成

9.8.2 应用

在很短时间内，CycleGAN（或受CycleGAN启发）的应用如雨后春

笋般涌现。人们通常会先创建模拟的虚拟环境，然后让它们变得逼

真。例如，假设要为一家自动驾驶汽车公司提供更多的训练数据，只

需在Unity或GTA 5图形引擎中对其进行仿真，然后使用CycleGAN转换

数据即可。

如果在数据集中需要特定危险场景，但是重新创建这些场景是成

本昂贵或者耗时的（例如，车祸或消防车加速赶往目的地的场景），

那么这种方法尤其有效。对于无人驾驶汽车公司来说，这对于平衡数

据集与风险情况非常有用，风险情况很少见，但正确的行为更重要。

这种框架的一个例子是循环一致的对抗域自适应（Cycle

Consistent Adversarial Domain Adaptation，CyCADA），[11]

但对

其工作方式的完整解释超出了本章的范围。这是因为还有很多这样的

框架，有些甚至在语言、音乐或其他形式的域适应方面尝试使用

CycleGAN。图9.9显示了CyCADA的架构和设计，可供读者从中一窥其复

杂性。

图9.9 这种结构要求读者对前面的内容有所熟悉。还有一件事需要指出：现在有

了一个额外的步骤来处理标签和语义理解，即任务损失（task loss）。它可以检

查生成图像的语义

9.9 小结

（1）由于需要完美的配对，图像到图像的转换框架通常很难训

练。CycleGAN通过使其成为一个不成对的域转换来解决这个问题。

（2）CycleGAN有如下个损失。

循环一致性损失，用于测量原始图像与转换到不同域并再次转回

原域的图像之间的差异。

对抗损失，可以确保生成逼真的图像。

恒等损失，可以保留图像的色彩空间。

（3）两个生成器使用U-Net架构，两个鉴别器使用基于PatchGAN

的架构。

（4）本章实现了CycleGAN的面向对象设计，并用它将苹果转换为

橙子。

（5）CCycleGAN的实际应用包括自动驾驶汽车的训练和能够在转

换过程中创建不同风格图像的扩展。

[1] Unpaired Image-to-Image Translation

Using Cycle￾Consistent Adversarial Networks,

by Jun-Yan Zhu et

al., 2017.

[2] 实际中要复杂一点，例如这取决于是否同时考虑了正向和反向的

循环一致性损失。但可以用它作为一个思维模式来思考对抗损失重要

性——记住有两个映射A-B-A和B-A-B，所以两个鉴别器在某种程度上

都是第一个。

[3] Jun Yan Zhu

et al., 2017.

[4] 正如读者将看到的，这只是意味着我们把整个块/张量恒等连接

到生成器解码部分的上色等价彩色的张量上。

[5] Jun-Yan Zhu 等人的论文（2017）。

[6] the Keras-GAN GitHub

repository by Erik Linder-Norén,

2017.

[7] pytorch-CycleGAN-and-pix2pix Frequently Asked

Questions,by Jun-Yan Zhu, April

2019.

[8] 实例归一化（Instance Normalization，IN）与第4章中的批归

一化类似，只是没有基于整个批量的信息进行归一化，而是分别对每

个通道中的每个特征图进行归一化。实例归一化通常会为风格迁移或

图像到图像的转换等任务带来更好的图像质量，这正是CycleGAN所需

要的！

[9] 这里，转置卷积是一个更准确的术语。可以把它看作卷积的反过

程或反卷积。

[10] Augmented Cyclic Adversarial

Learning for Low Resource

Domain Adaptation,by Ehsan Hosseini-Asl,

2019.

[11] CyCADA: Cycle-Consistent Adversarial

Domain Adaptation,

by Judy Hoffman et

al., 2017.

第三部分 何去何从

第三部分探讨了一些实际应用案例，并探索了一些可以应用第

一、二部分中关于GAN及其实现相关知识的领域。

第10章讨论对抗样本（通过故意欺骗来使分类器犯错误），这是

一个具有重要实践和理论意义的领域。

第11章探讨GAN在医学和时尚领域中的实际应用，其实现用到了本

书中介绍的GAN变体。

第12章概述GAN及其应用的道德伦理考量，还提到了一些新兴的

GAN技术，以供想继续探索本书以外知识的读者参考。

第10章 对抗样本

本章主要内容

这是一个先于GAN的迷人的研究领域，有着错综复杂的历史

计算机视觉中的深度学习方法

带有真实图像和噪声的对抗样本

在学习本书的过程中，读者已经把GAN理解为一个直观的概念。但

是在2014年，对于那些不熟悉对抗样本这个新兴领域的人，GAN以及

Ian Goodfellow和其他人在该领域的工作[1]，似乎可以看作一次信仰

上的飞跃。本章将深入探讨对抗样本，尤其是那些使用其他分类算法

会灾难性地失败的特殊构造的样本。

本章还讨论了对抗样本与GAN的联系，以及对抗学习为何和如何成

为机器学习中仍未能解决的难题——现有方法的一个重要但很少被讨

论的缺陷。即使对抗样本在机器学习的鲁棒性、公平性和（网络）安

全性中起着重要作用，这也是毋庸置疑的事实。

在过去5年中，机器学习的确在赶上和超越人类表现的能力上取得

了长足进步，例如在计算机视觉分类任务以及游戏上。[2] 但是，仅看

指标和ROC曲线[3]不足以了解神经网络做出的决定的依据（如何起作用

的）以及容易犯哪些错误。本章将略微提及前者，然后深入研究后

者。尽管本章几乎只涉及计算机视觉领域的问题，但是在诸如文本处

理以及其他领域中也可以找到对抗样本。[4]

当谈论神经网络的性能时，我们经常看到它们在大型ImageNet数

据集上的错误率低于人类。这种经常被引用的统计数据（最开始是学

术上的玩笑）掩盖了隐藏在该平均值背后的性能差异。人类的错误率

通常是由于他们无法辨认在此数据集中的不同品种的狗，机器学习的

犯错的原因则更为不详。经过进一步研究，对抗样本就产生了。

与人类不同，CV（计算机视觉）算法会纠结于性质非常不同的问

题，这个问题可能更倾向于是训练数据导致的。因为算法必须为任何

可能出现的图像做出预测，所以即使有很多训练数据，它也必须为完

全不同于已经在训练数据中见过的个别样本进行推断。

当训练了如Inception V3和VGG-19之类的网络后，我们发现了一

种效果惊人的图像分类方法，它在与训练数据接近的细流形（thin

manifold）上有很好的效果。但是人们试图在这些算法的分类能力找

漏洞时，发现了一个巨大缺陷——当前的机器学习算法很容易被很微

小的失真所愚弄。迄今为止，几乎所有成功的主流机器学习算法在一

定程度上受到该缺陷的影响，于是有人开始质疑机器学习究竟能不能

起作用。

注意

在监督学习条件下，请考虑我们的训练集。我们有一个训练流形

（manifold）——这只是一个描述有样本存在的高维分布的花哨的

词。例如，300像素×300像素图像处在270000维空间（300×300×3

色）中，这使得训练非常复杂。

10.1 对抗样本的背景

我们之所以在本书最后一部分设立本章，是出于以下考虑。

（1）借助对抗样本，我们可以尝试生成新样本来愚弄现有的系

统，使它错误地对输入进行分类。我们通常以邪恶的攻击者的身份或

以研究人员的身份这样做，以了解系统的鲁棒性。尽管对抗样本与GAN

存在重大差异，但它们密切相关。

（2）这将使读者了解为什么GAN如此难训练以及为什么现有的系

统如此不堪一击。

（3）对抗样本考虑了与 GAN 不同的一组应用，而我们希望读者

能够了解对抗样本功能的基础知识。

对抗样本的应用之所以如此有趣，原因有如下几个。

（1）如上讨论，对抗样本可用于恶意目的，因此测试关键系统的

鲁棒性非常重要。试想如果攻击者可以轻易地欺骗面部识别系统来访

问手机，那该怎么办？

（2）对抗样本使机器学习的过程更加清晰——这是一个日益重要

的主题。对抗性学习的表示法对于分类很有用，但是它不允许攻击者

找到受保护的事物，这可能是确保机器学习不歧视任一类的最佳方法

之一。

（3）同样可以使用对抗性学习来保护涉及隐私的敏感信息（可能

是医疗或财务信息），在这种情况下，我们仅关注无法恢复的个人信

息。

就目前的研究而言，学习对抗样本是开始理解防御对抗样本的唯

一方法，因为大多数论文会先描述它所防御的攻击类型，然后才是尝

试怎么解决。截至本书撰写时，还没有针对所有攻击类型的通用的防

御措施——这是否是去研究它的一个好的理由，还得取决于读者对对

抗样本的看法。本节不详细介绍防御措施，因为它超过了本章最后的

更高层次的思想，超出此范围的所有内容均不属于本书的范畴。

10.2 谎言，该死的谎言及分布

为了真正理解对抗样本，我们必须回到CV分类任务的领域，去一

定程度地了解这项任务的难度。从原始像素到最终能够对图像集进行

分类是颇具挑战性的。

这一定程度上是因为，为了得到一个真正可泛化的算法，我们必

须能够对一点也不像训练集中的数据做出合理的预测。此外，即使只

是稍微改变拍摄照片的角度，得到的图像和训练集中同一类的与它最

相近的图像之间也存在巨大的像素级差异。

当RGB空间中拥有100000个300×300图像样本的训练集时，算法必

须以某种方式处理一个270000维的数据。当考虑所有可能的图像（不

是实际观察到的图像，而是可能存在的图像）时，每个维度的像素值

都与其他维度无关，因为通过270000次掷256面的骰子始终可以生成不

同的有效图像。因此，理论上在8位颜色空间中有256270000（一个长达

650225位的数字）个样本。

这将需要大量样本来覆盖此空间，即使只覆盖它的1%。当然，这

些图像大多数没有任何意义。通常的训练集比这稀疏得多，因此需要

训练算法能够用相对有限的数据去推断它们还没有见过的数据分布区

域。这是因为大多数实际情况下，算法可能不会看到任何东西与在训

练集中看到的类似。

注意

100000个样本常被引作深度学习算法能真正奏效的最低标准。

算法必须有意义地泛化，即必须能够有意义地填充大部分空间但

却从未见过其中任何样本。计算机视觉算法之所以起作用，主要是因

为它们可以对大量的未见过的样本做出很好的推测，但它们的优势同

时也是最大的弱点。

10.3 训练的使用与滥用

本节介绍两种关于对抗样本的思考方式，一种基于第一原理，另

一种是通过类推得到的。思考对抗样本的第一种方式是从训练机器学

习进行分类的方法入手。记住，这里讨论的都是有数千万参数的网

络。在整个训练过程中，我们会更新其中的一些参数，以使分类结果

与训练集中提供的标签匹配。我们需要找到正确的参数更新的方式，

而这恰恰是随机梯度下降所能做到的。

回想一下在了解GAN之前的简单分类。这是某种可学习的分类函数

（例如，深度神经网络，或者DNN），它是以θ 作为参数（DNN的

参数）将 （例如图像）作为输入并产生分类 。在训练时，我们取 作

为预测值并将其与真实 进行比较，就是得到损失 的方式。然后，我

们更新 的参数，以使损失最小化。式10.1～式10.3总结了上述过

程[5]。

式10.1

式10.2

使得 式10.3

本质上，我们把预测定义为输入一个示例后神经网络的输出（式

10.1）。损失是权衡真实标签和预测标签之间的差异的某种形式（式

10.2）。这样就可以将整个问题表述为通过调整DNN参数来最小化真实

标签和预测标签之间的差异，然后对于给定样本构成预测值（式

10.3）。

这些都很有效，那实际上如何使分类损失最小化呢？如何解决式

10.3中所述的优化问题？通常使用基于SGD的方法来成批次地获取 ，

然后用损失函数关于当前参数 求导并乘以学习率α，去构成新参数

，参见式10.4。

式10.4

上述内容一定是最简洁的深度学习入门介绍了。掌握了这些基本

原理后，读者不妨思考一下，是否可以将功能如此强大的工具（SGD）

用于其他目的？例如，当增大而不是减小损失空间时会发生什么？事

实证明，最大化比最小化误差要容易得多——虽然容易，但也很重

要。正如许多重大发现一样，它最初是个表面上的小漏洞，后来发展

成了致命的缺陷：如果我们开始更新像素而不是更新权重，会发生什

么事情呢？如果蓄意更新像素，对抗样本便产生了。

经过简单的回顾，读者可能仍旧对SGD感到困惑，那么我们在图

10.1中回想一下典型的损失空间是什么样子的。

（a） （b）

（来源：Visualizing the Loss Landscape

of Neural Nets, by

Tom

Goldstein et al., 2018.）

图10.1 在典型的损失空间中，这是可以通过深度学习算法切实获得的损失值的类

型。图（a）为2D的等损失轮廓线，图（b）为损失空间的3D渲染（还记得第6章中

的登山类比吗？）

第二种有用的（尽管是不完美的）思维模型也有助于理解对抗样

本。读者可能会将对抗样本视为CGAN，就像在前两章中遇到的那样。

借助对抗样本，我们可以调整整个图像并尝试生成一个域转换

（domain transferred）或相似的图像（除了欺骗分类器的域）。

“生成器”可以简单地用随机梯度上升去轻微地调整图像，就可以骗

过一些分类器。

先不论哪种解释对读者来说容易理解，我们直接来看对抗样本，

来看一看它们是什么样子的。通过观察到改变后的图像很容易被错误

分类发现，我们发现了对抗样本。首个实现对抗样本的方法是快速梯

度符号法（Fast Sign Gradient Method，FSGM），它与之前的描述一

样简单。

从梯度更新开始（式10.4），查看一下符号，使它朝相反的方向

迈出一小步。在大多数情况下，这样产生的图像看起来和原图几乎是

相同的！图像胜却千言万语，图10.2展示了只需要很少的噪声就可以

达到的效果。

（a） （b）

（c）

图10.2 一点噪声会产生很大的不同，图（b）施加了图（c）所示的噪声（差

异）。图（c）被放大了大约300倍并进行了移位，以便可以生成有意义的图像

在未经修改的图像上运行经过预训练的ResNet-50分类器后查看预

测值最大的前3个类，如表10.1所示。

表10.1 预测值最大的前3个类

排名 类别 置信度

第一 mountain_tent 0.6873

第二 promontory 0.0736

排名 类别 置信度

第三 valley 0.0717

前三名看起来都很合乎情理，山坡mountain_tent排在第一位。

表10.2显示了对于对抗样本的预测，前三名完全没有山坡

mountain_tent，虽然至少匹配了“是户外景色”这一点，但即使是

经过修改的图像，也显然不会是吊桥suspension_bridge。

表10.2 对于对抗样本的预测

排名 分类 置信度

第一 volcano 0.5914

第二 suspension_bridge 0.1685

第三 valley 0.0869

这就是可以扭曲预测的程度，只有大约200个像素值的变化（相当

于把单个几乎是黑色的像素变成几乎是白色的像素）分配在整个图像

上。

另外，令人震惊的是，生成整个样本所需的代码甚少。本章将使

用一个名为foolbox的库，该库效果惊人，提供了许多用于生成对抗

样本的简便方法。从众所周知的导入开始，再加上foolbox——它是

一个专门设计用来更容易地实现对抗性攻击的代码库，如清单10.1所

示。

清单10.1 导入声明

import numpy as np

from keras.applications.resnet50 import ResNet50

from foolbox.criteria import Misclassification,

ConfidentMisclassification

from keras.preprocessing import image

as img

from keras.applications.resnet50 import preprocess_input,

decode_predictions

import matplotlib.pyplot as plt

import foolbox

import pprint as pp

Import keras

%matplotlib inline

然后定义一个可以方便加载更多图像的函数，如清单10.2所示。

清单10.2 辅助函数

def load_image(img_path: str):

image = img.load_img(img_path, target_size=(224,

224))

plt.imshow(image)

x = img.img_to_array(image)

return x

image = load_image('DSC_0897.jpg')

接下来，设置Keras以注册模型，并从Keras提供的函数中下载

ResNet-50，如清单10.3所示。

清单10.3 创建表10.1和表10.2

keras.backend.set_learning_phase(0) ←--- 实例化模型

kmodel = ResNet50(weights='ImageNet')

preprocessing = (np.array([104, 116,

123]), 1)

fmodel = foolbox.models.KerasModel(kmodel, bounds=(0,

255),

preprocessing=preprocessing) ←--- 通过Keras模型生成foolbox

模型对象

to_classify = np.expand_dims(image, axis=0)

←--- 图像维度变为（1,

224, 224, 3）符合 ResNet-50的输入要求（批量输入图像以生成预测）

preds = kmodel.predict(to_classify) ←---

生成预测并打印结果

print('Predicted:', pp.pprint(decode_predictions(preds, top=20)

[0]))

label = np.argmax(preds) ←---

得到最高值的索引值稍后作为标签输入

image = image[:, :,

::-1] ←--- ::1 反转颜色通道,

因为Keras-ResNet-

50输入要求BGR

attack = foolbox.attacks.FGSM(fmodel, threshold=.9,

←--- 创建攻

击对象设置高的错误分类标准

criterion=ConfidentMisclassification(.9))

adversarial = attack(image, label)

←--- 对源图像施加攻击

new_preds = kmodel.predict(np.expand_dims(adversarial, axis=0))

←--- 对抗样本上得到新的预测

print('Predicted:', pp.pprint(decode_predictions(new_preds, top=20)

[0]))

使用这些例子非常简单！读者可能会觉得也许只有ResNet-50会受

这些样本的影响。但是，我们在测试本章的各种代码时发现ResNet不

但被证明是最难破解的分类器，而且在每个ImageNet类别中，ResNet

在DAWNBench上都是无可争议的赢家（这是DAWNBench的CV类别中最具

挑战性的任务），如图10.3所示。[6]

图10.3 至少截至2019年7月上旬，DAWNBench是一个查看当前最新模型和ResNet-

50的主导地位的好地方

对抗样本的最大问题是它们的普遍性。对抗样本不仅限于深度学

习，还可以推广到不同的机器学习技术。如果针对一种技术生成一个

对抗样本，那么它很有可能也可以攻击另一种模型，如图10.4所示。

（来源：Transferability in Machine Learning:

from Phenomena to Black￾Box

Attacks Using Adversarial Samples,

by Nicolas Papernot et

al.,

2016.)

图10.4 数字表示为了欺骗该行中的分类器而精心设计的对抗样本的百分比，这些

样本也欺骗了该列的分类器。方法有深度神经网络（DNN）、逻辑回归（LR）、支

持向量机（SVM）、决策树（DT）、k近邻（kNN）和集成学习（Ens.）

10.4 信号与噪声

更糟糕的是，许多对抗样本非常容易构建，以至于可以轻松地通

过高斯噪声来欺骗分类器。高斯噪声可以从np.random.normal中采

样获得。为了证明ResNet-50 是一个颇具鲁棒性的体系结构，我们将

展示其他体系结构为此受到的更大的影响。

图10.5显示了在纯高斯噪声下运行ResNet-50的结果。但是，我们

可以对噪声本身进行对抗性攻击，以查看图像可以被错误分类到什么

程度。

清单10.4将使用投影梯度下降（Projected Gradient Descent，

PGD）进行攻击，如图10.6所示。尽管是一种简单的攻击，但它证实了

一种更高层次的解释。与以前的攻击不同，不管它会将我们带向何方

——甚至是“无效”的像素值——都会前进一步后将其投射回可行域

空间。现在我们将PGD攻击应用到图10.7中的高斯噪声上，并运行

ResNet-50去看看是如何做的。

图10.5 显然在大多数情况下，只单纯地用采样获得的噪声无法得到作为错误类别

的确切分类，这就是ResNet-50的优点。左侧归纳了使用的均值和方差，我们可以

看到它们的影响

图10.6 投影梯度下降朝着最佳方向迈出的一步，然后使用投影找到最近的点集中

的等价点。在这种情况下，我们试图确保最终得到的仍然是一个有效图：以 为

例采取最佳步骤到 ，然后将其投影到一个有效的图像集

图10.7 在对抗性噪声下运行ResNet-50时，得到了一个不同的结果：应用PGD攻

击后，大多数对象被错误分类了，即使PGD是很简单的攻击

为了证明大多数网络结构更更糟，我们将研究Inception V3——

该架构已在CV领域中声名鹊起。实际上，Inception V3已经被认为是

非常可靠的了——本书第5章中已经对其进行了介绍。图10.8展示了即

使是用于定义Inception分数（Inception分数用于评价生成模型的好

坏）的Inception V3网络，在简单的样本中仍然会失败。为了消除疑

问，我们可以认为Inception V3仍然是目前最好的预训练网络之一，

并且确实具有超高的准确率。

注意

这里只是常规的高斯噪声。你可以在代码中看到目前没有加入对

抗的步骤。诚然，你可能会说噪声可能是得到了更好的预处理，但这

仍是一个很大的对抗性弱点。

为了让读者想亲眼看看这些结果，我们会提供可以重现上述内容

的代码。因为代码都很类似，所以我们只提供一遍，下次将使用DRY化

的代码。

注意

有关不要重复你自己代码原则（Don’t Repeat Yourself，DRY）

的说明，参见维基百科的相关词条。

图10.8 应用于高斯噪声的Inception V3模型，注意，没有使用任何攻击，这些

噪声只是从分布中采样得到的

清单10.4 高斯噪声

fig = plt.figure(figsize=(20,20))

sigma_list = list(max_vals.sigma) ←---

均值和方差的列表（浮点数形

式）

mu_list = list(max_vals.mu)

conf_list = []

def make_subplot(x, y, z,

new_row=False): ←--- 生成绘制图10.8的核

心函数

rand_noise = np.random.normal(loc=mu, scale=sigma,

size=

(224,224, 3)) ←--- 每个均值和方差的样本噪声

rand_noise = np.clip(rand_noise, 0,

255.) ←--- 只允许0～255

的像素值

noise_preds = kmodel.predict(np.expand_dims(rand_noise,

axis=0)) ←--- 获得第一个预测

prediction, num = decode_predictions(noise_preds,

top=20)[0]

[0][1:3] ←--- 分别获取预测的类别和置信度

num = round(num *

100, 2)

conf_list.append(num)

ax = fig.add_subplot(x,y,z) ←---

为图10.8设置注释代码并添加注

释文字

ax.annotate(prediction, xy=(0.1, 0.6),

xycoords=ax.transAxes, fontsize=16, color=’yellow’)

ax.annotate(f’{num}%’ , xy=(0.1, 0.4),

xycoords=ax.transAxes, fontsize=20, color=’orange’)

if new_row:

ax.annotate(f’



$\mu$



:{mu},



$\sigma$



:{sigma}’ ,

xy=(-.2, 0.8), xycoords=ax.transAxes,

rotation=90, fontsize=16,

color=’black’)

ax.imshow(rand_noise / 255) ←---

除以255，[0, 25]变为[0, 1]

ax.axis(’off’)

for i in range(1,101):

←--- 主程序中的for循环，是为了允许图中插入子图

if (i-1) % 10==0:

mu = mu_list.pop(0)

sigma = sigma_list.pop(0)

make_subplot(10,10, i, new_row=True)

else:

make_subplot(10,10, i)

plt.show()

10.5 柳暗花明又一村

现在有些人开始担心对抗样本的安全性，但保持对假想攻击者的

清醒而有意义的认识是很重要的。如果攻击者可以对每个像素稍加更

改，那么为什么不能更改整幅图？[7] 为什么不输入一幅完全不同的

图？为什么传入的样本有着不可觉察的不同，而不是明显的不同？

有些人举了无人驾驶汽车和停车标志的对抗性例子。但是如果我

们能够做到这一点，为什么攻击者不会将喷漆涂在停车标志上，或者

干脆用高速限速标志遮挡停车标志呢？因为与对抗样本不同，这些

“传统攻击”是100%能起作用的，而对抗性攻击只有在被更好地转换

且不会被预处理破坏的情况下才起作用。

这并不意味着当你有关键任务的机器学习应用程序时，你就可以

忽略此问题。在大多数情况下，对抗性攻击比常见的攻击需要更多的

精力，因此记住这一点是值得的。

不过，与大多数安全隐患一样，对抗攻击也具有对抗防御，即试

图防御多种类型的攻击。本章介绍的攻击是一些较简单的攻击，但是

其实也存在更简单的攻击，例如通过MNIST绘制一条直线——即使这

样，也足以欺骗大多数分类器。

对抗防御是一个不断发展的“游戏”，其中针对某些（但不是全

部）类型的攻击提供了许多良好的防御。但转变真的可以如此之快，

以至于ICLR 2018交稿截止日期后仅3天，8个提出的并经过检验的防御

机制中，7个都被攻破了。[8]

10.6 GAN的对手

为了使GAN之间的联系更加清晰，我们假设有一个生成对抗样本的

系统，然后另一个系统用于说明该样本有多有效——有效程度取决于

样本是否成功地欺骗了系统。是不是想起了生成器（敌对者）和鉴别

器（分类算法）？这两种算法又在竞争：敌对者试图用轻微的图像干

扰来欺骗分类器，而分类器则努力不被愚弄。事实上，可以这样来

想，GAN几乎就是循环机器学习的对抗样本，最终生成了图像。

你还可以把迭代的对抗攻击看作使用的是GAN，而且要明确目标并

不是去生成最现实的样本，而是生成可以愚弄分类器的样本。当然，

我们必须始终记住二者仍存在重要的区别，而且通常在已部署的系统

中具有固定的分类器。但这并不妨碍在对抗训练中使用此思想，对抗

训练的一些实现甚至包括用欺骗分类器的对抗样本去重复训练分类

器。这些技术越来越接近典型的GAN的设定。

下面介绍一种技术，它作为可行的防御技术而占有一席之地。在

Robust Manifold Defense一文中，我们采取以下步骤来防御对抗样

本。[9]

（1）获取图像 （对抗性的或常规的），然后将其投影回潜在空

间 ，再用生成器 生成与 相似的样本，称

为 *。

（2）使用分类器 对此样本进行分类，结果为 ，与直接在

上运行分类相比，该样本已经很少被分错类了。

但是这种防御机制的作者发现，仍然存在一些模棱两可的情况。

在这种情况下，分类器确实会受到微小干扰的愚弄。不过还是鼓励读

者去阅读这篇论文，因为这些模棱两可的情况对于人类来说往往也不

明确，这是模型具有鲁棒性的标志。为了解决这个问题，我们在流形

上进行对抗训练：将一些对抗样本纳入训练集中，以便分类器学会将

它们与真实数据相区分。

这篇文章证明了使用GAN可以提供一种分类器，这种分类器在受到

轻微扰动（即使扰动是复杂方法产生的）后也不会完全崩溃。与大多

数防御一样，下游分类器的性能确实会下降，因为必须对分类器进行

训练，以使其隐式地处理这些对抗样本。但即便没有这一缺点，它也

不是一种通用的防御。

对抗训练也有一些有趣的应用，例如，在一段时间内，对抗训练

在半监督学习中取得了最好的成绩。[10] 随后，对抗训练受到了

GAN（第7章）和其他方法的挑战，但这并不意味着现在对抗训练不再

是最新技术了。

希望这给了你研究GAN和对抗样本的另一个理由，部分是因为在关

键任务的分类工作中，GAN可能是最先进的防御方法，抑或是由于本书

未涵盖其他应用[11]——那些最好留到Adversarial Examples in

Action一书中再去讲吧。

综上所述，本章已经提出了对抗样本的概念，并使其与GAN的联系

更加具体。这是一种未被重视过的联系，但这可以帮助你巩固对这一

具有挑战性的主题的理解。此外，对抗样本的一种防御措施就是GAN本

身！[12] 因此，GAN也具有弥补这一差距的潜力——这种差距可能正是

它们存在的理由。

10.7 结论

对抗样本是一个重要领域，因为即使是已经商用的计算机视觉产

品，也会受其困扰，很容易被学者们愚弄。[13] 除了安全性和机器学

习的可解释性应用，许多实际用途仍需有公平性和鲁棒性。

此外，对抗样本是巩固对深度学习和GAN的理解的绝佳方法。对抗

样本利用了“一般情况下训练分类器存在困难”以及“在某种情况下

欺骗分类器相对容易”这两点。分类器必须对许多图像进行预测，并

且由于存在多个自由度，可以轻松加一个特殊的偏移量来愚弄分类

器。结果是可以在不会明显改变图像的情况下很容易地获得对抗噪声

而完全改变图像的标签。

对抗样本可以在AI的许多领域中找到，不仅是在深度学习和计算

机视觉这两个领域中。但正如你从代码中看到的，在计算机视觉领域

中创建对抗样本的代码并不困难。针对这些示例的防御措施是存在

的，同时本章也展示了使用GAN进行防御的示例，但对抗样本的问题还

远未完全解决。

10.8 小结

（1）因滥用问题空间的维度而产生的对抗样本是机器学习的一个

重要层面，因为它们展示了GAN能起作用以及某些分类器很容易被破坏

的原因。

（2）我们可以轻松生成带有真实图像和噪声的对抗样本。

（3）很少有有意义的攻击向量可用于对抗样本。

（4）可以使用GAN来抵御对抗样本的应用，包括网络安全和机器

学习公平性等。

[1] Intriguing Properties of

Neural Networks, by Christian

Szegedy et al., 2014.

[2] 视觉分类任务中人类表现的构成原因是一个复杂的话题，但是至

少在如Dota 2和Go中，AI以极大的优势击败了人类专家。

[3] 特征（ROC）曲线说明了假阳性和假阴性之间的权衡，第2章中也

提到过，更多详细信息可查阅维基百科。

[4] Adversarial Attacks on

Deep Learning Models in

Natural

Language Processing: A Survey,

by Wei Emma Zhang

et al.,

2019. 另见Adversarial Examples That

Fool Both ComputerVision

and Time-Limited Humans, by

Gamaleldin F. Elsayed et

al.,

2018.

[5] 这只是简要的总结，囿于篇幅所限，我们必须省略一些详细信

息，希望读者已经理解背后隐藏的细节了。如果不了解的话，建议选

择选一本书去详细地学习这些细节，例如Francois Chollet撰写的

Deep Learning with Python（曼宁出版社，2017年）。

[6] Image Classification on

ImageNet, at DAWNBench.

[7] Motivating the Rules

of the Game for

Adversarial Example

Research,by Justin Gilmer et

al., 2018.

[8] ICLR是指International Conference on

Learning

Representation，这是规模较小但很出色的机器学习会议之一。请查

看2018年Anish Athalye的Twitter。应当指出的是，作者还没有审查

另外3项对抗防御。

[9] The Robust Manifold

Defense: Adversarial Training Using

Generative Models,by Ajil Jalal

et al., 2019.

[10] Virtual Adversarial Training:

A Regularization Method

for Supervised and Semi-Supervised

Learning,by Takeru Miyato

et al., 2018.

[11] 这在ICLR 2019上是一个激烈争论的话题。尽管大多数的此类对

话都是非正式的，但使用（伪）可逆生成模型作为对图像的“样本

外”性质进行分类的方法似乎是一条卓有成效的途径。

[12] Jalal et al.,

2019,

https://arxiv.org/pdf/1712.09196.pdf.

[13] Black-Box Adversarial Attacks

with Limited Queries and

Information, by Andrew Ilyas

et al., 2018.

第11章 GAN的实际应用

本章主要内容

GAN在医学领域的应用

GAN在时尚领域的应用

除了生成手写数字和把苹果变橙子这些吸引人的应用，GAN还可以

用于更多领域。本章将探索一些实际应用。毫无疑问，本章重点讨论

现实中已经使用过GAN的领域，因为本书主要目标之一是提供必要的知

识和工具，不仅希望让读者了解迄今为止GAN在各个领域所取得的成

就，更希望帮读者找到自己感兴趣的新应用场景。要开始这段旅程，

没有比看看几个成功的例子是更好的起点了。

读者已经看到了几个创新用例——Progressive GAN不仅能创造出

逼真的人脸图像，还能创造出更具实用价值的数据样本（如乳腺X线图

像），CycleGAN通过将视频游戏中的片段转换成电影般的场景来创建

逼真的模拟虚拟环境，然后将其用于自动驾驶汽车的训练。

我们在本章详细回顾了GAN的已有应用，并将探讨促成这些应用的

动机，是什么使它们恰恰从GAN所带来的进步中受益，以及它们是如何

实现的。

具体来说，我们将着眼于GAN在医学和时尚领域的应用。选择这两

个领域的原因如下。

（1）它们不仅展示了GAN的学术价值，更展示了其商业价值，体

现了GAN的研究者如何利用学术进展解决现实世界的问题。

（2）它们使用了可以通过本书中涉及的工具和技术来理解的GAN

模型。我们不会引入新的概念，而是着重介绍如何将前面已经实现的

模型用于MNIST以外的应用。

（3）它们是不需要专业知识即可理解的，例如GAN在化学和物理

领域的应用对于没有相关专业背景的人来说，是很难完全明白的。

此外，所选场景和示例说明了GAN的通用性。医学领域展示了GAN

如何在数据有限的情况下发挥作用，时尚领域展示了另一个极端——

在有大量数据集可用的场景中探索GAN的应用。即使读者对医学或时尚

不感兴趣，你在本章中了解到的工具和方法也适用于无数其他场景。

遗憾的是，由于训练数据往往是专有的或难以获得的，将要回顾

的实际应用几乎无法在代码教程中再现，本书只能提供GAN模型及选择

此模型原因的详细说明，而无法像其他章节那样提供完整的代码。到

本章结束时，你应该完全有能力实现本章中的任何应用——只需对我

们在前面实现的GAN模型进行小小的修改，并提供给定例子或类似的数

据集即可。好了，让我们开始吧！

11.1 医学领域的GAN

在本节中，我们介绍GAN在医学上的应用，看看如何使用GAN生成

的合成数据来增大训练数据集，以帮助提高诊断的准确率。

11.1.1 利用GAN提高诊断准确率

机器学习在医学领域的应用面临着一系列的挑战，而这恰恰使得

该领域能够很好地受益于GAN。因为收集医疗数据十分困难，很难获得

足够大的训练数据集来满足监督学习的需求。[1] 获取医疗场景的样

本往往是成本高昂且不切实际的。

与任何人都能获得的用于光学字符识别（OCR）的手写字母数据集

或用于自动驾驶汽车训练的路况录像不同，医疗场景的样本更难获

得，而且通常需要用专门的设备来收集，更别提还要着重考虑病人隐

私——这限制了医疗数据的收集和使用。

除了获取医疗数据集存在困难，正确地标注这些数据也是一个挑

战，这一过程通常需要专家针对特定情况进行标注。[2] 因此，许多

医学应用无法从深度学习和人工智能的进步中受益。

目前，人们已经开发了许多技术，用于帮助解决标记数据集小的

问题。我们在第7章介绍了如何在半监督情况下使用GAN来增强分类算

法的性能，展示了SGAN如何在仅使用一小部分标签进行训练的情况下

获得较高的准确率。但这仅仅解决了医学研究者所面临问题的一半，

半监督学习在有一个大数据集但只有一小部分被标记的情况下很有帮

助。在许多医疗应用中，为数据集的一小部分添加标签只是问题的一

部分——这一小部分通常是仅有的数据！换句话说，根本没有同一领

域的成千上万的额外样本在等着被标记或在半监督情况下使用。

医学研究人员试图使用数据增强技术克服数据集不足的挑战。就

图像来说，通常是小的调整和转换，如改变尺寸（放大和缩小）、平

移（左右移动和上下移动）和旋转。[3] 这些策略允许通过单个样本

来创建许多其他样本，从而扩展数据集的大小。图11.1显示了计算机

视觉领域常用的数据增强示例。

（来源：Data Augmentation: How to

Use Deep Learning When

You Have

Limited Data, by Bharath

Raj, 2018.）

图11.1 通过改变现有数据来增强数据集的技术，包括改变尺寸（放大和缩

小）、平移（左右移动和上下移动）和旋转。尽管传统的数据增强技术可以有效

地增加数据集的大小，但它只能带来有限的多样性增加

可以想象，标准数据增强有很多限制——首先，小的修改产生的

样本不会与原始图像有太大差别。因此，增加的样本并没有增加太多

的变化来帮助算法学习泛化，[4] 例如，手写数字数据集希望包含更

多不同的书写风格呈现的数字6，而不仅是同一个基础图像的不同排

列。

在医学诊断中，我们需要同一基本病理的不同样本，使用合成样

本（如GAN生成的样本）增强数据集，可能会进一步丰富可用数据，使

其超越传统的增强技术——这正是以色列研究人员Maayan Frid￾Adar、Eyal Klang、Michal Amitai、Jacob

Goldberger和Hayit

Greenspan着手研究的。

鉴于GAN可以在几乎任何领域合成高质量的图像，Frid-Adar和他

的同事决定探索GAN在医学数据增强方面的应用。他们选择将重点放在

提升肝脏病变分类的准确率上。他们关注肝脏病变的一个主要动机是

肝脏作为转移性癌症最常见的3个部位之一，仅2012年就造成74.5万余

人死亡。[5] 因此，能帮助医生诊断高危患者的机器学习工具和模

型，有可能挽救无数患者的生命并改善无数患者的预后结果。

11.1.2 方法

Frid-Adar和他的团队进退维谷：他们的目标是通过训练GAN来扩

充一个小的数据集，但是GAN本身需要大量的数据来训练。也就是说，

若想要使用GAN来创建一个大的数据集，首先需要一个大的数据集来训

练GAN。

他们想到了巧妙的解决办法：第1步，使用标准的数据增强技术来

创建更大的数据集；第2步，使用该数据集来训练GAN来生成合成样

本；第3步，他们使用第1步的增强数据集和第2步的GAN生成的合成样

本来训练肝脏病变分类器。

研究人员使用的GAN模型是第4章涉及的DCGAN的变体。为了证明

GAN在广泛的数据集和场景中的适用性，Frid-Adar等人只需稍作调整

和自定义，就可以把DCGAN用于他们的案例。如图11.2所示，模型中唯

一需要调整的部分是隐藏层的尺寸，以及生成器网络输出和鉴别器网

络输入的尺寸。

与MNIST数据集中尺寸为28×28×1的图像不同，此处GAN处理的是

尺寸为64×64×1的图像，Frid-Adar等人在论文中指出，他们使用了

5×5的卷积核，但这只是对网络超参数的一个小小改变。除了训练数

据给定的图像大小，这些调整都是通过反复试验确定的，研究人员不

断调整参数直到模型产生令人满意的图像。

（来源：Frid-Adar et al., 2018.）

图11.2 Frid-Adar等人采用DCGAN模型生成肝脏病变的合成图像来扩充数据集，

以提高分类准确率。模型结构类似于第4章中的DCGAN，强调了GAN在广泛的数据集

和场景中的适用性

（注意，图中只显示了伪样本的GAN流程图）

在回顾由Frid-Adar和他的团队设计的方法多有效之前，我们先停

下来看看自己对GAN的理解有多大的进步吧。我们在本书的第4章了解

了足够多的GAN知识，并将其应用于实际场景中。

11.1.3 结果

Frid-Adar和他的团队使用DCGAN进行数据增强，与基准（仅使用

传统的数据增强）相比，在分类准确率方面取得了显著提高。结果总

结在图11.3中，其中显示了分类准确率（ 轴）随着训练样本数量（

轴）的增加而增加。

线状虚线展示了通过使用GAN生成的合成样本来增强数据集所实现

的额外的准确率增加。Frid-Adar等人从增加传统增强样本不再提高准

确率的点开始，加入DCGAN生成的合成数据，分类的性能从0.8左右提

高到0.85以上，证明了GAN的有效性。

肝脏病变分类的改进只是众多数据受限的医学应用场景之一，它

们可以从GAN生成的合成样本来进行数据增强中获益。例如，由伦敦帝

国理工学院的Christopher Bowles带领的研究组利用GAN（第6章中讨

论的Progressive GAN）来提高脑分割任务的表现。[6] 性能的提高可

以解锁模型在实践中的应用，特别是在医学这样的领域，准确率可能

意味着生与死的区别。

（来源：Frid-Adar et al., 2018.）

图11.3 图显示了使用两种数据增强策略添加新样本时的分类准确率：标准/传统

数据增强；用DCGAN合成的样本增强。使用传统增强（点状虚线），分类性能峰值

在0.8左右。使用GAN生成样本（线状虚线）可以将准确率提高到0.85以上。点状

虚线表示使用传统数据增强的分类性能，随着新的（增强的）训练样本数量的增

加，性能也会提高，但是准确率的提高停滞在0.8左右，超过这个范围，再增加样

本也不能改进准确率了

接下来让我们转换思路，探索GAN在一个风险更低、考虑的因素和

面临的挑战完全不同的领域中的应用：时尚。

11.2 时尚领域的GAN

与很难获得数据的医学领域不同，在时尚领域，研究人员幸运地

拥有庞大的数据集。诸如Instagram和Pinterest之类的网站上有不计

其数的服装和衣物的图片，亚马逊和eBay等零售巨头则有数百万商品

的购买数据，从袜子到裙子，无所不包。

除了数据可用性，许多其他特性使得时尚领域非常适合AI的应

用。时尚品位因客户而异，个性化内容的能力有可能带来巨大的商业

利益。此外，时尚潮流变化频繁，对于品牌和零售商来说，快速反应

并适应顾客不断变化的偏好至关重要。

在本节中，我们将探索GAN在时尚领域的一些创新应用。

11.2.1 利用GAN设计服装

从无人机送货到无收银员的杂货店，亚马逊有关“未来主义”的

努力频上头条。2017年，亚马逊利用GAN技术培养AI时装设计师的雄心

壮志使他们又获得了一次头条报道。[7] 令人遗憾的是，这篇发表在

《麻省理工学院科技评论》上的文章仅提到使用GAN设计符合特定风格

的新产品，但缺乏必要的细节。

幸运的是，来自Adobe和加州大学圣地亚哥分校的研究人员发表了

一篇有着同样目标的论文[8]。他们的方法揭开了亚马逊AI研究实验室

试图重塑时尚的神秘面纱。使用从亚马逊搜集来的一个包含成千上万

的用户、商品和评论的数据集，文章的第一作者康望程和他的合作者

训练了两个独立的模型：推荐和创造模型。[9]

我们可以将推荐模型视为一个黑匣子，我们唯一需要知道的是它

的功能：对于任何用户-商品对，它都返回一个偏好得分，分数越高，

越符合该用户的品味。这并无与众不同之处。

创造模型更加新颖和有趣——不仅因为使用了 GAN，还得益于康

望程的团队设计了两个创造性的应用：一是创造一个符合给定用户时

尚品位的新时尚单品；二是基于用户的时尚偏好给出对现有时尚单品

的个性化修改。

下面我们将探讨他们是如何实现这些目标的。

11.2.2 方法

首先是模型，他们使用的是CGAN，并将商品的类别作为条件标

签。数据集分为6类：上衣（男式和女式）、裤子（男式和女式）和鞋

子（男式和女式）。

在第8章中，我们使用MNIST的标签来教CGAN生成所需的任何手写

数字。康望程等人以类似的方式使用类别标签训练CGAN来生成属于给

定类别的时尚单品。即使要分类的是衬衫和裤子而不是数字3和4，

CGAN模型的设置几乎与第8章中实现的相同，生成器利用随机噪声 和

条件信息（标签/类别 ）来合成图像，鉴别器输出给定的图像-类别对

是真实的概率。图11.4详细展示了他们使用的网络结构。

（来源：Kang et al., 2017,

https://arxiv.org/abs/1711.02231.）

图11.4 康等人在他们的研究中使用的CGAN生成器和鉴别器网络的结构。标签

代表服装的种类，研究人员使用它作为条件标签来指导生成器合成符合给定类

别的图像，并使用鉴别器来识别出真实的图像-类别对

每个框表示一个层；fc代表全连接层；st为卷积核的步长，其尺

寸（宽×高）为conv/deconv层前两个数字；conv表示使用常规卷积

层，deconv表示使用转置卷积层。conv/deconv之后的数字决定层的深

度，等效地来说是使用的卷积滤波器的数量；BN意为给定层的输出上

使用了批归一化处理；还要注意使用的是最小二乘损失而不是交叉熵

损失。

他们用CGAN为数据集中的每个顶级类别生成逼真的服装单品，并

在两个具有重大实用潜力的应用上进行了测试：创造新的个性化单品

以及对现有单品进行个性化更改。

11.2.3 创造新单品以符合个人偏好

为了确保生成的单品图像符合个人的时尚偏好，他们想出了一个

巧妙的方法：考虑到推荐模型是根据个体对给定商品的喜爱程度来给

现有商品打分的，因此如果生成的新单品能够最大化这个偏好得分，

那么很可能会生成符合个人风格和时尚品位的单品。

康望程等人借用经济学和选择理论中的一个术语，[10] 将此过程

称为偏好最大化（preference maximization）。这一方法的独特之

处在于，可能生成单品的范围没有仅限于训练数据的语料库，或者整

个亚马逊商品目录，由于使用了CGAN，他们可以将生成的新单品微调

到几乎无限的粒度（对于域对象来说，也就是实体分得越细，则粒度

越细）。

他们要解决的下一个问题是确保CGAN的生成器能够创造出能最大

限度地满足个人喜好的时尚单品，毕竟CGAN所接受的训练是只能为给

定类别而不是给定的人去生成逼真的图像。一种可能的选择是继续生

成图像并检查偏好得分，直到遇到一个得分足够高的图像。然而，考

虑到生成的图像千变万化，这种方法将非常低效且耗时。

康望程的团队把此问题框架化成一个优化问题来解决——约束最

大化。约束（算法必须在其中运行的边界）是由向量 的大小给出的潜

在空间的大小，他们使用标准大小（100维向量），每个数字在

[−1,1]范围内；而且为了使这些值可微（以便在优化算法中使用），

将向量 中的每个元素设置为tanh函数，并随机初始化。

随后他们采用了梯度上升法——梯度上升（gradient

ascent）。与梯度下降类似，梯度下降是在最陡下降方向上迭代地移

动，以最小化代价函数；而梯度上升是在增幅最大地方向迭代地移

动，以最大化奖励函数（在本例中，奖励函数是由推荐模型给出的得

分）。

结果如图11.5所示，将来自数据集的前3张单品图与6个不同用户

的前3张合成单品图进行了比较。能证明此解决方案独创性的点在于生

成的单品具有更高的偏好得分，这表明它们更适合用户的风格和偏

好。

（来源：Kang et al., 2017,

https://arxiv.org/abs/1711.02231.）

图11.5 康望程等人在文中给出的结果，每个单品图像都用其偏好得分进行了注

释。每行显示不同用户和单品类别（男式和女式上装、男式和女式下装、男式和

女式鞋）的结果

左边的3列显示来自数据集的得分最高的单品，右边的3列显示合

成的得分最高的单品，根据偏好得分，生成的单品更符合用户的品

位。

研究者们并没有就此止步，除了创造新单品，他们还探索了开发

的模型是否可以用于对现有单品进行修改，以适应用户的风格。鉴于

时尚先锋们品味的高度主观性，拥有改变一件衣服直到它“恰到好

处”地符合消费者偏好的能力，具有巨大的潜在商业利益。接下来看

看他们是如何解决这个挑战的。

11.2.4 修改现有单品以更符合个人偏好

潜在空间中的数字（由输入向量 表示）具有现实意义，并且在数

学上彼此接近的向量（由它们在高维空间中的距离度量）往往会产生

内容和风格相似的图像。因此，正如康望程等人指出的那样，为了生

成某些图像A的变体，需要做的就是找到生成器用来创造图像的潜在向

量 ，然后就可以从相邻的向量来生成相似的图像。

为了避免抽象，我们来看一个使用MNIST数据集的具体示例。假设

输入向量 输入生成器，会产生数字9的图像，如果我们输入向量 ，

数学层面来说在向量占据的100维潜在空间中非常接近 ，那么 会产

生另一个稍微不同的数字8的图像，如图11.6所示。在变分自编码器的

环境中，中间/压缩表示的工作方式就像GAN中的 一样。

图11.6 在潜在空间内移动得到的数字9的变化（图像来自第2章），附近向量在

同一个数字上产生变化。例如，当在第1行中从左向右移动时，数字9一开始稍微

向右倾斜，最终完全竖直；当移动到足够远的地方时，数字9会变形为另一个在视

觉上相似的数字。类似这样的渐进式变化同样适用于更复杂的数据集，其中的变

化往往更微妙

当然，在时尚领域，事情就更微妙了——毕竟，裙子的图片比数

字的灰度图像要复杂得多。在一个向量周围的潜在空间中移动，比如

创建T恤的图像，可以生成不同颜色、图案和样式（如V领T恤和圆领T

恤）。这完全取决于编码的类型和生成器在训练中内化的含义，而最

好的办法就是尝试。

这就带来了康望程的团队必须克服的下一个挑战。要使上述方法

奏效，我们需要改变图像的向量 。想修改合成的图像是很简单的：可

以在每次生成图像时记录向量 ，以便以后参考。使场景变得复杂的是

想要修改真实的图像。

根据定义，真实的图像不能由生成器生成，所以没有向量 。我们

能做的最好的事情就是找到尽可能接近要修改成图像的合成图像的潜

在空间表示。换句话说，我们必须找到一个向量 ，以供生成器合成

接近真实图像的图像，并用它来代替会产生真实图像的假设的 。

这正是康望程等人所做的。和前面一样，先将场景描述为一个优

化问题，然后根据所谓的重建损失（两幅图像之间差异的一种衡量，

损失越大，给定的一对图像之间的差异就越大）定义损失函数。以这

种方式表述了问题以后，再使用梯度下降法（最小化重建损失）迭代

地为任何真实图像寻找最接近的合成图像。一旦有了一个与真实图像

相似的伪图像（因此也有了生成真实图像的向量 ），就可以通过潜在

空间的操作来修改过来。

这就是康望程的团队设计的方法充分展示其潜力的地方。我们可

以在潜在空间中移动，直到生成类似想要修改成的图像的图像点，同

时还可以针对给定用户的偏好进行优化。我们可以在图11.7中看到这

个过程：当我们在每一行中从左到右移动时，衬衫和裤子逐渐变得更

加个性化。

（来源: Kang et al.,

2017, https://arxiv.org/abs/1711.02231.）

图11.7 6名购物者（3男3女）使用相同的初始图像的个性化流程：男性为polo

衫，女性为裤子

例如他们观察到，第1行的人想要更多样的颜色，第5行的人似乎

更喜欢明亮的颜色和模特忧郁的表情，最后一个人似乎更喜欢裙子而

不是牛仔裤。这是超个性化定制的表现，难怪亚马逊会注意到这一

点。

最左边的照片展示了来自训练数据集的真实单品；左边的第2张照

片展示了生成的最接近真实的图像——它被用作个性化过程的起点。

每个图像都用其偏好得分进行了注释，当从左到右移动时，单品会针

对给定的用户逐步进行优化。随着得分的增加，个性化过程提高了单

品与给定用户的风格和品味相匹配的可能性。

11.3 结论

本章所涉及的应用仅触及了GAN所能实现的功能的皮毛。仅在医学

领域和时尚领域就存在无数其他用例，更不必说其他领域了。可以肯

定的是，GAN已经扩展到了学术界以外的领域，无数的应用都在利用

GAN合成逼真数据的功能。

11.4 小结

（1）由于GAN的通用性，它们可以用于广泛的非学术应用，并且

很容易用于MNIST以外的场景。

（2）在医学领域，GAN生成的合成样本可以提高分类的准确率，

并能超出传统数据集增强技术所能达到的水平。

（3）在时尚领域，GAN可用于创造新单品和修改现有单品，以更

好地符合消费者的个人风格。这是通过生成能使推荐算法的偏好得分

最大化的单品图像实现的。

[1] Synthetic Data Augmentation

Using GAN for Improved

Liver

Lesion Classification, by Maayan

Frid-Adar et al., 2018.

[2] Synthetic Data Augmentation

Using GAN for Improved

Liver

Lesion Classification, by Maayan

Frid-Adar et al., 2018.

[3] 同上。

[4] Synthetic Data Augmentation

Using GAN for Improved

Liver

Lesion Classification, by Maayan

Frid-Adar et al., 2018.

[5] Cancer Incidence and

Mortality Worldwide: Sources,

Methods, and Major Patterns

in GLOBOCAN 2012,by J.

Ferlay et

al., 2015, International Journal

of Cancer.

[6] GAN Augmentation: Augmenting

Training Data Using

Generative Adversarial Networks,by Christopher

Bowles et al.,

2018.

[7] Amazon Has Developed

an AI Fashion Designer,

by Will

Knight, 2017, MIT Technology

Review.

[8] This AI Learns

Your Fashion Sense and

Invents Your Next

Outfit, by Jackie Snow,

2017, MIT Technology Review.

[9] Visually-Aware Fashion Recommendation

and Design with

Generative Image Models, by

Wang-Cheng Kang et al.,

2017.

[10] Introduction to Choice

Theory, by Jonathan Levin

and

Paul Milgrom, 2004.

第12章 展望未来

本章主要内容

生成模型的伦理问题

预计未来几年内将占主导地位的3项最新改进：RGAN、SAGAN和

BigGAN

关于以上3种前沿技术的更多阅读材料

关于本书主题和要点的总结

在最后一章，我们想简单阐述一下对GAN伦理问题的看法，还会讨

论一些有可能在未来变得更重要的创新。我们会谈到期望能够定义GAN

未来发展的一些最新思路，但不会给出代码。我们希望你已准备好迎

接即将到来的GAN之旅——即使是撰写本书时尚未发表的进展。最后，

请允许我们总结全书并做个依依不舍的道别吧！

12.1 伦理问题

全世界都开始意识到人工智能的伦理——包括GAN的伦理——是一

个重要的问题。一些机构已决定不发布其昂贵的、经过预先培训的模

型，以防被作为生成假新闻的工具滥用[1]。许多文章提到GAN可能有

潜在的恶意用途。[2]

我们都知道，错误的信息可能是一个严重的问题，因此可以生成

照片般逼真的合成图像的GAN可能会带来危险。

这不是一本关于人工智能伦理的书，我们因此只是简单地谈谈这

个话题。但我们坚信，所有人必须思考自己所做工作的道德规范，以

及可能带来的风险和意外后果。鉴于人工智能是一种可扩展升级的技

术，考虑我们所做的是否有助于创造一个理想中生活的世界是十分重

要的。

你应该思考自己的原则，并至少完成一项更加完善的道德准则的

学习。本书不打算讨论哪项准则更好——毕竟，人们还没有就更具现

世意义的问题达成道德准则上的共识——但是如果你还没有读过任何

其中一个，至少去了解一项完善的关于此方面的道德准则。

注意

你可以阅读谷歌的人工智能准则。人工智能与深度学习伦理道德

研究所详细介绍了这一准则。另见Larry Dignan在2017年撰写的

IBM’s Rometty Lays Out

AI Con-siderations, Ethical

Principles。

例如，被称为DeepFakes的技术虽然最初不是基于GAN，但已经被

许多人作为谈论道德担忧时的例子提及。[3] DeepFakes——一个深度

学习和伪图像的宝库——由于生成虚假的政治视频和合成非自愿的情

色内容，已经引起了争议。这项技术可能很快会发展到无法判断视频

或图像是否真实的地步，而且鉴于GAN合成新图像的能力，它可能很快

就会主导这一领域。

仅仅是说每个人都应该考虑他们的研究和代码的后果似乎是不够

的，但现实是没有哪种灵丹妙药能够解决所有问题。不管是在学术研

究还是在工业领域，我们都应该考虑这些影响，即使最初的关注点完

全是出于道德考虑。我们不想沉闷地说教，也不想给一个毫无根据的

媒体博人眼球的预测，但这的的确确是本书深切关心的问题。

人工智能的伦理俨然已成为一个真正的问题，前面已经提出了人

工智能生成虚假新闻、合成虚假的政治视频和非自愿的情色内容这3个

实际问题。实际的情况很复杂——有些人认为在人脸生成中GAN有青睐

女性面孔的倾向。换另一个角度，GAN也有帮助人工智能更符合伦理的

潜力，例如，在半监督环境中，通过合成面部识别问题中代表性不足

的类来提高在占席位较少群体中的分类质量。

编写本书的部分目的是让每个人都能更加了解GAN的能力和可能造

成的滥用。我们对GAN未来的学术和实际应用以及正在进行的研究感到

兴奋，但也意识到一些应用可能有消极作用，因为不可能“取消发

明”这样一项技术，所以必须意识到其能力。这绝不是说，如果GAN不

存在，世界将变得更好。GAN只是一种工具，众所周知，工具可能会被

滥用。

出于道义，我们必须谈论这种技术的前景和危险，否则一旦被一

小部分人滥用，就会一发不可收拾。尽管本书不是写给大众看的，但

我们希望除了目前在GAN领域占主导地位的学术界，这是迈向更广泛大

众认识的踏脚石。同样，我们正在进行的许多公众宣传活动，希望有

助于促进人们对该主题的更多了解和讨论。

随着越来越多的人意识到这项技术，即使是现在仍存在恶意破坏

分子，也将无法再欺骗任何人了。我们希望GAN永远不会成为恶意行为

的源头，虽然可能过于理想化。接下来最好的办法是人人都能了解

GAN，而不仅是学术界和恶意破坏团伙。我们也希望（目前所有证据似

乎指向了这一现实）GAN可以对艺术、科学和工程做出积极的贡献。人

们也在致力于DeepFake的检测，并借鉴了来自GAN和对抗样本的想法，

但必须谨慎，因为任何能够准确检测到这些DeepFake的分类器，同样

会为那些设法骗过它的虚假样本增添更多的可信度。

在许多方面，我们也希望开始更彻底的对话，而不是哗众取宠

——其实这是一个通过图书论坛或推特账户与我们联系的邀请。我们

意识到需要从不同的角度继续审视道德准则，也知道这些事情会随着

时间的推移而发展，特别是随着GAN的应用案例的增加而更加清晰。事

实上，有些人——比如硅谷著名风投公司安德森•霍洛维兹基金

（Andreessen Horowitz Fund）的Benedict Evans——认为规范或谈

论人工智能的伦理，并没有比谈论数据库的伦理更有意义，真正重要

的是使用方法，而不是技术本身。

12.2 GAN的创新

说到使用案例，我们知道GAN是一个不断发展的领域，因此想快速

介绍一些可能目前在学术界还没有先前章节中的一些主题那么成熟的

内容，但期望这些内容在未来会具有重要意义。秉承实用性的精神，

我们挑选了3个有趣且实用的GAN创新：一篇实用论文（RGAN）、一个

Github项目（SAGAN）和一项艺术应用（BigGAN）。

12.2.1 相对生成对抗网络（RGAN）

我们很少在论文原稿中看到如此简单和优雅但功能强大到足以击

败许多最先进算法的报道——相对生成对抗网络（Relativistic

GAN，RGAN）就是这样一个例子。RGAN的核心思想是，除了原始的

GAN（特别是在第5章中提到的NS-GAN），在生成器中添加了一个额外

的项，迫使生成器生成的数据看起来比实际数据更真实。

换言之，除了使伪数据看起来更真实，生成器还应使真实数据看

起来相对不太真实，从而可以提高训练的稳定性。当然，生成器能控

制的唯一数据是合成数据，因此只能相对地实现此目的。

RGAN的作者将其描述为WGAN的一个通用版本。我们从表5.1中的简

化损失函数开始。式 12.1 描述了鉴别器的损失函数，该函数衡量了

实际数据（ )）和生成数据 之间的差异；式12.2描述了生

成器的损失函数，该函数试图使鉴别器相信所看到的样本是真实的。

式12.1

式12.2

回到RGAN最接近的一种GAN——WGAN。要使生成的分布看起来真

实，必须移动一定的概率质量，WGAN要使这一概率质量最小化。从这

个意义上讲，RGAN和WGAN有许多相似之处（例如，鉴别器通常被称为

批评者，而WGAN在本文中是作为RGAN的一个特例展示）。归根结底，

这两种方法都是用唯一的数字来衡量所处的现状，还记得推土机距离

吗？

RGAN的创新之处在于不用再获得以前那种生成器总是在扮演追赶

者角色的无用动力了，换句话说，生成器正试图生成比实际数据更真

实的数据，这样它就不会总是处于防御状态。因此， )可以解释为

实际数据比生成数据更真实的概率。

在深入探讨差异之前，我们先引入一种略有不同的表示，以近似

原论文中所用的表示方法，但更加简化。在式12.3和式12.4中， )

充当类似于WGAN中的批评者角色，[4] 可以将其视为鉴别器， ()定义

为log(sigmoid( ))。原论文中用 代替 )表示伪样本，用带下标

的 表示真样本，但我们将遵循前几章中更简单的表示法。

式12.3

式12.4

以上等式只体现生成器中的一个关键区别：真实数据现在被添加

到损失函数中了，这个看似简单的技巧使生成器不再永久处于劣势。

为了在理想化的环境中理解这一点和另外两个观点，图12.1绘制出了

不同的鉴别器输出。

（来源：The Relativistic Discriminator: A

Key Element Missing from

Standard GAN, by Alexia

Jolicoeur-Martineau, 2018.)

图12.1 在散度最小化情况下（图（a）），生成器总是在追赶鉴别器（因为散度

总是0）。图（b）中展示了“好的”NS-GAN训练的过程，同样，生成器也不可能

赢。在图（c）中可以看到，现在生成器能够获胜，但更重要的是无论在哪个训练

阶段，生成器总是有一些要争取的东西（因此要恢复有用的梯度）

你可能会想，为什么仅增加这样一个项就值得关注呢？因为这个

简单的增加仅需很少额外的计算成本就能使得训练更加稳定。这一点

很重要，尤其是第5章中提及的Are GAN Created Equal?一文的作者认

为，迄今为止所有主要的GAN模型结构在针对额外的处理需求进行调整

时，比原始GAN仅有有限的改进。这是因为许多新的GAN模型架构只有

消耗巨大的计算成本才会更好，这使得它们的用处更小，但是RGAN有

可能全面改变GAN架构。

注意，即使一个方法可能只需要较少的更新步骤，但若是每个步

骤由于额外的计算花费而需要两倍的时间，这样做真的值得吗？大多

数会议的同行评议过程并不能避免这种弱点，所以你必须小心。

应用

你可能会问为什么这在实践中十分重要？在不到一年的时间里，

RGAN的论文已被引用了50多次，这对一位名不见经传的作者来说已经

是很高的引用量了。此外，人们已经使用RGAN发表了应用方向的文

章，例如用RGAN实现了最先进的语音增强（有史以来最佳性能），击

败了其他基于GAN和非GAN的方法。[5]

详细解释这篇论文已经超出了本书的范围，此处不再赘述。

12.2.2 自注意力生成对抗网络（SAGAN）

下一个将改变当前 GAN 发展格局的创新是自注意力生成对抗网络

（Self-Attention GAN，SAGAN）。注意力是建立在一个非常人性化

的观点之上的，即人是如何看待这个世界的——一次只能聚焦在有限

的地方[6]。GAN的注意力也是如此：人的意识只能有意地关注桌子的

一小部分，但是大脑能够通过快速微小的眼球运动（即扫视）将整张

桌子拼接在一起，同时仍然只聚焦于图像的一部分。

类似的原理在自然语言处理和计算机视觉等领域已有广泛的应

用。注意力可以帮助解决卷积神经网络（CNN）忽略图片大部分的问

题。众所周知，CNN依赖于由卷积大小决定的小的感受野。第5章中，

在GAN中感受野的大小很可能会导致问题出现——例如生成有多个头部

或身体的奶牛，但GAN并不会认为它们奇怪。

当生成或评估图像的子部分时，程序可能会看到一条腿出现在一

个区域，但无法看到其他的腿已经存在于另一个区域。这可能是因为

卷积忽略了物体的结构，或者因为腿或腿的旋转是由不同的、更高层

次的神经元来表示的——这些神经元彼此没有通信。经验丰富的数据

科学家会记得这是Hinton的CapsuleNets试图解决的问题，但并没有真

正奏效。没有人能绝对肯定地说出为什么注意力可以解决这个问题，

一种好的解释方式是它可以创建一个具有灵活感受野（形状）的特征

检测器聚焦在给定图片的几个关键区域（图12.2）。

当图像大小为512×512时，这个问题尤为明显——因为常用的最

大卷积大小是7，所以这是大量特征被忽略的关键所在！即使在更高层

次的节点中，神经网络也可能无法检查是否合适，例如处在正确位置

的头部。因此，只要奶牛有一个头挨着身体，网络就不关心任何其他

的头，但这种结构是错误的！

（来源：Convolution Arithmetic, by vdmoulin,

2016.）

图12.2 输出像素（2×2块）会忽略除突出显示地小区域之外的其他内容，注意

力帮助解决此问题

这些更高层次的表现更难解释，研究人员还没有就这种情况发生

的确切原因达成共识，但从经验上看，网络确实没有学到这些信息。

注意力使我们能够选择任何形状或大小的相关区域，并恰当地考虑它

们。参考图12.3来观察注意力可以灵活关注的区域类型。

（来源：Self-Attention Generative Adversarial Networks,by

Han Zhang,

2018.）

图12.3 在给定具有代表性的查询定位的情况下，注意机制最关注的图像区域。

可以看到注意力机制通常关心不同形状和大小的区域，这是一个好兆头，因为我

们希望它能挑出图像决定对象类型的区域

应用

DeOldify是Jeremy Howard教授的fast.ai课程的学生Jason Antic

制作的流行的SAGAN应用之一。DeOldify使用SAGAN为老旧的图像和绘

画着色，并达到了一个惊人的准确水平。图12.4展示将著名的历史照

片和绘画变成全彩色版本。

（a） （b）

图12.4 Deadwood, South Dakota，1877年。这是一本黑白印刷书中的插图，图

（b）中的图像实际上已经着色了

12.2.3 BigGAN

另一个让世界瞩目的网络是BigGAN。[7] BigGAN在ImageNet的所

有1000类图像上实现了高度逼真的512×512图像生成，在此之前，这

一壮举被认为在现有的GAN中几乎是不可能实现的。BigGAN取得了3倍

于之前最佳算法的视觉得分。简而言之，建立在SAGAN和频谱归一化的

基础上的BigGAN在5个方向上进行了进一步创新。

（1）把GAN扩展到以前难以置信的计算规模。BigGAN的作者们使

用了8倍于之前的批处理大小来训练，这是他们成功的一部分原因——

此举已经能使性能提高46%。理论上，训练BigGAN所需的资源总计价值

5.9万美元。[8]

（2）BigGAN的结构与SAGAN相比，每层中的通道（特征图）数量

是后者的1.5倍——这可能要归因于所用数据集的复杂性。

（3）如果通过控制对抗过程来提高生成器和鉴别器的稳定性，可

以带来总体上更好的效果。这一方法背后的数学知识超出了本书讨论

的范围，如果你对此感兴趣，可以从了解频谱归一化开始。不感兴趣

的人则大可放心，因为即使是文章作者，在后面的训练中也放弃了这

种策略，而且由于计算成本过高而使模式崩溃。

（4）引入截断技巧（truncation trick），给出了一种控制多

样性与保真度之间平衡的方法。如果在接近分布中心的位置采样（截

断），截断技巧可以获得更好的等效结果。这是有意义的——因为将

产生更好的样本，这也是BigGAN拥有“最丰富经验”之处。

（5）作者还介绍了另外3个理论进展，但是根据作者自己的结

果，这些操作似乎只对分数产生轻微的影响，还常导致稳定性降低。

实际上它们对提高计算效率很有用，但此处不予讨论。

应用

BigGAN一个迷人的艺术应用是Ganbreeder App，这得益于预先训

练的模型和Joel Simon的辛勤工作。Ganbreeder是一个基于网页的交

互式（免费的）探索BigGAN潜在空间的方法，作为一种创造新图像的

技术，已被广泛应用于艺术领域。

你既可以探索两个相邻的潜在空间，也可以在两个不同图像域的

两个样本之间使用线性插值来创建新图像。图12.5显示了一个在

Ganbreeder上进行创作的例子。

（来源：Ganbreeder）

图12.5 每次单击Make Children按钮，Ganbreer都会提供附近潜在空间中的一

系列突变，从而生成下面的3幅图像。可以从自己的样本或他人的样本开始，从而

使之成为一个合作的练习。这就是Crossbreed部分用途，可以从潜在空间的其他

部分选择另一个有趣的样本并混合。最后在Edit-Genes中可以编辑参数（在本例

中为城堡和石墙），并将更多或更少的特征添加到图片中

BigGAN值得进一步关注，因为DeepMind免费提供了所有这些计

算，并将预训练模型上传到了TensorFlow Hub上（TensorFlow Hub是

在第6章中使用的机器学习代码仓库）。

12.3 拓展阅读

碍于篇幅，我们无法涵盖许多在学术界和工业界的工作中日益风

靡的其他主题。我们将列出至少3个主题，并希望已经为你准备好了理

解这些文章所需的知识和工具。之所以只选了3个，是因为预计这一部

分会随GAN的发展变化得很快。

（1）Style GAN。Style GAN将GAN和“传统的”风格迁移相结

合，让用户可以更好地控制生成的输出。NVIDIA的CGAN已经成功产生

了惊人的全高清结果，并具有从更精细的细节到整体图像的多个控制

级别。这项工作建立在第6章的基础上，所以在深入研究此文之前，你

可能需要重新阅读一下。

（2）谱归一化（spectral normalization）。这是一种复杂的

正则化技术，需要用到高级的线性代数原理。现在只需记住它的用

途，即通过规范化网络中的权重来满足特定的属性来稳定训练，这正

是WGAN（第5章）所要求的。谱规归一化的作用和梯度惩罚相似。

（3）SPADE。SPADE又名GauGAN，是于2019年发布的最新成果。

就像第9章一开始叙述的那样，SPADE旨在基于图像的语义图合成真实

图像。图像分辨率可以达到512×256像素。这可能是3项技术中最具挑

战性的一项，但也是最吸引媒体关注的一项——该技术的演示令人印

象深刻！

GAN的领域中有太多的事情在更新换代，所以本节涉及的内容不可

能一直保持最前沿，但是希望就道德伦理和最新的有趣论文而言，我

们已经提供了研究这个不断发展的领域中问题的所需资源，更希望你

能很好地理解本章提出的GAN背后的创新。这些方法是否都会成为人们

惯用技术的一部分还是未知数，但我们觉得很有可能，也希望本节所

列的最新创新同样能实现这一愿景。

12.4 回顾与总结

我们希望书中所讨论的前沿技术能为你提供继续探索GAN足够的背

景材料，即使本书已接近尾声。在旅程结束之前，值得回顾一下我们

所学到的一切。

我们首先对GAN是什么以及它们是如何工作的进行了基本的解释

（第1章），并实现了GAN的一个简单版本（第3章）；使用一个简单版

本的自编码器介绍了生成模型（第2章）；介绍了GAN的理论（第3章和

第5章）及其缺陷，还有克服缺陷一些方法（第5章），这为后续章节

提供了基础知识和工具。

我们实现了几个最典型和最有影响力的GAN变体——DCGAN（第4

章）和CGAN（第8章），以及一些最先进和最复杂的——PGGAN（第6

章）和CycleGAN（第9章）；实现了SGAN（第7章），这是一个旨在解

决机器学习中最严重的缺点之一——缺少大量标记数据集的GAN变体；

还探讨了GAN的许多实际和创新应用（第11章），并给出了对抗样本

（第10章）——这些样本对所有机器学习都是一个挑战。

一路上，我们不断扩展理论和实践的工具箱。从IS和FID（第5

章）到像素级特征归一化（第6章）、批归一化（第4章）和

dropout（第7章），我们学习了能很好地为GAN和其他领域服务的一些

概念和技术。

回顾全书，值得强调的是探索GAN时一次又一次出现的几个主题。

（1）就实际应用的要求和应对理论上的约束而言，GAN 非常通

用。这在第 9 章中的CycleGAN案例中最显而易见。这项技术不但不受

前人需要成对数据的限制，而且几乎可以在任何域的样本之间进行转

换——从苹果到橙子，从马到斑马。GAN的多功能性在第6章中也很明

显，PGGAN可以学习生成与人脸和医学乳腺X线照片一样真实的图像；

在第7章中只需要做一些调整，就可以将鉴别器变成一个多分类器。

（2）GAN既是一门科学，也是一门艺术。对于GAN——一般来说乃

至是深度学习——的瑰丽和缺陷，我们对它们为何在实践中如此有效

的理解是有限的。很少有已知的数学证明，大多数成就只是实验中获

得的。这使得GAN容易受到许多训练陷阱的困扰，例如第5章中讨论过

的模式崩溃。幸运的是，研究人员发现了许多技巧，可以极大地缓解

这些挑战，从输入预处理到优化器和激活函数的选择，其中许多已经

在本书代码教程中学习实践过。正如本章所述的GAN变体所示，改进

GAN的技术正发展得如火如荼。

除了训练上的困难，即使像GAN这样强大又通用的技术，也有重要

的局限。GAN被许多人誉为赋予机器创造力的技术，这种说法在某种程

度上是正确的。在短短的几年内，GAN已经无可争议地成为合成伪数据

方面最先进的技术，但是还达不到与人类的创造力相匹敌的地步。

事实上，正如本书中一次又一次地展示的那样，GAN几乎可以模仿

任何现有数据集的特征并生成看起来像来自该数据集的样本。然而由

于其本身的性质，GAN不会偏离训练数据太远。例如，如果有一个古典

艺术杰作的训练数据集，GAN生成的样本将更像米开朗琪罗而不是杰克

逊•波洛克（美国画家，抽象表现主义绘画大师）。在一个新的能给予

机器真正自主性的人工智能模式出现之前，在赋予机器真正的自主性

之前，终究还是要由人类研究人员来引导GAN达到预期目标。

在尝试使用GAN及其应用时，你不仅要记住书中涵盖的实用技术、

提示和技巧，还要牢记本章中讨论的道德伦理方面的考量。借此，祝

你在未来的旅途中一切顺利！

12.5 小结

（1）本章谈到了AI和GAN的伦理问题，并讨论了道德准则、提高

认识和开放讨论的重要性。

（2）本章提供了将推动GAN未来发展的创新，以及隐藏在结构背

后更高层次的理念。

RGAN可以确保生成器考虑到真实数据和生成数据的相对似然。

SAGAN的注意力机制能与人类的感知起到类似的作用。

BigGAN能够以前所未有的高质量生成ImageNet所有1000个类别的

图片。

（3）本章强调了书中两个反复出现的主题：一是GAN用途广泛，

二是实验的必要性——因为与其他深度学习技术一样，GAN既是一门科

学，也是一门艺术。

[1] An AI That

Writes Convincing Prose Risks

Mass-Producing

Fake News, by Will

Knight, MIT Technology Review,

2019.

[2] Inside the World

of AI that Forges

Beautiful Art and

Terrifying Deepfakes, by Karen

Hao, MIT Technology Review,

2019. 另见 AI Gets

Creative Thanks to GANs

Innovations, by

Jakub Langr, Forbes, 2019.

[3] The Liar’s Dividend,

and Other Challenges of

Deep-Fake

News,by Paul Chadwick, The

Guardian, 2018. 另见If You

Thought

Fake News Was a

Problem, Wait for DeepFakes,

by Roula Khalaf,

2018, Financial Times.

[4] 此处跳过了一些细节，我们希望给出高屋建瓴的观点并且保持表

示法前后一致，这样你就能自己领悟并补齐细节。

[5] SERGAN: Speech Enhancement

Using Relativistic Generative

Adversarial Networks with Gradient

Penalty,by Deepak Baby and

Sarah Verhulst, 2019, IEEE-ICASSP.

[6] The Mind Is

Flat: The Illusion of

Mental Depth and the

Improvised Mind by Nick

Chater (Penguin, 2018).

[7] Large Scale GAN

Training for High Fidelity

Natural Image

Synthesis, by Andrew Brock

et al., 2019.

[8] 参见Mario Klingemann发表的Twitter.
