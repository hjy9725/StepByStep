[022] è¿™

ä½¿å¾—æ·±åº¦ä¿®

å‰ªäº†æ¨¡å‹å‹

ç¼©çš„çµæ´»ä¸”

å®ç”¨çš„æ–¹æ³•

ã€‚

--------------------------------------------------



[023] This

work follows a standard

depth pruning frame- work:

unimportant layers are first

removed, and the pruned

model is then fine-tuned

for performance recovery.

[023]

è¿™é¡¹å·¥ä½œéµ

å¾ªæ ‡å‡†çš„æ·±

åº¦ä¿®å‰ªæ¡†æ¶

- é¦–å…ˆè¦åˆ é™¤

ä¸é‡è¦çš„å±‚

ï¼Œç„¶åå¯¹ä¿®å‰ª

æ¨¡å‹è¿›è¡Œå¾®

è°ƒä»¥è¿›è¡Œæ€§

èƒ½æ¢å¤ã€‚



--------------------------------------------------

[024] In the literature,

depth pruning techniques designed

for dif- fusion transformers

or general transformers primarily

fo- cus on heuristic

approaches, such as carefully

designed importance scores [6,

36] or manually configured

pruning 1 arXiv:2412.01199v1

[cs.CV]  2 Dec

2024

[024] åœ¨æ–‡

çŒ®ä¸­ï¼Œè®¾è®¡ç”¨

äºå·®å¼‚å˜å‹

å™¨æˆ–é€šç”¨å˜

å‹å™¨è®¾è®¡çš„

æ·±åº¦ä¿®å‰ªæŠ€

æœ¯ä¸»è¦æ˜¯åŸº

äºå¯å‘å¼æ–¹

æ³•ï¼Œä¾‹å¦‚ç²¾å¿ƒ

è®¾è®¡çš„é‡è¦

æ€§å¾—åˆ†[6ï¼Œ36]æˆ–æ‰‹

åŠ¨é…ç½®çš„ä¿®

å‰ª1 arxivï¼š2412.011999v1

[cs.cv] [cs.cv] [cs.cv] 2024å¹´12æœˆ2æ—¥2024å¹´

12æœˆ2æ—¥

--------------------------------------------------



[025] schemes

[23, 54].

[025] æ–¹æ¡ˆ[23ï¼Œ54]ã€‚

--------------------------------------------------



[026] These

methods adhere to a

loss min- imization principle

[18, 37], aiming to

identify solutions that maintain

low loss or error

after pruning.

[026] è¿™

äº›æ–¹æ³•éµå®ˆ

æŸå¤±æœ€å°åŸ

åˆ™[18ï¼Œ37]ï¼Œæ—¨åœ¨è¯†åˆ«

åœ¨ä¿®å‰ªåä¿

æŒä½æŸå¤±æˆ–

é”™è¯¯çš„è§£å†³

æ–¹æ¡ˆã€‚

--------------------------------------------------



[027] This

paper investigates the effectiveness

of this widely used

principle in the context

of depth compression.

[027]

æœ¬æ–‡åœ¨

æ·±åº¦å‹ç¼©çš„

èƒŒæ™¯ä¸‹ç ”ç©¶

äº†è¯¥å¹¿æ³›ä½¿

ç”¨åŸç†çš„æœ‰

æ•ˆæ€§ã€‚



--------------------------------------------------

[028] Through experiments, we

examined the relationship between

calibration loss ob- served

post-pruning and the performance

after fine-tuning.

[028] é€šè¿‡å®

éªŒï¼Œæˆ‘ä»¬æ£€æŸ¥

äº†æ ¡å‡†æŸå¤±

æ¸—é€åçš„å

å»¶æœŸä¸å¾®è°ƒ

åçš„æ€§èƒ½ä¹‹

é—´çš„å…³ç³»ã€‚

--------------------------------------------------



[029] This

is achieved by extensively

sampling 100,000 models via

random pruning, exhibiting different

levels of calibra- tion

loss in the searching

space.

[029] è¿™

æ˜¯é€šè¿‡é€šè¿‡

éšæœºä¿®å‰ªè¿›

è¡Œå¹¿æ³›é‡‡æ ·

100,000æ¬¾æ¨¡å‹æ¥å®

ç°çš„ï¼Œåœ¨æœç´¢

ç©ºé—´ä¸­è¡¨ç°

å‡ºä¸åŒæ°´å¹³

çš„ç¢³çº¤ç»´æŸ

å¤±ã€‚

--------------------------------------------------



[030] Based

on this, we analyzed

the effectiveness of existing

pruning algorithms, such as

the feature similarity [6,

36] and sensitivity analysis

[18], which indeed achieve

low calibration losses in

the solution space.

[030]

åŸºäºæ­¤ï¼Œæˆ‘

ä»¬åˆ†æäº†ç°

æœ‰ä¿®å‰ªç®—æ³•

çš„æœ‰æ•ˆæ€§ï¼Œä¾‹

å¦‚ç‰¹å¾ç›¸ä¼¼

æ€§[6ï¼Œ36]å’Œçµæ•åº¦

åˆ†æ[18]ï¼Œå®ƒä»¬ç¡®

å®åœ¨è§£å†³æ–¹

æ¡ˆç©ºé—´ä¸­å®

ç°äº†ä½æ ¡å‡†

æŸå¤±ã€‚



--------------------------------------------------

[031] However, the performance

of all these models

after fine- tuning often

falls short of expectations.

[031] ä½†æ˜¯ï¼Œç²¾

ç»†è°ƒæ•´åæ‰€

æœ‰è¿™äº›æ¨¡å‹

çš„æ€§èƒ½é€šå¸¸

éƒ½æ²¡æœ‰æœŸæœ›

ã€‚



--------------------------------------------------

[032] This indicates that

the loss minimization principle

may not be well-suited

for diffusion transformers.

[032]

è¿™è¡¨æ˜æœ€å°

åŒ–åŸç†å¯èƒ½

ä¸é€‚åˆæ‰©æ•£

å˜å‹å™¨ã€‚



--------------------------------------------------

[033] Building on these

insights, we reassessed the

underly- ing principles for

effective layer pruning in

diffusion trans- formers.

[033]

åœ¨è¿™

äº›è§è§£çš„åŸº

ç¡€ä¸Šï¼Œæˆ‘ä»¬é‡

æ–°è¯„ä¼°äº†åœ¨

æ‰©æ•£å¼ä¼ è¾“

ä¸­ä¿®å‰ªæœ‰æ•ˆ

å±‚çš„åŸºæœ¬åŸ

ç†ã€‚



--------------------------------------------------

[034] Fine-tuning diffusion transformers

is an extremely time-consuming

process.

[034] å¾®è°ƒæ‰©æ•£

å˜å‹å™¨æ˜¯ä¸€

ä¸ªéå¸¸è€—æ—¶

çš„è¿‡ç¨‹ã€‚

--------------------------------------------------



[035] Instead

of searching for a

model that minimizes loss

immediately after pruning, we

propose identifying candidate models

with strong recoverability, en-

abling superior post-fine-tuning performance.

[035] æˆ‘ä»¬

æ²¡æœ‰åœ¨ä¿®å‰ª

åç«‹å³å¯»æ‰¾

å°†æŸå¤±ç«‹å³

æœ€å°åŒ–çš„æ¨¡

å‹ï¼Œè€Œæ˜¯æè®®

è¯†åˆ«å…·æœ‰å¼º

å¤§å¯æ¢å¤æ€§

ï¼Œä¼˜åŠ¿è¾ƒé«˜å

è°ƒèŠ‚æ€§èƒ½çš„

å€™é€‰æ¨¡å‹ã€‚



--------------------------------------------------

[036] Achieving this goal

is particularly challenging, as

it requires the in-

tegration of two distinct

processes, pruning and fine-tuning,

which involve non-differentiable operations

and cannot be directly

optimized via gradient descent.

[036] å®

ç°è¿™ä¸€ç›®æ ‡

ç‰¹åˆ«å…·æœ‰æŒ‘

æˆ˜æ€§ï¼Œå› ä¸ºå®ƒ

éœ€è¦å¯¹ä¸¤ä¸ª

ä¸åŒçš„è¿‡ç¨‹

è¿›è¡Œä¿®å‰ªå’Œ

å¾®è°ƒï¼Œè¿™æ¶‰åŠ

éä¸åŒçš„æ“

ä½œï¼Œå¹¶ä¸”ä¸èƒ½

é€šè¿‡æ¢¯åº¦ä¸‹

é™ç›´æ¥ä¼˜åŒ–

ã€‚



--------------------------------------------------

[037] To this end,

we propose a learnable

depth pruning method that

effectively integrates pruning and

fine-tuning.

[037] ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ

å‡ºäº†ä¸€ç§å¯

å­¦ä¹ çš„æ·±åº¦

ä¿®å‰ªæ–¹æ³•ï¼Œå¯

ä»¥æœ‰æ•ˆæ•´åˆ

ä¿®å‰ªå’Œå¾®è°ƒ

ã€‚

--------------------------------------------------



[038] As

shown in Figure 1,

we model the pruning

and fine- tuning of

a diffusion transformer as

a differentiable sam- pling

process of layer masks

[13, 17, 22], combined

with a co-optimized weight

update to simulate future

fine-tuning.

[038] å¦‚å›¾1æ‰€ç¤ºï¼Œæˆ‘

ä»¬å°†æ‰©æ•£å˜

å‹å™¨çš„ä¿®å‰ª

å’Œç²¾ç»†è°ƒæ•´

ä¸ºå±‚æ©æ¨¡çš„

å¯åŒºåˆ†çš„Samå›º

å®šè¿‡ç¨‹[13ï¼Œ17ï¼Œ22]ï¼Œå¹¶ç»“

åˆäº†åˆä½œçš„

æƒé‡æ›´æ–°ï¼Œä»¥

æ¨¡æ‹Ÿæœªæ¥çš„

å¾®è°ƒã€‚

--------------------------------------------------



[039] Our

objective is to iteratively

refine this distribution so

that networks with higher

recoverability are more likely

to be sampled.

[039]

æˆ‘ä»¬çš„

ç›®æ ‡æ˜¯è¿­ä»£

åœ°å®Œå–„æ­¤åˆ†

å¸ƒï¼Œä»¥ä¾¿æ›´æœ‰

å¯èƒ½é‡‡æ ·å…·

æœ‰è¾ƒé«˜å¯æ¢

å¤æ€§çš„ç½‘ç»œ

ã€‚



--------------------------------------------------

[040] This is achieved

through a straightforward strat-

egy: if a sampled

pruning decision results in

strong recover- ability, similar

pruning patterns will have

an increased prob- ability

of being sampled.

[040]

è¿™æ˜¯é€šè¿‡ç›´

æ¥çš„ç­–ç•¥æ¥

å®ç°çš„ï¼šå¦‚æœ

é‡‡æ ·ä¿®å‰ªå†³

ç­–ä¼šå¯¼è‡´å¼º

å¤§çš„æ¢å¤èƒ½

åŠ›ï¼Œé‚£ä¹ˆç›¸ä¼¼

çš„ä¿®å‰ªæ¨¡å¼

å°†å…·æœ‰æé«˜

é‡‡æ ·çš„æ¦‚ç‡

èƒ½åŠ›ã€‚



--------------------------------------------------

[041] This approach promotes

the ex- ploration of

potentially valuable solutions while

disregard- ing less effective

ones.

[041] è¿™ç§æ–¹

æ³•ä¿ƒè¿›äº†å¯¹

æ½œåœ¨æœ‰ä»·å€¼

çš„è§£å†³æ–¹æ¡ˆ

çš„æå‡ºï¼ŒåŒæ—¶

åˆæ— è§†æ•ˆç‡

è¾ƒä½çš„è§£å†³

æ–¹æ¡ˆã€‚

--------------------------------------------------



[042] Additionally,

the proposed method is

highly efficient, and we

demonstrate that a suitable

solu- tion can emerge

within a few training

steps.

[042] æ­¤å¤–ï¼Œæ‰€

æå‡ºçš„æ–¹æ³•

éå¸¸æœ‰æ•ˆï¼Œæˆ‘

ä»¬è¯æ˜å¯ä»¥

åœ¨å‡ ä¸ªè®­ç»ƒ

æ­¥éª¤ä¸­å‡ºç°

åˆé€‚çš„è§£å†³

æ–¹æ¡ˆã€‚

--------------------------------------------------



[043] To

evaluate the effectiveness of

the proposed method, we

conduct extensive experiments on

various transformer- based diffusion

models, including DiTs [40],

MARs [29], SiTs [34].

[043] ä¸ºäº†è¯„

ä¼°æ‰€æå‡ºæ–¹

æ³•çš„æœ‰æ•ˆæ€§

ï¼Œæˆ‘ä»¬å¯¹åŸºäº

å˜å‹å™¨çš„æ‰©

æ•£æ¨¡å‹è¿›è¡Œ

äº†å¹¿æ³›çš„å®

éªŒï¼ŒåŒ…æ‹¬DITS [40]ï¼ŒMARS [29]ï¼Œä½äº

[34]ã€‚

--------------------------------------------------



[044] The

learnable approach is highly

efficient.

[044] å¯å­¦ä¹ çš„æ–¹

æ³•éå¸¸æœ‰æ•ˆ

ã€‚

--------------------------------------------------



[045] It

is able to identify

redundant layers in diffusion

transform- ers with 1-epoch

training on the dataset,

which effectively crafts shallow

diffusion transformers from pre-trained

mod- els with high

recoverability.

[045] å®ƒèƒ½å¤Ÿé€šè¿‡

æ•°æ®é›†ä¸Šçš„

1ä¸ªä¸Šè¿°è®­ç»ƒ

æ¥è¯†åˆ«æ‰©æ•£

å˜æ¢ä¸­çš„å†—

ä½™å±‚ï¼Œä»è€Œæœ‰

æ•ˆåœ°ä»å…·æœ‰

é«˜å¯æ¢å¤æ€§

çš„é¢„è®­ç»ƒçš„

æ¨¡å‹ä¸­åˆ¶ä½œ

äº†æµ…æ‰©æ•£å˜

å‹å™¨ã€‚

--------------------------------------------------



[046] For

instance, while the models

pruned by TinyFusion initially

exhibit relatively high cal-

ibration loss after removing

50% of layers, they

recover quickly through fine-tuning,

achieving a significantly more

competitive FID score (5.73

vs. 22.28) compared to

base- line methods that

only minimize immediate loss,

using just 1% of

the pre-training cost.

[046]

ä¾‹å¦‚ï¼Œå°½

ç®¡åˆ é™¤50ï¼…çš„å±‚

åï¼Œæœ€åˆè¢«TinyFusionä¿®

å‰ªçš„æ¨¡å‹æœ€

åˆè¡¨ç°å‡ºç›¸

å¯¹è¾ƒé«˜çš„calæŸ

å¤±ï¼Œä½†ä¸ä»…ä½¿

ç”¨é¢„å…ˆåŸ¹è®­

çš„1ï¼…çš„åŸºæœ¬æ–¹

æ³•ç›¸æ¯”ï¼Œå®ƒä»¬

é€šè¿‡å¾®è°ƒå¾—

åˆ†è¿…é€Ÿæ¢å¤

ï¼Œè·å¾—äº†æ›´å…·

ç«äº‰åŠ›çš„FIDå¾—

åˆ†ï¼ˆ5.73 vs 22.28ï¼‰ï¼ˆ5.73å¯¹22.28ï¼‰ã€‚

--------------------------------------------------



[047] Additionally,

we also ex- plore

the role of knowledge

distillation in enhancing re-

coverability [20, 23] by

introducing a MaskedKD variant.

[047] æ­¤å¤–ï¼Œæˆ‘

ä»¬è¿˜é€šè¿‡å¼•

å…¥maskedkdå˜ä½“æ¥è¡¨

è¾¾çŸ¥è¯†è’¸é¦

åœ¨å¢å¼ºå¯è¦†

ç›–æ€§[20ï¼Œ23]ä¸­çš„ä½œ

ç”¨ã€‚



--------------------------------------------------

[048] MaskedKD mitigates the

negative impact of the

massive or outlier activations

[47] in hidden states,

which can signifi- cantly

affect the performance and

reliability of fine-tuning.

[048]

MaskEDKDå‡è½»äº†éš

è—çŠ¶æ€ä¸­å¤§

è§„æ¨¡æˆ–å¼‚å¸¸

æ¿€æ´»çš„è´Ÿé¢

å½±å“[47]ï¼Œè¿™å¯èƒ½

ä¼šæ˜¾ç€å½±å“

å¾®è°ƒçš„æ€§èƒ½

å’Œå¯é æ€§ã€‚



--------------------------------------------------

[049] With MaskedKD, the

FID score improves from

5.73 to 3.73 with

only 1% of pre-training

cost.

[049] å€Ÿ

åŠ©MaskedKDï¼ŒFIDå¾—åˆ†ä»5.73æ

é«˜åˆ°3.73ï¼Œä»…å åŸ¹

è®­å‰æˆæœ¬çš„

1ï¼…ã€‚

--------------------------------------------------



[050] Extending

the training to 7%

of the pre-training cost

further reduces the FID

to 2.86, just 0.4

higher than the original

model with doubled depth.

[050] å°†åŸ¹è®­æ‰©å¤§

åˆ°7ï¼…çš„åŸ¹è®­å‰

æˆæœ¬å°†FIDè¿›ä¸€

æ­¥é™ä½åˆ°2.86ï¼Œä»…

æ¯”åŸå§‹æ¨¡å‹

é«˜åº¦å¢åŠ äº†

0.4ã€‚



--------------------------------------------------

[051] Therefore, the main

contribution of this work

lies in a learnable

method to craft shallow

diffusion transformers from pre-trained

ones, which explicitly optimizes

the re- coverability of

pruned models.

[051] å› æ­¤ï¼Œè¿™é¡¹å·¥

ä½œçš„ä¸»è¦è´¡

çŒ®åœ¨äºä¸€ç§

å¯å­¦ä¹ çš„æ–¹

æ³•ï¼Œå¯ä»¥ä»é¢„

è®­ç»ƒçš„æ–¹æ³•

ä¸­åˆ¶ä½œæµ…æ‰©

æ•£å˜å‹å™¨ï¼Œè¯¥

æ–¹æ³•æ˜ç¡®ä¼˜

åŒ–äº†ä¿®å‰ªæ¨¡

å‹çš„å¯è¦†ç›–

æ€§ã€‚

--------------------------------------------------



[052] The

method is general for

various architectures, including DiTs,

MARs and SiTs.

[052]

è¯¥æ–¹æ³•æ˜¯

å„ç§æ¶æ„ï¼ŒåŒ…

æ‹¬DITï¼ŒMARSå’ŒSITSçš„ä¸€èˆ¬

æ–¹æ³•ã€‚



--------------------------------------------------

[053] 2.

[053] 2ã€‚

--------------------------------------------------



[054] Related

Works Network Pruning and

Depth Reduction.

[054] ç›¸å…³å·¥

ä½œç½‘ç»œä¿®å‰ª

å’Œæ·±åº¦å‡å°‘

ã€‚

--------------------------------------------------



[055] Network

prun- ing is a

widely used approach for

compressing pre-trained diffusion models

by eliminating redundant parameters

[3, 12, 31, 51].

[055] ç½‘ç»œä¿®å‰ªæ˜¯

ä¸€ç§é€šè¿‡æ¶ˆ

é™¤å†—ä½™å‚æ•°

æ¥å‹ç¼©é¢„è®­

ç»ƒæ‰©æ•£æ¨¡å‹

çš„å¹¿æ³›ä½¿ç”¨

æ–¹æ³•[3ï¼Œ12ï¼Œ31ï¼Œ51]ã€‚



--------------------------------------------------

[056] Diff-Pruning [12] introduces

a gradient- based technique

to streamline the width

of UNet, fol- lowed

by a simple fine-tuning

to recover the performance.

[056] DIFF-PRUNING [12]å¼•å…¥äº†

ä¸€ç§åŸºäºæ¢¯

åº¦çš„æŠ€æœ¯ï¼Œä»¥

ç®€åŒ–UNETçš„å®½åº¦

ï¼Œä»¥é€šè¿‡ç®€å•

çš„å¾®è°ƒæ¥æ¢

å¤æ€§èƒ½ã€‚

--------------------------------------------------



[057] SparseDM

[51] applies sparsity to

pre-trained diffusion models via

the Straight-Through Estimator (STE)

[2], achieving a 50%

reduction in MACs with

only a 1.22 in-

crease in FID on

average.

[057] Sparsedm [51]é€šè¿‡

ç›´é€šä¼°è®¡é‡

ï¼ˆSteï¼‰[2]å°†ç¨€ç–æ€§åº”

ç”¨äºé¢„è®­ç»ƒ

çš„æ‰©æ•£æ¨¡å‹

ï¼Œåœ¨MACä¸­é™ä½äº†

50ï¼…ï¼Œå¹³å‡FIDä»…1.22ä¸ªæŠ˜

ç—•ã€‚

--------------------------------------------------



[058] While

width pruning and spar-

sity help reduce memory

overhead, they often offer

lim- ited speed improvements,

especially on parallel devices

like GPUs.

[058] è™½ç„¶ä¿®å‰ª

å’Œå®½åº¦æœ‰åŠ©

äºå‡å°‘å†…å­˜

å¼€é”€ï¼Œä½†å®ƒä»¬

é€šå¸¸ä¼šæä¾›

é™åˆ¶çš„é€Ÿåº¦

æé«˜ï¼Œå°¤å…¶æ˜¯

åœ¨è¯¸å¦‚GPUä¹‹ç±»

çš„å¹³è¡Œè®¾å¤‡

ä¸Šã€‚

--------------------------------------------------



[059] Consequently,

depth reduction has gained

signifi- cant attention in

the past few years,

as removing entire lay-

ers enables better speedup

proportional to the pruning

ra- tio [24, 27,

28, 36, 54, 56,

58].

[059] å› æ­¤ï¼Œåœ¨è¿‡

å»çš„å‡ å¹´ä¸­

ï¼Œæ·±åº¦çš„é™ä½

å¼•èµ·äº†æ˜¾ç€

å…³æ³¨ï¼Œå› ä¸ºåˆ 

é™¤æ•´ä¸ªå¤–è¡Œ

å¯ä»¥æ›´å¥½åœ°

åŠ é€Ÿä¸ä¿®å‰ª

ra-tioæˆæ­£æ¯”[24ã€27ã€27ã€28ã€36ã€54ã€56ã€58]ã€‚

--------------------------------------------------



[060] Adaptive

depth reduction techniques, such

as MoD [41] and

depth-aware transform- ers [10],

have also been proposed.

[060] è¿˜æ

å‡ºäº†è‡ªé€‚åº”

æ·±åº¦è¿˜åŸæŠ€

æœ¯ï¼Œä¾‹å¦‚MOD [41]å’Œæ·±

åº¦æ„ŸçŸ¥çš„è½¬

åŒ–[10]ã€‚

--------------------------------------------------



[061] Despite

these advances, most existing

methods are still based

on empirical or heuris-

tic strategies, such as

carefully designed importance crite-

ria [36, 54], sensitivity

analyses [18] or manually

designed schemes [23], which

often do not yield

strong performance guarantee after

fine-tuning.

[061] å°½ç®¡å–å¾—

äº†è¿™äº›è¿›æ­¥

ï¼Œä½†å¤§å¤šæ•°ç°

æœ‰æ–¹æ³•ä»ç„¶

åŸºäºç»éªŒæˆ–

å¯å‘å¼ç­–ç•¥

ï¼Œä¾‹å¦‚ç²¾å¿ƒè®¾

è®¡çš„é‡è¦æ€§

è¿¹è±¡[36ï¼Œ54]ï¼Œæ•æ„Ÿæ€§

åˆ†æ[18]æˆ–æ‰‹åŠ¨

è®¾è®¡çš„æ–¹æ¡ˆ

[23]ï¼Œè¿™äº›æ–¹æ¡ˆé€š

å¸¸ä¸ä¼šåœ¨å¾®

è°ƒåäº§ç”Ÿå¼º

å¤§çš„æ€§èƒ½ä¿

è¯ã€‚

--------------------------------------------------



[062] Efficient

Diffusion Transformers.

[062] æœ‰æ•ˆçš„æ‰©

æ•£å˜å‹å™¨ã€‚

--------------------------------------------------



[063] Developing

efficient diffusion transformers has

become an appealing focus

within the community, where

significant efforts have been

made to enhance efficiency

from various perspectives, in-

cluding linear attention mechanisms

[15, 48, 52], compact

architectures [50], non-autoregressive transformers

[4, 14, 38, 49],

pruning [12, 23], quantization

[19, 30, 44], feature

2

[063] å¼€

å‘æœ‰æ•ˆçš„æ‰©

æ•£å˜å‹å™¨å·²

æˆä¸ºç¤¾åŒºä¸­

çš„ä¸€ä¸ªå¸å¼•

äººçš„é‡ç‚¹ï¼Œåœ¨

å„ç§è§’åº¦ï¼Œå·²

ç»åšå‡ºäº†å·¨

å¤§çš„åŠªåŠ›æ¥

æé«˜æ•ˆç‡ï¼ŒåŒ…

æ‹¬çº¿æ€§æ³¨æ„

æœºåˆ¶[15ï¼Œ48ï¼Œ52]ï¼Œç´§å‡‘å‹

å»ºç­‘[50]ï¼Œéè‡ªåŠ¨

æ€§å˜å‹å™¨[4ï¼Œ14ï¼Œ38ï¼Œ49]ï¼Œpruning [4,14,38,49]ï¼Œpruns

[12ï¼Œ23]ï¼Œé‡

åŒ–[19ï¼Œ30ï¼Œ30ï¼Œ44]



--------------------------------------------------

[064] Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer 1:2 Local Blocks

ğ“ğŸ ğ“ğŸ ğ“ğŸ‘ ğ“ğŸ’

0 1 0 1

0 1 0 1

âŠ• Weight Update Weight

Update Weight Update Weight

Update Î”ğœ™4 â‹…ğ”ª4 Î”ğœ™3

â‹…ğ”ª3 Î”ğœ™2 â‹…ğ”ª2 Î”ğœ™1

â‹…ğ”ª1 Retained Layer Retained

Layer ğ¦ğ¢ğ§ğ–’,ğš«ğš½ğ“›(ğ’™, ğš½+ ğš«ğš½,

ğ–’) Confident Sampling â‡’

Good solution identified 1:2

Local Blocks ğ”ª1 ğ”ª2

ğ”ª3 ğ”ª4 âŠ• âˆ¼

Mixed Sampling â‡’ Exploration

still in Progress Diff.

[064] å˜å‹å™¨å±‚

å˜å‹å™¨å±‚å˜

å‹å™¨å±‚å˜å‹

å™¨å±‚1ï¼š2å±€éƒ¨å—

ğ“ğŸ 0 1

0 1 0 1

0 1 0 1

0 1âŠ•é‡é‡æ›´æ–°é‡

é‡æ›´æ–°é‡é‡

æ›´æ–°é‡é‡æ›´

æ–°é‡é‡æ›´æ–°

é‡é‡æ›´æ–°é‡

é‡æ›´æ–°Î”4Î”3Î”3Î”2Î”2Î”2Î´ğœ™2Î”ğœ™1Î”ğœ™1Âµ1Â·ğ”ª1Â·ğ”ª1Â·ğ”ª1Âµ1ä¿ç•™

å±‚ä¿ç•™å±‚ï¼Œè‰¯

å¥½çš„å±‚è¯†åˆ«

å±‚ï¼ˆè‰¯å¥½çš„è§£

å†³æ–¹æ¡ˆï¼‰ï¼ŒèŒƒå›´

2ï¼Œğš«ğš½ğ“›ï¼Œğš«ğš½ğ“›ï¼Œğ’™+ ğš«ğš½+ ğš«ğš½ï¼Œğš½+

ğš«ğš½ï¼Œğ–’+ ğš«ğš½ï¼ŒSamplingï¼ŒSampl complativeã€‚ ğ”ª1ğ”ª2ğ”ª3ğ”ª4âŠ•4ã€œæ··åˆé‡‡æ ·â‡’æ¢

ç´¢ä»åœ¨è¿›è¡Œ

ä¸­ã€‚

--------------------------------------------------



[065] Sampling

Learnable Distribution âˆ¼ Diff.

[065] æŠ½æ ·å¯å­¦

ä¹ çš„åˆ†å¸ƒã€œå·®

å¼‚ã€‚



--------------------------------------------------

[066] Sampling Figure 2.

[066] é‡‡æ ·å›¾2ã€‚



--------------------------------------------------

[067] The proposed TinyFusion

method learns to perform

a differentiable sampling of

candidate solutions, jointly optimized

with a weight update

to estimate recoverability.

[067]

æ‹Ÿ

è®®çš„TinyFusionæ–¹æ³•å­¦

ä¼šäº†å¯¹å€™é€‰

è§£å†³æ–¹æ¡ˆè¿›

è¡Œå¯åŒºåˆ†çš„

é‡‡æ ·ï¼Œå…±åŒä¼˜

åŒ–äº†é‡é‡æ›´

æ–°ä»¥ä¼°ç®—å¯

æ¢å¤æ€§ã€‚



--------------------------------------------------

[068] This approach aims

to increase the likelihood

of favorable solutions that

ensure strong post-fine- tuning

performance.

[068] è¿™ç§

æ–¹æ³•æ—¨åœ¨å¢

åŠ æœ‰åˆ©è§£å†³

æ–¹æ¡ˆçš„å¯èƒ½

æ€§ï¼Œä»è€Œç¡®ä¿

å¼ºå¤§çš„ç»“æŸ

åè¡¨ç°ã€‚

--------------------------------------------------



[069] After

training, local structures with

the highest sampling probabilities

are retained.

[069] è®­ç»ƒ

åï¼Œä¿ç•™äº†é‡‡

æ ·æ¦‚ç‡æœ€é«˜

çš„æœ¬åœ°ç»“æ„

ã€‚

--------------------------------------------------



[070] caching

[35, 57], etc.

[070]

ç¼“å­˜[35ï¼Œ57]ï¼Œç­‰ã€‚



--------------------------------------------------

[071] In this work,

we focus on compress-

ing the depth of

pre-trained diffusion transformers and

in- troduce a learnable

method that directly optimizes

recover- ability, which is

able to achieve satisfactory

results with low re-training

costs.

[071] åœ¨è¿™

é¡¹å·¥ä½œä¸­ï¼Œæˆ‘

ä»¬ä¸“æ³¨äºå‹

ç¼©é¢„è®­ç»ƒçš„

æ‰©æ•£å˜å‹å™¨

çš„æ·±åº¦ï¼Œå¹¶èµ‹

äºˆä¸€ç§å¯å­¦

ä¹ çš„æ–¹æ³•ï¼Œè¯¥

æ–¹æ³•å¯ä»¥ç›´

æ¥ä¼˜åŒ–æ¢å¤

èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•

èƒ½å¤Ÿé€šè¿‡ä½

é‡æ–°è®­ç»ƒæˆ

æœ¬è·å¾—ä»¤äºº

æ»¡æ„çš„ç»“æœ

ã€‚

--------------------------------------------------



[072] 3.

[072] 3ã€‚



--------------------------------------------------

[073] Method 3.1.

[073]

æ–¹æ³•3.1ã€‚



--------------------------------------------------

[074] Shallow Generative Transformers

by Pruning This work

aims to derive a

shallow diffusion transformer by

pruning a pre-trained model.

[074] é€šè¿‡ä¿®

å‰ªè¿™é¡¹å·¥ä½œ

ï¼Œæµ…å±‚ç”Ÿæˆå˜

å‹å™¨æ—¨åœ¨é€š

è¿‡ä¿®å‰ªé¢„è®­

ç»ƒçš„æ¨¡å‹æ¥

å¾—å‡ºæµ…æ‰©æ•£

å˜å‹å™¨ã€‚



--------------------------------------------------

[075] For simplicity, all

vectors in this paper

are column vectors.

[075]

ä¸ºç®€

å•èµ·è§ï¼Œæœ¬æ–‡

ä¸­çš„æ‰€æœ‰å‘

é‡éƒ½æ˜¯åˆ—å‘

é‡ã€‚



--------------------------------------------------

[076] Consider a L-layer

trans- former, parameterized by

Î¦LÃ—D = [Ï•1, Ï•2,

Â· Â· Â· ,

Ï•L]âŠº, where each element

Ï•i encompasses all learnable

param- eters of a

transformer layer as a

D-dim column vector, which

includes the weights of

both attention layers and

MLPs.

[076] è€ƒè™‘ä¸€ä¸ª

ç”±Lå±‚trans-å‰è€…ï¼Œç”±

Ï†lÃ—d =

[Ï•1ï¼ŒÏ•2ï¼ŒÂ·ï¼ŒÏ•L]âŠºè¿›è¡Œå‚æ•°ï¼Œå…¶

ä¸­æ¯ä¸ªå…ƒç´ 

Ï•iåŒ…å«å˜å‹å™¨

å±‚çš„æ‰€æœ‰å¯

å­¦ä¹ çš„å‚æ•°

ä½œä¸ºD-DIMæŸ±å‘é‡

ï¼Œå…¶ä¸­åŒ…æ‹¬æ³¨

æ„å±‚å’ŒMLPçš„é‡

é‡ã€‚



--------------------------------------------------

[077] Depth pruning seeks

to find a binary

layer mask mLÃ—1 =

[m1, m2, Â· Â·

Â· , mL]âŠº, that

removes a layer by:

xi+1 = miÏ•i(xi) +

(1 âˆ’mi)xi = (

Ï•i(xi), if mi =

1, xi, otherwise, (1)

where the xi and

Ï•i(xi) refers to the

input and output of

layer Ï•i.

[077] æ·±åº¦ä¿®å‰ª

è¯•å›¾æ‰¾åˆ°äºŒ

è¿›åˆ¶æ©ç mlÃ—1

= [m1ï¼Œm2ï¼ŒÂ·Â·çº§

ï¼Œml]âŠºï¼Œè¯¥å±‚é€šè¿‡ï¼šxi + 1

= miDarticeï¼ˆxiï¼‰ +ï¼ˆ1 -miï¼‰xi

=ï¼ˆÏ•iï¼ˆxiï¼‰=ï¼ˆÏ•iï¼ˆxiï¼ˆxiï¼ˆxiï¼‰ï¼Œå¦‚

æœmi = 1ï¼Œxiï¼‰ï¼Œxi =

1ï¼Œxiï¼Œï¼ˆxiï¼‰ï¼Œxiå’Œxi ylyï¼ˆxiï¼‰ï¼Œï¼ˆxiï¼‰å’Œxi liverï¼Œï¼ˆxiï¼‰codiï¼ˆxiï¼‰å’Œxiï¼ˆxiï¼‰codiï¼ˆxiï¼‰å’Œ

ï¼ˆxi liverï¼Œï¼ˆ1ï¼‰

Ï•iã€‚



--------------------------------------------------

[078] To obtain the

mask, a common paradigm

in prior work is

to minimize the loss

L after pruning, which

can be formulated as

minm Ex [L(x, Î¦,

m)].

[078] ä¸ºäº†è·å¾—æ©

æ¨¡ï¼Œå…ˆå‰å·¥ä½œ

ä¸­çš„ä¸€ä¸ªå¸¸

è§èŒƒå¼æ˜¯å°†

ä¿®å‰ªåçš„æŸ

è€—læœ€å°åŒ–ï¼Œå¯

ä»¥å°†å…¶ä½œä¸º

minm ex

[lï¼ˆxï¼ŒÏ†ï¼Œmï¼‰]é…åˆ¶ã€‚



--------------------------------------------------

[079] However, as we

will show in the

experiments, this objective â€“

though widely adopted in

discriminative tasks â€“ may

not be well-suited to

pruning diffusion transformers.

[079]

ä½†æ˜¯ï¼Œæ­£

å¦‚æˆ‘ä»¬å°†åœ¨

å®éªŒä¸­æ˜¾ç¤º

çš„é‚£æ ·ï¼Œå°½ç®¡

åœ¨åˆ¤åˆ«ä»»åŠ¡

ä¸­å¹¿æ³›é‡‡ç”¨

äº†è¿™ä¸ªç›®æ ‡

ï¼Œä½†å¯èƒ½ä¸é€‚

åˆä¿®å‰ªæ‰©æ•£

å˜å‹å™¨ã€‚



--------------------------------------------------

[080] Instead, we are

more inter- ested in

the recoverability of pruned

models.

[080] å–è€Œ

ä»£ä¹‹çš„æ˜¯ï¼Œæˆ‘

ä»¬å¯¹ä¿®å‰ªæ¨¡

å‹çš„å¯æ¢å¤

æ€§æ›´åŠ æ„Ÿå…´

è¶£ã€‚

--------------------------------------------------



[081] To

achieve this, we incorporate

an additional weight update

into the optimization problem

and extend the objective

by: min m min

âˆ†Î¦ Ex [L(x, Î¦

+ âˆ†Î¦, m)] |

{z } Recoverability: Post-Fine-Tuning

Performance , (2) where

âˆ†Î¦ = {âˆ†Ï•1, âˆ†Ï•2,

Â· Â· Â· ,

âˆ†Ï•M} represents appro- priate

update from fine-tuning.

[081]

ä¸ºäº†å®ç°

è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘

ä»¬å°†é¢å¤–çš„

æƒé‡æ›´æ–°çº³

å…¥äº†ä¼˜åŒ–é—®

é¢˜ï¼Œå¹¶æ‰©å±•äº†

ç›®æ ‡ï¼šmin m min âˆ†Ï†

ex [lï¼ˆxï¼ŒÏ† + âˆ†Ï†ï¼Œmï¼‰]

| {z}å¯æ¢å¤

æ€§ï¼šæ¢å¤æ€§èƒ½

åçš„æ€§èƒ½ï¼Œï¼ˆ2ï¼‰å…¶

ä¸­âˆ†Ï† = {âˆ†

Ï•1ï¼Œâˆ† Ï•2ï¼ŒÂ·Â·Â·ï¼Œâˆ† Ï•m}è¡¨ç¤ºæ¥è‡ª

å¾®è°ƒçš„é€‚å½“

æ›´æ–°ã€‚

--------------------------------------------------



[082] The

objective formulated by Equation

2 poses two challenges:

1) The non-differentiable nature

of layer selection prevents

direct optimization us- ing

gradient descent; 2) The

inner optimization over the

retained layers makes it

computationally intractable to ex-

plore the entire search

space, as this process

necessitates se- lecting a

candidate model and fine-tuning

it for evaluation.

[082]

å…¬å¼2æ

å‡ºçš„ç›®æ ‡æ

å‡ºäº†ä¸¤ä¸ªæŒ‘

æˆ˜ï¼š1ï¼‰å±‚é€‰æ‹©çš„

éå·®å¼‚æ€§è´¨

é˜»æ­¢äº†ç›´æ¥

ä¼˜åŒ–çš„æ¢¯åº¦

ä¸‹é™ï¼› 2ï¼‰åœ¨ä¿ç•™

å±‚ä¸Šçš„å†…éƒ¨

ä¼˜åŒ–ä½¿å¾—åœ¨

è®¡ç®—ä¸Šæ£˜æ‰‹

å¯ä»¥é˜æ˜æ•´

ä¸ªæœç´¢ç©ºé—´

ï¼Œå› ä¸ºæ­¤è¿‡ç¨‹

éœ€è¦å°†å€™é€‰

æ¨¡å‹åˆ—ä¸ºå€™

é€‰æ¨¡å‹å¹¶è¿›

è¡Œå¾®è°ƒä»¥è¿›

è¡Œè¯„ä¼°ã€‚



--------------------------------------------------

[083] To address this,

we propose TinyFusion that

makes both the pruning

and recoverability optimizable.

[083]

ä¸ºäº†

è§£å†³è¿™ä¸ªé—®

é¢˜ï¼Œæˆ‘ä»¬æå‡º

äº†ä½¿ä¿®å‰ªå’Œ

å¯æ¢å¤æ€§ä¼˜

åŒ–çš„å¾®å°çŒ

æ³¨ã€‚



--------------------------------------------------

[084] 3.2.

[084] 3.2ã€‚

--------------------------------------------------



[085] TinyFusion:

Learnable Depth Pruning A

Probabilistic Perspective.

[085] å°å‹çŒæ³¨

ï¼šå¯å­¦ä¹ çš„æ·±

åº¦ä¿®å‰ªæ¦‚ç‡

çš„è§‚ç‚¹ã€‚

--------------------------------------------------



[086] This

work models Equa- tion

2 from a probabilistic

standpoint.

[086] è¿™é¡¹

å·¥ä½œä»æ¦‚ç‡

çš„è§’åº¦æ¨¡æ‹Ÿ

äº†æ–¹ç¨‹2ã€‚

--------------------------------------------------



[087] We

hypothesize that the mask

m produced by â€œidealâ€

pruning methods (might be

not unique) should follow

a certain distribution.

[087]

æˆ‘ä»¬

å‡è®¾ç”±â€œç†æƒ³

â€ä¿®å‰ªæ–¹æ³•ï¼ˆå¯

èƒ½ä¸æ˜¯å”¯ä¸€

çš„ï¼‰äº§ç”Ÿçš„é¢

è†œåº”éµå¾ªä¸€

å®šçš„åˆ†å¸ƒã€‚



--------------------------------------------------

[088] To model this,

it is intuitive to

associate every possible mask

m with a probability

value p(m), thus forming

a categori- cal distribution.

[088] ä¸º

äº†å¯¹æ­¤è¿›è¡Œ

å»ºæ¨¡ï¼Œå°†æ¯ä¸ª

å¯èƒ½çš„æ©ç 

Mä¸æ¦‚ç‡å€¼Pï¼ˆMï¼‰ç›¸

å…³è”æ˜¯ç›´è§‚

çš„ï¼Œä»è€Œå½¢æˆ

åˆ†ç±»åˆ†å¸ƒã€‚



--------------------------------------------------

[089] Without any prior

knowledge, the assess- ment

of pruning masks begins

with a uniform distribution.

[089] æ²¡

æœ‰ä»»ä½•å…ˆéªŒ

çŸ¥è¯†ï¼Œä¿®å‰ªå£

ç½©çš„è¯„ä¼°å§‹

äºå‡åŒ€çš„åˆ†

å¸ƒã€‚



--------------------------------------------------

[090] However, directly sampling

from this initial distribution

is highly inefficient due

to the vast search

space.

[090] ä½†æ˜¯ï¼Œç”±äº

åºå¤§çš„æœç´¢

ç©ºé—´ï¼Œè¯¥åˆå§‹

åˆ†å¸ƒç›´æ¥è¿›

è¡Œé‡‡æ ·æ•ˆç‡

å¾ˆé«˜ã€‚

--------------------------------------------------



[091] For

in- stance, pruning a

28-layer model by 50%

involves evalu- ating  28

14  = 40,

116, 600 possible solutions.

[091] å¯¹äºè¯´

æ˜ï¼Œå°†28å±‚æ¨¡å‹

ï¼ˆ50ï¼…ï¼‰ä¿®å‰ªä¸ºè¯„ä¼°

28 14 =

40ã€116ã€600å¯èƒ½çš„è§£å†³

æ–¹æ¡ˆã€‚



--------------------------------------------------

[092] To overcome this

challenge, this work introduces

an advanced and learn-

able algorithm capable of

using evaluation results as

feed- back to iteratively

refine the mask distribution.

[092] ä¸ºäº†å…‹

æœè¿™ä¸€æŒ‘æˆ˜

ï¼Œè¿™é¡¹å·¥ä½œå¼•

å…¥äº†ä¸€ç§èƒ½

å¤Ÿä½¿ç”¨è¯„ä¼°

ç»“æœä½œä¸ºå›

é¦ˆçš„é«˜çº§ä¸”

å…·æœ‰å­¦ä¹ ç®—

æ³•ï¼Œä»¥è¿”å›è¿­

ä»£çš„æ©ç åˆ†

å¸ƒã€‚



--------------------------------------------------

[093] The basic idea

is that if certain

masks exhibit positive results,

then other masks with

similar pattern may also

be potential so- lutions

and thus should have

a higher likelihood of

sam- pling in subsequent

evaluations, allowing for a

more fo- cused search

on promising solutions.

[093]

åŸºæœ¬æ€æƒ³

æ˜¯ï¼Œå¦‚æœæŸäº›

æ©è†œè¡¨ç°å‡º

ç§¯æçš„ç»“æœ

ï¼Œé‚£ä¹ˆå…¶ä»–æ¨¡

å¼ç›¸ä¼¼çš„æ©

è†œä¹Ÿå¯èƒ½æ˜¯

æ½œåœ¨çš„ï¼Œå› æ­¤

åœ¨éšåçš„è¯„

ä¼°ä¸­åº”è¯¥å…·

æœ‰æ›´é«˜çš„å¯

èƒ½æ€§ï¼Œä»è€Œå¯

ä»¥å¯¹æœ‰å¸Œæœ›

çš„è§£å†³æ–¹æ¡ˆ

è¿›è¡Œæ›´å¤šçš„

æœç´¢ã€‚



--------------------------------------------------

[094] However, the defi-

nition of â€œsimilarity patternâ€

is still unclear so

far.

[094] ä½†æ˜¯ï¼Œåˆ°

ç›®å‰ä¸ºæ­¢ï¼Œâ€œç›¸

ä¼¼æ€§æ¨¡å¼â€çš„

å®šä¹‰å°šä¸æ¸…

æ¥šã€‚

--------------------------------------------------



[095] 3

[095] 3



--------------------------------------------------

[096] Sampling Local Structures.

[096] é‡‡æ ·æœ¬åœ°

ç»“æ„ã€‚



--------------------------------------------------

[097] In this work,

we demon- strate that

local structures, as illustrated

in Figure 2, can

serve as effective anchors

for modeling the relationships

between different masks.

[097]

åœ¨è¿™é¡¹

å·¥ä½œä¸­ï¼Œæˆ‘ä»¬

è¡¨æ˜ï¼Œå¦‚å›¾2æ‰€

ç¤ºï¼Œå±€éƒ¨ç»“æ„

å¯ä»¥ç”¨ä½œå»º

æ¨¡ä¸åŒé®ç½©

ä¹‹é—´å…³ç³»çš„

æœ‰æ•ˆé”šç‚¹ã€‚



--------------------------------------------------

[098] If a pruning

mask leads to cer-

tain local structures and

yields competitive results after

fine- tuning, then other

masks yielding the same

local patterns are also

likely to be positive

solutions.

[098] å¦‚

æœä¿®å‰ªæ©æ¨¡

ä¼šå¯¼è‡´å±€éƒ¨

ç»“æ„ï¼Œå¹¶åœ¨ç»†

è°ƒæ—¶äº§ç”Ÿç«

äº‰ç»“æœï¼Œé‚£ä¹ˆ

å…¶ä»–æ©æ¨¡äº§

ç”Ÿç›¸åŒå±€éƒ¨

æ¨¡å¼çš„æ©æ¨¡

ä¹Ÿå¯èƒ½æ˜¯æ­£

æº¶æ¶²ã€‚

--------------------------------------------------



[099] This

can be achieved by

dividing the original model

into K non-overlapping blocks,

represented as Î¦ =

[Î¦1, Î¦2, Â· Â·

Â· , Î¦K]âŠº.

[099]

è¿™å¯ä»¥

é€šè¿‡å°†åŸå§‹

æ¨¡å‹åˆ’åˆ†ä¸º

kéé‡å å—ï¼ˆè¡¨

ç¤ºä¸ºÏ†= [Ï†1ï¼ŒÏ†2ï¼ŒÂ·ï¼ŒÏ†k]âŠºçš„éé‡

å å—ã€‚



--------------------------------------------------

[100] For simplicity, we

assume each block Î¦k

= [Ï•k1, Ï•k2, Â·

Â· Â· , Ï•kM]âŠºcontains

exactly M layers, although

they can have varied

lengths.

[100] ä¸ºç®€å•

èµ·è§ï¼Œæˆ‘ä»¬å‡

è®¾æ¯ä¸ªå—Ï†k= [Ï•k1ï¼ŒÏ•k2ï¼ŒÂ·Â·Â·ï¼ŒÏ•km]âŠºæ°

å¥½åŒ…å«må±‚ï¼Œå°½

ç®¡å®ƒä»¬çš„é•¿

åº¦å¯ä»¥å˜åŒ–

ã€‚

--------------------------------------------------



[101] Instead

of performing global layer

pruning, we propose an

N:M scheme for local

layer pruning, where, for

each block Î¦k with

M layers, N layers

are retained.

[101] æˆ‘ä»¬æ²¡æœ‰æ‰§

è¡Œå…¨å±€å±‚ä¿®

å‰ªï¼Œè€Œæ˜¯æå‡º

äº†å±€éƒ¨å±‚ä¿®

å‰ªçš„Nï¼šMæ–¹æ¡ˆï¼Œåœ¨

å…¶ä¸­ï¼Œå¯¹äºå¸¦

æœ‰Må±‚çš„æ¯ä¸ª

å—Ï†Kï¼Œnå±‚éƒ½ä¿ç•™

ã€‚

--------------------------------------------------



[102] This

results in a set

of local binary masks

m = [m1, m2,

.

[102] è¿™å¯¼è‡´ä¸€ç»„

æœ¬åœ°äºŒè¿›åˆ¶

æ©ç M =

[M1ï¼ŒM2ï¼Œã€‚



--------------------------------------------------

[103] .

[103] ã€‚

--------------------------------------------------



[104] .

[104] ã€‚



--------------------------------------------------

[105] , mK]âŠº.

[105]

ï¼ŒMk]âŠºã€‚



--------------------------------------------------

[106] Simi- larly, the

distribution of a local

mask mk is modeled

using a categorical distribution

p(mk).

[106] åŒæ ·ï¼Œä½¿

ç”¨åˆ†ç±»åˆ†å¸ƒ

Pï¼ˆMKï¼‰å¯¹å±€éƒ¨æ©ç 

MKçš„åˆ†å¸ƒè¿›è¡Œ

å»ºæ¨¡ã€‚

--------------------------------------------------



[107] We

perform independent sampling of

local binary masks and

combine them for prun-

ing, which presents the

joint distribution: p(m) =

p(m1) Â· p(m2) Â·

Â· Â· p(mK) (3)

If some local distributions

p(mk) exhibit high confidence

in the corresponding blocks,

the system will tend

to sam- ple those

positive patterns frequently and

keep active ex- plorations

in other local blocks.

[107] æˆ‘ä»¬å¯¹

å±€éƒ¨äºŒè¿›åˆ¶

è’™ç‰ˆè¿›è¡Œç‹¬

ç«‹çš„æŠ½æ ·ï¼Œå¹¶

å°†å®ƒä»¬ç»“åˆ

èµ·ä¿®å‰ªï¼Œå…¶ä¸­

æå‡ºäº†è”åˆ

åˆ†å¸ƒï¼šPï¼ˆMï¼‰= Pï¼ˆM1ï¼‰Â·Pï¼ˆM2ï¼‰Â·Pï¼ˆM2ï¼‰Â·Pï¼ˆMkï¼‰Pï¼ˆMKï¼‰ï¼ˆMKï¼‰ï¼ˆMKï¼‰ï¼ˆMKï¼‰ï¼ˆMKï¼‰ï¼ˆMKï¼‰ï¼ˆMKï¼‰å¦‚æœåœ¨

ç›¸åº”çš„å—ä¸­

è¡¨ç°å‡ºå¾ˆé«˜

çš„ä¿¡å¿ƒï¼Œåˆ™ç³»

ç»Ÿä¼šå€¾å‘äº

Sam-ple sam-ple

blocks starters starters store

Ex-Plorï¼Œå¹¶ä¿æŒå…¶ä»–

ç§¯æçš„ä½ç½®

- å¹¶ä¿æŒå…¶ä»–

extrors-plorã€‚



--------------------------------------------------

[108] Based on this

concept, we introduce differential

sampling to make the

above process learnable.

[108]

åŸºäºè¿™ä¸ªæ¦‚

å¿µï¼Œæˆ‘ä»¬ä»‹ç»

äº†å·®å¼‚é‡‡æ ·

ï¼Œä»¥ä½¿ä¸Šè¿°è¿‡

ç¨‹å¯å­¦ä¹ ã€‚



--------------------------------------------------

[109] Differentiable Sampling.

[109]

å¯

åŒºåˆ†çš„é‡‡æ ·

ã€‚



--------------------------------------------------

[110] Considering the sampling

pro- cess of a

local mask mk, which

corresponds a local block

Î¦k and is modeled

by a categorical distribution

p(mk).

[110] è€ƒè™‘åˆ°å±€éƒ¨

æ©ç MKçš„é‡‡æ ·

è¿‡ç¨‹ï¼Œè¯¥æ©è†œ

ä¸å±€éƒ¨å—Ï†Kç›¸

å¯¹åº”ï¼Œå¹¶é€šè¿‡

åˆ†ç±»åˆ†å¸ƒPï¼ˆMKï¼‰è¿›

è¡Œå»ºæ¨¡ã€‚

--------------------------------------------------



[111] With

the N:M scheme, there

are  M N 

possible masks.

[111] ä½¿ç”¨

Nï¼šMæ–¹æ¡ˆï¼Œæœ‰å¯èƒ½

çš„æ©æ¨¡ã€‚

--------------------------------------------------



[112] We

construct a special matrix

Ë†mN:M to enumerate all

possi- ble masks.

[112]

æˆ‘ä»¬

æ„å»ºä¸€ä¸ªç‰¹

æ®Šçš„çŸ©é˜µË†mnï¼šmæ¥

æšä¸¾æ‰€æœ‰å¯

èƒ½çš„é¢å…·ã€‚



--------------------------------------------------

[113] For example, 2:3

layer pruning will lead

to the candidate matrix

Ë†m2:3 = [[1, 1,

0] , [1, 0,

1] , [0, 1,

1]].

[113] ä¾‹

å¦‚ï¼Œ2ï¼š3å±‚ä¿®å‰ªå°†

å¯¼è‡´å€™é€‰çŸ©

é˜µË†m2ï¼š3 =

[[1ï¼Œ1ï¼Œ0]ï¼Œ[1ï¼Œ0ï¼Œ1]ï¼Œ[0ï¼Œ1ï¼Œ1ï¼Œ1]]ã€‚



--------------------------------------------------

[114] In this case,

each block will have

three probabilities p(mk) =

[pk1, pk2, pk3].

[114]

åœ¨è¿™ç§æƒ…

å†µä¸‹ï¼Œæ¯ä¸ªå—

å°†å…·æœ‰ä¸‰ä¸ª

æ¦‚ç‡Pï¼ˆMKï¼‰= [PK1ï¼ŒPK2ï¼ŒPK3]ã€‚



--------------------------------------------------

[115] For simplicity, we

omit mk and k

and use pi to

represent the probability of

sampling i-th element in

Ë†mN:M. A popular method

to make a sampling

process dif- ferentiable is

Gumbel-Softmax [13, 17, 22]:

y = one-hot

exp((gi + log pi)/Ï„)

P j exp((gj +

log pj)/Ï„) !

[115]

ä¸ºç®€å•

èµ·è§ï¼Œæˆ‘ä»¬çœ

ç•¥äº†MKå’ŒKï¼Œå¹¶ä½¿

ç”¨PIè¡¨ç¤ºåœ¨Ë†mnï¼šmä¸­

å¯¹ç¬¬i-thå…ƒç´ è¿›

è¡Œé‡‡æ ·çš„æ¦‚

ç‡ã€‚ä½¿é‡‡æ ·è¿‡

ç¨‹åˆ†åŒ–çš„ä¸€

ç§æµè¡Œæ–¹æ³•

æ˜¯Gumbel-Softmax [13ï¼Œ17ï¼Œ22]ï¼šy =ä¸€hot expï¼ˆï¼ˆï¼ˆgi

+ log piï¼‰/Ï„ï¼‰p j

expï¼ˆï¼ˆgj + log pjï¼‰/Ï„ï¼‰ï¼

--------------------------------------------------



[116] .

[116] ã€‚



--------------------------------------------------

[117] (4) where gi

is random noise drawn

from the Gumbel distribu-

tion Gumbel(0, 1) and

Ï„ refers to the

temperature term.

[117] ï¼ˆ4ï¼‰å…¶ä¸­GIæ˜¯

éšæœºå™ªå£°ï¼Œå®ƒ

æ˜¯ä»ç‰™é¾ˆåˆ†

å¸ƒï¼ˆ0ï¼Œ1ï¼‰ä¸­ç»˜åˆ¶çš„

ï¼ŒÏ„æ˜¯æŒ‡æ¸©åº¦é¡¹

ã€‚

--------------------------------------------------



[118] The

output y is the

index of the sampled

mask.

[118] è¾“å‡ºyæ˜¯é‡‡æ ·

è’™ç‰ˆçš„ç´¢å¼•

ã€‚

--------------------------------------------------



[119] Here

a Straight- Through Estimator

[2] is applied to

the one-hot operation, where

the onehot operation is

enabled during forward and

is treated as an

identity function during backward.

[119] åœ¨è¿™é‡Œï¼Œç›´æ¥

é€šè¿‡ä¼°è®¡å™¨

[2]åº”ç”¨äºå•é€Ÿ

æ“ä½œï¼Œè¯¥æ“ä½œ

åœ¨å‘å‰æ—¶å¯

ç”¨äº†orhotæ“ä½œï¼Œå¹¶

å°†å…¶è§†ä¸ºåœ¨

å‘åæ—¶ä½œä¸º

èº«ä»½å‡½æ•°ã€‚



--------------------------------------------------

[120] Leverag- ing the

one-hot index y and

the candidate set Ë†mN:M,

we can draw a

mask m âˆ¼p(m) through

a simple index operation:

m = yâŠºË†m (5)

Pretrained ğ‘Š A r

Identity f(x)=x â¨‚ ğ‘šğ‘–

â¨‚ + (1 âˆ’ğ‘šğ‘–)

ğ‘¥ğ‘– ğ‘¥ğ‘–+1 B ğ‘Ã—

Figure 3.

[120] æ 

æ†ç‡å•é«˜ç´¢

å¼•yå’Œå€™é€‰ç»„

é›†Ë†

mnï¼šmï¼Œæˆ‘ä»¬å¯ä»¥

é€šè¿‡ç®€å•çš„

ç´¢å¼•æ“ä½œç»˜

åˆ¶è’™ç‰ˆmã€œpï¼ˆmï¼‰ï¼šm = yâŠºË†mï¼ˆ5ï¼‰é¢„å¤„

ç†çš„a a

a a a a

a a råŒä¸€æ€§

fï¼ˆxï¼‰= xâ¨‚

+ï¼ˆ1 âˆ’1-ğ‘šğ‘– +ï¼ˆ1-ğ‘šğ‘– +

1 b + 1

bğ‘ +ï¼‰ã€‚



--------------------------------------------------

[121] An example of

forward propagation with differentiable

pruning mask mi and

LoRA for recoverability estimation.

[121] å¸¦æœ‰å¯æ¢å¤

æ€§ä¼°ç®—çš„å¯

æ¢å¤æ€§è’™ç‰ˆ

MIå’ŒLORAçš„æ­£å‘ä¼ 

æ’­çš„ä¸€ä¸ªä¾‹

å­ã€‚



--------------------------------------------------

[122] Notably, when Ï„

â†’0, the STE gradients

will approximate the true

gradients, yet with a

higher variance which is

neg- ative for training

[22].

[122] å€¼å¾—æ³¨æ„

çš„æ˜¯ï¼Œå½“Ï„â†’0æ—¶ï¼ŒSteæ¢¯

åº¦å°†è¿‘ä¼¼çœŸ

æ­£çš„æ¢¯åº¦ï¼Œä½†

å…·æœ‰æ›´é«˜çš„

æ–¹å·®ï¼Œå¯¹è®­ç»ƒ

çš„å·®å¼‚å¾ˆé«˜

[22]ã€‚

--------------------------------------------------



[123] Thus,

a scheduler is typically

em- ployed to initiate

training with a high

temperature, gradu- ally reducing

it over time.

[123]

å› æ­¤ï¼Œè°ƒåº¦ç¨‹

åºé€šå¸¸ä¼šä»¥

é«˜æ¸©å¯åŠ¨è®­

ç»ƒï¼Œéšç€æ—¶é—´

çš„æµé€è€Œé€

æ¸å‡å°‘ã€‚



--------------------------------------------------

[124] Joint Optimization with

Recoverability.

[124] è”åˆ

ä¼˜åŒ–ï¼Œå¯æ¢å¤

æ€§ã€‚

--------------------------------------------------



[125] With

differen- tiable sampling, we

are able to update

the underlying prob- ability

using gradient descent.

[125]

é€šè¿‡ä¸åŒ

çš„é‡‡æ ·ï¼Œæˆ‘ä»¬

èƒ½å¤Ÿä½¿ç”¨æ¢¯

åº¦ä¸‹é™æ¥æ›´

æ–°æ½œåœ¨çš„æ¦‚

ç‡ã€‚



--------------------------------------------------

[126] The training objective

in this work is

to maximize the recoverability

of sampled masks.

[126]

è¿™é¡¹å·¥ä½œ

çš„è®­ç»ƒç›®æ ‡

æ˜¯æœ€å¤§åŒ–é‡‡

æ ·é¢å…·çš„å¯

å›æ”¶æ€§ã€‚



--------------------------------------------------

[127] We reformulate the

objective in Equation 2

by incorporat- ing the

learnable distribution: min {p(mk)}

min âˆ†Î¦ Ex,{mkâˆ¼p(mk)} [L(x,

Î¦ + âˆ†Î¦, {mk}]

| {z } Recoverability:

Post-Fine-Tuning Performance , (6)

where {p(mk)} = {p(m1),

Â· Â· Â· ,

p(mK)} refer to the

cat- egorical distributions for

different local blocks.

[127]

æˆ‘ä»¬

é€šè¿‡åˆå¹¶å¯

å­¦ä¹ çš„åˆ†å¸ƒ

æ¥é‡æ–°é‡æ–°

åˆ¶å®šç›®æ ‡2ä¸­

çš„ç›®æ ‡ï¼šmin {pï¼ˆmkï¼‰} min âˆ†Ï†

exï¼Œ{mkã€œpï¼ˆmkï¼‰} [lï¼ˆxï¼ŒÏ† + âˆ†Ï†ï¼Œ{mk}

| {z} | {z}

| {z} | {z}æ¢å¤

æ€§ï¼šæ¢å¤æ€§ï¼šæ¢

å¤æ€§èƒ½ï¼šfine-fine-tonning

performanceï¼Œpost-fine-tuning performanceï¼Œpost-fine ther pï¼ˆmkï¼‰}æœ‰å…³

ä¸åŒå±€éƒ¨å—

çš„çŒ«åˆ†å¸ƒã€‚

--------------------------------------------------



[128] Based

on this formulation, we

further investigate how to

incorporate the fine-tuning information

into the training.

[128]

åŸº

äºæ­¤å…¬å¼ï¼Œæˆ‘

ä»¬è¿›ä¸€æ­¥ç ”

ç©¶äº†å¦‚ä½•å°†

å¾®è°ƒä¿¡æ¯çº³

å…¥åŸ¹è®­ã€‚



--------------------------------------------------

[129] We propose a

joint optimization of the

distribution and a weight

update âˆ†Î¦.

[129] æˆ‘ä»¬

æå‡ºäº†åˆ†å¸ƒ

çš„è”åˆä¼˜åŒ–

å’Œæƒé‡æ›´æ–°

âˆ†Ï†ã€‚

--------------------------------------------------



[130] Our

key idea is to

introduce a co-optimized update

âˆ†Î¦ for joint training.

[130] æˆ‘ä»¬çš„å…³é”®

æ€æƒ³æ˜¯å¼•å…¥

åˆä½œæ›´æ–°Î”Ï†ä»¥

è¿›è¡Œå…³èŠ‚è®­

ç»ƒã€‚



--------------------------------------------------

[131] A straightforward way

to craft the update

is to directly optimize

the original network.

[131]

åˆ¶ä½œæ›´æ–°

çš„ç›´æ¥æ–¹æ³•

æ˜¯ç›´æ¥ä¼˜åŒ–

åŸå§‹ç½‘ç»œã€‚



--------------------------------------------------

[132] However, the parameter

scale in a diffusion

transformer is usually huge,

and a full optimization

may make the training

process costly and not

that efficient.

[132] ä½†

æ˜¯ï¼Œæ‰©æ•£å˜å‹

å™¨ä¸­çš„å‚æ•°

é‡è¡¨é€šå¸¸æ˜¯

å·¨å¤§çš„ï¼Œå¹¶ä¸”

å®Œæ•´çš„ä¼˜åŒ–

å¯èƒ½ä¼šä½¿è®­

ç»ƒè¿‡ç¨‹æˆæœ¬

é«˜æ˜‚ï¼Œè€Œä¸”æ•ˆ

ç‡ä¸é«˜ã€‚

--------------------------------------------------



[133] To

this end, we show

that Parameter- Efficient Fine-Tuning

methods such as LoRA

[21] can be a

good choice to obtain

the required âˆ†Î¦.

[133]

ä¸ºæ­¤

ï¼Œæˆ‘ä»¬è¡¨æ˜å‚

æ•°æœ‰æ•ˆçš„å¾®

è°ƒæ–¹æ³•ï¼ˆä¾‹å¦‚

Lora [21]ï¼‰å¯ä»¥æ˜¯è·å¾—

æ‰€éœ€âˆ†Ï†çš„å¥½é€‰

æ‹©ã€‚



--------------------------------------------------

[134] For a single

linear matrix W in

Î¦, we simulate the

fine-tuned weights as: Wfine-tuned

= W + Î±âˆ†W

= W + Î±BA,

(7) where Î± is

a scalar hyperparameter that

scales the contribu- tion

of âˆ†W.

[134] å¯¹äºå•ä¸ª

çº¿æ€§çŸ©é˜µWÏ†ï¼Œæˆ‘

ä»¬å°†å¾®è°ƒçš„

æƒé‡æ¨¡æ‹Ÿä¸º

ï¼šwfine-tuned

= w +Î±Î”W= w

+Î±bAï¼Œï¼ˆ7ï¼‰ï¼Œå…¶ä¸­Î±æ˜¯æ ‡é‡

è¶…å‚æ•°ï¼Œå¯ç¼©

æ”¾Î”Wçš„è´¡çŒ®ã€‚



--------------------------------------------------

[135] Using LoRA significantly

reduces the num- ber

of parameters, facilitating efficient

exploration of differ- ent

pruning decisions.

[135] ä½¿

ç”¨æ´›æ‹‰å¤§å¤§

å‡å°‘äº†å‚æ•°

çš„æ•°é‡ï¼Œä»è€Œ

æœ‰åŠ©äºå¯¹ä¸

åŒçš„ä¿®å‰ªå†³

ç­–çš„æœ‰æ•ˆæ¢

ç´¢ã€‚

--------------------------------------------------



[136] As

shown in Figure 3,

we leverage the sampled

binary mask value mi

as the gate and

forward the network with

Equation 1, which suppresses

the layer outputs if

the sampled mask is

0 for the current

layer.

[136] å¦‚å›¾3æ‰€ç¤º

ï¼Œæˆ‘ä»¬åˆ©ç”¨é‡‡

æ ·çš„äºŒè¿›åˆ¶

æ©ç å€¼MIä½œä¸º

é—¨ï¼Œå¹¶ç”¨æ–¹ç¨‹

1å‘ç½‘ç»œè½¬å‘

ï¼Œå¦‚æœå½“å‰å±‚

é‡‡æ ·çš„æ©ç 

ä¸º0ï¼Œåˆ™æŠ‘åˆ¶å±‚

è¾“å‡ºçš„å±‚è¾“

å‡ºã€‚

--------------------------------------------------



[137] In

addition, the previously mentioned

STE will still provide

non-zero gradients to the

pruned layer, allowing it

to be fur- ther

updated.

[137] æ­¤å¤–ï¼Œå‰é¢

æåˆ°çš„Steä»å°†

ä¸ºä¿®å‰ªå±‚æ

ä¾›éé›¶æ¢¯åº¦

ï¼Œä»è€Œå¯ä»¥è¿›

è¡Œæ›´æ–°ã€‚

--------------------------------------------------



[138] This

is helpful in practice,

since some layers 4

[138] è¿™åœ¨

å®è·µä¸­å¾ˆæœ‰

å¸®åŠ©ï¼Œå› ä¸ºæœ‰

äº›å±‚4



--------------------------------------------------

[139] Method Depth #Param

Iters IS â†‘ FID

â†“ sFID â†“ Prec.

[139] æ–¹æ³•æ·±

åº¦#param Itersæ˜¯â†‘fidâ†“SFIDâ†“PRECã€‚

--------------------------------------------------



[140] â†‘

Recall â†‘ Sampling it/s

â†‘ DiT-XL/2 [40] 28

675 M 7,000 K

278.24 2.27 4.60 0.83

0.57 6.91 DiT-XL/2 [40]

28 675 M 2,000

K 240.22 2.73 4.46

0.83 0.55 6.91 DiT-XL/2

[40] 28 675 M

1,000 K 157.83 5.53

4.60 0.80 0.53 6.91

U-ViT-H/2 [1] 29 501

M 500 K 265.30

2.30 5.60 0.82 0.58

8.21 ShortGPT [36] 28â‡’19

459 M 100 K

132.79 7.93 5.25 0.76

0.53 10.07 TinyDiT-D19 (KD)

28â‡’19 459 M 100

K 242.29 2.90 4.63

0.84 0.54 10.07 TinyDiT-D19

(KD) 28â‡’19 459 M

500 K 251.02 2.55

4.57 0.83 0.55 10.07

DiT-L/2 [40] 24 458

M 1,000 K 196.26

3.73 4.62 0.82 0.54

9.73 U-ViT-L [1] 21

287 M 300 K

221.29 3.44 6.58 0.83

0.52 13.48 U-DiT-L [50]

22 204 M 400

K 246.03 3.37 4.49

0.86 0.50 - Diff-Pruning-50%

[12] 28 338 M

100 K 186.02 3.85

4.92 0.82 0.54 10.43

Diff-Pruning-75% [12] 28 169

M 100 K 83.78

14.58 6.28 0.72 0.53

13.59 ShortGPT [36] 28â‡’14

340 M 100 K

66.10 22.28 6.20 0.63

0.56 13.54 Flux-Lite [6]

28â‡’14 340 M 100

K 54.54 25.92 5.98

0.62 0.55 13.54 Sensitivity

Analysis [18] 28â‡’14 340

M 100 K 70.36

21.15 6.22 0.63 0.57

13.54 Oracle (BK-SDM) [23]

28â‡’14 340 M 100

K 141.18 7.43 6.09

0.75 0.55 13.54 TinyDiT-D14

28â‡’14 340 M 100

K 151.88 5.73 4.91

0.80 0.55 13.54 TinyDiT-D14

28â‡’14 340 M 500

K 198.85 3.92 5.69

0.78 0.58 13.54 TinyDiT-D14

(KD) 28â‡’14 340 M

100 K 207.27 3.73

5.04 0.81 0.54 13.54

TinyDiT-D14 (KD) 28â‡’14 340

M 500 K 234.50

2.86 4.75 0.82 0.55

13.54 DiT-B/2 [40] 12

130 M 1,000 K

119.63 10.12 5.39 0.73

0.55 28.30 U-DiT-B [50]

22 - 400 K

85.15 16.64 6.33 0.64

0.63 - TinyDiT-D7 (KD)

14â‡’7 173 M 500

K 166.91 5.87 5.43

0.78 0.53 26.81 Table

1.

[140] â†‘ Recall

â†‘ Sampling it/s â†‘

DiT-XL/2 [40] 28 675

M 7,000 K 278.24

2.27 4.60 0.83 0.57

6.91 DiT-XL/2 [40] 28

675 M 2,000 K

240.22 2.73 4.46 0.83

0.55 6.91 DiT-XL/2 [40]

28 675 M 1,000

K 157.83 5.53 4.60

0.80 0.53 6.91 U-ViT-H/2

[1] 29 501 M

500 K 265.30 2.30

5.60 0.82 0.58 8.21

ShortGPT [36] 28â‡’19 459

M 100 K 132.79

7.93 5.25 0.76 0.53

10.07 TinyDiT-D19 (KD) 28â‡’19

459 M 100 K

242.29 2.90 4.63 0.84

0.54 10.07 Tinydit-D19ï¼ˆKDï¼‰28â‡’19459 M

500 K 251.02 2.55

4.57 0.83 0.55 0.55

10.07 DIT-L/2 [40] 24

458 M 1,000 K

196.26.26.26 3.73 4.62 4.62

0.82 0.54 9.73 U-Vit-L

[1] 9.73 U-Vit-L [1]

U-Vit-L [1] 21 287

M 300 M 300

M 300 K 22129.44.44.44.44.44.44.44.44.44.44.44

4.44.44.44.44.44.444.44.44.44 k 2294.44.44.44.44.44.44 4.44

4.44 4.44 4.44 4.44

4.44 4.44 4.44.444.; U-DiT-L

[50] 22 204 M

400 K 246.03 3.37

4.49 0.86 0.50 -

Diff-Pruning-50% [12] 28 338

M 100 K 186.02

3.85 4.92 0.82 0.54

10.43 Diff-Pruning-75% [12] 28

169 M 100 K

83.78 14.58 6.28 0.72

0.53 13.59 ShortGPT [36]

28â‡’14 340 M 100

K 66.10 22.28 6.20

0.63 0.56 13.54 Flux-Lite

[6] 28â‡’14 340 M

100 K 54.54 25.92

5.98 0.62 0.55 13.54

Sensitivity Analysis [18] 28â‡’14

340 M 100 K

70.36 21.15 6.22 0.63

0.57 13.54 Oracle (BK-SDM)

[23] 28â‡’14340 M 100

K 141.18 7.43 6.09

0.75 0.55 0.55 13.54

TINYDIT-D1428â‡’14340 M 100 K

100 K 151.88 5.73

4.91 0.80 0.55 13.55

13.54 TINYDIT-D14 28çœ‹çœ‹14 340

M 500 K 198.85

3.92 5.62 5.62 5.62

0.78 14.78 148ï¼ˆ 28â‡’14

340 M 100 K

207.27 3.73 5.04 0.81

0.54 13.54 TinyDiT-D14 (KD)

28â‡’14 340 M 500

K 234.50 2.86 4.75

0.82 0.55 13.54 DiT-B/2

[40] 12 130 M

1,000 K 119.63 10.12

5.39 0.73 0.55 28.30

U-DiT-B [50] 22 -

400 K 85.15 16.64

6.33 0.64 0.63 -tinydit

-d7ï¼ˆkdï¼‰14â‡’7173 M 500 K

166.91 5.87 5.43 5.43

0.78 0.78 0.53 26.81è¡¨

1ã€‚

--------------------------------------------------



[141] Layer

pruning results for pre-trained

DiT-XL/2.

[141] é¢„å…ˆè®­ç»ƒçš„

DIT-XL/2çš„å±‚ä¿®å‰ªç»“

æœã€‚

--------------------------------------------------



[142] We

focus on two settings:

fast training with 100K

optimization steps and sufficient

fine-tuning with 500K steps.

[142] æˆ‘ä»¬ä¸“æ³¨

äºä¸¤ä¸ªè®¾ç½®

ï¼šé€šè¿‡100kä¼˜åŒ–æ­¥

éª¤è¿›è¡Œå¿«é€Ÿ

åŸ¹è®­ï¼Œå¹¶é€šè¿‡

500kæ­¥éª¤è¿›è¡Œäº†

è¶³å¤Ÿçš„å¾®è°ƒ

ã€‚



--------------------------------------------------

[143] Both fine-tuning and

Masked Knowledge Distillation (a

variant of KD, see

Sec.

[143] å¾®è°ƒå’Œæ©ç›–

çš„çŸ¥è¯†è’¸é¦

ï¼ˆKDçš„ä¸€ç§å˜ä½“

ï¼Œè¯·å‚è§ç§’ã€‚

--------------------------------------------------



[144] 4.4)

are used for recovery.

[144] 4.4ï¼‰ç”¨

äºæ¢å¤ã€‚



--------------------------------------------------

[145] might not be

competitive at the beginning,

but may emerge as

competitive candidates with sufficient

fine-tuning.

[145] ä¸€å¼€

å§‹å¯èƒ½ä¸æ˜¯

ç«äº‰æ€§çš„ï¼Œä½†

å¯èƒ½ä¼šæˆä¸º

å…·æœ‰è¶³å¤Ÿå¾®

è°ƒçš„ç«äº‰å€™

é€‰äººã€‚

--------------------------------------------------



[146] Pruning

Decision.

[146] ä¿®å‰ªå†³

å®šã€‚

--------------------------------------------------



[147] After

training, we retain those

local structures with the

highest probability and discard

the ad- ditional update

âˆ†Î¦.

[147] è®­ç»ƒåï¼Œæˆ‘

ä»¬ä¿ç•™äº†æœ€

é«˜æ¦‚ç‡çš„é‚£

äº›å±€éƒ¨ç»“æ„

ï¼Œå¹¶ä¸¢å¼ƒäº†å¹¿

å‘Šæ›´æ–°Î”Ï†ã€‚

--------------------------------------------------



[148] Then,

standard fine-tuning techniques can

be applied for recovery.

[148] ç„¶å

ï¼Œå¯ä»¥åº”ç”¨æ ‡

å‡†çš„å¾®è°ƒæŠ€

æœ¯è¿›è¡Œæ¢å¤

ã€‚



--------------------------------------------------

[149] 4.

[149] 4ã€‚

--------------------------------------------------



[150] Experiments

4.1.

[150] å®éªŒ4.1ã€‚

--------------------------------------------------



[151] Experimental

Settings Our experiments were

mainly conducted on Diffusion

Transformers [40] for class-conditional

image generation on ImageNet

256 Ã— 256 [8].

[151] å®éªŒè®¾

ç½®æˆ‘ä»¬çš„å®

éªŒä¸»è¦æ˜¯åœ¨

æ‰©æ•£å˜å‹å™¨

[40]ä¸Šè¿›è¡Œçš„ï¼Œä»¥

åœ¨ImageNet 256Ã—256 [8]ä¸Šè¿›è¡Œç±»

æ¡ä»¶å›¾åƒç”Ÿ

æˆã€‚

--------------------------------------------------



[152] For

evaluation, we fol- low

[9, 40] and report

the FrÂ´echet inception distance

(FID), Sliding FrÂ´echet Inception

Distance (sFID), Inception Scores

(IS), Precision and Recall

using the official reference

im- ages [9].

[152]

ä¸ºäº†è¿›è¡Œ

è¯„ä¼°ï¼Œæˆ‘ä»¬å°†

[9ï¼Œ40]æŠ¥å‘Šï¼Œå¹¶æŠ¥å‘Š

äº†ä½¿ç”¨æ­£å¼

çš„å‚è€ƒæ–‡çŒ®

[9]ï¼ŒæŠ¥å‘Šäº†FRÂ´Echet Inceptionè·ç¦»

ï¼ˆFIDï¼‰ï¼ŒSLIVE frÂ´echet Inceptionè·ç¦»ï¼ˆSFIDï¼‰ï¼ŒINCEPTIONåˆ†æ•°ï¼ˆISï¼‰ï¼Œç²¾

åº¦å’Œå¬å›ã€‚

--------------------------------------------------



[153] Additionally,

we also extend our

methods to other models,

including MARs [29] and

SiTs [34].

[153] æ­¤

å¤–ï¼Œæˆ‘ä»¬è¿˜å°†

æˆ‘ä»¬çš„æ–¹æ³•

æ‰©å±•åˆ°å…¶ä»–

æ¨¡å‹ï¼ŒåŒ…æ‹¬ç«

æ˜Ÿ[29]å’Œä½äº[34]ã€‚

--------------------------------------------------



[154] Experimental

details can be found

in the following sections

and appendix.

[154] å®

éªŒç»†èŠ‚å¯ä»¥

åœ¨ä»¥ä¸‹å„èŠ‚

å’Œé™„å½•ä¸­æ‰¾

åˆ°ã€‚

--------------------------------------------------



[155] 4.2.

[155] 4.2ã€‚



--------------------------------------------------

[156] Results on Diffusion

Transformers DiT.

[156] æ‰©æ•£å˜å‹

å™¨DITçš„ç»“æœã€‚

--------------------------------------------------



[157] This

work focuses on the

compression of DiTs [40].

[157] è¿™

é¡¹å·¥ä½œç€é‡

äºDITçš„å‹ç¼©[40]ã€‚



--------------------------------------------------

[158] We consider two

primary strategies as baselines:

the first 0 20

40 60 80 Compression

Ratio (%) 2 4

6 8 10 12

Speed Up 1.08 1.17

1.27 1.40 1.55 1.74

1.99 2.30 2.76 3.41

4.46 6.45 11.60 1.04

1.26 1.36 1.64 1.91

2.20 2.71 3.36 4.39

Depth Pruning Width Pruning

Linear Speedup Figure 4.

[158] We consider two

primary strategies as baselines:

the first 0 20

40 60 80 Compression

Ratio (%) 2 4

6 8 10 12

Speed Up 1.08 1.17

1.27 1.40 1.55 1.74

1.99 2.30 2.76 3.41

4.46 6.45 11.60 1.04

1.26 1.36 1.64 1.91

2.20 2.71 3.36 4.39

Depth Pruning Width Pruning

Linear Speedup Figure 4.

--------------------------------------------------



[159] Depth

pruning closely aligns with

the theoretical linear speed-up

relative to the compression

ratio.

[159] æ·±

åº¦ä¿®å‰ªä¸ç†

è®ºçº¿æ€§åŠ é€Ÿ

ç›¸å¯¹äºå‹ç¼©

ç‡ç´§å¯†å¯¹é½

ã€‚

--------------------------------------------------



[160] involves

using manually crafted patterns

to eliminate lay- ers.

[160] æ¶‰åŠä½¿ç”¨æ‰‹

åŠ¨åˆ¶ä½œçš„å›¾

æ¡ˆæ¶ˆé™¤å¤–è¡Œ

ã€‚



--------------------------------------------------

[161] For instance, BK-SDM

[23] employs heuristic assump-

tions to determine the

significance of specific layers,

such as the initial

or final layers.

[161]

ä¾‹å¦‚ï¼ŒBK-SDM [23]é‡‡ç”¨å¯

å‘å¼è¾…åŠ©æ¥

ç¡®å®šç‰¹å®šå±‚

çš„é‡è¦æ€§ï¼Œä¾‹

å¦‚åˆå§‹æˆ–æœ€

ç»ˆå±‚ã€‚



--------------------------------------------------

[162] The second strategy

is based on systematically

designed criteria to evaluate

layer impor- tance, such

as analyzing the similarity

between block in- puts

and outputs to determine

redundancy [6, 36]; this

ap- proach typically aims

to minimize performance degradation

after pruning.

[162] ç¬¬äºŒç§

ç­–ç•¥æ˜¯åŸºäº

ç³»ç»Ÿè®¾è®¡çš„

æ ‡å‡†æ¥è¯„ä¼°

å±‚çš„é‡è¦æ€§

ï¼Œä¾‹å¦‚åˆ†æå—

In-ofså’Œè¾“å‡ºä¹‹é—´

çš„ç›¸ä¼¼æ€§ä»¥

ç¡®å®šå†—ä½™[6ï¼Œ36]ï¼›è¿™

ç§æ–¹æ³•é€šå¸¸

æ—¨åœ¨æœ€å¤§ç¨‹

åº¦åœ°å‡å°‘ä¿®

å‰ªåçš„æ€§èƒ½

é™è§£ã€‚

--------------------------------------------------



[163] Table

1 presents representatives from

both strategies, including ShortGPT

[36], Flux-Lite [6], Diff-

Pruning [12], Sensitivity Analysis

[18] and BK-SDM [23],

which serve as baselines

for comparison.

[163] è¡¨1ä»‹ç»

äº†ä¸¤ç§ç­–ç•¥

çš„ä»£è¡¨ï¼ŒåŒ…æ‹¬

çŸ­é€Ÿåº¦[36]ï¼ŒFlux-Lite

[6]ï¼ŒDiff-pruning [12]ï¼Œçµæ•

åº¦åˆ†æ[18]å’ŒBK-SDM [23]ï¼Œå®ƒ

ä»¬æ˜¯æ¯”è¾ƒçš„

åŸºç¡€ã€‚

--------------------------------------------------



[164] Additionally,

5

[164] å¦å¤–ï¼Œ5

--------------------------------------------------



[165] Method

Depth Params Epochs FID

IS MAR-Large 32 479

M 400 1.78 296.0

MAR-Base 24 208 M

400 2.31 281.7 TinyMAR-D16

32â‡’16 277 M 40

2.28 283.4 SiT-XL/2 28

675 M 1,400 2.06

277.5 TinySiT-D14 28â‡’14 340

M 100 3.02 220.1

Table 2.

[165] Method

Depth Params Epochs FID

IS MAR-Large 32 479

M 400 1.78 296.0

MAR-Base 24 208 M

400 2.31 281.7 TinyMAR-D16

32â‡’16 277 M 40

2.28 283.4 SiT-XL/2 28

675 M 1,400 2.06

277.5 TinySiT-D14 28â‡’14 340

M 100 3.02 220.1

Table 2.



--------------------------------------------------

[166] Depth pruning results

on MARs [29] and

SiTs [34].

[166] å¯¹

ç«æ˜Ÿçš„æ·±åº¦

ä¿®å‰ªç»“æœ[29]å¹¶

ä½äº[34]ã€‚

--------------------------------------------------



[167] we

evaluate our method against

innovative architectural de- signs,

such as UViT [1],

U-DiT [50], and DTR

[39], which have demonstrated

improved training efficiency over

con- ventional DiTs.

[167]

æˆ‘ä»¬è¯„

ä¼°äº†é’ˆå¯¹åˆ›

æ–°å»ºç­‘çš„æ–¹

æ³•ï¼Œä¾‹å¦‚UVIT [1]ï¼ŒU-DIT [50]å’ŒDTR [39]ï¼Œè¿™

äº›æ–¹æ³•è¡¨æ˜

ï¼Œè¿™äº›æ–¹æ³•è¡¨

ç°å‡ºäº†æé«˜

çš„åŸ¹è®­æ•ˆç‡

ï¼Œè€Œä¸æ˜¯é™çº§

ã€‚

--------------------------------------------------



[168] Table

1 presents our findings

on compressing a pre-

trained DiT-XL/2 [40].

[168]

è¡¨1åˆ—å‡ºäº†æˆ‘

ä»¬å…³äºå‹ç¼©

é¢„è®­ç»ƒçš„DIT-XL/2 [40]çš„

å‘ç°ã€‚



--------------------------------------------------

[169] This model contains

28 transformer layers structured

with alternating Attention and

MLP lay- ers.

[169]

è¯¥æ¨¡å‹

åŒ…å«28ä¸ªå…·æœ‰

äº¤æ›¿æ³¨æ„å’Œ

MLPå¤–è¡Œçš„å˜å‹

å™¨å±‚ã€‚



--------------------------------------------------

[170] The proposed method

seeks to identify shallow

trans- formers with {7,

14, 19} sub-layers from

these 28 layers, to

maximize the post-fine-tuning performance.

[170] æ‰€æå‡º

çš„æ–¹æ³•æ—¨åœ¨

é‰´å®šè¿™28å±‚ä¸­

{7ã€14ã€19}å­å±‚çš„æµ…å‹

ï¼Œä»¥æœ€å¤§ç¨‹åº¦

åœ°æé«˜åè°ƒ

èŠ‚æ€§èƒ½ã€‚



--------------------------------------------------

[171] With only 7%

of the original training

cost (500K steps compared

to 7M steps), TinyDiT

achieves competitive performance rela-

tive to both pruning-based

methods and novel architectures.

[171] Tinyditåªæœ‰

7ï¼…çš„åŸå§‹åŸ¹è®­

æˆæœ¬ï¼ˆ500kæ­¥é•¿ï¼‰ï¼Œä¸

åŸºäºä¿®å‰ªçš„

æ–¹æ³•å’Œæ–°å‹

ä½“ç³»ç»“æ„ç›¸

å…³çš„ç«äº‰æ€§

èƒ½ã€‚



--------------------------------------------------

[172] For instance, a

DiT-L model trained from

scratch for 1M steps

achieves an FID score

of 3.73 with 458M

parameters.

[172] ä¾‹å¦‚ï¼Œé€šè¿‡

ä»å¤´å¼€å§‹è®­

ç»ƒ1Mæ­¥éª¤çš„DIT-Læ¨¡

å‹ä»¥4.58äº¿å‚æ•°

çš„æˆç»©è¾¾åˆ°

3.73çš„FIDåˆ†æ•°ã€‚

--------------------------------------------------



[173] In

contrast, the compressed TinyDiT-D14

model, with only 340M

parameters and a faster

sampling speed (13.54 it/s

vs. 9.73 it/s), yields

a significantly improved FID

of 2.86.

[173] ç›¸æ¯”

ä¹‹ä¸‹ï¼Œä»…å…·æœ‰

340må‚æ•°å’Œæ›´å¿«

çš„é‡‡æ ·é€Ÿåº¦

ï¼ˆ13.54

IT/s vs. 9.73 IT/sï¼‰çš„å‹ç¼©TinyDit-D14æ¨¡å‹

å¯æ˜¾ç€æé«˜

2.86çš„FIDã€‚

--------------------------------------------------



[174] On

parallel devices like GPUs,

the primary bottleneck in

trans- formers arises from

sequential operations within each

layer, which becomes more

pronounced as the number

of layers increases.

[174]

åœ¨è¯¸å¦‚GPUä¹‹

ç±»çš„å¹¶è¡Œè®¾

å¤‡ä¸Šï¼Œè½¬ç§»å™¨

ä¸­çš„ä¸»è¦ç“¶

é¢ˆæºäºæ¯ä¸€

å±‚å†…çš„é¡ºåº

æ“ä½œï¼Œéšç€å±‚

æ•°çš„å¢åŠ ï¼Œè¿™

ä¼šå˜å¾—æ›´åŠ 

æ˜æ˜¾ã€‚



--------------------------------------------------

[175] Depth pruning mitigates

this bottleneck by re-

moving entire transformer layers,

thereby reducing compu- tational

depth and optimizing the

workload.

[175] æ·±åº¦ä¿®

å‰ªé€šè¿‡é‡æ–°

ç§»åŠ¨æ•´ä¸ªå˜

å‹å™¨å±‚æ¥å‡

è½»è¿™ç§ç“¶é¢ˆ

ï¼Œä»è€Œå‡å°‘ç»„

åˆæ·±åº¦å¹¶ä¼˜

åŒ–å·¥ä½œé‡ã€‚

--------------------------------------------------



[176] By

compar- ison, width pruning

only reduces the number

of neurons within each

layer, limiting its speed-up

potential.

[176] ç›¸

æ¯”ä¹‹ä¸‹ï¼Œä¿®å‰ª

å®½åº¦ä»…ä¼šå‡

å°‘æ¯ä¸€å±‚å†…

ç¥ç»å…ƒçš„æ•°

é‡ï¼Œä»è€Œé™åˆ¶

äº†å…¶åŠ é€ŸåŠ¿

ã€‚

--------------------------------------------------



[177] As

shown in Figure 4,

depth pruning closely matches

the theoretical linear speed-up

as the compression ratio

increases, outper- forming width

pruning methods such as

Diff-Pruning [12].

[177] å¦‚å›¾4æ‰€ç¤ºï¼Œéš

ç€å‹ç¼©æ¯”çš„

å¢åŠ ï¼Œæ·±åº¦ä¿®

å‰ªä¸ç†è®ºçº¿

æ€§åŠ é€Ÿå¯†åˆ‡

åŒ¹é…ï¼Œè¯¸å¦‚diff-pruningä¹‹

ç±»çš„å®½åº¦å®½

åº¦è¾ƒé«˜[12]ã€‚

--------------------------------------------------



[178] MAR

& SiT.

[178] Marï¼†Sitã€‚

--------------------------------------------------



[179] Masked

Autoregressive (MAR) [29] mod-

els employ a diffusion

loss-based autoregressive framework in

a continuous-valued space, achieving

high-quality image generation without

the need for discrete

tokenization.

[179] è’™é¢

è‡ªå›æ—‹ï¼ˆMARï¼‰[29]æ¨¡å‹

åœ¨è¿ç»­å€¼çš„

ç©ºé—´ä¸­é‡‡ç”¨

åŸºäºæ‰©æ•£æŸ

å¤±çš„è‡ªå›æ—‹

æ¡†æ¶ï¼Œå®ç°äº†

é«˜è´¨é‡çš„å›¾

åƒç”Ÿæˆè€Œæ— 

éœ€ç¦»æ•£ä»¤ç‰Œ

åŒ–ã€‚

--------------------------------------------------



[180] The

MAR-Large model, with 32

transformer blocks, serves as

the baseline for comparison.

[180] å…·æœ‰32ä¸ªå˜

å‹å™¨å—çš„MAR-LARGEæ¨¡

å‹æ˜¯æ¯”è¾ƒçš„

åŸºçº¿ã€‚



--------------------------------------------------

[181] Applying our pruning

method, we reduced MAR

to a 16-block variant,

TinyMAR-D16, achieving an FID

of 2.28 and surpassing

the performance of the

24-block MAR-Base model with

only 10% of the

original training cost (40

epochs vs. 400 epochs).

[181] åº”ç”¨æˆ‘

ä»¬çš„ä¿®å‰ªæ–¹

æ³•ï¼Œæˆ‘ä»¬å°†MARé™

ä½åˆ°16å—å˜ä½“

Tinymar-D16ï¼Œè¾¾åˆ°2.28çš„FIDï¼Œå¹¶è¶…

è¿‡äº†24å—MAR-BASEæ¨¡å‹

çš„æ€§èƒ½ï¼Œåªæœ‰

10ï¼…çš„åŸå§‹è®­ç»ƒ

æˆæœ¬ï¼ˆ40ä¸ªæ—¶æœŸ

ï¼‰ï¼ˆ40ä¸ªæ—¶æœŸä¸400ä¸ª

æ—¶æœŸï¼‰ã€‚



--------------------------------------------------

[182] Our ap- proach

also generalizes to Scalable

Interpolant Transform- ers (SiT)

[34], an extension of

the DiT architecture that

employs a flow-based interpolant

framework to bridge data

100 101 Calibration Loss

0.00 0.25 0.50 0.75

1.00 1.25 1.50 1.75

2.00 Density Min: 0.195

Max: 37.694 Std: 1.300

Min: 0.195 Max: 37.694

Std: 1.300

Oracle Learnable

ShortGPT

Sensitivity Flux-Lite

Figure 5.

[182]

Our ap- proach also

generalizes to Scalable Interpolant

Transform- ers (SiT) [34],

an extension of the

DiT architecture that employs

a flow-based interpolant framework

to bridge data 100

101 Calibration Loss 0.00

0.25 0.50 0.75 1.00

1.25 1.50 1.75 2.00

Density Min: 0.195 Max:

37.694 Std: 1.300 Min:

0.195 Max: 37.694 STDï¼š1.300

Oracleå¯å­¦ä¹ 

çš„çŸ­æ—¶æ•æ„Ÿ

æ€§é€šé‡ -  lux-liteå›¾5ã€‚

--------------------------------------------------



[183] Distribution

of calibration loss through

random sampling of candidate

models.

[183] é€š

è¿‡éšæœºæŠ½æ ·

å€™é€‰æ¨¡å‹çš„

æ ¡å‡†æŸå¤±åˆ†

å¸ƒã€‚

--------------------------------------------------



[184] The

proposed learnable method achieves

the best post-fine-tuning FID

yet has a relatively

high initial loss com-

pared to other baselines.

[184] æ‰€æå‡ºçš„

å¯å­¦ä¹ æ–¹æ³•

å®ç°äº†æœ€ä½³

çš„éªŒè¯åFIDï¼Œä½†

å…·æœ‰ç›¸å¯¹è¾ƒ

é«˜çš„åˆå§‹æŸ

å¤±ä¸å…¶ä»–åŸº

çº¿ç›¸æ¯”ã€‚



--------------------------------------------------

[185] Strategy Loss IS

FID Prec.

[185] ç­–ç•¥

æŸå¤±æ˜¯FID

PRECã€‚



--------------------------------------------------

[186] Recall Max.

[186]

å›æƒ³

æœ€å¤§ã€‚



--------------------------------------------------

[187] Loss 37.69 NaN

NaN NaN NaN Med.

[187] æŸå¤±37.69 in Medã€‚

--------------------------------------------------



[188] Loss

0.99 149.51 6.45 0.78

0.53 Min.

[188] æŸ

å¤±0.99

149.51 6.45 0.78 0.53åˆ†é’Ÿã€‚

--------------------------------------------------



[189] Loss

0.20 73.10 20.69 0.63

0.58 Sensitivity 0.21 70.36

21.15 0.63 0.57 ShortGPT

[36] 0.20 66.10 22.28

0.63 0.56 Flux-Lite [6]

0.85 54.54 25.92 0.62

0.55 Oracle (BK-SDM) 1.28

141.18 7.43 0.75 0.55

Learnable 0.98 151.88 5.73

0.80 0.55 Table 3.

[189] æŸå¤±

0.20 73.10 20.69

0.63 0.58çµæ•åº¦0.21 70.36 21.15

0.63 0.57çŸ­æ—¶

é—´[36] 0.20 66.10

22.28 0.63 0.56 Flux-Lite

[6] 0.85 54.54 54.54

25.92 0.62 0.62 0.55

Oracleï¼ˆBK-SDMï¼‰ 151.88 5.73 0.80

0.55è¡¨3ã€‚



--------------------------------------------------

[190] Directly minimizing the

calibration loss may lead

to non-optimal solutions.

[190]

ç›´æ¥æœ€

å¤§ç¨‹åº¦åœ°å‡

å°‘æ ¡å‡†æŸå¤±

å¯èƒ½å¯¼è‡´é

æœ€ä½³è§£å†³æ–¹

æ¡ˆã€‚



--------------------------------------------------

[191] All pruned models

are fine-tuned without knowledge

distillation (KD) for 100K

steps.

[191] æ‰€æœ‰ä¿®å‰ª

æ¨¡å‹å‡ç»è¿‡

å¾®è°ƒï¼Œæ²¡æœ‰çŸ¥

è¯†è’¸é¦ï¼ˆKDï¼‰100Kæ­¥éª¤

ã€‚

--------------------------------------------------



[192] We

evaluate the fol- lowing

baselines: (1) Loss â€“

We randomly prune a

DiT-XL model to generate

100,000 models and select

models with different cali-

bration losses for fine-tuning;

(2) Metric-based Methods â€“

such as Sensitivity Analysis

and ShortGPT; (3) Oracle

â€“ We retain the

first and last layers

while uniformly pruning the

intermediate layers fol- lowing

[23]; (4) Learnable â€“

The proposed learnable method.

[192] æˆ‘ä»¬è¯„ä¼°äº†

ä»¥ä¸‹åŸºå‡†ï¼šï¼ˆ1ï¼‰æŸ

å¤± - æˆ‘ä»¬éšæœº

ä¿®å‰ªDIT-XLæ¨¡å‹ï¼Œä»¥

ç”Ÿæˆ100,000æ¬¾æ¨¡å‹

ï¼Œå¹¶é€‰æ‹©å…·æœ‰

ä¸åŒå¡è·¯æŸ

å¤±çš„æ¨¡å‹ä»¥

è¿›è¡Œå¾®è°ƒï¼›

ï¼ˆ2ï¼‰åŸº

äºåº¦é‡çš„æ–¹

æ³• - ä¾‹å¦‚çµæ•

åº¦åˆ†æå’ŒçŸ­

ç¨‹ï¼› ï¼ˆ3ï¼‰Oracle

- æˆ‘ä»¬ä¿ç•™

äº†ç¬¬ä¸€å±‚ä¹Ÿ

æ˜¯æœ€åä¸€å±‚

ï¼ŒåŒæ—¶å‡åŒ€ä¿®

å‰ªäº†ä¸­é—´å±‚

[23]ï¼› ï¼ˆ4ï¼‰å¯å­¦ä¹  -

æ‹Ÿè®®

çš„å¯å­¦ä¹ æ–¹

æ³•ã€‚



--------------------------------------------------

[193] and noise distributions.

[193] å’Œå™ªå£°åˆ†

å¸ƒã€‚



--------------------------------------------------

[194] The SiT-XL/2 model,

comprising 28 transformer blocks,

was pruned by 50%,

creating the TinySiT-D14 model.

[194] SIT-XL/2æ¨¡å‹ï¼ŒåŒ…æ‹¬

28ä¸ªå˜å‹å™¨å—

ï¼Œç”±50ï¼…ä¿®å‰ªï¼Œåˆ›å»º

TinySit-D14æ¨¡å‹ã€‚



--------------------------------------------------

[195] This pruned model

retains competi- tive performance

at only 7% of

the original training cost

(100 epochs vs. 1400

epochs).

[195] è¯¥ä¿®å‰ª

æ¨¡å‹ä»…ä»¥åŸ

å§‹åŸ¹è®­æˆæœ¬

çš„7ï¼…ï¼ˆ100ä¸ªæ—¶ä»£ä¸

1400ä¸ªæ—¶ä»£ï¼‰ä¿ç•™

ç«äº‰æ€§èƒ½ã€‚

--------------------------------------------------



[196] As

shown in Table 2,

these results demonstrate that

our pruning method is

adaptable across different diffusion

transformer variants, effectively reducing

the model size and

training time while maintain-

ing strong performance.

[196]

å¦‚

è¡¨2æ‰€ç¤ºï¼Œè¿™äº›

ç»“æœè¡¨æ˜ï¼Œæˆ‘

ä»¬çš„ä¿®å‰ªæ–¹

æ³•åœ¨ä¸åŒçš„

æ‰©æ•£å˜å‹å™¨

å˜ä½“ä¸­å…·æœ‰

é€‚åº”æ€§çš„é€‚

åº”æ€§ï¼Œä»è€Œæœ‰

æ•ˆåœ°å‡å°‘äº†

æ¨¡å‹çš„å¤§å°

å’Œè®­ç»ƒæ—¶é—´

ï¼ŒåŒæ—¶ä¿æŒå¼º

åŠ²çš„æ€§èƒ½ã€‚



--------------------------------------------------

[197] 4.3.

[197] 4.3ã€‚

--------------------------------------------------



[198] Analytical

Experiments Is Calibration Loss

the Primary Determinant?

[198]

åˆ†

æå®éªŒæ˜¯æ ¡

å‡†æŸå¤±çš„ä¸»

è¦å†³å®šå› ç´ 

å—ï¼Ÿ



--------------------------------------------------

[199] An es- sential

question in depth pruning

is how to identify

re- dundant layers in

pre-trained diffusion transformers.

[199]

æ·±åº¦ä¿®å‰ª

çš„ä¸€ä¸ªé—®é¢˜

æ˜¯å¦‚ä½•è¯†åˆ«

é¢„å…ˆè®­ç»ƒçš„

æ‰©æ•£å˜å‹å™¨

ä¸­çš„é‡å¤å±‚

ã€‚



--------------------------------------------------

[200] A common approach

involves minimizing the calibration

loss, based on the

assumption that a model

with lower calibra- tion

loss after pruning will

exhibit superior performance.

[200]

ä¸€ç§å…±åŒçš„

æ–¹æ³•æ¶‰åŠæœ€

å¤§ç¨‹åº¦åœ°å‡

å°‘æ ¡å‡†æŸå¤±

ï¼Œè¿™æ˜¯åŸºäºè¿™

æ ·çš„å‡è®¾ï¼šä¿®

å‰ªåè¾ƒä½ç¢³

çº¤ç»´æŸå¤±çš„

æ¨¡å‹å°†è¡¨ç°

å‡ºè¾ƒé«˜çš„æ€§

èƒ½ã€‚



--------------------------------------------------

[201] However, we demonstrate

in this section that

this hypothesis may not

hold for diffusion transformers.

[201] ä½†æ˜¯ï¼Œæˆ‘ä»¬

åœ¨æœ¬èŠ‚ä¸­è¯

æ˜ï¼Œè¯¥å‡è®¾å¯

èƒ½ä¸é€‚åˆæ‰©

æ•£å˜å‹å™¨ã€‚



--------------------------------------------------

[202] We begin by

ex- amining the solution

space through random depth

pruning at a 50%

ratio, generating 100,000 candidate

models with 6

[202]

æˆ‘

ä»¬é¦–å…ˆé€šè¿‡

ä»¥50ï¼…çš„æ¯”ä¾‹æ¥

æ‰«æè§£å†³æ–¹

æ¡ˆç©ºé—´ï¼Œä»¥6çš„

æ¯”ä¾‹äº§ç”Ÿ100,000ä¸ª

å€™é€‰æ¨¡å‹



--------------------------------------------------

[203] Pattern âˆ†W IS

â†‘ FID â†“ sFID

â†“ Prec.

[203] æ¨¡

å¼âˆ†Wæ˜¯â†‘fidâ†“SFIDâ†“PRECã€‚

--------------------------------------------------



[204] â†‘

Recall â†‘ 1:2 LoRA

54.75 33.39 29.56 0.56

0.62 2:4 LoRA 53.07

34.21 27.61 0.55 0.63

7:14 LoRA 34.97 49.41

28.48 0.46 0.56 1:2

Full 53.11 35.77 32.68

0.54 0.61 2:4 Full

53.63 34.41 29.93 0.55

0.62 7:14 Full 45.03

38.76 31.31 0.52 0.62

1:2 Frozen 45.08 39.56

31.13 0.52 0.60 2:4

Frozen 48.09 37.82 31.91

0.53 0.62 7:14 Frozen

34.09 49.75 31.06 0.46

0.56 Table 4.

[204]

â†‘ Recall â†‘ 1:2

LoRA 54.75 33.39 29.56

0.56 0.62 2:4 LoRA

53.07 34.21 27.61 0.55

0.63 7:14 LoRA 34.97

49.41 28.48 0.46 0.56

1:2 Full 53.11 35.77

32.68 0.54 0.61 2:4

Full 53.63 34.41 29.93

0.55 0.62 7:14 Full

45.03 38.76 31.31 0.52

0.62 1:2 Frozen 45.08

39.56 31.13 0.52 0.60

2:4 Frozen 48.09 37.82

31.91 0.53 0.62 7:14

Frozen 34.09 49.75 31.06

0.46 0.56 Table 4.

--------------------------------------------------



[205] Performance

comparison of TinyDiT-D14 models

com- pressed using various

pruning schemes and recoverability

estima- tion strategies.

[205]

ä½¿ç”¨å„

ç§ä¿®å‰ªæ–¹æ¡ˆ

å’Œå¯æ¢å¤æ€§

ä¼°è®¡ç­–ç•¥ç»„

åˆçš„TinyDit-D14æ¨¡å‹çš„

æ€§èƒ½æ¯”è¾ƒã€‚



--------------------------------------------------

[206] All models are

fine-tuned for 10,000 steps,

and FID scores are

computed on 10,000 sampled

images with 64 timesteps.

[206] æ‰€

æœ‰å‹å·å‡ä»¥

10,000ä¸ªæ­¥éª¤è¿›è¡Œ

å¾®è°ƒï¼Œå¹¶åœ¨10,000ä¸ª

å¸¦æœ‰64ä¸ªæ—¶é—´

æ­¥é•¿çš„é‡‡æ ·

å›¾åƒä¸Šè®¡ç®—

FIDåˆ†æ•°ã€‚



--------------------------------------------------

[207] calibration losses ranging

from 0.195 to 37.694

(see Fig- ure 5).

[207] æ ¡å‡†æŸ

å¤±èŒƒå›´ä¸º0.195è‡³

37.694ï¼ˆè§å›¾5ï¼‰ã€‚



--------------------------------------------------

[208] From these candidates,

we select models with

the highest and lowest

calibration losses for fine-tuning.

[208] ä»è¿™äº›

å€™é€‰äººä¸­ï¼Œæˆ‘

ä»¬é€‰æ‹©å…·æœ‰

æœ€é«˜å’Œæœ€ä½

æ ¡å‡†æŸå¤±çš„

æ¨¡å‹ä»¥è¿›è¡Œ

å¾®è°ƒã€‚



--------------------------------------------------

[209] No- tably, both

models result in unfavorable

outcomes, such as unstable

training (NaN) or suboptimal

FID scores (20.69), as

shown in Table 3.

[209] å¦‚è¡¨3æ‰€

ç¤ºï¼Œä¸¤ç§æ¨¡å‹

éƒ½æ²¡æœ‰é€ æˆ

ä¸åˆ©çš„ç»“æœ

ï¼Œä¾‹å¦‚ä¸ç¨³å®š

çš„è®­ç»ƒï¼ˆNANï¼‰æˆ–æ¬¡

ä¼˜FIDå¾—åˆ†ï¼ˆ20.69ï¼‰ã€‚



--------------------------------------------------

[210] Additionally, we conduct

a sensitiv- ity analysis

[18], a commonly used

technique to identify crucial

layers by measuring loss

disturbance upon layer re-

moval, which produces a

model with a low

calibration loss of 0.21.

[210] æ­¤å¤–

ï¼Œæˆ‘ä»¬è¿›è¡Œäº†

çµæ•åˆ†æ[18]ï¼Œè¿™

æ˜¯ä¸€ç§å¸¸ç”¨

çš„æŠ€æœ¯ï¼Œå¯ä»¥

é€šè¿‡æµ‹é‡å±‚

å±‚æ¬¡çš„æŸå¤±

éšœç¢æ¥è¯†åˆ«

å…³é”®å±‚ï¼Œè¯¥å±‚

æŸå¤±å±‚çš„æŸ

å¤±ï¼Œè¯¥æ¨¡å‹äº§

ç”Ÿäº†ä¸€ä¸ªæ¨¡

å‹ï¼Œä½æ ¡å‡†æŸ

å¤±ä¸º0.21ã€‚



--------------------------------------------------

[211] However, this modelâ€™s

FID score is similar

to that of the

model with the lowest

calibration loss.

[211] ä½†æ˜¯ï¼Œè¯¥

æ¨¡å‹çš„FIDå¾—åˆ†

ä¸æ ¡å‡†æŸå¤±

æœ€ä½çš„æ¨¡å‹

çš„å¾—åˆ†ç›¸ä¼¼

ã€‚

--------------------------------------------------



[212] Approaches

like ShortGPT [36] and

a recent approach for

compressing the Flux model

[6], which estimate similarity

or minimize mean squared

error (MSE) between input

and output states, reveal

a similar trend.

[212]

è¯¸å¦‚Shortgpt [36]å’Œæœ€è¿‘

å‹ç¼©é€šé‡æ¨¡

å‹[6]çš„æ–¹æ³•ï¼Œè¯¥

æ–¹æ³•ä¼°ç®—äº†

è¾“å…¥å’Œè¾“å‡º

çŠ¶æ€ä¹‹é—´çš„

å¹³å‡å¹³æ–¹è¯¯

å·®ï¼ˆMSEï¼‰ï¼Œä»è€Œæ­ç¤º

äº†ç›¸ä¼¼çš„è¶‹

åŠ¿ã€‚



--------------------------------------------------

[213] In contrast, methods

with mod- erate calibration

losses, such as Oracle

(often considered less competitive)

and one of the

randomly pruned models, achieve

FID scores of 7.43

and 6.45, respectively, demon-

strating significantly better performance

than models with minimal

calibration loss.

[213] ç›¸æ¯”ä¹‹ä¸‹

ï¼Œå…·æœ‰æ¨¡å—åŒ–

æ ¡å‡†æŸå¤±çš„

æ–¹æ³•ï¼Œä¾‹å¦‚Oracleï¼ˆé€š

å¸¸è®¤ä¸ºç«äº‰

åŠ›è¾ƒä½ï¼‰å’Œéš

æœºä¿®å‰ªçš„æ¨¡

å‹ä¹‹ä¸€ï¼Œåˆ†åˆ«

è¾¾åˆ°7.43å’Œ6.45çš„FIDå¾—

åˆ†ï¼Œæ¯”å…·æœ‰æœ€

å°æ ¡å‡†æŸå¤±

çš„æ¨¡å‹çš„æ¨¡

å‹æ˜æ˜¾æ›´å¥½

ã€‚

--------------------------------------------------



[214] These

findings suggest that, while

calibration loss may influence

post-fine-tuning performance to some

extent, it is not

the primary determinant for

diffu- sion transformers.

[214]

è¿™äº›å‘ç°è¡¨

æ˜ï¼Œè™½ç„¶æ ¡å‡†

æŸå¤±å¯èƒ½åœ¨

æŸç§ç¨‹åº¦ä¸Š

ä¼šå½±å“é¢„å®š

åçš„æ€§èƒ½ï¼Œä½†

å®ƒå¹¶ä¸æ˜¯æ‰©

æ•£å˜å‹å™¨çš„

ä¸»è¦å†³å®šå› 

ç´ ã€‚



--------------------------------------------------

[215] Instead, the modelâ€™s

capacity for perfor- mance

recovery during fine-tuning, termed

â€œrecoverability,â€ appears to be

more critical.

[215] å–è€Œä»£ä¹‹

çš„æ˜¯ï¼Œè¯¥æ¨¡å‹

åœ¨å¾®è°ƒè¿‡ç¨‹

ä¸­æ¢å¤æ€§èƒ½

çš„èƒ½åŠ›ç§°ä¸º

â€œå¯æ¢å¤æ€§â€ï¼Œä¼¼

ä¹æ›´ä¸ºå…³é”®

ã€‚

--------------------------------------------------



[216] Notably,

assessing recoverabil- ity using

traditional metrics is challenging,

as it requires a

learning process across the

entire dataset.

[216] å€¼å¾—æ³¨æ„çš„

æ˜¯ï¼Œä½¿ç”¨ä¼ ç»Ÿ

æŒ‡æ ‡è¯„ä¼°æ¢

å¤æ€§æ˜¯å…·æœ‰

æŒ‘æˆ˜æ€§çš„ï¼Œå› 

ä¸ºå®ƒéœ€è¦åœ¨

æ•´ä¸ªæ•°æ®é›†

ä¸­è¿›è¡Œå­¦ä¹ 

è¿‡ç¨‹ã€‚

--------------------------------------------------



[217] This

observation also explains why

the proposed method achieves

superior results (5.73) compared

to baseline methods.

[217]

è¯¥è§‚å¯Ÿ

ç»“æœè¿˜è§£é‡Š

äº†ä¸ºä»€ä¹ˆæ‰€

æå‡ºçš„æ–¹æ³•

ä¸åŸºçº¿æ–¹æ³•

ç›¸æ¯”å–å¾—äº†

ä¼˜è¶Šçš„ç»“æœ

ï¼ˆ5.73ï¼‰ã€‚



--------------------------------------------------

[218] Learnable Modeling of

Recoverability.

[218] å¯æ¢å¤æ€§çš„

å¯å­¦ä¹ å»ºæ¨¡

ã€‚

--------------------------------------------------



[219] To

overcome the limitations of

traditional metric-based methods, this

study introduces a learnable

approach to jointly optimize

pruning and model recoverability.

[219] ä¸ºäº†å…‹æœä¼ 

ç»ŸåŸºäºæŒ‡æ ‡

çš„æ–¹æ³•çš„å±€

é™æ€§ï¼Œæœ¬ç ”ç©¶

å¼•å…¥äº†ä¸€ç§

å¯å­¦ä¹ çš„æ–¹

æ³•ï¼Œå¯ä»¥å…±åŒ

ä¼˜åŒ–ä¿®å‰ªå’Œ

å»ºæ¨¡å¯å›æ”¶

æ€§ã€‚



--------------------------------------------------

[220] Table 3 illustrates

dif- ferent configurations of

the learnable method, including

the local pruning scheme

and update strategies for

recoverabil- ity estimation.

[220]

è¡¨3è¯´æ˜äº†

å¯å­¦ä¹ æ–¹æ³•

çš„ä¸åŒé…ç½®

ï¼ŒåŒ…æ‹¬å±€éƒ¨ä¿®

å‰ªæ–¹æ¡ˆå’Œæ›´

æ–°ä»¥æ¢å¤ä¼°

è®¡çš„ç­–ç•¥ã€‚



--------------------------------------------------

[221] For a 28-layer

DiT-XL/2 with a fixed

50% 0 2000 4000

6000 8000 10000 Train

iterations 0 1 2

3 4 5 6

7 8 9 10

11 12 13 14

15 16 17 18

19 20 21 22

23 24 25 26

27 Layer Index in

DiT-XL Figure 6.

[221]

å¯¹

äºå¸¦æœ‰å›ºå®š

50ï¼…0 2000 4000 6000

8000ç«è½¦è¿­ä»£çš„

28å±‚DIT-XL/2ï¼Œ0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 22

23 23 24 26

26 27 27 dit-XLä¸­çš„å±‚æŒ‡

æ•°å›¾6ã€‚

--------------------------------------------------



[222] Visualization

of the 2:4 decisions

in the learnable prun-

ing, with the confidence

level of each decision

highlighted through varying degrees

of transparency.

[222] 2ï¼š4åœ¨å¯å­¦

ä¹ çš„ä¿®å‰ªä¸­

çš„å†³å®šçš„å¯

è§†åŒ–ï¼Œæ¯ä¸ªå†³

ç­–çš„ç½®ä¿¡åº¦

éƒ½é€šè¿‡ä¸åŒ

ç¨‹åº¦çš„é€æ˜

åº¦å¼ºè°ƒã€‚

--------------------------------------------------



[223] More

visualization results for 1:2

and 7:14 schemes are

available in the appendix.

[223] é™„å½•

ä¸­æä¾›äº†1ï¼š2å’Œ

7:14æ–¹æ¡ˆçš„æ›´å¤š

å¯è§†åŒ–ç»“æœ

ã€‚



--------------------------------------------------

[224] layer pruning rate,

we examine three splitting

schemes: 1:2, 2:4, and

7:14.

[224] å±‚ä¿®å‰ªç‡ï¼Œæˆ‘

ä»¬æ£€æŸ¥äº†ä¸‰

ä¸ªåˆ†è£‚æ–¹æ¡ˆ

ï¼š1ï¼š2ã€2ï¼š4å’Œ7ï¼š14ã€‚

--------------------------------------------------



[225] In

the 1:2 scheme, for

example, every two transformer

layers form a local

block, with one layer

pruned.

[225] ä¾‹å¦‚ï¼Œåœ¨1ï¼š2æ–¹

æ¡ˆä¸­ï¼Œæ¯ä¸¤ä¸ª

å˜å‹å™¨å±‚å½¢

æˆä¸€ä¸ªå±€éƒ¨

å—ï¼Œå¹¶ä¿®å‰ªä¸€

å±‚ã€‚

--------------------------------------------------



[226] Larger

blocks introduce greater diversity

but sig- nificantly expand

the search space.

[226]

è¾ƒå¤§çš„å—

å¼•å…¥äº†æ›´å¤§

çš„å¤šæ ·æ€§ï¼Œä½†

å¾ˆæ˜æ˜¾åœ°æ‰©

å¤§äº†æœç´¢ç©º

é—´ã€‚



--------------------------------------------------

[227] For instance, the

7:14 scheme divides the

model into two segments,

each retain- ing 7

layers, resulting in  14

7  Ã— 2

= 6,864 possible solu-

tions.

[227] ä¾‹å¦‚ï¼Œ7:14æ–¹æ¡ˆ

å°†æ¨¡å‹åˆ’åˆ†

ä¸ºä¸¤ä¸ªæ®µï¼Œæ¯

ä¸ªç‰‡æ®µä¿ç•™

äº†7å±‚ï¼Œå¯¼è‡´14 7Ã—2

= 6,864å¯

èƒ½çš„è§£å†³æ–¹

æ¡ˆã€‚



--------------------------------------------------

[228] Conversely, smaller blocks

significantly reduce op- timization

difficulty and offer greater

flexibility.

[228] ç›¸åï¼Œè¾ƒå°

çš„å—å¤§å¤§å‡

å°‘äº†æ“ä½œå›°

éš¾ï¼Œå¹¶æä¾›äº†

æ›´å¤§çš„çµæ´»

æ€§ã€‚

--------------------------------------------------



[229] When

the distribution of one

block converges, the learning

on other blocks can

still progress.

[229] å½“ä¸€ä¸ªå—

çš„åˆ†å¸ƒæ”¶æ•›

æ—¶ï¼Œå…¶ä»–å—ä¸Š

çš„å­¦ä¹ ä»ç„¶

å¯ä»¥è¿›æ­¥ã€‚

--------------------------------------------------



[230] As

shown in Table 3,

the 1:2 con- figuration

achieves the optimal performance

after 10K fine- tuning

iterations.

[230] å¦‚

è¡¨3æ‰€ç¤ºï¼Œ1ï¼š2çš„å½¢

è±¡åœ¨10Kç»†è°ƒè¿­

ä»£åè¾¾åˆ°äº†

æœ€ä½³æ€§èƒ½ã€‚

--------------------------------------------------



[231] Additionally,

our empirical findings un-

derscore the effectiveness of

recoverability estimation using LoRA

or full fine-tuning.

[231]

æ­¤

å¤–ï¼Œæˆ‘ä»¬çš„ç»

éªŒå‘ç°æœªè§£

å†³ä½¿ç”¨LORAæˆ–å®Œ

æ•´å¾®è°ƒå¯æ¢

å¤æ€§ä¼°è®¡çš„

æœ‰æ•ˆæ€§ã€‚



--------------------------------------------------

[232] Both methods yield

positive post- fine-tuning outcomes,

with LoRA achieving superior

results (FID = 33.39)

compared to full fine-tuning

(FID = 35.77) under

the 1:2 scheme, as

LoRA has fewer trainable

parame- ters (0.9% relative

to full parameter training)

and can adapt more

efficiently to the randomness

of sampling.

[232] ä¸¤ç§

æ–¹æ³•éƒ½äº§ç”Ÿ

æ­£é¢è°ƒæŸ¥ç»“

æœï¼Œä¸1ï¼š2æ–¹æ¡ˆä¸‹

çš„å…¨é¢å¾®è°ƒ

ï¼ˆFID

= 35.77ï¼‰ç›¸æ¯”ï¼Œæ´›æ‹‰å–

å¾—äº†è¾ƒé«˜çš„

ç»“æœï¼ˆFID = 33.39ï¼‰ï¼Œå› ä¸ºæ´›

æ‹‰ï¼ˆLoraï¼‰å…·æœ‰è¾ƒå°‘

çš„å¯è®­ç»ƒçš„

å‚æ•°ï¼ˆç›¸å¯¹äº

å®Œå…¨å‚æ•°è®­

ç»ƒï¼‰ï¼Œå¹¶ä¸”å¯ä»¥

æ›´æœ‰æ•ˆåœ°é€‚

åº”é‡‡æ ·çš„éš

æœºæ€§ã€‚

--------------------------------------------------



[233] Visualization

of Learnable Decisions.

[233]

å¯å­¦ä¹ 

çš„å¯å­¦ä¹ å†³

ç­–ã€‚



--------------------------------------------------

[234] To gain deeper

in- sights into the

role of the learnable

method in pruning, we

visualize the learning process

in Figure 6.

[234]

ä¸ºäº†æ›´æ·±

å…¥åœ°äº†è§£å¯

å­¦ä¹ æ–¹æ³•åœ¨

ä¿®å‰ªä¸­çš„ä½œ

ç”¨ï¼Œæˆ‘ä»¬å°†å›¾

6ä¸­çš„å­¦ä¹ è¿‡

ç¨‹å¯è§†åŒ–ã€‚



--------------------------------------------------

[235] From bottom to

top, the i-th curve

represents the i-th layer

of the pruned model,

displaying its layer index

in the original DiT-XL/2.

[235] ä»

åº•éƒ¨åˆ°é¡¶éƒ¨

ï¼Œç¬¬i-thæ›²çº¿ä»£è¡¨

ä¿®å‰ªæ¨¡å‹çš„

ç¬¬Iå±‚ï¼Œåœ¨åŸå§‹

DIT-XL/2ä¸­æ˜¾ç¤ºå…¶å±‚

ç´¢å¼•ã€‚



--------------------------------------------------

[236] This visualization illustrates

the dynamics of pruning

de- cisions over training

iterations, where the transparency

of each data point

indicates the probability of

being sampled.

[236] è¿™ç§å¯

è§†åŒ–è¯´æ˜äº†

ä¿®å‰ªè®­ç»ƒè¿­

ä»£çš„ä¿®å‰ªæ•ˆ

æœçš„åŠ¨åŠ›å­¦

ï¼Œå…¶ä¸­æ¯ä¸ªæ•°

æ®ç‚¹çš„é€æ˜

åº¦è¡¨æ˜è¢«é‡‡

æ ·çš„æ¦‚ç‡ã€‚

--------------------------------------------------



[237] The

learnable method shows its

capacity to explore and

handle various layer combinations.

[237] å¯

å­¦ä¹ çš„æ–¹æ³•

æ˜¾ç¤ºäº†å…¶æ¢

ç´¢å’Œå¤„ç†å„

ç§å±‚ç»„åˆçš„

èƒ½åŠ›ã€‚



--------------------------------------------------

[238] Pruning decisions for

certain layers, such as

the 7-th and 8-th

in the compressed model,

are determined quickly and

remain stable through- out

the process.

[238] å¯¹æŸäº›

å±‚çš„ä¿®å‰ªå†³

ç­–ï¼Œä¾‹å¦‚å‹ç¼©

æ¨¡å‹ä¸­çš„7-ä¸‰

åˆ†ä¹‹ä¸€å’Œç¬¬

8-ä¸‰ï¼Œå¹¶åœ¨æ•´ä¸ª

è¿‡ç¨‹ä¸­ä¿æŒ

ç¨³å®šã€‚

--------------------------------------------------



[239] In

contrast, other layers, like

the 0-th layer, require

additional fine-tuning to estimate

their recoverabil- ity.

[239]

ç›¸æ¯”ä¹‹

ä¸‹ï¼Œå…¶ä»–å±‚ï¼ˆå¦‚

ç¬¬0å±‚ï¼‰éœ€è¦è¿›

è¡Œå…¶ä»–å¾®è°ƒ

æ¥ä¼°ç®—å…¶æ¢

å¤æ€§ã€‚



--------------------------------------------------

[240] Notably, some decisions

may change in the

later stages 7

[240]

å€¼å¾—æ³¨

æ„çš„æ˜¯ï¼ŒæŸäº›

å†³å®šå¯èƒ½ä¼š

åœ¨åæœŸç¬¬7é˜¶

æ®µå‘ç”Ÿå˜åŒ–

--------------------------------------------------



[241]

Figure 7.

[241] å›¾7ã€‚

--------------------------------------------------



[242] Images

generated by TinyDiT-D14 on

ImageNet 224Ã—224, pruned and

distilled from a DiT-XL/2.

[242] ç”±Tinydit-d14åœ¨Imagenet 224Ã—224ä¸Šäº§

ç”Ÿçš„å›¾åƒï¼Œä»

DIT-XL/2è¿›è¡Œä¿®å‰ªå’Œ

è’¸é¦ã€‚

--------------------------------------------------



[243] 102

101 100 0 100

101 102 Activation Value

(log) 0.0 0.1 0.2

0.3 0.4 Density

Max Activation: 191.20

Min Activation: -429.01 +Std:

12.54 -Std: -12.54 (a)

DiT-XL/2 (Teacher) 102 101

100 0 100 101

Activation Value (log) 0.0

0.1 0.2 0.3 0.4

0.5 Density  Max

Activation: 53.77  Min

Activation: -526.62 +Std: 14.15

-Std: -14.15 (b) TinyDiT-D14

(Student) Figure 8.

[243]

102 101 100 0

100 101 102 Activation

Value (log) 0.0 0.1

0.2 0.3 0.4 Density

Max Activation: 191.20

Min Activation: -429.01 +Std:

12.54 -Std: -12.54 (a)

DiT-XL/2 (Teacher) 102 101

100 0 100 101

Activation Value (log) 0.0

0.1 0.2 0.3 0.4

0.5 Density  Max

Activation: 53.77  Min

Activation: -526.62 +STDï¼š14.15 -STDï¼š-14.15ï¼ˆbï¼‰Tinydit

-D14ï¼ˆå­¦ç”Ÿï¼‰å›¾

8ã€‚



--------------------------------------------------

[244] Visualization of massive

activations [47] in DiTs.

[244] å¤§è§„æ¨¡æ¿€æ´»

çš„å¯è§†åŒ–[47]ã€‚



--------------------------------------------------

[245] Both teacher and

student models display large

activation values in their

hidden states.

[245] æ•™

å¸ˆå’Œå­¦ç”Ÿæ¨¡

å‹åœ¨å…¶éšè—

çŠ¶æ€ä¸‹éƒ½æ˜¾

ç¤ºå‡ºè¾ƒå¤§çš„

æ¿€æ´»å€¼ã€‚

--------------------------------------------------



[246] Directly

distilling these massive activations

may result in excessively

large losses and unstable

training.

[246] ç›´æ¥

æç‚¼è¿™äº›å¤§

è§„æ¨¡æ¿€æ´»å¯

èƒ½ä¼šå¯¼è‡´è¿‡

åº¦æŸå¤±å’Œè®­

ç»ƒä¸ç¨³å®šã€‚

--------------------------------------------------



[247] once

these layers have been

sufficiently optimized.

[247] ä¸€

æ—¦è¿™äº›å±‚å¾—

åˆ°å……åˆ†ä¼˜åŒ–

ã€‚

--------------------------------------------------



[248] The

training process ultimately concludes

with high sampling probabilities,

suggesting a converged learning

process with distributions approaching

a one-hot configuration.

[248]

åŸ¹è®­è¿‡ç¨‹æœ€

ç»ˆä»¥é«˜é‡‡æ ·

æ¦‚ç‡ç»“æŸï¼Œè¿™

è¡¨æ˜é€šè¿‡åˆ†

å¸ƒæ¥è¿‘å•æ¬¡

é…ç½®çš„åˆ†å¸ƒ

è¿›è¡Œäº†èåˆ

çš„å­¦ä¹ è¿‡ç¨‹

ã€‚



--------------------------------------------------

[249] After training, we

select the layers with

the highest probabilities for

subsequent fine-tuning.

[249] è®­ç»ƒåï¼Œæˆ‘ä»¬

é€‰æ‹©å…·æœ‰æœ€

é«˜æ¦‚ç‡çš„å±‚

ä»¥åè¿›è¡Œå¾®

è°ƒã€‚

--------------------------------------------------



[250] 4.4.

[250] 4.4ã€‚



--------------------------------------------------

[251] Knowledge Distillation for

Recovery In this work,

we also explore Knowledge

Distillation (KD) as an

enhanced fine-tuning method.

[251]

åœ¨è¿™é¡¹å·¥

ä½œä¸­æ¢å¤çš„

çŸ¥è¯†è’¸é¦ï¼Œæˆ‘

ä»¬è¿˜æ¢ç´¢äº†

çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä½œ

ä¸ºå¢å¼ºçš„å¾®

è°ƒæ–¹æ³•ã€‚



--------------------------------------------------

[252] As demonstrated in

Ta- ble 5, we

apply the vanilla knowledge

distillation approach proposed by

Hinton [20] to fine-tune

a TinyDiT-D14, using the

outputs of the pre-trained

DiT-XL/2 as a teacher

model for supervision.

[252]

å¦‚Table 5ä¸­

æ‰€ç¤ºï¼Œæˆ‘ä»¬ä½¿

ç”¨Hinton [20]æå‡ºçš„é¦™

è‰çŸ¥è¯†è’¸é¦

æ–¹æ³•å¯¹TinyDit-D14è¿›è¡Œ

å¾®è°ƒï¼Œä½¿ç”¨é¢„

å…ˆè®­ç»ƒçš„DIT-XL/2çš„

è¾“å‡ºä½œä¸ºç›‘

ç£çš„æ•™å¸ˆæ¨¡

å‹ã€‚

--------------------------------------------------



[253] We

employ a Mean Square

Error (MSE) loss to

align the outputs between

the shallow student model

and the deeper teacher

model, which effectively reduces

the FID at 100K

steps from 5.79 to

4.66.

[253] æˆ‘ä»¬é‡‡ç”¨

å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸ

å¤±æ¥å¯¹é½æµ…

å±‚å­¦ç”Ÿæ¨¡å‹

å’Œæ›´æ·±å±‚çš„

æ•™å¸ˆæ¨¡å‹ä¹‹

é—´çš„è¾“å‡ºï¼Œä»

è€Œæœ‰æ•ˆåœ°å°†

100Kæ­¥éª¤çš„FIDä»5.79é™

ä½åˆ°4.66ã€‚

--------------------------------------------------



[254] Masked

Knowledge Distillation.

[254] æ©ç›–çš„

çŸ¥è¯†è’¸é¦ã€‚

--------------------------------------------------



[255] Additionally,

we eval- uate representation

distillation (RepKD) [23, 42]

to transfer hidden states

from the teacher to

the student.

[255] æ­¤

å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°

è¡¨ç¤ºä»£è¡¨è’¸

é¦ï¼ˆREPKDï¼‰[23ï¼Œ42]å°†éšè—çŠ¶

æ€ä»è€å¸ˆè½¬

ç§»åˆ°å­¦ç”Ÿã€‚

--------------------------------------------------



[256] It

is important to note

that depth pruning does

not alter the hidden

dimen- sion of diffusion

transformers, allowing for direct

alignment fine-tuning Strategy Init.

[256] é‡

è¦çš„æ˜¯è¦æ³¨

æ„ï¼Œæ·±åº¦ä¿®å‰ª

ä¸ä¼šæ”¹å˜æ‰©

æ•£å˜å‹å™¨çš„

éšè—å°ºå¯¸ï¼Œä»

è€Œå…è®¸ç›´æ¥

å¯¹é½å¾®è°ƒç­–

ç•¥initã€‚



--------------------------------------------------

[257] Distill.

[257] è’¸é¦ã€‚

--------------------------------------------------



[258] Loss

FID @ 100K fine-tuning

- 5.79 Logits KD

- 4.66 RepKD 2840.1

NaN Masked KD (0.1Ïƒ)

15.4 NaN Masked KD

(2Ïƒ) 387.1 3.73 Masked

KD (4Ïƒ) 391.4 3.75

Table 5.

[258] æŸå¤±

FID

@ 100kå¾®è°ƒ-5.79 logits KD

-4.66 REPKD 2840.1 NANè’™ç‰ˆKDï¼ˆ0.1Ïƒï¼‰15.4

Nanè’™

ç‰ˆKDï¼ˆ2Ïƒï¼‰387.1 3.73è’™ç‰ˆKDï¼ˆ4Ïƒï¼‰391.4 3.75è¡¨5ã€‚

--------------------------------------------------



[259] Evaluation

of different fine-tuning strategies

for recovery.

[259] è¯„

ä¼°ä¸åŒçš„å¾®

è°ƒç­–ç•¥çš„æ¢

å¤ç­–ç•¥ã€‚

--------------------------------------------------



[260] Masked

RepKD ignores those massive

activations (|x| > kÏƒx)

in both teacher and

student, which enables effective

knowledge transfer between diffusion

transformers.

[260] è’™é¢

çš„repkdå¿½ç•¥äº†æ•™

å¸ˆå’Œå­¦ç”Ÿä¸­

çš„é‚£äº›å¤§è§„

æ¨¡æ¿€æ´»ï¼ˆ| x

|>kÏƒxï¼‰ï¼Œè¿™å¯

ä»¥åœ¨æ‰©æ•£å˜

å‹å™¨ä¹‹é—´æœ‰

æ•ˆåœ°ä¼ é€’çŸ¥

è¯†è½¬ç§»ã€‚



--------------------------------------------------

[261] of intermediate hidden

states.

[261] ä¸­é—´

éšè—çŠ¶æ€ã€‚

--------------------------------------------------



[262] For

practical implementation, we use

the block defined in

Section 3.2 as the

basic unit, ensuring that

the pruned local structure

in the pruned DiT

aligns with the output

of the original structure

in the teacher model.

[262] å¯¹

äºå®é™…å®æ–½

ï¼Œæˆ‘ä»¬å°†ç¬¬3.2èŠ‚

ä¸­å®šä¹‰çš„å—

ä½œä¸ºåŸºæœ¬å•

å…ƒï¼Œä»¥ç¡®ä¿ä¿®

å‰ªçš„DITä¸­ä¿®å‰ª

çš„æœ¬åœ°ç»“æ„

ä¸æ•™å¸ˆæ¨¡å‹

ä¸­åŸå§‹ç»“æ„

çš„è¾“å‡ºä¿æŒ

ä¸€è‡´ã€‚



--------------------------------------------------

[263] However, we encountered

significant training dif- ficulties

with this straightforward RepKD

approach due to massive

activations in the hidden

states, where both teacher

and student models occasionally

exhibit large activation values,

as shown in Figure

8.

[263] ä½†æ˜¯ï¼Œç”±

äºéšè—çŠ¶æ€

ä¸­çš„å¤§é‡æ¿€

æ´»ï¼Œæˆ‘ä»¬é‡åˆ°

äº†è¿™ç§ç›´æ¥

çš„repkdæ–¹æ³•é‡åˆ°

äº†é‡è¦çš„è®­

ç»ƒå›°éš¾ï¼Œåœ¨è¿™

äº›åŸ¹è®­çŠ¶æ€

ä¸‹ï¼Œæ•™å¸ˆå’Œå­¦

ç”Ÿæ¨¡å‹å¶å°”

éƒ½ä¼šè¡¨ç°å‡º

è¾ƒå¤§çš„æ¿€æ´»

å€¼ï¼Œå¦‚å›¾8æ‰€ç¤º

ã€‚

--------------------------------------------------



[264] Directly

distilling these ex- treme

activations can result in

excessively high loss values,

impairing the performance of

the student model.

[264]

ç›´æ¥æå–è¿™

äº›å…¸å‹çš„æ¿€

æ´»å¯èƒ½ä¼šå¯¼

è‡´è¿‡é«˜çš„æŸ

å¤±å€¼ï¼Œä»è€ŒæŸ

å®³å­¦ç”Ÿæ¨¡å‹

çš„è¡¨ç°ã€‚



--------------------------------------------------

[265] This issue has

also been observed in

other transformer-based genera- tive

models, such as certain

LLMs [47].

[265] åœ¨å…¶

ä»–åŸºäºå˜å‹

å™¨çš„å±æ€§æ¨¡

å‹ï¼ˆä¾‹å¦‚æŸäº›

LLMSï¼‰ä¸­ä¹Ÿè§‚å¯Ÿåˆ°

äº†è¿™ä¸ªé—®é¢˜

[47]ã€‚

--------------------------------------------------



[266] To

address this, we propose

a Masked RepKD variant

that selectively ex- cludes

these massive activations during

knowledge transfer.

[266] ä¸ºäº†è§£å†³è¿™

ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬

æå‡ºäº†ä¸€ä¸ª

æ©ç›–çš„repkdå˜ä½“

ï¼Œè¯¥å˜ä½“åœ¨çŸ¥

è¯†è½¬ç§»è¿‡ç¨‹

ä¸­æœ‰é€‰æ‹©åœ°

è¡¨è¾¾äº†è¿™äº›

å¤§è§„æ¨¡æ¿€æ´»

ã€‚

--------------------------------------------------



[267] We

employ a simple thresholding

method, |x âˆ’Âµx| <

kÏƒx, which ignores the

loss associated with these

extreme acti- vations.

[267]

æˆ‘ä»¬é‡‡ç”¨ä¸€

ç§ç®€å•çš„é˜ˆ

å€¼æ–¹æ³•ï¼Œ| x -Âµx |

<kÏƒxï¼Œå¿½ç•¥

ä¸è¿™äº›æç«¯

ä½œç”¨ç›¸å…³çš„

æŸå¤±ã€‚



--------------------------------------------------

[268] As shown in

Table 5, the Masked

RepKD approach with moderate

thresholds of 2Ïƒ and

4Ïƒ achieves satisfactory results,

demonstrating the robustness of

our method.

[268] å¦‚è¡¨5æ‰€

ç¤ºï¼Œå¸¦æœ‰2Ïƒå’Œ4Ïƒçš„

ä¸­ç­‰é˜ˆå€¼çš„

æ©ç›–repkdæ–¹æ³•å¯

å®ç°ä»¤äººæ»¡

æ„çš„ç»“æœï¼Œè¯

æ˜äº†æˆ‘ä»¬æ–¹

æ³•çš„é²æ£’æ€§

ã€‚

--------------------------------------------------



[269] Generated

Images.

[269] ç”Ÿæˆçš„å›¾åƒ

ã€‚

--------------------------------------------------



[270] In

Figure 7, We visualize

the gener- ated images

of the learned TinyDiT-D14,

distilled from an 8

[270] åœ¨å›¾7ä¸­ï¼Œæˆ‘ä»¬

å¯è§†åŒ–äº†ä»

8



--------------------------------------------------

[271] off-the-shelf DiT-XL/2 model.

[271] ç°æˆçš„DIT-XL/2å‹å·

ã€‚



--------------------------------------------------

[272] More visualization results

for SiTs and MARs

can be found in

the appendix.

[272] å¯ä»¥åœ¨é™„å½•

ä¸­æ‰¾åˆ°æ›´å¤š

çš„åå§¿å’Œç«

æ˜Ÿçš„å¯è§†åŒ–

ç»“æœã€‚

--------------------------------------------------



[273] 5.

[273] 5ã€‚



--------------------------------------------------

[274] Conclusions This work

introduces TinyFusion, a learnable

method for accelerating diffusion

transformers by removing redundant

layers.

[274] ç»“è®ºè¿™

é¡¹å·¥ä½œå¼•å…¥

äº†TinyFusionï¼Œè¿™æ˜¯ä¸€ç§

é€šè¿‡å»é™¤å†—

ä½™å±‚æ¥åŠ é€Ÿ

æ‰©æ•£å˜å‹å™¨

çš„å¯å­¦ä¹ æ–¹

æ³•ã€‚

--------------------------------------------------



[275] It

models the recoverability of

pruned models as an

optimizable objective and incorporates

differentiable sam- pling for

end-to-end training.

[275] å®ƒå°†ä¿®å‰ª

æ¨¡å‹çš„å¯æ¢

å¤æ€§å»ºæ¨¡ä¸º

ä¸€ä¸ªä¼˜åŒ–çš„

ç›®æ ‡ï¼Œå¹¶ç»“åˆ

äº†ç«¯åˆ°ç«¯è®­

ç»ƒçš„å¯åŒºåˆ†

å¥—ä»¶ã€‚

--------------------------------------------------



[276] Our

method generalizes to various

architectures like DiTs, MARs

and SiTs.

[276] æˆ‘ä»¬çš„

æ–¹æ³•æ¦‚æ‹¬ä¸º

è¯¸å¦‚DITï¼Œç«æ˜Ÿå’Œ

åç€çš„å„ç§

ä½“ç³»ç»“æ„ã€‚

--------------------------------------------------



[277] References

[1] Fan Bao, Shen

Nie, Kaiwen Xue, Yue

Cao, Chongxuan Li, Hang

Su, and Jun Zhu.

[277] References [1] Fan

Bao, Shen Nie, Kaiwen

Xue, Yue Cao, Chongxuan

Li, Hang Su, and

Jun Zhu.



--------------------------------------------------

[278] All are worth

words: A vit backbone

for diffusion models.

[278]

æ‰€

æœ‰éƒ½æ˜¯å€¼å¾—

çš„è¯ï¼šæ‰©æ•£æ¨¡

å‹çš„VITéª¨å¹²ã€‚



--------------------------------------------------

[279] In Proceedings of

the IEEE/CVF con- ference

on computer vision and

pattern recognition, pages 22669â€“22679,

2023.

[279] åœ¨

è®¡ç®—æœºè§†è§‰

å’Œæ¨¡å¼è¯†åˆ«

çš„IEEE/CVFä¼šè®®è®ºæ–‡

é›†ä¸­ï¼Œç¬¬22669â€“22679é¡µï¼Œ2023å¹´

ã€‚

--------------------------------------------------



[280] [2]

Yoshua Bengio, Nicholas LÂ´eonard,

and Aaron Courville.

[280]

[2] Yoshua Bengioï¼ŒNicholas LÂ´eonardå’ŒAaron

Courvilleã€‚



--------------------------------------------------

[281] Estimating or propagating

gradients through stochastic neurons

for conditional computation.

[281]

é€šè¿‡éšæœº

ç¥ç»å…ƒä¼°ç®—

æˆ–ä¼ æ’­æ¢¯åº¦

ä»¥è¿›è¡Œæ¡ä»¶

è®¡ç®—ã€‚



--------------------------------------------------

[282] arXiv preprint arXiv:1308.3432,

2013.

[282] ARXIVé¢„å°å‹

ARXIVï¼š1308.3432ï¼Œ2013ã€‚

--------------------------------------------------



[283] [3]

Thibault Castells, Hyoung-Kyu Song,

Bo-Kyeong Kim, and Shinkook

Choi.

[283] [3] Castelsï¼ŒCaselsï¼ŒBo-Kyuong

Kiå’ŒShinkyok Choiã€‚



--------------------------------------------------

[284] Ld-pruner: Efficient pruning

of latent diffu- sion

models using task-agnostic insights.

[284] LD-PRUNERï¼šä½¿ç”¨ä»»åŠ¡

ä¸åˆæ—¶å®œçš„

è§è§£å¯¹æ½œåœ¨

æ‰©æ•£æ¨¡å‹çš„

æœ‰æ•ˆä¿®å‰ªã€‚



--------------------------------------------------

[285] In Proceedings of

the IEEE/CVF Conference on

Computer Vision and Pattern

Recognition, pages 821â€“830, 2024.

[285] åœ¨

IEEE/CVFè®¡ç®—æœºè§†è§‰

å’Œæ¨¡å¼è¯†åˆ«

ä¼šè®®è®ºæ–‡é›†

ï¼Œç¬¬821-830é¡µï¼Œ2024å¹´ã€‚



--------------------------------------------------

[286] [4] Huiwen Chang,

Han Zhang, Lu Jiang,

Ce Liu, and William

T Freeman.

[286] [4]

Huiwen Chang, Han Zhang,

Lu Jiang, Ce Liu,

and William T Freeman.

--------------------------------------------------



[287] Maskgit:

Masked generative image transformer.

[287] MaskGitï¼šæ©ç›–

çš„ç”Ÿæˆå›¾åƒ

å˜å‹å™¨ã€‚



--------------------------------------------------

[288] In Conference on

Computer Vision and Pattern

Recognition, pages 11315â€“11325, 2022.

[288] åœ¨è®¡

ç®—æœºè§†è§‰å’Œ

æ¨¡å¼è¯†åˆ«ä¼š

è®®ä¸Šï¼Œç¬¬11315â€“11325é¡µï¼Œ2022å¹´

ã€‚



--------------------------------------------------

[289] [5] Junsong Chen,

Jincheng Yu, Chongjian Ge,

Lewei Yao, Enze Xie,

Yue Wu, Zhongdao Wang,

James Kwok, Ping Luo,

Huchuan Lu, and Zhenguo

Li.

[289] [5] Junsong

Chen, Jincheng Yu, Chongjian

Ge, Lewei Yao, Enze

Xie, Yue Wu, Zhongdao

Wang, James Kwok, Ping

Luo, Huchuan Lu, and

Zhenguo Li.



--------------------------------------------------

[290] Pixart-Î±: Fast training

of dif- fusion transformer

for photorealistic text-to-image synthesis,

2023.

[290] PixArt-Î±ï¼šå¯¹å½±åƒå­¦æ–‡

æœ¬å¯¹å›¾åƒåˆ

æˆçš„å·®å¼‚å˜

å‹å™¨çš„å¿«é€Ÿ

è®­ç»ƒï¼Œ2023å¹´ã€‚

--------------------------------------------------



[291] [6]

Javier MartÂ´Ä±n Daniel VerdÂ´u.

[291] [6]å“ˆç»´

å°”Â·é©¬ç‰¹Â·ä¸¹å°¼

å°”Â·ç»´å°”å¾·Â·ä¹Œ

ã€‚



--------------------------------------------------

[292] Flux.1 lite: Distilling

flux1.dev for efficient text-to-image

generation.

[292] Flux.1 Liteï¼šè’¸é¦Flux1.Devï¼Œä»¥è¿›è¡Œ

æœ‰æ•ˆçš„æ–‡æœ¬

å¯¹å›¾åƒç”Ÿæˆ

ã€‚

--------------------------------------------------



[293] 2024.

[293] 2024ã€‚



--------------------------------------------------

[294] [7] Tri Dao,

Dan Fu, Stefano Ermon,

Atri Rudra, and Christo-

pher RÂ´e.

[294] [7]

Tri Daoï¼ŒDan Fuï¼ŒStefano Ermonï¼ŒAtri

Rudraå’ŒChristo-Pher RÂ´eã€‚



--------------------------------------------------

[295] Flashattention: Fast and

memory-efficient exact at- tention

with io-awareness.

[295] é—ªå­˜ï¼šå…·æœ‰

IOæ„è¯†çš„å¿«é€Ÿ

å’Œè®°å¿†æ•ˆç‡

ç²¾ç¡®ã€‚

--------------------------------------------------



[296] Advances

in Neural Information Processing

Systems, 35:16344â€“16359, 2022.

[296]

ç¥ç»ä¿¡

æ¯å¤„ç†ç³»ç»Ÿ

çš„è¿›å±•ï¼Œ35ï¼š16344â€“16359ï¼Œ2022ã€‚



--------------------------------------------------

[297] [8] Jia Deng,

Wei Dong, Richard Socher,

Li-Jia Li, Kai Li,

and Li Fei-Fei.

[297]

[8] Jia Deng, Wei

Dong, Richard Socher, Li-Jia

Li, Kai Li, and

Li Fei-Fei.



--------------------------------------------------

[298] Imagenet: A large-scale

hierarchical image database.

[298]

ImageNetï¼šå¤§è§„

æ¨¡åˆ†å±‚å›¾åƒ

æ•°æ®åº“ã€‚



--------------------------------------------------

[299] In 2009 IEEE

conference on computer vision

and pattern recognition, pages

248â€“255.

[299] åœ¨2009å¹´

IEEEè®¡ç®—æœºè§†è§‰

å’Œæ¨¡å¼è¯†åˆ«

ä¼šè®®ä¸Šï¼Œç¬¬248-255é¡µ

ã€‚

--------------------------------------------------



[300] Ieee,

2009.

[300] IEEEï¼Œ2009å¹´ã€‚

--------------------------------------------------



[301] [9]

Prafulla Dhariwal and Alexander

Nichol.

[301] [9] Prafulla

Dhariwalå’ŒAlexander Nicholã€‚



--------------------------------------------------

[302] Diffusion models beat

gans on image synthesis.

[302] æ‰©æ•£æ¨¡

å‹åœ¨å›¾åƒåˆ

æˆä¸Šå‡»è´¥äº†

ganã€‚



--------------------------------------------------

[303] Advances in neural

informa- tion processing systems,

34:8780â€“8794, 2021.

[303] ç¥ç»ä¿¡æ¯å¤„

ç†ç³»ç»Ÿçš„è¿›

å±•ï¼Œ34ï¼š8780â€“8794ï¼Œ2021ã€‚

--------------------------------------------------



[304] [10]

Maha Elbayad, Jiatao Gu,

Edouard Grave, and Michael

Auli.

[304] [10] Maha

Elbayadï¼ŒJiatao Guï¼ŒEdouard Graveå’ŒMichael Auliã€‚

--------------------------------------------------



[305] Depth-adaptive

transformer.

[305] æ·±åº¦è‡ª

é€‚åº”å˜å‹å™¨

ã€‚

--------------------------------------------------



[306] arXiv

preprint arXiv:1910.10073, 2019.

[306]

ARXIVé¢„å°å‹ARXIVï¼š1910.10073ï¼Œ2019ã€‚



--------------------------------------------------

[307] [11] Patrick Esser,

Sumith Kulal, Andreas Blattmann,

Rahim Entezari, Jonas MÂ¨uller,

Harry Saini, Yam Levi,

Dominik Lorenz, Axel Sauer,

Frederic Boesel, et al.

[307] [11] Patrick Esserï¼ŒSumith

Kulalï¼ŒAndreas Blatmannï¼ŒRahim Entezariï¼ŒJonasMÃ©ullerï¼ŒHarry Sainiï¼ŒYam

Leviï¼ŒDominik Lorenzï¼ŒAxel Sauerï¼ŒFrederic Boeselç­‰ã€‚

--------------------------------------------------



[308] Scaling

recti- fied flow transformers

for high-resolution image synthesis.

[308] ç”¨

äºé«˜åˆ†è¾¨ç‡

å›¾åƒåˆæˆçš„

ç¼©æ”¾ç›´æµå˜

å‹å™¨ã€‚



--------------------------------------------------

[309] In Forty-first International

Conference on Machine Learn-

ing, 2024.

[309] åœ¨ç¬¬41ä¸ª

æœºå™¨å­¦ä¹ å›½

é™…ä¼šè®®ä¸Šï¼Œ2024å¹´

ã€‚

--------------------------------------------------



[310] [12]

Gongfan Fang, Xinyin Ma,

and Xinchao Wang.

[310]

[12] Gongfan Fang, Xinyin

Ma, and Xinchao Wang.

--------------------------------------------------



[311] Structural

pruning for diffusion models.

[311] æ‰©æ•£æ¨¡å‹çš„

ç»“æ„ä¿®å‰ªã€‚



--------------------------------------------------

[312] In Advances in

Neural Infor- mation Processing

Systems, 2023.

[312] åœ¨

ç¥ç»ä¿¡æ¯å¤„

ç†ç³»ç»Ÿçš„è¿›

å±•ä¸­ï¼Œ2023å¹´ã€‚

--------------------------------------------------



[313] [13]

Gongfan Fang, Hongxu Yin,

Saurav Muralidharan, Greg Heinrich,

Jeff Pool, Jan Kautz,

Pavlo Molchanov, and Xin-

chao Wang.

[313] [13]

Go Varlaan Cannï¼ŒHogxisï¼ŒHuzv Mealtrailharanï¼ŒJeffriovï¼ŒJeva

Moolavå’ŒXinï¼ŒChaa Caberã€‚



--------------------------------------------------

[314] Maskllm: Learnable semi-structured

sparsity for large language

models.

[314] maskllmï¼šå¤§

å‹è¯­è¨€æ¨¡å‹

çš„å¯å­¦ä¹ åŠ

ç»“æ„åŒ–ç¨€ç–

æ€§ã€‚

--------------------------------------------------



[315] arXiv

preprint arXiv:2409.17481, 2024.

[315]

Arxivé¢„å°å‹ARXIVï¼š2409.17481ï¼Œ2024ã€‚



--------------------------------------------------

[316] [14] Zhengcong Fei,

Mingyuan Fan, Changqian Yu,

Debang Li, and Junshi

Huang.

[316] [14] Zhengcong

Fei, Mingyuan Fan, Changqian

Yu, Debang Li, and

Junshi Huang.



--------------------------------------------------

[317] Scaling diffusion transformers

to 16 bil- lion

parameters.

[317] å°†

æ‰©æ•£å˜å‹å™¨

ç¼©æ”¾åˆ°16ä¸ªäºŒ

å…ƒå‚æ•°ã€‚

--------------------------------------------------



[318] arXiv

preprint arXiv:2407.11633, 2024.

[318]

ARXIVé¢„å°

å‹ARXIVï¼š2407.11633ï¼Œ2024ã€‚



--------------------------------------------------

[319] [15] Zhengcong Fei,

Mingyuan Fan, Changqian Yu,

Debang Li, Youqiang Zhang,

and Junshi Huang.

[319]

[15] Zhengcong Fei, Mingyuan

Fan, Changqian Yu, Debang

Li, Youqiang Zhang, and

Junshi Huang.



--------------------------------------------------

[320] Dimba: Transformer- mamba

diffusion models.

[320] DIMBAï¼šå˜å‹å™¨

-  Mambaæ‰©

æ•£æ¨¡å‹ã€‚

--------------------------------------------------



[321] arXiv

preprint arXiv:2406.01159, 2024.

[321]

ARXIVé¢„å°

å‹ARXIVï¼š2406.01159ï¼Œ2024ã€‚



--------------------------------------------------

[322] [16] Shanghua Gao,

Zhijie Lin, Xingyu Xie,

Pan Zhou, Ming- Ming

Cheng, and Shuicheng Yan.

[322] [16] Shanghua Gao,

Zhijie Lin, Xingyu Xie,

Pan Zhou, Ming- Ming

Cheng, and Shuicheng Yan.

--------------------------------------------------



[323] Editanything:

Empower- ing unparalleled flexibility

in image editing and

generation.

[323] ç¼–è¾‘ï¼šåœ¨å›¾

åƒç¼–è¾‘å’Œç”Ÿ

æˆä¸­èµ‹äºˆæ— 

ä¸ä¼¦æ¯”çš„çµ

æ´»æ€§ã€‚

--------------------------------------------------



[324] In

Proceedings of the 31st

ACM International Conference on

Multimedia, Demo track, 2023.

[324] åœ¨ç¬¬31å±Š

ACMå›½é™…å¤šåª’ä½“

ä¼šè®®ä¸Šï¼Œæ¼”ç¤º

æ›²ç›®ï¼Œ2023å¹´ã€‚



--------------------------------------------------

[325] [17] Emil Julius

Gumbel.

[325] [17] Emil

Julius Gumbelã€‚



--------------------------------------------------

[326] Statistical theory of

extreme values and some

practical applications: a series

of lectures.

[326] æå€¼

å’Œä¸€äº›å®é™…

åº”ç”¨çš„ç»Ÿè®¡

ç†è®ºï¼šä¸€ç³»åˆ—

è®²åº§ã€‚

--------------------------------------------------



[327] US

Gov- ernment Printing Office,

1954.

[327] ç¾å›½æ”¿

åºœå°åˆ·åŠå…¬

å®¤ï¼Œ1954å¹´ã€‚

--------------------------------------------------



[328] [18]

Song Han, Jeff Pool,

John Tran, and William

Dally.

[328] [18] Song

Hanï¼ŒJeff Poolï¼ŒJohn Tranå’ŒWilliam Dallyã€‚

--------------------------------------------------



[329] Learn-

ing both weights and

connections for efficient neural

net- work.

[329] å­¦ä¹ 

é‡é‡å’Œè¿æ¥

ä»¥è¿›è¡Œæœ‰æ•ˆ

çš„ç¥ç»ç½‘ç»œ

ã€‚

--------------------------------------------------



[330] Advances

in neural information processing

systems, 28, 2015.

[330]

ç¥ç»ä¿¡æ¯å¤„

ç†ç³»ç»Ÿçš„è¿›

å±•ï¼Œ2015å¹´2æœˆ28æ—¥ã€‚



--------------------------------------------------

[331] [19] Yefei He,

Luping Liu, Jing Liu,

Weijia Wu, Hong Zhou,

and Bohan Zhuang.

[331]

[19] Yefei He, Luping

Liu, Jing Liu, Weijia

Wu, Hong Zhou, and

Bohan Zhuang.



--------------------------------------------------

[332] Ptqd: Accurate post-training

quantization for diffusion models.

[332] PTQDï¼šæ‰©

æ•£æ¨¡å‹çš„å‡†

ç¡®åŸ¹è®­é‡åŒ–

ã€‚



--------------------------------------------------

[333] Advances in Neural

Information Pro- cessing Systems,

36, 2024.

[333] ç¥ç»ä¿¡æ¯è¿‡

ç¨‹ç³»ç»Ÿçš„è¿›

å±•ï¼Œ36ï¼Œ2024ã€‚

--------------------------------------------------



[334] [20]

Geoffrey Hinton, Oriol Vinyals,

Jeff Dean, et al.

[334] [20] Geoffrey Hintonï¼ŒOrol

Vinyalsï¼ŒJeff Deanç­‰ã€‚



--------------------------------------------------

[335] Distill- ing the

knowledge in a neural

network.

[335] åœ¨ç¥ç»

ç½‘ç»œä¸­æç‚¼

çŸ¥è¯†ã€‚

--------------------------------------------------



[336] arXiv

preprint arXiv:1503.02531, 2(7), 2015.

[336] ARXIVé¢„å°å‹

ARXIVï¼š1503.02531ï¼Œ2ï¼ˆ7ï¼‰ï¼Œ2015å¹´ã€‚



--------------------------------------------------

[337] [21] Edward J

Hu, yelong shen, Phillip

Wallis, Zeyuan Allen- Zhu,

Yuanzhi Li, Shean Wang,

Lu Wang, and Weizhu

Chen.

[337] [21] Edward

J Hu, yelong shen,

Phillip Wallis, Zeyuan Allen-

Zhu, Yuanzhi Li, Shean

Wang, Lu Wang, and

Weizhu Chen.



--------------------------------------------------

[338] LoRA: Low-rank adaptation

of large language models.

[338] LORAï¼šå¤§å‹è¯­è¨€

æ¨¡å‹çš„ä½çº§

æ”¹ç¼–ã€‚



--------------------------------------------------

[339] In In- ternational

Conference on Learning Representations,

2022.

[339] åœ¨2022å¹´å­¦

ä¹ è¡¨å¾çš„å›½

é™…ä¼šè®®ä¸Šã€‚

--------------------------------------------------



[340] [22]

Eric Jang, Shixiang Gu,

and Ben Poole.

[340]

[22] Eric Jang, Shixiang

Gu, and Ben Poole.

--------------------------------------------------



[341] Categorical

reparameterization with gumbel-softmax.

[341]

ä½¿

ç”¨Gumbel-Softmaxè¿›è¡Œåˆ†ç±»

é‡æ–°èšé›†ã€‚



--------------------------------------------------

[342] arXiv preprint arXiv:1611.01144,

2016.

[342] ARXIVé¢„

å°å‹ARXIVï¼š1611.01144ï¼Œ2016ã€‚

--------------------------------------------------



[343] [23]

Bo-Kyeong Kim, Hyoung-Kyu Song,

Thibault Castells, and Shinkook

Choi.

[343] ï¼ˆ23] Chostelså’ŒShillongsæ˜¯æœ‰

ç¦çš„ã€‚

--------------------------------------------------



[344] Bk-sdm:

Architecturally compressed stable diffusion

for efficient text-to-image generation.

[344] BK-SDMï¼šç»“æ„å‹

ç¼©çš„ç¨³å®šæ‰©

æ•£ï¼Œä»¥å®ç°æœ‰

æ•ˆçš„æ–‡æœ¬å¯¹

å›¾åƒç”Ÿæˆã€‚



--------------------------------------------------

[345] In Workshop on

Efficient Systems for Foundation

Models@ ICML2023, 2023.

[345]

åœ¨

åŸºç¡€æ¨¡å‹@ ICML2023ï¼Œ2023çš„

é«˜æ•ˆç³»ç»Ÿç ”

è®¨ä¼šä¸Šã€‚



--------------------------------------------------

[346] [24] Bo-Kyeong Kim,

Geonmin Kim, Tae-Ho Kim,

Thibault Castells, Shinkook Choi,

Junho Shin, and Hyoung-Kyu

Song.

[346] [24] Bo-Kyeong

Kimï¼ŒGeonmin Kimï¼ŒTae-Ho Kimï¼ŒThibault Castelsï¼ŒShillongã€‚

--------------------------------------------------



[347] Shortened

llama: A simple depth

pruning for large lan-

guage models.

[347] ç¼©çŸ­

çš„ç¾æ´²é©¼ï¼šå¤§

å‹è¯­è¨€æ¨¡å‹

çš„ç®€å•æ·±åº¦

ä¿®å‰ªã€‚

--------------------------------------------------



[348] arXiv

preprint arXiv:2402.02834, 11, 2024.

[348] Arxivé¢„å°å‹

ARXIVï¼š2402.02834ï¼Œ11ï¼Œ2024ã€‚



--------------------------------------------------

[349] [25] PKU-Yuan Lab

and Tuzhan AI etc.

[349] [25] PKU-YUAN LABå’ŒTUZHAN

AIç­‰ã€‚



--------------------------------------------------

[350] Open-sora-plan, 2024.

[350]

å¼€æ”¾å¼

è®¡åˆ’ï¼Œ2024å¹´ã€‚



--------------------------------------------------

[351] [26] Black Forest

Labs.

[351] [26]é»‘æ£®

æ—å®éªŒå®¤ã€‚

--------------------------------------------------



[352] FLUX,

2024.

[352] Fluxï¼Œ2024ã€‚

--------------------------------------------------



[353] [27]

Youngwan Lee, Yong-Ju Lee,

and Sung Ju Hwang.

[353] [27] Youngwan Lee,

Yong-Ju Lee, and Sung

Ju Hwang.



--------------------------------------------------

[354] Dit- pruner: Pruning

diffusion transformer models for

text-to- image synthesis using

human preference scores.

[354]

Dit-Prunerï¼šä½¿

ç”¨äººç±»åå¥½

å¾—åˆ†è¿›è¡Œä¿®

å‰ªæ‰©æ•£å˜å‹

å™¨æ¨¡å‹ï¼Œç”¨äº

æ–‡æœ¬å›¾åƒåˆ

æˆã€‚



--------------------------------------------------

[355] 9

[355] 9

--------------------------------------------------



[356] [28]

Youngwan Lee, Kwanyong Park,

Yoorhim Cho, Yong-Ju Lee,

and Sung Ju Hwang.

[356] [28] Youngwan Leeï¼ŒKwanyong

Parkï¼ŒYoorhim Choï¼ŒYong Ju Leeå’ŒSung

Ju Hwangã€‚



--------------------------------------------------

[357] Koala: self-attention mat-

ters in knowledge distillation

of latent diffusion models

for memory-efficient and fast

image synthesis.

[357] Koalaï¼šåœ¨æ½œåœ¨

æ‰©æ•£æ¨¡å‹çš„

çŸ¥è¯†è’¸é¦ä¸­

è¿›è¡Œè‡ªæˆ‘æ³¨

æ„äº‹é¡¹ï¼Œä»¥è¿›

è¡Œè®°å¿†æ•ˆç‡

å’Œå¿«é€Ÿå›¾åƒ

åˆæˆã€‚

--------------------------------------------------



[358] arXiv

e-prints, pages arXivâ€“2312, 2023.

[358] Arxivç”µå­æ‰“

å°ï¼Œç¬¬2312é¡µï¼Œ2023å¹´ã€‚



--------------------------------------------------

[359] [29] Tianhong Li,

Yonglong Tian, He Li,

Mingyang Deng, and Kaiming

He.

[359] [29] Tianhong

Li, Yonglong Tian, He

Li, Mingyang Deng, and

Kaiming He.



--------------------------------------------------

[360] Autoregressive image generation

without vec- tor quantization.

[360] è‡ª

å›å½’å›¾åƒç”Ÿ

æˆè€Œæ— éœ€é‡

åŒ–é‡åŒ–ã€‚



--------------------------------------------------

[361] arXiv preprint arXiv:2406.11838,

2024.

[361] Arxivé¢„å°

å‹ARXIVï¼š2406.11838ï¼Œ2024ã€‚

--------------------------------------------------



[362] [30]

Xiuyu Li, Yijiang Liu,

Long Lian, Huanrui Yang,

Zhen Dong, Daniel Kang,

Shanghang Zhang, and Kurt

Keutzer.

[362] [30] Xiuyu

Li, Yijiang Liu, Long

Lian, Huanrui Yang, Zhen

Dong, Daniel Kang, Shanghang

Zhang, and Kurt Keutzer.

--------------------------------------------------



[363] Q-diffusion:

Quantizing diffusion models.

[363]

Qæ‰©æ•£ï¼šé‡åŒ–

æ‰©æ•£æ¨¡å‹ã€‚



--------------------------------------------------

[364] In Proceedings of

the IEEE/CVF International Conference

on Computer Vi- sion,

pages 17535â€“17545, 2023.

[364]

åœ¨

IEEE/CVFå›½é™…è®¡ç®—æœº

VI-Sionä¼šè®®è®ºæ–‡é›†

ï¼Œç¬¬17535â€“ 17545å¹´ï¼Œ2023å¹´ã€‚



--------------------------------------------------

[365] [31] Yanyu Li,

Huan Wang, Qing Jin,

Ju Hu, Pavlo Chemerys,

Yun Fu, Yanzhi Wang,

Sergey Tulyakov, and Jian

Ren.

[365] [31] Yanyu

Li, Huan Wang, Qing

Jin, Ju Hu, Pavlo

Chemerys, Yun Fu, Yanzhi

Wang, Sergey Tulyakov, and

Jian Ren.



--------------------------------------------------

[366] Snap- fusion: Text-to-image

diffusion model on mobile

devices within two seconds.

[366] å¿«é€Ÿ

èåˆï¼šç§»åŠ¨è®¾

å¤‡ä¸Šçš„æ–‡æœ¬

åˆ°å›¾åƒæ‰©æ•£

æ¨¡å‹åœ¨ä¸¤ç§’

é’Ÿå†…ã€‚



--------------------------------------------------

[367] Advances in Neural

Information Pro- cessing Systems,

36, 2024.

[367] ç¥ç»ä¿¡

æ¯è¿‡ç¨‹ç³»ç»Ÿ

çš„è¿›å±•ï¼Œ36ï¼Œ2024ã€‚

--------------------------------------------------



[368] [32]

Shanchuan Lin, Anran Wang,

and Xiao Yang.

[368]

[32] Shanchuan Lin, Anran

Wang, and Xiao Yang.

--------------------------------------------------



[369] Sdxl-

lightning: Progressive adversarial diffusion

distillation.

[369] SDXL-é—ªç”µ

ï¼šè¿›è¡Œæ€§å¯¹æŠ—

æ‰©æ•£è’¸é¦ã€‚

--------------------------------------------------



[370] arXiv

preprint arXiv:2402.13929, 2024.

[370]

ARXIVé¢„

å°å‹ARXIVï¼š2402.13929ï¼Œ2024ã€‚



--------------------------------------------------

[371] [33] Cheng Lu,

Yuhao Zhou, Fan Bao,

Jianfei Chen, Chongxuan Li,

and Jun Zhu.

[371]

[33] Cheng Lu, Yuhao

Zhou, Fan Bao, Jianfei

Chen, Chongxuan Li, and

Jun Zhu.



--------------------------------------------------

[372] Dpm-solver: A fast

ode solver for diffusion

probabilistic model sampling in

around 10 steps.

[372]

DPM-Solverï¼šåœ¨å¤§çº¦

10ä¸ªæ­¥éª¤ä¸­è¿›

è¡Œæ‰©æ•£æ¦‚ç‡

æ¨¡å‹é‡‡æ ·çš„

å¿«é€ŸODEæ±‚è§£å™¨

ã€‚



--------------------------------------------------

[373] Advances in Neural

Information Processing Systems, 35:5775â€“5787,

2022.

[373] ç¥ç»ä¿¡æ¯å¤„

ç†ç³»ç»Ÿçš„è¿›

å±•ï¼Œ35ï¼š5775â€“5787ï¼Œ2022ã€‚

--------------------------------------------------



[374] [34]

Nanye Ma, Mark Goldstein,

Michael S Albergo, Nicholas

M Boffi, Eric Vanden-Eijnden,

and Saining Xie.

[374]

[34] Nanye Maï¼ŒMichael S

Albergoã€‚



--------------------------------------------------

[375] Sit: Explor- ing

flow and diffusion-based generative

models with scalable interpolant

transformers.

[375] SITï¼šå…·æœ‰å¯æ‰©

å±•çš„æ’å€¼å˜

å‹å™¨çš„æ¢ç´¢

æµé‡å’ŒåŸºäº

æ‰©æ•£çš„ç”Ÿæˆ

æ¨¡å‹ã€‚

--------------------------------------------------



[376] arXiv

preprint arXiv:2401.08740, 2024.

[376]

ARXIVé¢„å°å‹

ARXIVï¼š2401.08740ï¼Œ2024ã€‚



--------------------------------------------------

[377] [35] Xinyin Ma,

Gongfan Fang, Michael Bi

Mi, and Xinchao Wang.

[377] [35] Xinyin Ma,

Gongfan Fang, Michael Bi

Mi, and Xinchao Wang.

--------------------------------------------------



[378] Learning-to-cache:

Accelerating diffusion trans- former

via layer caching, 2024.

[378] å­¦ä¹ åˆ°ç¼“å­˜

ï¼šé€šè¿‡å±‚ç¼“å­˜

åŠ é€Ÿæ‰©æ•£è·¨

å‰æœŸï¼Œ2024å¹´ã€‚



--------------------------------------------------

[379] [36] Xin Men,

Mingyu Xu, Qingyu Zhang,

Bingning Wang, Hongyu Lin,

Yaojie Lu, Xianpei Han,

and Weipeng Chen.

[379]

[36] Xin Men, Mingyu

Xu, Qingyu Zhang, Bingning

Wang, Hongyu Lin, Yaojie

Lu, Xianpei Han, and

Weipeng Chen.



--------------------------------------------------

[380] Shortgpt: Layers in

large language models are

more redun- dant than

you expect.

[380] çŸ­æœŸ

ï¼šå¤§è¯­è¨€æ¨¡å‹

ä¸­çš„å±‚æ¬¡æ¯”

æ‚¨é¢„æœŸçš„è¦

é‡æ–°ä½¿ç”¨ã€‚

--------------------------------------------------



[381] arXiv

preprint arXiv:2403.03853, 2024.

[381]

Arxivé¢„

å°å‹ARXIVï¼š2403.03853ï¼Œ2024ã€‚



--------------------------------------------------

[382] [37] Pavlo Molchanov,

Stephen Tyree, Tero Karras,

Timo Aila, and Jan

Kautz.

[382] [37] Pavlo

Molchanovï¼ŒStephen Tyreeï¼ŒTero Karrasï¼ŒTimo Ailaå’ŒJan

Kautzã€‚



--------------------------------------------------

[383] Pruning convolutional neural

networks for re- source

efficient inference.

[383] ä¿®å‰ª

å·ç§¯ç¥ç»ç½‘

ç»œï¼Œä»¥æé«˜æ¨

ç†ã€‚

--------------------------------------------------



[384] arXiv

preprint arXiv:1611.06440, 2016.

[384]

Arxivé¢„å°å‹ARXIVï¼š1611.06440ï¼Œ2016ã€‚



--------------------------------------------------

[385] [38] Zanlin Ni,

Yulin Wang, Renping Zhou,

Jiayi Guo, Jinyi Hu,

Zhiyuan Liu, Shiji Song,

Yuan Yao, and Gao

Huang.

[385] [38] Zanlin

Ni, Yulin Wang, Renping

Zhou, Jiayi Guo, Jinyi

Hu, Zhiyuan Liu, Shiji

Song, Yuan Yao, and

Gao Huang.



--------------------------------------------------

[386] Revisiting non-autoregressive transformers

for efficient im- age

synthesis.

[386] é‡

æ–°å®¡è§†éè‡ª

åŠ¨è¿›å–çš„å˜

å‹å™¨ï¼Œä»¥è¿›è¡Œ

æœ‰æ•ˆçš„è¿›ä¸€

æ­¥ç»¼åˆã€‚

--------------------------------------------------



[387] In

Proceedings of the IEEE/CVF

Conference on Computer Vision

and Pattern Recognition, pages

7007â€“ 7016, 2024.

[387]

åœ¨IEEE/CVFè®¡

ç®—æœºè§†è§‰å’Œ

æ¨¡å¼è¯†åˆ«ä¼š

è®®è®ºæ–‡é›†ï¼Œç¬¬

7007â€“ 7016é¡µï¼Œ2024å¹´ã€‚



--------------------------------------------------

[388] [39] Byeongjun Park,

Sangmin Woo, Hyojun Go,

Jin-Young Kim, and Changick

Kim.

[388] [39] Byeongjun

Parkï¼ŒSangmin Wooï¼ŒHyojun Goï¼ŒJin-Young Kimå’ŒChangick

Kimã€‚



--------------------------------------------------

[389] Denoising task routing

for diffusion models.

[389]

æ‰©æ•£

æ¨¡å‹çš„å‰¥è½

ä»»åŠ¡è·¯ç”±ã€‚



--------------------------------------------------

[390] arXiv preprint arXiv:2310.07138,

2023.

[390] Arxivé¢„

å°å‹ARXIVï¼š2310.07138ï¼Œ2023ã€‚

--------------------------------------------------



[391] [40]

William Peebles and Saining

Xie.

[391] [40]å¨å»‰Â·çš®

å¸ƒå°”æ–¯ï¼ˆWilliam Peeblesï¼‰å’Œå¤

å¨å°”ï¼ˆXieï¼‰ã€‚

--------------------------------------------------



[392] Scalable

diffusion models with transformers.

[392] å…·æœ‰å˜

å‹å™¨çš„å¯æ‰©

å±•æ¨¡å‹ã€‚



--------------------------------------------------

[393] In Proceedings of

the IEEE/CVF Inter- national

Conference on Computer Vision,

pages 4195â€“4205, 2023.

[393]

åœ¨IEEE/CVFå…¨

å›½è®¡ç®—æœºè§†

è§‰ä¼šè®®è®ºæ–‡

é›†ï¼Œç¬¬4195â€“4205é¡µï¼Œ2023å¹´ã€‚



--------------------------------------------------

[394] [41] David Raposo,

Sam Ritter, Blake Richards,

Timothy Lillicrap, Peter Conway

Humphreys, and Adam San-

toro.

[394] [41] David

Raposoï¼ŒSam Ritterï¼ŒBlake Richardsï¼ŒTimothy Lillicrapï¼ŒPeter

Conway Humphreyså’Œ

Adam Santoroã€‚

--------------------------------------------------



[395] Mixture-of-depths:

Dynamically allocating com- pute

in transformer-based language models.

[395] æ·±å…¥çš„æ··åˆ

ç‰©ï¼šåŸºäºå˜å‹

å™¨çš„è¯­è¨€æ¨¡

å‹åŠ¨æ€åˆ†é…

è®¡ç®—ã€‚



--------------------------------------------------

[396] arXiv preprint arXiv:2404.02258,

2024.

[396] ARXIVé¢„å°å‹

ARXIVï¼š2404.02258ï¼Œ2024ã€‚

--------------------------------------------------



[397] [42]

Adriana Romero, Nicolas Ballas,

Samira Ebrahimi Kahou, Antoine

Chassang, Carlo Gatta, and

Yoshua Bengio.

[397] [42]

Adriana Romeroï¼ŒNicolas Ballasï¼ŒSamira Jewish

Kahouï¼ŒAntoine Chassangï¼ŒCarlo Gattaå’ŒYoshua Bengioã€‚

--------------------------------------------------



[398] Fitnets:

Hints for thin deep

nets.

[398] fitnetsï¼šè–„ç½‘çš„æ

ç¤ºã€‚

--------------------------------------------------



[399] arXiv

preprint arXiv:1412.6550, 2014.

[399]

Arxivé¢„å°å‹ARXIVï¼š1412.6550ï¼Œ2014ã€‚



--------------------------------------------------

[400] [43] Tim Salimans

and Jonathan Ho.

[400]

[43]è’‚

å§†Â·è¨åˆ©æ›¼æ–¯

ï¼ˆTim Salimansï¼‰å’Œä¹”çº³æ£®Â·ä½•

ï¼ˆJonathan Hoï¼‰ã€‚

--------------------------------------------------



[401] Progressive

distillation for fast sampling

of diffusion models.

[401]

ç”¨äºå¿«é€Ÿé‡‡

æ ·æ‰©æ•£æ¨¡å‹

çš„è¿›è¡Œæ€§è’¸

é¦ã€‚



--------------------------------------------------

[402] arXiv preprint arXiv:2202.00512,

2022.

[402] ARXIVé¢„å°å‹ARXIVï¼š2202.00512ï¼Œ2022ã€‚

--------------------------------------------------



[403] [44]

Yuzhang Shang, Zhihang Yuan,

Bin Xie, Bingzhe Wu,

and Yan Yan.

[403]

[44] Yuzhang Shang, Zhihang

Yuan, Bin Xie, Bingzhe

Wu, and Yan Yan.

--------------------------------------------------



[404] Post-training

quantization on diffusion models.

[404] æ‰©

æ•£æ¨¡å‹ä¸Šçš„

è®­ç»ƒåé‡åŒ–

ã€‚



--------------------------------------------------

[405] In Proceedings of

the IEEE/CVF conference on

computer vi- sion and

pattern recognition, pages 1972â€“1981,

2023.

[405] åœ¨IEEE/CVFè®¡ç®—æœºå½•

åƒå’Œæ¨¡å¼è¯†

åˆ«ä¼šè®®è®ºæ–‡

é›†ï¼Œç¬¬1972- 1981å¹´ï¼Œ2023å¹´ã€‚

--------------------------------------------------



[406] [45]

Jiaming Song, Chenlin Meng,

and Stefano Ermon.

[406]

[45] Jiaming Song, Chenlin

Meng, and Stefano Ermon.

--------------------------------------------------



[407] Denoising

diffusion implicit models.

[407]

å‰¥

ç¦»æ‰©æ•£éšå¼

æ¨¡å‹ã€‚



--------------------------------------------------

[408] arXiv preprint arXiv:2010.02502,

2020.

[408] ARXIVé¢„å°å‹

ARXIVï¼š2010.02502ï¼Œ2020ã€‚

--------------------------------------------------



[409] [46]

Yang Song, Prafulla Dhariwal,

Mark Chen, and Ilya

Sutskever.

[409] [46]æ­Œæ›²ï¼ŒPraflulla Dharillï¼ŒMark

Chenå’ŒIlio Suskverã€‚



--------------------------------------------------

[410] Consistency models.

[410]

ä¸€è‡´

æ€§æ¨¡å‹ã€‚



--------------------------------------------------

[411] arXiv preprint arXiv:2303.01469,

2023.

[411] Arxivé¢„å°

å‹ARXIVï¼š2303.01469ï¼Œ2023ã€‚

--------------------------------------------------



[412] [47]

Mingjie Sun, Xinlei Chen,

J Zico Kolter, and

Zhuang Liu.

[412] [47]

Mingjie Sun, Xinlei Chen,

J Zico Kolter, and

Zhuang Liu.



--------------------------------------------------

[413] Massive activations in

large language models.

[413]

å¤§è¯­è¨€æ¨¡

å‹ä¸­çš„å¤§é‡

æ¿€æ´»ã€‚



--------------------------------------------------

[414] arXiv preprint arXiv:2402.17762,

2024.

[414] Arxivé¢„å°å‹

ARXIVï¼š2402.17762ï¼Œ2024ã€‚

--------------------------------------------------



[415] [48]

Yao Teng, Yue Wu,

Han Shi, Xuefei Ning,

Guohao Dai, Yu Wang,

Zhenguo Li, and Xihui

Liu.

[415] [48] Yao

Teng, Yue Wu, Han

Shi, Xuefei Ning, Guohao

Dai, Yu Wang, Zhenguo

Li, and Xihui Liu.

--------------------------------------------------



[416] Dim:

Diffusion mamba for efficient

high-resolution image synthesis.

[416]

DIMï¼šæ‰©æ•£MAMBAï¼Œç”¨äºæœ‰

æ•ˆçš„é«˜åˆ†è¾¨

ç‡å›¾åƒåˆæˆ

ã€‚



--------------------------------------------------

[417] arXiv preprint arXiv:2405.14224,

2024.

[417] ARXIVé¢„å°å‹ARXIVï¼š2405.14224ï¼Œ2024ã€‚

--------------------------------------------------



[418] [49]

Keyu Tian, Yi Jiang,

Zehuan Yuan, Bingyue Peng,

and Li- wei Wang.

[418] [49] Keyu Tian,

Yi Jiang, Zehuan Yuan,

Bingyue Peng, and Li-

wei Wang.



--------------------------------------------------

[419] Visual autoregressive modeling:

Scalable image generation via

next-scale prediction.

[419] è§†è§‰

è‡ªå›å½’å»ºæ¨¡

ï¼šå¯æ‰©å±•å›¾åƒ

é€šè¿‡æ¢å¥è¯

é¢„æµ‹ã€‚

--------------------------------------------------



[420] 2024.

[420] 2024ã€‚



--------------------------------------------------

[421] [50] Yuchuan Tian,

Zhijun Tu, Hanting Chen,

Jie Hu, Chao Xu,

and Yunhe Wang.

[421]

[50] Yuchuan Tian, Zhijun

Tu, Hanting Chen, Jie

Hu, Chao Xu, and

Yunhe Wang.



--------------------------------------------------

[422] U-dits: Downsample tokens

in u-shaped diffusion transformers.

[422] U-Ditsï¼šUå½¢æ‰©æ•£

å˜å‹å™¨ä¸­çš„

ä¸‹æ ·å“ä»¤ç‰Œ

ã€‚



--------------------------------------------------

[423] arXiv preprint arXiv:2405.02730,

2024.

[423] Arxivé¢„å°å‹ARXIVï¼š2405.02730ï¼Œ2024ã€‚

--------------------------------------------------



[424] [51]

Kafeng Wang, Jianfei Chen,

He Li, Zhenpeng Mi,

and Jun Zhu.

[424]

[51] Kafeng Wang, Jianfei

Chen, He Li, Zhenpeng

Mi, and Jun Zhu.

--------------------------------------------------



[425] Sparsedm:

Toward sparse efficient diffusion

models.

[425] SparsedMï¼šæœç€

ç¨€ç–çš„æœ‰æ•ˆ

æ‰©æ•£æ¨¡å‹ã€‚

--------------------------------------------------



[426] arXiv

preprint arXiv:2404.10445, 2024.

[426]

ARXIVé¢„

å°å‹ARXIVï¼š2404.10445ï¼Œ2024ã€‚



--------------------------------------------------

[427] [52] Enze Xie,

Junsong Chen, Junyu Chen,

Han Cai, Yujun Lin,

Zhekai Zhang, Muyang Li,

Yao Lu, and Song

Han.

[427] [52] Enze

Xie, Junsong Chen, Junyu

Chen, Han Cai, Yujun

Lin, Zhekai Zhang, Muyang

Li, Yao Lu, and

Song Han.



--------------------------------------------------

[428] Sana: Ef- ficient

high-resolution image synthesis with

linear diffusion transformers.

[428]

SANAï¼šå…·æœ‰çº¿

æ€§æ‰©æ•£å˜å‹

å™¨çš„æ•ˆç‡é«˜

åˆ†è¾¨ç‡å›¾åƒ

åˆæˆã€‚



--------------------------------------------------

[429] arXiv preprint arXiv:2410.10629,

2024.

[429] ARXIVé¢„å°å‹

ARXIVï¼š2410.10629ï¼Œ2024ã€‚

--------------------------------------------------



[430] [53]

Ling Yang, Zhilong Zhang,

Yang Song, Shenda Hong,

Run- sheng Xu, Yue

Zhao, Wentao Zhang, Bin

Cui, and Ming- Hsuan

Yang.

[430] [53] Ling

Yang, Zhilong Zhang, Yang

Song, Shenda Hong, Run-

sheng Xu, Yue Zhao,

Wentao Zhang, Bin Cui,

and Ming- Hsuan Yang.

--------------------------------------------------



[431] Diffusion

models: A comprehensive survey

of methods and applications.

[431] æ‰©æ•£æ¨¡å‹ï¼šå¯¹

æ–¹æ³•å’Œåº”ç”¨

çš„å…¨é¢è°ƒæŸ¥

ã€‚



--------------------------------------------------

[432] ACM Computing Surveys,

56(4): 1â€“39, 2023.

[432]

ACMè®¡ç®—è°ƒæŸ¥ï¼Œ56ï¼ˆ4ï¼‰ï¼š1â€“39ï¼Œ2023ã€‚



--------------------------------------------------

[433] [54] Fang Yu,

Kun Huang, Meng Wang,

Yuan Cheng, Wei Chu,

and Li Cui.

[433]

[54] Fang Yu, Kun

Huang, Meng Wang, Yuan

Cheng, Wei Chu, and

Li Cui.



--------------------------------------------------

[434] Width & depth

pruning for vision transformers.

[434] è§†

è§‰å˜å‹å™¨çš„

å®½åº¦å’Œæ·±åº¦

ä¿®å‰ªã€‚



--------------------------------------------------

[435] In Conference on

Artificial Intelligence (AAAI), 2022.

[435] åœ¨äººå·¥

æ™ºèƒ½ä¼šè®®ä¸Š

ï¼ˆAAAIï¼‰ï¼Œ2022å¹´ã€‚



--------------------------------------------------

[436] [55] Tao Yu,

Runseng Feng, Ruoyu Feng,

Jinming Liu, Xin Jin,

Wenjun Zeng, and Zhibo

Chen.

[436] [55] Tao

Yu, Runseng Feng, Ruoyu

Feng, Jinming Liu, Xin

Jin, Wenjun Zeng, and

Zhibo Chen.



--------------------------------------------------

[437] Inpaint anything: Segment

anything meets image inpainting.

[437] æ¶‚æ¼†çš„ä»»

ä½•ä¸œè¥¿ï¼šæ®µçš„

ä»»ä½•ä¸œè¥¿éƒ½

ç¬¦åˆå›¾åƒè¦†

ç›–ã€‚



--------------------------------------------------

[438] arXiv preprint arXiv:2304.06790,

2023.

[438] ARXIVé¢„å°å‹ARXIVï¼š2304.06790ï¼Œ2023ã€‚

--------------------------------------------------



[439] [56]

Dingkun Zhang, Sijia Li,

Chen Chen, Qingsong Xie,

and Haonan Lu.

[439]

[ç¿»

è¯‘å¤±è´¥]



--------------------------------------------------

[440] Laptop-diff: Layer pruning

and normalized dis- 10

[440] ç¬”è®°

æœ¬ç”µè„‘æœ¨ï¼šä¿®

å‰ªå’Œæ ‡å‡†åŒ–

çš„å±‚



--------------------------------------------------

[441] tillation for compressing

diffusion models.

[441] ç”¨äºå‹

ç¼©æ‰©æ•£æ¨¡å‹

çš„è€•ä½œã€‚

--------------------------------------------------



[442] arXiv

preprint arXiv:2404.11098, 2024.

[442]

Arxivé¢„å°

å‹ARXIVï¼š2404.11098ï¼Œ2024ã€‚



--------------------------------------------------

[443] [57] Xuanlei Zhao,

Xiaolong Jin, Kai Wang,

and Yang You.

[443]

[57] Xuanlei Zhao, Xiaolong

Jin, Kai Wang, and

Yang You.



--------------------------------------------------

[444] Real-time video generation

with pyramid attention broad-

cast.

[444] å¸¦æœ‰é‡‘å­—

å¡”æ³¨æ„çš„å®

æ—¶è§†é¢‘å‘è¡Œ

å¹¿æ³›ã€‚

--------------------------------------------------



[445] arXiv

preprint arXiv:2408.12588, 2024.

[445]

ARXIVé¢„å°å‹

ARXIVï¼š2408.12588ï¼Œ2024ã€‚



--------------------------------------------------

[446] [58] Yang Zhao,

Yanwu Xu, Zhisheng Xiao,

and Tingbo Hou.

[446]

[58] Yang Zhao, Yanwu

Xu, Zhisheng Xiao, and

Tingbo Hou.



--------------------------------------------------

[447] Mobilediffusion: Subsecond text-to-image

generation on mobile devices.

[447] åŠ¨å‘˜iffusionï¼šç§»åŠ¨è®¾

å¤‡ä¸Šçš„æ¬¡è¦

æ–‡æœ¬å¯¹å›¾åƒ

ç”Ÿæˆã€‚



--------------------------------------------------

[448] arXiv preprint arXiv:2311.16567,

2023.

[448] Arxivé¢„å°å‹

ARXIVï¼š2311.16567ï¼Œ2023ã€‚

--------------------------------------------------



[449] [59]

Zangwei Zheng, Xiangyu Peng,

Tianji Yang, Chenhui Shen,

Shenggui Li, Hongxin Liu,

Yukun Zhou, Tianyi Li,

and Yang You.

[449]

[59] Zangwei Zheng, Xiangyu

Peng, Tianji Yang, Chenhui

Shen, Shenggui Li, Hongxin

Liu, Yukun Zhou, Tianyi

Li, and Yang You.

--------------------------------------------------



[450] Open-sora:

Democratizing efficient video production

for all, 2024.

[450]

å¼€æ”¾å¼ï¼šå°†æ‰€

æœ‰äººçš„é«˜æ•ˆ

è§†é¢‘åˆ¶ä½œæ°‘

ä¸»åŒ–ï¼Œ2024å¹´ã€‚



--------------------------------------------------

[451] 11

[451] 11

--------------------------------------------------



[452] TinyFusion:

Diffusion Transformers Learned Shallow

Supplementary Material 6.

[452]

å°å‹

çŒæ³¨ï¼šæ‰©æ•£å˜

å‹å™¨å­¦åˆ°çš„

æµ…è¡¥å……ææ–™

6ã€‚



--------------------------------------------------

[453] Experimental Details Models.

[453] å®éªŒç»†èŠ‚æ¨¡

å‹ã€‚



--------------------------------------------------

[454] Our experiments evaluate

the effectiveness of three

models: DiT-XL, MAR-Large, and

SiT-XL.

[454] æˆ‘ä»¬çš„å®

éªŒè¯„ä¼°äº†ä¸‰

ä¸ªæ¨¡å‹çš„æœ‰

æ•ˆæ€§ï¼šDIT-XLï¼ŒMAR-LARGEå’ŒSIT-XLã€‚

--------------------------------------------------



[455] Diffusion

Transformers (DiTs), inspired by

Vision Transformer (ViT) principles,

process spatial inputs as

sequences of patches.

[455]

æ‰©æ•£

å˜å‹å™¨ï¼ˆDITï¼‰ï¼Œçµæ„Ÿ

æ¥è‡ªè§†è§‰å˜

å‹å™¨ï¼ˆVITï¼‰åŸç†ï¼Œè¿‡

ç¨‹ç©ºé—´è¾“å…¥

ä½œä¸ºæ–‘å—åº

åˆ—ã€‚



--------------------------------------------------

[456] The DiT-XL model

features 28 transformer layers,

a hidden size of

1152, 16 attention heads,

and a 2 Ã—

2 patch size.

[456]

DIT-XLå‹å·å…·æœ‰

28ä¸ªå˜å‹å™¨å±‚

ï¼Œéšè—å°ºå¯¸ä¸º

1152ã€16ä¸ªæ³¨æ„åŠ›å¤´

å’Œ2Ã—2ä¸ªè´´ç‰‡å¤§

å°ã€‚



--------------------------------------------------

[457] It employs adaptive

layer normalization (AdaLN) to

improve training stability, comprising

675 million parameters and

trained for 1400 epochs.

[457] å®ƒé‡‡ç”¨è‡ª

é€‚åº”å±‚å½’ä¸€

åŒ–ï¼ˆADALNï¼‰æ¥æé«˜è®­

ç»ƒç¨³å®šæ€§ï¼ŒåŒ…

æ‹¬6.75äº¿ä¸ªå‚æ•°

ï¼Œå¹¶æ¥å—äº†1400ä¸ª

æ—¶æœŸçš„åŸ¹è®­

ã€‚



--------------------------------------------------

[458] Masked Autoregressive models

(MARs) are diffusion transformer

variants tailored for au-

toregressive image generation.

[458]

è’™é¢è‡ªå›æ—‹

æ¨¡å‹ï¼ˆMARSï¼‰æ˜¯é’ˆå¯¹

auä¾µèš€å›¾åƒç”Ÿ

æˆçš„æ‰©æ•£å˜

å‹å™¨å˜ç§ã€‚



--------------------------------------------------

[459] They utilize a

continuous- valued diffusion loss

framework to generate high-quality

outputs without discrete tokenization.

[459] ä»–

ä»¬åˆ©ç”¨ä¸€ä¸ª

è¿ç»­çš„æ‰©æ•£

æŸå¤±æ¡†æ¶æ¥

ç”Ÿæˆé«˜è´¨é‡

çš„è¾“å‡ºè€Œæ— 

éœ€ç¦»æ•£çš„ä»¤

ç‰ŒåŒ–ã€‚



--------------------------------------------------

[460] The MAR-Large model

includes 32 transformer layers,

a hidden size of

1024, 16 attention heads,

and bidirectional attention.

[460]

MAR-LARGEæ¨¡å‹åŒ…

æ‹¬32ä¸ªå˜å‹å™¨

å±‚ï¼Œéšè—å¤§å°

ä¸º1024ï¼Œ16ä¸ªæ³¨æ„åŠ›

å¤´å’ŒåŒå‘æ³¨

æ„ã€‚



--------------------------------------------------

[461] Like DiT, it

incorporates AdaLN for stable

training and effective to-

ken modeling, with 479

million parameters trained over

400 epochs.

[461] åƒDITä¸€æ ·ï¼Œå®ƒ

å°†Adalnçº³å…¥äº†ç¨³

å®šçš„è®­ç»ƒå’Œ

æœ‰æ•ˆçš„å»ºæ¨¡

ï¼Œå…¶ä¸­4.79äº¿ä¸ªå‚

æ•°è®­ç»ƒäº†400å¤š

ä¸ªæ—¶æœŸã€‚

--------------------------------------------------



[462] Finally,

Scalable Interpolant Transformers (SiTs)

extend the DiT framework

by introducing a flow-based

in- terpolant methodology, enabling

more flexible bridging be-

tween data and noise

distributions.

[462] æœ€å

ï¼Œå¯æ‰©å±•çš„æ’

å€¼æ’å…¥å˜å‹

å™¨ï¼ˆSITSï¼‰é€šè¿‡å¼•å…¥

åŸºäºæµåŠ¨çš„

æ„é€ æ–¹æ³•æ¥

æ‰©å±•DITæ¡†æ¶ï¼Œä»

è€Œåœ¨æ•°æ®å’Œ

å™ªå£°åˆ†å¸ƒä¹‹

é—´å®ç°äº†æ›´

çµæ´»çš„æ¡¥æ¥

ã€‚

--------------------------------------------------



[463] While

architecturally identical to DiT-XL,

the SiT-XL model leverages

this inter- polant approach

to facilitate modular experimentation

with interpolant selection and

sampling dynamics.

[463] å°½ç®¡åœ¨æ¶æ„

ä¸Šä¸DIT-XLç›¸åŒï¼Œä½†

SIT-XLæ¨¡å‹åˆ©ç”¨äº†

è¿™ç§é—´é—´æ–¹

æ³•æ¥ä¿ƒè¿›ä½¿

ç”¨æ’å€¼é€‰æ‹©

å’Œé‡‡æ ·åŠ¨åŠ›

å­¦çš„æ¨¡å—åŒ–

å®éªŒã€‚

--------------------------------------------------



[464] Datasets.

[464] æ•°æ®é›†

ã€‚



--------------------------------------------------

[465] We prepared the

ImageNet 256 Ã— 256

dataset by applying center

cropping and adaptive resizing

to main- tain the

original aspect ratio and

minimize distortion.

[465] æˆ‘ä»¬é€šè¿‡æ–½

åŠ ä¸­å¿ƒè£å‰ª

å’Œé€‚åº”æ€§è°ƒ

æ•´å¤§å°ä»¥ä½¿

åŸå§‹é•¿å®½æ¯”

å¹¶æœ€å¤§ç¨‹åº¦

åœ°å‡å°‘å¤±çœŸ

æ¥å‡†å¤‡Imagenet

256Ã—256æ•°æ®

é›†ã€‚



--------------------------------------------------

[466] The images were

then normalized to a

mean of 0.5 and

a stan- dard deviation

of 0.5.

[466] ç„¶åå°†å›¾

åƒæ ‡å‡†åŒ–ä¸º

å¹³å‡0.5ï¼Œæ ‡å‡†å

å·®ä¸º0.5ã€‚

--------------------------------------------------



[467] To

augment the dataset, we

applied random horizontal flipping

with a probability of

0.5.

[467] ä¸ºäº†å¢

åŠ æ•°æ®é›†ï¼Œæˆ‘

ä»¬ä»¥0.5çš„æ¦‚ç‡

åº”ç”¨éšæœºæ°´

å¹³ç¿»è½¬ã€‚

--------------------------------------------------



[468] To

accelerate training without using

Variational Autoencoder (VAE), we

pre-extracted features from the

images using a pre-trained

VAE.

[468] ä¸ºäº†

åŠ é€Ÿè®­ç»ƒè€Œ

æ— éœ€ä½¿ç”¨å„

ç§è‡ªåŠ¨ç¼–ç 

å™¨ï¼ˆVAEï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨

é¢„å…ˆè®­ç»ƒçš„

VAEä»å›¾åƒä¸­é¢„

å…ˆæå–ç‰¹å¾

ã€‚

--------------------------------------------------



[469] The

images were mapped to

their latent representations, normalized,

and the resulting feature

arrays were saved for

direct use during training.

[469] å°†å›¾åƒæ˜ å°„

åˆ°å…¶æ½œåœ¨è¡¨

ç¤ºï¼Œå¹¶å½’ä¸€åŒ–

ï¼Œå¹¶ä¿å­˜æ‰€å¾—

çš„åŠŸèƒ½é˜µåˆ—

åœ¨è®­ç»ƒè¿‡ç¨‹

ä¸­ç›´æ¥ä½¿ç”¨

ã€‚



--------------------------------------------------

[470] Training Details The

training process began with

obtain- ing pruned models

using the proposed learnable

pruning method as illustrated

in Figure 12.

[470]

åŸ¹è®­ç»†èŠ‚è®­

ç»ƒè¿‡ç¨‹å§‹äº

ä½¿ç”¨æ‰€æå‡º

çš„å¯å­¦ä¹ ä¿®

å‰ªæ–¹æ³•è·å¾—

ä¿®å‰ªæ¨¡å‹ï¼Œå¦‚

å›¾12æ‰€ç¤ºã€‚



--------------------------------------------------

[471] Pruning decisions were

made by a joint

optimization of pruning and

weight updates through LoRA

with a block size.

[471] ä¿®å‰ª

å†³å®šæ˜¯é€šè¿‡

é€šè¿‡å—å¤§å°

çš„æ´›æ‹‰ï¼ˆLoraï¼‰è¿›è¡Œ

ä¿®å‰ªå’Œé‡é‡

æ›´æ–°çš„è”åˆ

ä¼˜åŒ–åšå‡ºçš„

ã€‚



--------------------------------------------------

[472] In practice, the

block size is 2

for simplicity and the

models were trained for

100 epochs, except for

MAR, which was trained

for 40 epochs.

[472]

å®é™…ä¸Šï¼Œé™¤äº†

MARå¤–ï¼Œè¿˜ä¸º100ä¸ªæ—¶

æœŸçš„è®­ç»ƒäº†

å—å¤§å°ä¸º2ï¼Œå¹¶

ä¸”å¯¹æ¨¡å‹è¿›

è¡Œäº†100ä¸ªæ—¶æœŸ

çš„è®­ç»ƒï¼Œè€ŒMarè¿›

è¡Œäº†40ä¸ªæ—¶æœŸ

çš„è®­ç»ƒã€‚



--------------------------------------------------

[473] To enhance post-pruning

performance, the Masked Knowl-

edge Distillation (RepKD) method

was employed during the

recovery phase to transfer

knowledge from teacher mod-

0 2000 4000 6000

8000 10000 Train iterations

0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 23

24 25 26 27

Layer Index in DiT-XL

Figure 9.

[473] ä¸ºäº†

æé«˜çŒæœ¨å

çš„æ€§èƒ½ï¼Œåœ¨æ¢

å¤é˜¶æ®µé‡‡ç”¨

äº†æ©ç›–çš„çŸ¥

è¯†è’¸é¦ï¼ˆREPKDï¼‰æ–¹æ³•

ï¼Œä»¥ä»æ•™å¸ˆmod-

0 2000 4000 4000

4000 4000 8000 10000ç«

è½¦è¿­ä»£è½¬ç§»

çŸ¥è¯†0

1 2 3 4

5 6 5 6

7 8 9 10

11 12 13 13

13 14 15 15

15 17 17 17

17 19 20 22

22 22 22 22

22 22 22 22

22 22 27 27

27 27 27 27å±‚åœ¨Dit-XLå›¾

9ä¸­ã€‚

--------------------------------------------------



[474] 1:2

Pruning Decisions 0 2000

4000 6000 8000 10000

Train iterations 0 1

2 3 4 5

6 7 8 9

10 11 12 13

14 15 16 17

18 19 20 21

22 23 24 25

26 27 Layer Index

in DiT-XL Figure 10.

[474] 1ï¼š2ä¿®å‰ªå†³ç­–

0 2000 4000

6000 8000 10000ç«è½¦è¿­ä»£0 1

2 3 4 5

6 7 8 9

10 11 12 13

13 14 15 16

17 18 19 20

21 22 23 23

24 25 26 27

27 DIT-XLä¸­

çš„å±‚æŒ‡æ•°å›¾

10ã€‚



--------------------------------------------------

[475] 2:4 Pruning Decisions

0 2000 4000 6000

8000 10000 Train iterations

0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 23

24 25 26 27

Layer Index in DiT-XL

Figure 11.

[475] 2ï¼š4ä¿®å‰ªå†³ç­–0

2000 4000 6000 8000

10000ç«

è½¦è¿­ä»£0 1 2 3

4 5 6 7

8 9 10 11

12 13 13 14

15 16 17 18

19 20 21 22

23 23 24 25

26 27 27 DIT-XLä¸­çš„

å±‚æŒ‡æ•°å›¾11ã€‚

--------------------------------------------------



[476] 7:14

Pruning Decisions els to

pruned student models.

[476]

7:14ä¿®

å‰ªå†³ç­–å¯¹ä¿®

å‰ªçš„å­¦ç”Ÿæ¨¡

å‹ã€‚



--------------------------------------------------

[477] The RepKD approach

aligns the output predictions

and intermediate hidden states

of the pruned and

teacher models, with further

details provided in the

following section.

[477] REPKDæ–¹æ³•å°†æ”¾

ç½®é¢„æµ‹å’Œä¿®

å‰ªå’Œæ•™å¸ˆæ¨¡

å‹çš„ä¸­é—´éš

è—çŠ¶æ€ä¿æŒ

ä¸€è‡´ï¼Œå¹¶åœ¨ä¸‹

ä¸€èŠ‚ä¸­æä¾›

äº†æ›´å¤šç»†èŠ‚

ã€‚

--------------------------------------------------



[478] Additionally,

as Exponential Mov- ing

Averages (EMA) are updated

and used during image

generation, an excessively small

learning rate can weaken

EMAâ€™s effect, leading to

suboptimal outcomes.

[478] æ­¤å¤–ï¼Œéšç€æŒ‡

æ•°ç§»åŠ¨å¹³å‡

å€¼ï¼ˆEMAï¼‰åœ¨å›¾åƒç”Ÿ

æˆè¿‡ç¨‹ä¸­è¿›

è¡Œäº†æ›´æ–°å’Œ

ä½¿ç”¨ï¼Œå› æ­¤å­¦

ä¹ ç‡è¿‡å¤šä¼š

å‰Šå¼±EMAçš„æ•ˆæœ

ï¼Œä»è€Œå¯¼è‡´æ¬¡

ä¼˜ç»“æœã€‚

--------------------------------------------------



[479] To

address this, a progressive

learning rate scheduler was

implemented to gradually halve

the learning rate throughout

training.

[479] ä¸ºäº†

è§£å†³è¿™ä¸ªé—®

é¢˜ï¼Œå®æ–½äº†æ¸

è¿›å¼å­¦ä¹ ç‡

è°ƒåº¦ç¨‹åºï¼Œä»¥

é€æ­¥å°†æ•´ä¸ª

åŸ¹è®­çš„å­¦ä¹ 

ç‡é€æ¸å‡åŠ

ã€‚

--------------------------------------------------



[480] The

1

[480] 1

--------------------------------------------------



[481] Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Recoverability

Estimation Local Block Joint

Opt.

[481] å˜å‹å™¨å±‚å˜

å‹å™¨å±‚å˜å‹

å™¨å±‚å˜å‹å™¨

å±‚å˜å‹å™¨å±‚

å˜å‹å™¨å±‚å±‚

å˜å‹å™¨å±‚æ¢

å¤æ€§ä¼°è®¡å±€

éƒ¨å—æ¥å¤´é€‰

æ‹©ã€‚

--------------------------------------------------



[482] ğ’ğ¡ğšğ«ğğ

ğš«ğš½  (LoRA/Full) ğš½

Diff.

[482] ğ’ğ¡ğšğ«ğğï¼ˆlora/fullï¼‰ğš½å·®å¼‚ã€‚

--------------------------------------------------



[483] Sampling

Transformer Layer Transformer Layer

Winner Decision ğ¦ğ¢ğ§ğ“›(ğš½+ ğš«ğš½)

Update Update Categorical Distribution

~ Transformer Layer Transformer

Layer Differentiable Sampling of

Candidate Solutions Figure 12.

[483] é‡‡æ ·

å˜å‹å™¨å±‚å˜

å‹å™¨å±‚è·å¥–

è€…å†³ç­–ğ¦ğ¢ğ§ğ“›ï¼ˆğš½+ ğš«ğš½ï¼‰æ›´æ–°

æ›´æ–°åˆ†ç±»åˆ†

å¸ƒã€œå˜å‹å™¨å±‚

å˜å‹å™¨å±‚çš„

å€™é€‰è§£å†³æ–¹

æ¡ˆå¯åŒºåˆ†é‡‡

æ ·å›¾12ã€‚

--------------------------------------------------



[484] Learnable

depth pruning on a

local block Transformer Block

Transformer Block Transformer Block

Transformer Block Learning the

optimal sub-layers Transformer Block

Transformer Block DiT TinyDiT

Masked Distillation Massive Activation

( ğ‘¥> ğ‘˜â‹…ğœğ‘¥)

mask mask Hidden States

Hidden States Figure 13.

[484] å¯å­¦ä¹ 

çš„æ·±åº¦ä¿®å‰ª

åœ¨æœ¬åœ°å—å˜

å‹å™¨å—å˜å‹

å™¨å—å˜å‹å™¨

å—å˜å‹å™¨å—

ä¸­å­¦ä¹ æœ€ä½³

å­å±‚å˜å‹å™¨

å—å˜å‹å™¨å—

dit tinydit tinyditæ©ç›–è’¸é¦å¤§

é‡æ¿€æ´»ï¼ˆğ‘¥>ğ‘¥>ğ‘˜Â·åº‡æŠ¤

ï¼‰æ©ç æ©ç éš

è—çŠ¶æ€éšè—

çŠ¶æ€å›¾13ã€‚

--------------------------------------------------



[485] Masked

knowledge distillation with 2:4

blocks.

[485] å¸¦æœ‰

2ï¼š4å—çš„æ©ç›–çŸ¥

è¯†è’¸é¦ã€‚

--------------------------------------------------



[486] details

of each hyperparameter are

provided in Table 6.

[486] è¡¨6ä¸­

æä¾›äº†æ¯ä¸ª

è¶…å‚æ•°çš„è¯¦

ç»†ä¿¡æ¯ã€‚



--------------------------------------------------

[487] 7.

[487] 7ã€‚

--------------------------------------------------



[488] Visualization

of Pruning Decisions Figures

9, 10 and 11

visualize the dynamics of

pruning de- cisions during

training for the 1:2,

2:4, and 7:14 pruning

schemes.

[488] ä¿®å‰ª

å†³ç­–çš„å¯è§†

åŒ–å›¾9ã€10å’Œ11å¯è§†

åŒ–åœ¨1ï¼š2ã€2ï¼š2ï¼š4å’Œ7:14ä¿®å‰ª

æ–¹æ¡ˆçš„è®­ç»ƒ

è¿‡ç¨‹ä¸­ä¿®å‰ª

æ•ˆæœçš„åŠ¨åŠ›

å­¦ã€‚

--------------------------------------------------



[489] Different

divisions lead to varying

search spaces, which in

turn result in various

solutions.

[489] ä¸åŒçš„åˆ’

åˆ†å¯¼è‡´æœç´¢

ç©ºé—´ä¸åŒï¼Œè¿™

åˆå¯¼è‡´å„ç§

è§£å†³æ–¹æ¡ˆã€‚

--------------------------------------------------



[490] For

both the 1:2 and

2:4 schemes, good decisions

can be learned in

only one epoch, while

the 7:14 scheme encounters

optimization diffi- culty.

[490]

å¯¹

äº1ï¼š2å’Œ2ï¼š4æ–¹æ¡ˆï¼Œåª

èƒ½åœ¨ä¸€ä¸ªæ—¶

ä»£ä¸­å­¦ä¹ å¥½

çš„å†³å®šï¼Œè€Œ7:14æ–¹

æ¡ˆé‡åˆ°ä¼˜åŒ–

çš„æ„æˆã€‚



--------------------------------------------------

[491] This is due

to the  14 7

 =3,432 candidates, which

is too huge and

thus cannot be adequately

sampled within a single

epoch.

[491] è¿™æ˜¯

ç”±äº14 7

= 3,432ä¸ªå€™é€‰

äººï¼Œè¿™å¤ªå¤§äº†

ï¼Œå› æ­¤æ— æ³•åœ¨

å•ä¸ªæ—¶æœŸå†…

è¿›è¡Œå……åˆ†é‡‡

æ ·ã€‚



--------------------------------------------------

[492] Therefore, in practical

applications, we use the

1:2 or 2:4 schemes

for learnable layer pruning.

[492] å› æ­¤ï¼Œåœ¨å®

é™…åº”ç”¨ä¸­ï¼Œæˆ‘

ä»¬ä½¿ç”¨1ï¼š2æˆ–2ï¼š4çš„

æ–¹æ¡ˆè¿›è¡Œä¿®

å‰ªã€‚



--------------------------------------------------

[493] 8.

[493] 8ã€‚

--------------------------------------------------



[494] Details

of Masked Knowledge Distillation

Training Loss.

[494] æ©ç›–çŸ¥è¯†

è’¸é¦åŸ¹è®­æŸ

å¤±çš„ç»†èŠ‚ã€‚

--------------------------------------------------



[495] This

work deploys a standard

knowledge distillation to learn

a good student model

by mimicking the pre-trained

teacher.

[495] è¿™

é¡¹å·¥ä½œé€šè¿‡

æ¨¡ä»¿é¢„è®­ç»ƒ

çš„è€å¸ˆæ¥éƒ¨

ç½²æ ‡å‡†çŸ¥è¯†

è’¸é¦æ¥å­¦ä¹ 

ä¸€ä¸ªå¥½çš„å­¦

ç”Ÿæ¨¡å‹ã€‚

--------------------------------------------------



[496] The

loss function is formalized

as: L = Î±KD

Â· LKD + Î±Diff

Â· LDiff + Î²

Â· LRep (8) Here,

LKD denotes the Mean

Squared Error between the

outputs of the student

and teacher models.

[496]

æŸè€—

å‡½æ•°è¢«å½¢å¼

åŒ–ä¸ºï¼šl =Î±kdÂ·lkd +Î±diffÂ·ldiff +Î²Â·lrepï¼ˆ8ï¼‰ï¼ŒLKDè¡¨ç¤ºå­¦

ç”Ÿå’Œæ•™å¸ˆæ¨¡

å‹çš„è¾“å‡ºä¹‹

é—´çš„å¹³å‡å¹³

æ–¹è¯¯å·®ã€‚

--------------------------------------------------



[497] LDiff

repre- sents the original

pre-training loss function.

[497]

LDIFFä»£è¡¨

åŸå§‹çš„é¢„è®­

ç»ƒæŸå¤±å‡½æ•°

ã€‚



--------------------------------------------------

[498] Finally, LRep corresponds

to the masked distillation

loss applied to the

hidden states, as illustrated

in Figure 13, which

encourages alignment between the

intermediate representations of the

pruned model and the

original model.

[498] æœ€åï¼ŒLREPå¯¹åº”äº

åº”ç”¨äºéšè—

çŠ¶æ€çš„æ©ç›–

è’¸é¦æŸå¤±ï¼Œå¦‚

å›¾13æ‰€ç¤ºï¼Œè¿™é¼“

åŠ±äº†ä¿®å‰ªæ¨¡

å‹çš„ä¸­é—´è¡¨

ç¤ºä¸åŸå§‹æ¨¡

å‹ä¹‹é—´çš„ä¸€

è‡´æ€§ã€‚

--------------------------------------------------



[499] The

corresponding hyperparameters Î±KD, Î±Diff

and Î±Rep can be

found in Ta- ble

6.

[499] ç›¸åº”çš„

è¶…å‚æ•°Î±KDï¼ŒÎ±DIFFå’ŒÎ±REPå¯

ä»¥åœ¨Table 6ä¸­æ‰¾åˆ°

ã€‚

--------------------------------------------------



[500] Hidden

State Alignment.

[500] éšè—çš„çŠ¶æ€

å¯¹é½ã€‚

--------------------------------------------------



[501] The

masked distillation loss LRep

is critical for aligning

the intermediate representations of

the student and teacher

models.

[501] æ©ç›–çš„

è’¸é¦æŸå¤±LREPå¯¹

äºä½¿å­¦ç”Ÿå’Œ

æ•™å¸ˆæ¨¡å‹çš„

ä¸­é—´è¡¨ç¤ºè‡³

å…³é‡è¦ã€‚

--------------------------------------------------



[502] During

the recovery phase, each

layer of the student

model is designed to

repli- cate the output

hidden states of a

corresponding two-layer local block

from the teacher model.

[502] åœ¨æ¢

å¤é˜¶æ®µï¼Œå­¦ç”Ÿ

æ¨¡å‹çš„æ¯ä¸ª

å±‚éƒ½æ—¨åœ¨ä»

æ•™å¸ˆæ¨¡å‹ä¸­

è¡¥å……ç›¸åº”ä¸¤

å±‚æœ¬åœ°å—çš„

è¾“å‡ºçŠ¶æ€ã€‚



--------------------------------------------------

[503] Depth pruning does

not alter the internal

dimensions of the layers,

enabling direct alignment without

additional projection layers.

[503]

æ·±

åº¦ä¿®å‰ªä¸ä¼š

æ”¹å˜å±‚çš„å†…

éƒ¨ç»´åº¦ï¼Œä»è€Œ

æ— éœ€å…¶ä»–æŠ•

å½±å±‚å³å¯ç›´

æ¥å¯¹é½ã€‚



--------------------------------------------------

[504] For mod- els

such as SiTs, where

hidden state losses are

more pro- nounced due

to their unique interpolant-based

architecture, a smaller coefficient

Î² is applied to

LRep to mitigate poten-

tial training instability.

[504]

å¯¹äº

è¯¸å¦‚SITSä¹‹ç±»çš„

æ¨¡å—åŒ–ï¼Œç”±äº

å…¶ç‹¬ç‰¹çš„åŸº

äºæ’å…¥å¼çš„

æ¶æ„ï¼Œéšè—çš„

çŠ¶æ€æŸå¤±æ›´

åŠ å‘ˆç°ï¼Œå› æ­¤

å°†è¾ƒå°çš„ç³»

æ•°Î²åº”ç”¨äºLREPï¼Œä»¥

å‡è½»æ½œåœ¨çš„

è®­ç»ƒä¸ç¨³å®š

æ€§ã€‚



--------------------------------------------------

[505] The gradual decrease

in Î² through- out

training further reduces the

risk of negative impacts

on convergence.

[505] Î²è·¨è®­ç»ƒçš„

é€æ¸å‡å°‘è¿›

ä¸€æ­¥é™ä½äº†

å¯¹æ”¶æ•›çš„è´Ÿ

é¢å½±å“çš„é£

é™©ã€‚

--------------------------------------------------



[506] Iterative

Pruning and Distillation.

[506]

è¿­ä»£ä¿®å‰ª

å’Œè’¸é¦ã€‚



--------------------------------------------------

[507] Table 7 assesses

the effectiveness of iterative

pruning and teacher selection

strategies.

[507] è¡¨7è¯„

ä¼°äº†è¿­ä»£ä¿®

å‰ªå’Œæ•™å¸ˆé€‰

æ‹©ç­–ç•¥çš„æœ‰

æ•ˆæ€§ã€‚

--------------------------------------------------



[508] To

obtain a TinyDiT-D7, we

can either directly prune

a DiT-XL with 28

layers or craft a

TinyDiT-D14 first and then

iteratively produce the small

models.

[508] è¦è·å¾—

TinyDit-D7ï¼Œæˆ‘ä»¬å¯ä»¥ç›´

æ¥ä¿®å‰ªå¸¦æœ‰

28å±‚çš„DIT-XLï¼Œæˆ–è€…å…ˆ

åˆ¶ä½œTinyDit-D14ï¼Œç„¶åè¿­

ä»£äº§ç”Ÿå°å‹

æ¨¡å‹ã€‚

--------------------------------------------------



[509] To

investi- gate the impact

of teacher choice and

the method for obtain-

ing the initial weights

of the student model,

we derived the initial

weights of TinyDiT-D7 by

pruning both a pre-trained

model and a crafted

intermediate model.

[509] ä¸ºäº†æŠ•

èµ„æ•™å¸ˆé€‰æ‹©

çš„å½±å“ä»¥åŠ

è·å¾—å­¦ç”Ÿæ¨¡

å‹çš„åˆå§‹æƒ

é‡çš„æ–¹æ³•ï¼Œæˆ‘

ä»¬é€šè¿‡ä¿®å‰ª

é¢„è®­ç»ƒçš„æ¨¡

å‹å’Œåˆ¶ä½œçš„

ä¸­é—´æ¨¡å‹æ¥

å¾—å‡ºTinyDit-D7çš„åˆå§‹

æƒé‡ã€‚

--------------------------------------------------



[510] Subsequently,

we used both the

trained and crafted models

as teachers for the

pruned student models.

[510]

éšåï¼Œæˆ‘

ä»¬å°†è®­ç»ƒæœ‰

ç´ å’Œåˆ¶ä½œçš„

æ¨¡å‹éƒ½ç”¨ä½œ

ä¿®å‰ªçš„å­¦ç”Ÿ

æ¨¡å‹çš„è€å¸ˆ

ã€‚



--------------------------------------------------

[511] Across four experimental

set- tings, pruning and

distilling using the crafted

intermedi- ate model yielded

the best performance.

[511]

åœ¨å››ä¸ªå®éªŒ

æ€§è®¾ç½®ä¸­ï¼Œä½¿

ç”¨ç²¾å¿ƒè®¾è®¡

çš„ä¸­ä»‹æ¨¡å‹

è¿›è¡Œä¿®å‰ªå’Œ

è’¸é¦äº§ç”Ÿäº†

æœ€ä½³æ€§èƒ½ã€‚



--------------------------------------------------

[512] Notably, models pruned

from the crafted model

outperformed those pruned from

the pre-trained model regardless

of the teacher model

employed in the distillation

process.

[512] å€¼

å¾—æ³¨æ„çš„æ˜¯

ï¼Œä»åˆ¶ä½œæ¨¡å‹

ä¸­ä¿®å‰ªçš„æ¨¡

å‹ä¼˜äºä»é¢„

è®­ç»ƒæ¨¡å‹ä¸­

ä¿®å‰ªçš„æ¨¡å‹

ï¼Œè€Œä¸ç®¡è’¸é¦

è¿‡ç¨‹ä¸­ä½¿ç”¨

çš„æ•™å¸ˆæ¨¡å‹

å¦‚ä½•ã€‚

--------------------------------------------------



[513] We

attribute this su- 2

[513] æˆ‘ä»¬å½’

å› äºè¿™ä¸ªsu-2



--------------------------------------------------

[514] Model Optimizer Cosine

Sched.

[514] æ¨¡

å‹ä¼˜åŒ–å™¨ä½™

å¼¦è®¡åˆ’ã€‚

--------------------------------------------------



[515] Teacher

Î±KD Î±GT Î² Grad.

[515] è€å¸ˆ

AKD AGT Bæ¯•ä¸šã€‚

--------------------------------------------------



[516] Clip

Pruning Configs DiT-D19 AdamW(lr=2e-4,

wd=0.0) Î·min = 1e-4

DiT-XL 0.9 0.1 1e-2

â†’0 1.0 LoRA-1:2 DiT-D14

AdamW(lr=2e-4, wd=0.0 Î·min =

1e-4 DiT-XL 0.9 0.1

1e-2 â†’0 1.0 LoRA-1:2

DiT-D7 AdamW(lr=2e-4, wd=0.0) Î·min

= 1e-4 DiT-D14 0.9

0.1 1e-2 â†’0 1.0

LoRA-1:2 SiT-D14 AdamW(lr=2e-4, wd=0.0)

Î·min = 1e-4 SiT-XL

0.9 0.1 2e-4 â†’0

1.0 LoRA-1:2 MAR-D16 AdamW(lr=2e-4,

wd=0.0) Î·min = 1e-4

MAR-Large 0.9 0.1 1e-2

â†’0 1.0 LoRA-1:2 Table

6.

[516] å‰ªè¾‘ä¿®

å‰ªé…ç½®DIT-D19 ADAMWï¼ˆLR

= 2E-4ï¼ŒWD = 0.0ï¼‰Î·min=

1E-4 DIT-XL 0.9 0.9

0.1 1E-2â†’0 1.0 Lora-1ï¼š2

DIT-DIT-D14 ADAMWï¼ˆLR = 2E-4ï¼ŒWD

= 2e-4ï¼ŒWD = 0.0Î·min=

0.0Î·min= 1e-4 dit-4 Dit-4

Dit-4 Dit-4 Dit-4 Dit-4

Dit-4 Dit-4 Dit-loror lor

lor lorã€‚ DIT-D7 ADAMWï¼ˆLR

= 2E-4ï¼ŒWD = 0.0ï¼‰Î·min=

1e-4 Dit-D14 0.9 0.9

0.1 1E-2â†’0 1.0 Lora-1ï¼š2

Sit-D14 Adamwï¼ˆLR = 2E-4ï¼ŒWD

= 0.0ï¼‰ Adamwï¼ˆLR =

2E-4ï¼ŒWD = 0.0ï¼‰Î·min= 1E-4

Mar-large 0.9 0.1 1E-2â†’0

1.0 Lora-1ï¼š2è¡¨6ã€‚



--------------------------------------------------

[517] Training details and

hyper-parameters for mask training

Teacher Model Pruned From

IS FID sFID Prec.

[517] åŸ¹

è®­ç»†èŠ‚å’Œè¶…

å‚æ•°ç”¨äºæ©

ç›–åŸ¹è®­çš„æ•™

å¸ˆæ¨¡å‹ï¼Œæ˜¯FID SFID PRECã€‚

--------------------------------------------------



[518] Recall

DiT-XL/2 DiT-XL/2 29.46 56.18

26.03 0.43 0.51 DiT-XL/2

TinyDiT-D14 51.96 36.69 28.28

0.53 0.59 TinyDiT-D14 DiT-XL/2

28.30 58.73 29.53 0.41

0.50 TinyDiT-D14 TinyDiT-D14 57.97

32.47 26.05 0.55 0.60

Table 7.

[518] Recall

DiT-XL/2 DiT-XL/2 29.46 56.18

26.03 0.43 0.51 DiT-XL/2

Tâ€‹â€‹inyDiT-D14 51.96 36.69 28.28

0.53 0.59 TinyDiT-D14 DiT-XL/2

28.30 58.73 29.53 0.41

0.50 TinyDiT-D14 TinyDiT-D14 57.97

32.47 26.05 0.55 0.60è¡¨

7ã€‚

--------------------------------------------------



[519] TinyDiT-D7

is pruned and distilled

with different teacher models

for 10k, sample steps

is 64, original weights

are used for sampling

rather than EMA.

[519]

TinyDit-D7ç”¨ä¸åŒçš„æ•™

å¸ˆæ¨¡å‹è¿›è¡Œ

ä¿®å‰ªå’Œè’¸é¦

ï¼Œç”¨äº10Kï¼Œæ ·æœ¬æ­¥

éª¤ä¸º64ï¼ŒåŸå§‹æƒ

é‡ç”¨äºé‡‡æ ·

è€Œä¸æ˜¯EMAã€‚



--------------------------------------------------

[520] 100 200 300

400 500 Steps 3.0

3.5 4.0 4.5 5.0

5.5 FID Masked KD

Finetune DiT-L/2 Scratch Figure

14.

[520] 100 200

300 400 500æ­¥3.0 3.5

4.0 4.5 5.0 5.0

5.5 FIDè’™

ç‰ˆKD Finetune Dit-L/2åˆ®æ“¦å›¾14ã€‚

--------------------------------------------------



[521] FID

and training steps.

[521]

FIDå’Œ

åŸ¹è®­æ­¥éª¤ã€‚



--------------------------------------------------

[522] perior performance to

two factors: first, the

crafted modelâ€™s structure is

better adapted to knowledge

distillation since it was

trained using a distillation

method; second, the reduced

search space facilitates finding

a more favorable initial

state for the student

model.

[522] å¯¹

ä¸¤ä¸ªå› ç´ çš„

ç»Ÿæ²»æ€§èƒ½ï¼šé¦–

å…ˆï¼Œåˆ¶ä½œçš„æ¨¡

å‹çš„ç»“æ„æ›´

å¥½åœ°é€‚åº”äº†

çŸ¥è¯†è’¸é¦ï¼Œå› 

ä¸ºå®ƒæ˜¯ä½¿ç”¨

è’¸é¦æ–¹æ³•è®­

ç»ƒçš„ï¼›å…¶æ¬¡ï¼Œå‡

å°‘çš„æœç´¢ç©º

é—´æœ‰åŠ©äºä¸º

å­¦ç”Ÿæ¨¡å‹æ‰¾

åˆ°æ›´æœ‰åˆ©çš„

åˆå§‹çŠ¶æ€ã€‚

--------------------------------------------------



[523] 9.

[523] 9ã€‚



--------------------------------------------------

[524] Analytical Experiments Training

Strategies Figure 14 illustrates

the effective- ness of

standard fine-tuning and knowledge

distillation (KD), where we

prune DiT-XL to 14

layers and then ap-

ply various fine-tuning methods.

[524] åˆ†

æå®éªŒåŸ¹è®­

ç­–ç•¥å›¾14è¯´æ˜

äº†æ ‡å‡†å¾®è°ƒ

å’ŒçŸ¥è¯†è’¸é¦

ï¼ˆKDï¼‰çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘

ä»¬åœ¨å…¶ä¸­å°†

DIT-XLé™è‡³14å±‚ï¼Œç„¶å

å°†å„ç§å¾®è°ƒ

æ–¹æ³•é™è‡³14å±‚

ã€‚



--------------------------------------------------

[525] Figure 3 presents

the FID scores across

100K to 500K steps.

[525] å›¾3æ˜¾ç¤ºäº†åœ¨

100kè‡³500kæ­¥é•¿çš„FIDå¾—

åˆ†ã€‚



--------------------------------------------------

[526] It is evident

that the standard fine-tuning

method allows TinyDiT-D14 to

achieve performance comparable to

DiT-L while offering faster

in- ference.

[526] æ˜¾ç„¶ï¼Œæ ‡å‡†

çš„å¾®è°ƒæ–¹æ³•

å…è®¸TinyDit-D14å®ç°ä¸

DIT-Lç›¸å½“çš„æ€§èƒ½

ï¼ŒåŒæ—¶æä¾›æ›´

å¿«çš„ä¿¡æ¯ã€‚

--------------------------------------------------



[527] Additionally,

we confirm the significant

effective- ness of distillation,

which enables the model

to surpass DiT- L

at just 100K steps

and achieve better FID

scores than the 500K

standard fine-tuned TinyDiT-D14.

[527]

æ­¤

å¤–ï¼Œæˆ‘ä»¬ç¡®è®¤

äº†è’¸é¦çš„æ˜¾

ç€æœ‰æ•ˆæ€§ï¼Œè¿™

ä½¿è¯¥æ¨¡å‹èƒ½

å¤Ÿåœ¨ä»…100kæ­¥éª¤

ä¸­è¶…è¿‡Dit-Lï¼Œå¹¶ä¸”

æ¯”500Kæ ‡å‡†çš„å¾®

å‹TinyDit-D14è·å¾—äº†æ›´

å¥½çš„FIDå¾—åˆ†ã€‚



--------------------------------------------------

[528] This is because

the distillation of hidden

layers provides stronger supervision.

[528] è¿™

æ˜¯å› ä¸ºéšè—

å±‚çš„è’¸é¦æ

ä¾›äº†æ›´å¼ºçš„

ç›‘ç£ã€‚



--------------------------------------------------

[529] Further increasing the

training steps to 500K

leads to sig- nificantly

better results.

[529] è¿›ä¸€æ­¥

å°†è®­ç»ƒæ­¥éª¤

æé«˜åˆ°500Kï¼Œä»è€Œ

å–å¾—äº†æ›´å¥½

çš„ç»“æœã€‚

--------------------------------------------------



[530] Learning

Rate IS FID sFID

Prec.

[530] å­¦ä¹ 

ç‡æ˜¯FID SFID

PRECã€‚



--------------------------------------------------

[531] Recall lr=2e-4 207.27

3.73 5.04 0.8127 0.5401

lr=1e-4 194.31 4.10 5.01

0.8053 0.5413 lr=5e-5 161.40

6.63 6.69 0.7419 0.5705

Table 8.

[531] å›å¿†LR

= 2E-4 207.27 3.73

5.04 0.8127 0.5401 LR

= 1E-4 194.31 4.10

5.01 0.801 0.8053 0.5413

LR = 5E-5 161.40

6.63 6.63 6.69 6.69

0.7419 0.7419 0.5705è¡¨

8ã€‚

--------------------------------------------------



[532] The

effect of Learning rato

for TinyDiT-D14 finetuning w/o

knowledge distillation Learning Rate.

[532] é€šè¿‡çŸ¥è¯†è’¸

é¦å­¦ä¹ ç‡ï¼Œå­¦

ä¹ æ‹‰æ‰˜å¯¹TinyDit-D14çš„

finetuningçš„æ•ˆæœã€‚



--------------------------------------------------

[533] We also search

on some key hyperparam-

eters such as learning

rates in Table 8.

[533] æˆ‘ä»¬

è¿˜æœç´¢äº†è¡¨

8ä¸­çš„ä¸€äº›å…³

é”®è¶…å¸•æ‹‰å§†

è¯­ï¼ˆä¾‹å¦‚å­¦ä¹ 

ç‡ï¼‰ã€‚



--------------------------------------------------

[534] We identify the

ef- fectiveness of lr=2e-4

and apply it to

all models and exper-

iments.

[534] æˆ‘ä»¬ç¡®å®š

LR =

2E-4çš„æ•ˆç‡ï¼Œå¹¶å°†

å…¶åº”ç”¨äºæ‰€

æœ‰æ¨¡å‹å’Œå®

éªŒã€‚



--------------------------------------------------

[535] 10.

[535] 10ã€‚

--------------------------------------------------



[536] Visulization

Figure 15 and 16

showcase the generated images

from TinySiT-D14 and TinyMAR-D16,

which were compressed from

the official checkpoints.

[536]

å¯è§çš„å›¾

15å’Œ16æ˜¾ç¤ºäº†ä»

å®˜æ–¹æ£€æŸ¥ç‚¹

å‹ç¼©çš„Tinysit-D14å’ŒTinymar-D16çš„

ç”Ÿæˆå›¾åƒã€‚



--------------------------------------------------

[537] These models were

trained using only 7%

and 10% of the

original pre-training costs, respectively,

and were distilled using

the proposed masked knowledge

distillation method.

[537] è¿™

äº›æ¨¡å‹ä»…ä½¿

ç”¨åŸå§‹é¢„è®­

ç»ƒæˆæœ¬çš„7ï¼…å’Œ

10ï¼…åŸ¹è®­ï¼Œå¹¶ä½¿ç”¨

æ‹Ÿè®®çš„è’™ç‰ˆ

çŸ¥è¯†è’¸é¦æ–¹

æ³•è¿›è¡Œè’¸é¦

ã€‚

--------------------------------------------------



[538] Despite

compression, the models are

capable of generating plausible

results with only 50%

of depth.

[538] å°½ç®¡æœ‰å‹ç¼©

ï¼Œè¿™äº›æ¨¡å‹ä»

èƒ½å¤Ÿäº§ç”Ÿä»…

50ï¼…æ·±åº¦çš„åˆç†

ç»“æœã€‚

--------------------------------------------------



[539] 11.

[539] 11ã€‚



--------------------------------------------------

[540] Limitations In this

work, we explore a

learnable depth pruning method

to accelerate diffusion transformer

models for conditional image

generation.

[540] è¿™é¡¹å·¥

ä½œçš„å±€é™æ€§

ï¼Œæˆ‘ä»¬æ¢ç´¢äº†

ä¸€ç§å¯å­¦ä¹ 

çš„æ·±åº¦ä¿®å‰ª

æ–¹æ³•ï¼Œä»¥åŠ é€Ÿ

æœ‰æ¡ä»¶å›¾åƒ

ç”Ÿæˆçš„æ‰©æ•£

å˜å‹å™¨æ¨¡å‹

ã€‚

--------------------------------------------------



[541] As

Diffusion Transformers have shown

significant advancements in text-to-image

generation, it is valuable

to conduct a systematic

analysis of the impact

of layer removal within

the text-to-image tasks.

[541]

ç”±äºæ‰©æ•£å˜

å‹å™¨åœ¨æ–‡æœ¬

åˆ°å›¾åƒç”Ÿæˆ

æ–¹é¢å·²æ˜¾ç¤º

å‡ºæ˜¾ç€çš„è¿›

æ­¥ï¼Œå› æ­¤å¯¹æ–‡

æœ¬åˆ°å›¾åƒä»»

åŠ¡ä¸­å±‚æ‹†å¸

çš„å½±å“è¿›è¡Œ

ç³»ç»Ÿåˆ†ææ˜¯

æœ‰ä»·å€¼çš„ã€‚



--------------------------------------------------

[542] Additionally, there exist

other interesting depth pruning

strategies that need to

be studied, such as

more fine-grained pruning strate-

gies that remove attention

layers and MLP layers

indepen- dently instead of

removing entire transformer blocks.

[542] æ­¤

å¤–ï¼Œè¿˜æœ‰å…¶ä»–

éœ€è¦ç ”ç©¶çš„

æœ‰è¶£çš„æ·±åº¦

ä¿®å‰ªç­–ç•¥ï¼Œä¾‹

å¦‚æ›´ç»†ç²’åº¦

çš„ä¿®å‰ªç­–ç•¥

ï¼Œè¿™äº›ç­–ç•¥ä¼š

æ¶ˆé™¤æ³¨æ„åŠ›

å±‚å’ŒMLPå±‚ï¼Œè€Œä¸

æ˜¯åˆ é™¤æ•´ä¸ª

å˜å‹å™¨å—ã€‚



--------------------------------------------------

[543] We leave these

investigations for future work.

[543] æˆ‘

ä»¬å°†è¿™äº›è°ƒ

æŸ¥ç•™ç»™æœªæ¥

çš„å·¥ä½œã€‚



--------------------------------------------------

[544] 3

[544] 3

--------------------------------------------------



[545] Figure

15.

[545] å›¾15ã€‚

--------------------------------------------------



[546] Generated

images from TinySiT-D14 Figure

16.

[546] æ¥

è‡ªTinysit-D14çš„äº§ç”Ÿå›¾

åƒå›¾16ã€‚

--------------------------------------------------



[547] Generated

images from TinyMAR-D16 4

[547] è’‚å°¼é©¬

å°”-D16 4çš„äº§ç”Ÿå›¾

åƒ4

--------------------------------------------------
