[022] 这

使得深度修

剪了模型压

缩的灵活且

实用的方法

。

--------------------------------------------------



[023] This

work follows a standard

depth pruning frame- work:

unimportant layers are first

removed, and the pruned

model is then fine-tuned

for performance recovery.

[023]

这项工作遵

循标准的深

度修剪框架

- 首先要删除

不重要的层

，然后对修剪

模型进行微

调以进行性

能恢复。



--------------------------------------------------

[024] In the literature,

depth pruning techniques designed

for dif- fusion transformers

or general transformers primarily

fo- cus on heuristic

approaches, such as carefully

designed importance scores [6,

36] or manually configured

pruning 1 arXiv:2412.01199v1

[cs.CV]  2 Dec

2024

[024] 在文

献中，设计用

于差异变压

器或通用变

压器设计的

深度修剪技

术主要是基

于启发式方

法，例如精心

设计的重要

性得分[6，36]或手

动配置的修

剪1 arxiv：2412.011999v1

[cs.cv] [cs.cv] [cs.cv] 2024年12月2日2024年

12月2日

--------------------------------------------------



[025] schemes

[23, 54].

[025] 方案[23，54]。

--------------------------------------------------



[026] These

methods adhere to a

loss min- imization principle

[18, 37], aiming to

identify solutions that maintain

low loss or error

after pruning.

[026] 这

些方法遵守

损失最小原

则[18，37]，旨在识别

在修剪后保

持低损失或

错误的解决

方案。

--------------------------------------------------



[027] This

paper investigates the effectiveness

of this widely used

principle in the context

of depth compression.

[027]

本文在

深度压缩的

背景下研究

了该广泛使

用原理的有

效性。



--------------------------------------------------

[028] Through experiments, we

examined the relationship between

calibration loss ob- served

post-pruning and the performance

after fine-tuning.

[028] 通过实

验，我们检查

了校准损失

渗透后的后

延期与微调

后的性能之

间的关系。

--------------------------------------------------



[029] This

is achieved by extensively

sampling 100,000 models via

random pruning, exhibiting different

levels of calibra- tion

loss in the searching

space.

[029] 这

是通过通过

随机修剪进

行广泛采样

100,000款模型来实

现的，在搜索

空间中表现

出不同水平

的碳纤维损

失。

--------------------------------------------------



[030] Based

on this, we analyzed

the effectiveness of existing

pruning algorithms, such as

the feature similarity [6,

36] and sensitivity analysis

[18], which indeed achieve

low calibration losses in

the solution space.

[030]

基于此，我

们分析了现

有修剪算法

的有效性，例

如特征相似

性[6，36]和灵敏度

分析[18]，它们确

实在解决方

案空间中实

现了低校准

损失。



--------------------------------------------------

[031] However, the performance

of all these models

after fine- tuning often

falls short of expectations.

[031] 但是，精

细调整后所

有这些模型

的性能通常

都没有期望

。



--------------------------------------------------

[032] This indicates that

the loss minimization principle

may not be well-suited

for diffusion transformers.

[032]

这表明最小

化原理可能

不适合扩散

变压器。



--------------------------------------------------

[033] Building on these

insights, we reassessed the

underly- ing principles for

effective layer pruning in

diffusion trans- formers.

[033]

在这

些见解的基

础上，我们重

新评估了在

扩散式传输

中修剪有效

层的基本原

理。



--------------------------------------------------

[034] Fine-tuning diffusion transformers

is an extremely time-consuming

process.

[034] 微调扩散

变压器是一

个非常耗时

的过程。

--------------------------------------------------



[035] Instead

of searching for a

model that minimizes loss

immediately after pruning, we

propose identifying candidate models

with strong recoverability, en-

abling superior post-fine-tuning performance.

[035] 我们

没有在修剪

后立即寻找

将损失立即

最小化的模

型，而是提议

识别具有强

大可恢复性

，优势较高后

调节性能的

候选模型。



--------------------------------------------------

[036] Achieving this goal

is particularly challenging, as

it requires the in-

tegration of two distinct

processes, pruning and fine-tuning,

which involve non-differentiable operations

and cannot be directly

optimized via gradient descent.

[036] 实

现这一目标

特别具有挑

战性，因为它

需要对两个

不同的过程

进行修剪和

微调，这涉及

非不同的操

作，并且不能

通过梯度下

降直接优化

。



--------------------------------------------------

[037] To this end,

we propose a learnable

depth pruning method that

effectively integrates pruning and

fine-tuning.

[037] 为此，我们提

出了一种可

学习的深度

修剪方法，可

以有效整合

修剪和微调

。

--------------------------------------------------



[038] As

shown in Figure 1,

we model the pruning

and fine- tuning of

a diffusion transformer as

a differentiable sam- pling

process of layer masks

[13, 17, 22], combined

with a co-optimized weight

update to simulate future

fine-tuning.

[038] 如图1所示，我

们将扩散变

压器的修剪

和精细调整

为层掩模的

可区分的Sam固

定过程[13，17，22]，并结

合了合作的

权重更新，以

模拟未来的

微调。

--------------------------------------------------



[039] Our

objective is to iteratively

refine this distribution so

that networks with higher

recoverability are more likely

to be sampled.

[039]

我们的

目标是迭代

地完善此分

布，以便更有

可能采样具

有较高可恢

复性的网络

。



--------------------------------------------------

[040] This is achieved

through a straightforward strat-

egy: if a sampled

pruning decision results in

strong recover- ability, similar

pruning patterns will have

an increased prob- ability

of being sampled.

[040]

这是通过直

接的策略来

实现的：如果

采样修剪决

策会导致强

大的恢复能

力，那么相似

的修剪模式

将具有提高

采样的概率

能力。



--------------------------------------------------

[041] This approach promotes

the ex- ploration of

potentially valuable solutions while

disregard- ing less effective

ones.

[041] 这种方

法促进了对

潜在有价值

的解决方案

的提出，同时

又无视效率

较低的解决

方案。

--------------------------------------------------



[042] Additionally,

the proposed method is

highly efficient, and we

demonstrate that a suitable

solu- tion can emerge

within a few training

steps.

[042] 此外，所

提出的方法

非常有效，我

们证明可以

在几个训练

步骤中出现

合适的解决

方案。

--------------------------------------------------



[043] To

evaluate the effectiveness of

the proposed method, we

conduct extensive experiments on

various transformer- based diffusion

models, including DiTs [40],

MARs [29], SiTs [34].

[043] 为了评

估所提出方

法的有效性

，我们对基于

变压器的扩

散模型进行

了广泛的实

验，包括DITS [40]，MARS [29]，位于

[34]。

--------------------------------------------------



[044] The

learnable approach is highly

efficient.

[044] 可学习的方

法非常有效

。

--------------------------------------------------



[045] It

is able to identify

redundant layers in diffusion

transform- ers with 1-epoch

training on the dataset,

which effectively crafts shallow

diffusion transformers from pre-trained

mod- els with high

recoverability.

[045] 它能够通过

数据集上的

1个上述训练

来识别扩散

变换中的冗

余层，从而有

效地从具有

高可恢复性

的预训练的

模型中制作

了浅扩散变

压器。

--------------------------------------------------



[046] For

instance, while the models

pruned by TinyFusion initially

exhibit relatively high cal-

ibration loss after removing

50% of layers, they

recover quickly through fine-tuning,

achieving a significantly more

competitive FID score (5.73

vs. 22.28) compared to

base- line methods that

only minimize immediate loss,

using just 1% of

the pre-training cost.

[046]

例如，尽

管删除50％的层

后，最初被TinyFusion修

剪的模型最

初表现出相

对较高的cal损

失，但与仅使

用预先培训

的1％的基本方

法相比，它们

通过微调得

分迅速恢复

，获得了更具

竞争力的FID得

分（5.73 vs 22.28）（5.73对22.28）。

--------------------------------------------------



[047] Additionally,

we also ex- plore

the role of knowledge

distillation in enhancing re-

coverability [20, 23] by

introducing a MaskedKD variant.

[047] 此外，我

们还通过引

入maskedkd变体来表

达知识蒸馏

在增强可覆

盖性[20，23]中的作

用。



--------------------------------------------------

[048] MaskedKD mitigates the

negative impact of the

massive or outlier activations

[47] in hidden states,

which can signifi- cantly

affect the performance and

reliability of fine-tuning.

[048]

MaskEDKD减轻了隐

藏状态中大

规模或异常

激活的负面

影响[47]，这可能

会显着影响

微调的性能

和可靠性。



--------------------------------------------------

[049] With MaskedKD, the

FID score improves from

5.73 to 3.73 with

only 1% of pre-training

cost.

[049] 借

助MaskedKD，FID得分从5.73提

高到3.73，仅占培

训前成本的

1％。

--------------------------------------------------



[050] Extending

the training to 7%

of the pre-training cost

further reduces the FID

to 2.86, just 0.4

higher than the original

model with doubled depth.

[050] 将培训扩大

到7％的培训前

成本将FID进一

步降低到2.86，仅

比原始模型

高度增加了

0.4。



--------------------------------------------------

[051] Therefore, the main

contribution of this work

lies in a learnable

method to craft shallow

diffusion transformers from pre-trained

ones, which explicitly optimizes

the re- coverability of

pruned models.

[051] 因此，这项工

作的主要贡

献在于一种

可学习的方

法，可以从预

训练的方法

中制作浅扩

散变压器，该

方法明确优

化了修剪模

型的可覆盖

性。

--------------------------------------------------



[052] The

method is general for

various architectures, including DiTs,

MARs and SiTs.

[052]

该方法是

各种架构，包

括DIT，MARS和SITS的一般

方法。



--------------------------------------------------

[053] 2.

[053] 2。

--------------------------------------------------



[054] Related

Works Network Pruning and

Depth Reduction.

[054] 相关工

作网络修剪

和深度减少

。

--------------------------------------------------



[055] Network

prun- ing is a

widely used approach for

compressing pre-trained diffusion models

by eliminating redundant parameters

[3, 12, 31, 51].

[055] 网络修剪是

一种通过消

除冗余参数

来压缩预训

练扩散模型

的广泛使用

方法[3，12，31，51]。



--------------------------------------------------

[056] Diff-Pruning [12] introduces

a gradient- based technique

to streamline the width

of UNet, fol- lowed

by a simple fine-tuning

to recover the performance.

[056] DIFF-PRUNING [12]引入了

一种基于梯

度的技术，以

简化UNET的宽度

，以通过简单

的微调来恢

复性能。

--------------------------------------------------



[057] SparseDM

[51] applies sparsity to

pre-trained diffusion models via

the Straight-Through Estimator (STE)

[2], achieving a 50%

reduction in MACs with

only a 1.22 in-

crease in FID on

average.

[057] Sparsedm [51]通过

直通估计量

（Ste）[2]将稀疏性应

用于预训练

的扩散模型

，在MAC中降低了

50％，平均FID仅1.22个折

痕。

--------------------------------------------------



[058] While

width pruning and spar-

sity help reduce memory

overhead, they often offer

lim- ited speed improvements,

especially on parallel devices

like GPUs.

[058] 虽然修剪

和宽度有助

于减少内存

开销，但它们

通常会提供

限制的速度

提高，尤其是

在诸如GPU之类

的平行设备

上。

--------------------------------------------------



[059] Consequently,

depth reduction has gained

signifi- cant attention in

the past few years,

as removing entire lay-

ers enables better speedup

proportional to the pruning

ra- tio [24, 27,

28, 36, 54, 56,

58].

[059] 因此，在过

去的几年中

，深度的降低

引起了显着

关注，因为删

除整个外行

可以更好地

加速与修剪

ra-tio成正比[24、27、27、28、36、54、56、58]。

--------------------------------------------------



[060] Adaptive

depth reduction techniques, such

as MoD [41] and

depth-aware transform- ers [10],

have also been proposed.

[060] 还提

出了自适应

深度还原技

术，例如MOD [41]和深

度感知的转

化[10]。

--------------------------------------------------



[061] Despite

these advances, most existing

methods are still based

on empirical or heuris-

tic strategies, such as

carefully designed importance crite-

ria [36, 54], sensitivity

analyses [18] or manually

designed schemes [23], which

often do not yield

strong performance guarantee after

fine-tuning.

[061] 尽管取得

了这些进步

，但大多数现

有方法仍然

基于经验或

启发式策略

，例如精心设

计的重要性

迹象[36，54]，敏感性

分析[18]或手动

设计的方案

[23]，这些方案通

常不会在微

调后产生强

大的性能保

证。

--------------------------------------------------



[062] Efficient

Diffusion Transformers.

[062] 有效的扩

散变压器。

--------------------------------------------------



[063] Developing

efficient diffusion transformers has

become an appealing focus

within the community, where

significant efforts have been

made to enhance efficiency

from various perspectives, in-

cluding linear attention mechanisms

[15, 48, 52], compact

architectures [50], non-autoregressive transformers

[4, 14, 38, 49],

pruning [12, 23], quantization

[19, 30, 44], feature

2

[063] 开

发有效的扩

散变压器已

成为社区中

的一个吸引

人的重点，在

各种角度，已

经做出了巨

大的努力来

提高效率，包

括线性注意

机制[15，48，52]，紧凑型

建筑[50]，非自动

性变压器[4，14，38，49]，pruning [4,14,38,49]，pruns

[12，23]，量

化[19，30，30，44]



--------------------------------------------------

[064] Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer 1:2 Local Blocks

𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒

0 1 0 1

0 1 0 1

⊕ Weight Update Weight

Update Weight Update Weight

Update Δ𝜙4 ⋅𝔪4 Δ𝜙3

⋅𝔪3 Δ𝜙2 ⋅𝔪2 Δ𝜙1

⋅𝔪1 Retained Layer Retained

Layer 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽,

𝖒) Confident Sampling ⇒

Good solution identified 1:2

Local Blocks 𝔪1 𝔪2

𝔪3 𝔪4 ⊕ ∼

Mixed Sampling ⇒ Exploration

still in Progress Diff.

[064] 变压器层

变压器层变

压器层变压

器层1：2局部块

𝝓𝟏 0 1

0 1 0 1

0 1 0 1

0 1⊕重量更新重

量更新重量

更新重量更

新重量更新

重量更新重

量更新Δ4Δ3Δ3Δ2Δ2Δ2δ𝜙2Δ𝜙1Δ𝜙1µ1·𝔪1·𝔪1·𝔪1µ1保留

层保留层，良

好的层识别

层（良好的解

决方案），范围

2，𝚫𝚽𝓛，𝚫𝚽𝓛，𝒙+ 𝚫𝚽+ 𝚫𝚽，𝚽+

𝚫𝚽，𝖒+ 𝚫𝚽，Sampling，Sampl complative。 𝔪1𝔪2𝔪3𝔪4⊕4〜混合采样⇒探

索仍在进行

中。

--------------------------------------------------



[065] Sampling

Learnable Distribution ∼ Diff.

[065] 抽样可学

习的分布〜差

异。



--------------------------------------------------

[066] Sampling Figure 2.

[066] 采样图2。



--------------------------------------------------

[067] The proposed TinyFusion

method learns to perform

a differentiable sampling of

candidate solutions, jointly optimized

with a weight update

to estimate recoverability.

[067]

拟

议的TinyFusion方法学

会了对候选

解决方案进

行可区分的

采样，共同优

化了重量更

新以估算可

恢复性。



--------------------------------------------------

[068] This approach aims

to increase the likelihood

of favorable solutions that

ensure strong post-fine- tuning

performance.

[068] 这种

方法旨在增

加有利解决

方案的可能

性，从而确保

强大的结束

后表现。

--------------------------------------------------



[069] After

training, local structures with

the highest sampling probabilities

are retained.

[069] 训练

后，保留了采

样概率最高

的本地结构

。

--------------------------------------------------



[070] caching

[35, 57], etc.

[070]

缓存[35，57]，等。



--------------------------------------------------

[071] In this work,

we focus on compress-

ing the depth of

pre-trained diffusion transformers and

in- troduce a learnable

method that directly optimizes

recover- ability, which is

able to achieve satisfactory

results with low re-training

costs.

[071] 在这

项工作中，我

们专注于压

缩预训练的

扩散变压器

的深度，并赋

予一种可学

习的方法，该

方法可以直

接优化恢复

能力，该方法

能够通过低

重新训练成

本获得令人

满意的结果

。

--------------------------------------------------



[072] 3.

[072] 3。



--------------------------------------------------

[073] Method 3.1.

[073]

方法3.1。



--------------------------------------------------

[074] Shallow Generative Transformers

by Pruning This work

aims to derive a

shallow diffusion transformer by

pruning a pre-trained model.

[074] 通过修

剪这项工作

，浅层生成变

压器旨在通

过修剪预训

练的模型来

得出浅扩散

变压器。



--------------------------------------------------

[075] For simplicity, all

vectors in this paper

are column vectors.

[075]

为简

单起见，本文

中的所有向

量都是列向

量。



--------------------------------------------------

[076] Consider a L-layer

trans- former, parameterized by

ΦL×D = [ϕ1, ϕ2,

· · · ,

ϕL]⊺, where each element

ϕi encompasses all learnable

param- eters of a

transformer layer as a

D-dim column vector, which

includes the weights of

both attention layers and

MLPs.

[076] 考虑一个

由L层trans-前者，由

φl×d =

[ϕ1，ϕ2，·，ϕL]⊺进行参数，其

中每个元素

ϕi包含变压器

层的所有可

学习的参数

作为D-DIM柱向量

，其中包括注

意层和MLP的重

量。



--------------------------------------------------

[077] Depth pruning seeks

to find a binary

layer mask mL×1 =

[m1, m2, · ·

· , mL]⊺, that

removes a layer by:

xi+1 = miϕi(xi) +

(1 −mi)xi = (

ϕi(xi), if mi =

1, xi, otherwise, (1)

where the xi and

ϕi(xi) refers to the

input and output of

layer ϕi.

[077] 深度修剪

试图找到二

进制掩码ml×1

= [m1，m2，··级

，ml]⊺，该层通过：xi + 1

= miDartice（xi） +（1 -mi）xi

=（ϕi（xi）=（ϕi（xi（xi（xi），如

果mi = 1，xi），xi =

1，xi，（xi），xi和xi yly（xi），（xi）和xi liver，（xi）codi（xi）和xi（xi）codi（xi）和

（xi liver，（1）

ϕi。



--------------------------------------------------

[078] To obtain the

mask, a common paradigm

in prior work is

to minimize the loss

L after pruning, which

can be formulated as

minm Ex [L(x, Φ,

m)].

[078] 为了获得掩

模，先前工作

中的一个常

见范式是将

修剪后的损

耗l最小化，可

以将其作为

minm ex

[l（x，φ，m）]配制。



--------------------------------------------------

[079] However, as we

will show in the

experiments, this objective –

though widely adopted in

discriminative tasks – may

not be well-suited to

pruning diffusion transformers.

[079]

但是，正

如我们将在

实验中显示

的那样，尽管

在判别任务

中广泛采用

了这个目标

，但可能不适

合修剪扩散

变压器。



--------------------------------------------------

[080] Instead, we are

more inter- ested in

the recoverability of pruned

models.

[080] 取而

代之的是，我

们对修剪模

型的可恢复

性更加感兴

趣。

--------------------------------------------------



[081] To

achieve this, we incorporate

an additional weight update

into the optimization problem

and extend the objective

by: min m min

∆Φ Ex [L(x, Φ

+ ∆Φ, m)] |

{z } Recoverability: Post-Fine-Tuning

Performance , (2) where

∆Φ = {∆ϕ1, ∆ϕ2,

· · · ,

∆ϕM} represents appro- priate

update from fine-tuning.

[081]

为了实现

这一目标，我

们将额外的

权重更新纳

入了优化问

题，并扩展了

目标：min m min ∆φ

ex [l（x，φ + ∆φ，m）]

| {z}可恢复

性：恢复性能

后的性能，（2）其

中∆φ = {∆

ϕ1，∆ ϕ2，···，∆ ϕm}表示来自

微调的适当

更新。

--------------------------------------------------



[082] The

objective formulated by Equation

2 poses two challenges:

1) The non-differentiable nature

of layer selection prevents

direct optimization us- ing

gradient descent; 2) The

inner optimization over the

retained layers makes it

computationally intractable to ex-

plore the entire search

space, as this process

necessitates se- lecting a

candidate model and fine-tuning

it for evaluation.

[082]

公式2提

出的目标提

出了两个挑

战：1）层选择的

非差异性质

阻止了直接

优化的梯度

下降； 2）在保留

层上的内部

优化使得在

计算上棘手

可以阐明整

个搜索空间

，因为此过程

需要将候选

模型列为候

选模型并进

行微调以进

行评估。



--------------------------------------------------

[083] To address this,

we propose TinyFusion that

makes both the pruning

and recoverability optimizable.

[083]

为了

解决这个问

题，我们提出

了使修剪和

可恢复性优

化的微小灌

注。



--------------------------------------------------

[084] 3.2.

[084] 3.2。

--------------------------------------------------



[085] TinyFusion:

Learnable Depth Pruning A

Probabilistic Perspective.

[085] 小型灌注

：可学习的深

度修剪概率

的观点。

--------------------------------------------------



[086] This

work models Equa- tion

2 from a probabilistic

standpoint.

[086] 这项

工作从概率

的角度模拟

了方程2。

--------------------------------------------------



[087] We

hypothesize that the mask

m produced by “ideal”

pruning methods (might be

not unique) should follow

a certain distribution.

[087]

我们

假设由“理想

”修剪方法（可

能不是唯一

的）产生的面

膜应遵循一

定的分布。



--------------------------------------------------

[088] To model this,

it is intuitive to

associate every possible mask

m with a probability

value p(m), thus forming

a categori- cal distribution.

[088] 为

了对此进行

建模，将每个

可能的掩码

M与概率值P（M）相

关联是直观

的，从而形成

分类分布。



--------------------------------------------------

[089] Without any prior

knowledge, the assess- ment

of pruning masks begins

with a uniform distribution.

[089] 没

有任何先验

知识，修剪口

罩的评估始

于均匀的分

布。



--------------------------------------------------

[090] However, directly sampling

from this initial distribution

is highly inefficient due

to the vast search

space.

[090] 但是，由于

庞大的搜索

空间，该初始

分布直接进

行采样效率

很高。

--------------------------------------------------



[091] For

in- stance, pruning a

28-layer model by 50%

involves evalu- ating  28

14  = 40,

116, 600 possible solutions.

[091] 对于说

明，将28层模型

（50％）修剪为评估

28 14 =

40、116、600可能的解决

方案。



--------------------------------------------------

[092] To overcome this

challenge, this work introduces

an advanced and learn-

able algorithm capable of

using evaluation results as

feed- back to iteratively

refine the mask distribution.

[092] 为了克

服这一挑战

，这项工作引

入了一种能

够使用评估

结果作为回

馈的高级且

具有学习算

法，以返回迭

代的掩码分

布。



--------------------------------------------------

[093] The basic idea

is that if certain

masks exhibit positive results,

then other masks with

similar pattern may also

be potential so- lutions

and thus should have

a higher likelihood of

sam- pling in subsequent

evaluations, allowing for a

more fo- cused search

on promising solutions.

[093]

基本思想

是，如果某些

掩膜表现出

积极的结果

，那么其他模

式相似的掩

膜也可能是

潜在的，因此

在随后的评

估中应该具

有更高的可

能性，从而可

以对有希望

的解决方案

进行更多的

搜索。



--------------------------------------------------

[094] However, the defi-

nition of “similarity pattern”

is still unclear so

far.

[094] 但是，到

目前为止，“相

似性模式”的

定义尚不清

楚。

--------------------------------------------------



[095] 3

[095] 3



--------------------------------------------------

[096] Sampling Local Structures.

[096] 采样本地

结构。



--------------------------------------------------

[097] In this work,

we demon- strate that

local structures, as illustrated

in Figure 2, can

serve as effective anchors

for modeling the relationships

between different masks.

[097]

在这项

工作中，我们

表明，如图2所

示，局部结构

可以用作建

模不同遮罩

之间关系的

有效锚点。



--------------------------------------------------

[098] If a pruning

mask leads to cer-

tain local structures and

yields competitive results after

fine- tuning, then other

masks yielding the same

local patterns are also

likely to be positive

solutions.

[098] 如

果修剪掩模

会导致局部

结构，并在细

调时产生竞

争结果，那么

其他掩模产

生相同局部

模式的掩模

也可能是正

溶液。

--------------------------------------------------



[099] This

can be achieved by

dividing the original model

into K non-overlapping blocks,

represented as Φ =

[Φ1, Φ2, · ·

· , ΦK]⊺.

[099]

这可以

通过将原始

模型划分为

k非重叠块（表

示为φ= [φ1，φ2，·，φk]⊺的非重

叠块。



--------------------------------------------------

[100] For simplicity, we

assume each block Φk

= [ϕk1, ϕk2, ·

· · , ϕkM]⊺contains

exactly M layers, although

they can have varied

lengths.

[100] 为简单

起见，我们假

设每个块φk= [ϕk1，ϕk2，···，ϕkm]⊺恰

好包含m层，尽

管它们的长

度可以变化

。

--------------------------------------------------



[101] Instead

of performing global layer

pruning, we propose an

N:M scheme for local

layer pruning, where, for

each block Φk with

M layers, N layers

are retained.

[101] 我们没有执

行全局层修

剪，而是提出

了局部层修

剪的N：M方案，在

其中，对于带

有M层的每个

块φK，n层都保留

。

--------------------------------------------------



[102] This

results in a set

of local binary masks

m = [m1, m2,

.

[102] 这导致一组

本地二进制

掩码M =

[M1，M2，。



--------------------------------------------------

[103] .

[103] 。

--------------------------------------------------



[104] .

[104] 。



--------------------------------------------------

[105] , mK]⊺.

[105]

，Mk]⊺。



--------------------------------------------------

[106] Simi- larly, the

distribution of a local

mask mk is modeled

using a categorical distribution

p(mk).

[106] 同样，使

用分类分布

P（MK）对局部掩码

MK的分布进行

建模。

--------------------------------------------------



[107] We

perform independent sampling of

local binary masks and

combine them for prun-

ing, which presents the

joint distribution: p(m) =

p(m1) · p(m2) ·

· · p(mK) (3)

If some local distributions

p(mk) exhibit high confidence

in the corresponding blocks,

the system will tend

to sam- ple those

positive patterns frequently and

keep active ex- plorations

in other local blocks.

[107] 我们对

局部二进制

蒙版进行独

立的抽样，并

将它们结合

起修剪，其中

提出了联合

分布：P（M）= P（M1）·P（M2）·P（M2）·P（Mk）P（MK）（MK）（MK）（MK）（MK）（MK）（MK）如果在

相应的块中

表现出很高

的信心，则系

统会倾向于

Sam-ple sam-ple

blocks starters starters store

Ex-Plor，并保持其他

积极的位置

- 并保持其他

extrors-plor。



--------------------------------------------------

[108] Based on this

concept, we introduce differential

sampling to make the

above process learnable.

[108]

基于这个概

念，我们介绍

了差异采样

，以使上述过

程可学习。



--------------------------------------------------

[109] Differentiable Sampling.

[109]

可

区分的采样

。



--------------------------------------------------

[110] Considering the sampling

pro- cess of a

local mask mk, which

corresponds a local block

Φk and is modeled

by a categorical distribution

p(mk).

[110] 考虑到局部

掩码MK的采样

过程，该掩膜

与局部块φK相

对应，并通过

分类分布P（MK）进

行建模。

--------------------------------------------------



[111] With

the N:M scheme, there

are  M N 

possible masks.

[111] 使用

N：M方案，有可能

的掩模。

--------------------------------------------------



[112] We

construct a special matrix

ˆmN:M to enumerate all

possi- ble masks.

[112]

我们

构建一个特

殊的矩阵ˆmn：m来

枚举所有可

能的面具。



--------------------------------------------------

[113] For example, 2:3

layer pruning will lead

to the candidate matrix

ˆm2:3 = [[1, 1,

0] , [1, 0,

1] , [0, 1,

1]].

[113] 例

如，2：3层修剪将

导致候选矩

阵ˆm2：3 =

[[1，1，0]，[1，0，1]，[0，1，1，1]]。



--------------------------------------------------

[114] In this case,

each block will have

three probabilities p(mk) =

[pk1, pk2, pk3].

[114]

在这种情

况下，每个块

将具有三个

概率P（MK）= [PK1，PK2，PK3]。



--------------------------------------------------

[115] For simplicity, we

omit mk and k

and use pi to

represent the probability of

sampling i-th element in

ˆmN:M. A popular method

to make a sampling

process dif- ferentiable is

Gumbel-Softmax [13, 17, 22]:

y = one-hot

exp((gi + log pi)/τ)

P j exp((gj +

log pj)/τ) !

[115]

为简单

起见，我们省

略了MK和K，并使

用PI表示在ˆmn：m中

对第i-th元素进

行采样的概

率。使采样过

程分化的一

种流行方法

是Gumbel-Softmax [13，17，22]：y =一hot exp（（（gi

+ log pi）/τ）p j

exp（（gj + log pj）/τ）！

--------------------------------------------------



[116] .

[116] 。



--------------------------------------------------

[117] (4) where gi

is random noise drawn

from the Gumbel distribu-

tion Gumbel(0, 1) and

τ refers to the

temperature term.

[117] （4）其中GI是

随机噪声，它

是从牙龈分

布（0，1）中绘制的

，τ是指温度项

。

--------------------------------------------------



[118] The

output y is the

index of the sampled

mask.

[118] 输出y是采样

蒙版的索引

。

--------------------------------------------------



[119] Here

a Straight- Through Estimator

[2] is applied to

the one-hot operation, where

the onehot operation is

enabled during forward and

is treated as an

identity function during backward.

[119] 在这里，直接

通过估计器

[2]应用于单速

操作，该操作

在向前时启

用了orhot操作，并

将其视为在

向后时作为

身份函数。



--------------------------------------------------

[120] Leverag- ing the

one-hot index y and

the candidate set ˆmN:M,

we can draw a

mask m ∼p(m) through

a simple index operation:

m = y⊺ˆm (5)

Pretrained 𝑊 A r

Identity f(x)=x ⨂ 𝑚𝑖

⨂ + (1 −𝑚𝑖)

𝑥𝑖 𝑥𝑖+1 B 𝑁×

Figure 3.

[120] 杠

杆率单高索

引y和候选组

集ˆ

mn：m，我们可以

通过简单的

索引操作绘

制蒙版m〜p（m）：m = y⊺ˆm（5）预处

理的a a

a a a a

a a r同一性

f（x）= x⨂

+（1 −1-𝑚𝑖 +（1-𝑚𝑖 +

1 b + 1

b𝑁 +）。



--------------------------------------------------

[121] An example of

forward propagation with differentiable

pruning mask mi and

LoRA for recoverability estimation.

[121] 带有可恢复

性估算的可

恢复性蒙版

MI和LORA的正向传

播的一个例

子。



--------------------------------------------------

[122] Notably, when τ

→0, the STE gradients

will approximate the true

gradients, yet with a

higher variance which is

neg- ative for training

[22].

[122] 值得注意

的是，当τ→0时，Ste梯

度将近似真

正的梯度，但

具有更高的

方差，对训练

的差异很高

[22]。

--------------------------------------------------



[123] Thus,

a scheduler is typically

em- ployed to initiate

training with a high

temperature, gradu- ally reducing

it over time.

[123]

因此，调度程

序通常会以

高温启动训

练，随着时间

的流逝而逐

渐减少。



--------------------------------------------------

[124] Joint Optimization with

Recoverability.

[124] 联合

优化，可恢复

性。

--------------------------------------------------



[125] With

differen- tiable sampling, we

are able to update

the underlying prob- ability

using gradient descent.

[125]

通过不同

的采样，我们

能够使用梯

度下降来更

新潜在的概

率。



--------------------------------------------------

[126] The training objective

in this work is

to maximize the recoverability

of sampled masks.

[126]

这项工作

的训练目标

是最大化采

样面具的可

回收性。



--------------------------------------------------

[127] We reformulate the

objective in Equation 2

by incorporat- ing the

learnable distribution: min {p(mk)}

min ∆Φ Ex,{mk∼p(mk)} [L(x,

Φ + ∆Φ, {mk}]

| {z } Recoverability:

Post-Fine-Tuning Performance , (6)

where {p(mk)} = {p(m1),

· · · ,

p(mK)} refer to the

cat- egorical distributions for

different local blocks.

[127]

我们

通过合并可

学习的分布

来重新重新

制定目标2中

的目标：min {p（mk）} min ∆φ

ex，{mk〜p（mk）} [l（x，φ + ∆φ，{mk}

| {z} | {z}

| {z} | {z}恢复

性：恢复性：恢

复性能：fine-fine-tonning

performance，post-fine-tuning performance，post-fine ther p（mk）}有关

不同局部块

的猫分布。

--------------------------------------------------



[128] Based

on this formulation, we

further investigate how to

incorporate the fine-tuning information

into the training.

[128]

基

于此公式，我

们进一步研

究了如何将

微调信息纳

入培训。



--------------------------------------------------

[129] We propose a

joint optimization of the

distribution and a weight

update ∆Φ.

[129] 我们

提出了分布

的联合优化

和权重更新

∆φ。

--------------------------------------------------



[130] Our

key idea is to

introduce a co-optimized update

∆Φ for joint training.

[130] 我们的关键

思想是引入

合作更新Δφ以

进行关节训

练。



--------------------------------------------------

[131] A straightforward way

to craft the update

is to directly optimize

the original network.

[131]

制作更新

的直接方法

是直接优化

原始网络。



--------------------------------------------------

[132] However, the parameter

scale in a diffusion

transformer is usually huge,

and a full optimization

may make the training

process costly and not

that efficient.

[132] 但

是，扩散变压

器中的参数

量表通常是

巨大的，并且

完整的优化

可能会使训

练过程成本

高昂，而且效

率不高。

--------------------------------------------------



[133] To

this end, we show

that Parameter- Efficient Fine-Tuning

methods such as LoRA

[21] can be a

good choice to obtain

the required ∆Φ.

[133]

为此

，我们表明参

数有效的微

调方法（例如

Lora [21]）可以是获得

所需∆φ的好选

择。



--------------------------------------------------

[134] For a single

linear matrix W in

Φ, we simulate the

fine-tuned weights as: Wfine-tuned

= W + α∆W

= W + αBA,

(7) where α is

a scalar hyperparameter that

scales the contribu- tion

of ∆W.

[134] 对于单个

线性矩阵Wφ，我

们将微调的

权重模拟为

：wfine-tuned

= w +αΔW= w

+αbA，（7），其中α是标量

超参数，可缩

放ΔW的贡献。



--------------------------------------------------

[135] Using LoRA significantly

reduces the num- ber

of parameters, facilitating efficient

exploration of differ- ent

pruning decisions.

[135] 使

用洛拉大大

减少了参数

的数量，从而

有助于对不

同的修剪决

策的有效探

索。

--------------------------------------------------



[136] As

shown in Figure 3,

we leverage the sampled

binary mask value mi

as the gate and

forward the network with

Equation 1, which suppresses

the layer outputs if

the sampled mask is

0 for the current

layer.

[136] 如图3所示

，我们利用采

样的二进制

掩码值MI作为

门，并用方程

1向网络转发

，如果当前层

采样的掩码

为0，则抑制层

输出的层输

出。

--------------------------------------------------



[137] In

addition, the previously mentioned

STE will still provide

non-zero gradients to the

pruned layer, allowing it

to be fur- ther

updated.

[137] 此外，前面

提到的Ste仍将

为修剪层提

供非零梯度

，从而可以进

行更新。

--------------------------------------------------



[138] This

is helpful in practice,

since some layers 4

[138] 这在

实践中很有

帮助，因为有

些层4



--------------------------------------------------

[139] Method Depth #Param

Iters IS ↑ FID

↓ sFID ↓ Prec.

[139] 方法深

度#param Iters是↑fid↓SFID↓PREC。

--------------------------------------------------



[140] ↑

Recall ↑ Sampling it/s

↑ DiT-XL/2 [40] 28

675 M 7,000 K

278.24 2.27 4.60 0.83

0.57 6.91 DiT-XL/2 [40]

28 675 M 2,000

K 240.22 2.73 4.46

0.83 0.55 6.91 DiT-XL/2

[40] 28 675 M

1,000 K 157.83 5.53

4.60 0.80 0.53 6.91

U-ViT-H/2 [1] 29 501

M 500 K 265.30

2.30 5.60 0.82 0.58

8.21 ShortGPT [36] 28⇒19

459 M 100 K

132.79 7.93 5.25 0.76

0.53 10.07 TinyDiT-D19 (KD)

28⇒19 459 M 100

K 242.29 2.90 4.63

0.84 0.54 10.07 TinyDiT-D19

(KD) 28⇒19 459 M

500 K 251.02 2.55

4.57 0.83 0.55 10.07

DiT-L/2 [40] 24 458

M 1,000 K 196.26

3.73 4.62 0.82 0.54

9.73 U-ViT-L [1] 21

287 M 300 K

221.29 3.44 6.58 0.83

0.52 13.48 U-DiT-L [50]

22 204 M 400

K 246.03 3.37 4.49

0.86 0.50 - Diff-Pruning-50%

[12] 28 338 M

100 K 186.02 3.85

4.92 0.82 0.54 10.43

Diff-Pruning-75% [12] 28 169

M 100 K 83.78

14.58 6.28 0.72 0.53

13.59 ShortGPT [36] 28⇒14

340 M 100 K

66.10 22.28 6.20 0.63

0.56 13.54 Flux-Lite [6]

28⇒14 340 M 100

K 54.54 25.92 5.98

0.62 0.55 13.54 Sensitivity

Analysis [18] 28⇒14 340

M 100 K 70.36

21.15 6.22 0.63 0.57

13.54 Oracle (BK-SDM) [23]

28⇒14 340 M 100

K 141.18 7.43 6.09

0.75 0.55 13.54 TinyDiT-D14

28⇒14 340 M 100

K 151.88 5.73 4.91

0.80 0.55 13.54 TinyDiT-D14

28⇒14 340 M 500

K 198.85 3.92 5.69

0.78 0.58 13.54 TinyDiT-D14

(KD) 28⇒14 340 M

100 K 207.27 3.73

5.04 0.81 0.54 13.54

TinyDiT-D14 (KD) 28⇒14 340

M 500 K 234.50

2.86 4.75 0.82 0.55

13.54 DiT-B/2 [40] 12

130 M 1,000 K

119.63 10.12 5.39 0.73

0.55 28.30 U-DiT-B [50]

22 - 400 K

85.15 16.64 6.33 0.64

0.63 - TinyDiT-D7 (KD)

14⇒7 173 M 500

K 166.91 5.87 5.43

0.78 0.53 26.81 Table

1.

[140] ↑ Recall

↑ Sampling it/s ↑

DiT-XL/2 [40] 28 675

M 7,000 K 278.24

2.27 4.60 0.83 0.57

6.91 DiT-XL/2 [40] 28

675 M 2,000 K

240.22 2.73 4.46 0.83

0.55 6.91 DiT-XL/2 [40]

28 675 M 1,000

K 157.83 5.53 4.60

0.80 0.53 6.91 U-ViT-H/2

[1] 29 501 M

500 K 265.30 2.30

5.60 0.82 0.58 8.21

ShortGPT [36] 28⇒19 459

M 100 K 132.79

7.93 5.25 0.76 0.53

10.07 TinyDiT-D19 (KD) 28⇒19

459 M 100 K

242.29 2.90 4.63 0.84

0.54 10.07 Tinydit-D19（KD）28⇒19459 M

500 K 251.02 2.55

4.57 0.83 0.55 0.55

10.07 DIT-L/2 [40] 24

458 M 1,000 K

196.26.26.26 3.73 4.62 4.62

0.82 0.54 9.73 U-Vit-L

[1] 9.73 U-Vit-L [1]

U-Vit-L [1] 21 287

M 300 M 300

M 300 K 22129.44.44.44.44.44.44.44.44.44.44.44

4.44.44.44.44.44.444.44.44.44 k 2294.44.44.44.44.44.44 4.44

4.44 4.44 4.44 4.44

4.44 4.44 4.44.444.; U-DiT-L

[50] 22 204 M

400 K 246.03 3.37

4.49 0.86 0.50 -

Diff-Pruning-50% [12] 28 338

M 100 K 186.02

3.85 4.92 0.82 0.54

10.43 Diff-Pruning-75% [12] 28

169 M 100 K

83.78 14.58 6.28 0.72

0.53 13.59 ShortGPT [36]

28⇒14 340 M 100

K 66.10 22.28 6.20

0.63 0.56 13.54 Flux-Lite

[6] 28⇒14 340 M

100 K 54.54 25.92

5.98 0.62 0.55 13.54

Sensitivity Analysis [18] 28⇒14

340 M 100 K

70.36 21.15 6.22 0.63

0.57 13.54 Oracle (BK-SDM)

[23] 28⇒14340 M 100

K 141.18 7.43 6.09

0.75 0.55 0.55 13.54

TINYDIT-D1428⇒14340 M 100 K

100 K 151.88 5.73

4.91 0.80 0.55 13.55

13.54 TINYDIT-D14 28看看14 340

M 500 K 198.85

3.92 5.62 5.62 5.62

0.78 14.78 148（ 28⇒14

340 M 100 K

207.27 3.73 5.04 0.81

0.54 13.54 TinyDiT-D14 (KD)

28⇒14 340 M 500

K 234.50 2.86 4.75

0.82 0.55 13.54 DiT-B/2

[40] 12 130 M

1,000 K 119.63 10.12

5.39 0.73 0.55 28.30

U-DiT-B [50] 22 -

400 K 85.15 16.64

6.33 0.64 0.63 -tinydit

-d7（kd）14⇒7173 M 500 K

166.91 5.87 5.43 5.43

0.78 0.78 0.53 26.81表

1。

--------------------------------------------------



[141] Layer

pruning results for pre-trained

DiT-XL/2.

[141] 预先训练的

DIT-XL/2的层修剪结

果。

--------------------------------------------------



[142] We

focus on two settings:

fast training with 100K

optimization steps and sufficient

fine-tuning with 500K steps.

[142] 我们专注

于两个设置

：通过100k优化步

骤进行快速

培训，并通过

500k步骤进行了

足够的微调

。



--------------------------------------------------

[143] Both fine-tuning and

Masked Knowledge Distillation (a

variant of KD, see

Sec.

[143] 微调和掩盖

的知识蒸馏

（KD的一种变体

，请参见秒。

--------------------------------------------------



[144] 4.4)

are used for recovery.

[144] 4.4）用

于恢复。



--------------------------------------------------

[145] might not be

competitive at the beginning,

but may emerge as

competitive candidates with sufficient

fine-tuning.

[145] 一开

始可能不是

竞争性的，但

可能会成为

具有足够微

调的竞争候

选人。

--------------------------------------------------



[146] Pruning

Decision.

[146] 修剪决

定。

--------------------------------------------------



[147] After

training, we retain those

local structures with the

highest probability and discard

the ad- ditional update

∆Φ.

[147] 训练后，我

们保留了最

高概率的那

些局部结构

，并丢弃了广

告更新Δφ。

--------------------------------------------------



[148] Then,

standard fine-tuning techniques can

be applied for recovery.

[148] 然后

，可以应用标

准的微调技

术进行恢复

。



--------------------------------------------------

[149] 4.

[149] 4。

--------------------------------------------------



[150] Experiments

4.1.

[150] 实验4.1。

--------------------------------------------------



[151] Experimental

Settings Our experiments were

mainly conducted on Diffusion

Transformers [40] for class-conditional

image generation on ImageNet

256 × 256 [8].

[151] 实验设

置我们的实

验主要是在

扩散变压器

[40]上进行的，以

在ImageNet 256×256 [8]上进行类

条件图像生

成。

--------------------------------------------------



[152] For

evaluation, we fol- low

[9, 40] and report

the Fr´echet inception distance

(FID), Sliding Fr´echet Inception

Distance (sFID), Inception Scores

(IS), Precision and Recall

using the official reference

im- ages [9].

[152]

为了进行

评估，我们将

[9，40]报告，并报告

了使用正式

的参考文献

[9]，报告了FR´Echet Inception距离

（FID），SLIVE fr´echet Inception距离（SFID），INCEPTION分数（IS），精

度和召回。

--------------------------------------------------



[153] Additionally,

we also extend our

methods to other models,

including MARs [29] and

SiTs [34].

[153] 此

外，我们还将

我们的方法

扩展到其他

模型，包括火

星[29]和位于[34]。

--------------------------------------------------



[154] Experimental

details can be found

in the following sections

and appendix.

[154] 实

验细节可以

在以下各节

和附录中找

到。

--------------------------------------------------



[155] 4.2.

[155] 4.2。



--------------------------------------------------

[156] Results on Diffusion

Transformers DiT.

[156] 扩散变压

器DIT的结果。

--------------------------------------------------



[157] This

work focuses on the

compression of DiTs [40].

[157] 这

项工作着重

于DIT的压缩[40]。



--------------------------------------------------

[158] We consider two

primary strategies as baselines:

the first 0 20

40 60 80 Compression

Ratio (%) 2 4

6 8 10 12

Speed Up 1.08 1.17

1.27 1.40 1.55 1.74

1.99 2.30 2.76 3.41

4.46 6.45 11.60 1.04

1.26 1.36 1.64 1.91

2.20 2.71 3.36 4.39

Depth Pruning Width Pruning

Linear Speedup Figure 4.

[158] We consider two

primary strategies as baselines:

the first 0 20

40 60 80 Compression

Ratio (%) 2 4

6 8 10 12

Speed Up 1.08 1.17

1.27 1.40 1.55 1.74

1.99 2.30 2.76 3.41

4.46 6.45 11.60 1.04

1.26 1.36 1.64 1.91

2.20 2.71 3.36 4.39

Depth Pruning Width Pruning

Linear Speedup Figure 4.

--------------------------------------------------



[159] Depth

pruning closely aligns with

the theoretical linear speed-up

relative to the compression

ratio.

[159] 深

度修剪与理

论线性加速

相对于压缩

率紧密对齐

。

--------------------------------------------------



[160] involves

using manually crafted patterns

to eliminate lay- ers.

[160] 涉及使用手

动制作的图

案消除外行

。



--------------------------------------------------

[161] For instance, BK-SDM

[23] employs heuristic assump-

tions to determine the

significance of specific layers,

such as the initial

or final layers.

[161]

例如，BK-SDM [23]采用启

发式辅助来

确定特定层

的重要性，例

如初始或最

终层。



--------------------------------------------------

[162] The second strategy

is based on systematically

designed criteria to evaluate

layer impor- tance, such

as analyzing the similarity

between block in- puts

and outputs to determine

redundancy [6, 36]; this

ap- proach typically aims

to minimize performance degradation

after pruning.

[162] 第二种

策略是基于

系统设计的

标准来评估

层的重要性

，例如分析块

In-ofs和输出之间

的相似性以

确定冗余[6，36]；这

种方法通常

旨在最大程

度地减少修

剪后的性能

降解。

--------------------------------------------------



[163] Table

1 presents representatives from

both strategies, including ShortGPT

[36], Flux-Lite [6], Diff-

Pruning [12], Sensitivity Analysis

[18] and BK-SDM [23],

which serve as baselines

for comparison.

[163] 表1介绍

了两种策略

的代表，包括

短速度[36]，Flux-Lite

[6]，Diff-pruning [12]，灵敏

度分析[18]和BK-SDM [23]，它

们是比较的

基础。

--------------------------------------------------



[164] Additionally,

5

[164] 另外，5

--------------------------------------------------



[165] Method

Depth Params Epochs FID

IS MAR-Large 32 479

M 400 1.78 296.0

MAR-Base 24 208 M

400 2.31 281.7 TinyMAR-D16

32⇒16 277 M 40

2.28 283.4 SiT-XL/2 28

675 M 1,400 2.06

277.5 TinySiT-D14 28⇒14 340

M 100 3.02 220.1

Table 2.

[165] Method

Depth Params Epochs FID

IS MAR-Large 32 479

M 400 1.78 296.0

MAR-Base 24 208 M

400 2.31 281.7 TinyMAR-D16

32⇒16 277 M 40

2.28 283.4 SiT-XL/2 28

675 M 1,400 2.06

277.5 TinySiT-D14 28⇒14 340

M 100 3.02 220.1

Table 2.



--------------------------------------------------

[166] Depth pruning results

on MARs [29] and

SiTs [34].

[166] 对

火星的深度

修剪结果[29]并

位于[34]。

--------------------------------------------------



[167] we

evaluate our method against

innovative architectural de- signs,

such as UViT [1],

U-DiT [50], and DTR

[39], which have demonstrated

improved training efficiency over

con- ventional DiTs.

[167]

我们评

估了针对创

新建筑的方

法，例如UVIT [1]，U-DIT [50]和DTR [39]，这

些方法表明

，这些方法表

现出了提高

的培训效率

，而不是降级

。

--------------------------------------------------



[168] Table

1 presents our findings

on compressing a pre-

trained DiT-XL/2 [40].

[168]

表1列出了我

们关于压缩

预训练的DIT-XL/2 [40]的

发现。



--------------------------------------------------

[169] This model contains

28 transformer layers structured

with alternating Attention and

MLP lay- ers.

[169]

该模型

包含28个具有

交替注意和

MLP外行的变压

器层。



--------------------------------------------------

[170] The proposed method

seeks to identify shallow

trans- formers with {7,

14, 19} sub-layers from

these 28 layers, to

maximize the post-fine-tuning performance.

[170] 所提出

的方法旨在

鉴定这28层中

{7、14、19}子层的浅型

，以最大程度

地提高后调

节性能。



--------------------------------------------------

[171] With only 7%

of the original training

cost (500K steps compared

to 7M steps), TinyDiT

achieves competitive performance rela-

tive to both pruning-based

methods and novel architectures.

[171] Tinydit只有

7％的原始培训

成本（500k步长），与

基于修剪的

方法和新型

体系结构相

关的竞争性

能。



--------------------------------------------------

[172] For instance, a

DiT-L model trained from

scratch for 1M steps

achieves an FID score

of 3.73 with 458M

parameters.

[172] 例如，通过

从头开始训

练1M步骤的DIT-L模

型以4.58亿参数

的成绩达到

3.73的FID分数。

--------------------------------------------------



[173] In

contrast, the compressed TinyDiT-D14

model, with only 340M

parameters and a faster

sampling speed (13.54 it/s

vs. 9.73 it/s), yields

a significantly improved FID

of 2.86.

[173] 相比

之下，仅具有

340m参数和更快

的采样速度

（13.54

IT/s vs. 9.73 IT/s）的压缩TinyDit-D14模型

可显着提高

2.86的FID。

--------------------------------------------------



[174] On

parallel devices like GPUs,

the primary bottleneck in

trans- formers arises from

sequential operations within each

layer, which becomes more

pronounced as the number

of layers increases.

[174]

在诸如GPU之

类的并行设

备上，转移器

中的主要瓶

颈源于每一

层内的顺序

操作，随着层

数的增加，这

会变得更加

明显。



--------------------------------------------------

[175] Depth pruning mitigates

this bottleneck by re-

moving entire transformer layers,

thereby reducing compu- tational

depth and optimizing the

workload.

[175] 深度修

剪通过重新

移动整个变

压器层来减

轻这种瓶颈

，从而减少组

合深度并优

化工作量。

--------------------------------------------------



[176] By

compar- ison, width pruning

only reduces the number

of neurons within each

layer, limiting its speed-up

potential.

[176] 相

比之下，修剪

宽度仅会减

少每一层内

神经元的数

量，从而限制

了其加速势

。

--------------------------------------------------



[177] As

shown in Figure 4,

depth pruning closely matches

the theoretical linear speed-up

as the compression ratio

increases, outper- forming width

pruning methods such as

Diff-Pruning [12].

[177] 如图4所示，随

着压缩比的

增加，深度修

剪与理论线

性加速密切

匹配，诸如diff-pruning之

类的宽度宽

度较高[12]。

--------------------------------------------------



[178] MAR

& SiT.

[178] Mar＆Sit。

--------------------------------------------------



[179] Masked

Autoregressive (MAR) [29] mod-

els employ a diffusion

loss-based autoregressive framework in

a continuous-valued space, achieving

high-quality image generation without

the need for discrete

tokenization.

[179] 蒙面

自回旋（MAR）[29]模型

在连续值的

空间中采用

基于扩散损

失的自回旋

框架，实现了

高质量的图

像生成而无

需离散令牌

化。

--------------------------------------------------



[180] The

MAR-Large model, with 32

transformer blocks, serves as

the baseline for comparison.

[180] 具有32个变

压器块的MAR-LARGE模

型是比较的

基线。



--------------------------------------------------

[181] Applying our pruning

method, we reduced MAR

to a 16-block variant,

TinyMAR-D16, achieving an FID

of 2.28 and surpassing

the performance of the

24-block MAR-Base model with

only 10% of the

original training cost (40

epochs vs. 400 epochs).

[181] 应用我

们的修剪方

法，我们将MAR降

低到16块变体

Tinymar-D16，达到2.28的FID，并超

过了24块MAR-BASE模型

的性能，只有

10％的原始训练

成本（40个时期

）（40个时期与400个

时期）。



--------------------------------------------------

[182] Our ap- proach

also generalizes to Scalable

Interpolant Transform- ers (SiT)

[34], an extension of

the DiT architecture that

employs a flow-based interpolant

framework to bridge data

100 101 Calibration Loss

0.00 0.25 0.50 0.75

1.00 1.25 1.50 1.75

2.00 Density Min: 0.195

Max: 37.694 Std: 1.300

Min: 0.195 Max: 37.694

Std: 1.300

Oracle Learnable

ShortGPT

Sensitivity Flux-Lite

Figure 5.

[182]

Our ap- proach also

generalizes to Scalable Interpolant

Transform- ers (SiT) [34],

an extension of the

DiT architecture that employs

a flow-based interpolant framework

to bridge data 100

101 Calibration Loss 0.00

0.25 0.50 0.75 1.00

1.25 1.50 1.75 2.00

Density Min: 0.195 Max:

37.694 Std: 1.300 Min:

0.195 Max: 37.694 STD：1.300

Oracle可学习

的短时敏感

性通量 -  lux-lite图5。

--------------------------------------------------



[183] Distribution

of calibration loss through

random sampling of candidate

models.

[183] 通

过随机抽样

候选模型的

校准损失分

布。

--------------------------------------------------



[184] The

proposed learnable method achieves

the best post-fine-tuning FID

yet has a relatively

high initial loss com-

pared to other baselines.

[184] 所提出的

可学习方法

实现了最佳

的验证后FID，但

具有相对较

高的初始损

失与其他基

线相比。



--------------------------------------------------

[185] Strategy Loss IS

FID Prec.

[185] 策略

损失是FID

PREC。



--------------------------------------------------

[186] Recall Max.

[186]

回想

最大。



--------------------------------------------------

[187] Loss 37.69 NaN

NaN NaN NaN Med.

[187] 损失37.69 in Med。

--------------------------------------------------



[188] Loss

0.99 149.51 6.45 0.78

0.53 Min.

[188] 损

失0.99

149.51 6.45 0.78 0.53分钟。

--------------------------------------------------



[189] Loss

0.20 73.10 20.69 0.63

0.58 Sensitivity 0.21 70.36

21.15 0.63 0.57 ShortGPT

[36] 0.20 66.10 22.28

0.63 0.56 Flux-Lite [6]

0.85 54.54 25.92 0.62

0.55 Oracle (BK-SDM) 1.28

141.18 7.43 0.75 0.55

Learnable 0.98 151.88 5.73

0.80 0.55 Table 3.

[189] 损失

0.20 73.10 20.69

0.63 0.58灵敏度0.21 70.36 21.15

0.63 0.57短时

间[36] 0.20 66.10

22.28 0.63 0.56 Flux-Lite

[6] 0.85 54.54 54.54

25.92 0.62 0.62 0.55

Oracle（BK-SDM） 151.88 5.73 0.80

0.55表3。



--------------------------------------------------

[190] Directly minimizing the

calibration loss may lead

to non-optimal solutions.

[190]

直接最

大程度地减

少校准损失

可能导致非

最佳解决方

案。



--------------------------------------------------

[191] All pruned models

are fine-tuned without knowledge

distillation (KD) for 100K

steps.

[191] 所有修剪

模型均经过

微调，没有知

识蒸馏（KD）100K步骤

。

--------------------------------------------------



[192] We

evaluate the fol- lowing

baselines: (1) Loss –

We randomly prune a

DiT-XL model to generate

100,000 models and select

models with different cali-

bration losses for fine-tuning;

(2) Metric-based Methods –

such as Sensitivity Analysis

and ShortGPT; (3) Oracle

– We retain the

first and last layers

while uniformly pruning the

intermediate layers fol- lowing

[23]; (4) Learnable –

The proposed learnable method.

[192] 我们评估了

以下基准：（1）损

失 - 我们随机

修剪DIT-XL模型，以

生成100,000款模型

，并选择具有

不同卡路损

失的模型以

进行微调；

（2）基

于度量的方

法 - 例如灵敏

度分析和短

程； （3）Oracle

- 我们保留

了第一层也

是最后一层

，同时均匀修

剪了中间层

[23]； （4）可学习 -

拟议

的可学习方

法。



--------------------------------------------------

[193] and noise distributions.

[193] 和噪声分

布。



--------------------------------------------------

[194] The SiT-XL/2 model,

comprising 28 transformer blocks,

was pruned by 50%,

creating the TinySiT-D14 model.

[194] SIT-XL/2模型，包括

28个变压器块

，由50％修剪，创建

TinySit-D14模型。



--------------------------------------------------

[195] This pruned model

retains competi- tive performance

at only 7% of

the original training cost

(100 epochs vs. 1400

epochs).

[195] 该修剪

模型仅以原

始培训成本

的7％（100个时代与

1400个时代）保留

竞争性能。

--------------------------------------------------



[196] As

shown in Table 2,

these results demonstrate that

our pruning method is

adaptable across different diffusion

transformer variants, effectively reducing

the model size and

training time while maintain-

ing strong performance.

[196]

如

表2所示，这些

结果表明，我

们的修剪方

法在不同的

扩散变压器

变体中具有

适应性的适

应性，从而有

效地减少了

模型的大小

和训练时间

，同时保持强

劲的性能。



--------------------------------------------------

[197] 4.3.

[197] 4.3。

--------------------------------------------------



[198] Analytical

Experiments Is Calibration Loss

the Primary Determinant?

[198]

分

析实验是校

准损失的主

要决定因素

吗？



--------------------------------------------------

[199] An es- sential

question in depth pruning

is how to identify

re- dundant layers in

pre-trained diffusion transformers.

[199]

深度修剪

的一个问题

是如何识别

预先训练的

扩散变压器

中的重复层

。



--------------------------------------------------

[200] A common approach

involves minimizing the calibration

loss, based on the

assumption that a model

with lower calibra- tion

loss after pruning will

exhibit superior performance.

[200]

一种共同的

方法涉及最

大程度地减

少校准损失

，这是基于这

样的假设：修

剪后较低碳

纤维损失的

模型将表现

出较高的性

能。



--------------------------------------------------

[201] However, we demonstrate

in this section that

this hypothesis may not

hold for diffusion transformers.

[201] 但是，我们

在本节中证

明，该假设可

能不适合扩

散变压器。



--------------------------------------------------

[202] We begin by

ex- amining the solution

space through random depth

pruning at a 50%

ratio, generating 100,000 candidate

models with 6

[202]

我

们首先通过

以50％的比例来

扫描解决方

案空间，以6的

比例产生100,000个

候选模型



--------------------------------------------------

[203] Pattern ∆W IS

↑ FID ↓ sFID

↓ Prec.

[203] 模

式∆W是↑fid↓SFID↓PREC。

--------------------------------------------------



[204] ↑

Recall ↑ 1:2 LoRA

54.75 33.39 29.56 0.56

0.62 2:4 LoRA 53.07

34.21 27.61 0.55 0.63

7:14 LoRA 34.97 49.41

28.48 0.46 0.56 1:2

Full 53.11 35.77 32.68

0.54 0.61 2:4 Full

53.63 34.41 29.93 0.55

0.62 7:14 Full 45.03

38.76 31.31 0.52 0.62

1:2 Frozen 45.08 39.56

31.13 0.52 0.60 2:4

Frozen 48.09 37.82 31.91

0.53 0.62 7:14 Frozen

34.09 49.75 31.06 0.46

0.56 Table 4.

[204]

↑ Recall ↑ 1:2

LoRA 54.75 33.39 29.56

0.56 0.62 2:4 LoRA

53.07 34.21 27.61 0.55

0.63 7:14 LoRA 34.97

49.41 28.48 0.46 0.56

1:2 Full 53.11 35.77

32.68 0.54 0.61 2:4

Full 53.63 34.41 29.93

0.55 0.62 7:14 Full

45.03 38.76 31.31 0.52

0.62 1:2 Frozen 45.08

39.56 31.13 0.52 0.60

2:4 Frozen 48.09 37.82

31.91 0.53 0.62 7:14

Frozen 34.09 49.75 31.06

0.46 0.56 Table 4.

--------------------------------------------------



[205] Performance

comparison of TinyDiT-D14 models

com- pressed using various

pruning schemes and recoverability

estima- tion strategies.

[205]

使用各

种修剪方案

和可恢复性

估计策略组

合的TinyDit-D14模型的

性能比较。



--------------------------------------------------

[206] All models are

fine-tuned for 10,000 steps,

and FID scores are

computed on 10,000 sampled

images with 64 timesteps.

[206] 所

有型号均以

10,000个步骤进行

微调，并在10,000个

带有64个时间

步长的采样

图像上计算

FID分数。



--------------------------------------------------

[207] calibration losses ranging

from 0.195 to 37.694

(see Fig- ure 5).

[207] 校准损

失范围为0.195至

37.694（见图5）。



--------------------------------------------------

[208] From these candidates,

we select models with

the highest and lowest

calibration losses for fine-tuning.

[208] 从这些

候选人中，我

们选择具有

最高和最低

校准损失的

模型以进行

微调。



--------------------------------------------------

[209] No- tably, both

models result in unfavorable

outcomes, such as unstable

training (NaN) or suboptimal

FID scores (20.69), as

shown in Table 3.

[209] 如表3所

示，两种模型

都没有造成

不利的结果

，例如不稳定

的训练（NAN）或次

优FID得分（20.69）。



--------------------------------------------------

[210] Additionally, we conduct

a sensitiv- ity analysis

[18], a commonly used

technique to identify crucial

layers by measuring loss

disturbance upon layer re-

moval, which produces a

model with a low

calibration loss of 0.21.

[210] 此外

，我们进行了

灵敏分析[18]，这

是一种常用

的技术，可以

通过测量层

层次的损失

障碍来识别

关键层，该层

损失层的损

失，该模型产

生了一个模

型，低校准损

失为0.21。



--------------------------------------------------

[211] However, this model’s

FID score is similar

to that of the

model with the lowest

calibration loss.

[211] 但是，该

模型的FID得分

与校准损失

最低的模型

的得分相似

。

--------------------------------------------------



[212] Approaches

like ShortGPT [36] and

a recent approach for

compressing the Flux model

[6], which estimate similarity

or minimize mean squared

error (MSE) between input

and output states, reveal

a similar trend.

[212]

诸如Shortgpt [36]和最近

压缩通量模

型[6]的方法，该

方法估算了

输入和输出

状态之间的

平均平方误

差（MSE），从而揭示

了相似的趋

势。



--------------------------------------------------

[213] In contrast, methods

with mod- erate calibration

losses, such as Oracle

(often considered less competitive)

and one of the

randomly pruned models, achieve

FID scores of 7.43

and 6.45, respectively, demon-

strating significantly better performance

than models with minimal

calibration loss.

[213] 相比之下

，具有模块化

校准损失的

方法，例如Oracle（通

常认为竞争

力较低）和随

机修剪的模

型之一，分别

达到7.43和6.45的FID得

分，比具有最

小校准损失

的模型的模

型明显更好

。

--------------------------------------------------



[214] These

findings suggest that, while

calibration loss may influence

post-fine-tuning performance to some

extent, it is not

the primary determinant for

diffu- sion transformers.

[214]

这些发现表

明，虽然校准

损失可能在

某种程度上

会影响预定

后的性能，但

它并不是扩

散变压器的

主要决定因

素。



--------------------------------------------------

[215] Instead, the model’s

capacity for perfor- mance

recovery during fine-tuning, termed

“recoverability,” appears to be

more critical.

[215] 取而代之

的是，该模型

在微调过程

中恢复性能

的能力称为

“可恢复性”，似

乎更为关键

。

--------------------------------------------------



[216] Notably,

assessing recoverabil- ity using

traditional metrics is challenging,

as it requires a

learning process across the

entire dataset.

[216] 值得注意的

是，使用传统

指标评估恢

复性是具有

挑战性的，因

为它需要在

整个数据集

中进行学习

过程。

--------------------------------------------------



[217] This

observation also explains why

the proposed method achieves

superior results (5.73) compared

to baseline methods.

[217]

该观察

结果还解释

了为什么所

提出的方法

与基线方法

相比取得了

优越的结果

（5.73）。



--------------------------------------------------

[218] Learnable Modeling of

Recoverability.

[218] 可恢复性的

可学习建模

。

--------------------------------------------------



[219] To

overcome the limitations of

traditional metric-based methods, this

study introduces a learnable

approach to jointly optimize

pruning and model recoverability.

[219] 为了克服传

统基于指标

的方法的局

限性，本研究

引入了一种

可学习的方

法，可以共同

优化修剪和

建模可回收

性。



--------------------------------------------------

[220] Table 3 illustrates

dif- ferent configurations of

the learnable method, including

the local pruning scheme

and update strategies for

recoverabil- ity estimation.

[220]

表3说明了

可学习方法

的不同配置

，包括局部修

剪方案和更

新以恢复估

计的策略。



--------------------------------------------------

[221] For a 28-layer

DiT-XL/2 with a fixed

50% 0 2000 4000

6000 8000 10000 Train

iterations 0 1 2

3 4 5 6

7 8 9 10

11 12 13 14

15 16 17 18

19 20 21 22

23 24 25 26

27 Layer Index in

DiT-XL Figure 6.

[221]

对

于带有固定

50％0 2000 4000 6000

8000火车迭代的

28层DIT-XL/2，0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 22

23 23 24 26

26 27 27 dit-XL中的层指

数图6。

--------------------------------------------------



[222] Visualization

of the 2:4 decisions

in the learnable prun-

ing, with the confidence

level of each decision

highlighted through varying degrees

of transparency.

[222] 2：4在可学

习的修剪中

的决定的可

视化，每个决

策的置信度

都通过不同

程度的透明

度强调。

--------------------------------------------------



[223] More

visualization results for 1:2

and 7:14 schemes are

available in the appendix.

[223] 附录

中提供了1：2和

7:14方案的更多

可视化结果

。



--------------------------------------------------

[224] layer pruning rate,

we examine three splitting

schemes: 1:2, 2:4, and

7:14.

[224] 层修剪率，我

们检查了三

个分裂方案

：1：2、2：4和7：14。

--------------------------------------------------



[225] In

the 1:2 scheme, for

example, every two transformer

layers form a local

block, with one layer

pruned.

[225] 例如，在1：2方

案中，每两个

变压器层形

成一个局部

块，并修剪一

层。

--------------------------------------------------



[226] Larger

blocks introduce greater diversity

but sig- nificantly expand

the search space.

[226]

较大的块

引入了更大

的多样性，但

很明显地扩

大了搜索空

间。



--------------------------------------------------

[227] For instance, the

7:14 scheme divides the

model into two segments,

each retain- ing 7

layers, resulting in  14

7  × 2

= 6,864 possible solu-

tions.

[227] 例如，7:14方案

将模型划分

为两个段，每

个片段保留

了7层，导致14 7×2

= 6,864可

能的解决方

案。



--------------------------------------------------

[228] Conversely, smaller blocks

significantly reduce op- timization

difficulty and offer greater

flexibility.

[228] 相反，较小

的块大大减

少了操作困

难，并提供了

更大的灵活

性。

--------------------------------------------------



[229] When

the distribution of one

block converges, the learning

on other blocks can

still progress.

[229] 当一个块

的分布收敛

时，其他块上

的学习仍然

可以进步。

--------------------------------------------------



[230] As

shown in Table 3,

the 1:2 con- figuration

achieves the optimal performance

after 10K fine- tuning

iterations.

[230] 如

表3所示，1：2的形

象在10K细调迭

代后达到了

最佳性能。

--------------------------------------------------



[231] Additionally,

our empirical findings un-

derscore the effectiveness of

recoverability estimation using LoRA

or full fine-tuning.

[231]

此

外，我们的经

验发现未解

决使用LORA或完

整微调可恢

复性估计的

有效性。



--------------------------------------------------

[232] Both methods yield

positive post- fine-tuning outcomes,

with LoRA achieving superior

results (FID = 33.39)

compared to full fine-tuning

(FID = 35.77) under

the 1:2 scheme, as

LoRA has fewer trainable

parame- ters (0.9% relative

to full parameter training)

and can adapt more

efficiently to the randomness

of sampling.

[232] 两种

方法都产生

正面调查结

果，与1：2方案下

的全面微调

（FID

= 35.77）相比，洛拉取

得了较高的

结果（FID = 33.39），因为洛

拉（Lora）具有较少

的可训练的

参数（相对于

完全参数训

练），并且可以

更有效地适

应采样的随

机性。

--------------------------------------------------



[233] Visualization

of Learnable Decisions.

[233]

可学习

的可学习决

策。



--------------------------------------------------

[234] To gain deeper

in- sights into the

role of the learnable

method in pruning, we

visualize the learning process

in Figure 6.

[234]

为了更深

入地了解可

学习方法在

修剪中的作

用，我们将图

6中的学习过

程可视化。



--------------------------------------------------

[235] From bottom to

top, the i-th curve

represents the i-th layer

of the pruned model,

displaying its layer index

in the original DiT-XL/2.

[235] 从

底部到顶部

，第i-th曲线代表

修剪模型的

第I层，在原始

DIT-XL/2中显示其层

索引。



--------------------------------------------------

[236] This visualization illustrates

the dynamics of pruning

de- cisions over training

iterations, where the transparency

of each data point

indicates the probability of

being sampled.

[236] 这种可

视化说明了

修剪训练迭

代的修剪效

果的动力学

，其中每个数

据点的透明

度表明被采

样的概率。

--------------------------------------------------



[237] The

learnable method shows its

capacity to explore and

handle various layer combinations.

[237] 可

学习的方法

显示了其探

索和处理各

种层组合的

能力。



--------------------------------------------------

[238] Pruning decisions for

certain layers, such as

the 7-th and 8-th

in the compressed model,

are determined quickly and

remain stable through- out

the process.

[238] 对某些

层的修剪决

策，例如压缩

模型中的7-三

分之一和第

8-三，并在整个

过程中保持

稳定。

--------------------------------------------------



[239] In

contrast, other layers, like

the 0-th layer, require

additional fine-tuning to estimate

their recoverabil- ity.

[239]

相比之

下，其他层（如

第0层）需要进

行其他微调

来估算其恢

复性。



--------------------------------------------------

[240] Notably, some decisions

may change in the

later stages 7

[240]

值得注

意的是，某些

决定可能会

在后期第7阶

段发生变化

--------------------------------------------------



[241]

Figure 7.

[241] 图7。

--------------------------------------------------



[242] Images

generated by TinyDiT-D14 on

ImageNet 224×224, pruned and

distilled from a DiT-XL/2.

[242] 由Tinydit-d14在Imagenet 224×224上产

生的图像，从

DIT-XL/2进行修剪和

蒸馏。

--------------------------------------------------



[243] 102

101 100 0 100

101 102 Activation Value

(log) 0.0 0.1 0.2

0.3 0.4 Density

Max Activation: 191.20

Min Activation: -429.01 +Std:

12.54 -Std: -12.54 (a)

DiT-XL/2 (Teacher) 102 101

100 0 100 101

Activation Value (log) 0.0

0.1 0.2 0.3 0.4

0.5 Density  Max

Activation: 53.77  Min

Activation: -526.62 +Std: 14.15

-Std: -14.15 (b) TinyDiT-D14

(Student) Figure 8.

[243]

102 101 100 0

100 101 102 Activation

Value (log) 0.0 0.1

0.2 0.3 0.4 Density

Max Activation: 191.20

Min Activation: -429.01 +Std:

12.54 -Std: -12.54 (a)

DiT-XL/2 (Teacher) 102 101

100 0 100 101

Activation Value (log) 0.0

0.1 0.2 0.3 0.4

0.5 Density  Max

Activation: 53.77  Min

Activation: -526.62 +STD：14.15 -STD：-14.15（b）Tinydit

-D14（学生）图

8。



--------------------------------------------------

[244] Visualization of massive

activations [47] in DiTs.

[244] 大规模激活

的可视化[47]。



--------------------------------------------------

[245] Both teacher and

student models display large

activation values in their

hidden states.

[245] 教

师和学生模

型在其隐藏

状态下都显

示出较大的

激活值。

--------------------------------------------------



[246] Directly

distilling these massive activations

may result in excessively

large losses and unstable

training.

[246] 直接

提炼这些大

规模激活可

能会导致过

度损失和训

练不稳定。

--------------------------------------------------



[247] once

these layers have been

sufficiently optimized.

[247] 一

旦这些层得

到充分优化

。

--------------------------------------------------



[248] The

training process ultimately concludes

with high sampling probabilities,

suggesting a converged learning

process with distributions approaching

a one-hot configuration.

[248]

培训过程最

终以高采样

概率结束，这

表明通过分

布接近单次

配置的分布

进行了融合

的学习过程

。



--------------------------------------------------

[249] After training, we

select the layers with

the highest probabilities for

subsequent fine-tuning.

[249] 训练后，我们

选择具有最

高概率的层

以后进行微

调。

--------------------------------------------------



[250] 4.4.

[250] 4.4。



--------------------------------------------------

[251] Knowledge Distillation for

Recovery In this work,

we also explore Knowledge

Distillation (KD) as an

enhanced fine-tuning method.

[251]

在这项工

作中恢复的

知识蒸馏，我

们还探索了

知识蒸馏（KD）作

为增强的微

调方法。



--------------------------------------------------

[252] As demonstrated in

Ta- ble 5, we

apply the vanilla knowledge

distillation approach proposed by

Hinton [20] to fine-tune

a TinyDiT-D14, using the

outputs of the pre-trained

DiT-XL/2 as a teacher

model for supervision.

[252]

如Table 5中

所示，我们使

用Hinton [20]提出的香

草知识蒸馏

方法对TinyDit-D14进行

微调，使用预

先训练的DIT-XL/2的

输出作为监

督的教师模

型。

--------------------------------------------------



[253] We

employ a Mean Square

Error (MSE) loss to

align the outputs between

the shallow student model

and the deeper teacher

model, which effectively reduces

the FID at 100K

steps from 5.79 to

4.66.

[253] 我们采用

均方误差（MSE）损

失来对齐浅

层学生模型

和更深层的

教师模型之

间的输出，从

而有效地将

100K步骤的FID从5.79降

低到4.66。

--------------------------------------------------



[254] Masked

Knowledge Distillation.

[254] 掩盖的

知识蒸馏。

--------------------------------------------------



[255] Additionally,

we eval- uate representation

distillation (RepKD) [23, 42]

to transfer hidden states

from the teacher to

the student.

[255] 此

外，我们评估

表示代表蒸

馏（REPKD）[23，42]将隐藏状

态从老师转

移到学生。

--------------------------------------------------



[256] It

is important to note

that depth pruning does

not alter the hidden

dimen- sion of diffusion

transformers, allowing for direct

alignment fine-tuning Strategy Init.

[256] 重

要的是要注

意，深度修剪

不会改变扩

散变压器的

隐藏尺寸，从

而允许直接

对齐微调策

略init。



--------------------------------------------------

[257] Distill.

[257] 蒸馏。

--------------------------------------------------



[258] Loss

FID @ 100K fine-tuning

- 5.79 Logits KD

- 4.66 RepKD 2840.1

NaN Masked KD (0.1σ)

15.4 NaN Masked KD

(2σ) 387.1 3.73 Masked

KD (4σ) 391.4 3.75

Table 5.

[258] 损失

FID

@ 100k微调-5.79 logits KD

-4.66 REPKD 2840.1 NAN蒙版KD（0.1σ）15.4

Nan蒙

版KD（2σ）387.1 3.73蒙版KD（4σ）391.4 3.75表5。

--------------------------------------------------



[259] Evaluation

of different fine-tuning strategies

for recovery.

[259] 评

估不同的微

调策略的恢

复策略。

--------------------------------------------------



[260] Masked

RepKD ignores those massive

activations (|x| > kσx)

in both teacher and

student, which enables effective

knowledge transfer between diffusion

transformers.

[260] 蒙面

的repkd忽略了教

师和学生中

的那些大规

模激活（| x

|>kσx），这可

以在扩散变

压器之间有

效地传递知

识转移。



--------------------------------------------------

[261] of intermediate hidden

states.

[261] 中间

隐藏状态。

--------------------------------------------------



[262] For

practical implementation, we use

the block defined in

Section 3.2 as the

basic unit, ensuring that

the pruned local structure

in the pruned DiT

aligns with the output

of the original structure

in the teacher model.

[262] 对

于实际实施

，我们将第3.2节

中定义的块

作为基本单

元，以确保修

剪的DIT中修剪

的本地结构

与教师模型

中原始结构

的输出保持

一致。



--------------------------------------------------

[263] However, we encountered

significant training dif- ficulties

with this straightforward RepKD

approach due to massive

activations in the hidden

states, where both teacher

and student models occasionally

exhibit large activation values,

as shown in Figure

8.

[263] 但是，由

于隐藏状态

中的大量激

活，我们遇到

了这种直接

的repkd方法遇到

了重要的训

练困难，在这

些培训状态

下，教师和学

生模型偶尔

都会表现出

较大的激活

值，如图8所示

。

--------------------------------------------------



[264] Directly

distilling these ex- treme

activations can result in

excessively high loss values,

impairing the performance of

the student model.

[264]

直接提取这

些典型的激

活可能会导

致过高的损

失值，从而损

害学生模型

的表现。



--------------------------------------------------

[265] This issue has

also been observed in

other transformer-based genera- tive

models, such as certain

LLMs [47].

[265] 在其

他基于变压

器的属性模

型（例如某些

LLMS）中也观察到

了这个问题

[47]。

--------------------------------------------------



[266] To

address this, we propose

a Masked RepKD variant

that selectively ex- cludes

these massive activations during

knowledge transfer.

[266] 为了解决这

个问题，我们

提出了一个

掩盖的repkd变体

，该变体在知

识转移过程

中有选择地

表达了这些

大规模激活

。

--------------------------------------------------



[267] We

employ a simple thresholding

method, |x −µx| <

kσx, which ignores the

loss associated with these

extreme acti- vations.

[267]

我们采用一

种简单的阈

值方法，| x -µx |

<kσx，忽略

与这些极端

作用相关的

损失。



--------------------------------------------------

[268] As shown in

Table 5, the Masked

RepKD approach with moderate

thresholds of 2σ and

4σ achieves satisfactory results,

demonstrating the robustness of

our method.

[268] 如表5所

示，带有2σ和4σ的

中等阈值的

掩盖repkd方法可

实现令人满

意的结果，证

明了我们方

法的鲁棒性

。

--------------------------------------------------



[269] Generated

Images.

[269] 生成的图像

。

--------------------------------------------------



[270] In

Figure 7, We visualize

the gener- ated images

of the learned TinyDiT-D14,

distilled from an 8

[270] 在图7中，我们

可视化了从

8



--------------------------------------------------

[271] off-the-shelf DiT-XL/2 model.

[271] 现成的DIT-XL/2型号

。



--------------------------------------------------

[272] More visualization results

for SiTs and MARs

can be found in

the appendix.

[272] 可以在附录

中找到更多

的坐姿和火

星的可视化

结果。

--------------------------------------------------



[273] 5.

[273] 5。



--------------------------------------------------

[274] Conclusions This work

introduces TinyFusion, a learnable

method for accelerating diffusion

transformers by removing redundant

layers.

[274] 结论这

项工作引入

了TinyFusion，这是一种

通过去除冗

余层来加速

扩散变压器

的可学习方

法。

--------------------------------------------------



[275] It

models the recoverability of

pruned models as an

optimizable objective and incorporates

differentiable sam- pling for

end-to-end training.

[275] 它将修剪

模型的可恢

复性建模为

一个优化的

目标，并结合

了端到端训

练的可区分

套件。

--------------------------------------------------



[276] Our

method generalizes to various

architectures like DiTs, MARs

and SiTs.

[276] 我们的

方法概括为

诸如DIT，火星和

坐着的各种

体系结构。

--------------------------------------------------



[277] References

[1] Fan Bao, Shen

Nie, Kaiwen Xue, Yue

Cao, Chongxuan Li, Hang

Su, and Jun Zhu.

[277] References [1] Fan

Bao, Shen Nie, Kaiwen

Xue, Yue Cao, Chongxuan

Li, Hang Su, and

Jun Zhu.



--------------------------------------------------

[278] All are worth

words: A vit backbone

for diffusion models.

[278]

所

有都是值得

的词：扩散模

型的VIT骨干。



--------------------------------------------------

[279] In Proceedings of

the IEEE/CVF con- ference

on computer vision and

pattern recognition, pages 22669–22679,

2023.

[279] 在

计算机视觉

和模式识别

的IEEE/CVF会议论文

集中，第22669–22679页，2023年

。

--------------------------------------------------



[280] [2]

Yoshua Bengio, Nicholas L´eonard,

and Aaron Courville.

[280]

[2] Yoshua Bengio，Nicholas L´eonard和Aaron

Courville。



--------------------------------------------------

[281] Estimating or propagating

gradients through stochastic neurons

for conditional computation.

[281]

通过随机

神经元估算

或传播梯度

以进行条件

计算。



--------------------------------------------------

[282] arXiv preprint arXiv:1308.3432,

2013.

[282] ARXIV预印型

ARXIV：1308.3432，2013。

--------------------------------------------------



[283] [3]

Thibault Castells, Hyoung-Kyu Song,

Bo-Kyeong Kim, and Shinkook

Choi.

[283] [3] Castels，Casels，Bo-Kyuong

Ki和Shinkyok Choi。



--------------------------------------------------

[284] Ld-pruner: Efficient pruning

of latent diffu- sion

models using task-agnostic insights.

[284] LD-PRUNER：使用任务

不合时宜的

见解对潜在

扩散模型的

有效修剪。



--------------------------------------------------

[285] In Proceedings of

the IEEE/CVF Conference on

Computer Vision and Pattern

Recognition, pages 821–830, 2024.

[285] 在

IEEE/CVF计算机视觉

和模式识别

会议论文集

，第821-830页，2024年。



--------------------------------------------------

[286] [4] Huiwen Chang,

Han Zhang, Lu Jiang,

Ce Liu, and William

T Freeman.

[286] [4]

Huiwen Chang, Han Zhang,

Lu Jiang, Ce Liu,

and William T Freeman.

--------------------------------------------------



[287] Maskgit:

Masked generative image transformer.

[287] MaskGit：掩盖

的生成图像

变压器。



--------------------------------------------------

[288] In Conference on

Computer Vision and Pattern

Recognition, pages 11315–11325, 2022.

[288] 在计

算机视觉和

模式识别会

议上，第11315–11325页，2022年

。



--------------------------------------------------

[289] [5] Junsong Chen,

Jincheng Yu, Chongjian Ge,

Lewei Yao, Enze Xie,

Yue Wu, Zhongdao Wang,

James Kwok, Ping Luo,

Huchuan Lu, and Zhenguo

Li.

[289] [5] Junsong

Chen, Jincheng Yu, Chongjian

Ge, Lewei Yao, Enze

Xie, Yue Wu, Zhongdao

Wang, James Kwok, Ping

Luo, Huchuan Lu, and

Zhenguo Li.



--------------------------------------------------

[290] Pixart-α: Fast training

of dif- fusion transformer

for photorealistic text-to-image synthesis,

2023.

[290] PixArt-α：对影像学文

本对图像合

成的差异变

压器的快速

训练，2023年。

--------------------------------------------------



[291] [6]

Javier Mart´ın Daniel Verd´u.

[291] [6]哈维

尔·马特·丹尼

尔·维尔德·乌

。



--------------------------------------------------

[292] Flux.1 lite: Distilling

flux1.dev for efficient text-to-image

generation.

[292] Flux.1 Lite：蒸馏Flux1.Dev，以进行

有效的文本

对图像生成

。

--------------------------------------------------



[293] 2024.

[293] 2024。



--------------------------------------------------

[294] [7] Tri Dao,

Dan Fu, Stefano Ermon,

Atri Rudra, and Christo-

pher R´e.

[294] [7]

Tri Dao，Dan Fu，Stefano Ermon，Atri

Rudra和Christo-Pher R´e。



--------------------------------------------------

[295] Flashattention: Fast and

memory-efficient exact at- tention

with io-awareness.

[295] 闪存：具有

IO意识的快速

和记忆效率

精确。

--------------------------------------------------



[296] Advances

in Neural Information Processing

Systems, 35:16344–16359, 2022.

[296]

神经信

息处理系统

的进展，35：16344–16359，2022。



--------------------------------------------------

[297] [8] Jia Deng,

Wei Dong, Richard Socher,

Li-Jia Li, Kai Li,

and Li Fei-Fei.

[297]

[8] Jia Deng, Wei

Dong, Richard Socher, Li-Jia

Li, Kai Li, and

Li Fei-Fei.



--------------------------------------------------

[298] Imagenet: A large-scale

hierarchical image database.

[298]

ImageNet：大规

模分层图像

数据库。



--------------------------------------------------

[299] In 2009 IEEE

conference on computer vision

and pattern recognition, pages

248–255.

[299] 在2009年

IEEE计算机视觉

和模式识别

会议上，第248-255页

。

--------------------------------------------------



[300] Ieee,

2009.

[300] IEEE，2009年。

--------------------------------------------------



[301] [9]

Prafulla Dhariwal and Alexander

Nichol.

[301] [9] Prafulla

Dhariwal和Alexander Nichol。



--------------------------------------------------

[302] Diffusion models beat

gans on image synthesis.

[302] 扩散模

型在图像合

成上击败了

gan。



--------------------------------------------------

[303] Advances in neural

informa- tion processing systems,

34:8780–8794, 2021.

[303] 神经信息处

理系统的进

展，34：8780–8794，2021。

--------------------------------------------------



[304] [10]

Maha Elbayad, Jiatao Gu,

Edouard Grave, and Michael

Auli.

[304] [10] Maha

Elbayad，Jiatao Gu，Edouard Grave和Michael Auli。

--------------------------------------------------



[305] Depth-adaptive

transformer.

[305] 深度自

适应变压器

。

--------------------------------------------------



[306] arXiv

preprint arXiv:1910.10073, 2019.

[306]

ARXIV预印型ARXIV：1910.10073，2019。



--------------------------------------------------

[307] [11] Patrick Esser,

Sumith Kulal, Andreas Blattmann,

Rahim Entezari, Jonas M¨uller,

Harry Saini, Yam Levi,

Dominik Lorenz, Axel Sauer,

Frederic Boesel, et al.

[307] [11] Patrick Esser，Sumith

Kulal，Andreas Blatmann，Rahim Entezari，JonasMéuller，Harry Saini，Yam

Levi，Dominik Lorenz，Axel Sauer，Frederic Boesel等。

--------------------------------------------------



[308] Scaling

recti- fied flow transformers

for high-resolution image synthesis.

[308] 用

于高分辨率

图像合成的

缩放直流变

压器。



--------------------------------------------------

[309] In Forty-first International

Conference on Machine Learn-

ing, 2024.

[309] 在第41个

机器学习国

际会议上，2024年

。

--------------------------------------------------



[310] [12]

Gongfan Fang, Xinyin Ma,

and Xinchao Wang.

[310]

[12] Gongfan Fang, Xinyin

Ma, and Xinchao Wang.

--------------------------------------------------



[311] Structural

pruning for diffusion models.

[311] 扩散模型的

结构修剪。



--------------------------------------------------

[312] In Advances in

Neural Infor- mation Processing

Systems, 2023.

[312] 在

神经信息处

理系统的进

展中，2023年。

--------------------------------------------------



[313] [13]

Gongfan Fang, Hongxu Yin,

Saurav Muralidharan, Greg Heinrich,

Jeff Pool, Jan Kautz,

Pavlo Molchanov, and Xin-

chao Wang.

[313] [13]

Go Varlaan Cann，Hogxis，Huzv Mealtrailharan，Jeffriov，Jeva

Moolav和Xin，Chaa Caber。



--------------------------------------------------

[314] Maskllm: Learnable semi-structured

sparsity for large language

models.

[314] maskllm：大

型语言模型

的可学习半

结构化稀疏

性。

--------------------------------------------------



[315] arXiv

preprint arXiv:2409.17481, 2024.

[315]

Arxiv预印型ARXIV：2409.17481，2024。



--------------------------------------------------

[316] [14] Zhengcong Fei,

Mingyuan Fan, Changqian Yu,

Debang Li, and Junshi

Huang.

[316] [14] Zhengcong

Fei, Mingyuan Fan, Changqian

Yu, Debang Li, and

Junshi Huang.



--------------------------------------------------

[317] Scaling diffusion transformers

to 16 bil- lion

parameters.

[317] 将

扩散变压器

缩放到16个二

元参数。

--------------------------------------------------



[318] arXiv

preprint arXiv:2407.11633, 2024.

[318]

ARXIV预印

型ARXIV：2407.11633，2024。



--------------------------------------------------

[319] [15] Zhengcong Fei,

Mingyuan Fan, Changqian Yu,

Debang Li, Youqiang Zhang,

and Junshi Huang.

[319]

[15] Zhengcong Fei, Mingyuan

Fan, Changqian Yu, Debang

Li, Youqiang Zhang, and

Junshi Huang.



--------------------------------------------------

[320] Dimba: Transformer- mamba

diffusion models.

[320] DIMBA：变压器

-  Mamba扩

散模型。

--------------------------------------------------



[321] arXiv

preprint arXiv:2406.01159, 2024.

[321]

ARXIV预印

型ARXIV：2406.01159，2024。



--------------------------------------------------

[322] [16] Shanghua Gao,

Zhijie Lin, Xingyu Xie,

Pan Zhou, Ming- Ming

Cheng, and Shuicheng Yan.

[322] [16] Shanghua Gao,

Zhijie Lin, Xingyu Xie,

Pan Zhou, Ming- Ming

Cheng, and Shuicheng Yan.

--------------------------------------------------



[323] Editanything:

Empower- ing unparalleled flexibility

in image editing and

generation.

[323] 编辑：在图

像编辑和生

成中赋予无

与伦比的灵

活性。

--------------------------------------------------



[324] In

Proceedings of the 31st

ACM International Conference on

Multimedia, Demo track, 2023.

[324] 在第31届

ACM国际多媒体

会议上，演示

曲目，2023年。



--------------------------------------------------

[325] [17] Emil Julius

Gumbel.

[325] [17] Emil

Julius Gumbel。



--------------------------------------------------

[326] Statistical theory of

extreme values and some

practical applications: a series

of lectures.

[326] 极值

和一些实际

应用的统计

理论：一系列

讲座。

--------------------------------------------------



[327] US

Gov- ernment Printing Office,

1954.

[327] 美国政

府印刷办公

室，1954年。

--------------------------------------------------



[328] [18]

Song Han, Jeff Pool,

John Tran, and William

Dally.

[328] [18] Song

Han，Jeff Pool，John Tran和William Dally。

--------------------------------------------------



[329] Learn-

ing both weights and

connections for efficient neural

net- work.

[329] 学习

重量和连接

以进行有效

的神经网络

。

--------------------------------------------------



[330] Advances

in neural information processing

systems, 28, 2015.

[330]

神经信息处

理系统的进

展，2015年2月28日。



--------------------------------------------------

[331] [19] Yefei He,

Luping Liu, Jing Liu,

Weijia Wu, Hong Zhou,

and Bohan Zhuang.

[331]

[19] Yefei He, Luping

Liu, Jing Liu, Weijia

Wu, Hong Zhou, and

Bohan Zhuang.



--------------------------------------------------

[332] Ptqd: Accurate post-training

quantization for diffusion models.

[332] PTQD：扩

散模型的准

确培训量化

。



--------------------------------------------------

[333] Advances in Neural

Information Pro- cessing Systems,

36, 2024.

[333] 神经信息过

程系统的进

展，36，2024。

--------------------------------------------------



[334] [20]

Geoffrey Hinton, Oriol Vinyals,

Jeff Dean, et al.

[334] [20] Geoffrey Hinton，Orol

Vinyals，Jeff Dean等。



--------------------------------------------------

[335] Distill- ing the

knowledge in a neural

network.

[335] 在神经

网络中提炼

知识。

--------------------------------------------------



[336] arXiv

preprint arXiv:1503.02531, 2(7), 2015.

[336] ARXIV预印型

ARXIV：1503.02531，2（7），2015年。



--------------------------------------------------

[337] [21] Edward J

Hu, yelong shen, Phillip

Wallis, Zeyuan Allen- Zhu,

Yuanzhi Li, Shean Wang,

Lu Wang, and Weizhu

Chen.

[337] [21] Edward

J Hu, yelong shen,

Phillip Wallis, Zeyuan Allen-

Zhu, Yuanzhi Li, Shean

Wang, Lu Wang, and

Weizhu Chen.



--------------------------------------------------

[338] LoRA: Low-rank adaptation

of large language models.

[338] LORA：大型语言

模型的低级

改编。



--------------------------------------------------

[339] In In- ternational

Conference on Learning Representations,

2022.

[339] 在2022年学

习表征的国

际会议上。

--------------------------------------------------



[340] [22]

Eric Jang, Shixiang Gu,

and Ben Poole.

[340]

[22] Eric Jang, Shixiang

Gu, and Ben Poole.

--------------------------------------------------



[341] Categorical

reparameterization with gumbel-softmax.

[341]

使

用Gumbel-Softmax进行分类

重新聚集。



--------------------------------------------------

[342] arXiv preprint arXiv:1611.01144,

2016.

[342] ARXIV预

印型ARXIV：1611.01144，2016。

--------------------------------------------------



[343] [23]

Bo-Kyeong Kim, Hyoung-Kyu Song,

Thibault Castells, and Shinkook

Choi.

[343] （23] Chostels和Shillongs是有

福的。

--------------------------------------------------



[344] Bk-sdm:

Architecturally compressed stable diffusion

for efficient text-to-image generation.

[344] BK-SDM：结构压

缩的稳定扩

散，以实现有

效的文本对

图像生成。



--------------------------------------------------

[345] In Workshop on

Efficient Systems for Foundation

Models@ ICML2023, 2023.

[345]

在

基础模型@ ICML2023，2023的

高效系统研

讨会上。



--------------------------------------------------

[346] [24] Bo-Kyeong Kim,

Geonmin Kim, Tae-Ho Kim,

Thibault Castells, Shinkook Choi,

Junho Shin, and Hyoung-Kyu

Song.

[346] [24] Bo-Kyeong

Kim，Geonmin Kim，Tae-Ho Kim，Thibault Castels，Shillong。

--------------------------------------------------



[347] Shortened

llama: A simple depth

pruning for large lan-

guage models.

[347] 缩短

的美洲驼：大

型语言模型

的简单深度

修剪。

--------------------------------------------------



[348] arXiv

preprint arXiv:2402.02834, 11, 2024.

[348] Arxiv预印型

ARXIV：2402.02834，11，2024。



--------------------------------------------------

[349] [25] PKU-Yuan Lab

and Tuzhan AI etc.

[349] [25] PKU-YUAN LAB和TUZHAN

AI等。



--------------------------------------------------

[350] Open-sora-plan, 2024.

[350]

开放式

计划，2024年。



--------------------------------------------------

[351] [26] Black Forest

Labs.

[351] [26]黑森

林实验室。

--------------------------------------------------



[352] FLUX,

2024.

[352] Flux，2024。

--------------------------------------------------



[353] [27]

Youngwan Lee, Yong-Ju Lee,

and Sung Ju Hwang.

[353] [27] Youngwan Lee,

Yong-Ju Lee, and Sung

Ju Hwang.



--------------------------------------------------

[354] Dit- pruner: Pruning

diffusion transformer models for

text-to- image synthesis using

human preference scores.

[354]

Dit-Pruner：使

用人类偏好

得分进行修

剪扩散变压

器模型，用于

文本图像合

成。



--------------------------------------------------

[355] 9

[355] 9

--------------------------------------------------



[356] [28]

Youngwan Lee, Kwanyong Park,

Yoorhim Cho, Yong-Ju Lee,

and Sung Ju Hwang.

[356] [28] Youngwan Lee，Kwanyong

Park，Yoorhim Cho，Yong Ju Lee和Sung

Ju Hwang。



--------------------------------------------------

[357] Koala: self-attention mat-

ters in knowledge distillation

of latent diffusion models

for memory-efficient and fast

image synthesis.

[357] Koala：在潜在

扩散模型的

知识蒸馏中

进行自我注

意事项，以进

行记忆效率

和快速图像

合成。

--------------------------------------------------



[358] arXiv

e-prints, pages arXiv–2312, 2023.

[358] Arxiv电子打

印，第2312页，2023年。



--------------------------------------------------

[359] [29] Tianhong Li,

Yonglong Tian, He Li,

Mingyang Deng, and Kaiming

He.

[359] [29] Tianhong

Li, Yonglong Tian, He

Li, Mingyang Deng, and

Kaiming He.



--------------------------------------------------

[360] Autoregressive image generation

without vec- tor quantization.

[360] 自

回归图像生

成而无需量

化量化。



--------------------------------------------------

[361] arXiv preprint arXiv:2406.11838,

2024.

[361] Arxiv预印

型ARXIV：2406.11838，2024。

--------------------------------------------------



[362] [30]

Xiuyu Li, Yijiang Liu,

Long Lian, Huanrui Yang,

Zhen Dong, Daniel Kang,

Shanghang Zhang, and Kurt

Keutzer.

[362] [30] Xiuyu

Li, Yijiang Liu, Long

Lian, Huanrui Yang, Zhen

Dong, Daniel Kang, Shanghang

Zhang, and Kurt Keutzer.

--------------------------------------------------



[363] Q-diffusion:

Quantizing diffusion models.

[363]

Q扩散：量化

扩散模型。



--------------------------------------------------

[364] In Proceedings of

the IEEE/CVF International Conference

on Computer Vi- sion,

pages 17535–17545, 2023.

[364]

在

IEEE/CVF国际计算机

VI-Sion会议论文集

，第17535– 17545年，2023年。



--------------------------------------------------

[365] [31] Yanyu Li,

Huan Wang, Qing Jin,

Ju Hu, Pavlo Chemerys,

Yun Fu, Yanzhi Wang,

Sergey Tulyakov, and Jian

Ren.

[365] [31] Yanyu

Li, Huan Wang, Qing

Jin, Ju Hu, Pavlo

Chemerys, Yun Fu, Yanzhi

Wang, Sergey Tulyakov, and

Jian Ren.



--------------------------------------------------

[366] Snap- fusion: Text-to-image

diffusion model on mobile

devices within two seconds.

[366] 快速

融合：移动设

备上的文本

到图像扩散

模型在两秒

钟内。



--------------------------------------------------

[367] Advances in Neural

Information Pro- cessing Systems,

36, 2024.

[367] 神经信

息过程系统

的进展，36，2024。

--------------------------------------------------



[368] [32]

Shanchuan Lin, Anran Wang,

and Xiao Yang.

[368]

[32] Shanchuan Lin, Anran

Wang, and Xiao Yang.

--------------------------------------------------



[369] Sdxl-

lightning: Progressive adversarial diffusion

distillation.

[369] SDXL-闪电

：进行性对抗

扩散蒸馏。

--------------------------------------------------



[370] arXiv

preprint arXiv:2402.13929, 2024.

[370]

ARXIV预

印型ARXIV：2402.13929，2024。



--------------------------------------------------

[371] [33] Cheng Lu,

Yuhao Zhou, Fan Bao,

Jianfei Chen, Chongxuan Li,

and Jun Zhu.

[371]

[33] Cheng Lu, Yuhao

Zhou, Fan Bao, Jianfei

Chen, Chongxuan Li, and

Jun Zhu.



--------------------------------------------------

[372] Dpm-solver: A fast

ode solver for diffusion

probabilistic model sampling in

around 10 steps.

[372]

DPM-Solver：在大约

10个步骤中进

行扩散概率

模型采样的

快速ODE求解器

。



--------------------------------------------------

[373] Advances in Neural

Information Processing Systems, 35:5775–5787,

2022.

[373] 神经信息处

理系统的进

展，35：5775–5787，2022。

--------------------------------------------------



[374] [34]

Nanye Ma, Mark Goldstein,

Michael S Albergo, Nicholas

M Boffi, Eric Vanden-Eijnden,

and Saining Xie.

[374]

[34] Nanye Ma，Michael S

Albergo。



--------------------------------------------------

[375] Sit: Explor- ing

flow and diffusion-based generative

models with scalable interpolant

transformers.

[375] SIT：具有可扩

展的插值变

压器的探索

流量和基于

扩散的生成

模型。

--------------------------------------------------



[376] arXiv

preprint arXiv:2401.08740, 2024.

[376]

ARXIV预印型

ARXIV：2401.08740，2024。



--------------------------------------------------

[377] [35] Xinyin Ma,

Gongfan Fang, Michael Bi

Mi, and Xinchao Wang.

[377] [35] Xinyin Ma,

Gongfan Fang, Michael Bi

Mi, and Xinchao Wang.

--------------------------------------------------



[378] Learning-to-cache:

Accelerating diffusion trans- former

via layer caching, 2024.

[378] 学习到缓存

：通过层缓存

加速扩散跨

前期，2024年。



--------------------------------------------------

[379] [36] Xin Men,

Mingyu Xu, Qingyu Zhang,

Bingning Wang, Hongyu Lin,

Yaojie Lu, Xianpei Han,

and Weipeng Chen.

[379]

[36] Xin Men, Mingyu

Xu, Qingyu Zhang, Bingning

Wang, Hongyu Lin, Yaojie

Lu, Xianpei Han, and

Weipeng Chen.



--------------------------------------------------

[380] Shortgpt: Layers in

large language models are

more redun- dant than

you expect.

[380] 短期

：大语言模型

中的层次比

您预期的要

重新使用。

--------------------------------------------------



[381] arXiv

preprint arXiv:2403.03853, 2024.

[381]

Arxiv预

印型ARXIV：2403.03853，2024。



--------------------------------------------------

[382] [37] Pavlo Molchanov,

Stephen Tyree, Tero Karras,

Timo Aila, and Jan

Kautz.

[382] [37] Pavlo

Molchanov，Stephen Tyree，Tero Karras，Timo Aila和Jan

Kautz。



--------------------------------------------------

[383] Pruning convolutional neural

networks for re- source

efficient inference.

[383] 修剪

卷积神经网

络，以提高推

理。

--------------------------------------------------



[384] arXiv

preprint arXiv:1611.06440, 2016.

[384]

Arxiv预印型ARXIV：1611.06440，2016。



--------------------------------------------------

[385] [38] Zanlin Ni,

Yulin Wang, Renping Zhou,

Jiayi Guo, Jinyi Hu,

Zhiyuan Liu, Shiji Song,

Yuan Yao, and Gao

Huang.

[385] [38] Zanlin

Ni, Yulin Wang, Renping

Zhou, Jiayi Guo, Jinyi

Hu, Zhiyuan Liu, Shiji

Song, Yuan Yao, and

Gao Huang.



--------------------------------------------------

[386] Revisiting non-autoregressive transformers

for efficient im- age

synthesis.

[386] 重

新审视非自

动进取的变

压器，以进行

有效的进一

步综合。

--------------------------------------------------



[387] In

Proceedings of the IEEE/CVF

Conference on Computer Vision

and Pattern Recognition, pages

7007– 7016, 2024.

[387]

在IEEE/CVF计

算机视觉和

模式识别会

议论文集，第

7007– 7016页，2024年。



--------------------------------------------------

[388] [39] Byeongjun Park,

Sangmin Woo, Hyojun Go,

Jin-Young Kim, and Changick

Kim.

[388] [39] Byeongjun

Park，Sangmin Woo，Hyojun Go，Jin-Young Kim和Changick

Kim。



--------------------------------------------------

[389] Denoising task routing

for diffusion models.

[389]

扩散

模型的剥落

任务路由。



--------------------------------------------------

[390] arXiv preprint arXiv:2310.07138,

2023.

[390] Arxiv预

印型ARXIV：2310.07138，2023。

--------------------------------------------------



[391] [40]

William Peebles and Saining

Xie.

[391] [40]威廉·皮

布尔斯（William Peebles）和夏

威尔（Xie）。

--------------------------------------------------



[392] Scalable

diffusion models with transformers.

[392] 具有变

压器的可扩

展模型。



--------------------------------------------------

[393] In Proceedings of

the IEEE/CVF Inter- national

Conference on Computer Vision,

pages 4195–4205, 2023.

[393]

在IEEE/CVF全

国计算机视

觉会议论文

集，第4195–4205页，2023年。



--------------------------------------------------

[394] [41] David Raposo,

Sam Ritter, Blake Richards,

Timothy Lillicrap, Peter Conway

Humphreys, and Adam San-

toro.

[394] [41] David

Raposo，Sam Ritter，Blake Richards，Timothy Lillicrap，Peter

Conway Humphreys和

Adam Santoro。

--------------------------------------------------



[395] Mixture-of-depths:

Dynamically allocating com- pute

in transformer-based language models.

[395] 深入的混合

物：基于变压

器的语言模

型动态分配

计算。



--------------------------------------------------

[396] arXiv preprint arXiv:2404.02258,

2024.

[396] ARXIV预印型

ARXIV：2404.02258，2024。

--------------------------------------------------



[397] [42]

Adriana Romero, Nicolas Ballas,

Samira Ebrahimi Kahou, Antoine

Chassang, Carlo Gatta, and

Yoshua Bengio.

[397] [42]

Adriana Romero，Nicolas Ballas，Samira Jewish

Kahou，Antoine Chassang，Carlo Gatta和Yoshua Bengio。

--------------------------------------------------



[398] Fitnets:

Hints for thin deep

nets.

[398] fitnets：薄网的提

示。

--------------------------------------------------



[399] arXiv

preprint arXiv:1412.6550, 2014.

[399]

Arxiv预印型ARXIV：1412.6550，2014。



--------------------------------------------------

[400] [43] Tim Salimans

and Jonathan Ho.

[400]

[43]蒂

姆·萨利曼斯

（Tim Salimans）和乔纳森·何

（Jonathan Ho）。

--------------------------------------------------



[401] Progressive

distillation for fast sampling

of diffusion models.

[401]

用于快速采

样扩散模型

的进行性蒸

馏。



--------------------------------------------------

[402] arXiv preprint arXiv:2202.00512,

2022.

[402] ARXIV预印型ARXIV：2202.00512，2022。

--------------------------------------------------



[403] [44]

Yuzhang Shang, Zhihang Yuan,

Bin Xie, Bingzhe Wu,

and Yan Yan.

[403]

[44] Yuzhang Shang, Zhihang

Yuan, Bin Xie, Bingzhe

Wu, and Yan Yan.

--------------------------------------------------



[404] Post-training

quantization on diffusion models.

[404] 扩

散模型上的

训练后量化

。



--------------------------------------------------

[405] In Proceedings of

the IEEE/CVF conference on

computer vi- sion and

pattern recognition, pages 1972–1981,

2023.

[405] 在IEEE/CVF计算机录

像和模式识

别会议论文

集，第1972- 1981年，2023年。

--------------------------------------------------



[406] [45]

Jiaming Song, Chenlin Meng,

and Stefano Ermon.

[406]

[45] Jiaming Song, Chenlin

Meng, and Stefano Ermon.

--------------------------------------------------



[407] Denoising

diffusion implicit models.

[407]

剥

离扩散隐式

模型。



--------------------------------------------------

[408] arXiv preprint arXiv:2010.02502,

2020.

[408] ARXIV预印型

ARXIV：2010.02502，2020。

--------------------------------------------------



[409] [46]

Yang Song, Prafulla Dhariwal,

Mark Chen, and Ilya

Sutskever.

[409] [46]歌曲，Praflulla Dharill，Mark

Chen和Ilio Suskver。



--------------------------------------------------

[410] Consistency models.

[410]

一致

性模型。



--------------------------------------------------

[411] arXiv preprint arXiv:2303.01469,

2023.

[411] Arxiv预印

型ARXIV：2303.01469，2023。

--------------------------------------------------



[412] [47]

Mingjie Sun, Xinlei Chen,

J Zico Kolter, and

Zhuang Liu.

[412] [47]

Mingjie Sun, Xinlei Chen,

J Zico Kolter, and

Zhuang Liu.



--------------------------------------------------

[413] Massive activations in

large language models.

[413]

大语言模

型中的大量

激活。



--------------------------------------------------

[414] arXiv preprint arXiv:2402.17762,

2024.

[414] Arxiv预印型

ARXIV：2402.17762，2024。

--------------------------------------------------



[415] [48]

Yao Teng, Yue Wu,

Han Shi, Xuefei Ning,

Guohao Dai, Yu Wang,

Zhenguo Li, and Xihui

Liu.

[415] [48] Yao

Teng, Yue Wu, Han

Shi, Xuefei Ning, Guohao

Dai, Yu Wang, Zhenguo

Li, and Xihui Liu.

--------------------------------------------------



[416] Dim:

Diffusion mamba for efficient

high-resolution image synthesis.

[416]

DIM：扩散MAMBA，用于有

效的高分辨

率图像合成

。



--------------------------------------------------

[417] arXiv preprint arXiv:2405.14224,

2024.

[417] ARXIV预印型ARXIV：2405.14224，2024。

--------------------------------------------------



[418] [49]

Keyu Tian, Yi Jiang,

Zehuan Yuan, Bingyue Peng,

and Li- wei Wang.

[418] [49] Keyu Tian,

Yi Jiang, Zehuan Yuan,

Bingyue Peng, and Li-

wei Wang.



--------------------------------------------------

[419] Visual autoregressive modeling:

Scalable image generation via

next-scale prediction.

[419] 视觉

自回归建模

：可扩展图像

通过换句话

预测。

--------------------------------------------------



[420] 2024.

[420] 2024。



--------------------------------------------------

[421] [50] Yuchuan Tian,

Zhijun Tu, Hanting Chen,

Jie Hu, Chao Xu,

and Yunhe Wang.

[421]

[50] Yuchuan Tian, Zhijun

Tu, Hanting Chen, Jie

Hu, Chao Xu, and

Yunhe Wang.



--------------------------------------------------

[422] U-dits: Downsample tokens

in u-shaped diffusion transformers.

[422] U-Dits：U形扩散

变压器中的

下样品令牌

。



--------------------------------------------------

[423] arXiv preprint arXiv:2405.02730,

2024.

[423] Arxiv预印型ARXIV：2405.02730，2024。

--------------------------------------------------



[424] [51]

Kafeng Wang, Jianfei Chen,

He Li, Zhenpeng Mi,

and Jun Zhu.

[424]

[51] Kafeng Wang, Jianfei

Chen, He Li, Zhenpeng

Mi, and Jun Zhu.

--------------------------------------------------



[425] Sparsedm:

Toward sparse efficient diffusion

models.

[425] SparsedM：朝着

稀疏的有效

扩散模型。

--------------------------------------------------



[426] arXiv

preprint arXiv:2404.10445, 2024.

[426]

ARXIV预

印型ARXIV：2404.10445，2024。



--------------------------------------------------

[427] [52] Enze Xie,

Junsong Chen, Junyu Chen,

Han Cai, Yujun Lin,

Zhekai Zhang, Muyang Li,

Yao Lu, and Song

Han.

[427] [52] Enze

Xie, Junsong Chen, Junyu

Chen, Han Cai, Yujun

Lin, Zhekai Zhang, Muyang

Li, Yao Lu, and

Song Han.



--------------------------------------------------

[428] Sana: Ef- ficient

high-resolution image synthesis with

linear diffusion transformers.

[428]

SANA：具有线

性扩散变压

器的效率高

分辨率图像

合成。



--------------------------------------------------

[429] arXiv preprint arXiv:2410.10629,

2024.

[429] ARXIV预印型

ARXIV：2410.10629，2024。

--------------------------------------------------



[430] [53]

Ling Yang, Zhilong Zhang,

Yang Song, Shenda Hong,

Run- sheng Xu, Yue

Zhao, Wentao Zhang, Bin

Cui, and Ming- Hsuan

Yang.

[430] [53] Ling

Yang, Zhilong Zhang, Yang

Song, Shenda Hong, Run-

sheng Xu, Yue Zhao,

Wentao Zhang, Bin Cui,

and Ming- Hsuan Yang.

--------------------------------------------------



[431] Diffusion

models: A comprehensive survey

of methods and applications.

[431] 扩散模型：对

方法和应用

的全面调查

。



--------------------------------------------------

[432] ACM Computing Surveys,

56(4): 1–39, 2023.

[432]

ACM计算调查，56（4）：1–39，2023。



--------------------------------------------------

[433] [54] Fang Yu,

Kun Huang, Meng Wang,

Yuan Cheng, Wei Chu,

and Li Cui.

[433]

[54] Fang Yu, Kun

Huang, Meng Wang, Yuan

Cheng, Wei Chu, and

Li Cui.



--------------------------------------------------

[434] Width & depth

pruning for vision transformers.

[434] 视

觉变压器的

宽度和深度

修剪。



--------------------------------------------------

[435] In Conference on

Artificial Intelligence (AAAI), 2022.

[435] 在人工

智能会议上

（AAAI），2022年。



--------------------------------------------------

[436] [55] Tao Yu,

Runseng Feng, Ruoyu Feng,

Jinming Liu, Xin Jin,

Wenjun Zeng, and Zhibo

Chen.

[436] [55] Tao

Yu, Runseng Feng, Ruoyu

Feng, Jinming Liu, Xin

Jin, Wenjun Zeng, and

Zhibo Chen.



--------------------------------------------------

[437] Inpaint anything: Segment

anything meets image inpainting.

[437] 涂漆的任

何东西：段的

任何东西都

符合图像覆

盖。



--------------------------------------------------

[438] arXiv preprint arXiv:2304.06790,

2023.

[438] ARXIV预印型ARXIV：2304.06790，2023。

--------------------------------------------------



[439] [56]

Dingkun Zhang, Sijia Li,

Chen Chen, Qingsong Xie,

and Haonan Lu.

[439]

[翻

译失败]



--------------------------------------------------

[440] Laptop-diff: Layer pruning

and normalized dis- 10

[440] 笔记

本电脑木：修

剪和标准化

的层



--------------------------------------------------

[441] tillation for compressing

diffusion models.

[441] 用于压

缩扩散模型

的耕作。

--------------------------------------------------



[442] arXiv

preprint arXiv:2404.11098, 2024.

[442]

Arxiv预印

型ARXIV：2404.11098，2024。



--------------------------------------------------

[443] [57] Xuanlei Zhao,

Xiaolong Jin, Kai Wang,

and Yang You.

[443]

[57] Xuanlei Zhao, Xiaolong

Jin, Kai Wang, and

Yang You.



--------------------------------------------------

[444] Real-time video generation

with pyramid attention broad-

cast.

[444] 带有金字

塔注意的实

时视频发行

广泛。

--------------------------------------------------



[445] arXiv

preprint arXiv:2408.12588, 2024.

[445]

ARXIV预印型

ARXIV：2408.12588，2024。



--------------------------------------------------

[446] [58] Yang Zhao,

Yanwu Xu, Zhisheng Xiao,

and Tingbo Hou.

[446]

[58] Yang Zhao, Yanwu

Xu, Zhisheng Xiao, and

Tingbo Hou.



--------------------------------------------------

[447] Mobilediffusion: Subsecond text-to-image

generation on mobile devices.

[447] 动员iffusion：移动设

备上的次要

文本对图像

生成。



--------------------------------------------------

[448] arXiv preprint arXiv:2311.16567,

2023.

[448] Arxiv预印型

ARXIV：2311.16567，2023。

--------------------------------------------------



[449] [59]

Zangwei Zheng, Xiangyu Peng,

Tianji Yang, Chenhui Shen,

Shenggui Li, Hongxin Liu,

Yukun Zhou, Tianyi Li,

and Yang You.

[449]

[59] Zangwei Zheng, Xiangyu

Peng, Tianji Yang, Chenhui

Shen, Shenggui Li, Hongxin

Liu, Yukun Zhou, Tianyi

Li, and Yang You.

--------------------------------------------------



[450] Open-sora:

Democratizing efficient video production

for all, 2024.

[450]

开放式：将所

有人的高效

视频制作民

主化，2024年。



--------------------------------------------------

[451] 11

[451] 11

--------------------------------------------------



[452] TinyFusion:

Diffusion Transformers Learned Shallow

Supplementary Material 6.

[452]

小型

灌注：扩散变

压器学到的

浅补充材料

6。



--------------------------------------------------

[453] Experimental Details Models.

[453] 实验细节模

型。



--------------------------------------------------

[454] Our experiments evaluate

the effectiveness of three

models: DiT-XL, MAR-Large, and

SiT-XL.

[454] 我们的实

验评估了三

个模型的有

效性：DIT-XL，MAR-LARGE和SIT-XL。

--------------------------------------------------



[455] Diffusion

Transformers (DiTs), inspired by

Vision Transformer (ViT) principles,

process spatial inputs as

sequences of patches.

[455]

扩散

变压器（DIT），灵感

来自视觉变

压器（VIT）原理，过

程空间输入

作为斑块序

列。



--------------------------------------------------

[456] The DiT-XL model

features 28 transformer layers,

a hidden size of

1152, 16 attention heads,

and a 2 ×

2 patch size.

[456]

DIT-XL型号具有

28个变压器层

，隐藏尺寸为

1152、16个注意力头

和2×2个贴片大

小。



--------------------------------------------------

[457] It employs adaptive

layer normalization (AdaLN) to

improve training stability, comprising

675 million parameters and

trained for 1400 epochs.

[457] 它采用自

适应层归一

化（ADALN）来提高训

练稳定性，包

括6.75亿个参数

，并接受了1400个

时期的培训

。



--------------------------------------------------

[458] Masked Autoregressive models

(MARs) are diffusion transformer

variants tailored for au-

toregressive image generation.

[458]

蒙面自回旋

模型（MARS）是针对

au侵蚀图像生

成的扩散变

压器变种。



--------------------------------------------------

[459] They utilize a

continuous- valued diffusion loss

framework to generate high-quality

outputs without discrete tokenization.

[459] 他

们利用一个

连续的扩散

损失框架来

生成高质量

的输出而无

需离散的令

牌化。



--------------------------------------------------

[460] The MAR-Large model

includes 32 transformer layers,

a hidden size of

1024, 16 attention heads,

and bidirectional attention.

[460]

MAR-LARGE模型包

括32个变压器

层，隐藏大小

为1024，16个注意力

头和双向注

意。



--------------------------------------------------

[461] Like DiT, it

incorporates AdaLN for stable

training and effective to-

ken modeling, with 479

million parameters trained over

400 epochs.

[461] 像DIT一样，它

将Adaln纳入了稳

定的训练和

有效的建模

，其中4.79亿个参

数训练了400多

个时期。

--------------------------------------------------



[462] Finally,

Scalable Interpolant Transformers (SiTs)

extend the DiT framework

by introducing a flow-based

in- terpolant methodology, enabling

more flexible bridging be-

tween data and noise

distributions.

[462] 最后

，可扩展的插

值插入变压

器（SITS）通过引入

基于流动的

构造方法来

扩展DIT框架，从

而在数据和

噪声分布之

间实现了更

灵活的桥接

。

--------------------------------------------------



[463] While

architecturally identical to DiT-XL,

the SiT-XL model leverages

this inter- polant approach

to facilitate modular experimentation

with interpolant selection and

sampling dynamics.

[463] 尽管在架构

上与DIT-XL相同，但

SIT-XL模型利用了

这种间间方

法来促进使

用插值选择

和采样动力

学的模块化

实验。

--------------------------------------------------



[464] Datasets.

[464] 数据集

。



--------------------------------------------------

[465] We prepared the

ImageNet 256 × 256

dataset by applying center

cropping and adaptive resizing

to main- tain the

original aspect ratio and

minimize distortion.

[465] 我们通过施

加中心裁剪

和适应性调

整大小以使

原始长宽比

并最大程度

地减少失真

来准备Imagenet

256×256数据

集。



--------------------------------------------------

[466] The images were

then normalized to a

mean of 0.5 and

a stan- dard deviation

of 0.5.

[466] 然后将图

像标准化为

平均0.5，标准偏

差为0.5。

--------------------------------------------------



[467] To

augment the dataset, we

applied random horizontal flipping

with a probability of

0.5.

[467] 为了增

加数据集，我

们以0.5的概率

应用随机水

平翻转。

--------------------------------------------------



[468] To

accelerate training without using

Variational Autoencoder (VAE), we

pre-extracted features from the

images using a pre-trained

VAE.

[468] 为了

加速训练而

无需使用各

种自动编码

器（VAE），我们使用

预先训练的

VAE从图像中预

先提取特征

。

--------------------------------------------------



[469] The

images were mapped to

their latent representations, normalized,

and the resulting feature

arrays were saved for

direct use during training.

[469] 将图像映射

到其潜在表

示，并归一化

，并保存所得

的功能阵列

在训练过程

中直接使用

。



--------------------------------------------------

[470] Training Details The

training process began with

obtain- ing pruned models

using the proposed learnable

pruning method as illustrated

in Figure 12.

[470]

培训细节训

练过程始于

使用所提出

的可学习修

剪方法获得

修剪模型，如

图12所示。



--------------------------------------------------

[471] Pruning decisions were

made by a joint

optimization of pruning and

weight updates through LoRA

with a block size.

[471] 修剪

决定是通过

通过块大小

的洛拉（Lora）进行

修剪和重量

更新的联合

优化做出的

。



--------------------------------------------------

[472] In practice, the

block size is 2

for simplicity and the

models were trained for

100 epochs, except for

MAR, which was trained

for 40 epochs.

[472]

实际上，除了

MAR外，还为100个时

期的训练了

块大小为2，并

且对模型进

行了100个时期

的训练，而Mar进

行了40个时期

的训练。



--------------------------------------------------

[473] To enhance post-pruning

performance, the Masked Knowl-

edge Distillation (RepKD) method

was employed during the

recovery phase to transfer

knowledge from teacher mod-

0 2000 4000 6000

8000 10000 Train iterations

0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 23

24 25 26 27

Layer Index in DiT-XL

Figure 9.

[473] 为了

提高灌木后

的性能，在恢

复阶段采用

了掩盖的知

识蒸馏（REPKD）方法

，以从教师mod-

0 2000 4000 4000

4000 4000 8000 10000火

车迭代转移

知识0

1 2 3 4

5 6 5 6

7 8 9 10

11 12 13 13

13 14 15 15

15 17 17 17

17 19 20 22

22 22 22 22

22 22 22 22

22 22 27 27

27 27 27 27层在Dit-XL图

9中。

--------------------------------------------------



[474] 1:2

Pruning Decisions 0 2000

4000 6000 8000 10000

Train iterations 0 1

2 3 4 5

6 7 8 9

10 11 12 13

14 15 16 17

18 19 20 21

22 23 24 25

26 27 Layer Index

in DiT-XL Figure 10.

[474] 1：2修剪决策

0 2000 4000

6000 8000 10000火车迭代0 1

2 3 4 5

6 7 8 9

10 11 12 13

13 14 15 16

17 18 19 20

21 22 23 23

24 25 26 27

27 DIT-XL中

的层指数图

10。



--------------------------------------------------

[475] 2:4 Pruning Decisions

0 2000 4000 6000

8000 10000 Train iterations

0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

16 17 18 19

20 21 22 23

24 25 26 27

Layer Index in DiT-XL

Figure 11.

[475] 2：4修剪决策0

2000 4000 6000 8000

10000火

车迭代0 1 2 3

4 5 6 7

8 9 10 11

12 13 13 14

15 16 17 18

19 20 21 22

23 23 24 25

26 27 27 DIT-XL中的

层指数图11。

--------------------------------------------------



[476] 7:14

Pruning Decisions els to

pruned student models.

[476]

7:14修

剪决策对修

剪的学生模

型。



--------------------------------------------------

[477] The RepKD approach

aligns the output predictions

and intermediate hidden states

of the pruned and

teacher models, with further

details provided in the

following section.

[477] REPKD方法将放

置预测和修

剪和教师模

型的中间隐

藏状态保持

一致，并在下

一节中提供

了更多细节

。

--------------------------------------------------



[478] Additionally,

as Exponential Mov- ing

Averages (EMA) are updated

and used during image

generation, an excessively small

learning rate can weaken

EMA’s effect, leading to

suboptimal outcomes.

[478] 此外，随着指

数移动平均

值（EMA）在图像生

成过程中进

行了更新和

使用，因此学

习率过多会

削弱EMA的效果

，从而导致次

优结果。

--------------------------------------------------



[479] To

address this, a progressive

learning rate scheduler was

implemented to gradually halve

the learning rate throughout

training.

[479] 为了

解决这个问

题，实施了渐

进式学习率

调度程序，以

逐步将整个

培训的学习

率逐渐减半

。

--------------------------------------------------



[480] The

1

[480] 1

--------------------------------------------------



[481] Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Recoverability

Estimation Local Block Joint

Opt.

[481] 变压器层变

压器层变压

器层变压器

层变压器层

变压器层层

变压器层恢

复性估计局

部块接头选

择。

--------------------------------------------------



[482] 𝐒𝐡𝐚𝐫𝐞𝐝

𝚫𝚽  (LoRA/Full) 𝚽

Diff.

[482] 𝐒𝐡𝐚𝐫𝐞𝐝（lora/full）𝚽差异。

--------------------------------------------------



[483] Sampling

Transformer Layer Transformer Layer

Winner Decision 𝐦𝐢𝐧𝓛(𝚽+ 𝚫𝚽)

Update Update Categorical Distribution

~ Transformer Layer Transformer

Layer Differentiable Sampling of

Candidate Solutions Figure 12.

[483] 采样

变压器层变

压器层获奖

者决策𝐦𝐢𝐧𝓛（𝚽+ 𝚫𝚽）更新

更新分类分

布〜变压器层

变压器层的

候选解决方

案可区分采

样图12。

--------------------------------------------------



[484] Learnable

depth pruning on a

local block Transformer Block

Transformer Block Transformer Block

Transformer Block Learning the

optimal sub-layers Transformer Block

Transformer Block DiT TinyDiT

Masked Distillation Massive Activation

( 𝑥> 𝑘⋅𝜎𝑥)

mask mask Hidden States

Hidden States Figure 13.

[484] 可学习

的深度修剪

在本地块变

压器块变压

器块变压器

块变压器块

中学习最佳

子层变压器

块变压器块

dit tinydit tinydit掩盖蒸馏大

量激活（𝑥>𝑥>𝑘·庇护

）掩码掩码隐

藏状态隐藏

状态图13。

--------------------------------------------------



[485] Masked

knowledge distillation with 2:4

blocks.

[485] 带有

2：4块的掩盖知

识蒸馏。

--------------------------------------------------



[486] details

of each hyperparameter are

provided in Table 6.

[486] 表6中

提供了每个

超参数的详

细信息。



--------------------------------------------------

[487] 7.

[487] 7。

--------------------------------------------------



[488] Visualization

of Pruning Decisions Figures

9, 10 and 11

visualize the dynamics of

pruning de- cisions during

training for the 1:2,

2:4, and 7:14 pruning

schemes.

[488] 修剪

决策的可视

化图9、10和11可视

化在1：2、2：2：4和7:14修剪

方案的训练

过程中修剪

效果的动力

学。

--------------------------------------------------



[489] Different

divisions lead to varying

search spaces, which in

turn result in various

solutions.

[489] 不同的划

分导致搜索

空间不同，这

又导致各种

解决方案。

--------------------------------------------------



[490] For

both the 1:2 and

2:4 schemes, good decisions

can be learned in

only one epoch, while

the 7:14 scheme encounters

optimization diffi- culty.

[490]

对

于1：2和2：4方案，只

能在一个时

代中学习好

的决定，而7:14方

案遇到优化

的构成。



--------------------------------------------------

[491] This is due

to the  14 7

 =3,432 candidates, which

is too huge and

thus cannot be adequately

sampled within a single

epoch.

[491] 这是

由于14 7

= 3,432个候选

人，这太大了

，因此无法在

单个时期内

进行充分采

样。



--------------------------------------------------

[492] Therefore, in practical

applications, we use the

1:2 or 2:4 schemes

for learnable layer pruning.

[492] 因此，在实

际应用中，我

们使用1：2或2：4的

方案进行修

剪。



--------------------------------------------------

[493] 8.

[493] 8。

--------------------------------------------------



[494] Details

of Masked Knowledge Distillation

Training Loss.

[494] 掩盖知识

蒸馏培训损

失的细节。

--------------------------------------------------



[495] This

work deploys a standard

knowledge distillation to learn

a good student model

by mimicking the pre-trained

teacher.

[495] 这

项工作通过

模仿预训练

的老师来部

署标准知识

蒸馏来学习

一个好的学

生模型。

--------------------------------------------------



[496] The

loss function is formalized

as: L = αKD

· LKD + αDiff

· LDiff + β

· LRep (8) Here,

LKD denotes the Mean

Squared Error between the

outputs of the student

and teacher models.

[496]

损耗

函数被形式

化为：l =αkd·lkd +αdiff·ldiff +β·lrep（8），LKD表示学

生和教师模

型的输出之

间的平均平

方误差。

--------------------------------------------------



[497] LDiff

repre- sents the original

pre-training loss function.

[497]

LDIFF代表

原始的预训

练损失函数

。



--------------------------------------------------

[498] Finally, LRep corresponds

to the masked distillation

loss applied to the

hidden states, as illustrated

in Figure 13, which

encourages alignment between the

intermediate representations of the

pruned model and the

original model.

[498] 最后，LREP对应于

应用于隐藏

状态的掩盖

蒸馏损失，如

图13所示，这鼓

励了修剪模

型的中间表

示与原始模

型之间的一

致性。

--------------------------------------------------



[499] The

corresponding hyperparameters αKD, αDiff

and αRep can be

found in Ta- ble

6.

[499] 相应的

超参数αKD，αDIFF和αREP可

以在Table 6中找到

。

--------------------------------------------------



[500] Hidden

State Alignment.

[500] 隐藏的状态

对齐。

--------------------------------------------------



[501] The

masked distillation loss LRep

is critical for aligning

the intermediate representations of

the student and teacher

models.

[501] 掩盖的

蒸馏损失LREP对

于使学生和

教师模型的

中间表示至

关重要。

--------------------------------------------------



[502] During

the recovery phase, each

layer of the student

model is designed to

repli- cate the output

hidden states of a

corresponding two-layer local block

from the teacher model.

[502] 在恢

复阶段，学生

模型的每个

层都旨在从

教师模型中

补充相应两

层本地块的

输出状态。



--------------------------------------------------

[503] Depth pruning does

not alter the internal

dimensions of the layers,

enabling direct alignment without

additional projection layers.

[503]

深

度修剪不会

改变层的内

部维度，从而

无需其他投

影层即可直

接对齐。



--------------------------------------------------

[504] For mod- els

such as SiTs, where

hidden state losses are

more pro- nounced due

to their unique interpolant-based

architecture, a smaller coefficient

β is applied to

LRep to mitigate poten-

tial training instability.

[504]

对于

诸如SITS之类的

模块化，由于

其独特的基

于插入式的

架构，隐藏的

状态损失更

加呈现，因此

将较小的系

数β应用于LREP，以

减轻潜在的

训练不稳定

性。



--------------------------------------------------

[505] The gradual decrease

in β through- out

training further reduces the

risk of negative impacts

on convergence.

[505] β跨训练的

逐渐减少进

一步降低了

对收敛的负

面影响的风

险。

--------------------------------------------------



[506] Iterative

Pruning and Distillation.

[506]

迭代修剪

和蒸馏。



--------------------------------------------------

[507] Table 7 assesses

the effectiveness of iterative

pruning and teacher selection

strategies.

[507] 表7评

估了迭代修

剪和教师选

择策略的有

效性。

--------------------------------------------------



[508] To

obtain a TinyDiT-D7, we

can either directly prune

a DiT-XL with 28

layers or craft a

TinyDiT-D14 first and then

iteratively produce the small

models.

[508] 要获得

TinyDit-D7，我们可以直

接修剪带有

28层的DIT-XL，或者先

制作TinyDit-D14，然后迭

代产生小型

模型。

--------------------------------------------------



[509] To

investi- gate the impact

of teacher choice and

the method for obtain-

ing the initial weights

of the student model,

we derived the initial

weights of TinyDiT-D7 by

pruning both a pre-trained

model and a crafted

intermediate model.

[509] 为了投

资教师选择

的影响以及

获得学生模

型的初始权

重的方法，我

们通过修剪

预训练的模

型和制作的

中间模型来

得出TinyDit-D7的初始

权重。

--------------------------------------------------



[510] Subsequently,

we used both the

trained and crafted models

as teachers for the

pruned student models.

[510]

随后，我

们将训练有

素和制作的

模型都用作

修剪的学生

模型的老师

。



--------------------------------------------------

[511] Across four experimental

set- tings, pruning and

distilling using the crafted

intermedi- ate model yielded

the best performance.

[511]

在四个实验

性设置中，使

用精心设计

的中介模型

进行修剪和

蒸馏产生了

最佳性能。



--------------------------------------------------

[512] Notably, models pruned

from the crafted model

outperformed those pruned from

the pre-trained model regardless

of the teacher model

employed in the distillation

process.

[512] 值

得注意的是

，从制作模型

中修剪的模

型优于从预

训练模型中

修剪的模型

，而不管蒸馏

过程中使用

的教师模型

如何。

--------------------------------------------------



[513] We

attribute this su- 2

[513] 我们归

因于这个su-2



--------------------------------------------------

[514] Model Optimizer Cosine

Sched.

[514] 模

型优化器余

弦计划。

--------------------------------------------------



[515] Teacher

αKD αGT β Grad.

[515] 老师

AKD AGT B毕业。

--------------------------------------------------



[516] Clip

Pruning Configs DiT-D19 AdamW(lr=2e-4,

wd=0.0) ηmin = 1e-4

DiT-XL 0.9 0.1 1e-2

→0 1.0 LoRA-1:2 DiT-D14

AdamW(lr=2e-4, wd=0.0 ηmin =

1e-4 DiT-XL 0.9 0.1

1e-2 →0 1.0 LoRA-1:2

DiT-D7 AdamW(lr=2e-4, wd=0.0) ηmin

= 1e-4 DiT-D14 0.9

0.1 1e-2 →0 1.0

LoRA-1:2 SiT-D14 AdamW(lr=2e-4, wd=0.0)

ηmin = 1e-4 SiT-XL

0.9 0.1 2e-4 →0

1.0 LoRA-1:2 MAR-D16 AdamW(lr=2e-4,

wd=0.0) ηmin = 1e-4

MAR-Large 0.9 0.1 1e-2

→0 1.0 LoRA-1:2 Table

6.

[516] 剪辑修

剪配置DIT-D19 ADAMW（LR

= 2E-4，WD = 0.0）ηmin=

1E-4 DIT-XL 0.9 0.9

0.1 1E-2→0 1.0 Lora-1：2

DIT-DIT-D14 ADAMW（LR = 2E-4，WD

= 2e-4，WD = 0.0ηmin=

0.0ηmin= 1e-4 dit-4 Dit-4

Dit-4 Dit-4 Dit-4 Dit-4

Dit-4 Dit-4 Dit-loror lor

lor lor。 DIT-D7 ADAMW（LR

= 2E-4，WD = 0.0）ηmin=

1e-4 Dit-D14 0.9 0.9

0.1 1E-2→0 1.0 Lora-1：2

Sit-D14 Adamw（LR = 2E-4，WD

= 0.0） Adamw（LR =

2E-4，WD = 0.0）ηmin= 1E-4

Mar-large 0.9 0.1 1E-2→0

1.0 Lora-1：2表6。



--------------------------------------------------

[517] Training details and

hyper-parameters for mask training

Teacher Model Pruned From

IS FID sFID Prec.

[517] 培

训细节和超

参数用于掩

盖培训的教

师模型，是FID SFID PREC。

--------------------------------------------------



[518] Recall

DiT-XL/2 DiT-XL/2 29.46 56.18

26.03 0.43 0.51 DiT-XL/2

TinyDiT-D14 51.96 36.69 28.28

0.53 0.59 TinyDiT-D14 DiT-XL/2

28.30 58.73 29.53 0.41

0.50 TinyDiT-D14 TinyDiT-D14 57.97

32.47 26.05 0.55 0.60

Table 7.

[518] Recall

DiT-XL/2 DiT-XL/2 29.46 56.18

26.03 0.43 0.51 DiT-XL/2

T​​inyDiT-D14 51.96 36.69 28.28

0.53 0.59 TinyDiT-D14 DiT-XL/2

28.30 58.73 29.53 0.41

0.50 TinyDiT-D14 TinyDiT-D14 57.97

32.47 26.05 0.55 0.60表

7。

--------------------------------------------------



[519] TinyDiT-D7

is pruned and distilled

with different teacher models

for 10k, sample steps

is 64, original weights

are used for sampling

rather than EMA.

[519]

TinyDit-D7用不同的教

师模型进行

修剪和蒸馏

，用于10K，样本步

骤为64，原始权

重用于采样

而不是EMA。



--------------------------------------------------

[520] 100 200 300

400 500 Steps 3.0

3.5 4.0 4.5 5.0

5.5 FID Masked KD

Finetune DiT-L/2 Scratch Figure

14.

[520] 100 200

300 400 500步3.0 3.5

4.0 4.5 5.0 5.0

5.5 FID蒙

版KD Finetune Dit-L/2刮擦图14。

--------------------------------------------------



[521] FID

and training steps.

[521]

FID和

培训步骤。



--------------------------------------------------

[522] perior performance to

two factors: first, the

crafted model’s structure is

better adapted to knowledge

distillation since it was

trained using a distillation

method; second, the reduced

search space facilitates finding

a more favorable initial

state for the student

model.

[522] 对

两个因素的

统治性能：首

先，制作的模

型的结构更

好地适应了

知识蒸馏，因

为它是使用

蒸馏方法训

练的；其次，减

少的搜索空

间有助于为

学生模型找

到更有利的

初始状态。

--------------------------------------------------



[523] 9.

[523] 9。



--------------------------------------------------

[524] Analytical Experiments Training

Strategies Figure 14 illustrates

the effective- ness of

standard fine-tuning and knowledge

distillation (KD), where we

prune DiT-XL to 14

layers and then ap-

ply various fine-tuning methods.

[524] 分

析实验培训

策略图14说明

了标准微调

和知识蒸馏

（KD）的有效性，我

们在其中将

DIT-XL降至14层，然后

将各种微调

方法降至14层

。



--------------------------------------------------

[525] Figure 3 presents

the FID scores across

100K to 500K steps.

[525] 图3显示了在

100k至500k步长的FID得

分。



--------------------------------------------------

[526] It is evident

that the standard fine-tuning

method allows TinyDiT-D14 to

achieve performance comparable to

DiT-L while offering faster

in- ference.

[526] 显然，标准

的微调方法

允许TinyDit-D14实现与

DIT-L相当的性能

，同时提供更

快的信息。

--------------------------------------------------



[527] Additionally,

we confirm the significant

effective- ness of distillation,

which enables the model

to surpass DiT- L

at just 100K steps

and achieve better FID

scores than the 500K

standard fine-tuned TinyDiT-D14.

[527]

此

外，我们确认

了蒸馏的显

着有效性，这

使该模型能

够在仅100k步骤

中超过Dit-L，并且

比500K标准的微

型TinyDit-D14获得了更

好的FID得分。



--------------------------------------------------

[528] This is because

the distillation of hidden

layers provides stronger supervision.

[528] 这

是因为隐藏

层的蒸馏提

供了更强的

监督。



--------------------------------------------------

[529] Further increasing the

training steps to 500K

leads to sig- nificantly

better results.

[529] 进一步

将训练步骤

提高到500K，从而

取得了更好

的结果。

--------------------------------------------------



[530] Learning

Rate IS FID sFID

Prec.

[530] 学习

率是FID SFID

PREC。



--------------------------------------------------

[531] Recall lr=2e-4 207.27

3.73 5.04 0.8127 0.5401

lr=1e-4 194.31 4.10 5.01

0.8053 0.5413 lr=5e-5 161.40

6.63 6.69 0.7419 0.5705

Table 8.

[531] 回忆LR

= 2E-4 207.27 3.73

5.04 0.8127 0.5401 LR

= 1E-4 194.31 4.10

5.01 0.801 0.8053 0.5413

LR = 5E-5 161.40

6.63 6.63 6.69 6.69

0.7419 0.7419 0.5705表

8。

--------------------------------------------------



[532] The

effect of Learning rato

for TinyDiT-D14 finetuning w/o

knowledge distillation Learning Rate.

[532] 通过知识蒸

馏学习率，学

习拉托对TinyDit-D14的

finetuning的效果。



--------------------------------------------------

[533] We also search

on some key hyperparam-

eters such as learning

rates in Table 8.

[533] 我们

还搜索了表

8中的一些关

键超帕拉姆

语（例如学习

率）。



--------------------------------------------------

[534] We identify the

ef- fectiveness of lr=2e-4

and apply it to

all models and exper-

iments.

[534] 我们确定

LR =

2E-4的效率，并将

其应用于所

有模型和实

验。



--------------------------------------------------

[535] 10.

[535] 10。

--------------------------------------------------



[536] Visulization

Figure 15 and 16

showcase the generated images

from TinySiT-D14 and TinyMAR-D16,

which were compressed from

the official checkpoints.

[536]

可见的图

15和16显示了从

官方检查点

压缩的Tinysit-D14和Tinymar-D16的

生成图像。



--------------------------------------------------

[537] These models were

trained using only 7%

and 10% of the

original pre-training costs, respectively,

and were distilled using

the proposed masked knowledge

distillation method.

[537] 这

些模型仅使

用原始预训

练成本的7％和

10％培训，并使用

拟议的蒙版

知识蒸馏方

法进行蒸馏

。

--------------------------------------------------



[538] Despite

compression, the models are

capable of generating plausible

results with only 50%

of depth.

[538] 尽管有压缩

，这些模型仍

能够产生仅

50％深度的合理

结果。

--------------------------------------------------



[539] 11.

[539] 11。



--------------------------------------------------

[540] Limitations In this

work, we explore a

learnable depth pruning method

to accelerate diffusion transformer

models for conditional image

generation.

[540] 这项工

作的局限性

，我们探索了

一种可学习

的深度修剪

方法，以加速

有条件图像

生成的扩散

变压器模型

。

--------------------------------------------------



[541] As

Diffusion Transformers have shown

significant advancements in text-to-image

generation, it is valuable

to conduct a systematic

analysis of the impact

of layer removal within

the text-to-image tasks.

[541]

由于扩散变

压器在文本

到图像生成

方面已显示

出显着的进

步，因此对文

本到图像任

务中层拆卸

的影响进行

系统分析是

有价值的。



--------------------------------------------------

[542] Additionally, there exist

other interesting depth pruning

strategies that need to

be studied, such as

more fine-grained pruning strate-

gies that remove attention

layers and MLP layers

indepen- dently instead of

removing entire transformer blocks.

[542] 此

外，还有其他

需要研究的

有趣的深度

修剪策略，例

如更细粒度

的修剪策略

，这些策略会

消除注意力

层和MLP层，而不

是删除整个

变压器块。



--------------------------------------------------

[543] We leave these

investigations for future work.

[543] 我

们将这些调

查留给未来

的工作。



--------------------------------------------------

[544] 3

[544] 3

--------------------------------------------------



[545] Figure

15.

[545] 图15。

--------------------------------------------------



[546] Generated

images from TinySiT-D14 Figure

16.

[546] 来

自Tinysit-D14的产生图

像图16。

--------------------------------------------------



[547] Generated

images from TinyMAR-D16 4

[547] 蒂尼马

尔-D16 4的产生图

像4

--------------------------------------------------
