[010] Introduction

Diffusion Transformers have emerged

as a cornerstone ar-

chitecture for generative tasks,

achieving notable success in

areas such as image

[11, 26, 40] and

video synthe- sis [25,

59].

[010] 引言扩散

变压器已成

为生成任务

的基石 -

在图

像[11、26、40]和视频综

合[25、59]等领域取

得了显着成

功。



--------------------------------------------------

[011] This success has

also led to the

widespread availability of high-quality

pre-trained models on the

Inter- net, greatly accelerating

and supporting the development

of various downstream applications

[5, 16, 53, 55].

[011] 这一成功

还导致了互

联网上高质

量预训练的

模型的广泛

可用性，极大

地加速和支

持了各种下

游应用程序

的开发[5，16，53，55]。



--------------------------------------------------

[012] However, pre-trained diffusion

transformers usually come with

con- *Equal contribution †Corresponding

author Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Differentiable Sampling of

Layer Mask 𝖒

Recoverability Estimation with 𝚫𝚽

1 0 1 0

Local Block Joint Opt.

[012] 但是

，预训练的扩

散变压器通

常具有相等

的贡献†相应

的作者变压

器层变压器

层变压器层

变压器层层

层掩模的可

恢复性估计

的可恢复性

估计，𝚫𝚽1 0 1

0局部块

接头选择。



--------------------------------------------------

[013] Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer 𝚫𝚽  (LoRA/Full)

𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽, 𝖒)

𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒

Figure 1.

[013] 变

压器层变压

器层变压器

层变压器层

变压器层变

压器层𝚫𝚽（lora/full）𝐦𝐢𝐧𝖒，𝚫𝚽𝓛（𝒙，𝚽+

𝚫𝚽，𝖒，𝖒）𝝓𝟏 𝝓𝟏 𝝓𝟑 𝝓𝟑图1。

--------------------------------------------------



[014] This

work presents a learnable

approach for pruning the

depth of pre-trained diffusion

transformers.

[014] 这

项工作提出

了一种可学

习的方法，用

于修剪预训

练的扩散变

压器的深度

。

--------------------------------------------------



[015] Our

method simulta- neously optimizes

a differentiable sampling process

of layer masks and

a weight update to

identify a highly recoverable

solution, en- suring that

the pruned model maintains

competitive performance after fine-tuning.

[015] 我们的方法

同时优化了

层掩模的可

区分采样过

程和重量更

新，以识别高

度可恢复的

解决方案，并

确保修剪模

型在微调后

保持竞争性

能。



--------------------------------------------------

[016] siderable inference costs

due to the huge

parameter scale, which poses

significant challenges for deployment.

[016] 由于巨大

的参数量表

而导致的推

理成本很大

，这给部署带

来了重大挑

战。



--------------------------------------------------

[017] To re- solve

this problem, there has

been growing interest from

both the research community

and industry in developing

lightweight models [12, 23,

32, 58].

[017] 为了解决

这个问题，研

究社区和行

业对开发轻

质模型的兴

趣越来越大

[12，23，32，58]。

--------------------------------------------------



[018] The

efficiency of diffusion models

is typically influ- enced

by various factors, including

the number of sampling

steps [33, 43, 45,

46], operator design [7,

48, 52], compu- tational

precision [19, 30, 44],

network width [3, 12]

and depth [6, 23,

36].

[018] 扩散模型的

效率通常受

到各种因素

的影响，包括

采样步骤的

数量[33，43，45，46]，操作员

设计[7，48，52]，组合精

度[19，30，44]，网络宽度

[3，12] [3，12]和深度[6，23，36]。

--------------------------------------------------



[019] In

this work, we focus

on model compres- sion

through depth pruning [36,

54], which removes entire

layers from the network

to reduce the latency.

[019] 在这

项工作中，我

们通过深度

修剪[36，54]专注于

模型的组合

，该模型从网

络中删除了

整个层以减

少延迟。



--------------------------------------------------

[020] Depth prun- ing

offers a significant advantage

in practice: it can

achieve a linear acceleration

ratio relative to the

compression rate on both

parallel and non-parallel devices.

[020] 深度

修剪在实践

中提供了重

要的优势：相

对于并行设

备和非平行

设备的压缩

率，它可以达

到线性加速

度比率。



--------------------------------------------------

[021] For example, as

will be demonstrated in

this work, while 50%

width prun- ing [12]

only yields a 1.6×

speedup, pruning 50% of

the layers results in

a 2× speedup.

[021]

例如

，正如这项工

作所证明的

那样，宽度为

50％的宽度[12]仅产

生1.6倍的加速

，修剪50％的层导

致2倍加速。



--------------------------------------------------

[022] This makes depth

pruning a flexible and

practical method for model

compression.

[022] 这

使得深度修

剪了模型压

缩的灵活且

实用的方法

。

--------------------------------------------------



[023] This

work follows a standard

depth pruning frame- work:

unimportant layers are first

removed, and the pruned

model is then fine-tuned

for performance recovery.

[023]

这项工作遵

循标准的深

度修剪框架

- 首先要删除

不重要的层

，然后对修剪

模型进行微

调以进行性

能恢复。



--------------------------------------------------

[024] In the literature,

depth pruning techniques designed

for dif- fusion transformers

or general transformers primarily

fo- cus on heuristic

approaches, such as carefully

designed importance scores [6,

36] or manually configured

pruning 1 arXiv:2412.01199v1

[cs.CV]  2 Dec

2024

[024] 在文

献中，设计用

于差异变压

器或通用变

压器设计的

深度修剪技

术主要是基

于启发式方

法，例如精心

设计的重要

性得分[6，36]或手

动配置的修

剪1 arxiv：2412.011999v1

[cs.cv] [cs.cv] [cs.cv] 2024年12月2日2024年

12月2日

--------------------------------------------------



[025] schemes

[23, 54].

[025] 方案[23，54]。

--------------------------------------------------



[026] These

methods adhere to a

loss min- imization principle

[18, 37], aiming to

identify solutions that maintain

low loss or error

after pruning.

[026] 这

些方法遵守

损失最小原

则[18，37]，旨在识别

在修剪后保

持低损失或

错误的解决

方案。

--------------------------------------------------



[027] This

paper investigates the effectiveness

of this widely used

principle in the context

of depth compression.

[027]

本文在

深度压缩的

背景下研究

了该广泛使

用原理的有

效性。



--------------------------------------------------

[028] Through experiments, we

examined the relationship between

calibration loss ob- served

post-pruning and the performance

after fine-tuning.

[028] 通过实

验，我们检查

了校准损失

渗透后的后

延期与微调

后的性能之

间的关系。

--------------------------------------------------



[029] This

is achieved by extensively

sampling 100,000 models via

random pruning, exhibiting different

levels of calibra- tion

loss in the searching

space.

[029] 这

是通过通过

随机修剪进

行广泛采样

100,000款模型来实

现的，在搜索

空间中表现

出不同水平

的碳纤维损

失。

--------------------------------------------------



[030] Based

on this, we analyzed

the effectiveness of existing

pruning algorithms, such as

the feature similarity [6,

36] and sensitivity analysis

[18], which indeed achieve

low calibration losses in

the solution space.

[030]

基于此，我

们分析了现

有修剪算法

的有效性，例

如特征相似

性[6，36]和灵敏度

分析[18]，它们确

实在解决方

案空间中实

现了低校准

损失。



--------------------------------------------------

[031] However, the performance

of all these models

after fine- tuning often

falls short of expectations.

[031] 但是，精

细调整后所

有这些模型

的性能通常

都没有期望

。



--------------------------------------------------

[032] This indicates that

the loss minimization principle

may not be well-suited

for diffusion transformers.

[032]

这表明最小

化原理可能

不适合扩散

变压器。



--------------------------------------------------

[033] Building on these

insights, we reassessed the

underly- ing principles for

effective layer pruning in

diffusion trans- formers.

[033]

在这

些见解的基

础上，我们重

新评估了在

扩散式传输

中修剪有效

层的基本原

理。



--------------------------------------------------

[034] Fine-tuning diffusion transformers

is an extremely time-consuming

process.

[034] 微调扩散

变压器是一

个非常耗时

的过程。

--------------------------------------------------



[035] Instead

of searching for a

model that minimizes loss

immediately after pruning, we

propose identifying candidate models

with strong recoverability, en-

abling superior post-fine-tuning performance.

[035] 我们

没有在修剪

后立即寻找

将损失立即

最小化的模

型，而是提议

识别具有强

大可恢复性

，优势较高后

调节性能的

候选模型。



--------------------------------------------------

[036] Achieving this goal

is particularly challenging, as

it requires the in-

tegration of two distinct

processes, pruning and fine-tuning,

which involve non-differentiable operations

and cannot be directly

optimized via gradient descent.

[036] 实

现这一目标

特别具有挑

战性，因为它

需要对两个

不同的过程

进行修剪和

微调，这涉及

非不同的操

作，并且不能

通过梯度下

降直接优化

。



--------------------------------------------------

[037] To this end,

we propose a learnable

depth pruning method that

effectively integrates pruning and

fine-tuning.

[037] 为此，我们提

出了一种可

学习的深度

修剪方法，可

以有效整合

修剪和微调

。

--------------------------------------------------



[038] As

shown in Figure 1,

we model the pruning

and fine- tuning of

a diffusion transformer as

a differentiable sam- pling

process of layer masks

[13, 17, 22], combined

with a co-optimized weight

update to simulate future

fine-tuning.

[038] 如图1所示，我

们将扩散变

压器的修剪

和精细调整

为层掩模的

可区分的Sam固

定过程[13，17，22]，并结

合了合作的

权重更新，以

模拟未来的

微调。

--------------------------------------------------



[039] Our

objective is to iteratively

refine this distribution so

that networks with higher

recoverability are more likely

to be sampled.

[039]

我们的

目标是迭代

地完善此分

布，以便更有

可能采样具

有较高可恢

复性的网络

。



--------------------------------------------------

[040] This is achieved

through a straightforward strat-

egy: if a sampled

pruning decision results in

strong recover- ability, similar

pruning patterns will have

an increased prob- ability

of being sampled.

[040]

这是通过直

接的策略来

实现的：如果

采样修剪决

策会导致强

大的恢复能

力，那么相似

的修剪模式

将具有提高

采样的概率

能力。



--------------------------------------------------

[041] This approach promotes

the ex- ploration of

potentially valuable solutions while

disregard- ing less effective

ones.

[041] 这种方

法促进了对

潜在有价值

的解决方案

的提出，同时

又无视效率

较低的解决

方案。

--------------------------------------------------



[042] Additionally,

the proposed method is

highly efficient, and we

demonstrate that a suitable

solu- tion can emerge

within a few training

steps.

[042] 此外，所

提出的方法

非常有效，我

们证明可以

在几个训练

步骤中出现

合适的解决

方案。

--------------------------------------------------



[043] To

evaluate the effectiveness of

the proposed method, we

conduct extensive experiments on

various transformer- based diffusion

models, including DiTs [40],

MARs [29], SiTs [34].

[043] 为了评

估所提出方

法的有效性

，我们对基于

变压器的扩

散模型进行

了广泛的实

验，包括DITS [40]，MARS [29]，位于

[34]。

--------------------------------------------------



[044] The

learnable approach is highly

efficient.

[044] 可学习的方

法非常有效

。

--------------------------------------------------



[045] It

is able to identify

redundant layers in diffusion

transform- ers with 1-epoch

training on the dataset,

which effectively crafts shallow

diffusion transformers from pre-trained

mod- els with high

recoverability.

[045] 它能够通过

数据集上的

1个上述训练

来识别扩散

变换中的冗

余层，从而有

效地从具有

高可恢复性

的预训练的

模型中制作

了浅扩散变

压器。

--------------------------------------------------



[046] For

instance, while the models

pruned by TinyFusion initially

exhibit relatively high cal-

ibration loss after removing

50% of layers, they

recover quickly through fine-tuning,

achieving a significantly more

competitive FID score (5.73

vs. 22.28) compared to

base- line methods that

only minimize immediate loss,

using just 1% of

the pre-training cost.

[046]

例如，尽

管删除50％的层

后，最初被TinyFusion修

剪的模型最

初表现出相

对较高的cal损

失，但与仅使

用预先培训

的1％的基本方

法相比，它们

通过微调得

分迅速恢复

，获得了更具

竞争力的FID得

分（5.73 vs 22.28）（5.73对22.28）。

--------------------------------------------------



[047] Additionally,

we also ex- plore

the role of knowledge

distillation in enhancing re-

coverability [20, 23] by

introducing a MaskedKD variant.

[047] 此外，我

们还通过引

入maskedkd变体来表

达知识蒸馏

在增强可覆

盖性[20，23]中的作

用。



--------------------------------------------------

[048] MaskedKD mitigates the

negative impact of the

massive or outlier activations

[47] in hidden states,

which can signifi- cantly

affect the performance and

reliability of fine-tuning.

[048]

MaskEDKD减轻了隐

藏状态中大

规模或异常

激活的负面

影响[47]，这可能

会显着影响

微调的性能

和可靠性。



--------------------------------------------------

[049] With MaskedKD, the

FID score improves from

5.73 to 3.73 with

only 1% of pre-training

cost.

[049] 借

助MaskedKD，FID得分从5.73提

高到3.73，仅占培

训前成本的

1％。

--------------------------------------------------



[050] Extending

the training to 7%

of the pre-training cost

further reduces the FID

to 2.86, just 0.4

higher than the original

model with doubled depth.

[050] 将培训扩大

到7％的培训前

成本将FID进一

步降低到2.86，仅

比原始模型

高度增加了

0.4。



--------------------------------------------------

[051] Therefore, the main

contribution of this work

lies in a learnable

method to craft shallow

diffusion transformers from pre-trained

ones, which explicitly optimizes

the re- coverability of

pruned models.

[051] 因此，这项工

作的主要贡

献在于一种

可学习的方

法，可以从预

训练的方法

中制作浅扩

散变压器，该

方法明确优

化了修剪模

型的可覆盖

性。

--------------------------------------------------



[052] The

method is general for

various architectures, including DiTs,

MARs and SiTs.

[052]

该方法是

各种架构，包

括DIT，MARS和SITS的一般

方法。



--------------------------------------------------

[053] 2.

[053] 2。

--------------------------------------------------



[054] Related

Works Network Pruning and

Depth Reduction.

[054] 相关工

作网络修剪

和深度减少

。

--------------------------------------------------



[055] Network

prun- ing is a

widely used approach for

compressing pre-trained diffusion models

by eliminating redundant parameters

[3, 12, 31, 51].

[055] 网络修剪是

一种通过消

除冗余参数

来压缩预训

练扩散模型

的广泛使用

方法[3，12，31，51]。



--------------------------------------------------

[056] Diff-Pruning [12] introduces

a gradient- based technique

to streamline the width

of UNet, fol- lowed

by a simple fine-tuning

to recover the performance.

[056] DIFF-PRUNING [12]引入了

一种基于梯

度的技术，以

简化UNET的宽度

，以通过简单

的微调来恢

复性能。

--------------------------------------------------



[057] SparseDM

[51] applies sparsity to

pre-trained diffusion models via

the Straight-Through Estimator (STE)

[2], achieving a 50%

reduction in MACs with

only a 1.22 in-

crease in FID on

average.

[057] Sparsedm [51]通过

直通估计量

（Ste）[2]将稀疏性应

用于预训练

的扩散模型

，在MAC中降低了

50％，平均FID仅1.22个折

痕。

--------------------------------------------------



[058] While

width pruning and spar-

sity help reduce memory

overhead, they often offer

lim- ited speed improvements,

especially on parallel devices

like GPUs.

[058] 虽然修剪

和宽度有助

于减少内存

开销，但它们

通常会提供

限制的速度

提高，尤其是

在诸如GPU之类

的平行设备

上。

--------------------------------------------------



[059] Consequently,

depth reduction has gained

signifi- cant attention in

the past few years,

as removing entire lay-

ers enables better speedup

proportional to the pruning

ra- tio [24, 27,

28, 36, 54, 56,

58].

[059] 因此，在过

去的几年中

，深度的降低

引起了显着

关注，因为删

除整个外行

可以更好地

加速与修剪

ra-tio成正比[24、27、27、28、36、54、56、58]。

--------------------------------------------------



[060] Adaptive

depth reduction techniques, such

as MoD [41] and

depth-aware transform- ers [10],

have also been proposed.

[060] 还提

出了自适应

深度还原技

术，例如MOD [41]和深

度感知的转

化[10]。

--------------------------------------------------



[061] Despite

these advances, most existing

methods are still based

on empirical or heuris-

tic strategies, such as

carefully designed importance crite-

ria [36, 54], sensitivity

analyses [18] or manually

designed schemes [23], which

often do not yield

strong performance guarantee after

fine-tuning.

[061] 尽管取得

了这些进步

，但大多数现

有方法仍然

基于经验或

启发式策略

，例如精心设

计的重要性

迹象[36，54]，敏感性

分析[18]或手动

设计的方案

[23]，这些方案通

常不会在微

调后产生强

大的性能保

证。

--------------------------------------------------



[062] Efficient

Diffusion Transformers.

[062] 有效的扩

散变压器。

--------------------------------------------------



[063] Developing

efficient diffusion transformers has

become an appealing focus

within the community, where

significant efforts have been

made to enhance efficiency

from various perspectives, in-

cluding linear attention mechanisms

[15, 48, 52], compact

architectures [50], non-autoregressive transformers

[4, 14, 38, 49],

pruning [12, 23], quantization

[19, 30, 44], feature

2

[063] 开

发有效的扩

散变压器已

成为社区中

的一个吸引

人的重点，在

各种角度，已

经做出了巨

大的努力来

提高效率，包

括线性注意

机制[15，48，52]，紧凑型

建筑[50]，非自动

性变压器[4，14，38，49]，pruning [4,14,38,49]，pruns

[12，23]，量

化[19，30，30，44]



--------------------------------------------------

[064] Transformer Layer Transformer

Layer Transformer Layer Transformer

Layer 1:2 Local Blocks

𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒

0 1 0 1

0 1 0 1

⊕ Weight Update Weight

Update Weight Update Weight

Update Δ𝜙4 ⋅𝔪4 Δ𝜙3

⋅𝔪3 Δ𝜙2 ⋅𝔪2 Δ𝜙1

⋅𝔪1 Retained Layer Retained

Layer 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽,

𝖒) Confident Sampling ⇒

Good solution identified 1:2

Local Blocks 𝔪1 𝔪2

𝔪3 𝔪4 ⊕ ∼

Mixed Sampling ⇒ Exploration

still in Progress Diff.

[064] 变压器层

变压器层变

压器层变压

器层1：2局部块

𝝓𝟏 0 1

0 1 0 1

0 1 0 1

0 1⊕重量更新重

量更新重量

更新重量更

新重量更新

重量更新重

量更新Δ4Δ3Δ3Δ2Δ2Δ2δ𝜙2Δ𝜙1Δ𝜙1µ1·𝔪1·𝔪1·𝔪1µ1保留

层保留层，良

好的层识别

层（良好的解

决方案），范围

2，𝚫𝚽𝓛，𝚫𝚽𝓛，𝒙+ 𝚫𝚽+ 𝚫𝚽，𝚽+

𝚫𝚽，𝖒+ 𝚫𝚽，Sampling，Sampl complative。 𝔪1𝔪2𝔪3𝔪4⊕4〜混合采样⇒探

索仍在进行

中。

--------------------------------------------------



[065] Sampling

Learnable Distribution ∼ Diff.

[065] 抽样可学

习的分布〜差

异。



--------------------------------------------------

[066] Sampling Figure 2.

[066] 采样图2。



--------------------------------------------------

[067] The proposed TinyFusion

method learns to perform

a differentiable sampling of

candidate solutions, jointly optimized

with a weight update

to estimate recoverability.

[067]

拟

议的TinyFusion方法学

会了对候选

解决方案进

行可区分的

采样，共同优

化了重量更

新以估算可

恢复性。



--------------------------------------------------

[068] This approach aims

to increase the likelihood

of favorable solutions that

ensure strong post-fine- tuning

performance.

[068] 这种

方法旨在增

加有利解决

方案的可能

性，从而确保

强大的结束

后表现。

--------------------------------------------------



[069] After

training, local structures with

the highest sampling probabilities

are retained.

[069] 训练

后，保留了采

样概率最高

的本地结构

。

--------------------------------------------------



[070] caching

[35, 57], etc.

[070]

缓存[35，57]，等。



--------------------------------------------------

[071] In this work,

we focus on compress-

ing the depth of

pre-trained diffusion transformers and

in- troduce a learnable

method that directly optimizes

recover- ability, which is

able to achieve satisfactory

results with low re-training

costs.

[071] 在这

项工作中，我

们专注于压

缩预训练的

扩散变压器

的深度，并赋

予一种可学

习的方法，该

方法可以直

接优化恢复

能力，该方法

能够通过低

重新训练成

本获得令人

满意的结果

。

--------------------------------------------------



[072] 3.

[072] 3。



--------------------------------------------------

[073] Method 3.1.

[073]

方法3.1。



--------------------------------------------------

[074] Shallow Generative Transformers

by Pruning This work

aims to derive a

shallow diffusion transformer by

pruning a pre-trained model.

[074] 通过修

剪这项工作

，浅层生成变

压器旨在通

过修剪预训

练的模型来

得出浅扩散

变压器。



--------------------------------------------------

[075] For simplicity, all

vectors in this paper

are column vectors.

[075]

为简

单起见，本文

中的所有向

量都是列向

量。



--------------------------------------------------

[076] Consider a L-layer

trans- former, parameterized by

ΦL×D = [ϕ1, ϕ2,

· · · ,

ϕL]⊺, where each element

ϕi encompasses all learnable

param- eters of a

transformer layer as a

D-dim column vector, which

includes the weights of

both attention layers and

MLPs.

[076] 考虑一个

由L层trans-前者，由

φl×d =

[ϕ1，ϕ2，·，ϕL]⊺进行参数，其

中每个元素

ϕi包含变压器

层的所有可

学习的参数

作为D-DIM柱向量

，其中包括注

意层和MLP的重

量。



--------------------------------------------------

[077] Depth pruning seeks

to find a binary

layer mask mL×1 =

[m1, m2, · ·

· , mL]⊺, that

removes a layer by:

xi+1 = miϕi(xi) +

(1 −mi)xi = (

ϕi(xi), if mi =

1, xi, otherwise, (1)

where the xi and

ϕi(xi) refers to the

input and output of

layer ϕi.

[077] 深度修剪

试图找到二

进制掩码ml×1

= [m1，m2，··级

，ml]⊺，该层通过：xi + 1

= miDartice（xi） +（1 -mi）xi

=（ϕi（xi）=（ϕi（xi（xi（xi），如

果mi = 1，xi），xi =

1，xi，（xi），xi和xi yly（xi），（xi）和xi liver，（xi）codi（xi）和xi（xi）codi（xi）和

（xi liver，（1）

ϕi。



--------------------------------------------------

[078] To obtain the

mask, a common paradigm

in prior work is

to minimize the loss

L after pruning, which

can be formulated as

minm Ex [L(x, Φ,

m)].

[078] 为了获得掩

模，先前工作

中的一个常

见范式是将

修剪后的损

耗l最小化，可

以将其作为

minm ex

[l（x，φ，m）]配制。



--------------------------------------------------

[079] However, as we

will show in the

experiments, this objective –

though widely adopted in

discriminative tasks – may

not be well-suited to

pruning diffusion transformers.

[079]

但是，正

如我们将在

实验中显示

的那样，尽管

在判别任务

中广泛采用

了这个目标

，但可能不适

合修剪扩散

变压器。



--------------------------------------------------

[080] Instead, we are

more inter- ested in

the recoverability of pruned

models.

[080] 取而

代之的是，我

们对修剪模

型的可恢复

性更加感兴

趣。

--------------------------------------------------



[081] To

achieve this, we incorporate

an additional weight update

into the optimization problem

and extend the objective

by: min m min

∆Φ Ex [L(x, Φ

+ ∆Φ, m)] |

{z } Recoverability: Post-Fine-Tuning

Performance , (2) where

∆Φ = {∆ϕ1, ∆ϕ2,

· · · ,

∆ϕM} represents appro- priate

update from fine-tuning.

[081]

为了实现

这一目标，我

们将额外的

权重更新纳

入了优化问

题，并扩展了

目标：min m min ∆φ

ex [l（x，φ + ∆φ，m）]

| {z}可恢复

性：恢复性能

后的性能，（2）其

中∆φ = {∆

ϕ1，∆ ϕ2，···，∆ ϕm}表示来自

微调的适当

更新。

--------------------------------------------------



[082] The

objective formulated by Equation

2 poses two challenges:

1) The non-differentiable nature

of layer selection prevents

direct optimization us- ing

gradient descent; 2) The

inner optimization over the

retained layers makes it

computationally intractable to ex-

plore the entire search

space, as this process

necessitates se- lecting a

candidate model and fine-tuning

it for evaluation.

[082]

公式2提

出的目标提

出了两个挑

战：1）层选择的

非差异性质

阻止了直接

优化的梯度

下降； 2）在保留

层上的内部

优化使得在

计算上棘手

可以阐明整

个搜索空间

，因为此过程

需要将候选

模型列为候

选模型并进

行微调以进

行评估。



--------------------------------------------------

[083] To address this,

we propose TinyFusion that

makes both the pruning

and recoverability optimizable.

[083]

为了

解决这个问

题，我们提出

了使修剪和

可恢复性优

化的微小灌

注。



--------------------------------------------------

[084] 3.2.

[084] 3.2。

--------------------------------------------------



[085] TinyFusion:

Learnable Depth Pruning A

Probabilistic Perspective.

[085] 小型灌注

：可学习的深

度修剪概率

的观点。

--------------------------------------------------



[086] This

work models Equa- tion

2 from a probabilistic

standpoint.

[086] 这项

工作从概率

的角度模拟

了方程2。

--------------------------------------------------



[087] We

hypothesize that the mask

m produced by “ideal”

pruning methods (might be

not unique) should follow

a certain distribution.

[087]

我们

假设由“理想

”修剪方法（可

能不是唯一

的）产生的面

膜应遵循一

定的分布。



--------------------------------------------------

[088] To model this,

it is intuitive to

associate every possible mask

m with a probability

value p(m), thus forming

a categori- cal distribution.

[088] 为

了对此进行

建模，将每个

可能的掩码

M与概率值P（M）相

关联是直观

的，从而形成

分类分布。



--------------------------------------------------

[089] Without any prior

knowledge, the assess- ment

of pruning masks begins

with a uniform distribution.

[089] 没

有任何先验

知识，修剪口

罩的评估始

于均匀的分

布。



--------------------------------------------------

[090] However, directly sampling

from this initial distribution

is highly inefficient due

to the vast search

space.

[090] 但是，由于

庞大的搜索

空间，该初始

分布直接进

行采样效率

很高。

--------------------------------------------------



[091] For

in- stance, pruning a

28-layer model by 50%

involves evalu- ating
