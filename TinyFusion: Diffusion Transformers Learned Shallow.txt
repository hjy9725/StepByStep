1
ğ¦ğ¢ğ§ğ–’,ğš«ğš½ ğ“›(ğ’™, ğš½ + ğš«ğš½, ğ–’)
ğš«ğš½
(LoRA/Full)
Transformer Layer 0
T Transformer Layer ransformer Layer
Transformer Layer 1
Joint
Opt. Transformer Layer
Transformer Layer
Transformer Layer 0
TrTarnasnfosfromrmereLraLyaeyrer
Transformer Layer 1
ğ¦ğ¢ğ§ğ–’,ğš«ğš½ ğ“› ğ’™( , ğš½ + ğš«ğš½, ğ–’)
ğš«ğš½
(LoRA/Full)
Transformer Layer 0
T Transformer Layer ransformer Layer
Transformer Layer 1
Joint
Opt. Transformer Layer
Transformer Layer
Transformer Layer 0
TrTarnasnfosfromrmereLraLyaeyrer
Transformer Layer 1
TinyFusion: Diffusion Transformers Learned Shallow
TinyFusion:æ‰©æ•£å˜å‹å™¨å­¦ä¹ æµ…
Gongfan Fang,* Kunjun Li,* Xinyin Ma, Xinchao
Wangâ€  National University of Singapore
æ–¹ã€*ã€æå¤å›*ã€é©¬ã€ç‹æ–°è¶…
{gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg
maxinyin}@u.nus.edu é¾šå‡¡æ˜†éƒ¡ï¼Œxinchao@nus.edu.sg
Abstract
æ‘˜è¦
Diffusion Transformers have demonstrated remarkable
capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we
present TinyFusion, a depth pruning method designed to reæ‰©æ•£å˜æ¢åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†éå‡¡çš„èƒ½åŠ›ï¼Œä½†
é€šå¸¸ä¼´éšç€è¿‡å¤šçš„å‚æ•°åŒ–ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­ç›¸å½“å¤§
çš„æ¨ç†å¼€é”€ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦å‰ªææ–¹
æ³• TinyFusion
ğ“ğŸ’
ğ“ğŸ’
ğ“ğŸ‘
ğ“ğŸ‘
ğ“ğŸ
ğ“ğŸ
ğ“ğŸ
ğ“ğŸ
2
Local Block
æœ¬åœ°å—
Differentiabl
e Sampling
of Layer Mask ğ–’Recoverability Estimation with ğš«ğš½
ğš«ğš½å±‚æ©æ¨¡ğ–’Recoverability ä¼°è®¡çš„å¯å¾®åˆ†é‡‡æ ·
move redundant layers from diffusion transformers via
end- to-end learning. The core principle of our approach
is to create a pruned model with high recoverability,
allowing it to regain strong performance after fine-tuning.
To accom- plish this, we introduce a differentiable
sampling technique to make pruning learnable, paired
with a co-optimized pa- rameter to simulate future finetuning. While prior works focus on minimizing loss or
error after pruning, our method explicitly models and
optimizes the post-fine-tuning perfor- mance of pruned
models. Experimental results indicate that this learnable
paradigm offers substantial benefits for layer pruning of
diffusion transformers, surpassing exist- ing importancebased and error-based methods. Addition- ally,
TinyFusion exhibits strong generalization across di- verse
architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a
shallow diffusion transformer at less than 7% of the preé€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ å°†å†—ä½™å±‚ä»æ‰©æ•£å˜å‹å™¨ä¸­ç§»å‡ºã€‚æˆ‘ä»¬
æ–¹æ³•çš„æ ¸å¿ƒåŸåˆ™æ˜¯åˆ›å»ºä¸€ä¸ªå…·æœ‰é«˜åº¦å¯æ¢å¤æ€§çš„ä¿®å‰ª
æ¨¡å‹ï¼Œå…è®¸å®ƒåœ¨å¾®è°ƒåé‡æ–°è·å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚ä¸ºäº†å®
ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯å¾®åˆ†çš„é‡‡æ ·æŠ€æœ¯æ¥ä½¿å‰ª
æå˜å¾—å¯å­¦ä¹ ï¼Œå¹¶ä¸ä¸€ä¸ªå…±åŒä¼˜åŒ–çš„å‚æ•°é…å¯¹æ¥æ¨¡æ‹Ÿ
æœªæ¥çš„å¾®è°ƒã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œé›†ä¸­åœ¨ä¿®å‰ªåæœ€å°åŒ–æŸ
å¤±æˆ–è¯¯å·®ï¼Œä½†æ˜¯æˆ‘ä»¬çš„æ–¹æ³•æ˜¾å¼åœ°å»ºæ¨¡å’Œä¼˜åŒ–ä¿®å‰ªæ¨¡
å‹çš„å¾®è°ƒåæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¯å­¦ä¹ çš„èŒƒä¾‹
ä¸ºæ‰©æ•£å˜æ¢å™¨çš„å±‚ä¿®å‰ªæä¾›äº†å®è´¨æ€§çš„å¥½å¤„ï¼Œè¶…è¿‡äº†
ç° æœ‰ çš„ åŸº äº é‡ è¦ æ€§ å’Œ åŸº äº é”™ è¯¯ çš„ æ–¹ æ³• ã€‚ æ­¤
å¤–ï¼ŒTinyFusion åœ¨ä¸åŒçš„æ¶æ„ä¸Šè¡¨ç°å‡ºå¾ˆå¼ºçš„é€šç”¨
æ€§ï¼Œæ¯”å¦‚ DiTsã€MARs å’Œ SiTsã€‚ç”¨ DiT-XL è¿›è¡Œçš„å®éªŒ
è¡¨æ˜ï¼ŒTinyFusion èƒ½ä»¥ä¸åˆ°é¢„æ‰©æ•£å˜å‹å™¨çš„ 7%åˆ¶é€ ä¸€
ä¸ªæµ…æ‰©æ•£å˜å‹å™¨
training cost, achieving a 2Ã— speedup with an FID score
of 2.86, outperforming competitors with comparable effiåŸ¹è®­æˆæœ¬ï¼Œå®ç°äº† 2 å€çš„åŠ é€Ÿï¼ŒFID å¾—åˆ†ä¸º 2.86ï¼Œåœ¨
æ•ˆç‡ç›¸å½“çš„æƒ…å†µä¸‹ä¼˜äºç«äº‰å¯¹æ‰‹
ciency. Code is available at https://github.com/
VainF/TinyFusion
æ•ˆç‡ã€‚ä»£ç å¯ä»ä»¥ä¸‹ç½‘å€è·å¾— https://github.com/
VainF/TinyFusion
1. Introduction
2. ä»‹ç»
Diffusion Transformers have emerged as a cornerstone architecture for generative tasks, achieving notable success in
areas such as image [11, 26, 40] and video synthe- sis
[25, 59]. This success has also led to the widespread
availability of high-quality pre-trained models on the Internet, greatly accelerating and supporting the development of
various downstream applications [5, 16, 53, 55]. However,
pre-trained diffusion transformers usually come with conæ‰©æ•£å˜å‹å™¨å·²ç»æˆä¸ºç”Ÿæˆä»»åŠ¡çš„åŸºç¡€æ¶æ„ï¼Œåœ¨å›¾åƒç­‰é¢†
åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸ[11,26,40]å’Œ è§† é¢‘ åˆ æˆ [25,59].
è¿™ ä¸€ æˆ åŠŸ ä¹Ÿ å¯¼ è‡´ äº† é«˜ è´¨ é‡ çš„ é¢„ è®­ ç»ƒ æ¨¡ å‹ åœ¨ äº’ è”
ç½‘ ä¸Š çš„ å¹¿ æ³› ä½¿ ç”¨ ï¼Œ æ å¤§ åœ° åŠ  é€Ÿ å’Œ æ”¯ æŒ äº† å„ ç§ ä¸‹
æ¸¸ åº” ç”¨ ç¨‹ åº çš„ å¼€ å‘ [5,16,53,55].ç„¶è€Œï¼Œé¢„å…ˆè®­ç»ƒ
çš„æ‰©æ•£å˜å‹å™¨é€šå¸¸é™„å¸¦æœ‰
*Equal contribution
*å¹³ç­‰è´¡çŒ®
â€ Corresponding author a
é€šè®¯ä½œè€…
r
X
i
v:2
4
1
2.0
1
1
9
9
v
1 [c
s.C
V] 2
D
e
c
a
r
X
i
v:2
4
1
2.0
1
1
9
9
v
1
[
c
s.C
V
]
2
3
Figure 1. This work presents a learnable approach for pruning
the depth of pre-trained diffusion transformers. Our method
simulta- neously optimizes a differentiable sampling process of
layer masks and a weight update to identify a highly
recoverable solution, en- suring that the pruned model
maintains competitive performance after fine-tuning.
å›¾ä¸€ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•æ¥ä¿®å‰ªé¢„è®­ç»ƒæ‰©æ•£
å˜å‹å™¨çš„æ·±åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶ä¼˜åŒ–äº†å±‚æ©ç çš„å¯å¾®åˆ†é‡‡æ ·
è¿‡ç¨‹å’Œæƒé‡æ›´æ–°ï¼Œä»¥ç¡®å®šé«˜åº¦å¯æ¢å¤çš„è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿ä¿®å‰ª
æ¨¡å‹åœ¨å¾®è°ƒåä¿æŒæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚
siderable inference costs due to the huge parameter scale,
which poses significant challenges for deployment. To resolve this problem, there has been growing interest from
both the research community and industry in developing
lightweight models [12, 23, 32, 58].
å·¨å¤§çš„å‚æ•°è§„æ¨¡å¸¦æ¥äº†å¯è§‚çš„æ¨ç†æˆæœ¬ï¼Œè¿™ç»™éƒ¨ç½²å¸¦
æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†é‡æ–°è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶ç•Œå’Œ
å·¥ ä¸š ç•Œ å¯¹ å¼€ å‘ è½» å‹ æ¨¡ å‹ çš„ å…´ è¶£ è¶Š æ¥ è¶Š å¤§
[12,23,32,58].
The efficiency of diffusion models is typically influenced by various factors, including the number of
sampling steps [33, 43, 45, 46], operator design [7, 48,
52], compuæ‰©æ•£æ¨¡å‹çš„æ•ˆç‡é€šå¸¸å—å„ç§å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬å–æ ·
æ­¥éª¤çš„æ•°é‡[33,43,45,46],æ“ä½œå‘˜è®¾è®¡[7,48,52],è®¡
ç®—æœºtational precision [19, 30, 44], network width [3, 12] and
depth [6, 23, 36]. In this work, we focus on model
compres- sion through depth pruning [36, 54], which
removes entire layers from the network to reduce the
latency. Depth prun- ing offers a significant advantage in
practice: it can achieve a linear acceleration ratio relative
to the compression rate on both parallel and non-parallel
devices. For example, as will be demonstrated in this
work, while 50% width prun- ing [12] only yields a 1.6Ã—
speedup, pruning 50% of the layers results in a 2Ã—
speedup. This makes depth pruning a flexible and practical
method for model compression.
é™ æ€ ç²¾ åº¦ [19,30,44], ç½‘ ç»œ å®½ åº¦ [3,12] å’Œ æ·± åº¦
[6,23,36].åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é›†ä¸­äºé€šè¿‡æ·±åº¦å‰ªæ
çš„æ¨¡å‹å‹ç¼©[36,54],ä»ç½‘ç»œä¸­åˆ é™¤æ•´ä¸ªå±‚ä»¥å‡å°‘å»¶
è¿Ÿã€‚æ·±åº¦å‹ç¼©åœ¨å®è·µä¸­æä¾›äº†ä¸€ä¸ªæ˜¾è‘—çš„ä¼˜åŠ¿:å®ƒå¯ä»¥
åœ¨å¹¶è¡Œå’Œéå¹¶è¡Œè®¾å¤‡ä¸Šå®ç°ç›¸å¯¹äºå‹ç¼©ç‡çš„çº¿æ€§åŠ é€Ÿ
æ¯”ã€‚ä¾‹å¦‚ï¼Œæ­£å¦‚æœ¬ç ”ç©¶å°†å±•ç¤ºçš„ï¼Œå½“ 50%å®½åº¦ä¿®å‰ªæ—¶
[12]ä»…äº§ç”Ÿ 1.6 å€çš„åŠ é€Ÿï¼Œä¿®å‰ª 50%çš„å±‚ä¼šäº§ç”Ÿ 2 å€çš„
åŠ é€Ÿã€‚è¿™ä½¿å¾—æ·±åº¦ä¿®å‰ªæˆä¸ºæ¨¡å‹å‹ç¼©çš„çµæ´»ä¸”å®ç”¨çš„
æ–¹æ³•ã€‚
This work follows a standard depth pruning framework: unimportant layers are first removed, and the pruned
model is then fine-tuned for performance recovery. In
the literature, depth pruning techniques designed for diffusion transformers or general transformers primarily focus on heuristic approaches, such as carefully designed
importance scores [6, 36] or manually configured pruning
è¿™é¡¹å·¥ä½œéµå¾ªæ ‡å‡†çš„æ·±åº¦ä¿®å‰ªæ¡†æ¶:é¦–å…ˆåˆ é™¤ä¸é‡
4
è¦çš„å±‚ï¼Œç„¶åå¾®è°ƒä¿®å‰ªåçš„æ¨¡å‹ä»¥æ¢å¤æ€§èƒ½ã€‚åœ¨æ–‡
çŒ®ä¸­ï¼Œä¸ºæ‰©æ•£å˜å‹å™¨æˆ–é€šç”¨å˜å‹å™¨è®¾è®¡çš„æ·±åº¦å‰ªæ
æŠ€æœ¯ä¸»è¦é›†ä¸­äºå¯å‘å¼æ–¹æ³•ï¼Œä¾‹å¦‚ç²¾å¿ƒè®¾è®¡çš„é‡è¦
æ€§åˆ†æ•°[6,36]æˆ–è€…æ‰‹åŠ¨é…ç½®ä¿®å‰ª
5
schemes [23, 54]. These methods adhere to a loss minimization principle [18, 37], aiming to identify solutions
that maintain low loss or error after pruning. This paper
investigates the effectiveness of this widely used principle
in the context of depth compression. Through
experiments, we examined the relationship between
calibration loss ob- served post-pruning and the
performance after fine-tuning. This is achieved by
extensively sampling 100,000 models via random pruning,
exhibiting different levels of calibra- tion loss in the
searching space. Based on this, we analyzed the
effectiveness of existing pruning algorithms, such as the
feature similarity [6, 36] and sensitivity analysis [18],
which indeed achieve low calibration losses in the solution
space. However, the performance of all these models after
fine- tuning often falls short of expectations. This
indicates that the loss minimization principle may not be
well-suited for diffusion transformers.
è®¡åˆ’[23,54].è¿™ äº› æ–¹ æ³• éµ å¾ª æŸ å¤± æœ€ å° åŒ– åŸ åˆ™
[18,37],æ—¨åœ¨è¯†åˆ«ä¿®å‰ªåä¿æŒä½æŸå¤±æˆ–è¯¯å·®çš„è§£å†³æ–¹
æ¡ˆã€‚æœ¬æ–‡ç ”ç©¶äº†è¿™ä¸€å¹¿æ³›ä½¿ç”¨çš„åŸåˆ™åœ¨æ·±åº¦å‹ç¼©ç¯å¢ƒ
ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æ£€éªŒäº†ä¿®å‰ªåçš„æ ¡å‡†æŸ
å¤±å’Œå¾®è°ƒåçš„æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚è¿™æ˜¯é€šè¿‡éšæœºå‰ªæå¯¹
100ï¼Œ000 ä¸ªæ¨¡å‹è¿›è¡Œå¹¿æ³›é‡‡æ ·æ¥å®ç°çš„ï¼Œåœ¨æœç´¢ç©ºé—´
ä¸­è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„æ ¡å‡†æŸå¤±ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œåˆ†æäº†
ç°æœ‰å‰ªæç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¦‚ç‰¹å¾ç›¸ä¼¼åº¦[6,36]å’Œæ•æ„Ÿ
æ€§åˆ†æ[18],è¿™ç¡®å®å®ç°äº†è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­çš„ä½æ ¡å‡†æŸ
è€—ã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›æ¨¡å‹åœ¨å¾®è°ƒåçš„æ€§èƒ½å¾€å¾€è¾¾ä¸åˆ°
é¢„æœŸã€‚è¿™è¡¨æ˜æŸè€—æœ€å°åŒ–åŸç†å¯èƒ½ä¸å¤ªé€‚åˆæ‰©æ•£å˜å‹
å™¨ã€‚
Building on these insights, we reassessed the underlying principles for effective layer pruning in diffusion
trans- formers. Fine-tuning diffusion transformers is an
extremely time-consuming process. Instead of searching
for a model that minimizes loss immediately after pruning,
we propose identifying candidate models with strong
recoverability, en- abling superior post-fine-tuning
performance. Achieving this goal is particularly
challenging, as it requires the in- tegration of two distinct
processes, pruning and fine-tuning, which involve nondifferentiable operations and cannot be directly optimized
via gradient descent.
åŸºäºè¿™äº›è®¤è¯†ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†æ‰©æ•£å˜å‹å™¨ä¸­æœ‰æ•ˆ
å±‚ä¿®å‰ªçš„åŸºæœ¬åŸç†ã€‚å¾®è°ƒæ‰©æ•£å˜å‹å™¨æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶
çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬å»ºè®®è¯†åˆ«å…·æœ‰å¼ºå¯æ¢å¤æ€§çš„å€™é€‰æ¨¡å‹ï¼Œ
ä»è€Œå®ç°å“è¶Šçš„å¾®è°ƒåæ€§èƒ½ï¼Œè€Œä¸æ˜¯åœ¨ä¿®å‰ªåç«‹å³æœ
ç´¢æœ€å°åŒ–æŸå¤±çš„æ¨¡å‹ã€‚å®ç°è¿™ä¸ªç›®æ ‡å°¤å…¶å…·æœ‰æŒ‘æˆ˜
æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦é›†æˆä¸¤ä¸ªä¸åŒçš„è¿‡ç¨‹ï¼Œä¿®å‰ªå’Œå¾®è°ƒï¼Œ
è¿™æ¶‰åŠä¸å¯å¾®çš„æ“ä½œï¼Œå¹¶ä¸”ä¸èƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜
åŒ–ã€‚
To this end, we propose a learnable depth pruning
method that effectively integrates pruning and fine-tuning.
As shown in Figure 1, we model the pruning and finetuning of a diffusion transformer as a differentiable sampling process of layer masks [13, 17, 22], combined with a
co-optimized weight update to simulate future fine-tuning.
Our objective is to iteratively refine this distribution so that
networks with higher recoverability are more likely to be
sampled. This is achieved through a straightforward strategy: if a sampled pruning decision results in strong recoverability, similar pruning patterns will have an increased probability of being sampled. This approach promotes the exploration of potentially valuable solutions while disregarding less effective ones. Additionally, the proposed method is
highly efficient, and we demonstrate that a suitable solu- tion
can emerge within a few training steps.
ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œæœ‰æ•ˆ
åœ°é›†æˆäº†å‰ªæå’Œå¾®è°ƒã€‚å¦‚å›¾æ‰€ç¤º 1,æˆ‘ä»¬å°†æ‰©æ•£å˜æ¢å™¨çš„
ä¿®å‰ªå’Œå¾®è°ƒå»ºæ¨¡ä¸ºå±‚æ©æ¨¡çš„å¯å¾®åˆ†é‡‡æ ·è¿‡ç¨‹[13,17,22],
ç»“åˆå…±åŒä¼˜åŒ–çš„æƒé‡æ›´æ–°æ¥æ¨¡æ‹Ÿæœªæ¥çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ç›®
æ ‡æ˜¯åå¤ä¼˜åŒ–è¿™ç§åˆ†å¸ƒï¼Œä»¥ä¾¿å…·æœ‰æ›´é«˜å¯æ¢å¤æ€§çš„ç½‘ç»œ
æ›´æœ‰å¯èƒ½è¢«é‡‡æ ·ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªç®€å•çš„ç­–ç•¥å®ç°çš„:å¦‚æœ
ä¸€ä¸ªæŠ½æ ·çš„å‰ªæå†³ç­–å¯¼è‡´å¼ºçš„å¯æ¢å¤æ€§ï¼Œåˆ™ç›¸ä¼¼çš„å‰ªæ
æ¨¡å¼è¢«æŠ½æ ·çš„å¯èƒ½æ€§å°†å¢åŠ ã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†å¯¹æ½œåœ¨æœ‰
ä»·å€¼çš„è§£å†³æ–¹æ¡ˆçš„æ¢ç´¢ï¼Œè€Œå¿½ç•¥äº†ä¸å¤ªæœ‰æ•ˆçš„è§£å†³æ–¹
æ¡ˆã€‚æ­¤å¤–ï¼Œæå‡ºçš„æ–¹æ³•æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ª
åˆé€‚çš„è§£å†³æ–¹æ¡ˆå¯ä»¥å‡ºç°åœ¨å‡ ä¸ªè®­ç»ƒæ­¥éª¤ã€‚
To evaluate the effectiveness of the proposed method, we
conduct extensive experiments on various transformerbased diffusion models, including DiTs [40], MARs [29],
SiTs [34]. The learnable approach is highly efficient. It is
able to identify redundant layers in diffusion transform- ers
with 1-epoch training on the dataset, which effectively crafts
shallow diffusion transformers from pre-trained mod- els
with high recoverability. For instance, while the models
pruned by TinyFusion initially exhibit relatively high calä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹å„ç§åŸºäºå˜
å‹å™¨çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†å¤§é‡çš„å®éªŒï¼ŒåŒ…æ‹¬ DiTs[40],ç«
æ˜Ÿ[29],å[34].è¿™ ç§ å¯ å­¦ çš„ æ–¹ æ³• é å¸¸ æœ‰ æ•ˆ ã€‚
å®ƒ èƒ½ å¤Ÿ åˆ© ç”¨ æ•° æ® é›† ä¸Š çš„ 1 ä¸ª å† å…ƒ è®­ ç»ƒ æ¥
è¯† åˆ« æ‰© æ•£ å˜ æ¢ å™¨ ä¸­ çš„ å†— ä½™ å±‚ ï¼Œ è¿™ æœ‰ æ•ˆ åœ°
ä» å…· æœ‰ é«˜ æ¢ å¤ æ€§ çš„ é¢„ è®­ ç»ƒ æ¨¡ å‹ ä¸­ åˆ¶ ä½œ æµ…
å±‚ æ‰© æ•£ å˜ æ¢ å™¨ ã€‚ ä¾‹ å¦‚ ï¼Œ è™½ ç„¶ é€š è¿‡
T i n y F u s i o n ä¿® å‰ª çš„ æ¨¡ å‹ æœ€ åˆ è¡¨ ç° å‡º ç›¸
å¯¹ è¾ƒ é«˜ çš„ c a l -
6
ibration loss after removing 50% of layers, they
recover quickly through fine-tuning, achieving a
significantly more competitive FID score (5.73 vs.
22.28) compared to base- line methods that only
minimize immediate loss, using just 1% of the pretraining cost. Additionally, we also
ex- plore the role of knowledge distillation in
enhancing re- coverability [20, 23] by introducing a
MaskedKD variant. MaskedKD mitigates the negative
impact of the massive or outlier activations [47] in
hidden states, which can signifi- cantly affect the
performance and reliability of fine-tuning. With
MaskedKD, the FID score improves from 5.73 to 3.73
with only 1% of pre-training cost. Extending the training
to 7% of the pre-training cost further reduces the FID to
2.86, just 0.4 higher than the original model with
doubled depth. Therefore, the main contribution of this
work lies in a learnable method to craft shallow
diffusion transformers from pre-trained ones, which
explicitly optimizes the re- coverability of pruned
models. The method is general for
æŒ¯åŠ¨æŸå¤±åœ¨ç§»é™¤ 50%çš„å±‚åï¼Œå®ƒä»¬é€šè¿‡å¾®è°ƒå¿«é€Ÿæ¢
å¤ï¼Œä¸ä»…æœ€å°åŒ–å³æ—¶æŸå¤±çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè·å¾—äº†æ›´
å…·ç«äº‰åŠ›çš„ FID åˆ†æ•°(5.73 å¯¹ 22.28)ï¼Œä»…ä½¿ç”¨ 1%çš„é¢„
è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†çŸ¥è¯†æç‚¼åœ¨æé«˜å¯é‡
ç”¨æ€§ä¸­çš„ä½œç”¨[20,23]é€šè¿‡å¼•å…¥ MaskedKD å˜
ä½“ã€‚MaskedKD å‡è½»äº†å¤§é‡æˆ–å¼‚å¸¸æ¿€æ´»çš„è´Ÿé¢å½±å“[47]
è¿™å¯èƒ½ä¼šæ˜¾è‘—å½±å“å¾®è°ƒçš„æ€§èƒ½å’Œå¯é æ€§ã€‚ä½¿ç”¨
MaskedKDï¼ŒFID åˆ†æ•°ä» 5.73 æé«˜åˆ° 3.73ï¼Œè€Œåªéœ€ 1%çš„
é¢„åŸ¹è®­æˆæœ¬ã€‚å°†è®­ç»ƒæ‰©å±•åˆ°é¢„è®­ç»ƒæˆæœ¬çš„ 7%è¿›ä¸€æ­¥å°†
FID é™ä½åˆ° 2.86ï¼Œä»…æ¯”æ·±åº¦åŠ å€çš„åŸå§‹æ¨¡å‹é«˜ 0.4ã€‚å› 
æ­¤ï¼Œè¿™é¡¹å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•ï¼Œä»
é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨æ¥åˆ¶ä½œæµ…å±‚æ‰©æ•£å˜æ¢å™¨ï¼Œè¿™ç§æ–¹
æ³•æ˜¾å¼åœ°ä¼˜åŒ–äº†ä¿®å‰ªæ¨¡å‹çš„å¯è¦†ç›–æ€§ã€‚è¯¥æ–¹æ³•ä¸€èˆ¬ç”¨
äº
various architectures, including DiTs, MARs and SiTs.
å„ç§æ¶æ„ï¼ŒåŒ…æ‹¬ DiTsï¼ŒMARs å’Œ SiTsã€‚
3. Related Works
4. ç›¸å…³ä½œå“
Network Pruning and Depth Reduction. Network
prun- ing is a widely used approach for compressing pretrained diffusion models by eliminating redundant
parameters [3, 12, 31, 51]. Diff-Pruning [12] introduces a
gradient- based technique to streamline the width of
UNet, fol- lowed by a simple fine-tuning to recover the
performance. SparseDM [51] applies sparsity to pretrained diffusion models via the Straight-Through
Estimator (STE) [2], achieving a 50% reduction in MACs
with only a 1.22 in- crease in FID on average. While
width pruning and spar- sity help reduce memory
overhead, they often offer lim- ited speed improvements,
especially on parallel devices like GPUs. Consequently,
depth reduction has gained signifi- cant attention in the
past few years, as removing entire lay- ers enables better
speedup proportional to the pruning ra- tio [24, 27, 28, 36,
54, 56, 58]. Adaptive depth reduction techniques, such as
MoD [41] and depth-aware transform- ers [10], have also
been proposed. Despite these advances, most existing
methods are still based on empirical or heuris- tic
strategies, such as carefully designed importance crite- ria
[36, 54], sensitivity analyses [18] or manually designed
schemes [23], which often do not yield strong performance
guarantee after fine-tuning.
ç½‘ç»œä¿®å‰ªå’Œæ·±åº¦ç¼©å‡ã€‚ç½‘ç»œä¿®å‰ªæ˜¯é€šè¿‡æ¶ˆé™¤å†—ä½™å‚æ•°
æ¥ å‹ ç¼© é¢„ è®­ ç»ƒ æ‰© æ•£ æ¨¡ å‹ çš„ å¹¿ æ³› ä½¿ ç”¨ çš„ æ–¹ æ³•
[3,12,31,51].å·® å¼‚ ä¿® å‰ª [12]å¼•å…¥åŸºäºæ¢¯åº¦çš„æŠ€æœ¯
æ¥ç®€åŒ– UNet çš„å®½åº¦ï¼Œç„¶åé€šè¿‡ç®€å•çš„å¾®è°ƒæ¥æ¢å¤æ€§
èƒ½ã€‚SparseDM[51]é€šè¿‡ç›´é€šä¼°è®¡å™¨(STE)å°†ç¨€ç–æ€§åº”ç”¨
äºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹[2],MAC å‡å°‘ 50%ï¼Œè€Œ FID å¹³å‡ä»…
å¢åŠ  1.22%ã€‚è™½ç„¶å®½åº¦ä¿®å‰ªå’Œç¨€ç–æ€§æœ‰åŠ©äºå‡å°‘å†…å­˜
å¼€é”€ï¼Œä½†å®ƒä»¬é€šå¸¸åªèƒ½æä¾›æœ‰é™çš„é€Ÿåº¦æå‡ï¼Œå°¤å…¶æ˜¯
åœ¨ GPU ç­‰å¹¶è¡Œè®¾å¤‡ä¸Šã€‚å› æ­¤ï¼Œåœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œæ·±åº¦
7
å‡å°‘å·²ç»è·å¾—äº†æå¤§çš„å…³æ³¨ï¼Œå› ä¸ºç§»é™¤æ•´ä¸ªå±‚èƒ½å¤Ÿ
å® ç° ä¸ ä¿® å‰ª æ¯” ç‡ æˆ æ¯” ä¾‹ çš„ æ›´ å¥½ çš„ åŠ  é€Ÿ
[24,27,28,36,54,56,58]. è‡ª é€‚ åº” æ·± åº¦ å‡ å°‘ æŠ€
æœ¯ ï¼Œ ä¾‹ å¦‚ M o D [41]å’Œæ·±åº¦æ„ŸçŸ¥è½¬æ¢å™¨[10],ä¹Ÿè¢«æ
å‡ºæ¥äº†ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ–¹æ³•
ä»ç„¶æ˜¯åŸºäºç»éªŒæˆ–å¯å‘å¼ç­–ç•¥ï¼Œå¦‚ç²¾å¿ƒè®¾è®¡çš„é‡è¦
æ€§æ ‡å‡†[36,54],æ•æ„Ÿæ€§åˆ†æ[18]æˆ–è€…äººå·¥è®¾è®¡çš„æ–¹
æ¡ˆ[23],è¿™é€šå¸¸åœ¨å¾®è°ƒåä¸èƒ½äº§ç”Ÿå¼ºæœ‰åŠ›çš„æ€§èƒ½ä¿
è¯ã€‚
Efficient Diffusion Transformers. Developing
efficient diffusion transformers has become an
appealing focus within the community, where
significant efforts have been made to enhance efficiency
from various perspectives, in- cluding linear attention
mechanisms [15, 48, 52], compact
é«˜æ•ˆæ‰©æ•£å˜å‹å™¨ã€‚å¼€å‘æœ‰æ•ˆçš„æ‰©æ•£å˜å‹å™¨å·²ç»æˆä¸º
ç¤¾åŒºå†…å¸å¼•äººçš„ç„¦ç‚¹ï¼Œå…¶ä¸­å·²ç»ä»å„ç§è§’åº¦è¿›è¡Œäº†
æ˜¾ è‘— çš„ åŠª åŠ› æ¥ æ é«˜ æ•ˆ ç‡ ï¼Œ åŒ… æ‹¬ çº¿ æ€§ æ³¨ æ„ æœº åˆ¶
[15,48,52],å°å‹çš„ï¼Œç´§å‡‘çš„
architectures [50], non-autoregressive transformers [4, 14,
ç»“æ„[50],éè‡ªå›å½’å˜å‹å™¨[4,14,
38, 49], pruning [12, 23], quantization [19, 30, 44], feature
38,49],ä¿®å‰ª[12,23],é‡åŒ–[19,30,44],ç‰¹å¾
8
Learnable Distribution
å¯å­¦åˆ†å¸ƒ
ğ¦ğ¢ğ§ğ–’,ğš«ğš½ ğ“›(ğ’™, ğš½ + ğš«ğš½, ğ–’)
9
1:2 Local Blocks
ğ“ğŸ’Transformer Layer ğ”ª4 1 0
ğ“ğŸ‘ Transformer Layer ğ”ª3 0 1
1:2 Local Blocks
ğ“ğŸ Transformer Layer ğ”ª21 0
ğ“ğŸ Transformer Layer ğ”ª10 1
1:2 Local Blocks
ğ“ğŸ’Transformer Layer ğ”ª4 1 0
ğ“ğŸ‘ Transformer Layer ğ”ª3 0 1
1:2 Local Blocks
ğ“ğŸ Transformer Layer ğ”ª21 0
ğ“ğŸ Transformer Layer ğ”ª10 1
ğ¦ğ¢ğ§ğ–’ ğš«ğš½ ğ“› ğ’™ ğš½ ğš«ğš½ ğ–’ , ( , + , )
Diff.
Sampling
å·®å¼‚ã€‚æŠ½
æ ·
âˆ¼
âˆ¼
Retained Layer
ä¿ç•™å±‚
Weight
Update
10
Retained Layer
Retained Layer
é‡é‡æ›´æ–°
âŠ•
âŠ•
Weight Update
é‡é‡æ›´æ–°
Î”
ï¿½
ï¿½
4
â‹… ğ”ª4
Î”ğœ™3 â‹…
ğ”ª3
Î”ğœ™4
â‹… ğ”ª4
Î”ğœ™3
â‹… ğ”ª3
Mixed Sampling â‡’ Exploration still in Progress
æ··åˆå–æ ·å‹˜æ¢ä»åœ¨è¿›è¡Œä¸­
Diff.
Sampling
å·®å¼‚ã€‚æŠ½
æ ·
âˆ¼
âˆ¼
Weight Update é‡é‡æ›´
æ–°
âŠ•
âŠ•
Weight
Update
é‡é‡æ›´
æ–°
11
1 2 L
1 2 L
Î”ğœ™2 â‹… ğ”ª2 Î”ğœ™1 â‹… ğ”ª1 Î”ğœ™ â‹… ğ”ª ğœ™ â‹… ğ”ª 2 2 Î” 1 1
Confident Sampling â‡’ Good solution identified
è‡ªä¿¡çš„é‡‡æ ·ç¡®å®šäº†è‰¯å¥½çš„è§£å†³æ–¹æ¡ˆ
Figure 2. The proposed TinyFusion method learns to perform a differentiable sampling of candidate solutions, jointly optimized with a
weight update to estimate recoverability. This approach aims to increase the likelihood of favorable solutions that ensure strong postfine- tuning performance. After training, local structures with the highest sampling probabilities are retained.
å›¾äºŒã€‚æå‡ºçš„ TinyFusion æ–¹æ³•å­¦ä¹ æ‰§è¡Œå€™é€‰è§£çš„å¯åŒºåˆ†é‡‡æ ·ï¼Œä¸æƒé‡æ›´æ–°è”åˆä¼˜åŒ–ä»¥ä¼°è®¡å¯æ¢å¤æ€§ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å¢åŠ æœ‰åˆ©
è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ï¼Œä»¥ç¡®ä¿å¼ºå¤§çš„å¾®è°ƒåæ€§èƒ½ã€‚åœ¨è®­ç»ƒä¹‹åï¼Œå…·æœ‰æœ€é«˜é‡‡æ ·æ¦‚ç‡çš„å±€éƒ¨ç»“æ„è¢«ä¿ç•™ã€‚
caching [35, 57], etc. In this work, we focus on compressing the depth of pre-trained diffusion transformers and introduce a learnable method that directly optimizes
recover- ability, which is able to achieve satisfactory results
with low re-training costs.
è´®è—[35,57],ç­‰ç­‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é›†ä¸­äºå‹ç¼©
é¢„è®­ç»ƒæ‰©æ•£å˜å‹å™¨çš„æ·±åº¦ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§å¯å­¦ä¹ çš„æ–¹
æ³•ï¼Œç›´æ¥ä¼˜åŒ–æ¢å¤èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„é‡æ–°è®­ç»ƒæˆæœ¬
è·å¾—æ»¡æ„çš„ç»“æœã€‚
5. Method
6. æ–¹æ³•
6.1. Shallow Generative Transformers by Pruning
6.2. åŸºäºå‰ªæçš„æµ…å±‚ç”Ÿæˆå˜å‹å™¨
This work aims to derive a shallow diffusion transformer
by pruning a pre-trained model. For simplicity, all vectors
in this paper are column vectors. Consider a L-layer transformer, parameterized by Î¦LÃ—D = [Ï• , Ï• , Â· Â· Â· , Ï• ]
âŠº
,
where each element Ï•i
encompasses all learnable parameters of a transformer layer as a D-dim column vector,
è¿™é¡¹å·¥ä½œçš„ç›®çš„æ˜¯é€šè¿‡ä¿®å‰ªä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¾—
åˆ°ä¸€ä¸ªæµ…å±‚æ‰©æ•£å˜å‹å™¨ã€‚ä¸ºç®€å•èµ·è§ï¼Œæœ¬æ–‡ä¸­æ‰€æœ‰å‘
é‡å‡ä¸ºåˆ—å‘é‡ã€‚è€ƒè™‘ç”± Ï†lÃ—d =[Ï•ï¼ŒÏ•ï¼ŒÏ• âŠº] ]å‚æ•°åŒ–
çš„ l å±‚å˜å‹å™¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´  Ï•i åŒ…å«å˜å‹å™¨å±‚çš„æ‰€æœ‰
å¯å­¦ä¹ å‚æ•°ä½œä¸º d ç»´åˆ—å‘é‡ï¼Œ
which includes the weights of both attention layers and
MLPs. Depth pruning seeks to find a binary layer mask
mLÃ—1 = [m1, m2, Â· Â· Â· , mL]
âŠº
, that removes a layer by:
å…¶åŒ…æ‹¬å…³æ³¨å±‚å’Œ MLPs çš„æƒé‡ã€‚æ·±åº¦ä¿®å‰ªå¯»æ±‚æ‰¾åˆ°äºŒè¿›
åˆ¶å±‚æ©ç  m1Ã—1 =[m1ï¼Œm2ï¼Œï¼ŒmL]âŠºï¼Œå®ƒé€šè¿‡ä»¥ä¸‹æ–¹å¼
ç§»é™¤å±‚:
where âˆ†Î¦ = {âˆ†Ï•1
, âˆ†Ï•2
, Â· Â· Â· , âˆ†Ï•M } represents
appro- priate update from fine-tuning. The objective
formulated by Equation 2 poses two challenges: 1) The
non-differentiable
12
i i i i i
i i i i i
å…¶ä¸­âˆÏ†= {âˆ†Ï•1ï¼Œâˆ†Ï•2ï¼Œâˆ†Ï•M }ä»£è¡¨å¾®è°ƒçš„é€‚å½“æ›´
æ–°ã€‚ç”±ç­‰å¼è¡¨è¾¾çš„ç›®æ ‡ 2 æå‡ºäº†ä¸¤ä¸ªæŒ‘æˆ˜:1)ä¸å¯å¾®
nature of layer selection prevents direct optimization using gradient descent; 2) The inner optimization over the
retained layers makes it computationally intractable to
ex- plore the entire search space, as this process
necessitates se- lecting a candidate model and finetuning it for evaluation. To address this, we propose
TinyFusion that makes both the pruning and
recoverability optimizable.
å±‚é€‰æ‹©çš„æ€§è´¨é˜»æ­¢äº†ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„ç›´æ¥ä¼˜åŒ–ï¼›2)
å¯¹ä¿ç•™å±‚çš„å†…éƒ¨ä¼˜åŒ–ä½¿å¾—å¼€å‘æ•´ä¸ªæœç´¢ç©ºé—´åœ¨è®¡ç®—
ä¸Šéš¾ä»¥å¤„ç†ï¼Œå› ä¸ºè¯¥è¿‡ç¨‹éœ€è¦é€‰æ‹©å€™é€‰æ¨¡å‹å¹¶å¯¹å…¶
è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ
å‡ºäº† TinyFusionï¼Œä½¿å¾—ä¿®å‰ªå’Œå¯æ¢å¤æ€§éƒ½æ˜¯å¯ä¼˜åŒ–
çš„ã€‚
6.3. TinyFusion: Learnable Depth Pruning
6.4. TinyFusion:å¯å­¦ä¹ çš„æ·±åº¦ä¿®å‰ª
A Probabilistic Perspective. This work models Equation 2 from a probabilistic standpoint. We hypothesize that
the mask m produced by â€œidealâ€ pruning methods (might
be not unique) should follow a certain distribution. To
model this, it is intuitive to associate every possible mask
æ¦‚ç‡è§‚ç‚¹ã€‚è¿™éƒ¨ä½œå“æ¨¡æ‹Ÿäº†ç­‰å¼ 2 ä»æ¦‚ç‡çš„è§’åº¦æ¥
çœ‹ã€‚æˆ‘ä»¬å‡è®¾ç”±â€œç†æƒ³çš„â€ä¿®å‰ªæ–¹æ³•(å¯èƒ½ä¸æ˜¯å”¯ä¸€çš„)
äº§ç”Ÿçš„æ©ç  m åº”è¯¥éµå¾ªä¸€å®šçš„åˆ†å¸ƒã€‚ä¸ºäº†å¯¹æ­¤è¿›è¡Œå»º
æ¨¡ï¼Œå°†æ¯ä¸ªå¯èƒ½çš„æ©ç å…³è”èµ·æ¥æ˜¯å¾ˆç›´è§‚çš„
x = m Ï• (x ) + (1 âˆ’ m
)x
x = m (x)+(1m)x Ï•
=
(
Ï•i
(xi),if mi = 1, =( i(xi),if Ï• Â·ç±³= 1ï¼Œ
13
xi,
xi,
m with a probability value
p(m), thus forming a categorim å…·æœ‰æ¦‚ç‡å€¼ p(m)ï¼Œä»
è€Œå½¢æˆä¸€ä¸ªèŒƒç•´
(1)
(1)
where the xi and Ï•i
(xi) refers to the input and output of
layer Ï•i
. To obtain the mask, a common paradigm in prior
work is to minimize the loss L after pruning, which can
å…¶ä¸­ï¼ŒÏ•i(xi å’Œ xi æŒ‡çš„æ˜¯å±‚ Ï•i.çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥
è·å¾—æ©ç ï¼Œç°æœ‰å·¥ä½œä¸­çš„å¸¸è§èŒƒä¾‹æ˜¯åœ¨ä¿®å‰ªä¹‹åæœ€
å°åŒ–æŸå¤± lï¼Œè¿™å¯ä»¥
be formulated as minm Ex [L(x, Î¦, m)]. However, as
we
å…¬å¼åŒ–ä¸º minm Ex [L(xï¼ŒÏ†ï¼Œm)]ã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬
will show in the experiments, this objective â€“ though widely
å°†åœ¨å®éªŒä¸­è¡¨æ˜ï¼Œè¿™ä¸ªç›®æ ‡â€”â€”è™½ç„¶å¹¿æ³›
adopted in discriminative tasks â€“ may not be well-suited to
pruning diffusion transformers. Instead, we are more interested in the recoverability of pruned models. To achieve
this, we incorporate an additional weight update into the
optimization problem and extend the objective by:
åœ¨è¾¨åˆ«ä»»åŠ¡ä¸­é‡‡ç”¨-å¯èƒ½ä¸å¤ªé€‚åˆä¿®å‰ªæ‰©æ•£å˜å‹å™¨ã€‚ç›¸
åï¼Œæˆ‘ä»¬å¯¹ä¿®å‰ªæ¨¡å‹çš„å¯æ¢å¤æ€§æ›´æ„Ÿå…´è¶£ã€‚ä¸ºäº†å®ç°
è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†é¢å¤–çš„æƒé‡æ›´æ–°åˆå¹¶åˆ°ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œ
å¹¶é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰©å±•ç›®æ ‡:
ment of pruning masks begins with a uniform distribution. cal distribution. Without any prior knowledge, the
cal distribution. Without any prior
i
i
14
ating = 40, 116, 600 possible solutions. To
overcome
ating = 40, 116, 600 possible solutions.
To overcome
ä¿®å‰ªé®ç½©çš„æ­¥éª¤ä»å‡åŒ€åˆ†å¸ƒå¼€å§‹ã€‚
However, directly sampling from this initial distribution
is highly inefficient due to the vast search space. For instance, pruning a 28-layer model by 50% involves
evaluç„¶è€Œï¼Œç”±äºå·¨å¤§çš„æœç´¢ç©ºé—´ï¼Œä»è¯¥åˆå§‹åˆ†å¸ƒç›´æ¥é‡‡
æ ·æ˜¯éå¸¸ä½æ•ˆçš„ã€‚ä¾‹å¦‚ï¼Œå°† 28 å±‚æ¨¡å‹ä¿®å‰ª 50%æ¶‰åŠ
è¯„ä¼°
28
28
14
14
this challenge, this work introduces an advanced and learnè¿™é¡¹æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§å…ˆè¿›çš„å­¦ä¹ able algorithm capable of using evaluation results as feedback to iteratively refine the mask distribution. The basic
idea is that if certain masks exhibit positive results, then
other masks with similar pattern may also be potential solutions and thus should have a higher likelihood of samèƒ½å¤Ÿä½¿ç”¨è¯„ä¼°ç»“æœä½œä¸ºåé¦ˆæ¥è¿­ä»£æ”¹è¿›æ©æ¨¡åˆ†å¸ƒçš„ç®—
æ³•ã€‚åŸºæœ¬æ€æƒ³æ˜¯ï¼Œå¦‚æœæŸäº›æ©ç æ˜¾ç¤ºå‡ºè‚¯å®šçš„ç»“æœï¼Œ
é‚£ä¹ˆå…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„å…¶ä»–æ©ç ä¹Ÿå¯èƒ½æ˜¯æ½œåœ¨çš„è§£å†³æ–¹
æ¡ˆï¼Œå› æ­¤åº”è¯¥å…·æœ‰æ›´é«˜çš„åŒ¹é…å¯èƒ½æ€§
min
éƒ¨mm
min Ex [L(x, Î¦ + âˆ†Î¦,
m)]
min Ex
[L(xï¼Œ
Ï†+âˆÏ†
ï¼Œm)]
âˆ†Î¦âˆ†Î¦
, (2)
15
, (2) pling in subsequent
evaluations, allowing
for a more foåœ¨éšå
çš„è¯„ä¼°
ä¸­ï¼Œè€ƒ
è™‘åˆ°æ›´å¤šçš„
cused search on promising solutions. However, the defi- ä¸“æ³¨äºå¯»æ‰¾æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®šä¹‰Rec
|
overability: Post-F
{
in
z
e-Tuning
Perform
}
ance
å¯æ¥å—æ€§:è°ƒæ•´åçš„æ€§èƒ½
nition of â€œsimilarity patternâ€ is still unclear so far.
16
â€œç›¸ä¼¼æ¨¡å¼â€çš„å®šä¹‰è‡³ä»Šä»ä¸
æ˜ç¡®ã€‚
With the N:M scheme, there are possible masks.
We
With the N:M scheme, there are possible
masks. We
17
Sampling Local Structures. In this work, we demonstrate that local structures, as illustrated in Figure 2, can
serve as effective anchors for modeling the relationships
between different masks. If a pruning mask leads to certain local structures and yields competitive results after
fine- tuning, then other masks yielding the same local
patterns are also likely to be positive solutions. This can be
achieved by dividing the original model into K nonoverlapping blocks,
é‡‡æ ·å±€éƒ¨ç»“æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†å±€éƒ¨ç»“
æ„ï¼Œå¦‚å›¾æ‰€ç¤º 2,å¯ä»¥ä½œä¸ºå¯¹ä¸åŒæ©ç ä¹‹é—´çš„å…³ç³»è¿›è¡Œ
å»ºæ¨¡çš„æœ‰æ•ˆé”šã€‚å¦‚æœå‰ªææ©ç å¯¼è‡´æŸäº›å±€éƒ¨ç»“æ„ï¼Œå¹¶
åœ¨å¾®è°ƒåäº§ç”Ÿç«äº‰æ€§ç»“æœï¼Œåˆ™äº§ç”Ÿç›¸åŒå±€éƒ¨æ¨¡å¼çš„å…¶
ä»–æ©ç ä¹Ÿå¯èƒ½æ˜¯æ­£è§£ã€‚è¿™å¯ä»¥é€šè¿‡å°†åŸå§‹æ¨¡å‹åˆ†æˆ K
ä¸ªä¸é‡å çš„å—æ¥å®ç°ï¼Œ
represented as Î¦ = [Î¦1, Î¦2, Â· Â· Â· , Î¦K]
âŠº
. For simplicity, we
assume each block Î¦k = [Ï•k1, Ï•k2, Â· Â· Â· , Ï•kM ]
âŠº
contains
exactly M layers, although they can have varied lengths.
è¡¨ç¤ºä¸º Ï†=[Ï†1ï¼ŒÏ†2ï¼Œï¼ŒÏ†k]âŠº.ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬
å‡è®¾æ¯ä¸ªå— Ï†k =[ kÏ• 1ï¼ŒÏ•k2ï¼ŒÏ• âŠº km] ]æ­£å¥½åŒ…å« m å±‚ï¼Œ
å°½ç®¡å®ƒä»¬å¯ä»¥å…·æœ‰ä¸åŒçš„é•¿åº¦ã€‚
Instead of performing global layer pruning, we propose an
N:M scheme for local layer pruning, where, for each block
Î¦k with M layers, N layers are retained. This results in a
ä»£æ›¿æ‰§è¡Œå…¨å±€å±‚ä¿®å‰ªï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå±€éƒ¨å±‚ä¿®å‰ªçš„
N:M æ–¹æ¡ˆï¼Œå…¶ä¸­ï¼Œå¯¹äºå…·æœ‰ M å±‚çš„æ¯ä¸ªå— Ï†kï¼Œä¿ç•™ N
å±‚ã€‚è¿™å¯¼è‡´äº†
set of local binary masks m = [m1, m2, . . . , mK]
âŠº
.
Simi- larly, the distribution of a local mask mk is modeled
using a categorical distribution p(mk). We perform
independent
å±€éƒ¨äºŒè¿›åˆ¶æ©ç é›†åˆ m = [m1ï¼Œm2ï¼Œ.ã€‚ã€‚ï¼ŒmK]âŠº.ç±»
ä¼¼åœ°ï¼Œä½¿ç”¨åˆ†ç±»åˆ†å¸ƒ p(mk)æ¥æ¨¡æ‹Ÿå±€éƒ¨æ©ç  mk çš„åˆ†
å¸ƒã€‚æˆ‘ä»¬ç‹¬ç«‹è¡¨æ¼”
sampling of local binary masks and combine them for
prun- ing, which presents the joint distribution:
å¯¹æœ¬åœ°äºŒè¿›åˆ¶æ©ç è¿›è¡Œé‡‡æ ·ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆèµ·æ¥è¿›è¡Œ
prun ingï¼Œä»è€Œå‘ˆç°è”åˆåˆ†å¸ƒ:
p(m) = p(m1) Â· p(m2) Â· Â· Â· p(mK) (3)
p(m) = p(m1) p(m2) p(mK)
(3)
If some local distributions p(mk) exhibit high confidence
in the corresponding blocks, the system will tend to sample those positive patterns frequently and keep active explorations in other local blocks. Based on this concept, we
introduce differential sampling to make the above process
learnable.
å¦‚æœä¸€äº›å±€éƒ¨åˆ†å¸ƒ p(mk)åœ¨ç›¸åº”çš„å—ä¸­è¡¨ç°å‡ºé«˜ç½®ä¿¡
åº¦ï¼Œåˆ™ç³»ç»Ÿå°†å€¾å‘äºé¢‘ç¹åœ°é‡‡æ ·é‚£äº›æ­£æ¨¡å¼ï¼Œå¹¶åœ¨å…¶
ä»–å±€éƒ¨å—ä¸­ä¿æŒæ´»è·ƒçš„æ¢ç´¢ã€‚åŸºäºè¿™ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬å¼•
å…¥å·®åˆ†é‡‡æ ·ï¼Œä½¿ä¸Šè¿°è¿‡ç¨‹å¯ä»¥å­¦ä¹ ã€‚
Differentiable Sampling. Considering the sampling process of a local mask mk, which corresponds a local block Î¦k
and is modeled by a categorical distribution p(mk).
å¯å¾®åˆ†æŠ½æ ·ã€‚è€ƒè™‘å¯¹åº”äºå±€éƒ¨å— Ï†k å¹¶ä¸”ç”±åˆ†ç±»åˆ†å¸ƒ
p(mk)å»ºæ¨¡çš„å±€éƒ¨æ©ç  mk çš„é‡‡æ ·è¿‡ç¨‹ã€‚ M
M
N
Nç”·::Mç”·æ™®é€š
ğ‘¥ğ‘–+1
B
r
A
ğ‘¥ğ‘–
Pretrained ğ‘Š Identity f(x)=x
ğ‘¥ğ‘–+1
B
r
A
ğ‘¥ğ‘–
Pretrained ğ‘Š Identity f(x)=x
18
ğ‘šğ‘–
â¨‚
+ (1 âˆ’ ğ‘šğ‘–)
â¨‚
ğ‘šğ‘–
â¨‚
+ (1 ) âˆ’ ğ‘šğ‘–
â¨‚
ğ‘ Ã—
ğ‘ Ã—
Figure 3. An example of forward propagation with differentiable
pruning mask mi and LoRA for recoverability estimation.
å›¾ 3ã€‚ç”¨äºå¯æ¢å¤æ€§ä¼°è®¡çš„å…·æœ‰å¯å¾®åˆ†ä¿®å‰ªæ©æ¨¡ mi å’Œ LoRA
çš„å‰å‘ä¼ æ’­çš„ä¾‹å­ã€‚
Notably, when Ï„ â†’ 0, the STE gradients will approximate
the true gradients, yet with a higher variance which is negative for training [22]. Thus, a scheduler is typically emå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ Ï„ â†’ 0 æ—¶ï¼ŒSTE æ¢¯åº¦å°†æ¥è¿‘çœŸå®æ¢¯
åº¦ï¼Œä½†å…·æœ‰è¾ƒé«˜çš„æ–¹å·®ï¼Œè¿™å¯¹è®­ç»ƒæ˜¯ä¸åˆ©çš„[22].å› 
æ­¤ ï¼Œ è°ƒ åº¦ å™¨ é€š å¸¸ æ˜¯ e m -
ployed to initiate training with a high temperature, gradually reducing it over time.
é‡‡ç”¨é«˜æ¸©å¼€å§‹è®­ç»ƒï¼Œéšç€æ—¶é—´çš„æ¨ç§»é€æ¸é™ä½æ¸©åº¦ã€‚
Joint Optimization with Recoverability. With differentiable sampling, we are able to update the underlying probability using gradient descent. The training objective in
this work is to maximize the recoverability of sampled
masks. We reformulate the objective in Equation 2 by
incorporat- ing the learnable distribution:
å…·æœ‰å¯æ¢å¤æ€§çš„è”åˆä¼˜åŒ–ã€‚é€šè¿‡ä¸åŒçš„é‡‡æ ·ï¼Œæˆ‘ä»¬èƒ½
å¤Ÿä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°æ½œåœ¨çš„æ¦‚ç‡ã€‚è¿™é¡¹å·¥ä½œçš„åŸ¹è®­
ç›®æ ‡æ˜¯æœ€å¤§é™åº¦åœ°æé«˜å–æ ·å£ç½©çš„å¯æ¢å¤æ€§ã€‚æˆ‘ä»¬é‡
æ–°è¡¨è¿°äº†ç­‰å¼ä¸­çš„ç›®æ ‡ 2 é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„åˆ†å¸ƒ:
minmin Ex,{mk âˆ¼p(mk )}
[L(x, Î¦ + âˆ†Î¦, {mk}],(6)
æ• æ• Ex ï¼Œ { MKâˆp(MK)}[L(x ï¼Œ Ï†+âˆÏ† ï¼Œ
{mk}]ï¼Œ(6)
{p(m )} âˆ†Î¦
{ p(m)}âˆÏ†
Recoverability: Post-Fine-Tuning Performance
å¯æ¢å¤æ€§:å¾®è°ƒåçš„æ€§èƒ½
where {p(mk)} = {p(m1), Â· Â· Â· , p(mK)} refer to the
cat- egorical distributions for different local blocks.
Based on this formulation, we further investigate how to
19
incorporate
å…¶ä¸­{p(mk)} = {p(m1)ï¼Œï¼Œp(mk)}è¡¨ç¤ºä¸åŒå±€éƒ¨
å—çš„å‡ ä½•åˆ†å¸ƒã€‚åŸºäºè¿™ä¸ªå…¬å¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶
å¦‚ä½•åˆå¹¶
the fine-tuning information into the training. We
propose a joint optimization of the distribution and a
weight update
å°†å¾®è°ƒä¿¡æ¯çº³å…¥åŸ¹è®­ã€‚æˆ‘ä»¬æå‡ºäº†åˆ†å¸ƒçš„è”åˆä¼˜åŒ–å’Œ
æƒé‡æ›´æ–° âˆ†Î¦. Our key idea is to introduce a co-optimized update âˆ†Î¦ âˆ†Î¦.æˆ‘ä»¬çš„ä¸»è¦æ€æƒ³æ˜¯å¼•å…¥ä¸€ä¸ªå…±åŒä¼˜åŒ–çš„æ›´æ–°âˆÏ†
construct a special matrix
mË†
æ„é€ ä¸€ä¸ªç‰¹æ®Šçš„çŸ©é˜µ
to enumerate all possi- åˆ—ä¸¾æ‰€æœ‰å¯èƒ½-
20
for joint training. A
straightforward way to craft the update
è¿›è¡Œè”åˆè®­ç»ƒã€‚åˆ¶ä½œ æ›´æ–°çš„ç®€å•æ–¹æ³•
ble masks. For example, 2:3 layer pruning will lead to
the candidate matrix mË† 2:3 = [[1, 1, 0] , [1, 0, 1] , [0,
1, 1]]. In this case, each block will have three
probabilities p(mk) = [pk1, pk2, pk3]. For simplicity, we
omit mk and k and use pi to represent the probability of
sampling i-th element in mË† N:M . A popular method
to make a sampling process difble é¢å…·ã€‚æ¯”å¦‚ 2:3 å±‚å‰ªæä¼šå¯¼è‡´å€™é€‰çŸ©é˜µ mË‡2:3
=[[1ï¼Œ1ï¼Œ0]ï¼Œ[1ï¼Œ0ï¼Œ1]ï¼Œ[0ï¼Œ1ï¼Œ1]]ã€‚åœ¨è¿™ç§æƒ…
å†µ ä¸‹ ï¼Œ æ¯ ä¸ª å— å°† å…· æœ‰ ä¸‰ ä¸ª æ¦‚ ç‡ p(mk) =
[pk1ï¼Œpk2ï¼Œpk3]ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬çœç•¥äº† mk
å’Œ kï¼Œè€Œç”¨ pi æ¥è¡¨ç¤ºåœ¨ MË‡N:M ä¸­é‡‡æ ·ç¬¬ I ä¸ªå…ƒç´ çš„
æ¦‚ç‡ferentiable is Gumbel-Softmax [13, 17, 22]: å¯å˜å‚æ•°æ˜¯ Gumbel-Softmax[13,17,22]:
is to directly optimize the original network. However, the
parameter scale in a diffusion transformer is usually huge,
and a full optimization may make the training process
costly and not that efficient. To this end, we show that
Parameter- Efficient Fine-Tuning methods such as LoRA
[21] can be a good choice to obtain the required âˆ†Î¦. For
a single linear
21
P
exp( + log
P
exp + log
å°±æ˜¯ç›´æ¥ä¼˜åŒ–åŸæœ‰ç½‘ç»œã€‚ç„¶è€Œï¼Œæ‰©æ•£å˜å‹å™¨ä¸­çš„å‚
æ•°è§„æ¨¡é€šå¸¸å¾ˆå¤§ï¼Œå®Œå…¨ä¼˜åŒ–å¯èƒ½ä¼šä½¿è®­ç»ƒè¿‡ç¨‹æˆæœ¬
é«˜æ˜‚ä¸”æ•ˆç‡ä¸é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å‚æ•°é«˜æ•ˆå¾®è°ƒ
æ–¹æ³•ï¼Œå¦‚ LoRA[21]å¯ä»¥æ˜¯è·å¾—æ‰€éœ€âˆÏ† çš„å¥½é€‰æ‹©ã€‚å¯¹
äºå•ä¸ªçº¿æ€§
one-hot exp(( gi + log pi ) /Ï„ )
!
. (4)
ä¸€çƒ­ exp((gi + log pi)/Ï„)ï¼ã€‚
(4)
matrix W in Î¦, we simulate the fine-tuned weights as:
y
y
j
j
jj
22
Ï† ä¸­çš„çŸ©é˜µ Wï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿå¾®è°ƒçš„æƒé‡ä¸º:
Wfine-tuned = W + Î±âˆ†W = W + Î±BA, (7)
wfine-tuned = W+Î± W â‡¼ = W+Î±BAï¼Œ (7)
where gi is random noise drawn from the Gumbel distribution Gumbel(0, 1) and Ï„ refers to the temperature term.
The output y is the index of the sampled mask. Here a
Straight- Through Estimator [2] is applied to the one-hot
operation, where the onehot operation is enabled during
forward and is treated as an identity function during
backward. Leverag- ing the one-hot index y and the
candidate set mË† N:M , we can
å…¶ä¸­ï¼Œgi æ˜¯å–è‡ª Gumbel åˆ†å¸ƒ Gumbel(0ï¼Œ1)çš„éšæœºå™ª
å£°ï¼ŒÏ„ æŒ‡æ¸©åº¦é¡¹ã€‚è¾“å‡º y æ˜¯é‡‡æ ·æ©ç çš„ç´¢å¼•ã€‚è¿™é‡Œæ˜¯
ä¸€ä¸ªç›´é€šä¼°è®¡å™¨[2]åº”ç”¨äº one-hot æ“ä½œï¼Œå…¶ä¸­ onehot æ“ä½œåœ¨å‰è¿›æœŸé—´å¯ç”¨ï¼Œåœ¨åé€€æœŸé—´è¢«è§†ä¸ºæ ‡è¯†å‡½
æ•°ã€‚åˆ©ç”¨ç‹¬çƒ­ç´¢å¼• y å’Œå€™é€‰é›† M \u N:Mï¼Œæˆ‘ä»¬å¯ä»¥
draw a mask m âˆ¼ p(m) through a simple index operation:
é€šè¿‡ä¸€ä¸ªç®€å•çš„ç´¢å¼•æ“ä½œç»˜åˆ¶ä¸€ä¸ªæ©ç  mâˆp(m ):
where Î± is a scalar hyperparameter that scales the
contribu- tion of âˆ†W. Using LoRA significantly
reduces the num- ber of parameters, facilitating efficient
exploration of differ- ent pruning decisions. As shown in
Figure 3, we leverage the sampled binary mask value mi
as the gate and forward the network with Equation 1,
which suppresses the layer outputs if the sampled mask
is 0 for the current layer. In addition, the previously
mentioned STE will still provide non-zero gradients to
the pruned layer, allowing it to be fur-
23
å…¶ä¸­ï¼ŒÎ± æ˜¯ä¸€ä¸ªæ ‡é‡è¶…å‚æ•°ï¼Œç”¨äºè¡¡é‡ w çš„è´¡çŒ®ã€‚
ä½¿ç”¨ LoRA å¯æ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡ï¼Œæœ‰åŠ©äºæœ‰æ•ˆæ¢ç´¢ä¸
åŒçš„ä¿®å‰ªå†³ç­–ã€‚å¦‚å›¾æ‰€ç¤º 3,æˆ‘ä»¬åˆ©ç”¨é‡‡æ ·çš„äºŒè¿›åˆ¶
æ©ç å€¼ mi ä½œä¸ºé—¨ï¼Œå¹¶ç”¨ç­‰å¼è½¬å‘ç½‘ç»œ 1,å¦‚æœå½“å‰å±‚
çš„é‡‡æ ·æ©ç ä¸º 0ï¼Œåˆ™æŠ‘åˆ¶å±‚è¾“å‡ºã€‚æ­¤å¤–ï¼Œå‰é¢æåˆ°çš„
STE ä»ç„¶ä¼šæä¾›éé›¶æ¢¯åº¦ä¿®å‰ªå±‚ï¼Œå…è®¸å®ƒæ˜¯æ¯›çš®
m = y
âŠº mË†
m = y mâŠº Ë†
(5) (5)
24
ther updated. This is helpful
in practice, since some layers
å·²æ›´æ–°ã€‚è¿™åœ¨å®
è·µä¸­æ˜¯æœ‰å¸®åŠ©
çš„ï¼Œå› ä¸ºä¸€äº›å±‚
25
Method Depth #Param Iters IS â†‘ FID â†“ sFID â†“ Prec. â†‘ Recall â†‘ Sampling it/s â†‘
æ–¹æ³• æ·±åº¦ #Param Iters æ˜¯â†‘ FID â†“ sFID â†“ Precã€‚
â†‘
å›å¿†â†‘ é‡‡æ · it/s â†‘
DiT-XL/2 [40] 28 675 M 7,000 K 278.24 2.27 4.60 0.83 0.57 6.91
DiT-XL/2 [40] 28 675 M 2,000 K 240.22 2.73 4.46 0.83 0.55 6.91
DiT-XL/2 [40] 28 675 M 1,000 K 157.83 5.53 4.60 0.80 0.53 6.91
U-ViT-H/2 [1] 29 501 M 500 K 265.30 2.30 5.60 0.82 0.58 8.21
ShortGPT [36] 28â‡’19 459 M 100 K 132.79 7.93 5.25 0.76 0.53 10.07
TinyDiT-D19 (KD) 28â‡’19 459 M 100 K 242.29 2.90 4.63 0.84 0.54 10.07
TinyDiT-D19 (KD) 28â‡’19 459 M 500 K 251.02 2.55 4.57 0.83 0.55 10.07
DiT-XL/2[40] 28 675 ç±³ 7000K 278.2
4
2.27 4.60 0.8
3
0.5
7
6.91
DiT-XL/2[40] 28 675 ç±³ 2000K 240.2
2
2.73 4.46 0.8
3
0.5
5
6.91
DiT-XL/2[40] 28 675 ç±³ 100 ä¸‡ 157.8
3
5.53 4.60 0.8
0
0.5
3
6.91
ç»´ç”Ÿç´  H/2[1] 29 501 ç±³ 50 ä¸‡ 265.3
0
2.30 5.60 0.8
2
0.5
8
8.21
ShortGPT[36] 28 1 â‡’ 9 459 ç±³ 10 ä¸‡è‹±
é•‘
132.7
9
7.93 5.25 0.7
6
0.5
3
10.07
TinyDiT-D19 (KD) 28 1 â‡’ 9 459 ç±³ 10 ä¸‡è‹±
é•‘
242.2
9
2.90 4.63 0.8
4
0.5
4
10.07
TinyDiT-D19 (KD) 28 1 â‡’ 9 459 ç±³ 50 ä¸‡ 251.0
2
2.55 4.57 0.8
3
0.5
5
10.07
DiT-L/2 [40] 24 458 M 1,000 K 196.26 3.73 4.62 0.82 0.54 9.73
U-ViT-L [1] 21 287 M 300 K 221.29 3.44 6.58 0.83 0.52 13.48
U-DiT-L [50] 22 204 M 400 K 246.03 3.37 4.49 0.86 0.50 -
Diff-Pruning-50% [12] 28 338 M 100 K 186.02 3.85 4.92 0.82 0.54 10.43
Diff-Pruning-75% [12] 28 169 M 100 K 83.78 14.58 6.28 0.72 0.53 13.59
ShortGPT [36] 28â‡’14 340 M 100 K 66.10 22.28 6.20 0.63 0.56 13.54
Flux-Lite [6] 28â‡’14 340 M 100 K 54.54 25.92 5.98 0.62 0.55 13.54
Sensitivity Analysis [18] 28â‡’14 340 M 100 K 70.36 21.15 6.22 0.63 0.57 13.54
Oracle (BK-SDM) [23] 28â‡’14 340 M 100 K 141.18 7.43 6.09 0.75 0.55 13.54
TinyDiT-D14 28â‡’14 340 M 100 K 151.88 5.73 4.91 0.80 0.55 13.54
TinyDiT-D14 28â‡’14 340 M 500 K 198.85 3.92 5.69 0.78 0.58 13.54
TinyDiT-D14 (KD) 28â‡’14 340 M 100 K 207.27 3.73 5.04 0.81 0.54 13.54
TinyDiT-D14 (KD) 28â‡’14 340 M 500 K 234.50 2.86 4.75 0.82 0.55 13.54
DiT-L/2[40] 24 458 ç±³ 100 ä¸‡ 196.26 3.73 4.62 0.8
2
0.5
4
9.73
ç»´ç”Ÿç´  L[1] 21 287 ç±³ 30 ä¸‡ 221.29 3.44 6.58 0.8
3
0.5
2
13.4
8
U-DiT-L[50] 22 204 ç±³ 40 ä¸‡ 246.03 3.37 4.49 0.8
6
0.5
0
-
å·®å¼‚ä¿®å‰ª-50%[12] 28 338 ç±³ 10 ä¸‡è‹±
é•‘
186.02 3.85 4.92 0.8
2
0.5
4
10.4
3
å·®å¼‚ä¿®å‰ª-75%[12] 28 169 ç±³ 10 ä¸‡è‹±
é•‘
83.78 14.58 6.28 0.7
2
0.5
3
13.5
9
ShortGPT[36] 28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
66.10 22.28 6.20 0.6
3
0.5
6
13.5
4
åŠ©ç„Šå‰‚-Lite[6] 28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
54.54 25.92 5.98 0.6
2
0.5
5
13.5
4
æ•æ„Ÿæ€§åˆ†æ[18] 28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
70.36 21.15 6.22 0.6
3
0.5
7
13.5
4
ç”²éª¨æ–‡å…¬å¸(é»‘è‰² SDM)
[23]
28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
141.18 7.43 6.09 0.7
5
0.5
5
13.5
4
è’‚å°¼è¿ªç‰¹-D14 28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
151.88 5.73 4.91 0.8
0
0.5
5
13.5
4
è’‚å°¼è¿ªç‰¹-D14 28 1 â‡’ 4 340 ç±³ 50 ä¸‡ 198.85 3.92 5.69 0.7
8
0.5
8
13.5
4
è’‚å°¼è¿ª-D14 (KD) 28 1 â‡’ 4 340 ç±³ 10 ä¸‡è‹±
é•‘
207.27 3.73 5.04 0.8
1
0.5
4
13.5
4
26
è’‚å°¼è¿ª-D14 (KD) 28 1 â‡’ 4 340 ç±³ 50 ä¸‡ 234.50 2.86 4.75 0.8
2
0.5
5
13.5
4
DiT-B/2 [40]
U-DiT-B [50]
12
22
130 M
-
1,000 K
400 K
119.63
85.15
10.12
16.64
5.39
6.33
0.73
0.64
0.55
0.63
28.30
-
TinyDiT-D7 (KD) 14â‡’7 173 M 500 K 166.91 5.87 5.43 0.78 0.53 26.81
DiT-B/2[40]
U-DiT-B[50]
12
22
130 ç±³
-
100 ä¸‡
40 ä¸‡
119.63
85.15
10.12
16.64
5.39
6.33
0.73
0.64
0.55
0.63
28.30
-
TinyDiT-D7 (KD) 14â‡’7 173 ç±³ 50 ä¸‡ 166.91 5.87 5.43 0.78 0.53 26.81
Table 1. Layer pruning results for pre-trained DiT-XL/2. We focus on two settings: fast training with 100K optimization steps and
sufficient fine-tuning with 500K steps. Both fine-tuning and Masked Knowledge Distillation (a variant of KD, see Sec. 4.4) are used for
recovery.
è¡¨ 1ã€‚é¢„è®­ç»ƒ DiT-XL/2 çš„å±‚ä¿®å‰ªç»“æœã€‚æˆ‘ä»¬å…³æ³¨ä¸¤ä¸ªè®¾ç½®:100K ä¼˜åŒ–æ­¥éª¤çš„å¿«é€Ÿè®­ç»ƒå’Œ 500K æ­¥éª¤çš„å……åˆ†å¾®è°ƒã€‚å¾®è°ƒå’Œæ©è”½çŸ¥
è¯†è’¸é¦(KD çš„ä¸€ç§å˜ä½“ï¼Œå‚è§ç¬¬ã€‚4.4)ç”¨äºæ¢å¤ã€‚
might not be competitive at the beginning, but may emerge 12
å¼€å§‹æ—¶å¯èƒ½æ²¡æœ‰ç«äº‰åŠ›ï¼Œä½†å¯èƒ½ä¼šå´­éœ²å¤´è§’ 12
as competitive candidates with sufficient fine-tuning.
ä½œä¸ºæœ‰ç«äº‰åŠ›çš„å€™é€‰äººã€‚
Pruning Decision. After training, we retain those local
structures with the highest probability and discard the additional update âˆ†Î¦. Then, standard fine-tuning techniques
can be applied for recovery.
ä¿®å‰ªå†³å®šã€‚åœ¨è®­ç»ƒä¹‹åï¼Œæˆ‘ä»¬ä¿ç•™é‚£äº›å…·æœ‰æœ€é«˜æ¦‚ç‡
çš„å±€éƒ¨ç»“æ„ï¼Œå¹¶ä¸”ä¸¢å¼ƒé¢å¤–çš„æ›´æ–°âˆÏ†ã€‚ç„¶åï¼Œå¯ä»¥
åº”ç”¨æ ‡å‡†å¾®è°ƒæŠ€æœ¯è¿›è¡Œæ¢å¤ã€‚
7. Experiments
8. å®éªŒ
8
8
6
6
4
å››
2
2
0 20
40
60
80
0 20
40
60
80
Compressio
n Ratio (%)
å‹ç¼©æ¯”(%)
Depth Prun
Width Prun
ing
ing
11.60
Linear Speedup
6.45
4.46
1.992.30
3.41
2.76 3.36 2.71
1.081.171.27
1.04 1.401.551.74
1.26
1.36 1.64 1.912.20
Depth Prun
Width Prun
ing
ing
11.60
Linear Speedup
6.45
4.46
1.992.30
3.41
2.76 3.36 2.71
1.081.171.27
1.04 1.401.551.74
1.26
1.36 1.64 1.912.20
1
1
27
4.39
4.39
8.1. Experimental Settings
8.2. å®éªŒè®¾ç½®
Our experiments were mainly conducted on Diffusion
Transformers [40] for class-conditional image generation
on ImageNet 256 Ã— 256 [8]. For evaluation, we follow [9, 40] and report the FreÂ´chet inception distance
(FID), Sliding FreÂ´chet Inception Distance (sFID), Inception
Scores
æˆ‘ ä»¬ çš„ å® éªŒ ä¸» è¦ åœ¨ æ‰© æ•£ å˜ å‹ å™¨ ä¸Š è¿› è¡Œ [40] å¯¹ äº
ImageNet 256 Ã— 256 ä¸Šçš„ç±»æ¡ä»¶å›¾åƒç”Ÿæˆ[8].å¯¹ äº
è¯„ ä¼° ï¼Œ æˆ‘ ä»¬ éµ å¾ª [9,40]å¹¶æŠ¥å‘Šé¢‘ç‡èµ·å§‹è·ç¦»
(FID)ã€æ»‘åŠ¨é¢‘ç‡èµ·å§‹è·ç¦»(sFID)ã€èµ·å§‹åˆ†æ•°
(IS), Precision and Recall using the official reference images [9]. Additionally, we also extend our methods to
other models, including MARs [29] and SiTs [34].
Experimental details can be found in the following
sections and appendix.
(æ˜¯)ã€ç²¾åº¦å’Œå¬å›ä½¿ç”¨å®˜æ–¹å‚è€ƒå›¾åƒ[9].æ­¤å¤–ï¼Œæˆ‘ä»¬
è¿˜å°†æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°å…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬ç«æ˜Ÿ[29]åä¸‹
æ¥[34].å®éªŒç»†èŠ‚å¯ä»¥åœ¨ä¸‹é¢çš„ç« èŠ‚å’Œé™„å½•ä¸­æ‰¾åˆ°ã€‚
8.3. Results on Diffusion Transformers
8.4. æ‰©æ•£å˜å‹å™¨çš„ç»“æœ
DiT. This work focuses on the compression of DiTs [40].
We consider two primary strategies as baselines: the first
DiTã€‚è¿™é¡¹å·¥ä½œçš„é‡ç‚¹æ˜¯ DiTs çš„å‹ç¼©[40].æˆ‘ä»¬è€ƒè™‘ä¸¤
ä¸ªä¸»è¦çš„ç­–ç•¥ä½œä¸ºåŸºçº¿:ç¬¬ä¸€ä¸ª
Figure 4. Depth pruning closely aligns with the theoretical linear
speed-up relative to the compression ratio.
28
å›¾ 4ã€‚æ·±åº¦ä¿®å‰ªä¸ç›¸å¯¹äºå‹ç¼©æ¯”çš„ç†è®ºçº¿æ€§åŠ é€Ÿéå¸¸ä¸€
è‡´ã€‚
involves using manually crafted patterns to eliminate
lay- ers. For instance, BK-SDM [23] employs heuristic
assump- tions to determine the significance of specific
layers, such as the initial or final layers. The second
strategy is based on systematically designed criteria to
evaluate layer impor- tance, such as analyzing the
similarity between block in- puts and outputs to
determine redundancy [6, 36]; this ap- proach typically
aims to minimize performance degradation after
pruning. Table 1 presents representatives from both
strategies, including ShortGPT [36], Flux-Lite [6], DiffPruning [12], Sensitivity Analysis [18] and BK-SDM
[23], which serve as baselines for
comparison.Additionally,
æ¶‰åŠä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æ¨¡å¼ï¼Œä»¥æ¶ˆé™¤é“ºè®¾ã€‚ä¾‹å¦‚ï¼ŒSDM
é“¶è¡Œ[23]é‡‡ç”¨å¯å‘å¼å‡è®¾æ¥ç¡®å®šç‰¹å®šå±‚çš„é‡è¦æ€§ï¼Œä¾‹
å¦‚åˆå§‹å±‚æˆ–æœ€ç»ˆå±‚ã€‚ç¬¬äºŒç§ç­–ç•¥åŸºäºç³»ç»Ÿè®¾è®¡çš„æ ‡å‡†æ¥
è¯„ä¼°å±‚çš„é‡è¦æ€§ï¼Œä¾‹å¦‚åˆ†æå—è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ç›¸ä¼¼æ€§
æ¥ç¡®å®šå†—ä½™[6,36];è¿™ç§æ–¹æ³•é€šå¸¸æ—¨åœ¨å‡å°‘ä¿®å‰ªåçš„
æ€§èƒ½ä¸‹é™ã€‚æ¡Œå­ 1 å±•ç¤ºä¸¤ç§ç­–ç•¥çš„ä»£è¡¨ï¼ŒåŒ…æ‹¬
ShortGPT[36],åŠ©ç„Šå‰‚-Lite[6],å·®å¼‚ä¿®å‰ª[12],æ•æ„Ÿ
æ€§åˆ†æ[18]å’Œ BK-SDM[23],ä½œä¸ºæ¯”è¾ƒçš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œ
ShortGPT rnable
100
101
Lea
ShortGPT rnable
100
101
Lea
29
2.00
2.00
1.75
1.75
1.50
1.50
1.25
1.25
1.00
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.00
0.00
Sensitivity
çµæ•åº¦
Min:
Method Depth Params Epochs FID IS
MAR-Large 32 479 M 400 1.78 296.0
MAR-Base 24 208 M 400 2.31 281.7
TinyMAR-D16 32â‡’16 277 M 40 2.28 283.4
SiT-XL/2 28 675 M 1,400 2.06 277.5
TinySiT-D14 28â‡’14 340 M 100 3.02 220.1
Min:
Method Depth Params Epochs FID IS
MAR-Large 32 479 M 400 1.78 296.0
MAR-Base 24 208 M 400 2.31 281.7
TinyMAR-D16 32â‡’16 277 M 40 2.28 283.4
SiT-XL/2 28 675 M 1,400 2.06 277.5
TinySiT-D14 28â‡’14 340 M 100 3.02 220.1
Dens Densi
30
Flux-Lite Oracle
é€šé‡ç²¾ç®€ Oracle
C
a
libration Loss
æ ¡å‡†æŸå¤±
Table 2. Depth pruning results on MARs [29] and SiTs [34].
è¡¨äºŒã€‚ç«æ˜Ÿä¸Šçš„æ·±åº¦ä¿®å‰ªç»“æœ[29]åä¸‹æ¥[34].
we evaluate our method against innovative architectural
de- signs, such as UViT [1], U-DiT [50], and DTR [39],
which have demonstrated improved training efficiency
over con- ventional DiTs.
æˆ‘ä»¬ç”¨åˆ›æ–°çš„å»ºç­‘è®¾è®¡æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ¯”å¦‚
UViT[1],U-DiT[50],è¿˜æœ‰ DTR[39],è¿™æ˜¾ç¤ºå‡ºæ¯”ä¼ ç»Ÿçš„
DiTs æ›´é«˜çš„åŸ¹è®­æ•ˆç‡ã€‚
Table 1 presents our findings on compressing a pretrained DiT-XL/2 [40]. This model contains 28
transformer layers structured with alternating Attention
and MLP layæ¡Œå­ 1 ä»‹ç»æˆ‘ä»¬åœ¨å‹ç¼©é¢„è®­ç»ƒçš„ DiT-XL/2 ä¸Šçš„å‘ç°
[40].è¯¥æ¨¡å‹åŒ…å« 28 ä¸ªå˜å‹å™¨å±‚ï¼Œé‡‡ç”¨äº¤æ›¿æ³¨æ„å’Œ MLP
å±‚ç»“æ„
ers. The proposed method seeks to identify shallow transformers with {7, 14, 19} sub-layers from these 28
layers, to maximize the post-fine-tuning performance.
With only 7% of the original training cost (500K steps
compared to
å‘ƒ ã€‚ æ‰€ æ å‡º çš„ æ–¹ æ³• è¯• å›¾ ä» è¿™ 28 å±‚ ä¸­ è¯† åˆ« å…· æœ‰
{7ï¼Œ14ï¼Œ19}å­å±‚çš„æµ…å˜å‹å™¨ï¼Œä»¥æœ€å¤§åŒ–åå¾®è°ƒæ€§èƒ½ã€‚
åªæœ‰åŸå§‹åŸ¹è®­æˆæœ¬çš„ 7%(50 ä¸‡æ­¥ï¼Œç›¸æ¯”ä¹‹ä¸‹
7M steps), TinyDiT achieves competitive performance relative to both pruning-based methods and novel
architectures. For instance, a DiT-L model trained from
scratch for 1M steps achieves an FID score of 3.73 with
458M parameters. In contrast, the compressed TinyDiTD14 model, with only 340M parameters and a faster
sampling speed (13.54 it/s vs. 9.73 it/s), yields a
significantly improved FID of 2.86. On parallel devices
like GPUs, the primary bottleneck in trans- formers arises
from sequential operations within each layer, which
becomes more pronounced as the number of layers
increases. Depth pruning mitigates this bottleneck by removing entire transformer layers, thereby reducing
compu- tational depth and optimizing the workload. By
compar- ison, width pruning only reduces the number of
neurons within each layer, limiting its speed-up potential. As
shown in Figure 4, depth pruning closely matches the
theoretical linear speed-up as the compression ratio
increases, outper- forming width pruning methods such as
Diff-Pruning [12].
ç›¸å¯¹äºåŸºäºä¿®å‰ªçš„æ–¹æ³•å’Œæ–°é¢–çš„ä½“ç³»ç»“æ„ï¼ŒTinyDiT å®ç°
äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä»é›¶å¼€å§‹è®­ç»ƒ 1M æ­¥çš„ DiT-L
æ¨¡å‹åœ¨ 458M å‚æ•°ä¸‹è·å¾—äº† 3.73 çš„ FID åˆ†æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ
åªæœ‰ 340M å‚æ•°å’Œæ›´å¿«é‡‡æ ·é€Ÿåº¦ (13.54 it/s å¯¹ 9.73
it/s)çš„å‹ç¼© TinyDiT-D14 æ¨¡å‹äº§ç”Ÿäº† 2.86 çš„æ˜¾è‘—æ”¹å–„çš„
FIDã€‚åœ¨ GPU ç­‰å¹¶è¡Œè®¾å¤‡ä¸Šï¼Œå˜å‹å™¨çš„ä¸»è¦ç“¶é¢ˆæ¥è‡ªæ¯å±‚
å†…çš„é¡ºåºæ“ä½œï¼Œéšç€å±‚æ•°çš„å¢åŠ ï¼Œè¿™ä¸€é—®é¢˜å˜å¾—æ›´åŠ çª
å‡ºã€‚æ·±åº¦ä¿®å‰ªé€šè¿‡é‡æ–°ç§»åŠ¨æ•´ä¸ªè½¬æ¢å™¨å±‚æ¥ç¼“è§£è¿™ä¸€ç“¶
é¢ˆï¼Œä»è€Œå‡å°‘è®¡ç®—æ·±åº¦å¹¶ä¼˜åŒ–å·¥ä½œè´Ÿè½½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå®½
åº¦ä¿®å‰ªåªä¼šå‡å°‘æ¯ä¸€å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡ï¼Œé™åˆ¶å…¶åŠ é€Ÿæ½œ
åŠ›ã€‚å¦‚å›¾æ‰€ç¤º 4,éšç€å‹ç¼©æ¯”çš„å¢åŠ ï¼Œæ·±åº¦ä¿®å‰ªä¸ç†è®ºä¸Š
çš„çº¿æ€§åŠ é€Ÿéå¸¸åŒ¹é…ï¼Œè¾“å‡ºå½¢æˆå®½åº¦ä¿®å‰ªçš„æ–¹æ³•ï¼Œä¾‹å¦‚
å·®å¼‚ä¿®å‰ª[12].
MAR & SiT. Masked Autoregressive (MAR) [29] mod- els
employ a diffusion loss-based autoregressive framework in a
continuous-valued space, achieving high-quality image
generation without the need for discrete tokenization. The
MAR-Large model, with 32 transformer blocks, serves as
the baseline for comparison. Applying our pruning method,
we reduced MAR to a 16-block variant, TinyMAR-D16,
achieving an FID of 2.28 and surpassing the performance of
the 24-block MAR-Base model with only 10% of the
original training cost (40 epochs vs. 400 epochs). Our approach also generalizes to Scalable Interpolant Transformers (SiT) [34], an extension of the DiT architecture that
employs a flow-based interpolant framework to bridge data
MAR & SiTã€‚æ©è”½è‡ªå›å½’[29]æ¨¡å‹åœ¨è¿ç»­å€¼ç©ºé—´ä¸­é‡‡ç”¨åŸº
äºæ‰©æ•£æŸå¤±çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ— éœ€ç¦»æ•£ç¬¦å·åŒ–å³å¯å®ç°é«˜
è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚å…·æœ‰ 32 ä¸ªå˜å‹å™¨å—çš„ MAR-Large æ¨¡å‹
ç”¨ä½œæ¯”è¾ƒçš„åŸºçº¿ã€‚åº”ç”¨æˆ‘ä»¬çš„å‰ªææ–¹æ³•ï¼Œæˆ‘ä»¬å°† MAR å‡
å°‘åˆ° 16 ä¸ªå—çš„å˜ä½“ TinyMAR-D16ï¼Œå®ç°äº† 2.28 çš„ FIDï¼Œ
å¹¶è¶…è¿‡äº† 24 ä¸ªå—çš„ MAR-Base æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶è®­ç»ƒæˆæœ¬
ä»…ä¸ºåŸå§‹è®­ç»ƒæˆæœ¬çš„ 10%(40 ä¸ªå†å…ƒå¯¹ 400 ä¸ªå†å…ƒ)ã€‚æˆ‘
ä»¬çš„æ–¹æ³•ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å¯ä¼¸ç¼©çš„æ’å€¼å˜æ¢å™¨ (SiT)
31
[34],DiT ä½“ç³»ç»“æ„çš„æ‰©å±•ï¼Œé‡‡ç”¨åŸºäºæµçš„æ’å€¼æ¡†æ¶æ¥
æ¡¥æ¥æ•°æ®
Figure 5. Distribution of calibration loss through random
sampling of candidate models. The proposed learnable method
achieves the best post-fine-tuning FID yet has a relatively high
initial loss com- pared to other baselines.
32
å›¾ 5ã€‚é€šè¿‡å€™é€‰æ¨¡å‹çš„éšæœºæŠ½æ ·æ ¡å‡†æŸå¤±çš„åˆ†å¸ƒã€‚æ‰€æå‡º
çš„å¯å­¦ä¹ æ–¹æ³•å®ç°äº†æœ€ä½³çš„å¾®è°ƒå FIDï¼Œä½†æ˜¯ä¸å…¶ä»–åŸºçº¿
ç›¸æ¯”å…·æœ‰ç›¸å¯¹è¾ƒé«˜çš„åˆå§‹æŸå¤±ã€‚
Strategy Loss IS FID Prec. Recall
Max. Loss 37.69 NaN NaN NaN NaN
Med. Loss 0.99 149.51 6.45 0.78 0.53
Min. Loss 0.20 73.10 20.69 0.63 0.58
Sensitivity 0.21 70.36 21.15 0.63 0.57
ShortGPT [36] 0.20 66.10 22.28 0.63 0.56
Flux-Lite [6] 0.85 54.54 25.92 0.62 0.55
Oracle (BK-SDM) 1.28 141.18 7.43 0.75 0.55
æˆ˜ç•¥ å¤±è´¥ æ˜¯ æ¡…æ “ Prec
ã€‚
å›å¿†
æœ€å¤§å€¼å¤±è´¥ 37.69 åœ†ç›˜
çƒ¤é¥¼
åœ†ç›˜
çƒ¤é¥¼
åœ†ç›˜
çƒ¤é¥¼
åœ†ç›˜
çƒ¤é¥¼
åŒ»å­¦ã€‚å¤±è´¥ 0.99 149.5
1
6.45 0.78 0.53
é‡æ»´å¤±è´¥ 0.20 73.10 20.69 0.63 0.58
çµæ•åº¦ 0.21 70.36 21.15 0.63 0.57
ShortGPT[36] 0.20 66.10 22.28 0.63 0.56
åŠ©ç„Šå‰‚-Lite[6] 0.85 54.54 25.92 0.62 0.55
ç”²éª¨æ–‡å…¬å¸(é»‘è‰² SDM)
1.28
141.1
8
7.43 0.75 0.55
Table 3. Directly minimizing the calibration loss may lead to
non-optimal solutions. All pruned models are fine-tuned
without knowledge distillation (KD) for 100K steps. We
evaluate the fol- lowing baselines: (1) Loss â€“ We randomly
prune a DiT-XL model to generate 100,000 models and select
models with different cali- bration losses for fine-tuning; (2)
Metric-based Methods â€“ such as Sensitivity Analysis and
ShortGPT; (3) Oracle â€“ We retain the first and last layers while
uniformly pruning the intermediate layers fol- lowing [23]; (4)
Learnable â€“ The proposed learnable method.
è¡¨ 3ã€‚ç›´æ¥æœ€å°åŒ–æ ¡å‡†æŸè€—å¯èƒ½å¯¼è‡´éæœ€ä½³è§£å†³æ–¹æ¡ˆã€‚æ‰€
æœ‰ä¿®å‰ªåçš„æ¨¡å‹åœ¨æ²¡æœ‰çŸ¥è¯†æå–(KD)çš„æƒ…å†µä¸‹è¢«å¾®è°ƒ 100K
æ­¥ã€‚æˆ‘ä»¬è¯„ä¼°ä»¥ä¸‹åŸºçº¿:(1)æŸè€—-æˆ‘ä»¬éšæœºä¿®å‰ª DiT-XL æ¨¡
å‹ä»¥ç”Ÿæˆ 100ï¼Œ000 ä¸ªæ¨¡å‹ï¼Œå¹¶é€‰æ‹©å…·æœ‰ä¸åŒæ ¡å‡†æŸè€—çš„
æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›(2)åŸºäºåº¦é‡çš„æ–¹æ³•â€”â€”å¦‚æ•æ„Ÿæ€§åˆ†æå’Œ
ShortGPT(3)Oracleâ€”â€”æˆ‘ä»¬ä¿ç•™ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚ï¼ŒåŒ
æ—¶ç»Ÿä¸€åˆ å‡åé¢çš„ä¸­é—´å±‚[23];(4)å¯å­¦â€”â€”å»ºè®®çš„å¯å­¦æ–¹
æ³•ã€‚
and noise distributions. The SiT-XL/2 model,
comprising 28 transformer blocks, was pruned by 50%,
creating the TinySiT-D14 model. This pruned model
retains competi- tive performance at only 7% of the
original training cost (100 epochs vs. 1400 epochs). As
shown in Table 2, these results demonstrate that our
pruning method is adaptable across different diffusion
transformer variants, effectively reducing the model size
and training time while maintain- ing strong performance.
å’Œå™ªå£°åˆ†å¸ƒã€‚åŒ…å« 28 ä¸ªå˜å‹å™¨å—çš„ SiT-XL/2 æ¨¡å‹è¢«
å‰Šå‡äº† 50%ï¼Œåˆ›å»ºäº† TinySiT-D14 æ¨¡å‹ã€‚è¿™ç§ä¿®å‰ªåçš„
æ¨¡å‹ä»…ç”¨åŸå§‹è®­ç»ƒæˆæœ¬çš„ 7%å°±ä¿æŒäº†ç«äº‰æ€§èƒ½(100 ä¸ª
æ—¶æœŸå¯¹ 1400 ä¸ªæ—¶æœŸ)ã€‚å¦‚è¡¨ä¸­æ‰€ç¤º 2,è¿™äº›ç»“æœè¡¨æ˜ï¼Œ
æˆ‘ä»¬çš„å‰ªææ–¹æ³•é€‚ç”¨äºä¸åŒçš„æ‰©æ•£å˜å‹å™¨å˜é‡ï¼Œæœ‰æ•ˆ
åœ°å‡å°‘äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ€§
èƒ½ã€‚
8.5. Analytical Experiments
8.6. åˆ†æå®éªŒ
Is Calibration Loss the Primary Determinant? An essential question in depth pruning is how to identify redundant layers in pre-trained diffusion transformers. A
common approach involves minimizing the calibration loss,
based on the assumption that a model with lower calibration loss after pruning will exhibit superior performance.
However, we demonstrate in this section that this
hypothesis may not hold for diffusion transformers. We
begin by ex- amining the solution space through random
depth pruning at a 50% ratio, generating 100,000
candidate models with
æ ¡å‡†æŸè€—æ˜¯ä¸»è¦å†³å®šå› ç´ å—ï¼Ÿæ·±åº¦ä¿®å‰ªä¸­çš„ä¸€ä¸ªåŸºæœ¬
é—®é¢˜æ˜¯å¦‚ä½•è¯†åˆ«é¢„è®­ç»ƒæ‰©æ•£å˜æ¢å™¨ä¸­çš„å†—ä½™å±‚ã€‚åŸºäº
ä¿®å‰ªåå…·æœ‰è¾ƒä½æ ¡å‡†æŸè€—çš„æ¨¡å‹å°†è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½çš„
å‡è®¾ï¼Œä¸€ç§å¸¸è§çš„æ–¹æ³•åŒ…æ‹¬æœ€å°åŒ–æ ¡å‡†æŸè€—ã€‚ç„¶è€Œï¼Œ
æˆ‘ä»¬åœ¨è¿™ä¸€èŠ‚ä¸­è¯æ˜ï¼Œè¿™ä¸€å‡è®¾å¯èƒ½ä¸é€‚ç”¨äºæ‰©æ•£å˜
å‹å™¨ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡ä»¥ 50%çš„æ¯”ç‡è¿›è¡Œéšæœºæ·±åº¦ä¿®å‰ª
æ¥æ£€æŸ¥è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œç”Ÿæˆ 100ï¼Œ000 ä¸ªå€™é€‰æ¨¡å‹
Learnable 0.98 151.88 5.73 0.80 0.55
Learnable 0.98 151.88 5.73 0.80 0.55
Pattern âˆ†WIS â†‘FID â†“ sFID â†“ Prec. â†‘ Recall â†‘ Pattern âˆ†WIS â†‘FID â†“ sFID â†“ Prec. â†‘ Recall â†‘
33
27
27
26
26
25
25
24
24
23
23
22
22
21
21
20
20
19
19
18
18
17
17
16
16
15
15
14
14
13
13
12
12
11
11
10
10
9
9
8
8
7
ä¸ƒ
6
6
5
5
4
å››
3
3
2
2
1
ä¸€
Table 4. Performance comparison of TinyDiT-D14 models compressed using various pruning schemes and recoverability
estima- tion strategies. All models are fine-tuned for 10,000
steps, and FID scores are computed on 10,000 sampled images
with 64 timesteps.
è¡¨ 4ã€‚ä½¿ç”¨ä¸åŒå‰ªææ–¹æ¡ˆå’Œå¯æ¢å¤æ€§ä¼°è®¡ç­–ç•¥å‹ç¼©çš„
TinyDiT-D14 æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒã€‚æ‰€æœ‰æ¨¡å‹éƒ½å¾®è°ƒäº† 10ï¼Œ000
æ­¥ï¼ŒFID åˆ†æ•°æ˜¯åœ¨ 64 ä¸ªæ—¶é—´æ­¥çš„ 10ï¼Œ000 ä¸ªé‡‡æ ·å›¾åƒä¸Šè®¡ç®—
çš„ã€‚
calibration losses ranging from 0.195 to 37.694 (see Figure 5). From these candidates, we select models with the
highest and lowest calibration losses for fine-tuning. Notably, both models result in unfavorable outcomes, such as
unstable training (NaN) or suboptimal FID scores (20.69),
as shown in Table 3. Additionally, we conduct a sensitivity analysis [18], a commonly used technique to identify
crucial layers by measuring loss disturbance upon layer removal, which produces a model with a low calibration loss
of 0.21. However, this modelâ€™s FID score is similar to that
of the model with the lowest calibration loss. Approaches
like ShortGPT [36] and a recent approach for compressing
the Flux model [6], which estimate similarity or minimize
mean squared error (MSE) between input and output
states, reveal a similar trend. In contrast, methods with
mod- erate calibration losses, such as Oracle (often
considered less competitive) and one of the randomly
pruned models, achieve FID scores of 7.43 and 6.45,
respectively, demon- strating significantly better
performance than models with minimal calibration loss.
These findings suggest that, while calibration loss may
influence post-fine-tuning performance to some extent, it is
not the primary determinant for diffu- sion transformers.
Instead, the modelâ€™s capacity for perfor- mance recovery
during fine-tuning, termed â€œrecoverability,â€ appears to be
more critical. Notably, assessing recoverabil- ity using
traditional metrics is challenging, as it requires a learning
process across the entire dataset. This observation also
explains why the proposed method achieves superior results
(5.73) compared to baseline methods.
æ ¡å‡†æŸå¤±èŒƒå›´ä» 0.195 åˆ° 37.694(è§å›¾ 5).ä» è¿™ äº› å€™ é€‰
æ¨¡ å‹ ä¸­ ï¼Œ æˆ‘ ä»¬ é€‰ æ‹© å…· æœ‰ æœ€ é«˜ å’Œ æœ€ ä½ æ ¡ å‡† æŸ è€— çš„
æ¨¡ å‹ è¿› è¡Œ å¾® è°ƒ ã€‚ ä¸ ï¼Œ ä¸¤ ç§ æ¨¡ å‹ éƒ½ å¯¼ è‡´ ä¸ åˆ© çš„ ç»“
æœ ï¼Œ å¦‚ ä¸ ç¨³ å®š çš„ è®­ ç»ƒ ( N a N ) æˆ– æ¬¡ ä¼˜ çš„ F I D åˆ† æ•°
( 2 0 . 6 9 ) ï¼Œ å¦‚ è¡¨ æ‰€ ç¤º 3.æ­¤ å¤– ï¼Œ æˆ‘ ä»¬ è¿˜ è¿› è¡Œ äº† æ•
æ„Ÿ æ€§ åˆ† æ [18],ä¸€ç§å¸¸ç”¨æŠ€æœ¯ï¼Œé€šè¿‡æµ‹é‡å±‚ç§»åŠ¨æ—¶çš„æŸ
å¤±æ‰°åŠ¨æ¥è¯†åˆ«å…³é”®å±‚ï¼Œäº§ç”Ÿä¸€ä¸ªæ ¡å‡†æŸå¤±ä¸º 0.21 çš„ä½æ¨¡
å‹ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹çš„ FID åˆ†æ•°ä¸å…·æœ‰æœ€ä½æ ¡å‡†æŸå¤±çš„æ¨¡
å‹çš„åˆ†æ•°ç›¸ä¼¼ã€‚åƒ ShortGPT è¿™æ ·çš„æ–¹æ³•[36]ä»¥åŠæœ€è¿‘ä¸€
ç§å‹ç¼©é€šé‡æ¨¡å‹çš„æ–¹æ³•[6],å…¶ä¼°è®¡è¾“å…¥å’Œè¾“å‡ºçŠ¶æ€ä¹‹é—´
ç›¸ä¼¼æ€§æˆ–æœ€å°åŒ–å‡æ–¹è¯¯å·®(MSE ),æ­ç¤ºäº†ç›¸ä¼¼çš„è¶‹åŠ¿ã€‚ç›¸
æ¯”ä¹‹ä¸‹ï¼Œå…·æœ‰é€‚åº¦æ ¡å‡†æŸå¤±çš„æ–¹æ³•ï¼Œå¦‚ Oracle(é€šå¸¸è¢«
è®¤ä¸ºç«äº‰åŠ›è¾ƒå¼±)å’Œéšæœºä¿®å‰ªæ¨¡å‹ä¹‹ä¸€ï¼Œåˆ†åˆ«è·å¾— 7.43
å’Œ 6.45 çš„ FID åˆ†æ•°ï¼Œè¡¨æ˜å…¶æ€§èƒ½æ˜æ˜¾ä¼˜äºå…·æœ‰æœ€å°æ ¡å‡†
æŸå¤±çš„æ¨¡å‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶æ ¡å‡†æŸè€—å¯èƒ½åœ¨æŸç§
ç¨‹åº¦ä¸Šå½±å“å¾®è°ƒåçš„æ€§èƒ½ï¼Œä½†å®ƒä¸æ˜¯æ‰©æ•£å˜å‹å™¨çš„ä¸»è¦
å†³å®šå› ç´ ã€‚ç›¸åï¼Œæ¨¡å‹åœ¨å¾®è°ƒæœŸé—´çš„æ€§èƒ½æ¢å¤èƒ½åŠ›(ç§°
ä¸ºâ€œå¯æ¢å¤æ€§â€)ä¼¼ä¹æ›´ä¸ºå…³é”®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ä¼ 
ç»ŸæŒ‡æ ‡è¯„ä¼°å¯æ¢å¤æ€§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦è·¨æ•´ä¸ªæ•°
æ®é›†çš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ä¸€è§‚å¯Ÿä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆä¸åŸºçº¿æ–¹æ³•
ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è·å¾—äº†æ›´å¥½çš„ç»“æœ(5.73)ã€‚
Learnable Modeling of Recoverability. To overcome the
0
0 2000 4000 6000 8000 10000
0
0 2000 4000 6000 8000 10000
1:2 LoRA 54.75 33.39 29.5
6
0.5
6
0.6
2
2:4 LoRA 53.07 34.21 27.6
1
0.5
5
0.6
3
7:14 LoRA 34.97 49.41 28.4
8
0.4
6
0.5
6
1:2 Full 53.11 35.77 32.6
8
0.5
4
0.6
1
2:4 Full 53.63 34.41 29.9
3
0.5
5
0.6
2
7:14 Full 45.03 38.76 31.3 0.5 0.6
1:2 LoRA 54.75 33.39 29.5
6
0.5
6
0.6
2
2:4 LoRA 53.07 34.21 27.6
1
0.5
5
0.6
3
7:14 LoRA 34.97 49.41 28.4
8
0.4
6
0.5
6
1:2 Full 53.11 35.77 32.6
8
0.5
4
0.6
1
2:4 Full 53.63 34.41 29.9
3
0.5
5
0.6
2
7:14 Full 45.03 38.76 31.3 0.5 0.6
Layer Index in DiT- Layer Index in DiT-
34
limitations of traditional metric-based methods, this study
introduces a learnable approach to jointly optimize
pruning and model recoverability. Table 3 illustrates different configurations of the learnable method, including
the local pruning scheme and update strategies for
recoverabil- ity estimation. For a 28-layer DiT-XL/2 with
a fixed 50%
å¯æ¢å¤æ€§çš„å¯å­¦å»ºæ¨¡ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿçš„åŸºäºåº¦é‡çš„æ–¹
æ³•çš„å±€é™æ€§ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•æ¥è”åˆ
ä¼˜åŒ–å‰ªæå’Œæ¨¡å‹å¯æ¢å¤æ€§ã€‚æ¡Œå­ 3 è¯´æ˜äº†å¯å­¦ä¹ æ–¹æ³•
çš„ä¸åŒé…ç½®ï¼ŒåŒ…æ‹¬ç”¨äºå¯æ¢å¤æ€§ä¼°è®¡çš„å±€éƒ¨ä¿®å‰ªæ–¹æ¡ˆ
å’Œæ›´æ–°ç­–ç•¥ã€‚å¯¹äºå›ºå®š 50%çš„ 28 å±‚ DiT-XL/2
Train iterations
35
ing 7 layers, resulting in Ã— 2 = 6,864 possible
soluing 7 layers, resulting in Ã— 2 = 6,864
possible soluè®­ç»ƒè¿­ä»£
Figure 6. Visualization of the 2:4 decisions in the learnable
prun- ing, with the confidence level of each decision
highlighted through varying degrees of transparency. More
visualization results for 1:2 and 7:14 schemes are available in
the appendix.
å›¾ 6ã€‚å¯å­¦ä¹ è¿‡ç¨‹ä¸­ 2:4 å†³ç­–çš„å¯è§†åŒ–ï¼Œé€šè¿‡ä¸åŒç¨‹åº¦çš„
é€æ˜åº¦çªå‡ºæ¯ä¸ªå†³ç­–çš„ç½®ä¿¡åº¦ã€‚é™„å½•ä¸­æä¾›äº† 1:2 å’Œ 7:14
æ–¹æ¡ˆçš„æ›´å¤šå¯è§†åŒ–ç»“æœã€‚
layer pruning rate, we examine three splitting schemes:
1:2, 2:4, and 7:14. In the 1:2 scheme, for example,
every two transformer layers form a local block, with
one layer pruned. Larger blocks introduce greater
diversity but sig- nificantly expand the search space.
For instance, the 7:14 scheme divides the model into
two segments, each retainå±‚å‰ªæç‡ï¼Œæˆ‘ä»¬æ£€æŸ¥ä¸‰ä¸ªåˆ†è£‚æ–¹æ¡ˆ :1:2ï¼Œ2:4 å’Œ
7:14ã€‚ä¾‹å¦‚ï¼Œåœ¨ 1:2 æ–¹æ¡ˆä¸­ï¼Œæ¯ä¸¤ä¸ªå˜æ¢å™¨å±‚å½¢æˆ
ä¸€ä¸ªå±€éƒ¨å—ï¼Œå…¶ä¸­ä¸€å±‚è¢«ä¿®å‰ªã€‚è¾ƒå¤§çš„å—å¼•å…¥äº†æ›´
å¤§çš„å¤šæ ·æ€§ï¼Œä½†æ˜¯æ˜¾è‘—åœ°æ‰©å±•äº†æœç´¢ç©ºé—´ã€‚ä¾‹
å¦‚ï¼Œ7:14 æ–¹æ¡ˆå°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ä¿ç•™14
14
7
ä¸ƒ
tions. Conversely, smaller blocks significantly reduce opé€‰é¡¹ã€‚ç›¸åï¼Œè¾ƒå°çš„æ¨¡å—ä¼šæ˜¾è‘—é™ä½ optimization difficulty and offer greater flexibility. When
the distribution of one block converges, the learning on
other blocks can still progress. As shown in Table 3, the
1:2 con- figuration achieves the optimal performance
after 10K fine- tuning iterations. Additionally, our
empirical findings un- derscore the effectiveness of
recoverability estimation using LoRA or full fine-tuning.
Both methods yield positive post- fine-tuning outcomes,
with LoRA achieving superior results (FID = 33.39)
compared to full fine-tuning (FID = 35.77) under the 1:2
scheme, as LoRA has fewer trainable parame- ters
(0.9% relative to full parameter training) and can adapt
more efficiently to the randomness of sampling.
ä¼˜åŒ–éš¾åº¦å¹¶æä¾›æ›´å¤§çš„çµæ´»æ€§ã€‚å½“ä¸€ä¸ªå—çš„åˆ†å¸ƒæ”¶
æ•›æ—¶ï¼Œå…¶ä»–å—ä¸Šçš„å­¦ä¹ ä»ç„¶å¯ä»¥è¿›è¡Œã€‚å¦‚è¡¨ä¸­æ‰€ç¤º
3,åœ¨ 10K å¾®è°ƒè¿­ä»£ä¹‹åï¼Œ1:2 é…ç½®å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚
æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»éªŒå‘ç°ä½äºä½¿ç”¨ LoRA æˆ–å®Œå…¨å¾®è°ƒçš„
å¯æ¢å¤æ€§ä¼°è®¡çš„æœ‰æ•ˆæ€§ã€‚ä¸¤ç§æ–¹æ³•éƒ½äº§ç”Ÿäº†ç§¯æçš„å¾®
è°ƒåç»“æœï¼Œä¸ 1:2 æ–¹æ¡ˆä¸‹çš„å®Œå…¨å¾®è°ƒ(FID = 35.77)ç›¸
æ¯”ï¼ŒLoRA å–å¾—äº†æ›´å¥½çš„ç»“æœ(FID = 33.39)ï¼Œå› ä¸º
LoRA å…·æœ‰æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°(ç›¸å¯¹äºå®Œå…¨å‚æ•°è®­ç»ƒä¸º
0.9%)ï¼Œå¹¶ä¸”å¯ä»¥æ›´æœ‰æ•ˆåœ°é€‚åº”é‡‡æ ·çš„éšæœºæ€§ã€‚
Visualization of Learnable Decisions. To gain deeper
in- sights into the role of the learnable method in pruning,
we visualize the learning process in Figure 6. From
bottom to top, the i-th curve represents the i-th layer of the
pruned model, displaying its layer index in the original
DiT-XL/2. This visualization illustrates the dynamics of
pruning de- cisions over training iterations, where the
transparency of each data point indicates the probability of
being sampled. The learnable method shows its capacity to
explore and handle various layer combinations. Pruning
decisions for certain layers, such as the 7-th and 8-th in the
compressed model, are determined quickly and remain
stable through- out the process. In contrast, other layers,
like the 0-th layer, require additional fine-tuning to
estimate their recoverabil- ity. Notably, some decisions
may change in the later stages
å¯å­¦ä¹ å†³ç­–çš„å¯è§†åŒ–ã€‚ä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£å¯å­¦ä¹ æ–¹æ³•
åœ¨ä¿®å‰ªä¸­çš„ä½œç”¨ï¼Œæˆ‘ä»¬å°†å­¦ä¹ è¿‡ç¨‹å½¢è±¡åŒ–ä¸ºå›¾ 6.ä»ä¸‹
åˆ°ä¸Šï¼Œç¬¬ I æ¡æ›²çº¿ä»£è¡¨ä¿®å‰ªæ¨¡å‹çš„ç¬¬ I å±‚ï¼Œæ˜¾ç¤º
å…¶åœ¨åŸå§‹ D i T - X L / 2 ä¸­çš„å±‚ç´¢å¼•ã€‚è¿™ç§å¯è§†åŒ–è¯´
æ˜ äº† è®­ ç»ƒ è¿­ ä»£ ä¸­ ä¿® å‰ª å†³ ç­– çš„ åŠ¨ æ€ ï¼Œ å…¶ ä¸­ æ¯ ä¸ª æ•°
æ® ç‚¹ çš„ é€ æ˜ åº¦ è¡¨ ç¤º è¢« é‡‡ æ · çš„ æ¦‚ ç‡ ã€‚ å¯ å­¦ ä¹  çš„ æ–¹
æ³• æ˜¾ ç¤º äº† å…¶ æ¢ ç´¢ å’Œ å¤„ ç† å„ ç§ å±‚ ç»„ åˆ çš„ èƒ½ åŠ› ã€‚ æŸ
äº›å±‚çš„ä¿®å‰ªå†³å®šï¼Œä¾‹å¦‚å‹ç¼©æ¨¡å‹ä¸­çš„ç¬¬ 7 å±‚å’Œç¬¬
8 å±‚ï¼Œè¢«å¿«é€Ÿç¡®å®šï¼Œå¹¶åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šã€‚
ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–å±‚ï¼Œå¦‚ç¬¬ 0 å±‚ï¼Œéœ€è¦é¢å¤–çš„å¾®è°ƒ
æ¥ ä¼° è®¡ å®ƒ ä»¬ çš„ å¯ æ¢ å¤ æ€§ ã€‚ å€¼ å¾— æ³¨ æ„ çš„ æ˜¯ ï¼Œ æœ‰ äº›
å†³å®šå¯èƒ½ä¼šåœ¨åæœŸå‘ç”Ÿå˜åŒ–
36
Figure 7. Images generated by TinyDiT-D14 on ImageNet 224Ã—224, pruned and distilled from a
DiT-XL/2.
å›¾ 7ã€‚ç”± TinyDiT-D14 åœ¨ ImageNet 224Ã—224 ä¸Šç”Ÿæˆçš„å›¾åƒï¼Œä» DiT-XL/2 ä¸­è¿›è¡Œä¿®å‰ªå’Œæå–ã€‚
0.4
0.4
0.3
0.3
0.2
0.2
0.1
0.1
0.0
0.0
10
2
1
0
1
10
0
0
10
0
10
1
1
0
2
102
101
100
0
100
101
102
Activ
ation
Value
(log)
æ¿€æ´»å€¼
(å¯¹æ•°)
(a) DiT-XL/2 (Teacher)
(b) DiT-XL/2(æ•™å¸ˆ)
Den Dens
Min Activation: -
-Std: -
+Std:
Max Activation:
Min Activation: -
-Std: -
+Std:
Max Activation:
37
0.5
0.5
0.4
0.4
0.3
0.3
0.2
0.2
0.1
0.1
0.0
0.0 102
101
100 0 100
101
102 101 100 0 100 101
Activation Value (log)
æ¿€æ´»å€¼(å¯¹æ•°)
(c) TinyDiT-D14 (Student)
(d) è’‚å°¼è¿ªç‰¹-D14(å­¦ç”Ÿ)
Figure 8. Visualization of massive activations [47] in DiTs. Both
teacher and student models display large activation values in
their hidden states. Directly distilling these massive activations
may result in excessively large losses and unstable training.
å›¾ 8ã€‚å¤§è§„æ¨¡æ¿€æ´»çš„å¯è§†åŒ–[47]åœ¨ DiTs ä¸­ã€‚æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹
åœ¨å®ƒä»¬çš„éšè—çŠ¶æ€ä¸‹éƒ½æ˜¾ç¤ºå¤§çš„æ¿€æ´»å€¼ã€‚ç›´æ¥æç‚¼è¿™äº›æµ·é‡
æ¿€æ´»ï¼Œå¯èƒ½ä¼šå¯¼è‡´æŸå¤±è¿‡å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚
once these layers have been sufficiently optimized. The
training process ultimately concludes with high sampling
probabilities, suggesting a converged learning process
with distributions approaching a one-hot configuration.
After training, we select the layers with the highest
probabilities for subsequent fine-tuning.
ä¸€æ—¦è¿™äº›å±‚è¢«å……åˆ†ä¼˜åŒ–ã€‚è®­ç»ƒè¿‡ç¨‹æœ€ç»ˆä»¥é«˜é‡‡æ ·æ¦‚ç‡
ç»“æŸï¼Œè¿™è¡¨æ˜åˆ†å¸ƒæ¥è¿‘ä¸€ä¸ªçƒ­ç‚¹é…ç½®çš„æ”¶æ•›å­¦ä¹ è¿‡
ç¨‹ã€‚è®­ç»ƒåï¼Œæˆ‘ä»¬é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å±‚è¿›è¡Œåç»­å¾®è°ƒã€‚
8.7. Knowledge Distillation for Recovery
8.8. çŸ¥è¯†è’¸é¦å¤å…´è¿åŠ¨
In this work, we also explore Knowledge Distillation (KD)
as an enhanced fine-tuning method. As demonstrated in
Ta- ble 5, we apply the vanilla knowledge distillation
approach proposed by Hinton [20] to fine-tune a TinyDiTD14, using the outputs of the pre-trained DiT-XL/2 as a
teacher model for supervision. We employ a Mean Square
Error (MSE) loss to align the outputs between the shallow
student model and the deeper teacher model, which
effectively reduces the FID at 100K steps from 5.79 to
4.66.
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†çŸ¥è¯†è’¸é¦(KD)ä½œä¸ºä¸€ç§
å¢å¼ºçš„å¾®è°ƒæ–¹æ³•ã€‚å¦‚è¡¨ä¸­æ‰€ç¤º 5,æˆ‘ä»¬åº”ç”¨è¾›é¡¿æå‡ºçš„æ™®
é€šçŸ¥è¯†æç‚¼æ–¹æ³•[20]ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„ DiT-XL/2 çš„è¾“å‡ºä½œä¸º
ç›‘ç£çš„æ•™å¸ˆæ¨¡å‹æ¥å¾®è°ƒ TinyDiT-D14ã€‚æˆ‘ä»¬é‡‡ç”¨å‡æ–¹è¯¯å·®
(MSE)æŸå¤±æ¥å¯¹é½æµ…å±‚å­¦ç”Ÿæ¨¡å‹å’Œæ·±å±‚æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„è¾“
å‡ºï¼Œè¿™æœ‰æ•ˆåœ°å°† 100K æ­¥é•¿çš„ FID ä» 5.79 é™ä½åˆ° 4.66ã€‚
Masked Knowledge Distillation. Additionally, we evaluate representation distillation (RepKD) [23, 42] to transfer
hidden states from the teacher to the student. It is important
to note that depth pruning does not alter the hidden dimension of diffusion transformers, allowing for direct alignment
è’™é¢çŸ¥è¯†è’¸é¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¡¨ç¤ºè’¸é¦(RepKD)
[23,42]æŠŠéšè—çŠ¶æ€ä»è€å¸ˆèº«ä¸Šè½¬ç§»åˆ°å­¦ç”Ÿèº«ä¸Šã€‚å€¼å¾—æ³¨æ„
çš„æ˜¯ï¼Œæ·±åº¦ä¿®å‰ªä¸ä¼šæ”¹å˜æ‰©æ•£å˜æ¢å™¨çš„éšè—å°ºå¯¸ï¼Œä»è€Œå…è®¸
ç›´æ¥å¯¹å‡†
Den Dens
Min Activation: -
-Std: -
+Std:
Max Activation:
fine-tuning Strategy Initfine-tuning
Logits KD
RepKD
Masked KD (0.1Ïƒ)
Masked KD (2Ïƒ)
Masked KD (4Ïƒ)
Min Activation: -
-Std: -
+Std:
Max Activation:
fine-tuning Strategy Initfine-tuning
Logits KD
RepKD
Masked KD (0.1Ïƒ)
Masked KD (2Ïƒ)
Masked KD (4Ïƒ)
38
Table 5. Evaluation of different fine-tuning strategies for
recovery. Masked RepKD ignores those massive activations
(|x| > kÏƒx) in both teacher and student, which enables
effective knowledge transfer between diffusion transformers.
è¡¨ 5ã€‚è¯„ä¼°ä¸åŒçš„æ¢å¤å¾®è°ƒç­–ç•¥ã€‚Masked RepKD å¿½ç•¥äº†æ•™å¸ˆ
å’Œå­¦ç”Ÿä¸­çš„å¤§é‡æ¿€æ´»(|x| > kÏƒx)ï¼Œä»è€Œå®ç°äº†æ‰©æ•£è½¬æ¢å™¨
ä¹‹é—´çš„æœ‰æ•ˆçŸ¥è¯†è½¬ç§»ã€‚
of intermediate hidden states. For practical
implementation, we use the block defined in Section 3.2 as
the basic unit, ensuring that the pruned local structure in
the pruned DiT aligns with the output of the original
structure in the teacher model. However, we encountered
significant training dif- ficulties with this straightforward
RepKD approach due to massive activations in the hidden
states, where both teacher and student models occasionally
exhibit large activation values, as shown in Figure 8.
Directly distilling these ex- treme activations can result in
excessively high loss values, impairing the performance of
the student model. This issue has also been observed in
other transformer-based genera- tive models, such as
certain LLMs [47]. To address this, we propose a Masked
RepKD variant that selectively ex- cludes these massive
activations during knowledge transfer.
ä¸­é—´éšè—çŠ¶æ€ã€‚å¯¹äºå®é™…å®æ–½ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬èŠ‚ä¸­å®šä¹‰
çš„æ¨¡å— 3.2 ä½œä¸ºåŸºæœ¬å•å…ƒï¼Œç¡®ä¿ç»ä¿®å‰ªçš„ DiT ä¸­çš„ç»
ä¿®å‰ªçš„å±€éƒ¨ç»“æ„ä¸æ•™å¸ˆæ¨¡å‹ä¸­çš„åŸå§‹ç»“æ„çš„è¾“å‡ºå¯¹
é½ã€‚ç„¶è€Œï¼Œç”±äºéšè—çŠ¶æ€ä¸­çš„å¤§é‡æ¿€æ´»ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™
ç§ç®€å•çš„ RepKD æ–¹æ³•é‡åˆ°äº†å¾ˆå¤§çš„è®­ç»ƒå›°éš¾ï¼Œå…¶ä¸­æ•™
å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹å¶å°”ä¼šæ˜¾ç¤ºè¾ƒå¤§çš„æ¿€æ´»å€¼ï¼Œå¦‚å›¾æ‰€ç¤º 8.
ç›´ æ¥ æ å– è¿™ äº› å¼‚ å¸¸ æ¿€ æ´» ä¼š å¯¼ è‡´ è¿‡ é«˜ çš„ æŸ å¤±
å€¼ ï¼Œ å‰Š å¼± å­¦ ç”Ÿ æ¨¡ å‹ çš„ æ€§ èƒ½ ã€‚ åœ¨ å…¶ ä»– åŸº äº å˜ å‹
å™¨ çš„ é€š ç”¨ æ¨¡ å‹ ä¸­ ä¹Ÿ è§‚ å¯Ÿ åˆ° äº† è¿™ ä¸ª é—® é¢˜ ï¼Œ ä¾‹ å¦‚
æŸ äº› L L M [47].ä¸º äº† è§£ å†³ è¿™ ä¸ª é—® é¢˜ ï¼Œ æˆ‘ ä»¬ æ å‡º
äº† ä¸€ ä¸ª å± è”½ R e p K D å˜ ä½“ ï¼Œ å®ƒ åœ¨ çŸ¥ è¯† è½¬ ç§» è¿‡ ç¨‹
ä¸­ é€‰ æ‹© æ€§ åœ° æ’ é™¤ è¿™ äº› å¤§ è§„ æ¨¡ æ¿€ æ´» ã€‚
We employ a simple thresholding method, |x âˆ’ Âµx| < kÏƒx,
which ignores the loss associated with these extreme actiæˆ‘ä»¬é‡‡ç”¨ä¸€ç§ç®€å•çš„é˜ˆå€¼æ–¹æ³•| x x | < kÏƒxï¼Œå®ƒå¿½
ç•¥äº†ä¸è¿™äº›æç«¯æ´»åŠ¨ç›¸å…³çš„æŸè€—
vations. As shown in Table 5, the Masked RepKD
approach with moderate thresholds of 2Ïƒ and 4Ïƒ achieves
satisfactory results, demonstrating the robustness of our
method.
vationsã€‚å¦‚è¡¨ä¸­æ‰€ç¤º 5,å…·æœ‰ä¸­ç­‰é˜ˆå€¼ 2Ïƒ å’Œ 4Ïƒ çš„æ©
è”½ RepKD æ–¹æ³•è·å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„
é²æ£’æ€§ã€‚
Generated Images. In Figure 7, We visualize the generated images of the learned TinyDiT-D14, distilled from an
ç”Ÿæˆçš„å›¾åƒã€‚åœ¨å›¾ä¸­ 7,æˆ‘ä»¬æƒ³è±¡ç€åšå­¦çš„å° D14 çš„å½¢
è±¡ï¼Œè¿™äº›å½¢è±¡æ˜¯ä»
39
off-the-shelf DiT-XL/2 model. More visualization results
for SiTs and MARs can be found in the appendix.
ç°æˆçš„ DiT-XL/2 å‹å·ã€‚æ›´å¤š SiTs å’Œ MARs çš„å¯è§†åŒ–ç»“
æœå¯ä»¥åœ¨é™„å½•ä¸­æ‰¾åˆ°ã€‚
9. Conclusions
10. ç»“è®º
This work introduces TinyFusion, a learnable method for
accelerating diffusion transformers by removing redundant
layers. It models the recoverability of pruned models as an
optimizable objective and incorporates differentiable sampling for end-to-end training. Our method generalizes to
various architectures like DiTs, MARs and SiTs.
è¿™é¡¹å·¥ä½œä»‹ç»äº† TinyFusionï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç§»é™¤å†—ä½™
å±‚æ¥åŠ é€Ÿæ‰©æ•£å˜å‹å™¨çš„å¯å­¦æ–¹æ³•ã€‚å®ƒå°†ä¿®å‰ªæ¨¡å‹çš„å¯
æ¢å¤æ€§å»ºæ¨¡ä¸ºå¯ä¼˜åŒ–çš„ç›®æ ‡ï¼Œå¹¶ç»“åˆäº†ç«¯åˆ°ç«¯è®­ç»ƒçš„
å¯ åŒº åˆ† é‡‡ æ · ã€‚ æˆ‘ ä»¬ çš„ æ–¹ æ³• é€‚ ç”¨ äº å„ ç§ æ¶ æ„ ï¼Œ å¦‚
DiTsã€MARs å’Œ SiTsã€‚
References
å‚è€ƒ
[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,
Hang Su, and Jun Zhu. All are worth words: A vit
backbone for diffusion models. In Proceedings of the
IEEE/CVF con- ference on computer vision and pattern
recognition, pages 22669â€“22679, 2023.
[2] èŒƒè±¹ã€ç”³è‚ã€ã€è–›ã€ã€æå´‡å®£ã€ã€ã€‚æ‰€æœ‰éƒ½å€¼å¾—ä¸€è¯´:
æ‰©æ•£æ¨¡å‹çš„ vit ä¸»å¹²ã€‚IEEE/CVF è®¡ç®—æœºè§†è§‰å’Œæ¨¡å¼è¯†
åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 22669-22679 é¡µï¼Œ2023ã€‚
[3] Yoshua Bengio, Nicholas LeÂ´onard, and Aaron Courville.
Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint
arXiv:1308.3432, 2013.
[4] çº¦èˆ’é˜¿Â·æœ¬å‰å¥¥ï¼Œå°¼å¤æ‹‰æ–¯Â·å‹’å¥¥çº³å¾·å’Œäºšä¼¦Â·åº“ç»´
å°” ã€‚ ä¸º æ¡ ä»¶ è®¡ ç®— é€š è¿‡ éš æœº ç¥ ç» å…ƒ ä¼° è®¡ æˆ– ä¼  æ’­ æ¢¯
åº¦ã€‚arXiv é¢„å°æœ¬ arXiv:1308.3432ï¼Œ2013ã€‚
[5] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and
Shinkook Choi. Ld-pruner: Efficient pruning of latent
diffu- sion models using task-agnostic insights. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 821â€“830, 2024.
[6] Thibault Castells ï¼Œ Hyoung-Kyu Song ï¼Œ Bo-Kyeong
Kim å’Œ Shinkook Choiã€‚Ld-pruner:ä½¿ç”¨ä»»åŠ¡ä¸å¯çŸ¥çš„
æ´å¯ŸåŠ›æœ‰æ•ˆåœ°ä¿®å‰ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚IEEE/CVF è®¡ç®—æœºè§†
è§‰å’Œæ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 821-830 é¡µï¼Œ2024ã€‚
[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William
T Freeman. Maskgit: Masked generative image
transformer. In Conference on Computer Vision and
Pattern Recognition, pages 11315â€“11325, 2022.
[8] å¼ æ…§æ–‡ï¼Œï¼Œï¼Œåˆ˜ç­–å’Œå¨å»‰Â·TÂ·å¼—é‡Œæ›¼ã€‚é®ç½©ç”Ÿæˆå›¾åƒ
è½¬æ¢å™¨ã€‚åœ¨è®¡ç®—æœºè§†è§‰å’Œæ¨¡å¼è¯†åˆ«ä¼šè®®ä¸Šï¼Œç¬¬ 11315-
11325 é¡µï¼Œ2022ã€‚
[9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart-Î±: Fast training of diffusion transformer for photorealistic text-to-image synthesis,
2023.
[10]ã€é™ˆã€ä¿é‡‘åŸã€è‘›å´‡å¥ã€å§šä¹ä¼Ÿã€è°¢æ©æ³½ã€ã€ç‹ä¸­é“ã€éƒ­
ç‚³æ±Ÿã€ç½—å¹³ã€é™†æ²ªå·å’Œã€‚Pixart-Î±:ç”¨äºç…§ç‰‡çº§çœŸå®æ–‡æœ¬
åˆ°å›¾åƒåˆæˆçš„ dif- fusion å˜æ¢å™¨çš„å¿«é€Ÿè®­ç»ƒï¼Œ2023ã€‚
[11]Javier MartÂ´Ä±n Daniel VerduÂ´. Flux.1 lite: Distilling flux1.dev
for efficient text-to-image generation. 2024.
[12]å“ˆç»´å°”Â·é©¬ä¸Â·ä¸¹å°¼å°”Â·ç»´å°”æœã€‚Flux.1 lite:æå–
flux1.dev ä»¥å®ç°é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚2024.
[13]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher ReÂ´. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information
Processing Systems, 35:16344â€“16359, 2022.
[14]å´”é“ï¼Œä¸¹èŠ™ï¼Œæ–¯ç‰¹å‡¡è¯ºÂ·å„è’™ï¼Œé˜¿ç‰¹ä¸½Â·èŒ¹å¾·æ‹‰å’Œå…‹é‡Œæ–¯
å¤šä½›Â·é›·ã€‚Flashattention:å…·æœ‰ io æ„ŸçŸ¥èƒ½åŠ›çš„å¿«é€Ÿä¸”é«˜
æ•ˆçš„ç²¾ç¡®æ³¨æ„åŠ›ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ35:16344â€“
16359ï¼Œ2022ã€‚
[15]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248â€“255. Ieee, 2009.
[16]é‚“ä½³ï¼Œé­ä¸œï¼Œç†æŸ¥å¾·Â·ç´¢å½»ï¼ŒæÂ·ï¼Œå’Œã€‚Imagenet:ä¸€ä¸ª
å¤§è§„æ¨¡åˆ†å±‚å›¾åƒæ•°æ®åº“ã€‚2009 å¹´ IEEE è®¡ç®—æœºè§†è§‰å’Œæ¨¡å¼
è¯†åˆ«ä¼šè®®ï¼Œç¬¬ 248-255 é¡µã€‚Ieeeï¼Œ2009 å¹´ã€‚
[17]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural information processing systems, 34:8780â€“8794, 2021.
[18]æ™®æ‹‰èŠ™æ‹‰Â·å¾·é‡Œç“¦å°”å’Œäºšå†å±±å¤§Â·å°¼ç§‘å°”ã€‚æ‰©æ•£æ¨¡å‹åœ¨å›¾
åƒ åˆ æˆ ä¸Š å‡» è´¥ äº† ç”˜ æ–¯ ã€‚ ç¥ ç» ä¿¡ æ¯ å¤„ ç† ç³» ç»Ÿ è¿›
å±•ï¼Œ34:8780â€“8794ï¼Œ2021ã€‚
[19] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
Depth-adaptive transformer. arXiv preprint
arXiv:1910.10073, 2019.
[20] ç›å“ˆÂ·è‰¾å°”å·´äºšå¾·ã€é™¶ä½³Â·å¤ã€çˆ±å¾·åÂ·æ ¼é›·å¤«å’Œè¿ˆå…‹å°”
Â· å¥¥ åˆ© ã€‚ æ·± åº¦ è‡ª é€‚ åº” å˜ å‹ å™¨ ã€‚ arXiv é¢„ å° æœ¬
arXiv:1910.10073ï¼Œ2019ã€‚
[21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim
Entezari, Jonas MuÂ¨ller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti- fied
flow transformers for high-resolution image synthesis.
[22] Patrick Esser ï¼Œ Sumith Kulal ï¼Œ Andreas
Blattmannï¼ŒRahim Entezariï¼ŒJonas Mu ller ï¼ŒHarry
Saini ï¼Œ Yam Levi ï¼Œ å¼  ç§€ å¤ Â· æ´› ä¼¦ èŒ¨ ï¼Œ Axel
Sauerï¼ŒFrederic Boeselï¼Œç­‰,ã€Šç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ
çš„ç¼©æ”¾æ•´æµæµé‡å˜æ¢å™¨ã€‹ã€‚
40
In Forty-first International Conference on Machine
Learn- ing, 2024. 2024 å¹´ç¬¬å››åä¸€å±Šæœºå™¨å­¦ä¹ å›½é™…ä¼šè®®ã€‚
[23] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural
pruning for diffusion models. In Advances in Neural Information Processing Systems, 2023.
[24] æ–¹ï¼Œï¼Œé©¬ï¼Œç‹æ–°è¶…ã€‚æ‰©æ•£æ¨¡å‹çš„ç»“æ„ä¿®å‰ªã€‚ç¥ç»ä¿¡æ¯å¤„
ç†ç³»ç»Ÿè¿›å±•ï¼Œ2023ã€‚
[25] Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg
Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, and Xinchao Wang. Maskllm: Learnable semi-structured sparsity
for large language models. arXiv preprint
arXiv:2409.17481, 2024.
[26] æ–¹ã€ã€å°¹ã€ç»æ‹‰å¤«Â·ç©†æ‹‰åˆ©å“ˆå…°ã€æ ¼é›·æ ¼Â·æµ·å› é‡Œå¸Œã€
æ°å¤«Â·æ™®å°”ã€æ‰¬Â·è€ƒèŒ¨ã€å¸•å¤«æ´›Â·è«å°”æŸ¥è¯ºå¤«å’Œè¾›Â·ç‹
è¶…ã€‚Maskllm:å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯å­¦ä¹ åŠç»“æ„åŒ–ç¨€ç–
æ€§ã€‚arXiv é¢„å°æœ¬ arXiv:2409.17481ï¼Œ2024ã€‚
[27] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li,
and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024.
[28] è´¹æ­£èªã€èŒƒæ˜è¿œã€äºæ˜Œè°¦ã€æã€é»„å†›å¸ˆã€‚å°†æ‰©æ•£å˜å‹å™¨
ç¼© æ”¾ åˆ° 16 äº¿ ä¸ª å‚ æ•° ã€‚ arXiv é¢„ å° æœ¬
arXiv:2407.11633ï¼Œ2024ã€‚
[29] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li,
Youqiang Zhang, and Junshi Huang. Dimba: Transformermamba diffusion models. arXiv preprint arXiv:2406.01159,
2024.
[30] è´¹æ­£èªï¼ŒèŒƒæ˜è¿œï¼Œä¿æ˜Œè°¦ï¼Œï¼Œæï¼Œå¼ å‹å¼ºï¼Œé»„å†›å¸ˆã€‚è¿ª
å§† å·´ : å˜ å‹ å™¨ - æ›¼ å·´ æ‰© æ•£ æ¨¡ å‹ ã€‚ arXiv é¢„ å° æœ¬
arXiv:2406.01159ï¼Œ2024ã€‚
[31] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, MingMing Cheng, and Shuicheng Yan. Editanything: Empowering unparalleled flexibility in image editing and generation.
In Proceedings of the 31st ACM International Conference
on Multimedia, Demo track, 2023.
[32] é«˜ã€ã€è°¢å…´å®‡ã€ã€ç¨‹æ˜æ˜ã€ä¸¥æ°´æˆã€‚Editanything:èµ‹
äºˆå›¾åƒç¼–è¾‘å’Œç”Ÿæˆæ— ä¸ä¼¦æ¯”çš„çµæ´»æ€§ã€‚ç¬¬ 31 å±Š ACM å¤š
åª’ä½“å›½é™…ä¼šè®®è®ºæ–‡é›†ï¼Œæ¼”ç¤ºéŸ³è½¨ï¼Œ2023ã€‚
[33] Emil Julius Gumbel. Statistical theory of extreme values
and some practical applications: a series of lectures. US
Gov- ernment Printing Office, 1954.
[34] åŸƒç±³å°”Â·æœ±åˆ©å¶æ–¯Â·å†ˆè´å°”ã€‚æå€¼ç»Ÿè®¡ç†è®ºåŠä¸€äº›å®é™…
åº”ç”¨:ç³»åˆ—è®²åº§ã€‚ç¾å›½æ”¿åºœå°åˆ·å±€ï¼Œ1954 å¹´ã€‚
[35] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems,
28, 2015.
[36] å®‹å¯’ã€æ°å¤«Â·æ™®å°”ã€çº¦ç¿°Â·ç‰¹å…°ã€å¨å»‰Â·æˆ´åˆ©ã€‚å­¦ä¹ æœ‰
æ•ˆç¥ç»ç½‘ç»œçš„æƒé‡å’Œè¿æ¥ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›
å±•ï¼Œ28ï¼Œ2015ã€‚
[37] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou,
and Bohan Zhuang. Ptqd: Accurate post-training
quantization for diffusion models. Advances in Neural
Information Pro- cessing Systems, 36, 2024.
[38] ä½•ã€ã€åˆ˜ã€ã€ã€ã€åº„åšæ¶µã€‚Ptqd:æ‰©æ•£æ¨¡å‹çš„ç²¾ç¡®è®­ç»ƒ
åé‡åŒ–ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ36ï¼Œ2024ã€‚
[39] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2(7), 2015.
[40] Geoffrey Hinton ï¼ŒOriol Vinyalsï¼Œ Jeff Dean ï¼Œç­‰
41
äºº ï¼Œ ä» ç¥ ç» ç½‘ ç»œ ä¸­ æ å– çŸ¥ è¯† ã€‚ arXiv é¢„ å° æœ¬
arXiv:1503.02531ï¼Œ2(7)ï¼Œ2015ã€‚
[41] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. LoRA: Low-rank adaptation of large language
models. In In- ternational Conference on Learning
Representations, 2022.
[42] çˆ±å¾·åÂ·JÂ·èƒ¡ï¼ŒÂ·æ²ˆï¼Œè²åˆ©æ™®Â·æ²ƒåˆ©æ–¯ï¼ŒÂ·è‰¾ä¼¦Â·
æœ±ï¼Œï¼ŒShean Wangï¼Œï¼Œå’Œé™ˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½é˜¶é€‚
åº”ã€‚2022 å¹´å›½é™…å­¦ä¹ ä»£è¡¨ä¼šè®®ã€‚
[43] Eric Jang, Shixiang Gu, and Ben Poole. Categorical
reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
[44] åŸƒé‡Œå…‹Â·å¼ ã€çŸ³ç¥¥Â·å¤å’Œæœ¬Â·æ™®å°”ã€‚ç”¨ gumbelsoftmax è¿› è¡Œ åˆ† ç±» é‡ æ–° å‚ æ•° åŒ– ã€‚ arXiv é¢„ å° æœ¬
arXiv:1611.01144ï¼Œ2016ã€‚
[45] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells,
and Shinkook Choi. Bk-sdm: Architecturally compressed
stable diffusion for efficient text-to-image generation. In
Workshop on Efficient Systems for Foundation Models@
ICML2023, 2023.
[46] Bo-Kyeong Kim ï¼Œ Hyoung-Kyu Song ï¼Œ Thibault
Castells å’Œ Shinkook Choiã€‚Bk-sdm:ç”¨äºé«˜æ•ˆæ–‡æœ¬
åˆ°å›¾åƒç”Ÿæˆçš„æ¶æ„å‹ç¼©ç¨³å®šæ‰©æ•£ã€‚åŸºç¡€æ¨¡å‹é«˜æ•ˆç³»ç»Ÿ
ç ”è®¨ä¼š@ ICML2023ï¼Œ2023ã€‚
[47] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault
Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu
Song. Shortened llama: A simple depth pruning for large
lan- guage models. arXiv preprint arXiv:2402.02834, 11,
2024.
[48] é‡‘åšäº¬ã€é‡‘ä¿Šæ°‘ã€é‡‘æ³°æµ©ã€è’‚åšç‰¹Â·å¡æ–¯ç‰¹å°”æ–¯ã€å´”
æ–°å›½ã€ç”³ä¿Šæµ©å’Œå®‹æ°¸å¥ã€‚ç¼©çŸ­çš„ç¾Šé©¼:å¤§å‹è¯­è¨€æ¨¡å‹
çš„ ç®€ å• æ·± åº¦ ä¿® å‰ª ã€‚ arXiv é¢„ å° æœ¬
arXiv:2402.02834ï¼Œ11ï¼Œ2024ã€‚
[49] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024.
[50] åŒ—äº¬å¤§å­¦è¢å®éªŒå®¤å’Œæ¶‚å±•è‰¾ç­‰ã€‚å¼€æ”¾å¼ç´¢æ‹‰è®¡åˆ’ï¼Œ2024
å¹´ã€‚
[51] Black Forest Labs. FLUX, 2024.
[52] é»‘æ£®æ—å®éªŒå®¤ã€‚é€šé‡ï¼Œ2024ã€‚
[53] Youngwan Lee, Yong-Ju Lee, and Sung Ju Hwang. Ditpruner: Pruning diffusion transformer models for text-toimage synthesis using human preference scores.
[54] ææ°¸ä¸‡ï¼Œææ°¸æŸ±å’Œé»„æˆæŸ±ã€‚Dit- pruner:ä½¿ç”¨äººç±»å
å¥½åˆ†æ•°ä¿®å‰ªæ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ã€‚
42
[55] Youngwan Lee, Kwanyong Park, Yoorhim Cho, Yong-Ju
Lee, and Sung Ju Hwang. Koala: self-attention matters in knowledge distillation of latent diffusion models for
memory-efficient and fast image synthesis. arXiv e-prints,
pages arXivâ€“2312, 2023.
[56] ææ°¸ä¸‡ï¼Œå…³æ°¸æœ´ï¼Œèµµä½‘è´¤ï¼Œææ°¸æŸ±å’Œé»„æˆæŸ±ã€‚è€ƒæ‹‰:ç”¨
äºè®°å¿†æœ‰æ•ˆå’Œå¿«é€Ÿå›¾åƒåˆæˆçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†æç‚¼
ä¸­ çš„ è‡ª æˆ‘ æ³¨ æ„ æ æ–™ ã€‚ arXiv ç”µ å­ ç‰ˆ ï¼Œ ç¬¬ arXivâ€“
2312ã€2023 é¡µã€‚
[57] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and
Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024.
[58] æï¼Œï¼Œç”°æ°¸é¾™ï¼Œï¼Œé‚“ï¼Œã€‚æ— çŸ¢é‡é‡åŒ–çš„è‡ªå›å½’å›¾åƒç”Ÿ
æˆã€‚arXiv é¢„å°æœ¬ arXiv:2406.11838ï¼Œ2024ã€‚
[59] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen
Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
Q-diffusion: Quantizing diffusion models. In Proceedings
of the IEEE/CVF International Conference on Computer
Vi- sion, pages 17535â€“17545, 2023.
[60] æç§€ç‰ï¼Œï¼Œé¾™è²ï¼Œæ¨ç„•ç‘ï¼Œè‘£æŒ¯ï¼Œåº·å¤§ç‰›ï¼Œå¼ ä¸Š
èˆªï¼ŒKurt Keutzerã€‚é‡åŒ–æ‰©æ•£æ¨¡å‹ã€‚IEEE/CVF å›½é™…è®¡
ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 17535-17545 é¡µï¼Œ2023 å¹´ã€‚
[61] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,
Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
Snap- fusion: Text-to-image diffusion model on mobile
devices within two seconds. Advances in Neural
Information Pro- cessing Systems, 36, 2024.
[62] ã€æã€ã€é‡‘é’ã€èƒ¡å·¨è‘—ã€å¸•å¤«æ´›Â·åˆ‡æ¢…åˆ©æ–¯ã€å‚…äº‘ã€ã€
è°¢å°”ç›–Â·å›¾åˆ©äºšç§‘å¤«å’Œä»»å¥ã€‚Snap- fusion:ç§»åŠ¨è®¾å¤‡
ä¸Šä¸¤ç§’å†…æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›
å±•ï¼Œ36ï¼Œ2024ã€‚
[63] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation.
arXiv preprint arXiv:2402.13929, 2024.
[64] æ—å±±å·ï¼Œç‹å’Œè‚–æ‰¬ã€‚Sdxl-é—ªç”µ:æ¸è¿›å¯¹æŠ—æ€§æ‰©æ•£è’¸
é¦ã€‚arXiv é¢„å°æœ¬ arXiv:2402.13929ï¼Œ2024ã€‚arXiv
preprint arXiv:2402.13929
[65] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for
diffusion probabilistic model sampling in around 10 steps.
Advances in Neural Information Processing Systems,
35:5775â€“5787, 2022.
[66] ã€å‘¨ã€ã€èŒƒè±¹ã€ã€æå´‡å®£ã€ã€‚Dpm-solver:ç”¨äºæ‰©æ•£æ¦‚
ç‡æ¨¡å‹é‡‡æ ·çš„å¿«é€Ÿ ode æ±‚è§£å™¨ï¼Œå¤§çº¦ 10 æ­¥ã€‚ç¥ç»ä¿¡æ¯
å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ35:5775â€“5787ï¼Œ2022ã€‚
[67] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas
M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit:
Explor- ing flow and diffusion-based generative models with
scalable interpolant transformers. arXiv preprint
arXiv:2401.08740, 2024.
[68] å¶æ¥ Â·é©¬ã€é©¬å…‹Â·æˆˆå°”èŒ¨å¦ã€è¿ˆå…‹å°”Â·SÂ·é˜¿å°”ä¼¯æˆˆã€
å°¼å¤æ‹‰æ–¯Â·MÂ·åšè²ã€åŸƒé‡Œå…‹Â·èŒƒç™»-è‰¾é‡‘ç™»å’Œè°¢èµ›
å®ã€‚Sit:æ¢ç´¢åŸºäºæµåŠ¨å’Œæ‰©æ•£çš„å¯æ‰©å±•æ’å€¼å˜å‹å™¨ç”Ÿæˆ
æ¨¡å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2401.08740ï¼Œ2024ã€‚
[69] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao
Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching, 2024.
[70] é©¬ï¼Œæ–¹ï¼Œç±³å¼€æœ—çªç½—ï¼Œç‹æ–°æ½®ã€‚ä»å­¦ä¹ åˆ°ç¼“å­˜:é€šè¿‡å±‚ç¼“
å­˜åŠ é€Ÿæ‰©æ•£è½¬æ¢ï¼Œ2024ã€‚
[71] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang,
Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.
Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024.
[72] å¿» é—¨ ã€ è®¸ æ˜ å®‡ ã€ ã€ ç‹ ç‚³ å® ã€ ã€ è·¯ è€€ æ° ã€ éŸ© å…ˆ åŸ¹ ã€
é™ˆã€‚Shortgpt:å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å±‚æ¯”æ‚¨é¢„æœŸçš„æ›´åŠ å†—
ä½™ã€‚arXiv é¢„å°æœ¬ arXiv:2403.03853ï¼Œ2024ã€‚
[73] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440,
2016.
[74] Pavlo Molchanovã€Stephen Tyreeã€Tero Karrasã€Timo
Aila å’Œ Jan Kautzã€‚ä¿®å‰ªå·ç§¯ç¥ç»ç½‘ç»œç”¨äºèµ„æºæœ‰æ•ˆæ¨
ç†ã€‚arXiv é¢„å°æœ¬ arXiv:1611.06440ï¼Œ2016ã€‚
[75] Zanlin Ni, Yulin Wang, Renping Zhou, Jiayi Guo, Jinyi Hu,
Zhiyuan Liu, Shiji Song, Yuan Yao, and Gao Huang.
Revisiting non-autoregressive transformers for efficient image synthesis. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 7007â€“
7016, 2024.
[76] å€ªèµæ—ã€ã€ã€å‘¨ã€ã€èƒ¡é”¦ä¸€ã€ã€å®‹ä¸–åŸºã€ã€ã€‚é‡æ–°å®¡è§†é
è‡ªå›å½’å˜å‹å™¨çš„æœ‰æ•ˆå›¾åƒåˆæˆã€‚IEEE/CVF è®¡ç®—æœºè§†è§‰å’Œ
æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 7007â€“7016 é¡µï¼Œ2024ã€‚
[77] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim,
and Changick Kim. Denoising task routing for diffusion
models. arXiv preprint arXiv:2310.07138, 2023.
[78] ç§‰å†›å…¬å›­ï¼Œæ¡‘æ•å®‡ï¼Œå­ä¿Šæˆˆï¼Œé‡‘é•‡æ°¸å’Œå¼ åŸºé‡‘ã€‚æ‰©æ•£æ¨¡å‹çš„
å»å™ªä»»åŠ¡è·¯ç”±ã€‚arXiv é¢„å°æœ¬ arXiv:2310.07138ï¼Œ2023ã€‚
[79] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195â€“4205,
2023.
[80] å¨å»‰Â·çš®å¸ƒå°”æ–¯å’Œè°¢èµ›å®ã€‚å¸¦å˜å‹å™¨çš„å¯æ‰©å±•æ‰©æ•£æ¨¡
å‹ã€‚IEEE/CVF å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡é›†ï¼Œç¬¬ 4195-4205
é¡µï¼Œ2023ã€‚
43
[81] David Raposo, Sam Ritter, Blake Richards, Timothy
Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv
preprint arXiv:2404.02258, 2024.
[82] å¤§å«Â·æ‹‰æ³¢ç´¢ã€è¨å§†Â·é‡Œç‰¹ã€å¸ƒè±å…‹Â·ç†æŸ¥å…¹ã€è’‚è«è¥¿
Â·è‰è‰å¡æ™®ã€å½¼å¾—Â·åº·å¨Â·æ±‰å¼—è±æ–¯å’Œäºšå½“Â·åœ£æ‰˜ç½—ã€‚
æ·±åº¦æ··åˆ:åœ¨åŸºäº transformer çš„è¯­è¨€æ¨¡å‹ä¸­åŠ¨æ€åˆ†é…
è®¡ç®—ã€‚arXiv é¢„å°æœ¬ arXiv:2404.02258ï¼Œ2024ã€‚
[83] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio.
Fitnets: Hints for thin deep nets. arXiv preprint
arXiv:1412.6550, 2014.
[84] é˜¿å¾·é‡Œäºšå¨œÂ·ç½—æ¢…ç½—ã€å°¼å¤æ‹‰æ–¯Â·å·´æ‹‰æ–¯ã€å‘¨æ¬£å®‡Â·æ˜“
åœæ‹‰å¸Œç±³Â·å¡èƒ¡ã€å®‰æ‰˜ä¸‡Â·æŸ¥æ¡‘ã€å¡æ´›Â·åŠ å¡”å’Œçº¦èˆ’é˜¿
Â·æœ¬å‰å¥¥ã€‚Fitnets:è–„æ·±ç½‘çš„æç¤ºã€‚arXiv é¢„å°æœ¬
arXiv:1412.6550ï¼Œ2014ã€‚
[85] Tim Salimans and Jonathan Ho. Progressive distillation
for fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512, 2022.
[86] è’‚å§†Â·è¨åˆ©æ›¼æ–¯å’Œä¹”çº³æ£®Â·ä½•ã€‚æ‰©æ•£æ¨¡å‹å¿«é€Ÿå–æ ·çš„æ¸
è¿›è’¸é¦ã€‚arXiv é¢„å°æœ¬ arXiv:2202.00512ï¼Œ2022ã€‚
[87] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and
Yan Yan. Post-training quantization on diffusion models. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1972â€“1981, 2023.
[88] å°šï¼Œè¢æ™ºèˆªï¼Œï¼Œå´ç‚³å“²ï¼Œã€‚æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒåé‡
åŒ– ã€‚ IEEE/CVF è®¡ ç®— æœº è§† è§‰ å’Œ æ¨¡ å¼ è¯† åˆ« ä¼š è®® è®º æ–‡
é›†ï¼Œ1972-1981ï¼Œ2023 é¡µã€‚
[89] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020.
[90] å®‹å®¶æ˜ï¼Œå­Ÿå’Œæ–¯ç‰¹å‡¡è¯ºåŸƒå°”è’™ã€‚å»å™ªæ‰©æ•£éšå¼æ¨¡
å‹ã€‚arXiv é¢„å°æœ¬ arXiv:2010.02502ï¼Œ2020ã€‚
[91] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469, 2023.
[92] å®‹æ´‹ã€æ™®æ‹‰å¯Œæ‹‰Â·å¾·é‡Œç“¦å°”ã€é™ˆå”å±±å’Œä¼Šåˆ©äºšÂ·è‹èŒ¨åŸº
å¼— ã€‚ ä¸€ è‡´ æ€§ æ¨¡ å‹ ã€‚ arXiv é¢„ å° æœ¬
arXiv:2303.01469ï¼Œ2023ã€‚
[93] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu.
Massive activations in large language models. arXiv preprint
arXiv:2402.17762, 2024.
[94] å­™æ˜æ´ï¼Œé™ˆï¼Œç§‘ç‰¹å’Œã€‚å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¤§é‡æ¿€
æ´»ã€‚arXiv é¢„å°æœ¬ arXiv:2402.17762ï¼Œ2024ã€‚
[95] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai,
Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion
mamba for efficient high-resolution image synthesis. arXiv
preprint arXiv:2405.14224, 2024.
[96] å§šè…¾ï¼Œï¼Œï¼Œï¼Œå®ï¼Œï¼Œæˆ´ï¼Œï¼Œï¼Œï¼Œåˆ˜ã€‚Dim:ç”¨äºé«˜æ•ˆé«˜åˆ†
è¾¨ ç‡ å›¾ åƒ åˆ æˆ çš„ æ‰© æ•£ mamba ã€‚ arXiv é¢„ å° æœ¬
arXiv:2405.14224ï¼Œ2024ã€‚
[97] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image
generation via next-scale prediction. 2024.
[98] ç”°å…‹å®‡ï¼Œï¼Œè¢æ³½ç„•ï¼Œå½­å†°æœˆï¼Œã€‚è§†è§‰è‡ªå›å½’å»ºæ¨¡:é€šè¿‡
ä¸‹ä¸€å°ºåº¦é¢„æµ‹çš„å¯ç¼©æ”¾å›¾åƒç”Ÿæˆã€‚2024.
[99] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu,
and Yunhe Wang. U-dits: Downsample tokens in u-shaped
diffusion transformers. arXiv preprint arXiv:2405.02730,
2024.
[100]ã€æ¶‚å¿—å†›ã€é™ˆæ±‰åº­ã€ã€å’Œã€‚u-dits:u å½¢æ‰©æ•£å˜æ¢å™¨ä¸­
çš„ ä¸‹ é‡‡ æ · æ ‡ è®° ã€‚ arXiv é¢„ å° æœ¬
44
arXiv:2405.02730ï¼Œ2024ã€‚
[101]Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, and
Jun Zhu. Sparsedm: Toward sparse efficient diffusion
models. arXiv preprint arXiv:2404.10445, 2024.
[102]ç‹å…‹å³°ã€ã€ã€ç±³å’Œã€‚ç¨€ç–æœ‰æ•ˆæ‰©æ•£æ¨¡å¼ã€‚arXiv é¢„å°
æœ¬ arXiv:2404.10445 ï¼Œ 2024 ã€‚ arXiv preprint
arXiv:2404.10445
[103]Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun
Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han.
Sana: Ef- ficient high-resolution image synthesis with
linear diffusion transformers. arXiv preprint
arXiv:2410.10629, 2024.
[104]è°¢ æ© æ³½ ã€ ã€ é™ˆ ã€ ã€ ã€ å¼  å“² å‡¯ ã€ æ æ² é˜³ ã€ ã€ å®‹
å¯’ã€‚Sana:ç”¨çº¿æ€§æ‰©æ•£å˜å‹å™¨è¿›è¡Œæœ‰æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾
åƒåˆæˆã€‚arXiv é¢„å°æœ¬ arXiv:2410.10629ï¼Œ2024ã€‚
[105]Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong,
Run- sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and
Ming- Hsuan Yang. Diffusion models: A comprehensive
survey of methods and applications. ACM Computing
Surveys, 56(4): 1â€“39, 2023.
[106]å‡Œé˜³ã€ã€ã€æ²ˆå¤§æ´ªã€ã€ã€ã€ã€æ˜ã€‚æ‰©æ•£æ¨¡å‹:æ–¹æ³•å’Œ
åº” ç”¨ çš„ ç»¼ åˆ è°ƒ æŸ¥ ã€‚ ç¾ å›½ è®¡ ç®— æœº å­¦ ä¼š è®¡ ç®— è°ƒ
æŸ¥ï¼Œ56(4):1â€“39ï¼Œ2023ã€‚
[107]Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei
Chu, and Li Cui. Width & depth pruning for vision
transformers. In Conference on Artificial Intelligence
(AAAI), 2022.
[108]éƒèŠ³ã€é»„æ˜†ã€ç‹çŒ›ã€ç¨‹è¿œã€é­åˆå’Œå´”è‰ã€‚è§†è§‰å˜å½¢å™¨
çš„å®½åº¦å’Œæ·±åº¦ä¿®å‰ªã€‚äººå·¥æ™ºèƒ½å¤§ä¼š (AAAI)ï¼Œ2022
å¹´ã€‚
[109]Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin
Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv
preprint arXiv:2304.06790, 2023.
[110]é™¶ç‘œï¼Œå†¯æ¶¦ç”Ÿï¼Œå†¯è‹¥ç‘œï¼Œï¼Œï¼Œæ›¾æ–‡å†›ï¼Œé™ˆå¿—æ³¢ã€‚ä¿®å¤
ä»»ä½•ä¸œè¥¿:åˆ†å‰²ä»»ä½•ä¸œè¥¿æ»¡è¶³å›¾åƒä¿®å¤ã€‚arXiv é¢„å°
æœ¬ arXiv:2304.06790ï¼Œ2023ã€‚
[111]Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and
Haonan Lu. Laptop-diff: Layer pruning and normalized
dis-
[112]å¼ å®šå¤ï¼Œï¼Œï¼Œï¼Œè°¢ï¼Œé™†æµ©å—ã€‚Laptop-diff:å±‚ä¿®å‰ªå’Œ
è§„èŒƒåŒ– dis-
45
tillation for compressing diffusion models. arXiv preprint
arXiv:2404.11098, 2024.
å‹ ç¼© æ‰© æ•£ æ¨¡ å‹ çš„ tillation ã€‚ arXiv é¢„ å° æœ¬
arXiv:2404.11098ï¼Œ2024ã€‚
[113]Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You.
Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024.
[114]èµµè½©é›·ã€é‡‘å°é¾™ã€ç‹å‡¯å’Œå°¤æ¨ã€‚é‡‘å­—å¡”æ³¨æ„åŠ›å¹¿æ’­çš„å®
æ—¶è§†é¢‘ç”Ÿæˆã€‚arXiv é¢„å°æœ¬ arXiv:2408.12588ï¼Œ2024ã€‚
[115]Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou.
Mobilediffusion: Subsecond text-to-image generation on
mobile devices. arXiv preprint arXiv:2311.16567, 2023.
[116]æ¨é’Šï¼Œè®¸ï¼Œè‚–æ™ºèƒœï¼Œä¾¯å»·æ³¢ã€‚Mobilediffusion:ç§»åŠ¨è®¾
å¤‡ ä¸Š çš„ äºš ç§’ çº§ æ–‡ æœ¬ åˆ° å›¾ åƒ ç”Ÿ æˆ ã€‚ arXiv é¢„ å° æœ¬
arXiv:2311.16567ï¼Œ2023ã€‚
[117]Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui
Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li,
and Yang You. Open-sora: Democratizing efficient video
production for all, 2024.
[118]éƒ‘è—ä¼Ÿã€å½­ç¿”å®‡ã€æ¨å¤©å‰ã€ç”³ã€æç”Ÿè´µã€ã€ã€ã€
å’Œã€‚Open-sora:å¤§ä¼—åŒ–é«˜æ•ˆè§†é¢‘åˆ¶ä½œï¼Œ2024ã€‚
1
TinyFusion: Diffusion Transformers Learned Shallow
TinyFusion:æ‰©æ•£å˜å‹å™¨å­¦ä¹ æµ…
Supplementary Material
è¡¥å……ææ–™
11. Experimental Details
12. å®éªŒç»†èŠ‚
Models. Our experiments evaluate the effectiveness of
three models: DiT-XL, MAR-Large, and SiT-XL. Diffusion
Transformers (DiTs), inspired by Vision Transformer (ViT)
principles, process spatial inputs as sequences of patches.
The DiT-XL model features 28 transformer layers, a
hidden
æ¨¡ç‰¹ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°äº†ä¸‰ä¸ªæ¨¡å‹çš„æœ‰æ•ˆæ€§:DiTXLã€MAR-Large å’Œ SiT-XLã€‚å—è§†è§‰è½¬æ¢å™¨(ViT)åŸç†çš„
å¯å‘ï¼Œæ‰©æ•£è½¬æ¢å™¨(DiTs)å°†ç©ºé—´è¾“å…¥å¤„ç†ä¸ºé¢ç‰‡åº
åˆ—ã€‚DiT-XL æ¨¡å‹å…·æœ‰ 28 ä¸ªå˜å‹å™¨å±‚ï¼Œä¸€ä¸ªéšè—çš„
size of 1152, 16 attention heads, and a 2 Ã— 2 patch size. It
employs adaptive layer normalization (AdaLN) to improve
1152 çš„å¤§å°ï¼Œ16 ä¸ªæ³¨æ„å¤´ï¼Œä»¥åŠ 2 Ã— 2 çš„è´´ç‰‡å¤§
å°ã€‚å®ƒé‡‡ç”¨è‡ªé€‚åº”å±‚æ ‡å‡†åŒ–(AdaLN)æ¥æ”¹å–„
training stability, comprising 675 million parameters and
trained for 1400 epochs. Masked Autoregressive models
(MARs) are diffusion transformer variants tailored for autoregressive image generation. They utilize a continuousvalued diffusion loss framework to generate high-quality
outputs without discrete tokenization. The MAR-Large
model includes 32 transformer layers, a hidden size of
1024, 16 attention heads, and bidirectional attention. Like
DiT, it incorporates AdaLN for stable training and
effective to- ken modeling, with 479 million parameters
trained over 400 epochs. Finally, Scalable Interpolant
Transformers (SiTs) extend the DiT framework by
introducing a flow-based in- terpolant methodology,
enabling more flexible bridging be- tween data and noise
distributions. While architecturally identical to DiT-XL,
the SiT-XL model leverages this inter- polant approach to
facilitate modular experimentation with interpolant
selection and sampling dynamics.
è®­ç»ƒç¨³å®šæ€§ï¼ŒåŒ…æ‹¬ 6.75 äº¿ä¸ªå‚æ•°ï¼Œè®­ç»ƒäº† 1400 ä¸ªæ—¶
æœŸã€‚æ©è”½è‡ªå›å½’æ¨¡å‹(MARs)æ˜¯ä¸ºè‡ªå›å½’å›¾åƒç”Ÿæˆå®šåˆ¶
çš„æ‰©æ•£å˜æ¢å™¨å˜ä½“ã€‚å®ƒä»¬åˆ©ç”¨è¿ç»­å€¼æ‰©æ•£æŸå¤±æ¡†æ¶æ¥
ç”Ÿæˆé«˜è´¨é‡çš„è¾“å‡ºï¼Œè€Œæ— éœ€ç¦»æ•£ç¬¦å·åŒ–ã€‚MAR-Large
æ¨¡å‹åŒ…æ‹¬ 32 ä¸ªå˜å½¢å±‚ã€1024 çš„éšè—å¤§å°ã€16 ä¸ªæ³¨æ„
åŠ›å¤´å’ŒåŒå‘æ³¨æ„åŠ›ã€‚åƒ DiT ä¸€æ ·ï¼Œå®ƒåŒ…å« AdaLNï¼Œç”¨äº
ç¨³å®šçš„è®­ç»ƒå’Œæœ‰æ•ˆçš„ to- ken å»ºæ¨¡ï¼Œåœ¨ 400 ä¸ªæ—¶æœŸå†…
è®­ç»ƒäº† 4.79 äº¿ä¸ªå‚æ•°ã€‚æœ€åï¼Œå¯æ‰©å±•æ’å€¼å˜å‹å™¨
(sit)é€šè¿‡å¼•å…¥åŸºäºæµé‡çš„æ’å€¼æ–¹æ³•ï¼Œæ‰©å±•äº† DiT æ¡†
æ¶ï¼Œå®ç°äº†æ•°æ®å’Œå™ªå£°åˆ†å¸ƒä¹‹é—´æ›´çµæ´»çš„æ¡¥æ¥ã€‚è™½ç„¶
åœ¨æ¶æ„ä¸Šä¸ DiT-XL ç›¸åŒï¼Œä½† SiT-XL æ¨¡å‹åˆ©ç”¨è¿™ç§æ’å€¼
æ–¹æ³•æ¥ä¿ƒè¿›æ’å€¼é€‰æ‹©å’Œé‡‡æ ·åŠ¨æ€çš„æ¨¡å—åŒ–å®éªŒã€‚
Datasets. We prepared the ImageNet 256 Ã— 256 dataset by
applying center cropping and adaptive resizing to main- tain
the original aspect ratio and minimize distortion. The
æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨ä¸­å¿ƒè£å‰ªå’Œè‡ªé€‚åº”è°ƒæ•´å¤§å°æ¥å‡†
å¤‡ ImageNet 256 Ã— 256 æ•°æ®é›†ï¼Œä»¥ä¿æŒåŸå§‹çºµæ¨ªæ¯”å¹¶
æœ€å°åŒ–å¤±çœŸã€‚è¿™
images were then normalized to a mean of 0.5 and a standard deviation of 0.5. To augment the dataset, we applied
random horizontal flipping with a probability of 0.5. To
accelerate training without using Variational Autoencoder
(VAE), we pre-extracted features from the images using a
pre-trained VAE. The images were mapped to their latent
representations, normalized, and the resulting feature arrays
were saved for direct use during training.
ç„¶åå°†å›¾åƒå½’ä¸€åŒ–ä¸ºå¹³å‡å€¼ 0.5 å’Œæ ‡å‡†åå·® 0.5ã€‚ä¸ºäº†
æ‰©å……æ•°æ®é›†ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ¦‚ç‡ä¸º 0.5 çš„éšæœºæ°´å¹³ç¿»è½¬ã€‚
ä¸ºäº†åœ¨ä¸ä½¿ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨(VAE)çš„æƒ…å†µä¸‹åŠ é€Ÿè®­ç»ƒï¼Œ
æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„ VAE ä»å›¾åƒä¸­é¢„å…ˆæå–ç‰¹å¾ã€‚å›¾åƒè¢«
æ˜ å°„åˆ°å®ƒä»¬çš„æ½œåœ¨è¡¨ç¤ºï¼Œå½’ä¸€åŒ–ï¼Œå¹¶ä¸”ç»“æœç‰¹å¾é˜µåˆ—è¢«
ä¿å­˜ç”¨äºè®­ç»ƒæœŸé—´çš„ç›´æ¥ä½¿ç”¨ã€‚
Training Details The training process began with obtaining pruned models using the proposed learnable pruning
method as illustrated in Figure 12. Pruning decisions were
made by a joint optimization of pruning and weight updates
through LoRA with a block size. In practice, the block size
is 2 for simplicity and the models were trained for 100
epochs, except for MAR, which was trained for 40 epochs.
To enhance post-pruning performance, the Masked Knowledge Distillation (RepKD) method was employed during the
recovery phase to transfer knowledge from teacher modè®­ç»ƒç»†èŠ‚è®­ç»ƒè¿‡ç¨‹ä»ä½¿ç”¨å¦‚å›¾æ‰€ç¤ºçš„å»ºè®®çš„å¯å­¦ä¹ ä¿®å‰ª
æ–¹æ³•è·å¾—ä¿®å‰ªæ¨¡å‹å¼€å§‹ 12.å‰ªæå†³ç­–ç”±å‰ªæå’Œæƒé‡æ›´
æ–°çš„è”åˆä¼˜åŒ–é€šè¿‡å…·æœ‰å—å¤§å°çš„ L o R A åšå‡ºã€‚å®é™…
ä¸Šï¼Œä¸ºç®€å•èµ·è§ï¼Œå—å¤§å°ä¸º 2ï¼Œæ¨¡å‹è¢«è®­ç»ƒ 1 0 0 ä¸ª
æ—¶æœŸï¼Œé™¤äº† M A Rï¼Œå®ƒè¢«è®­ç»ƒ 40 ä¸ªæ—¶æœŸã€‚ä¸ºäº†æé«˜
å‰ªæåçš„æ€§èƒ½ï¼Œåœ¨æ¢å¤é˜¶æ®µé‡‡ç”¨äº†å±è”½çŸ¥è¯†æå–æ–¹
æ³•æ¥è½¬ç§»æ•™å¸ˆæ¨¡å‹ä¸­çš„çŸ¥è¯†
2
27
27
26
26
25
25
24
24
23
23
22
22
21
21
20
20
19
19
18
18
17
17
16
16
15
15
14
14
13
13
12
12
11
11
10
10
9
9
8
8
7
ä¸ƒ
6
6
5
5
4
å››
3
3
2
2
1
ä¸€
Train iterations
è®­ç»ƒè¿­ä»£
Figure 9. 1:2 Pruning Decisions
å›¾ 9ã€‚1:2 ä¿®å‰ªå†³ç­–
27
27
26
26
25
25
24
24
23
23
22
22
21
21
20
20
19
19
18
18
17
17
16
16
15
15
14
14
13
13
12
12
11
11
10
10
9
9
8
8
7
ä¸ƒ
6
6
5
5
4
å››
0
0
0 2000 4000 6000 8000 10000
0 2000 4000 6000 8000 10000
Train iterations
è®­ç»ƒè¿­ä»£
Figure 10. 2:4 Pruning Decisions
å›¾ 10ã€‚2:4 ä¿®å‰ªå†³ç­–
27
27
26
26
25
25
24
24
23
23
22
22
21
21
20
0
0 2000 4000 6000 8000 10000
0
0 2000 4000 6000 8000 10000
Layer Index in DiT- Layer Index in DiT3
2
3
2
Layer Index in DiT- Layer Index in DiT0
0 2000 4000 6000 8000 10000 0
0200040006000800010000Layer Index in DiT- Layer Index in DiT-
3
20
19
19
18
18
17
17
16
16
15
15
14
14
13
13
12
12
11
11
10
10
9
9
8
8
7
ä¸ƒ
6
6
5
5
4
å››
3
3
2
2
1
ä¸€
Train iterations
è®­ç»ƒè¿­ä»£
Figure 11. 7:14 Pruning Decisions
å›¾ 11ã€‚7:14 ä¿®å‰ªå†³ç­–
els to pruned student models. The RepKD approach
aligns the output predictions and intermediate hidden
states of the pruned and teacher models, with further
details provided in the following section. Additionally,
as Exponential Mov- ing Averages (EMA) are updated
and used during image generation, an excessively small
learning rate can weaken EMAâ€™s effect, leading to
suboptimal outcomes. To address this, a progressive
learning rate scheduler was implemented to gradually
halve the learning rate throughout training. The
els åˆ°ä¿®å‰ªè¿‡çš„å­¦ç”Ÿæ¨¡å‹ã€‚RepKD æ–¹æ³•å°†è¾“å‡ºé¢„æµ‹ä¸
ä¿®å‰ªæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„ä¸­é—´éšè—çŠ¶æ€å¯¹é½ï¼Œåœ¨ä¸‹ä¸€
èŠ‚ä¸­æä¾›äº†è¿›ä¸€æ­¥çš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œéšç€æŒ‡æ•°ç§»åŠ¨å¹³
å‡å€¼(EMA)åœ¨å›¾åƒç”ŸæˆæœŸé—´è¢«æ›´æ–°å’Œä½¿ç”¨ï¼Œè¿‡å°çš„å­¦
ä¹ ç‡ä¼šå‰Šå¼± EMA çš„æ•ˆæœï¼Œå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³
è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªæ¸è¿›å¼å­¦ä¹ ç‡è®¡åˆ’ç¨‹
åºï¼Œä»¥ä¾¿åœ¨æ•´ä¸ªåŸ¹è®­è¿‡ç¨‹ä¸­é€æ¸å°†å­¦ä¹ ç‡å‡åŠã€‚è¿™
Transformer Layer Transformer Layer Transformer Layer Transformer Layer
Transformer Layer Transformer Layer Transformer Layer Transformer Layer
4
Update Categorical Distribution
ğ¦ğ¢ğ§ ğ“›(ğš½ + ğš«ğš½)
ğš½
ğ’ğ¡ğšğ«ğğ ğš«ğš½
(LoRA/Full)
Update Categorical Distribution
ğ¦ğ¢ğ§ ğ“›(ğš½ + ğš«ğš½)
ğš½
ğ’ğ¡ğšğ«ğğ ğš«ğš½
(LoRA/Full)
TTfLLocal Block
æœ¬åœ°å—
5
 y
Diff.
Sampling
~
Joint
Opt. Update Transformer Layer
Transformer Layer
Transformer Layer
TrTarnasnfosfromrmereLraLyaeyrer
Transformer Layer
Differentiable Sampling of Candidate Solutions Recoverability Estimation Winner Decision
Sampling
~
Opt. Update Transformer Layer
Transformer Layer Transformer Layer
TrTarnasnfosfromrmereLraLyaeyrer
Transformer Layer
Differentiable Sampling of Candidate Solutions Recoverability Estimation Winner Decision
mask
Masked
Distillation mask
Massive Activation ( ğ‘¥ > ğ‘˜ â‹… ğœğ‘¥)
Transformer Block
Transformer Block Transformer Block
Transformer Block
Transformer Block
Transformer Block
mask
Masked
Distillation mask
Massive Activation ( ğ‘¥ > ğ‘˜ â‹… ğœğ‘¥)
Transformer Block
Transformer Block
Transformer Block
Transformer Block
Transformer Block
Transformer Block
Figure 12.
Learnable depth pruning on a local block
å›¾ 12ã€‚å±€éƒ¨å—ä¸Šçš„å¯å­¦ä¹ æ·±åº¦ä¿®å‰ª
Hidden States Hidden States
éšè—çŠ¶æ€ éšè—çŠ¶æ€ corresponds to the masked distillation loss applied to the
hidden states, as illustrated in Figure 13, which encourages
alignment between the intermediate representations of the
pruned model and the original model. The corresponding
hyperparameters Î±KD, Î±Diff and Î±Rep can be found in Ta- ble
6.
6
å¯¹åº”äºåº”ç”¨äºéšè—çŠ¶æ€çš„æ©è”½è’¸é¦æŸå¤±ï¼Œå¦‚å›¾
æ‰€ç¤º 13,è¿™ä¿ƒè¿›äº†ä¿®å‰ªæ¨¡å‹å’ŒåŸå§‹æ¨¡å‹çš„ä¸­é—´
è¡¨ç¤ºä¹‹é—´çš„å¯¹é½ã€‚ç›¸åº”çš„è¶…å‚æ•° Î±KDã€Î±Diff
å’Œ Î±Rep å¯åœ¨è¡¨ä¸­æ‰¾åˆ° 6.
DiT
æ‘©å°”æ–¯è®¯å·çš„
çŸ­éŸ³
Learning
the
optimal
sublayers
å­¦ä¹ æœ€
ä½³å­å±‚
7
culty. This is due to the =3,432 candidates, which is
too
culty. This is due to the =3,432
candidates, which is too
Tiny
DiT
Tiny
DiT
Hidden State
Alignment. The
masked distillation
loss LRep is critical
for aligning the
intermediate
representations of
the student and
teacher
mo
del
s.D
uri
ng
the
rec
ove
ry
éšè—çŠ¶æ€å¯¹é½ã€‚å±è”½è’¸é¦æŸå¤± LRep å¯¹äºå¯¹é½å­¦ç”Ÿå’Œæ•™
å¸ˆæ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºæ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨æ¢å¤æœŸé—´
Figure 13. Masked knowledge distillation with 2:4 blocks.
å›¾ 13ã€‚åŸºäº 2:4 å—çš„éšè”½çŸ¥è¯†æå–ã€‚
details of each hyperparameter are provided in Table 6.
è¡¨ä¸­æä¾›äº†æ¯ä¸ªè¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ 6.
13. Visualization of Pruning Decisions
14. ä¿®å‰ªå†³ç­–çš„å¯è§†åŒ–
Figures 9, 10 and 11 visualize the dynamics of pruning decisions during training for the 1:2, 2:4, and 7:14 pruning
schemes. Different divisions lead to varying search spaces,
which in turn result in various solutions. For both the 1:2
and 2:4 schemes, good decisions can be learned in only
one epoch, while the 7:14 scheme encounters optimization
diffiæ•°å­— 9,10 å’Œ 11 åœ¨ 1:2ã€2:4 å’Œ 7:14 ä¿®å‰ªæ–¹æ¡ˆçš„è®­ç»ƒæœŸ
é—´ï¼Œå¯è§†åŒ–ä¿®å‰ªå†³ç­–çš„åŠ¨æ€ã€‚ä¸åŒçš„åˆ’åˆ†å¯¼è‡´ä¸åŒçš„
æœç´¢ç©ºé—´ï¼Œè¿›è€Œå¯¼è‡´ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äº 1:2 å’Œ 2:4
æ–¹æ¡ˆï¼Œå¥½çš„å†³ç­–åªèƒ½åœ¨ä¸€ä¸ªæ—¶æœŸä¸­å­¦ä¹ ï¼Œè€Œ 7:14 æ–¹æ¡ˆ
é‡åˆ°ä¼˜åŒ–å›°éš¾
14
14
7
ä¸ƒ
huge and thus cannot be adequately sampled within a
single
å·¨å¤§ï¼Œå› æ­¤æ— æ³•åœ¨å•ä¸ª
epoch. Therefore, in practical applications, we use the 1:2
or 2:4 schemes for learnable layer pruning.
æ–°çºªå…ƒã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 1:2 æˆ– 2:4
æ–¹æ¡ˆè¿›è¡Œå¯å­¦ä¹ çš„å±‚ä¿®å‰ªã€‚
15. Details of Masked Knowledge
Distillation
16. è’™é¢çŸ¥è¯†è’¸é¦çš„ç»†èŠ‚
Training Loss. This work deploys a standard knowledge
distillation to learn a good student model by mimicking
the pre-trained teacher. The loss function is formalized as:
åŸ¹è®­æŸå¤±ã€‚è¿™é¡¹å·¥ä½œéƒ¨ç½²äº†ä¸€ä¸ªæ ‡å‡†çš„çŸ¥è¯†æç‚¼ï¼Œé€š
è¿‡æ¨¡ä»¿é¢„å…ˆè®­ç»ƒçš„è€å¸ˆæ¥å­¦ä¹ ä¸€ä¸ªå¥½çš„å­¦ç”Ÿæ¨¡å‹ã€‚æŸ
å¤±å‡½æ•°è¢«å½¢å¼åŒ–ä¸º:
L = Î±KD Â· LKD + Î±Diff Â· LDiff + Î² Â· LRep (8)
L = Î±KD LKD + Î±Diff LDiff + Î²
LRep (8)
Here, LKD denotes the Mean Squared Error between the
outputs of the student and teacher models. LDiff repre- sents
the original pre-training loss function. Finally, LRep
è¿™é‡Œï¼ŒLKD è¡¨ç¤ºå­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å‡æ–¹è¯¯
å·®ã€‚LDiff ä»£è¡¨åŸå§‹çš„é¢„è®­ç»ƒæŸå¤±å‡½æ•°ã€‚æœ€åï¼ŒLRep
8
phase, each layer of the student model is designed to
repli- cate the output hidden states of a corresponding
two-layer local block from the teacher model. Depth
pruning does not alter the internal dimensions of the
layers, enabling direct alignment without additional
projection layers. For mod- els such as SiTs, where
hidden state losses are more pro- nounced due to their
unique interpolant-based architecture,
é˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹çš„æ¯ä¸€å±‚è¢«è®¾è®¡æˆä»æ•™å¸ˆæ¨¡å‹å¤åˆ¶ç›¸
åº”çš„ä¸¤å±‚å±€éƒ¨å—çš„è¾“å‡ºéšè—çŠ¶æ€ã€‚æ·±åº¦ä¿®å‰ªä¸ä¼šæ”¹å˜
å±‚çš„å†…éƒ¨å°ºå¯¸ï¼Œä»è€Œæ— éœ€é¢å¤–çš„æŠ•å½±å±‚å³å¯ç›´æ¥å¯¹
é½ã€‚å¯¹äº sit ç­‰æ¨¡å‹ï¼Œç”±äºå…¶ç‹¬ç‰¹çš„åŸºäºæ’å€¼çš„æ¶
æ„ï¼Œéšè—çŠ¶æ€æŸå¤±æ›´åŠ æ˜æ˜¾ï¼Œ
a smaller coefficient Î² is applied to LRep to mitigate potential training instability. The gradual decrease in Î² throughè¾ƒå°çš„ç³»æ•° Î² åº”ç”¨äº LRep ä»¥å‡è½»æ½œåœ¨çš„è®­ç»ƒä¸ç¨³å®š
æ€§ã€‚Î² é€šè¿‡-é€æ¸å‡å°‘
out training further reduces the risk of negative impacts on
convergence.
out è®­ç»ƒè¿›ä¸€æ­¥é™ä½äº†å¯¹æ”¶æ•›äº§ç”Ÿè´Ÿé¢å½±å“çš„é£é™©ã€‚
Iterative Pruning and Distillation. Table 7 assesses the
effectiveness of iterative pruning and teacher selection
strategies. To obtain a TinyDiT-D7, we can either directly
prune a DiT-XL with 28 layers or craft a TinyDiT-D14
first and then iteratively produce the small models. To
investi- gate the impact of teacher choice and the method
for obtain- ing the initial weights of the student model, we
derived the initial weights of TinyDiT-D7 by pruning both
a pre-trained model and a crafted intermediate model.
Subsequently, we used both the trained and crafted models
as teachers for the pruned student models. Across four
experimental set- tings, pruning and distilling using the
crafted intermedi- ate model yielded the best performance.
Notably, models pruned from the crafted model
outperformed those pruned from the pre-trained model
regardless of the teacher model employed in the
distillation process. We attribute this suè¿­ä»£ä¿®å‰ªå’Œæç‚¼ã€‚æ¡Œå­ 7 è¯„ä¼°è¿­ä»£ä¿®å‰ªå’Œæ•™å¸ˆé€‰æ‹©ç­–
ç•¥çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è·å¾— TinyDiT-D7ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä¿®
å‰ª 28 å±‚çš„ DiT-XLï¼Œæˆ–è€…é¦–å…ˆåˆ¶ä½œ TinyDiT-D14ï¼Œç„¶å
è¿­ä»£äº§ç”Ÿå°æ¨¡å‹ã€‚ä¸ºäº†ç ”ç©¶æ•™å¸ˆé€‰æ‹©çš„å½±å“å’Œè·å¾—å­¦
ç”Ÿæ¨¡å‹åˆå§‹æƒé‡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ä¿®å‰ªé¢„å…ˆè®­ç»ƒçš„æ¨¡
å‹å’Œç²¾å¿ƒåˆ¶ä½œçš„ä¸­é—´æ¨¡å‹æ¥å¯¼å‡º TinyDiT-D7 çš„åˆå§‹æƒ
é‡ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ç»è¿‡è®­ç»ƒå’Œç²¾å¿ƒåˆ¶ä½œçš„æ¨¡å‹ä½œä¸º
ç»è¿‡ä¿®å‰ªçš„å­¦ç”Ÿæ¨¡å‹çš„æ•™å¸ˆã€‚é€šè¿‡å››ä¸ªå®éªŒè®¾ç½®ï¼Œä½¿
ç”¨ç²¾å¿ƒåˆ¶ä½œçš„ä¸­é—´æ¨¡å‹è¿›è¡Œä¿®å‰ªå’Œæå–äº§ç”Ÿäº†æœ€ä½³æ€§
èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®ºæå–è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•™å¸ˆæ¨¡å‹
å¦‚ä½•ï¼Œä»ç²¾å¿ƒåˆ¶ä½œçš„æ¨¡å‹ä¸­åˆ å‡çš„æ¨¡å‹éƒ½ä¼˜äºä»é¢„å…ˆ
è®­ç»ƒçš„æ¨¡å‹ä¸­åˆ å‡çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ­¤å½’å› äº su-
Teacher Model Pruned From IS FID sFID Prec
.
Recal
l
DiT-XL/2 DiT-XL/2 29.46 56.18 26.03 0.43 0.51
DiT-XL/2 TinyDiT-D14 51.96 36.69 28.28 0.53 0.59
TinyDiT-D14 DiT-XL/2 28.30 58.73 29.53 0.41 0.50
TinyDiT-D14 TinyDiT-D14 57.97 32.47 26.05 0.55 0.60
Learning Rate IS FID sFID Prec. Recall
lr=2e-4 207.27 3.73 5.04 0.8127 0.5401
lr=1e-4 194.31 4.10 5.01 0.8053 0.5413
lr=5e-5 161.40 6.63 6.69 0.7419 0.5705
Teacher Model Pruned From IS FID sFID Prec
.
Recal
l
DiT-XL/2 DiT-XL/2 29.46 56.18 26.03 0.43 0.51
DiT-XL/2 TinyDiT-D14 51.96 36.69 28.28 0.53 0.59
TinyDiT-D14 DiT-XL/2 28.30 58.73 29.53 0.41 0.50
TinyDiT-D14 TinyDiT-D14 57.97 32.47 26.05 0.55 0.60
Learning Rate IS FID sFID Prec. Recall
lr=2e-4 207.27 3.73 5.04 0.8127 0.5401
lr=1e-4 194.31 4.10 5.01 0.8053 0.5413
lr=5e-5 161.40 6.63 6.69 0.7419 0.5705
9
Model Optimizer Cosine Sched. Teacher Î±KD Î±GT Î² Grad. Clip Pruning Configs
DiT-D19 AdamW(lr=2e-4, wd=0.0) Î·min = 1e-4 DiT-XL 0.9 0.1 1e-2 â†’ 0 1.0 LoRA-1:2
DiT-D14 AdamW(lr=2e-4, wd=0.0 Î·min = 1e-4 DiT-XL 0.9 0.1 1e-2 â†’ 0 1.0 LoRA-1:2
DiT-D7 AdamW(lr=2e-4, wd=0.0) Î·min = 1e-4 DiT-D14 0.9 0.1 1e-2 â†’ 0 1.0 LoRA-1:2
SiT-D14 AdamW(lr=2e-4, wd=0.0) Î·min = 1e-4 SiT-XL 0.9 0.1 2e-4 â†’ 0 1.0 LoRA-1:2
MAR-D16 AdamW(lr=2e-4, wd=0.0) Î·min = 1e-4 MAR-Large 0.9 0.1 1e-2 â†’ 0 1.0 LoRA-1:2
æ¨¡å‹ ã€è®¡ç®—æœºã€‘ä¼˜åŒ–ç¨‹åº ä½™å¼¦ Schedã€‚ æ•™å¸ˆ Î±K
D
Î±G
T
Î² æ¯•ä¸šç”Ÿã€‚
å¤¹å­
ä¿®å‰ªé…ç½®
DiT-D19 AdamW(lr=2e4ï¼Œwd=0.0)
Î·min = 1e-4 DiT-XL 0.9 0.1 1e-2 â†’
0
1.0 åŠ³æ‹‰-1:2
è¿ªç‰¹-D14 AdamW(lr=2e-4ï¼Œwd=0.0 Î·min = 1e-4 DiT-XL 0.9 0.1 1e-2 â†’
0
1.0 åŠ³æ‹‰-1:2
DiT-D7 AdamW(lr=2e4ï¼Œwd=0.0)
Î·min = 1e-4 è¿ªç‰¹-D14 0.9 0.1 1e-2 â†’
0
1.0 åŠ³æ‹‰-1:2
åä¸‹-D14 AdamW(lr=2e4ï¼Œwd=0.0)
Î·min = 1e-4 SiT-XL 0.9 0.1 2e-4 â†’
0
1.0 åŠ³æ‹‰-1:2
3 æœˆ 16 æ—¥ AdamW(lr=2e4ï¼Œwd=0.0)
Î·min = 1e-4 è¶…å¤§çš„ 0.9 0.1 1e-2 â†’
0
1.0 åŠ³æ‹‰-1:2
Table 6. Training details and hyper-parameters for mask training
è¡¨ 6ã€‚é¢å…·è®­ç»ƒçš„è®­ç»ƒç»†èŠ‚å’Œè¶…å‚æ•°
Table 7. TinyDiT-D7 is pruned and distilled with different
teacher models for 10k, sample steps is 64, original weights are
used for sampling rather than EMA.
è¡¨ 7ã€‚TinyDiT-D7 ç”¨ 10k çš„ä¸åŒæ•™å¸ˆæ¨¡å‹è¿›è¡Œå‰ªæå’Œæå–ï¼Œ
æ ·æœ¬æ­¥æ•°ä¸º 64ï¼ŒåŸå§‹æƒé‡ç”¨äºé‡‡æ ·è€Œä¸æ˜¯ EMAã€‚
Table 8. The effect of Learning rato for TinyDiT-D14 finetuning
w/o knowledge distillation
1
Masked KD Finetune
DiT-L/2 Masked KScratch D Finetune
DiT-L/2 Scratch
è¡¨ 8ã€‚å­¦ä¹ ç‡å¯¹è’‚å°¼è¿ªç‰¹-D14 å¾®è°ƒæ— çŸ¥è¯†æå–çš„å½±å“
5.5
5.5
5.0
5.0
4.5
4.5
4.0
4.0
3.5
3.5
3.0
3.0
100 200 300 400 500
100 200 300 400 500
Steps
æ­¥ä¼
Figure 14. FID and training steps.
å›¾ 14ã€‚FID å’ŒåŸ¹è®­æ­¥éª¤ã€‚
F F
1
L
e
a
r
n
i
n
g
R
a
t
e
.
W
e
a
l
s
o search on some key hyperparam- eters
such as learning rates in Table 8. We
identify the ef- fectiveness of lr=2e-4 and
apply it to all models and exper- iments.
å­¦ä¹ ç‡ã€‚æˆ‘ä»¬è¿˜æœç´¢äº†ä¸€äº›å…³é”®çš„è¶…å‚æ•°ï¼Œå¦‚è¡¨ä¸­çš„
å­¦ä¹ ç‡ 8.æˆ‘ ä»¬ ç¡® å®š äº† l r = 2 e - 4 çš„ æœ‰ æ•ˆ æ€§ ï¼Œ å¹¶ å°†
å…¶ åº”ç”¨ äºæ‰€ æœ‰çš„ æ¨¡å‹ å’Œå® éªŒã€‚
10. Visulization
10.å¯è§†åŒ–
Figure 15 and 16 showcase the generated images from
TinySiT-D14 and TinyMAR-D16, which were
compressed from the official checkpoints. These models
were trained using only 7% and 10% of the original pretraining costs, respectively, and were distilled using the
proposed masked knowledge distillation method. Despite
compression, the
æ•°å­— 15 å’Œ 16 å±•ç¤ºä» TinySiT-D14 å’Œ TinyMAR-D16 ç”Ÿæˆ
çš„å›¾åƒï¼Œè¿™äº›å›¾åƒæ˜¯ä»å®˜æ–¹æ£€æŸ¥ç«™å‹ç¼©è€Œæ¥çš„ã€‚è¿™äº›
æ¨¡å‹åˆ†åˆ«åªä½¿ç”¨åŸå§‹é¢„è®­ç»ƒæˆæœ¬çš„ 7%å’Œ 10%è¿›è¡Œè®­
ç»ƒï¼Œå¹¶ä½¿ç”¨æ‰€æå‡ºçš„æ©è”½çŸ¥è¯†æå–æ–¹æ³•è¿›è¡Œæå–ã€‚å°½
ç®¡è¿›è¡Œäº†å‹ç¼©
perior performance to two factors: first, the crafted
modelâ€™s structure is better adapted to knowledge
distillation since it was trained using a distillation method;
second, the reduced search space facilitates finding a more
favorable initial state for the student model.
perior æ€§èƒ½å–å†³äºä¸¤ä¸ªå› ç´ :é¦–å…ˆï¼Œç²¾é›•ç»†ç¢çš„æ¨¡å‹
çš„ç»“æ„æ›´å¥½åœ°é€‚åº”çŸ¥è¯†æç‚¼ï¼Œå› ä¸ºå®ƒæ˜¯ä½¿ç”¨æç‚¼æ–¹æ³•
è®­ç»ƒçš„ï¼›ç¬¬äºŒï¼Œå‡å°‘çš„æœç´¢ç©ºé—´æœ‰åŠ©äºä¸ºå­¦ç”Ÿæ¨¡å‹æ‰¾
åˆ°æ›´æœ‰åˆ©çš„åˆå§‹çŠ¶æ€ã€‚
17. Analytical Experiments
18. åˆ†æå®éªŒ
Training Strategies Figure 14 illustrates the effectiveness of standard fine-tuning and knowledge distillation
(KD), where we prune DiT-XL to 14 layers and then apply various fine-tuning methods. Figure 3 presents the FID
scores across 100K to 500K steps. It is evident that the
standard fine-tuning method allows TinyDiT-D14 to achieve
performance comparable to DiT-L while offering faster inference. Additionally, we confirm the significant
effective- ness of distillation, which enables the model to
surpass DiT- L at just 100K steps and achieve better FID
scores than the 500K standard fine-tuned TinyDiT-D14.
This is because the distillation of hidden layers provides
stronger supervision. Further increasing the training steps
to 500K leads to sig- nificantly better results.
åŸ¹è®­ç­–ç•¥å›¾ 14 è¯´æ˜äº†æ ‡å‡†å¾®è°ƒå’ŒçŸ¥è¯†æå–(KD)çš„æœ‰æ•ˆ
æ€§ï¼Œå…¶ä¸­æˆ‘ä»¬å°† DiT-XL åˆ å‡åˆ° 14 å±‚ï¼Œç„¶ååº”ç”¨å„ç§
å¾®è°ƒæ–¹æ³•ã€‚å›¾ 3 æ˜¾ç¤ºäº† 100K è‡³ 500K çº§çš„ FID åˆ†æ•°ã€‚å¾ˆ
æ˜æ˜¾ï¼Œæ ‡å‡†å¾®è°ƒæ–¹æ³•å…è®¸ TinyDiT-D14 è·å¾—ä¸ DiT-L
ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›æ›´å¿«çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯å®äº†
è’¸é¦çš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œè¿™ä½¿å¾—è¯¥æ¨¡å‹ä»…ç”¨ 100K æ­¥å°±è¶…è¿‡äº†
DiT- Lï¼Œå¹¶è·å¾—äº†æ¯” 500K æ ‡å‡†å¾®è°ƒ TinyDiT-D14 æ›´å¥½çš„
FID åˆ†æ•°ã€‚è¿™æ˜¯å› ä¸ºéšè—å±‚çš„æç‚¼æä¾›äº†æ›´å¼ºçš„ç›‘ç£ã€‚
å°†è®­ç»ƒæ­¥æ•°è¿›ä¸€æ­¥å¢åŠ åˆ° 500K ä¼šäº§ç”Ÿæ˜æ˜¾æ›´å¥½çš„ç»“æœã€‚
1
models are capable of generating plausible results with
only 50% of depth.
æ¨¡å‹åªéœ€è¦ 50%çš„æ·±åº¦å°±èƒ½äº§ç”Ÿå¯ä¿¡çš„ç»“æœã€‚
11. Limitations
11.é™åˆ¶
In this work, we explore a learnable depth pruning method
to accelerate diffusion transformer models for conditional
image generation. As Diffusion Transformers have shown
significant advancements in text-to-image generation, it is
valuable to conduct a systematic analysis of the impact of
layer removal within the text-to-image tasks. Additionally,
there exist other interesting depth pruning strategies that
need to be studied, such as more fine-grained pruning
strate- gies that remove attention layers and MLP layers
indepen- dently instead of removing entire transformer
blocks. We leave these investigations for future work.
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§å¯å­¦ä¹ çš„æ·±åº¦å‰ªææ–¹
æ³•ï¼Œä»¥åŠ é€Ÿæ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„æ¡ä»¶å›¾åƒç”Ÿæˆã€‚ç”±äºæ‰©
æ•£å˜å‹å™¨åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä¸­å·²ç»æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„è¿›
æ­¥ï¼Œæ‰€ä»¥åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­è¿›è¡Œå±‚å»é™¤çš„å½±å“çš„
ç³»ç»Ÿåˆ†ææ˜¯æœ‰ä»·å€¼çš„ã€‚æ­¤å¤–ï¼Œè¿˜å­˜åœ¨å…¶ä»–éœ€è¦ç ”ç©¶çš„
æœ‰è¶£çš„æ·±åº¦ä¿®å‰ªç­–ç•¥ï¼Œä¾‹å¦‚æ›´ç»†ç²’åº¦çš„ä¿®å‰ªç­–ç•¥ï¼Œè¯¥
ç­–ç•¥ç‹¬ç«‹åœ°ç§»é™¤æ³¨æ„å±‚å’Œ MLP å±‚ï¼Œè€Œä¸æ˜¯ç§»é™¤æ•´ä¸ªå˜
æ¢å™¨å—ã€‚æˆ‘ä»¬æŠŠè¿™äº›è°ƒæŸ¥ç•™ç»™æœªæ¥çš„å·¥ä½œã€‚
1
Figure 15. Generated images from TinySiT-D14
1
å›¾ 15ã€‚ä» TinySiT-D14 ç”Ÿæˆçš„å›¾åƒ
Figure 16. Generated images from TinyMAR-D16
å›¾ 16ã€‚TinyMAR-D16 ç”Ÿæˆçš„å›¾åƒ