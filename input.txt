[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wang† National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization, resulting in considerable infer- ence overhead in real-world applications.
[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wang† National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization,导致在现实世界应用中大量的推论开销。

--------------------------------------------------

[002] In this work, we present TinyFusion, a depth pruning method designed to re- move redundant layers from diffusion transformers via end- to-end learning.
[002] 在这项工作中，我们提出了TinyFusion，这是一种深度修剪方法，旨在通过最终学习将冗余层从扩散变压器中移动。

--------------------------------------------------

[003] The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning.
[003] 我们方法的核心原理是创建具有高可恢复性的修剪模型，从而使其在微调后恢复了强劲的性能。

--------------------------------------------------

[004] To accom- plish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized pa- rameter to simulate future fine-tuning.
[004] 为了实现这一目标，我们引入了一种可区分的抽样技术，以使修剪可学习，并与合作的Pa-Rameter配对，以模拟未来的微调。

--------------------------------------------------

[005] While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning perfor- mance of pruned models.
[005] 虽然先前的工作重点是最大程度地减少修剪后的损失或错误，但我们的方法显式模型并优化了修剪模型的预先调整后调节性能。

--------------------------------------------------

[006] Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing exist- ing importance-based and error-based methods.
[006] 实验结果表明，这种可学习的范式为扩散变压器的层修剪提供了可观的好处，超过了基于重要性和基于错误的方法的存在。

--------------------------------------------------

[007] Addition- ally, TinyFusion exhibits strong generalization across di- verse architectures, such as DiTs, MARs, and SiTs.
[007] 补充说，微小的灌注表现出跨个性体系结构（例如DIT，MARS和SITS）的强烈概括。

--------------------------------------------------

[008] Ex- periments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre- training cost, achieving a 2× speedup with an FID score of 2.86, outperforming competitors with comparable effi- ciency.
[008] DIT-XL的概念表明，TinyFusion可以以不到预训练成本的7％的7％来制作浅扩散变压器，以2.86的FID得分达到2×加速，优于竞争对手，表现优于相当的效率。

--------------------------------------------------

[009] Code is available at https://github.com/ VainF/TinyFusion 1.
[009] 代码可在https://github.com/ vainf/tinyfusion 1中找到。

--------------------------------------------------

[010] Introduction Diffusion Transformers have emerged as a cornerstone ar- chitecture for generative tasks, achieving notable success in areas such as image [11, 26, 40] and video synthe- sis [25, 59].
[010] 引言扩散变压器已成为生成任务的基石 - 在图像[11、26、40]和视频综合[25、59]等领域取得了显着成功。

--------------------------------------------------

[011] This success has also led to the widespread availability of high-quality pre-trained models on the Inter- net, greatly accelerating and supporting the development of various downstream applications [5, 16, 53, 55].
[011] 这一成功还导致了互联网上高质量预训练的模型的广泛可用性，极大地加速和支持了各种下游应用程序的开发[5，16，53，55]。

--------------------------------------------------

[012] However, pre-trained diffusion transformers usually come with con- *Equal contribution †Corresponding author Transformer Layer Transformer Layer Transformer Layer Transformer Layer Differentiable Sampling of Layer Mask 𝖒  Recoverability Estimation with 𝚫𝚽  1 0 1 0 Local Block Joint Opt.
[012] 但是，预训练的扩散变压器通常具有相等的贡献†相应的作者变压器层变压器层变压器层变压器层层层掩模的可恢复性估计的可恢复性估计，𝚫𝚽1 0 1 0局部块接头选择。

--------------------------------------------------

[013] Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer 𝚫𝚽  (LoRA/Full) 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽, 𝖒) 𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒 Figure 1.
[013] 变压器层变压器层变压器层变压器层变压器层变压器层𝚫𝚽（lora/full）𝐦𝐢𝐧𝖒，𝚫𝚽𝓛（𝒙，𝚽+ 𝚫𝚽，𝖒，𝖒）𝝓𝟏 𝝓𝟏 𝝓𝟑 𝝓𝟑图1。

--------------------------------------------------

[014] This work presents a learnable approach for pruning the depth of pre-trained diffusion transformers.
[014] 这项工作提出了一种可学习的方法，用于修剪预训练的扩散变压器的深度。

--------------------------------------------------

[015] Our method simulta- neously optimizes a differentiable sampling process of layer masks and a weight update to identify a highly recoverable solution, en- suring that the pruned model maintains competitive performance after fine-tuning.
[015] 我们的方法同时优化了层掩模的可区分采样过程和重量更新，以识别高度可恢复的解决方案，并确保修剪模型在微调后保持竞争性能。

--------------------------------------------------

[016] siderable inference costs due to the huge parameter scale, which poses significant challenges for deployment.
[016] 由于巨大的参数量表而导致的推理成本很大，这给部署带来了重大挑战。

--------------------------------------------------

[017] To re- solve this problem, there has been growing interest from both the research community and industry in developing lightweight models [12, 23, 32, 58].
[017] 为了解决这个问题，研究社区和行业对开发轻质模型的兴趣越来越大[12，23，32，58]。

--------------------------------------------------

[018] The efficiency of diffusion models is typically influ- enced by various factors, including the number of sampling steps [33, 43, 45, 46], operator design [7, 48, 52], compu- tational precision [19, 30, 44], network width [3, 12] and depth [6, 23, 36].
[018] 扩散模型的效率通常受到各种因素的影响，包括采样步骤的数量[33，43，45，46]，操作员设计[7，48，52]，组合精度[19，30，44]，网络宽度[3，12] [3，12]和深度[6，23，36]。

--------------------------------------------------

[019] In this work, we focus on model compres- sion through depth pruning [36, 54], which removes entire layers from the network to reduce the latency.
[019] 在这项工作中，我们通过深度修剪[36，54]专注于模型的组合，该模型从网络中删除了整个层以减少延迟。

--------------------------------------------------

[020] Depth prun- ing offers a significant advantage in practice: it can achieve a linear acceleration ratio relative to the compression rate on both parallel and non-parallel devices.
[020] 深度修剪在实践中提供了重要的优势：相对于并行设备和非平行设备的压缩率，它可以达到线性加速度比率。

--------------------------------------------------

[021] For example, as will be demonstrated in this work, while 50% width prun- ing [12] only yields a 1.6× speedup, pruning 50% of the layers results in a 2× speedup.
[021] 例如，正如这项工作所证明的那样，宽度为50％的宽度[12]仅产生1.6倍的加速，修剪50％的层导致2倍加速。

--------------------------------------------------

[022] This makes depth pruning a flexible and practical method for model compression.
[022] 这使得深度修剪了模型压缩的灵活且实用的方法。

--------------------------------------------------

[023] This work follows a standard depth pruning frame- work: unimportant layers are first removed, and the pruned model is then fine-tuned for performance recovery.
[023] 这项工作遵循标准的深度修剪框架 - 首先要删除不重要的层，然后对修剪模型进行微调以进行性能恢复。

--------------------------------------------------

[024] In the literature, depth pruning techniques designed for dif- fusion transformers or general transformers primarily fo- cus on heuristic approaches, such as carefully designed importance scores [6, 36] or manually configured pruning 1 arXiv:2412.01199v1  [cs.CV]  2 Dec 2024
[024] 在文献中，设计用于差异变压器或通用变压器设计的深度修剪技术主要是基于启发式方法，例如精心设计的重要性得分[6，36]或手动配置的修剪1 arxiv：2412.011999v1 [cs.cv] [cs.cv] [cs.cv] 2024年12月2日2024年12月2日

--------------------------------------------------

[025] schemes [23, 54].
[025] 方案[23，54]。

--------------------------------------------------

[026] These methods adhere to a loss min- imization principle [18, 37], aiming to identify solutions that maintain low loss or error after pruning.
[026] 这些方法遵守损失最小原则[18，37]，旨在识别在修剪后保持低损失或错误的解决方案。

--------------------------------------------------

[027] This paper investigates the effectiveness of this widely used principle in the context of depth compression.
[027] 本文在深度压缩的背景下研究了该广泛使用原理的有效性。

--------------------------------------------------

[028] Through experiments, we examined the relationship between calibration loss ob- served post-pruning and the performance after fine-tuning.
[028] 通过实验，我们检查了校准损失渗透后的后延期与微调后的性能之间的关系。

--------------------------------------------------

[029] This is achieved by extensively sampling 100,000 models via random pruning, exhibiting different levels of calibra- tion loss in the searching space.
[029] 这是通过通过随机修剪进行广泛采样100,000款模型来实现的，在搜索空间中表现出不同水平的碳纤维损失。

--------------------------------------------------

[030] Based on this, we analyzed the effectiveness of existing pruning algorithms, such as the feature similarity [6, 36] and sensitivity analysis [18], which indeed achieve low calibration losses in the solution space.
[030] 基于此，我们分析了现有修剪算法的有效性，例如特征相似性[6，36]和灵敏度分析[18]，它们确实在解决方案空间中实现了低校准损失。

--------------------------------------------------

[031] However, the performance of all these models after fine- tuning often falls short of expectations.
[031] 但是，精细调整后所有这些模型的性能通常都没有期望。

--------------------------------------------------

[032] This indicates that the loss minimization principle may not be well-suited for diffusion transformers.
[032] 这表明最小化原理可能不适合扩散变压器。

--------------------------------------------------

[033] Building on these insights, we reassessed the underly- ing principles for effective layer pruning in diffusion trans- formers.
[033] 在这些见解的基础上，我们重新评估了在扩散式传输中修剪有效层的基本原理。

--------------------------------------------------

[034] Fine-tuning diffusion transformers is an extremely time-consuming process.
[034] 微调扩散变压器是一个非常耗时的过程。

--------------------------------------------------

[035] Instead of searching for a model that minimizes loss immediately after pruning, we propose identifying candidate models with strong recoverability, en- abling superior post-fine-tuning performance.
[035] 我们没有在修剪后立即寻找将损失立即最小化的模型，而是提议识别具有强大可恢复性，优势较高后调节性能的候选模型。

--------------------------------------------------

[036] Achieving this goal is particularly challenging, as it requires the in- tegration of two distinct processes, pruning and fine-tuning, which involve non-differentiable operations and cannot be directly optimized via gradient descent.
[036] 实现这一目标特别具有挑战性，因为它需要对两个不同的过程进行修剪和微调，这涉及非不同的操作，并且不能通过梯度下降直接优化。

--------------------------------------------------

[037] To this end, we propose a learnable depth pruning method that effectively integrates pruning and fine-tuning.
[037] 为此，我们提出了一种可学习的深度修剪方法，可以有效整合修剪和微调。

--------------------------------------------------

[038] As shown in Figure 1, we model the pruning and fine- tuning of a diffusion transformer as a differentiable sam- pling process of layer masks [13, 17, 22], combined with a co-optimized weight update to simulate future fine-tuning.
[038] 如图1所示，我们将扩散变压器的修剪和精细调整为层掩模的可区分的Sam固定过程[13，17，22]，并结合了合作的权重更新，以模拟未来的微调。

--------------------------------------------------

[039] Our objective is to iteratively refine this distribution so that networks with higher recoverability are more likely to be sampled.
[039] 我们的目标是迭代地完善此分布，以便更有可能采样具有较高可恢复性的网络。

--------------------------------------------------

[040] This is achieved through a straightforward strat- egy: if a sampled pruning decision results in strong recover- ability, similar pruning patterns will have an increased prob- ability of being sampled.
[040] 这是通过直接的策略来实现的：如果采样修剪决策会导致强大的恢复能力，那么相似的修剪模式将具有提高采样的概率能力。

--------------------------------------------------

[041] This approach promotes the ex- ploration of potentially valuable solutions while disregard- ing less effective ones.
[041] 这种方法促进了对潜在有价值的解决方案的提出，同时又无视效率较低的解决方案。

--------------------------------------------------

[042] Additionally, the proposed method is highly efficient, and we demonstrate that a suitable solu- tion can emerge within a few training steps.
[042] 此外，所提出的方法非常有效，我们证明可以在几个训练步骤中出现合适的解决方案。

--------------------------------------------------

[043] To evaluate the effectiveness of the proposed method, we conduct extensive experiments on various transformer- based diffusion models, including DiTs [40], MARs [29], SiTs [34].
[043] 为了评估所提出方法的有效性，我们对基于变压器的扩散模型进行了广泛的实验，包括DITS [40]，MARS [29]，位于[34]。

--------------------------------------------------

[044] The learnable approach is highly efficient.
[044] 可学习的方法非常有效。

--------------------------------------------------

[045] It is able to identify redundant layers in diffusion transform- ers with 1-epoch training on the dataset, which effectively crafts shallow diffusion transformers from pre-trained mod- els with high recoverability.
[045] 它能够通过数据集上的1个上述训练来识别扩散变换中的冗余层，从而有效地从具有高可恢复性的预训练的模型中制作了浅扩散变压器。

--------------------------------------------------

[046] For instance, while the models pruned by TinyFusion initially exhibit relatively high cal- ibration loss after removing 50% of layers, they recover quickly through fine-tuning, achieving a significantly more competitive FID score (5.73 vs. 22.28) compared to base- line methods that only minimize immediate loss, using just 1% of the pre-training cost.
[046] 例如，尽管删除50％的层后，最初被TinyFusion修剪的模型最初表现出相对较高的cal损失，但与仅使用预先培训的1％的基本方法相比，它们通过微调得分迅速恢复，获得了更具竞争力的FID得分（5.73 vs 22.28）（5.73对22.28）。

--------------------------------------------------

[047] Additionally, we also ex- plore the role of knowledge distillation in enhancing re- coverability [20, 23] by introducing a MaskedKD variant.
[047] 此外，我们还通过引入maskedkd变体来表达知识蒸馏在增强可覆盖性[20，23]中的作用。

--------------------------------------------------

[048] MaskedKD mitigates the negative impact of the massive or outlier activations [47] in hidden states, which can signifi- cantly affect the performance and reliability of fine-tuning.
[048] MaskEDKD减轻了隐藏状态中大规模或异常激活的负面影响[47]，这可能会显着影响微调的性能和可靠性。

--------------------------------------------------

[049] With MaskedKD, the FID score improves from 5.73 to 3.73 with only 1% of pre-training cost.
[049] 借助MaskedKD，FID得分从5.73提高到3.73，仅占培训前成本的1％。

--------------------------------------------------

[050] Extending the training to 7% of the pre-training cost further reduces the FID to 2.86, just 0.4 higher than the original model with doubled depth.
[050] 将培训扩大到7％的培训前成本将FID进一步降低到2.86，仅比原始模型高度增加了0.4。

--------------------------------------------------

[051] Therefore, the main contribution of this work lies in a learnable method to craft shallow diffusion transformers from pre-trained ones, which explicitly optimizes the re- coverability of pruned models.
[051] 因此，这项工作的主要贡献在于一种可学习的方法，可以从预训练的方法中制作浅扩散变压器，该方法明确优化了修剪模型的可覆盖性。

--------------------------------------------------

[052] The method is general for various architectures, including DiTs, MARs and SiTs.
[052] 该方法是各种架构，包括DIT，MARS和SITS的一般方法。

--------------------------------------------------

[053] 2.
[053] 2。

--------------------------------------------------

[054] Related Works Network Pruning and Depth Reduction.
[054] 相关工作网络修剪和深度减少。

--------------------------------------------------

[055] Network prun- ing is a widely used approach for compressing pre-trained diffusion models by eliminating redundant parameters [3, 12, 31, 51].
[055] 网络修剪是一种通过消除冗余参数来压缩预训练扩散模型的广泛使用方法[3，12，31，51]。

--------------------------------------------------

[056] Diff-Pruning [12] introduces a gradient- based technique to streamline the width of UNet, fol- lowed by a simple fine-tuning to recover the performance.
[056] DIFF-PRUNING [12]引入了一种基于梯度的技术，以简化UNET的宽度，以通过简单的微调来恢复性能。

--------------------------------------------------

[057] SparseDM [51] applies sparsity to pre-trained diffusion models via the Straight-Through Estimator (STE) [2], achieving a 50% reduction in MACs with only a 1.22 in- crease in FID on average.
[057] Sparsedm [51]通过直通估计量（Ste）[2]将稀疏性应用于预训练的扩散模型，在MAC中降低了50％，平均FID仅1.22个折痕。

--------------------------------------------------

[058] While width pruning and spar- sity help reduce memory overhead, they often offer lim- ited speed improvements, especially on parallel devices like GPUs.
[058] 虽然修剪和宽度有助于减少内存开销，但它们通常会提供限制的速度提高，尤其是在诸如GPU之类的平行设备上。

--------------------------------------------------

[059] Consequently, depth reduction has gained signifi- cant attention in the past few years, as removing entire lay- ers enables better speedup proportional to the pruning ra- tio [24, 27, 28, 36, 54, 56, 58].
[059] 因此，在过去的几年中，深度的降低引起了显着关注，因为删除整个外行可以更好地加速与修剪ra-tio成正比[24、27、27、28、36、54、56、58]。

--------------------------------------------------

[060] Adaptive depth reduction techniques, such as MoD [41] and depth-aware transform- ers [10], have also been proposed.
[060] 还提出了自适应深度还原技术，例如MOD [41]和深度感知的转化[10]。

--------------------------------------------------

[061] Despite these advances, most existing methods are still based on empirical or heuris- tic strategies, such as carefully designed importance crite- ria [36, 54], sensitivity analyses [18] or manually designed schemes [23], which often do not yield strong performance guarantee after fine-tuning.
[061] 尽管取得了这些进步，但大多数现有方法仍然基于经验或启发式策略，例如精心设计的重要性迹象[36，54]，敏感性分析[18]或手动设计的方案[23]，这些方案通常不会在微调后产生强大的性能保证。

--------------------------------------------------

[062] Efficient Diffusion Transformers.
[062] 有效的扩散变压器。

--------------------------------------------------

[063] Developing efficient diffusion transformers has become an appealing focus within the community, where significant efforts have been made to enhance efficiency from various perspectives, in- cluding linear attention mechanisms [15, 48, 52], compact architectures [50], non-autoregressive transformers [4, 14, 38, 49], pruning [12, 23], quantization [19, 30, 44], feature 2
[063] 开发有效的扩散变压器已成为社区中的一个吸引人的重点，在各种角度，已经做出了巨大的努力来提高效率，包括线性注意机制[15，48，52]，紧凑型建筑[50]，非自动性变压器[4，14，38，49]，pruning [4,14,38,49]，pruns [12，23]，量化[19，30，30，44]

--------------------------------------------------

[064] Transformer Layer Transformer Layer Transformer Layer Transformer Layer 1:2 Local Blocks 𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒 0 1 0 1 0 1 0 1 ⊕ Weight Update Weight Update Weight Update Weight Update Δ𝜙4 ⋅𝔪4 Δ𝜙3 ⋅𝔪3 Δ𝜙2 ⋅𝔪2 Δ𝜙1 ⋅𝔪1 Retained Layer Retained Layer 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽, 𝖒) Confident Sampling ⇒ Good solution identified 1:2 Local Blocks 𝔪1 𝔪2 𝔪3 𝔪4 ⊕ ∼ Mixed Sampling ⇒ Exploration still in Progress Diff.
[064] 变压器层变压器层变压器层变压器层1：2局部块𝝓𝟏 0 1 0 1 0 1 0 1 0 1 0 1⊕重量更新重量更新重量更新重量更新重量更新重量更新重量更新Δ4Δ3Δ3Δ2Δ2Δ2δ𝜙2Δ𝜙1Δ𝜙1µ1·𝔪1·𝔪1·𝔪1µ1保留层保留层，良好的层识别层（良好的解决方案），范围2，𝚫𝚽𝓛，𝚫𝚽𝓛，𝒙+ 𝚫𝚽+ 𝚫𝚽，𝚽+ 𝚫𝚽，𝖒+ 𝚫𝚽，Sampling，Sampl complative。 𝔪1𝔪2𝔪3𝔪4⊕4〜混合采样⇒探索仍在进行中。

--------------------------------------------------

[065] Sampling Learnable Distribution ∼ Diff.
[065] 抽样可学习的分布〜差异。

--------------------------------------------------

[066] Sampling Figure 2.
[066] 采样图2。

--------------------------------------------------

[067] The proposed TinyFusion method learns to perform a differentiable sampling of candidate solutions, jointly optimized with a weight update to estimate recoverability.
[067] 拟议的TinyFusion方法学会了对候选解决方案进行可区分的采样，共同优化了重量更新以估算可恢复性。

--------------------------------------------------

[068] This approach aims to increase the likelihood of favorable solutions that ensure strong post-fine- tuning performance.
[068] 这种方法旨在增加有利解决方案的可能性，从而确保强大的结束后表现。

--------------------------------------------------

[069] After training, local structures with the highest sampling probabilities are retained.
[069] 训练后，保留了采样概率最高的本地结构。

--------------------------------------------------

[070] caching [35, 57], etc.
[070] 缓存[35，57]，等。

--------------------------------------------------

[071] In this work, we focus on compress- ing the depth of pre-trained diffusion transformers and in- troduce a learnable method that directly optimizes recover- ability, which is able to achieve satisfactory results with low re-training costs.
[071] 在这项工作中，我们专注于压缩预训练的扩散变压器的深度，并赋予一种可学习的方法，该方法可以直接优化恢复能力，该方法能够通过低重新训练成本获得令人满意的结果。

--------------------------------------------------

[072] 3.
[072] 3。

--------------------------------------------------

[073] Method 3.1.
[073] 方法3.1。

--------------------------------------------------

[074] Shallow Generative Transformers by Pruning This work aims to derive a shallow diffusion transformer by pruning a pre-trained model.
[074] 通过修剪这项工作，浅层生成变压器旨在通过修剪预训练的模型来得出浅扩散变压器。

--------------------------------------------------

[075] For simplicity, all vectors in this paper are column vectors.
[075] 为简单起见，本文中的所有向量都是列向量。

--------------------------------------------------

[076] Consider a L-layer trans- former, parameterized by ΦL×D = [ϕ1, ϕ2, · · · , ϕL]⊺, where each element ϕi encompasses all learnable param- eters of a transformer layer as a D-dim column vector, which includes the weights of both attention layers and MLPs.
[076] 考虑一个由L层trans-前者，由φl×d = [ϕ1，ϕ2，·，ϕL]⊺进行参数，其中每个元素ϕi包含变压器层的所有可学习的参数作为D-DIM柱向量，其中包括注意层和MLP的重量。

--------------------------------------------------

[077] Depth pruning seeks to find a binary layer mask mL×1 = [m1, m2, · · · , mL]⊺, that removes a layer by: xi+1 = miϕi(xi) + (1 −mi)xi = ( ϕi(xi), if mi = 1, xi, otherwise, (1) where the xi and ϕi(xi) refers to the input and output of layer ϕi.
[077] 深度修剪试图找到二进制掩码ml×1 = [m1，m2，··级，ml]⊺，该层通过：xi + 1 = miDartice（xi） +（1 -mi）xi =（ϕi（xi）=（ϕi（xi（xi（xi），如果mi = 1，xi），xi = 1，xi，（xi），xi和xi yly（xi），（xi）和xi liver，（xi）codi（xi）和xi（xi）codi（xi）和（xi liver，（1） ϕi。

--------------------------------------------------

[078] To obtain the mask, a common paradigm in prior work is to minimize the loss L after pruning, which can be formulated as minm Ex [L(x, Φ, m)].
[078] 为了获得掩模，先前工作中的一个常见范式是将修剪后的损耗l最小化，可以将其作为minm ex [l（x，φ，m）]配制。

--------------------------------------------------

[079] However, as we will show in the experiments, this objective – though widely adopted in discriminative tasks – may not be well-suited to pruning diffusion transformers.
[079] 但是，正如我们将在实验中显示的那样，尽管在判别任务中广泛采用了这个目标，但可能不适合修剪扩散变压器。

--------------------------------------------------

[080] Instead, we are more inter- ested in the recoverability of pruned models.
[080] 取而代之的是，我们对修剪模型的可恢复性更加感兴趣。

--------------------------------------------------

[081] To achieve this, we incorporate an additional weight update into the optimization problem and extend the objective by: min m min ∆Φ Ex [L(x, Φ + ∆Φ, m)] | {z } Recoverability: Post-Fine-Tuning Performance , (2) where ∆Φ = {∆ϕ1, ∆ϕ2, · · · , ∆ϕM} represents appro- priate update from fine-tuning.
[081] 为了实现这一目标，我们将额外的权重更新纳入了优化问题，并扩展了目标：min m min ∆φ ex [l（x，φ + ∆φ，m）] | {z}可恢复性：恢复性能后的性能，（2）其中∆φ = {∆ ϕ1，∆ ϕ2，···，∆ ϕm}表示来自微调的适当更新。

--------------------------------------------------

[082] The objective formulated by Equation 2 poses two challenges: 1) The non-differentiable nature of layer selection prevents direct optimization us- ing gradient descent; 2) The inner optimization over the retained layers makes it computationally intractable to ex- plore the entire search space, as this process necessitates se- lecting a candidate model and fine-tuning it for evaluation.
[082] 公式2提出的目标提出了两个挑战：1）层选择的非差异性质阻止了直接优化的梯度下降； 2）在保留层上的内部优化使得在计算上棘手可以阐明整个搜索空间，因为此过程需要将候选模型列为候选模型并进行微调以进行评估。

--------------------------------------------------

[083] To address this, we propose TinyFusion that makes both the pruning and recoverability optimizable.
[083] 为了解决这个问题，我们提出了使修剪和可恢复性优化的微小灌注。

--------------------------------------------------

[084] 3.2.
[084] 3.2。

--------------------------------------------------

[085] TinyFusion: Learnable Depth Pruning A Probabilistic Perspective.
[085] 小型灌注：可学习的深度修剪概率的观点。

--------------------------------------------------

[086] This work models Equa- tion 2 from a probabilistic standpoint.
[086] 这项工作从概率的角度模拟了方程2。

--------------------------------------------------

[087] We hypothesize that the mask m produced by “ideal” pruning methods (might be not unique) should follow a certain distribution.
[087] 我们假设由“理想”修剪方法（可能不是唯一的）产生的面膜应遵循一定的分布。

--------------------------------------------------

[088] To model this, it is intuitive to associate every possible mask m with a probability value p(m), thus forming a categori- cal distribution.
[088] 为了对此进行建模，将每个可能的掩码M与概率值P（M）相关联是直观的，从而形成分类分布。

--------------------------------------------------

[089] Without any prior knowledge, the assess- ment of pruning masks begins with a uniform distribution.
[089] 没有任何先验知识，修剪口罩的评估始于均匀的分布。

--------------------------------------------------

[090] However, directly sampling from this initial distribution is highly inefficient due to the vast search space.
[090] 但是，由于庞大的搜索空间，该初始分布直接进行采样效率很高。

--------------------------------------------------

[091] For in- stance, pruning a 28-layer model by 50% involves evalu- ating 