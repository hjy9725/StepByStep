[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wang† National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization, resulting in considerable infer- ence overhead in real-world applications.
[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wang† National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization,导致在现实世界应用中大量的推论开销。

--------------------------------------------------

[002] In this work, we present TinyFusion, a depth pruning method designed to re- move redundant layers from diffusion transformers via end- to-end learning.
[002] 在这项工作中，我们提出了TinyFusion，这是一种深度修剪方法，旨在通过最终学习将冗余层从扩散变压器中移动。

--------------------------------------------------

[003] The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning.
[003] 我们方法的核心原理是创建具有高可恢复性的修剪模型，从而使其在微调后恢复了强劲的性能。

--------------------------------------------------

[004] To accom- plish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized pa- rameter to simulate future fine-tuning.
[004] 为了实现这一目标，我们引入了一种可区分的抽样技术，以使修剪可学习，并与合作的Pa-Rameter配对，以模拟未来的微调。

--------------------------------------------------

[005] While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning perfor- mance of pruned models.
[005] 虽然先前的工作重点是最大程度地减少修剪后的损失或错误，但我们的方法显式模型并优化了修剪模型的预先调整后调节性能。

--------------------------------------------------

[006] Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing exist- ing importance-based and error-based methods.
[006] 实验结果表明，这种可学习的范式为扩散变压器的层修剪提供了可观的好处，超过了基于重要性和基于错误的方法的存在。

--------------------------------------------------

[007] Addition- ally, TinyFusion exhibits strong generalization across di- verse architectures, such as DiTs, MARs, and SiTs.
[007] 补充说，微小的灌注表现出跨个性体系结构（例如DIT，MARS和SITS）的强烈概括。

--------------------------------------------------

[008] Ex- periments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre- training cost, achieving a 2× speedup with an FID score of 2.86, outperforming competitors with comparable effi- ciency.
[008] DIT-XL的概念表明，TinyFusion可以以不到预训练成本的7％的7％来制作浅扩散变压器，以2.86的FID得分达到2×加速，优于竞争对手，表现优于相当的效率。

--------------------------------------------------

[009] Code is available at https://github.com/ VainF/TinyFusion 1.
[009] 代码可在https://github.com/ vainf/tinyfusion 1中找到。

--------------------------------------------------

[010] Introduction Diffusion Transformers have emerged as a cornerstone ar- chitecture for generative tasks, achieving notable success in areas such as image [11, 26, 40] and video synthe- sis [25, 59].
[010] 引言扩散变压器已成为生成任务的基石 - 在图像[11、26、40]和视频综合[25、59]等领域取得了显着成功。

--------------------------------------------------

[011] This success has also led to the widespread availability of high-quality pre-trained models on the Inter- net, greatly accelerating and supporting the development of various downstream applications [5, 16, 53, 55].
[011] 这一成功还导致了互联网上高质量预训练的模型的广泛可用性，极大地加速和支持了各种下游应用程序的开发[5，16，53，55]。

--------------------------------------------------

[012] However, pre-trained diffusion transformers usually come with con- *Equal contribution †Corresponding author Transformer Layer Transformer Layer Transformer Layer Transformer Layer Differentiable Sampling of Layer Mask 𝖒  Recoverability Estimation with 𝚫𝚽  1 0 1 0 Local Block Joint Opt.
[012] 但是，预训练的扩散变压器通常具有相等的贡献†相应的作者变压器层变压器层变压器层变压器层层层掩模的可恢复性估计的可恢复性估计，𝚫𝚽1 0 1 0局部块接头选择。

--------------------------------------------------

[013] Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer 𝚫𝚽  (LoRA/Full) 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽, 𝖒) 𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒 Figure 1.
[013] 变压器层变压器层变压器层变压器层变压器层变压器层𝚫𝚽（lora/full）𝐦𝐢𝐧𝖒，𝚫𝚽𝓛（𝒙，𝚽+ 𝚫𝚽，𝖒，𝖒）𝝓𝟏 𝝓𝟏 𝝓𝟑 𝝓𝟑图1。

--------------------------------------------------

[014] This work presents a learnable approach for pruning the depth of pre-trained diffusion transformers.
[014] 这项工作提出了一种可学习的方法，用于修剪预训练的扩散变压器的深度。

--------------------------------------------------

[015] Our method simulta- neously optimizes a differentiable sampling process of layer masks and a weight update to identify a highly recoverable solution, en- suring that the pruned model maintains competitive performance after fine-tuning.
[015] 我们的方法同时优化了层掩模的可区分采样过程和重量更新，以识别高度可恢复的解决方案，并确保修剪模型在微调后保持竞争性能。

--------------------------------------------------

[016] siderable inference costs due to the huge parameter scale, which poses significant challenges for deployment.
[016] 由于巨大的参数量表而导致的推理成本很大，这给部署带来了重大挑战。

--------------------------------------------------

[017] To re- solve this problem, there has been growing interest from both the research community and industry in developing lightweight models [12, 23, 32, 58].
[017] 为了解决这个问题，研究社区和行业对开发轻质模型的兴趣越来越大[12，23，32，58]。

--------------------------------------------------

[018] The efficiency of diffusion models is typically influ- enced by various factors, including the number of sampling steps [33, 43, 45, 46], operator design [7, 48, 52], compu- tational precision [19, 30, 44], network width [3, 12] and depth [6, 23, 36].
[018] 扩散模型的效率通常受到各种因素的影响，包括采样步骤的数量[33，43，45，46]，操作员设计[7，48，52]，组合精度[19，30，44]，网络宽度[3，12] [3，12]和深度[6，23，36]。

--------------------------------------------------

[019] In this work, we focus on model compres- sion through depth pruning [36, 54], which removes entire layers from the network to reduce the latency.
[019] 在这项工作中，我们通过深度修剪[36，54]专注于模型的组合，该模型从网络中删除了整个层以减少延迟。

--------------------------------------------------

[020] Depth prun- ing offers a significant advantage in practice: it can achieve a linear acceleration ratio relative to the compression rate on both parallel and non-parallel devices.
[020] 深度修剪在实践中提供了重要的优势：相对于并行设备和非平行设备的压缩率，它可以达到线性加速度比率。

--------------------------------------------------

[021] For example, as will be demonstrated in this work, while 50% width prun- ing [12] only yields a 1.6× speedup, pruning 50% of the layers results in a 2× speedup.
[021] 例如，正如这项工作所证明的那样，宽度为50％的宽度[12]仅产生1.6倍的加速，修剪50％的层导致2倍加速。

--------------------------------------------------

[022] This makes depth pruning a flexible and practical method for model compression.
[022] 这使得深度修剪了模型压缩的灵活且实用的方法。

--------------------------------------------------

[023] This work follows a standard depth pruning frame- work: unimportant layers are first removed, and the pruned model is then fine-tuned for performance recovery.
[023] 这项工作遵循标准的深度修剪框架 - 首先要删除不重要的层，然后对修剪模型进行微调以进行性能恢复。

--------------------------------------------------

[024] In the literature, depth pruning techniques designed for dif- fusion transformers or general transformers primarily fo- cus on heuristic approaches, such as carefully designed importance scores [6, 36] or manually configured pruning 1 arXiv:2412.01199v1  [cs.CV]  2 Dec 2024
[024] 在文献中，设计用于差异变压器或通用变压器设计的深度修剪技术主要是基于启发式方法，例如精心设计的重要性得分[6，36]或手动配置的修剪1 arxiv：2412.011999v1 [cs.cv] [cs.cv] [cs.cv] 2024年12月2日2024年12月2日

--------------------------------------------------

[025] schemes [23, 54].
[025] 方案[23，54]。

--------------------------------------------------

[026] These methods adhere to a loss min- imization principle [18, 37], aiming to identify solutions that maintain low loss or error after pruning.
[026] 这些方法遵守损失最小原则[18，37]，旨在识别在修剪后保持低损失或错误的解决方案。

--------------------------------------------------

[027] This paper investigates the effectiveness of this widely used principle in the context of depth compression.
[027] 本文在深度压缩的背景下研究了该广泛使用原理的有效性。

--------------------------------------------------

[028] Through experiments, we examined the relationship between calibration loss ob- served post-pruning and the performance after fine-tuning.
[028] 通过实验，我们检查了校准损失渗透后的后延期与微调后的性能之间的关系。

--------------------------------------------------

[029] This is achieved by extensively sampling 100,000 models via random pruning, exhibiting different levels of calibra- tion loss in the searching space.
[029] 这是通过通过随机修剪进行广泛采样100,000款模型来实现的，在搜索空间中表现出不同水平的碳纤维损失。

--------------------------------------------------

[030] Based on this, we analyzed the effectiveness of existing pruning algorithms, such as the feature similarity [6, 36] and sensitivity analysis [18], which indeed achieve low calibration losses in the solution space.
[030] 基于此，我们分析了现有修剪算法的有效性，例如特征相似性[6，36]和灵敏度分析[18]，它们确实在解决方案空间中实现了低校准损失。

--------------------------------------------------

[031] However, the performance of all these models after fine- tuning often falls short of expectations.
[031] 但是，精细调整后所有这些模型的性能通常都没有期望。

--------------------------------------------------

[032] This indicates that the loss minimization principle may not be well-suited for diffusion transformers.
[032] 这表明最小化原理可能不适合扩散变压器。

--------------------------------------------------

[033] Building on these insights, we reassessed the underly- ing principles for effective layer pruning in diffusion trans- formers.
[033] 在这些见解的基础上，我们重新评估了在扩散式传输中修剪有效层的基本原理。

--------------------------------------------------

[034] Fine-tuning diffusion transformers is an extremely time-consuming process.
[034] 微调扩散变压器是一个非常耗时的过程。

--------------------------------------------------

[035] Instead of searching for a model that minimizes loss immediately after pruning, we propose identifying candidate models with strong recoverability, en- abling superior post-fine-tuning performance.
[035] 我们没有在修剪后立即寻找将损失立即最小化的模型，而是提议识别具有强大可恢复性，优势较高后调节性能的候选模型。

--------------------------------------------------

[036] Achieving this goal is particularly challenging, as it requires the in- tegration of two distinct processes, pruning and fine-tuning, which involve non-differentiable operations and cannot be directly optimized via gradient descent.
[036] 实现这一目标特别具有挑战性，因为它需要对两个不同的过程进行修剪和微调，这涉及非不同的操作，并且不能通过梯度下降直接优化。

--------------------------------------------------

[037] To this end, we propose a learnable depth pruning method that effectively integrates pruning and fine-tuning.
[037] 为此，我们提出了一种可学习的深度修剪方法，可以有效整合修剪和微调。

--------------------------------------------------

[038] As shown in Figure 1, we model the pruning and fine- tuning of a diffusion transformer as a differentiable sam- pling process of layer masks [13, 17, 22], combined with a co-optimized weight update to simulate future fine-tuning.
[038] 如图1所示，我们将扩散变压器的修剪和精细调整为层掩模的可区分的Sam固定过程[13，17，22]，并结合了合作的权重更新，以模拟未来的微调。

--------------------------------------------------

[039] Our objective is to iteratively refine this distribution so that networks with higher recoverability are more likely to be sampled.
[039] 我们的目标是迭代地完善此分布，以便更有可能采样具有较高可恢复性的网络。

--------------------------------------------------

[040] This is achieved through a straightforward strat- egy: if a sampled pruning decision results in strong recover- ability, similar pruning patterns will have an increased prob- ability of being sampled.
[040] 这是通过直接的策略来实现的：如果采样修剪决策会导致强大的恢复能力，那么相似的修剪模式将具有提高采样的概率能力。

--------------------------------------------------

[041] This approach promotes the ex- ploration of potentially valuable solutions while disregard- ing less effective ones.
[041] 这种方法促进了对潜在有价值的解决方案的提出，同时又无视效率较低的解决方案。

--------------------------------------------------

[042] Additionally, the proposed method is highly efficient, and we demonstrate that a suitable solu- tion can emerge within a few training steps.
[042] 此外，所提出的方法非常有效，我们证明可以在几个训练步骤中出现合适的解决方案。

--------------------------------------------------

[043] To evaluate the effectiveness of the proposed method, we conduct extensive experiments on various transformer- based diffusion models, including DiTs [40], MARs [29], SiTs [34].
[043] 为了评估所提出方法的有效性，我们对基于变压器的扩散模型进行了广泛的实验，包括DITS [40]，MARS [29]，位于[34]。

--------------------------------------------------

[044] The learnable approach is highly efficient.
[044] 可学习的方法非常有效。

--------------------------------------------------

[045] It is able to identify redundant layers in diffusion transform- ers with 1-epoch training on the dataset, which effectively crafts shallow diffusion transformers from pre-trained mod- els with high recoverability.
[045] 它能够通过数据集上的1个上述训练来识别扩散变换中的冗余层，从而有效地从具有高可恢复性的预训练的模型中制作了浅扩散变压器。

--------------------------------------------------

[046] For instance, while the models pruned by TinyFusion initially exhibit relatively high cal- ibration loss after removing 50% of layers, they recover quickly through fine-tuning, achieving a significantly more competitive FID score (5.73 vs. 22.28) compared to base- line methods that only minimize immediate loss, using just 1% of the pre-training cost.
[046] 例如，尽管删除50％的层后，最初被TinyFusion修剪的模型最初表现出相对较高的cal损失，但与仅使用预先培训的1％的基本方法相比，它们通过微调得分迅速恢复，获得了更具竞争力的FID得分（5.73 vs 22.28）（5.73对22.28）。

--------------------------------------------------

[047] Additionally, we also ex- plore the role of knowledge distillation in enhancing re- coverability [20, 23] by introducing a MaskedKD variant.
[047] 此外，我们还通过引入maskedkd变体来表达知识蒸馏在增强可覆盖性[20，23]中的作用。

--------------------------------------------------

[048] MaskedKD mitigates the negative impact of the massive or outlier activations [47] in hidden states, which can signifi- cantly affect the performance and reliability of fine-tuning.
[048] MaskEDKD减轻了隐藏状态中大规模或异常激活的负面影响[47]，这可能会显着影响微调的性能和可靠性。

--------------------------------------------------

[049] With MaskedKD, the FID score improves from 5.73 to 3.73 with only 1% of pre-training cost.
[049] 借助MaskedKD，FID得分从5.73提高到3.73，仅占培训前成本的1％。

--------------------------------------------------

[050] Extending the training to 7% of the pre-training cost further reduces the FID to 2.86, just 0.4 higher than the original model with doubled depth.
[050] 将培训扩大到7％的培训前成本将FID进一步降低到2.86，仅比原始模型高度增加了0.4。

--------------------------------------------------

[051] Therefore, the main contribution of this work lies in a learnable method to craft shallow diffusion transformers from pre-trained ones, which explicitly optimizes the re- coverability of pruned models.
[051] 因此，这项工作的主要贡献在于一种可学习的方法，可以从预训练的方法中制作浅扩散变压器，该方法明确优化了修剪模型的可覆盖性。

--------------------------------------------------

[052] The method is general for various architectures, including DiTs, MARs and SiTs.
[052] 该方法是各种架构，包括DIT，MARS和SITS的一般方法。

--------------------------------------------------

[053] 2.
[053] 2。

--------------------------------------------------

[054] Related Works Network Pruning and Depth Reduction.
[054] 相关工作网络修剪和深度减少。

--------------------------------------------------

[055] Network prun- ing is a widely used approach for compressing pre-trained diffusion models by eliminating redundant parameters [3, 12, 31, 51].
[055] 网络修剪是一种通过消除冗余参数来压缩预训练扩散模型的广泛使用方法[3，12，31，51]。

--------------------------------------------------

[056] Diff-Pruning [12] introduces a gradient- based technique to streamline the width of UNet, fol- lowed by a simple fine-tuning to recover the performance.
[056] DIFF-PRUNING [12]引入了一种基于梯度的技术，以简化UNET的宽度，以通过简单的微调来恢复性能。

--------------------------------------------------

[057] SparseDM [51] applies sparsity to pre-trained diffusion models via the Straight-Through Estimator (STE) [2], achieving a 50% reduction in MACs with only a 1.22 in- crease in FID on average.
[057] Sparsedm [51]通过直通估计量（Ste）[2]将稀疏性应用于预训练的扩散模型，在MAC中降低了50％，平均FID仅1.22个折痕。

--------------------------------------------------

[058] While width pruning and spar- sity help reduce memory overhead, they often offer lim- ited speed improvements, especially on parallel devices like GPUs.
[058] 虽然修剪和宽度有助于减少内存开销，但它们通常会提供限制的速度提高，尤其是在诸如GPU之类的平行设备上。

--------------------------------------------------

[059] Consequently, depth reduction has gained signifi- cant attention in the past few years, as removing entire lay- ers enables better speedup proportional to the pruning ra- tio [24, 27, 28, 36, 54, 56, 58].
[059] 因此，在过去的几年中，深度的降低引起了显着关注，因为删除整个外行可以更好地加速与修剪ra-tio成正比[24、27、27、28、36、54、56、58]。

--------------------------------------------------

[060] Adaptive depth reduction techniques, such as MoD [41] and depth-aware transform- ers [10], have also been proposed.
[060] 还提出了自适应深度还原技术，例如MOD [41]和深度感知的转化[10]。

--------------------------------------------------

[061] Despite these advances, most existing methods are still based on empirical or heuris- tic strategies, such as carefully designed importance crite- ria [36, 54], sensitivity analyses [18] or manually designed schemes [23], which often do not yield strong performance guarantee after fine-tuning.
[061] 尽管取得了这些进步，但大多数现有方法仍然基于经验或启发式策略，例如精心设计的重要性迹象[36，54]，敏感性分析[18]或手动设计的方案[23]，这些方案通常不会在微调后产生强大的性能保证。

--------------------------------------------------

[062] Efficient Diffusion Transformers.
[062] 有效的扩散变压器。

--------------------------------------------------

[063] Developing efficient diffusion transformers has become an appealing focus within the community, where significant efforts have been made to enhance efficiency from various perspectives, in- cluding linear attention mechanisms [15, 48, 52], compact architectures [50], non-autoregressive transformers [4, 14, 38, 49], pruning [12, 23], quantization [19, 30, 44], feature 2
[063] 开发有效的扩散变压器已成为社区中的一个吸引人的重点，在各种角度，已经做出了巨大的努力来提高效率，包括线性注意机制[15，48，52]，紧凑型建筑[50]，非自动性变压器[4，14，38，49]，pruning [4,14,38,49]，pruns [12，23]，量化[19，30，30，44]

--------------------------------------------------

[064] Transformer Layer Transformer Layer Transformer Layer Transformer Layer 1:2 Local Blocks 𝝓𝟏 𝝓𝟐 𝝓𝟑 𝝓𝟒 0 1 0 1 0 1 0 1 ⊕ Weight Update Weight Update Weight Update Weight Update Δ𝜙4 ⋅𝔪4 Δ𝜙3 ⋅𝔪3 Δ𝜙2 ⋅𝔪2 Δ𝜙1 ⋅𝔪1 Retained Layer Retained Layer 𝐦𝐢𝐧𝖒,𝚫𝚽𝓛(𝒙, 𝚽+ 𝚫𝚽, 𝖒) Confident Sampling ⇒ Good solution identified 1:2 Local Blocks 𝔪1 𝔪2 𝔪3 𝔪4 ⊕ ∼ Mixed Sampling ⇒ Exploration still in Progress Diff.
[064] 变压器层变压器层变压器层变压器层1：2局部块𝝓𝟏 0 1 0 1 0 1 0 1 0 1 0 1⊕重量更新重量更新重量更新重量更新重量更新重量更新重量更新Δ4Δ3Δ3Δ2Δ2Δ2δ𝜙2Δ𝜙1Δ𝜙1µ1·𝔪1·𝔪1·𝔪1µ1保留层保留层，良好的层识别层（良好的解决方案），范围2，𝚫𝚽𝓛，𝚫𝚽𝓛，𝒙+ 𝚫𝚽+ 𝚫𝚽，𝚽+ 𝚫𝚽，𝖒+ 𝚫𝚽，Sampling，Sampl complative。 𝔪1𝔪2𝔪3𝔪4⊕4〜混合采样⇒探索仍在进行中。

--------------------------------------------------

[065] Sampling Learnable Distribution ∼ Diff.
[065] 抽样可学习的分布〜差异。

--------------------------------------------------

[066] Sampling Figure 2.
[066] 采样图2。

--------------------------------------------------

[067] The proposed TinyFusion method learns to perform a differentiable sampling of candidate solutions, jointly optimized with a weight update to estimate recoverability.
[067] 拟议的TinyFusion方法学会了对候选解决方案进行可区分的采样，共同优化了重量更新以估算可恢复性。

--------------------------------------------------

[068] This approach aims to increase the likelihood of favorable solutions that ensure strong post-fine- tuning performance.
[068] 这种方法旨在增加有利解决方案的可能性，从而确保强大的结束后表现。

--------------------------------------------------

[069] After training, local structures with the highest sampling probabilities are retained.
[069] 训练后，保留了采样概率最高的本地结构。

--------------------------------------------------

[070] caching [35, 57], etc.
[070] 缓存[35，57]，等。

--------------------------------------------------

[071] In this work, we focus on compress- ing the depth of pre-trained diffusion transformers and in- troduce a learnable method that directly optimizes recover- ability, which is able to achieve satisfactory results with low re-training costs.
[071] 在这项工作中，我们专注于压缩预训练的扩散变压器的深度，并赋予一种可学习的方法，该方法可以直接优化恢复能力，该方法能够通过低重新训练成本获得令人满意的结果。

--------------------------------------------------

[072] 3.
[072] 3。

--------------------------------------------------

[073] Method 3.1.
[073] 方法3.1。

--------------------------------------------------

[074] Shallow Generative Transformers by Pruning This work aims to derive a shallow diffusion transformer by pruning a pre-trained model.
[074] 通过修剪这项工作，浅层生成变压器旨在通过修剪预训练的模型来得出浅扩散变压器。

--------------------------------------------------

[075] For simplicity, all vectors in this paper are column vectors.
[075] 为简单起见，本文中的所有向量都是列向量。

--------------------------------------------------

[076] Consider a L-layer trans- former, parameterized by ΦL×D = [ϕ1, ϕ2, · · · , ϕL]⊺, where each element ϕi encompasses all learnable param- eters of a transformer layer as a D-dim column vector, which includes the weights of both attention layers and MLPs.
[076] 考虑一个由L层trans-前者，由φl×d = [ϕ1，ϕ2，·，ϕL]⊺进行参数，其中每个元素ϕi包含变压器层的所有可学习的参数作为D-DIM柱向量，其中包括注意层和MLP的重量。

--------------------------------------------------

[077] Depth pruning seeks to find a binary layer mask mL×1 = [m1, m2, · · · , mL]⊺, that removes a layer by: xi+1 = miϕi(xi) + (1 −mi)xi = ( ϕi(xi), if mi = 1, xi, otherwise, (1) where the xi and ϕi(xi) refers to the input and output of layer ϕi.
[077] 深度修剪试图找到二进制掩码ml×1 = [m1，m2，··级，ml]⊺，该层通过：xi + 1 = miDartice（xi） +（1 -mi）xi =（ϕi（xi）=（ϕi（xi（xi（xi），如果mi = 1，xi），xi = 1，xi，（xi），xi和xi yly（xi），（xi）和xi liver，（xi）codi（xi）和xi（xi）codi（xi）和（xi liver，（1） ϕi。

--------------------------------------------------

[078] To obtain the mask, a common paradigm in prior work is to minimize the loss L after pruning, which can be formulated as minm Ex [L(x, Φ, m)].
[078] 为了获得掩模，先前工作中的一个常见范式是将修剪后的损耗l最小化，可以将其作为minm ex [l（x，φ，m）]配制。

--------------------------------------------------

[079] However, as we will show in the experiments, this objective – though widely adopted in discriminative tasks – may not be well-suited to pruning diffusion transformers.
[079] 但是，正如我们将在实验中显示的那样，尽管在判别任务中广泛采用了这个目标，但可能不适合修剪扩散变压器。

--------------------------------------------------

[080] Instead, we are more inter- ested in the recoverability of pruned models.
[080] 取而代之的是，我们对修剪模型的可恢复性更加感兴趣。

--------------------------------------------------

[081] To achieve this, we incorporate an additional weight update into the optimization problem and extend the objective by: min m min ∆Φ Ex [L(x, Φ + ∆Φ, m)] | {z } Recoverability: Post-Fine-Tuning Performance , (2) where ∆Φ = {∆ϕ1, ∆ϕ2, · · · , ∆ϕM} represents appro- priate update from fine-tuning.
[081] 为了实现这一目标，我们将额外的权重更新纳入了优化问题，并扩展了目标：min m min ∆φ ex [l（x，φ + ∆φ，m）] | {z}可恢复性：恢复性能后的性能，（2）其中∆φ = {∆ ϕ1，∆ ϕ2，···，∆ ϕm}表示来自微调的适当更新。

--------------------------------------------------

[082] The objective formulated by Equation 2 poses two challenges: 1) The non-differentiable nature of layer selection prevents direct optimization us- ing gradient descent; 2) The inner optimization over the retained layers makes it computationally intractable to ex- plore the entire search space, as this process necessitates se- lecting a candidate model and fine-tuning it for evaluation.
[082] 公式2提出的目标提出了两个挑战：1）层选择的非差异性质阻止了直接优化的梯度下降； 2）在保留层上的内部优化使得在计算上棘手可以阐明整个搜索空间，因为此过程需要将候选模型列为候选模型并进行微调以进行评估。

--------------------------------------------------

[083] To address this, we propose TinyFusion that makes both the pruning and recoverability optimizable.
[083] 为了解决这个问题，我们提出了使修剪和可恢复性优化的微小灌注。

--------------------------------------------------

[084] 3.2.
[084] 3.2。

--------------------------------------------------

[085] TinyFusion: Learnable Depth Pruning A Probabilistic Perspective.
[085] 小型灌注：可学习的深度修剪概率的观点。

--------------------------------------------------

[086] This work models Equa- tion 2 from a probabilistic standpoint.
[086] 这项工作从概率的角度模拟了方程2。

--------------------------------------------------

[087] We hypothesize that the mask m produced by “ideal” pruning methods (might be not unique) should follow a certain distribution.
[087] 我们假设由“理想”修剪方法（可能不是唯一的）产生的面膜应遵循一定的分布。

--------------------------------------------------

[088] To model this, it is intuitive to associate every possible mask m with a probability value p(m), thus forming a categori- cal distribution.
[088] 为了对此进行建模，将每个可能的掩码M与概率值P（M）相关联是直观的，从而形成分类分布。

--------------------------------------------------

[089] Without any prior knowledge, the assess- ment of pruning masks begins with a uniform distribution.
[089] 没有任何先验知识，修剪口罩的评估始于均匀的分布。

--------------------------------------------------

[090] However, directly sampling from this initial distribution is highly inefficient due to the vast search space.
[090] 但是，由于庞大的搜索空间，该初始分布直接进行采样效率很高。

--------------------------------------------------

[091] For in- stance, pruning a 28-layer model by 50% involves evalu- ating  28 14  = 40, 116, 600 possible solutions.
[091] 对于说明，将28层模型（50％）修剪为评估28 14 = 40、116、600可能的解决方案。

--------------------------------------------------

[092] To overcome this challenge, this work introduces an advanced and learn- able algorithm capable of using evaluation results as feed- back to iteratively refine the mask distribution.
[092] 为了克服这一挑战，这项工作引入了一种能够使用评估结果作为回馈的高级且具有学习算法，以返回迭代的掩码分布。

--------------------------------------------------

[093] The basic idea is that if certain masks exhibit positive results, then other masks with similar pattern may also be potential so- lutions and thus should have a higher likelihood of sam- pling in subsequent evaluations, allowing for a more fo- cused search on promising solutions.
[093] 基本思想是，如果某些掩膜表现出积极的结果，那么其他模式相似的掩膜也可能是潜在的，因此在随后的评估中应该具有更高的可能性，从而可以对有希望的解决方案进行更多的搜索。

--------------------------------------------------

[094] However, the defi- nition of “similarity pattern” is still unclear so far.
[094] 但是，到目前为止，“相似性模式”的定义尚不清楚。

--------------------------------------------------

[095] 3
[095] 3

--------------------------------------------------

[096] Sampling Local Structures.
[096] 采样本地结构。

--------------------------------------------------

[097] In this work, we demon- strate that local structures, as illustrated in Figure 2, can serve as effective anchors for modeling the relationships between different masks.
[097] 在这项工作中，我们表明，如图2所示，局部结构可以用作建模不同遮罩之间关系的有效锚点。

--------------------------------------------------

[098] If a pruning mask leads to cer- tain local structures and yields competitive results after fine- tuning, then other masks yielding the same local patterns are also likely to be positive solutions.
[098] 如果修剪掩模会导致局部结构，并在细调时产生竞争结果，那么其他掩模产生相同局部模式的掩模也可能是正溶液。

--------------------------------------------------

[099] This can be achieved by dividing the original model into K non-overlapping blocks, represented as Φ = [Φ1, Φ2, · · · , ΦK]⊺.
[099] 这可以通过将原始模型划分为k非重叠块（表示为φ= [φ1，φ2，·，φk]⊺的非重叠块。

--------------------------------------------------

[100] For simplicity, we assume each block Φk = [ϕk1, ϕk2, · · · , ϕkM]⊺contains exactly M layers, although they can have varied lengths.
[100] 为简单起见，我们假设每个块φk= [ϕk1，ϕk2，···，ϕkm]⊺恰好包含m层，尽管它们的长度可以变化。

--------------------------------------------------

[101] Instead of performing global layer pruning, we propose an N:M scheme for local layer pruning, where, for each block Φk with M layers, N layers are retained.
[101] 我们没有执行全局层修剪，而是提出了局部层修剪的N：M方案，在其中，对于带有M层的每个块φK，n层都保留。

--------------------------------------------------

[102] This results in a set of local binary masks m = [m1, m2, .
[102] 这导致一组本地二进制掩码M = [M1，M2，。

--------------------------------------------------

[103] .
[103] 。

--------------------------------------------------

[104] .
[104] 。

--------------------------------------------------

[105] , mK]⊺.
[105] ，Mk]⊺。

--------------------------------------------------

[106] Simi- larly, the distribution of a local mask mk is modeled using a categorical distribution p(mk).
[106] 同样，使用分类分布P（MK）对局部掩码MK的分布进行建模。

--------------------------------------------------

[107] We perform independent sampling of local binary masks and combine them for prun- ing, which presents the joint distribution: p(m) = p(m1) · p(m2) · · · p(mK) (3) If some local distributions p(mk) exhibit high confidence in the corresponding blocks, the system will tend to sam- ple those positive patterns frequently and keep active ex- plorations in other local blocks.
[107] 我们对局部二进制蒙版进行独立的抽样，并将它们结合起修剪，其中提出了联合分布：P（M）= P（M1）·P（M2）·P（M2）·P（Mk）P（MK）（MK）（MK）（MK）（MK）（MK）（MK）如果在相应的块中表现出很高的信心，则系统会倾向于Sam-ple sam-ple blocks starters starters store Ex-Plor，并保持其他积极的位置 - 并保持其他extrors-plor。

--------------------------------------------------

[108] Based on this concept, we introduce differential sampling to make the above process learnable.
[108] 基于这个概念，我们介绍了差异采样，以使上述过程可学习。

--------------------------------------------------

[109] Differentiable Sampling.
[109] 可区分的采样。

--------------------------------------------------

[110] Considering the sampling pro- cess of a local mask mk, which corresponds a local block Φk and is modeled by a categorical distribution p(mk).
[110] 考虑到局部掩码MK的采样过程，该掩膜与局部块φK相对应，并通过分类分布P（MK）进行建模。

--------------------------------------------------

[111] With the N:M scheme, there are  M N  possible masks.
[111] 使用N：M方案，有可能的掩模。

--------------------------------------------------

[112] We construct a special matrix ˆmN:M to enumerate all possi- ble masks.
[112] 我们构建一个特殊的矩阵ˆmn：m来枚举所有可能的面具。

--------------------------------------------------

[113] For example, 2:3 layer pruning will lead to the candidate matrix ˆm2:3 = [[1, 1, 0] , [1, 0, 1] , [0, 1, 1]].
[113] 例如，2：3层修剪将导致候选矩阵ˆm2：3 = [[1，1，0]，[1，0，1]，[0，1，1，1]]。

--------------------------------------------------

[114] In this case, each block will have three probabilities p(mk) = [pk1, pk2, pk3].
[114] 在这种情况下，每个块将具有三个概率P（MK）= [PK1，PK2，PK3]。

--------------------------------------------------

[115] For simplicity, we omit mk and k and use pi to represent the probability of sampling i-th element in ˆmN:M. A popular method to make a sampling process dif- ferentiable is Gumbel-Softmax [13, 17, 22]: y = one-hot   exp((gi + log pi)/τ) P j exp((gj + log pj)/τ) !
[115] 为简单起见，我们省略了MK和K，并使用PI表示在ˆmn：m中对第i-th元素进行采样的概率。使采样过程分化的一种流行方法是Gumbel-Softmax [13，17，22]：y =一hot exp（（（gi + log pi）/τ）p j exp（（gj + log pj）/τ）！

--------------------------------------------------

[116] .
[116] 。

--------------------------------------------------

[117] (4) where gi is random noise drawn from the Gumbel distribu- tion Gumbel(0, 1) and τ refers to the temperature term.
[117] （4）其中GI是随机噪声，它是从牙龈分布（0，1）中绘制的，τ是指温度项。

--------------------------------------------------

[118] The output y is the index of the sampled mask.
[118] 输出y是采样蒙版的索引。

--------------------------------------------------

[119] Here a Straight- Through Estimator [2] is applied to the one-hot operation, where the onehot operation is enabled during forward and is treated as an identity function during backward.
[119] 在这里，直接通过估计器[2]应用于单速操作，该操作在向前时启用了orhot操作，并将其视为在向后时作为身份函数。

--------------------------------------------------

[120] Leverag- ing the one-hot index y and the candidate set ˆmN:M, we can draw a mask m ∼p(m) through a simple index operation: m = y⊺ˆm (5) Pretrained 𝑊 A r Identity f(x)=x ⨂ 𝑚𝑖 ⨂ + (1 −𝑚𝑖) 𝑥𝑖 𝑥𝑖+1 B 𝑁× Figure 3.
[120] 杠杆率单高索引y和候选组集ˆ mn：m，我们可以通过简单的索引操作绘制蒙版m〜p（m）：m = y⊺ˆm（5）预处理的a a a a a a a a r同一性f（x）= x⨂ +（1 −1-𝑚𝑖 +（1-𝑚𝑖 + 1 b + 1 b𝑁 +）。

--------------------------------------------------

[121] An example of forward propagation with differentiable pruning mask mi and LoRA for recoverability estimation.
[121] 带有可恢复性估算的可恢复性蒙版MI和LORA的正向传播的一个例子。

--------------------------------------------------

[122] Notably, when τ →0, the STE gradients will approximate the true gradients, yet with a higher variance which is neg- ative for training [22].
[122] 值得注意的是，当τ→0时，Ste梯度将近似真正的梯度，但具有更高的方差，对训练的差异很高[22]。

--------------------------------------------------

[123] Thus, a scheduler is typically em- ployed to initiate training with a high temperature, gradu- ally reducing it over time.
[123] 因此，调度程序通常会以高温启动训练，随着时间的流逝而逐渐减少。

--------------------------------------------------

[124] Joint Optimization with Recoverability.
[124] 联合优化，可恢复性。

--------------------------------------------------

[125] With differen- tiable sampling, we are able to update the underlying prob- ability using gradient descent.
[125] 通过不同的采样，我们能够使用梯度下降来更新潜在的概率。

--------------------------------------------------

[126] The training objective in this work is to maximize the recoverability of sampled masks.
[126] 这项工作的训练目标是最大化采样面具的可回收性。

--------------------------------------------------

[127] We reformulate the objective in Equation 2 by incorporat- ing the learnable distribution: min {p(mk)} min ∆Φ Ex,{mk∼p(mk)} [L(x, Φ + ∆Φ, {mk}] | {z } Recoverability: Post-Fine-Tuning Performance , (6) where {p(mk)} = {p(m1), · · · , p(mK)} refer to the cat- egorical distributions for different local blocks.
[127] 我们通过合并可学习的分布来重新重新制定目标2中的目标：min {p（mk）} min ∆φ ex，{mk〜p（mk）} [l（x，φ + ∆φ，{mk} | {z} | {z} | {z} | {z}恢复性：恢复性：恢复性能：fine-fine-tonning performance，post-fine-tuning performance，post-fine ther p（mk）}有关不同局部块的猫分布。

--------------------------------------------------

[128] Based on this formulation, we further investigate how to incorporate the fine-tuning information into the training.
[128] 基于此公式，我们进一步研究了如何将微调信息纳入培训。

--------------------------------------------------

[129] We propose a joint optimization of the distribution and a weight update ∆Φ.
[129] 我们提出了分布的联合优化和权重更新∆φ。

--------------------------------------------------

[130] Our key idea is to introduce a co-optimized update ∆Φ for joint training.
[130] 我们的关键思想是引入合作更新Δφ以进行关节训练。

--------------------------------------------------

[131] A straightforward way to craft the update is to directly optimize the original network.
[131] 制作更新的直接方法是直接优化原始网络。

--------------------------------------------------

[132] However, the parameter scale in a diffusion transformer is usually huge, and a full optimization may make the training process costly and not that efficient.
[132] 但是，扩散变压器中的参数量表通常是巨大的，并且完整的优化可能会使训练过程成本高昂，而且效率不高。

--------------------------------------------------

[133] To this end, we show that Parameter- Efficient Fine-Tuning methods such as LoRA [21] can be a good choice to obtain the required ∆Φ.
[133] 为此，我们表明参数有效的微调方法（例如Lora [21]）可以是获得所需∆φ的好选择。

--------------------------------------------------

[134] For a single linear matrix W in Φ, we simulate the fine-tuned weights as: Wfine-tuned = W + α∆W = W + αBA, (7) where α is a scalar hyperparameter that scales the contribu- tion of ∆W.
[134] 对于单个线性矩阵Wφ，我们将微调的权重模拟为：wfine-tuned = w +αΔW= w +αbA，（7），其中α是标量超参数，可缩放ΔW的贡献。

--------------------------------------------------

[135] Using LoRA significantly reduces the num- ber of parameters, facilitating efficient exploration of differ- ent pruning decisions.
[135] 使用洛拉大大减少了参数的数量，从而有助于对不同的修剪决策的有效探索。

--------------------------------------------------

[136] As shown in Figure 3, we leverage the sampled binary mask value mi as the gate and forward the network with Equation 1, which suppresses the layer outputs if the sampled mask is 0 for the current layer.
[136] 如图3所示，我们利用采样的二进制掩码值MI作为门，并用方程1向网络转发，如果当前层采样的掩码为0，则抑制层输出的层输出。

--------------------------------------------------

[137] In addition, the previously mentioned STE will still provide non-zero gradients to the pruned layer, allowing it to be fur- ther updated.
[137] 此外，前面提到的Ste仍将为修剪层提供非零梯度，从而可以进行更新。

--------------------------------------------------

[138] This is helpful in practice, since some layers 4
[138] 这在实践中很有帮助，因为有些层4

--------------------------------------------------

[139] Method Depth #Param Iters IS ↑ FID ↓ sFID ↓ Prec.
[139] 方法深度#param Iters是↑fid↓SFID↓PREC。

--------------------------------------------------

[140] ↑ Recall ↑ Sampling it/s ↑ DiT-XL/2 [40] 28 675 M 7,000 K 278.24 2.27 4.60 0.83 0.57 6.91 DiT-XL/2 [40] 28 675 M 2,000 K 240.22 2.73 4.46 0.83 0.55 6.91 DiT-XL/2 [40] 28 675 M 1,000 K 157.83 5.53 4.60 0.80 0.53 6.91 U-ViT-H/2 [1] 29 501 M 500 K 265.30 2.30 5.60 0.82 0.58 8.21 ShortGPT [36] 28⇒19 459 M 100 K 132.79 7.93 5.25 0.76 0.53 10.07 TinyDiT-D19 (KD) 28⇒19 459 M 100 K 242.29 2.90 4.63 0.84 0.54 10.07 TinyDiT-D19 (KD) 28⇒19 459 M 500 K 251.02 2.55 4.57 0.83 0.55 10.07 DiT-L/2 [40] 24 458 M 1,000 K 196.26 3.73 4.62 0.82 0.54 9.73 U-ViT-L [1] 21 287 M 300 K 221.29 3.44 6.58 0.83 0.52 13.48 U-DiT-L [50] 22 204 M 400 K 246.03 3.37 4.49 0.86 0.50 - Diff-Pruning-50% [12] 28 338 M 100 K 186.02 3.85 4.92 0.82 0.54 10.43 Diff-Pruning-75% [12] 28 169 M 100 K 83.78 14.58 6.28 0.72 0.53 13.59 ShortGPT [36] 28⇒14 340 M 100 K 66.10 22.28 6.20 0.63 0.56 13.54 Flux-Lite [6] 28⇒14 340 M 100 K 54.54 25.92 5.98 0.62 0.55 13.54 Sensitivity Analysis [18] 28⇒14 340 M 100 K 70.36 21.15 6.22 0.63 0.57 13.54 Oracle (BK-SDM) [23] 28⇒14 340 M 100 K 141.18 7.43 6.09 0.75 0.55 13.54 TinyDiT-D14 28⇒14 340 M 100 K 151.88 5.73 4.91 0.80 0.55 13.54 TinyDiT-D14 28⇒14 340 M 500 K 198.85 3.92 5.69 0.78 0.58 13.54 TinyDiT-D14 (KD) 28⇒14 340 M 100 K 207.27 3.73 5.04 0.81 0.54 13.54 TinyDiT-D14 (KD) 28⇒14 340 M 500 K 234.50 2.86 4.75 0.82 0.55 13.54 DiT-B/2 [40] 12 130 M 1,000 K 119.63 10.12 5.39 0.73 0.55 28.30 U-DiT-B [50] 22 - 400 K 85.15 16.64 6.33 0.64 0.63 - TinyDiT-D7 (KD) 14⇒7 173 M 500 K 166.91 5.87 5.43 0.78 0.53 26.81 Table 1.
[140] ↑ Recall ↑ Sampling it/s ↑ DiT-XL/2 [40] 28 675 M 7,000 K 278.24 2.27 4.60 0.83 0.57 6.91 DiT-XL/2 [40] 28 675 M 2,000 K 240.22 2.73 4.46 0.83 0.55 6.91 DiT-XL/2 [40] 28 675 M 1,000 K 157.83 5.53 4.60 0.80 0.53 6.91 U-ViT-H/2 [1] 29 501 M 500 K 265.30 2.30 5.60 0.82 0.58 8.21 ShortGPT [36] 28⇒19 459 M 100 K 132.79 7.93 5.25 0.76 0.53 10.07 TinyDiT-D19 (KD) 28⇒19 459 M 100 K 242.29 2.90 4.63 0.84 0.54 10.07 Tinydit-D19（KD）28⇒19459 M 500 K 251.02 2.55 4.57 0.83 0.55 0.55 10.07 DIT-L/2 [40] 24 458 M 1,000 K 196.26.26.26 3.73 4.62 4.62 0.82 0.54 9.73 U-Vit-L [1] 9.73 U-Vit-L [1] U-Vit-L [1] 21 287 M 300 M 300 M 300 K 22129.44.44.44.44.44.44.44.44.44.44.44 4.44.44.44.44.44.444.44.44.44 k 2294.44.44.44.44.44.44 4.44 4.44 4.44 4.44 4.44 4.44 4.44 4.44.444.; U-DiT-L [50] 22 204 M 400 K 246.03 3.37 4.49 0.86 0.50 - Diff-Pruning-50% [12] 28 338 M 100 K 186.02 3.85 4.92 0.82 0.54 10.43 Diff-Pruning-75% [12] 28 169 M 100 K 83.78 14.58 6.28 0.72 0.53 13.59 ShortGPT [36] 28⇒14 340 M 100 K 66.10 22.28 6.20 0.63 0.56 13.54 Flux-Lite [6] 28⇒14 340 M 100 K 54.54 25.92 5.98 0.62 0.55 13.54 Sensitivity Analysis [18] 28⇒14 340 M 100 K 70.36 21.15 6.22 0.63 0.57 13.54 Oracle (BK-SDM) [23] 28⇒14340 M 100 K 141.18 7.43 6.09 0.75 0.55 0.55 13.54 TINYDIT-D1428⇒14340 M 100 K 100 K 151.88 5.73 4.91 0.80 0.55 13.55 13.54 TINYDIT-D14 28看看14 340 M 500 K 198.85 3.92 5.62 5.62 5.62 0.78 14.78 148（ 28⇒14 340 M 100 K 207.27 3.73 5.04 0.81 0.54 13.54 TinyDiT-D14 (KD) 28⇒14 340 M 500 K 234.50 2.86 4.75 0.82 0.55 13.54 DiT-B/2 [40] 12 130 M 1,000 K 119.63 10.12 5.39 0.73 0.55 28.30 U-DiT-B [50] 22 - 400 K 85.15 16.64 6.33 0.64 0.63 -tinydit -d7（kd）14⇒7173 M 500 K 166.91 5.87 5.43 5.43 0.78 0.78 0.53 26.81表1。

--------------------------------------------------

[141] Layer pruning results for pre-trained DiT-XL/2.
[141] 预先训练的DIT-XL/2的层修剪结果。

--------------------------------------------------

[142] We focus on two settings: fast training with 100K optimization steps and sufficient fine-tuning with 500K steps.
[142] 我们专注于两个设置：通过100k优化步骤进行快速培训，并通过500k步骤进行了足够的微调。

--------------------------------------------------

[143] Both fine-tuning and Masked Knowledge Distillation (a variant of KD, see Sec.
[143] 微调和掩盖的知识蒸馏（KD的一种变体，请参见秒。

--------------------------------------------------

[144] 4.4) are used for recovery.
[144] 4.4）用于恢复。

--------------------------------------------------

[145] might not be competitive at the beginning, but may emerge as competitive candidates with sufficient fine-tuning.
[145] 一开始可能不是竞争性的，但可能会成为具有足够微调的竞争候选人。

--------------------------------------------------

[146] Pruning Decision.
[146] 修剪决定。

--------------------------------------------------

[147] After training, we retain those local structures with the highest probability and discard the ad- ditional update ∆Φ.
[147] 训练后，我们保留了最高概率的那些局部结构，并丢弃了广告更新Δφ。

--------------------------------------------------

[148] Then, standard fine-tuning techniques can be applied for recovery.
[148] 然后，可以应用标准的微调技术进行恢复。

--------------------------------------------------

[149] 4.
[149] 4。

--------------------------------------------------

[150] Experiments 4.1.
[150] 实验4.1。

--------------------------------------------------

[151] Experimental Settings Our experiments were mainly conducted on Diffusion Transformers [40] for class-conditional image generation on ImageNet 256 × 256 [8].
[151] 实验设置我们的实验主要是在扩散变压器[40]上进行的，以在ImageNet 256×256 [8]上进行类条件图像生成。

--------------------------------------------------

[152] For evaluation, we fol- low [9, 40] and report the Fr´echet inception distance (FID), Sliding Fr´echet Inception Distance (sFID), Inception Scores (IS), Precision and Recall using the official reference im- ages [9].
[152] 为了进行评估，我们将[9，40]报告，并报告了使用正式的参考文献[9]，报告了FR´Echet Inception距离（FID），SLIVE fr´echet Inception距离（SFID），INCEPTION分数（IS），精度和召回。

--------------------------------------------------

[153] Additionally, we also extend our methods to other models, including MARs [29] and SiTs [34].
[153] 此外，我们还将我们的方法扩展到其他模型，包括火星[29]和位于[34]。

--------------------------------------------------

[154] Experimental details can be found in the following sections and appendix.
[154] 实验细节可以在以下各节和附录中找到。

--------------------------------------------------

[155] 4.2.
[155] 4.2。

--------------------------------------------------

[156] Results on Diffusion Transformers DiT.
[156] 扩散变压器DIT的结果。

--------------------------------------------------

[157] This work focuses on the compression of DiTs [40].
[157] 这项工作着重于DIT的压缩[40]。

--------------------------------------------------

[158] We consider two primary strategies as baselines: the first 0 20 40 60 80 Compression Ratio (%) 2 4 6 8 10 12 Speed Up 1.08 1.17 1.27 1.40 1.55 1.74 1.99 2.30 2.76 3.41 4.46 6.45 11.60 1.04 1.26 1.36 1.64 1.91 2.20 2.71 3.36 4.39 Depth Pruning Width Pruning Linear Speedup Figure 4.
[158] We consider two primary strategies as baselines: the first 0 20 40 60 80 Compression Ratio (%) 2 4 6 8 10 12 Speed Up 1.08 1.17 1.27 1.40 1.55 1.74 1.99 2.30 2.76 3.41 4.46 6.45 11.60 1.04 1.26 1.36 1.64 1.91 2.20 2.71 3.36 4.39 Depth Pruning Width Pruning Linear Speedup Figure 4.

--------------------------------------------------

[159] Depth pruning closely aligns with the theoretical linear speed-up relative to the compression ratio.
[159] 深度修剪与理论线性加速相对于压缩率紧密对齐。

--------------------------------------------------

[160] involves using manually crafted patterns to eliminate lay- ers.
[160] 涉及使用手动制作的图案消除外行。

--------------------------------------------------

[161] For instance, BK-SDM [23] employs heuristic assump- tions to determine the significance of specific layers, such as the initial or final layers.
[161] 例如，BK-SDM [23]采用启发式辅助来确定特定层的重要性，例如初始或最终层。

--------------------------------------------------

[162] The second strategy is based on systematically designed criteria to evaluate layer impor- tance, such as analyzing the similarity between block in- puts and outputs to determine redundancy [6, 36]; this ap- proach typically aims to minimize performance degradation after pruning.
[162] 第二种策略是基于系统设计的标准来评估层的重要性，例如分析块In-ofs和输出之间的相似性以确定冗余[6，36]；这种方法通常旨在最大程度地减少修剪后的性能降解。

--------------------------------------------------

[163] Table 1 presents representatives from both strategies, including ShortGPT [36], Flux-Lite [6], Diff- Pruning [12], Sensitivity Analysis [18] and BK-SDM [23], which serve as baselines for comparison.
[163] 表1介绍了两种策略的代表，包括短速度[36]，Flux-Lite [6]，Diff-pruning [12]，灵敏度分析[18]和BK-SDM [23]，它们是比较的基础。

--------------------------------------------------

[164] Additionally, 5
[164] 另外，5

--------------------------------------------------

[165] Method Depth Params Epochs FID IS MAR-Large 32 479 M 400 1.78 296.0 MAR-Base 24 208 M 400 2.31 281.7 TinyMAR-D16 32⇒16 277 M 40 2.28 283.4 SiT-XL/2 28 675 M 1,400 2.06 277.5 TinySiT-D14 28⇒14 340 M 100 3.02 220.1 Table 2.
[165] Method Depth Params Epochs FID IS MAR-Large 32 479 M 400 1.78 296.0 MAR-Base 24 208 M 400 2.31 281.7 TinyMAR-D16 32⇒16 277 M 40 2.28 283.4 SiT-XL/2 28 675 M 1,400 2.06 277.5 TinySiT-D14 28⇒14 340 M 100 3.02 220.1 Table 2.

--------------------------------------------------

[166] Depth pruning results on MARs [29] and SiTs [34].
[166] 对火星的深度修剪结果[29]并位于[34]。

--------------------------------------------------

[167] we evaluate our method against innovative architectural de- signs, such as UViT [1], U-DiT [50], and DTR [39], which have demonstrated improved training efficiency over con- ventional DiTs.
[167] 我们评估了针对创新建筑的方法，例如UVIT [1]，U-DIT [50]和DTR [39]，这些方法表明，这些方法表现出了提高的培训效率，而不是降级。

--------------------------------------------------

[168] Table 1 presents our findings on compressing a pre- trained DiT-XL/2 [40].
[168] 表1列出了我们关于压缩预训练的DIT-XL/2 [40]的发现。

--------------------------------------------------

[169] This model contains 28 transformer layers structured with alternating Attention and MLP lay- ers.
[169] 该模型包含28个具有交替注意和MLP外行的变压器层。

--------------------------------------------------

[170] The proposed method seeks to identify shallow trans- formers with {7, 14, 19} sub-layers from these 28 layers, to maximize the post-fine-tuning performance.
[170] 所提出的方法旨在鉴定这28层中{7、14、19}子层的浅型，以最大程度地提高后调节性能。

--------------------------------------------------

[171] With only 7% of the original training cost (500K steps compared to 7M steps), TinyDiT achieves competitive performance rela- tive to both pruning-based methods and novel architectures.
[171] Tinydit只有7％的原始培训成本（500k步长），与基于修剪的方法和新型体系结构相关的竞争性能。

--------------------------------------------------

[172] For instance, a DiT-L model trained from scratch for 1M steps achieves an FID score of 3.73 with 458M parameters.
[172] 例如，通过从头开始训练1M步骤的DIT-L模型以4.58亿参数的成绩达到3.73的FID分数。

--------------------------------------------------

[173] In contrast, the compressed TinyDiT-D14 model, with only 340M parameters and a faster sampling speed (13.54 it/s vs. 9.73 it/s), yields a significantly improved FID of 2.86.
[173] 相比之下，仅具有340m参数和更快的采样速度（13.54 IT/s vs. 9.73 IT/s）的压缩TinyDit-D14模型可显着提高2.86的FID。

--------------------------------------------------

[174] On parallel devices like GPUs, the primary bottleneck in trans- formers arises from sequential operations within each layer, which becomes more pronounced as the number of layers increases.
[174] 在诸如GPU之类的并行设备上，转移器中的主要瓶颈源于每一层内的顺序操作，随着层数的增加，这会变得更加明显。

--------------------------------------------------

[175] Depth pruning mitigates this bottleneck by re- moving entire transformer layers, thereby reducing compu- tational depth and optimizing the workload.
[175] 深度修剪通过重新移动整个变压器层来减轻这种瓶颈，从而减少组合深度并优化工作量。

--------------------------------------------------

[176] By compar- ison, width pruning only reduces the number of neurons within each layer, limiting its speed-up potential.
[176] 相比之下，修剪宽度仅会减少每一层内神经元的数量，从而限制了其加速势。

--------------------------------------------------

[177] As shown in Figure 4, depth pruning closely matches the theoretical linear speed-up as the compression ratio increases, outper- forming width pruning methods such as Diff-Pruning [12].
[177] 如图4所示，随着压缩比的增加，深度修剪与理论线性加速密切匹配，诸如diff-pruning之类的宽度宽度较高[12]。

--------------------------------------------------

[178] MAR & SiT.
[178] Mar＆Sit。

--------------------------------------------------

[179] Masked Autoregressive (MAR) [29] mod- els employ a diffusion loss-based autoregressive framework in a continuous-valued space, achieving high-quality image generation without the need for discrete tokenization.
[179] 蒙面自回旋（MAR）[29]模型在连续值的空间中采用基于扩散损失的自回旋框架，实现了高质量的图像生成而无需离散令牌化。

--------------------------------------------------

[180] The MAR-Large model, with 32 transformer blocks, serves as the baseline for comparison.
[180] 具有32个变压器块的MAR-LARGE模型是比较的基线。

--------------------------------------------------

[181] Applying our pruning method, we reduced MAR to a 16-block variant, TinyMAR-D16, achieving an FID of 2.28 and surpassing the performance of the 24-block MAR-Base model with only 10% of the original training cost (40 epochs vs. 400 epochs).
[181] 应用我们的修剪方法，我们将MAR降低到16块变体Tinymar-D16，达到2.28的FID，并超过了24块MAR-BASE模型的性能，只有10％的原始训练成本（40个时期）（40个时期与400个时期）。

--------------------------------------------------

[182] Our ap- proach also generalizes to Scalable Interpolant Transform- ers (SiT) [34], an extension of the DiT architecture that employs a flow-based interpolant framework to bridge data 100 101 Calibration Loss 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Density Min: 0.195 Max: 37.694 Std: 1.300 Min: 0.195 Max: 37.694 Std: 1.300      Oracle Learnable ShortGPT     Sensitivity Flux-Lite     Figure 5.
[182] Our ap- proach also generalizes to Scalable Interpolant Transform- ers (SiT) [34], an extension of the DiT architecture that employs a flow-based interpolant framework to bridge data 100 101 Calibration Loss 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Density Min: 0.195 Max: 37.694 Std: 1.300 Min: 0.195 Max: 37.694 STD：1.300 Oracle可学习的短时敏感性通量 -  lux-lite图5。

--------------------------------------------------

[183] Distribution of calibration loss through random sampling of candidate models.
[183] 通过随机抽样候选模型的校准损失分布。

--------------------------------------------------

[184] The proposed learnable method achieves the best post-fine-tuning FID yet has a relatively high initial loss com- pared to other baselines.
[184] 所提出的可学习方法实现了最佳的验证后FID，但具有相对较高的初始损失与其他基线相比。

--------------------------------------------------

[185] Strategy Loss IS FID Prec.
[185] 策略损失是FID PREC。

--------------------------------------------------

[186] Recall Max.
[186] 回想最大。

--------------------------------------------------

[187] Loss 37.69 NaN NaN NaN NaN Med.
[187] 损失37.69 in Med。

--------------------------------------------------

[188] Loss 0.99 149.51 6.45 0.78 0.53 Min.
[188] 损失0.99 149.51 6.45 0.78 0.53分钟。

--------------------------------------------------

[189] Loss 0.20 73.10 20.69 0.63 0.58 Sensitivity 0.21 70.36 21.15 0.63 0.57 ShortGPT [36] 0.20 66.10 22.28 0.63 0.56 Flux-Lite [6] 0.85 54.54 25.92 0.62 0.55 Oracle (BK-SDM) 1.28 141.18 7.43 0.75 0.55 Learnable 0.98 151.88 5.73 0.80 0.55 Table 3.
[189] 损失0.20 73.10 20.69 0.63 0.58灵敏度0.21 70.36 21.15 0.63 0.57短时间[36] 0.20 66.10 22.28 0.63 0.56 Flux-Lite [6] 0.85 54.54 54.54 25.92 0.62 0.62 0.55 Oracle（BK-SDM） 151.88 5.73 0.80 0.55表3。

--------------------------------------------------

[190] Directly minimizing the calibration loss may lead to non-optimal solutions.
[190] 直接最大程度地减少校准损失可能导致非最佳解决方案。

--------------------------------------------------

[191] All pruned models are fine-tuned without knowledge distillation (KD) for 100K steps.
[191] 所有修剪模型均经过微调，没有知识蒸馏（KD）100K步骤。

--------------------------------------------------

[192] We evaluate the fol- lowing baselines: (1) Loss – We randomly prune a DiT-XL model to generate 100,000 models and select models with different cali- bration losses for fine-tuning; (2) Metric-based Methods – such as Sensitivity Analysis and ShortGPT; (3) Oracle – We retain the first and last layers while uniformly pruning the intermediate layers fol- lowing [23]; (4) Learnable – The proposed learnable method.
[192] 我们评估了以下基准：（1）损失 - 我们随机修剪DIT-XL模型，以生成100,000款模型，并选择具有不同卡路损失的模型以进行微调； （2）基于度量的方法 - 例如灵敏度分析和短程； （3）Oracle  - 我们保留了第一层也是最后一层，同时均匀修剪了中间层[23]； （4）可学习 - 拟议的可学习方法。

--------------------------------------------------

[193] and noise distributions.
[193] 和噪声分布。

--------------------------------------------------

[194] The SiT-XL/2 model, comprising 28 transformer blocks, was pruned by 50%, creating the TinySiT-D14 model.
[194] SIT-XL/2模型，包括28个变压器块，由50％修剪，创建TinySit-D14模型。

--------------------------------------------------

[195] This pruned model retains competi- tive performance at only 7% of the original training cost (100 epochs vs. 1400 epochs).
[195] 该修剪模型仅以原始培训成本的7％（100个时代与1400个时代）保留竞争性能。

--------------------------------------------------

[196] As shown in Table 2, these results demonstrate that our pruning method is adaptable across different diffusion transformer variants, effectively reducing the model size and training time while maintain- ing strong performance.
[196] 如表2所示，这些结果表明，我们的修剪方法在不同的扩散变压器变体中具有适应性的适应性，从而有效地减少了模型的大小和训练时间，同时保持强劲的性能。

--------------------------------------------------

[197] 4.3.
[197] 4.3。

--------------------------------------------------

[198] Analytical Experiments Is Calibration Loss the Primary Determinant?
[198] 分析实验是校准损失的主要决定因素吗？

--------------------------------------------------

[199] An es- sential question in depth pruning is how to identify re- dundant layers in pre-trained diffusion transformers.
[199] 深度修剪的一个问题是如何识别预先训练的扩散变压器中的重复层。

--------------------------------------------------

[200] A common approach involves minimizing the calibration loss, based on the assumption that a model with lower calibra- tion loss after pruning will exhibit superior performance.
[200] 一种共同的方法涉及最大程度地减少校准损失，这是基于这样的假设：修剪后较低碳纤维损失的模型将表现出较高的性能。

--------------------------------------------------

[201] However, we demonstrate in this section that this hypothesis may not hold for diffusion transformers.
[201] 但是，我们在本节中证明，该假设可能不适合扩散变压器。

--------------------------------------------------

[202] We begin by ex- amining the solution space through random depth pruning at a 50% ratio, generating 100,000 candidate models with 6
[202] 我们首先通过以50％的比例来扫描解决方案空间，以6的比例产生100,000个候选模型

--------------------------------------------------

[203] Pattern ∆W IS ↑ FID ↓ sFID ↓ Prec.
[203] 模式∆W是↑fid↓SFID↓PREC。

--------------------------------------------------

[204] ↑ Recall ↑ 1:2 LoRA 54.75 33.39 29.56 0.56 0.62 2:4 LoRA 53.07 34.21 27.61 0.55 0.63 7:14 LoRA 34.97 49.41 28.48 0.46 0.56 1:2 Full 53.11 35.77 32.68 0.54 0.61 2:4 Full 53.63 34.41 29.93 0.55 0.62 7:14 Full 45.03 38.76 31.31 0.52 0.62 1:2 Frozen 45.08 39.56 31.13 0.52 0.60 2:4 Frozen 48.09 37.82 31.91 0.53 0.62 7:14 Frozen 34.09 49.75 31.06 0.46 0.56 Table 4.
[204] ↑ Recall ↑ 1:2 LoRA 54.75 33.39 29.56 0.56 0.62 2:4 LoRA 53.07 34.21 27.61 0.55 0.63 7:14 LoRA 34.97 49.41 28.48 0.46 0.56 1:2 Full 53.11 35.77 32.68 0.54 0.61 2:4 Full 53.63 34.41 29.93 0.55 0.62 7:14 Full 45.03 38.76 31.31 0.52 0.62 1:2 Frozen 45.08 39.56 31.13 0.52 0.60 2:4 Frozen 48.09 37.82 31.91 0.53 0.62 7:14 Frozen 34.09 49.75 31.06 0.46 0.56 Table 4.

--------------------------------------------------

[205] Performance comparison of TinyDiT-D14 models com- pressed using various pruning schemes and recoverability estima- tion strategies.
[205] 使用各种修剪方案和可恢复性估计策略组合的TinyDit-D14模型的性能比较。

--------------------------------------------------

[206] All models are fine-tuned for 10,000 steps, and FID scores are computed on 10,000 sampled images with 64 timesteps.
[206] 所有型号均以10,000个步骤进行微调，并在10,000个带有64个时间步长的采样图像上计算FID分数。

--------------------------------------------------

[207] calibration losses ranging from 0.195 to 37.694 (see Fig- ure 5).
[207] 校准损失范围为0.195至37.694（见图5）。

--------------------------------------------------

[208] From these candidates, we select models with the highest and lowest calibration losses for fine-tuning.
[208] 从这些候选人中，我们选择具有最高和最低校准损失的模型以进行微调。

--------------------------------------------------

[209] No- tably, both models result in unfavorable outcomes, such as unstable training (NaN) or suboptimal FID scores (20.69), as shown in Table 3.
[209] 如表3所示，两种模型都没有造成不利的结果，例如不稳定的训练（NAN）或次优FID得分（20.69）。

--------------------------------------------------

[210] Additionally, we conduct a sensitiv- ity analysis [18], a commonly used technique to identify crucial layers by measuring loss disturbance upon layer re- moval, which produces a model with a low calibration loss of 0.21.
[210] 此外，我们进行了灵敏分析[18]，这是一种常用的技术，可以通过测量层层次的损失障碍来识别关键层，该层损失层的损失，该模型产生了一个模型，低校准损失为0.21。

--------------------------------------------------

[211] However, this model’s FID score is similar to that of the model with the lowest calibration loss.
[211] 但是，该模型的FID得分与校准损失最低的模型的得分相似。

--------------------------------------------------

[212] Approaches like ShortGPT [36] and a recent approach for compressing the Flux model [6], which estimate similarity or minimize mean squared error (MSE) between input and output states, reveal a similar trend.
[212] 诸如Shortgpt [36]和最近压缩通量模型[6]的方法，该方法估算了输入和输出状态之间的平均平方误差（MSE），从而揭示了相似的趋势。

--------------------------------------------------

[213] In contrast, methods with mod- erate calibration losses, such as Oracle (often considered less competitive) and one of the randomly pruned models, achieve FID scores of 7.43 and 6.45, respectively, demon- strating significantly better performance than models with minimal calibration loss.
[213] 相比之下，具有模块化校准损失的方法，例如Oracle（通常认为竞争力较低）和随机修剪的模型之一，分别达到7.43和6.45的FID得分，比具有最小校准损失的模型的模型明显更好。

--------------------------------------------------

[214] These findings suggest that, while calibration loss may influence post-fine-tuning performance to some extent, it is not the primary determinant for diffu- sion transformers.
[214] 这些发现表明，虽然校准损失可能在某种程度上会影响预定后的性能，但它并不是扩散变压器的主要决定因素。

--------------------------------------------------

[215] Instead, the model’s capacity for perfor- mance recovery during fine-tuning, termed “recoverability,” appears to be more critical.
[215] 取而代之的是，该模型在微调过程中恢复性能的能力称为“可恢复性”，似乎更为关键。

--------------------------------------------------

[216] Notably, assessing recoverabil- ity using traditional metrics is challenging, as it requires a learning process across the entire dataset.
[216] 值得注意的是，使用传统指标评估恢复性是具有挑战性的，因为它需要在整个数据集中进行学习过程。

--------------------------------------------------

[217] This observation also explains why the proposed method achieves superior results (5.73) compared to baseline methods.
[217] 该观察结果还解释了为什么所提出的方法与基线方法相比取得了优越的结果（5.73）。

--------------------------------------------------

[218] Learnable Modeling of Recoverability.
[218] 可恢复性的可学习建模。

--------------------------------------------------

[219] To overcome the limitations of traditional metric-based methods, this study introduces a learnable approach to jointly optimize pruning and model recoverability.
[219] 为了克服传统基于指标的方法的局限性，本研究引入了一种可学习的方法，可以共同优化修剪和建模可回收性。

--------------------------------------------------

[220] Table 3 illustrates dif- ferent configurations of the learnable method, including the local pruning scheme and update strategies for recoverabil- ity estimation.
[220] 表3说明了可学习方法的不同配置，包括局部修剪方案和更新以恢复估计的策略。

--------------------------------------------------

[221] For a 28-layer DiT-XL/2 with a fixed 50% 0 2000 4000 6000 8000 10000 Train iterations 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Layer Index in DiT-XL Figure 6.
[221] 对于带有固定50％0 2000 4000 6000 8000火车迭代的28层DIT-XL/2，0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 22 23 23 24 26 26 27 27 dit-XL中的层指数图6。

--------------------------------------------------

[222] Visualization of the 2:4 decisions in the learnable prun- ing, with the confidence level of each decision highlighted through varying degrees of transparency.
[222] 2：4在可学习的修剪中的决定的可视化，每个决策的置信度都通过不同程度的透明度强调。

--------------------------------------------------

[223] More visualization results for 1:2 and 7:14 schemes are available in the appendix.
[223] 附录中提供了1：2和7:14方案的更多可视化结果。

--------------------------------------------------

[224] layer pruning rate, we examine three splitting schemes: 1:2, 2:4, and 7:14.
[224] 层修剪率，我们检查了三个分裂方案：1：2、2：4和7：14。

--------------------------------------------------

[225] In the 1:2 scheme, for example, every two transformer layers form a local block, with one layer pruned.
[225] 例如，在1：2方案中，每两个变压器层形成一个局部块，并修剪一层。

--------------------------------------------------

[226] Larger blocks introduce greater diversity but sig- nificantly expand the search space.
[226] 较大的块引入了更大的多样性，但很明显地扩大了搜索空间。

--------------------------------------------------

[227] For instance, the 7:14 scheme divides the model into two segments, each retain- ing 7 layers, resulting in  14 7  × 2 = 6,864 possible solu- tions.
[227] 例如，7:14方案将模型划分为两个段，每个片段保留了7层，导致14 7×2 = 6,864可能的解决方案。

--------------------------------------------------

[228] Conversely, smaller blocks significantly reduce op- timization difficulty and offer greater flexibility.
[228] 相反，较小的块大大减少了操作困难，并提供了更大的灵活性。

--------------------------------------------------

[229] When the distribution of one block converges, the learning on other blocks can still progress.
[229] 当一个块的分布收敛时，其他块上的学习仍然可以进步。

--------------------------------------------------

[230] As shown in Table 3, the 1:2 con- figuration achieves the optimal performance after 10K fine- tuning iterations.
[230] 如表3所示，1：2的形象在10K细调迭代后达到了最佳性能。

--------------------------------------------------

[231] Additionally, our empirical findings un- derscore the effectiveness of recoverability estimation using LoRA or full fine-tuning.
[231] 此外，我们的经验发现未解决使用LORA或完整微调可恢复性估计的有效性。

--------------------------------------------------

[232] Both methods yield positive post- fine-tuning outcomes, with LoRA achieving superior results (FID = 33.39) compared to full fine-tuning (FID = 35.77) under the 1:2 scheme, as LoRA has fewer trainable parame- ters (0.9% relative to full parameter training) and can adapt more efficiently to the randomness of sampling.
[232] 两种方法都产生正面调查结果，与1：2方案下的全面微调（FID = 35.77）相比，洛拉取得了较高的结果（FID = 33.39），因为洛拉（Lora）具有较少的可训练的参数（相对于完全参数训练），并且可以更有效地适应采样的随机性。

--------------------------------------------------

[233] Visualization of Learnable Decisions.
[233] 可学习的可学习决策。

--------------------------------------------------

[234] To gain deeper in- sights into the role of the learnable method in pruning, we visualize the learning process in Figure 6.
[234] 为了更深入地了解可学习方法在修剪中的作用，我们将图6中的学习过程可视化。

--------------------------------------------------

[235] From bottom to top, the i-th curve represents the i-th layer of the pruned model, displaying its layer index in the original DiT-XL/2.
[235] 从底部到顶部，第i-th曲线代表修剪模型的第I层，在原始DIT-XL/2中显示其层索引。

--------------------------------------------------

[236] This visualization illustrates the dynamics of pruning de- cisions over training iterations, where the transparency of each data point indicates the probability of being sampled.
[236] 这种可视化说明了修剪训练迭代的修剪效果的动力学，其中每个数据点的透明度表明被采样的概率。

--------------------------------------------------

[237] The learnable method shows its capacity to explore and handle various layer combinations.
[237] 可学习的方法显示了其探索和处理各种层组合的能力。

--------------------------------------------------

[238] Pruning decisions for certain layers, such as the 7-th and 8-th in the compressed model, are determined quickly and remain stable through- out the process.
[238] 对某些层的修剪决策，例如压缩模型中的7-三分之一和第8-三，并在整个过程中保持稳定。

--------------------------------------------------

[239] In contrast, other layers, like the 0-th layer, require additional fine-tuning to estimate their recoverabil- ity.
[239] 相比之下，其他层（如第0层）需要进行其他微调来估算其恢复性。

--------------------------------------------------

[240] Notably, some decisions may change in the later stages 7
[240] 值得注意的是，某些决定可能会在后期第7阶段发生变化

--------------------------------------------------

[241] Figure 7.
[241] 图7。

--------------------------------------------------

[242] Images generated by TinyDiT-D14 on ImageNet 224×224, pruned and distilled from a DiT-XL/2.
[242] 由Tinydit-d14在Imagenet 224×224上产生的图像，从DIT-XL/2进行修剪和蒸馏。

--------------------------------------------------

[243] 102 101 100 0 100 101 102 Activation Value (log) 0.0 0.1 0.2 0.3 0.4 Density  Max Activation: 191.20  Min Activation: -429.01 +Std: 12.54 -Std: -12.54 (a) DiT-XL/2 (Teacher) 102 101 100 0 100 101 Activation Value (log) 0.0 0.1 0.2 0.3 0.4 0.5 Density  Max Activation: 53.77  Min Activation: -526.62 +Std: 14.15 -Std: -14.15 (b) TinyDiT-D14 (Student) Figure 8.
[243] 102 101 100 0 100 101 102 Activation Value (log) 0.0 0.1 0.2 0.3 0.4 Density  Max Activation: 191.20  Min Activation: -429.01 +Std: 12.54 -Std: -12.54 (a) DiT-XL/2 (Teacher) 102 101 100 0 100 101 Activation Value (log) 0.0 0.1 0.2 0.3 0.4 0.5 Density  Max Activation: 53.77  Min Activation: -526.62 +STD：14.15 -STD：-14.15（b）Tinydit -D14（学生）图8。

--------------------------------------------------

[244] Visualization of massive activations [47] in DiTs.
[244] 大规模激活的可视化[47]。

--------------------------------------------------

[245] Both teacher and student models display large activation values in their hidden states.
[245] 教师和学生模型在其隐藏状态下都显示出较大的激活值。

--------------------------------------------------

[246] Directly distilling these massive activations may result in excessively large losses and unstable training.
[246] 直接提炼这些大规模激活可能会导致过度损失和训练不稳定。

--------------------------------------------------

[247] once these layers have been sufficiently optimized.
[247] 一旦这些层得到充分优化。

--------------------------------------------------

[248] The training process ultimately concludes with high sampling probabilities, suggesting a converged learning process with distributions approaching a one-hot configuration.
[248] 培训过程最终以高采样概率结束，这表明通过分布接近单次配置的分布进行了融合的学习过程。

--------------------------------------------------

[249] After training, we select the layers with the highest probabilities for subsequent fine-tuning.
[249] 训练后，我们选择具有最高概率的层以后进行微调。

--------------------------------------------------

[250] 4.4.
[250] 4.4。

--------------------------------------------------

[251] Knowledge Distillation for Recovery In this work, we also explore Knowledge Distillation (KD) as an enhanced fine-tuning method.
[251] 在这项工作中恢复的知识蒸馏，我们还探索了知识蒸馏（KD）作为增强的微调方法。

--------------------------------------------------

[252] As demonstrated in Ta- ble 5, we apply the vanilla knowledge distillation approach proposed by Hinton [20] to fine-tune a TinyDiT-D14, using the outputs of the pre-trained DiT-XL/2 as a teacher model for supervision.
[252] 如Table 5中所示，我们使用Hinton [20]提出的香草知识蒸馏方法对TinyDit-D14进行微调，使用预先训练的DIT-XL/2的输出作为监督的教师模型。

--------------------------------------------------

[253] We employ a Mean Square Error (MSE) loss to align the outputs between the shallow student model and the deeper teacher model, which effectively reduces the FID at 100K steps from 5.79 to 4.66.
[253] 我们采用均方误差（MSE）损失来对齐浅层学生模型和更深层的教师模型之间的输出，从而有效地将100K步骤的FID从5.79降低到4.66。

--------------------------------------------------

[254] Masked Knowledge Distillation.
[254] 掩盖的知识蒸馏。

--------------------------------------------------

[255] Additionally, we eval- uate representation distillation (RepKD) [23, 42] to transfer hidden states from the teacher to the student.
[255] 此外，我们评估表示代表蒸馏（REPKD）[23，42]将隐藏状态从老师转移到学生。

--------------------------------------------------

[256] It is important to note that depth pruning does not alter the hidden dimen- sion of diffusion transformers, allowing for direct alignment fine-tuning Strategy Init.
[256] 重要的是要注意，深度修剪不会改变扩散变压器的隐藏尺寸，从而允许直接对齐微调策略init。

--------------------------------------------------

[257] Distill.
[257] 蒸馏。

--------------------------------------------------

[258] Loss FID @ 100K fine-tuning - 5.79 Logits KD - 4.66 RepKD 2840.1 NaN Masked KD (0.1σ) 15.4 NaN Masked KD (2σ) 387.1 3.73 Masked KD (4σ) 391.4 3.75 Table 5.
[258] 损失FID @ 100k微调-5.79 logits KD -4.66 REPKD 2840.1 NAN蒙版KD（0.1σ）15.4 Nan蒙版KD（2σ）387.1 3.73蒙版KD（4σ）391.4 3.75表5。

--------------------------------------------------

[259] Evaluation of different fine-tuning strategies for recovery.
[259] 评估不同的微调策略的恢复策略。

--------------------------------------------------

[260] Masked RepKD ignores those massive activations (|x| > kσx) in both teacher and student, which enables effective knowledge transfer between diffusion transformers.
[260] 蒙面的repkd忽略了教师和学生中的那些大规模激活（| x |>kσx），这可以在扩散变压器之间有效地传递知识转移。

--------------------------------------------------

[261] of intermediate hidden states.
[261] 中间隐藏状态。

--------------------------------------------------

[262] For practical implementation, we use the block defined in Section 3.2 as the basic unit, ensuring that the pruned local structure in the pruned DiT aligns with the output of the original structure in the teacher model.
[262] 对于实际实施，我们将第3.2节中定义的块作为基本单元，以确保修剪的DIT中修剪的本地结构与教师模型中原始结构的输出保持一致。

--------------------------------------------------

[263] However, we encountered significant training dif- ficulties with this straightforward RepKD approach due to massive activations in the hidden states, where both teacher and student models occasionally exhibit large activation values, as shown in Figure 8.
[263] 但是，由于隐藏状态中的大量激活，我们遇到了这种直接的repkd方法遇到了重要的训练困难，在这些培训状态下，教师和学生模型偶尔都会表现出较大的激活值，如图8所示。

--------------------------------------------------

[264] Directly distilling these ex- treme activations can result in excessively high loss values, impairing the performance of the student model.
[264] 直接提取这些典型的激活可能会导致过高的损失值，从而损害学生模型的表现。

--------------------------------------------------

[265] This issue has also been observed in other transformer-based genera- tive models, such as certain LLMs [47].
[265] 在其他基于变压器的属性模型（例如某些LLMS）中也观察到了这个问题[47]。

--------------------------------------------------

[266] To address this, we propose a Masked RepKD variant that selectively ex- cludes these massive activations during knowledge transfer.
[266] 为了解决这个问题，我们提出了一个掩盖的repkd变体，该变体在知识转移过程中有选择地表达了这些大规模激活。

--------------------------------------------------

[267] We employ a simple thresholding method, |x −µx| < kσx, which ignores the loss associated with these extreme acti- vations.
[267] 我们采用一种简单的阈值方法，| x -µx | <kσx，忽略与这些极端作用相关的损失。

--------------------------------------------------

[268] As shown in Table 5, the Masked RepKD approach with moderate thresholds of 2σ and 4σ achieves satisfactory results, demonstrating the robustness of our method.
[268] 如表5所示，带有2σ和4σ的中等阈值的掩盖repkd方法可实现令人满意的结果，证明了我们方法的鲁棒性。

--------------------------------------------------

[269] Generated Images.
[269] 生成的图像。

--------------------------------------------------

[270] In Figure 7, We visualize the gener- ated images of the learned TinyDiT-D14, distilled from an 8
[270] 在图7中，我们可视化了从8

--------------------------------------------------

[271] off-the-shelf DiT-XL/2 model.
[271] 现成的DIT-XL/2型号。

--------------------------------------------------

[272] More visualization results for SiTs and MARs can be found in the appendix.
[272] 可以在附录中找到更多的坐姿和火星的可视化结果。

--------------------------------------------------

[273] 5.
[273] 5。

--------------------------------------------------

[274] Conclusions This work introduces TinyFusion, a learnable method for accelerating diffusion transformers by removing redundant layers.
[274] 结论这项工作引入了TinyFusion，这是一种通过去除冗余层来加速扩散变压器的可学习方法。

--------------------------------------------------

[275] It models the recoverability of pruned models as an optimizable objective and incorporates differentiable sam- pling for end-to-end training.
[275] 它将修剪模型的可恢复性建模为一个优化的目标，并结合了端到端训练的可区分套件。

--------------------------------------------------

[276] Our method generalizes to various architectures like DiTs, MARs and SiTs.
[276] 我们的方法概括为诸如DIT，火星和坐着的各种体系结构。

--------------------------------------------------

[277] References [1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
[277] References [1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.

--------------------------------------------------

[278] All are worth words: A vit backbone for diffusion models.
[278] 所有都是值得的词：扩散模型的VIT骨干。

--------------------------------------------------

[279] In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 22669–22679, 2023.
[279] 在计算机视觉和模式识别的IEEE/CVF会议论文集中，第22669–22679页，2023年。

--------------------------------------------------

[280] [2] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
[280] [2] Yoshua Bengio，Nicholas L´eonard和Aaron Courville。

--------------------------------------------------

[281] Estimating or propagating gradients through stochastic neurons for conditional computation.
[281] 通过随机神经元估算或传播梯度以进行条件计算。

--------------------------------------------------

[282] arXiv preprint arXiv:1308.3432, 2013.
[282] ARXIV预印型ARXIV：1308.3432，2013。

--------------------------------------------------

[283] [3] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi.
[283] [3] Castels，Casels，Bo-Kyuong Ki和Shinkyok Choi。

--------------------------------------------------

[284] Ld-pruner: Efficient pruning of latent diffu- sion models using task-agnostic insights.
[284] LD-PRUNER：使用任务不合时宜的见解对潜在扩散模型的有效修剪。

--------------------------------------------------

[285] In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 821–830, 2024.
[285] 在IEEE/CVF计算机视觉和模式识别会议论文集，第821-830页，2024年。

--------------------------------------------------

[286] [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman.
[286] [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman.

--------------------------------------------------

[287] Maskgit: Masked generative image transformer.
[287] MaskGit：掩盖的生成图像变压器。

--------------------------------------------------

[288] In Conference on Computer Vision and Pattern Recognition, pages 11315–11325, 2022.
[288] 在计算机视觉和模式识别会议上，第11315–11325页，2022年。

--------------------------------------------------

[289] [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li.
[289] [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li.

--------------------------------------------------

[290] Pixart-α: Fast training of dif- fusion transformer for photorealistic text-to-image synthesis, 2023.
[290] PixArt-α：对影像学文本对图像合成的差异变压器的快速训练，2023年。

--------------------------------------------------

[291] [6] Javier Mart´ın Daniel Verd´u.
[291] [6]哈维尔·马特·丹尼尔·维尔德·乌。

--------------------------------------------------

[292] Flux.1 lite: Distilling flux1.dev for efficient text-to-image generation.
[292] Flux.1 Lite：蒸馏Flux1.Dev，以进行有效的文本对图像生成。

--------------------------------------------------

[293] 2024.
[293] 2024。

--------------------------------------------------

[294] [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo- pher R´e.
[294] [7] Tri Dao，Dan Fu，Stefano Ermon，Atri Rudra和Christo-Pher R´e。

--------------------------------------------------

[295] Flashattention: Fast and memory-efficient exact at- tention with io-awareness.
[295] 闪存：具有IO意识的快速和记忆效率精确。

--------------------------------------------------

[296] Advances in Neural Information Processing Systems, 35:16344–16359, 2022.
[296] 神经信息处理系统的进展，35：16344–16359，2022。

--------------------------------------------------

[297] [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
[297] [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

--------------------------------------------------

[298] Imagenet: A large-scale hierarchical image database.
[298] ImageNet：大规模分层图像数据库。

--------------------------------------------------

[299] In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
[299] 在2009年IEEE计算机视觉和模式识别会议上，第248-255页。

--------------------------------------------------

[300] Ieee, 2009.
[300] IEEE，2009年。

--------------------------------------------------

[301] [9] Prafulla Dhariwal and Alexander Nichol.
[301] [9] Prafulla Dhariwal和Alexander Nichol。

--------------------------------------------------

[302] Diffusion models beat gans on image synthesis.
[302] 扩散模型在图像合成上击败了gan。

--------------------------------------------------

[303] Advances in neural informa- tion processing systems, 34:8780–8794, 2021.
[303] 神经信息处理系统的进展，34：8780–8794，2021。

--------------------------------------------------

[304] [10] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
[304] [10] Maha Elbayad，Jiatao Gu，Edouard Grave和Michael Auli。

--------------------------------------------------

[305] Depth-adaptive transformer.
[305] 深度自适应变压器。

--------------------------------------------------

[306] arXiv preprint arXiv:1910.10073, 2019.
[306] ARXIV预印型ARXIV：1910.10073，2019。

--------------------------------------------------

[307] [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.
[307] [11] Patrick Esser，Sumith Kulal，Andreas Blatmann，Rahim Entezari，JonasMéuller，Harry Saini，Yam Levi，Dominik Lorenz，Axel Sauer，Frederic Boesel等。

--------------------------------------------------

[308] Scaling recti- fied flow transformers for high-resolution image synthesis.
[308] 用于高分辨率图像合成的缩放直流变压器。

--------------------------------------------------

[309] In Forty-first International Conference on Machine Learn- ing, 2024.
[309] 在第41个机器学习国际会议上，2024年。

--------------------------------------------------

[310] [12] Gongfan Fang, Xinyin Ma, and Xinchao Wang.
[310] [12] Gongfan Fang, Xinyin Ma, and Xinchao Wang.

--------------------------------------------------

[311] Structural pruning for diffusion models.
[311] 扩散模型的结构修剪。

--------------------------------------------------

[312] In Advances in Neural Infor- mation Processing Systems, 2023.
[312] 在神经信息处理系统的进展中，2023年。

--------------------------------------------------

[313] [13] Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, and Xin- chao Wang.
[313] [13] Go Varlaan Cann，Hogxis，Huzv Mealtrailharan，Jeffriov，Jeva Moolav和Xin，Chaa Caber。

--------------------------------------------------

[314] Maskllm: Learnable semi-structured sparsity for large language models.
[314] maskllm：大型语言模型的可学习半结构化稀疏性。

--------------------------------------------------

[315] arXiv preprint arXiv:2409.17481, 2024.
[315] Arxiv预印型ARXIV：2409.17481，2024。

--------------------------------------------------

[316] [14] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang.
[316] [14] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang.

--------------------------------------------------

[317] Scaling diffusion transformers to 16 bil- lion parameters.
[317] 将扩散变压器缩放到16个二元参数。

--------------------------------------------------

[318] arXiv preprint arXiv:2407.11633, 2024.
[318] ARXIV预印型ARXIV：2407.11633，2024。

--------------------------------------------------

[319] [15] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang.
[319] [15] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang.

--------------------------------------------------

[320] Dimba: Transformer- mamba diffusion models.
[320] DIMBA：变压器 -  Mamba扩散模型。

--------------------------------------------------

[321] arXiv preprint arXiv:2406.01159, 2024.
[321] ARXIV预印型ARXIV：2406.01159，2024。

--------------------------------------------------

[322] [16] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming- Ming Cheng, and Shuicheng Yan.
[322] [16] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming- Ming Cheng, and Shuicheng Yan.

--------------------------------------------------

[323] Editanything: Empower- ing unparalleled flexibility in image editing and generation.
[323] 编辑：在图像编辑和生成中赋予无与伦比的灵活性。

--------------------------------------------------

[324] In Proceedings of the 31st ACM International Conference on Multimedia, Demo track, 2023.
[324] 在第31届ACM国际多媒体会议上，演示曲目，2023年。

--------------------------------------------------

[325] [17] Emil Julius Gumbel.
[325] [17] Emil Julius Gumbel。

--------------------------------------------------

[326] Statistical theory of extreme values and some practical applications: a series of lectures.
[326] 极值和一些实际应用的统计理论：一系列讲座。

--------------------------------------------------

[327] US Gov- ernment Printing Office, 1954.
[327] 美国政府印刷办公室，1954年。

--------------------------------------------------

[328] [18] Song Han, Jeff Pool, John Tran, and William Dally.
[328] [18] Song Han，Jeff Pool，John Tran和William Dally。

--------------------------------------------------

[329] Learn- ing both weights and connections for efficient neural net- work.
[329] 学习重量和连接以进行有效的神经网络。

--------------------------------------------------

[330] Advances in neural information processing systems, 28, 2015.
[330] 神经信息处理系统的进展，2015年2月28日。

--------------------------------------------------

[331] [19] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.
[331] [19] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.

--------------------------------------------------

[332] Ptqd: Accurate post-training quantization for diffusion models.
[332] PTQD：扩散模型的准确培训量化。

--------------------------------------------------

[333] Advances in Neural Information Pro- cessing Systems, 36, 2024.
[333] 神经信息过程系统的进展，36，2024。

--------------------------------------------------

[334] [20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al.
[334] [20] Geoffrey Hinton，Orol Vinyals，Jeff Dean等。

--------------------------------------------------

[335] Distill- ing the knowledge in a neural network.
[335] 在神经网络中提炼知识。

--------------------------------------------------

[336] arXiv preprint arXiv:1503.02531, 2(7), 2015.
[336] ARXIV预印型ARXIV：1503.02531，2（7），2015年。

--------------------------------------------------

[337] [21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
[337] [21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

--------------------------------------------------

[338] LoRA: Low-rank adaptation of large language models.
[338] LORA：大型语言模型的低级改编。

--------------------------------------------------

[339] In In- ternational Conference on Learning Representations, 2022.
[339] 在2022年学习表征的国际会议上。

--------------------------------------------------

[340] [22] Eric Jang, Shixiang Gu, and Ben Poole.
[340] [22] Eric Jang, Shixiang Gu, and Ben Poole.

--------------------------------------------------

[341] Categorical reparameterization with gumbel-softmax.
[341] 使用Gumbel-Softmax进行分类重新聚集。

--------------------------------------------------

[342] arXiv preprint arXiv:1611.01144, 2016.
[342] ARXIV预印型ARXIV：1611.01144，2016。

--------------------------------------------------

[343] [23] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi.
[343] （23] Chostels和Shillongs是有福的。

--------------------------------------------------

[344] Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation.
[344] BK-SDM：结构压缩的稳定扩散，以实现有效的文本对图像生成。

--------------------------------------------------

[345] In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.
[345] 在基础模型@ ICML2023，2023的高效系统研讨会上。

--------------------------------------------------

[346] [24] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song.
[346] [24] Bo-Kyeong Kim，Geonmin Kim，Tae-Ho Kim，Thibault Castels，Shillong。

--------------------------------------------------

[347] Shortened llama: A simple depth pruning for large lan- guage models.
[347] 缩短的美洲驼：大型语言模型的简单深度修剪。

--------------------------------------------------

[348] arXiv preprint arXiv:2402.02834, 11, 2024.
[348] Arxiv预印型ARXIV：2402.02834，11，2024。

--------------------------------------------------

[349] [25] PKU-Yuan Lab and Tuzhan AI etc.
[349] [25] PKU-YUAN LAB和TUZHAN AI等。

--------------------------------------------------

[350] Open-sora-plan, 2024.
[350] 开放式计划，2024年。

--------------------------------------------------

[351] [26] Black Forest Labs.
[351] [26]黑森林实验室。

--------------------------------------------------

[352] FLUX, 2024.
[352] Flux，2024。

--------------------------------------------------

[353] [27] Youngwan Lee, Yong-Ju Lee, and Sung Ju Hwang.
[353] [27] Youngwan Lee, Yong-Ju Lee, and Sung Ju Hwang.

--------------------------------------------------

[354] Dit- pruner: Pruning diffusion transformer models for text-to- image synthesis using human preference scores.
[354] Dit-Pruner：使用人类偏好得分进行修剪扩散变压器模型，用于文本图像合成。

--------------------------------------------------

[355] 9
[355] 9

--------------------------------------------------

[356] [28] Youngwan Lee, Kwanyong Park, Yoorhim Cho, Yong-Ju Lee, and Sung Ju Hwang.
[356] [28] Youngwan Lee，Kwanyong Park，Yoorhim Cho，Yong Ju Lee和Sung Ju Hwang。

--------------------------------------------------

[357] Koala: self-attention mat- ters in knowledge distillation of latent diffusion models for memory-efficient and fast image synthesis.
[357] Koala：在潜在扩散模型的知识蒸馏中进行自我注意事项，以进行记忆效率和快速图像合成。

--------------------------------------------------

[358] arXiv e-prints, pages arXiv–2312, 2023.
[358] Arxiv电子打印，第2312页，2023年。

--------------------------------------------------

[359] [29] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He.
[359] [29] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He.

--------------------------------------------------

[360] Autoregressive image generation without vec- tor quantization.
[360] 自回归图像生成而无需量化量化。

--------------------------------------------------

[361] arXiv preprint arXiv:2406.11838, 2024.
[361] Arxiv预印型ARXIV：2406.11838，2024。

--------------------------------------------------

[362] [30] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
[362] [30] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.

--------------------------------------------------

[363] Q-diffusion: Quantizing diffusion models.
[363] Q扩散：量化扩散模型。

--------------------------------------------------

[364] In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 17535–17545, 2023.
[364] 在IEEE/CVF国际计算机VI-Sion会议论文集，第17535– 17545年，2023年。

--------------------------------------------------

[365] [31] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
[365] [31] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.

--------------------------------------------------

[366] Snap- fusion: Text-to-image diffusion model on mobile devices within two seconds.
[366] 快速融合：移动设备上的文本到图像扩散模型在两秒钟内。

--------------------------------------------------

[367] Advances in Neural Information Pro- cessing Systems, 36, 2024.
[367] 神经信息过程系统的进展，36，2024。

--------------------------------------------------

[368] [32] Shanchuan Lin, Anran Wang, and Xiao Yang.
[368] [32] Shanchuan Lin, Anran Wang, and Xiao Yang.

--------------------------------------------------

[369] Sdxl- lightning: Progressive adversarial diffusion distillation.
[369] SDXL-闪电：进行性对抗扩散蒸馏。

--------------------------------------------------

[370] arXiv preprint arXiv:2402.13929, 2024.
[370] ARXIV预印型ARXIV：2402.13929，2024。

--------------------------------------------------

[371] [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
[371] [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.

--------------------------------------------------

[372] Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
[372] DPM-Solver：在大约10个步骤中进行扩散概率模型采样的快速ODE求解器。

--------------------------------------------------

[373] Advances in Neural Information Processing Systems, 35:5775–5787, 2022.
[373] 神经信息处理系统的进展，35：5775–5787，2022。

--------------------------------------------------

[374] [34] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.
[374] [34] Nanye Ma，Michael S Albergo。

--------------------------------------------------

[375] Sit: Explor- ing flow and diffusion-based generative models with scalable interpolant transformers.
[375] SIT：具有可扩展的插值变压器的探索流量和基于扩散的生成模型。

--------------------------------------------------

[376] arXiv preprint arXiv:2401.08740, 2024.
[376] ARXIV预印型ARXIV：2401.08740，2024。

--------------------------------------------------

[377] [35] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang.
[377] [35] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang.

--------------------------------------------------

[378] Learning-to-cache: Accelerating diffusion trans- former via layer caching, 2024.
[378] 学习到缓存：通过层缓存加速扩散跨前期，2024年。

--------------------------------------------------

[379] [36] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.
[379] [36] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.

--------------------------------------------------

[380] Shortgpt: Layers in large language models are more redun- dant than you expect.
[380] 短期：大语言模型中的层次比您预期的要重新使用。

--------------------------------------------------

[381] arXiv preprint arXiv:2403.03853, 2024.
[381] Arxiv预印型ARXIV：2403.03853，2024。

--------------------------------------------------

[382] [37] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
[382] [37] Pavlo Molchanov，Stephen Tyree，Tero Karras，Timo Aila和Jan Kautz。

--------------------------------------------------

[383] Pruning convolutional neural networks for re- source efficient inference.
[383] 修剪卷积神经网络，以提高推理。

--------------------------------------------------

[384] arXiv preprint arXiv:1611.06440, 2016.
[384] Arxiv预印型ARXIV：1611.06440，2016。

--------------------------------------------------

[385] [38] Zanlin Ni, Yulin Wang, Renping Zhou, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Shiji Song, Yuan Yao, and Gao Huang.
[385] [38] Zanlin Ni, Yulin Wang, Renping Zhou, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Shiji Song, Yuan Yao, and Gao Huang.

--------------------------------------------------

[386] Revisiting non-autoregressive transformers for efficient im- age synthesis.
[386] 重新审视非自动进取的变压器，以进行有效的进一步综合。

--------------------------------------------------

[387] In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7007– 7016, 2024.
[387] 在IEEE/CVF计算机视觉和模式识别会议论文集，第7007– 7016页，2024年。

--------------------------------------------------

[388] [39] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim.
[388] [39] Byeongjun Park，Sangmin Woo，Hyojun Go，Jin-Young Kim和Changick Kim。

--------------------------------------------------

[389] Denoising task routing for diffusion models.
[389] 扩散模型的剥落任务路由。

--------------------------------------------------

[390] arXiv preprint arXiv:2310.07138, 2023.
[390] Arxiv预印型ARXIV：2310.07138，2023。

--------------------------------------------------

[391] [40] William Peebles and Saining Xie.
[391] [40]威廉·皮布尔斯（William Peebles）和夏威尔（Xie）。

--------------------------------------------------

[392] Scalable diffusion models with transformers.
[392] 具有变压器的可扩展模型。

--------------------------------------------------

[393] In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 4195–4205, 2023.
[393] 在IEEE/CVF全国计算机视觉会议论文集，第4195–4205页，2023年。

--------------------------------------------------

[394] [41] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam San- toro.
[394] [41] David Raposo，Sam Ritter，Blake Richards，Timothy Lillicrap，Peter Conway Humphreys和Adam Santoro。

--------------------------------------------------

[395] Mixture-of-depths: Dynamically allocating com- pute in transformer-based language models.
[395] 深入的混合物：基于变压器的语言模型动态分配计算。

--------------------------------------------------

[396] arXiv preprint arXiv:2404.02258, 2024.
[396] ARXIV预印型ARXIV：2404.02258，2024。

--------------------------------------------------

[397] [42] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.
[397] [42] Adriana Romero，Nicolas Ballas，Samira Jewish Kahou，Antoine Chassang，Carlo Gatta和Yoshua Bengio。

--------------------------------------------------

[398] Fitnets: Hints for thin deep nets.
[398] fitnets：薄网的提示。

--------------------------------------------------

[399] arXiv preprint arXiv:1412.6550, 2014.
[399] Arxiv预印型ARXIV：1412.6550，2014。

--------------------------------------------------

[400] [43] Tim Salimans and Jonathan Ho.
[400] [43]蒂姆·萨利曼斯（Tim Salimans）和乔纳森·何（Jonathan Ho）。

--------------------------------------------------

[401] Progressive distillation for fast sampling of diffusion models.
[401] 用于快速采样扩散模型的进行性蒸馏。

--------------------------------------------------

[402] arXiv preprint arXiv:2202.00512, 2022.
[402] ARXIV预印型ARXIV：2202.00512，2022。

--------------------------------------------------

[403] [44] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.
[403] [44] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.

--------------------------------------------------

[404] Post-training quantization on diffusion models.
[404] 扩散模型上的训练后量化。

--------------------------------------------------

[405] In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 1972–1981, 2023.
[405] 在IEEE/CVF计算机录像和模式识别会议论文集，第1972- 1981年，2023年。

--------------------------------------------------

[406] [45] Jiaming Song, Chenlin Meng, and Stefano Ermon.
[406] [45] Jiaming Song, Chenlin Meng, and Stefano Ermon.

--------------------------------------------------

[407] Denoising diffusion implicit models.
[407] 剥离扩散隐式模型。

--------------------------------------------------

[408] arXiv preprint arXiv:2010.02502, 2020.
[408] ARXIV预印型ARXIV：2010.02502，2020。

--------------------------------------------------

[409] [46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
[409] [46]歌曲，Praflulla Dharill，Mark Chen和Ilio Suskver。

--------------------------------------------------

[410] Consistency models.
[410] 一致性模型。

--------------------------------------------------

[411] arXiv preprint arXiv:2303.01469, 2023.
[411] Arxiv预印型ARXIV：2303.01469，2023。

--------------------------------------------------

[412] [47] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu.
[412] [47] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu.

--------------------------------------------------

[413] Massive activations in large language models.
[413] 大语言模型中的大量激活。

--------------------------------------------------

[414] arXiv preprint arXiv:2402.17762, 2024.
[414] Arxiv预印型ARXIV：2402.17762，2024。

--------------------------------------------------

[415] [48] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu.
[415] [48] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu.

--------------------------------------------------

[416] Dim: Diffusion mamba for efficient high-resolution image synthesis.
[416] DIM：扩散MAMBA，用于有效的高分辨率图像合成。

--------------------------------------------------

[417] arXiv preprint arXiv:2405.14224, 2024.
[417] ARXIV预印型ARXIV：2405.14224，2024。

--------------------------------------------------

[418] [49] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li- wei Wang.
[418] [49] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Li- wei Wang.

--------------------------------------------------

[419] Visual autoregressive modeling: Scalable image generation via next-scale prediction.
[419] 视觉自回归建模：可扩展图像通过换句话预测。

--------------------------------------------------

[420] 2024.
[420] 2024。

--------------------------------------------------

[421] [50] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang.
[421] [50] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang.

--------------------------------------------------

[422] U-dits: Downsample tokens in u-shaped diffusion transformers.
[422] U-Dits：U形扩散变压器中的下样品令牌。

--------------------------------------------------

[423] arXiv preprint arXiv:2405.02730, 2024.
[423] Arxiv预印型ARXIV：2405.02730，2024。

--------------------------------------------------

[424] [51] Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, and Jun Zhu.
[424] [51] Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, and Jun Zhu.

--------------------------------------------------

[425] Sparsedm: Toward sparse efficient diffusion models.
[425] SparsedM：朝着稀疏的有效扩散模型。

--------------------------------------------------

[426] arXiv preprint arXiv:2404.10445, 2024.
[426] ARXIV预印型ARXIV：2404.10445，2024。

--------------------------------------------------

[427] [52] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han.
[427] [52] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han.

--------------------------------------------------

[428] Sana: Ef- ficient high-resolution image synthesis with linear diffusion transformers.
[428] SANA：具有线性扩散变压器的效率高分辨率图像合成。

--------------------------------------------------

[429] arXiv preprint arXiv:2410.10629, 2024.
[429] ARXIV预印型ARXIV：2410.10629，2024。

--------------------------------------------------

[430] [53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run- sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming- Hsuan Yang.
[430] [53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run- sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming- Hsuan Yang.

--------------------------------------------------

[431] Diffusion models: A comprehensive survey of methods and applications.
[431] 扩散模型：对方法和应用的全面调查。

--------------------------------------------------

[432] ACM Computing Surveys, 56(4): 1–39, 2023.
[432] ACM计算调查，56（4）：1–39，2023。

--------------------------------------------------

[433] [54] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui.
[433] [54] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui.

--------------------------------------------------

[434] Width & depth pruning for vision transformers.
[434] 视觉变压器的宽度和深度修剪。

--------------------------------------------------

[435] In Conference on Artificial Intelligence (AAAI), 2022.
[435] 在人工智能会议上（AAAI），2022年。

--------------------------------------------------

[436] [55] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen.
[436] [55] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen.

--------------------------------------------------

[437] Inpaint anything: Segment anything meets image inpainting.
[437] 涂漆的任何东西：段的任何东西都符合图像覆盖。

--------------------------------------------------

[438] arXiv preprint arXiv:2304.06790, 2023.
[438] ARXIV预印型ARXIV：2304.06790，2023。

--------------------------------------------------

[439] [56] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu.
[439] [翻译失败]

--------------------------------------------------

[440] Laptop-diff: Layer pruning and normalized dis- 10
[440] 笔记本电脑木：修剪和标准化的层

--------------------------------------------------

[441] tillation for compressing diffusion models.
[441] 用于压缩扩散模型的耕作。

--------------------------------------------------

[442] arXiv preprint arXiv:2404.11098, 2024.
[442] Arxiv预印型ARXIV：2404.11098，2024。

--------------------------------------------------

[443] [57] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You.
[443] [57] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You.

--------------------------------------------------

[444] Real-time video generation with pyramid attention broad- cast.
[444] 带有金字塔注意的实时视频发行广泛。

--------------------------------------------------

[445] arXiv preprint arXiv:2408.12588, 2024.
[445] ARXIV预印型ARXIV：2408.12588，2024。

--------------------------------------------------

[446] [58] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou.
[446] [58] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou.

--------------------------------------------------

[447] Mobilediffusion: Subsecond text-to-image generation on mobile devices.
[447] 动员iffusion：移动设备上的次要文本对图像生成。

--------------------------------------------------

[448] arXiv preprint arXiv:2311.16567, 2023.
[448] Arxiv预印型ARXIV：2311.16567，2023。

--------------------------------------------------

[449] [59] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.
[449] [59] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.

--------------------------------------------------

[450] Open-sora: Democratizing efficient video production for all, 2024.
[450] 开放式：将所有人的高效视频制作民主化，2024年。

--------------------------------------------------

[451] 11
[451] 11

--------------------------------------------------

[452] TinyFusion: Diffusion Transformers Learned Shallow Supplementary Material 6.
[452] 小型灌注：扩散变压器学到的浅补充材料6。

--------------------------------------------------

[453] Experimental Details Models.
[453] 实验细节模型。

--------------------------------------------------

[454] Our experiments evaluate the effectiveness of three models: DiT-XL, MAR-Large, and SiT-XL.
[454] 我们的实验评估了三个模型的有效性：DIT-XL，MAR-LARGE和SIT-XL。

--------------------------------------------------

[455] Diffusion Transformers (DiTs), inspired by Vision Transformer (ViT) principles, process spatial inputs as sequences of patches.
[455] 扩散变压器（DIT），灵感来自视觉变压器（VIT）原理，过程空间输入作为斑块序列。

--------------------------------------------------

[456] The DiT-XL model features 28 transformer layers, a hidden size of 1152, 16 attention heads, and a 2 × 2 patch size.
[456] DIT-XL型号具有28个变压器层，隐藏尺寸为1152、16个注意力头和2×2个贴片大小。

--------------------------------------------------

[457] It employs adaptive layer normalization (AdaLN) to improve training stability, comprising 675 million parameters and trained for 1400 epochs.
[457] 它采用自适应层归一化（ADALN）来提高训练稳定性，包括6.75亿个参数，并接受了1400个时期的培训。

--------------------------------------------------

[458] Masked Autoregressive models (MARs) are diffusion transformer variants tailored for au- toregressive image generation.
[458] 蒙面自回旋模型（MARS）是针对au侵蚀图像生成的扩散变压器变种。

--------------------------------------------------

[459] They utilize a continuous- valued diffusion loss framework to generate high-quality outputs without discrete tokenization.
[459] 他们利用一个连续的扩散损失框架来生成高质量的输出而无需离散的令牌化。

--------------------------------------------------

[460] The MAR-Large model includes 32 transformer layers, a hidden size of 1024, 16 attention heads, and bidirectional attention.
[460] MAR-LARGE模型包括32个变压器层，隐藏大小为1024，16个注意力头和双向注意。

--------------------------------------------------

[461] Like DiT, it incorporates AdaLN for stable training and effective to- ken modeling, with 479 million parameters trained over 400 epochs.
[461] 像DIT一样，它将Adaln纳入了稳定的训练和有效的建模，其中4.79亿个参数训练了400多个时期。

--------------------------------------------------

[462] Finally, Scalable Interpolant Transformers (SiTs) extend the DiT framework by introducing a flow-based in- terpolant methodology, enabling more flexible bridging be- tween data and noise distributions.
[462] 最后，可扩展的插值插入变压器（SITS）通过引入基于流动的构造方法来扩展DIT框架，从而在数据和噪声分布之间实现了更灵活的桥接。

--------------------------------------------------

[463] While architecturally identical to DiT-XL, the SiT-XL model leverages this inter- polant approach to facilitate modular experimentation with interpolant selection and sampling dynamics.
[463] 尽管在架构上与DIT-XL相同，但SIT-XL模型利用了这种间间方法来促进使用插值选择和采样动力学的模块化实验。

--------------------------------------------------

[464] Datasets.
[464] 数据集。

--------------------------------------------------

[465] We prepared the ImageNet 256 × 256 dataset by applying center cropping and adaptive resizing to main- tain the original aspect ratio and minimize distortion.
[465] 我们通过施加中心裁剪和适应性调整大小以使原始长宽比并最大程度地减少失真来准备Imagenet 256×256数据集。

--------------------------------------------------

[466] The images were then normalized to a mean of 0.5 and a stan- dard deviation of 0.5.
[466] 然后将图像标准化为平均0.5，标准偏差为0.5。

--------------------------------------------------

[467] To augment the dataset, we applied random horizontal flipping with a probability of 0.5.
[467] 为了增加数据集，我们以0.5的概率应用随机水平翻转。

--------------------------------------------------

[468] To accelerate training without using Variational Autoencoder (VAE), we pre-extracted features from the images using a pre-trained VAE.
[468] 为了加速训练而无需使用各种自动编码器（VAE），我们使用预先训练的VAE从图像中预先提取特征。

--------------------------------------------------

[469] The images were mapped to their latent representations, normalized, and the resulting feature arrays were saved for direct use during training.
[469] 将图像映射到其潜在表示，并归一化，并保存所得的功能阵列在训练过程中直接使用。

--------------------------------------------------

[470] Training Details The training process began with obtain- ing pruned models using the proposed learnable pruning method as illustrated in Figure 12.
[470] 培训细节训练过程始于使用所提出的可学习修剪方法获得修剪模型，如图12所示。

--------------------------------------------------

[471] Pruning decisions were made by a joint optimization of pruning and weight updates through LoRA with a block size.
[471] 修剪决定是通过通过块大小的洛拉（Lora）进行修剪和重量更新的联合优化做出的。

--------------------------------------------------

[472] In practice, the block size is 2 for simplicity and the models were trained for 100 epochs, except for MAR, which was trained for 40 epochs.
[472] 实际上，除了MAR外，还为100个时期的训练了块大小为2，并且对模型进行了100个时期的训练，而Mar进行了40个时期的训练。

--------------------------------------------------

[473] To enhance post-pruning performance, the Masked Knowl- edge Distillation (RepKD) method was employed during the recovery phase to transfer knowledge from teacher mod- 0 2000 4000 6000 8000 10000 Train iterations 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Layer Index in DiT-XL Figure 9.
[473] 为了提高灌木后的性能，在恢复阶段采用了掩盖的知识蒸馏（REPKD）方法，以从教师mod- 0 2000 4000 4000 4000 4000 8000 10000火车迭代转移知识0 1 2 3 4 5 6 5 6 7 8 9 10 11 12 13 13 13 14 15 15 15 17 17 17 17 19 20 22 22 22 22 22 22 22 22 22 22 22 27 27 27 27 27 27层在Dit-XL图9中。

--------------------------------------------------

[474] 1:2 Pruning Decisions 0 2000 4000 6000 8000 10000 Train iterations 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Layer Index in DiT-XL Figure 10.
[474] 1：2修剪决策0 2000 4000 6000 8000 10000火车迭代0 1 2 3 4 5 6 7 8 9 10 11 12 13 13 14 15 16 17 18 19 20 21 22 23 23 24 25 26 27 27 DIT-XL中的层指数图10。

--------------------------------------------------

[475] 2:4 Pruning Decisions 0 2000 4000 6000 8000 10000 Train iterations 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Layer Index in DiT-XL Figure 11.
[475] 2：4修剪决策0 2000 4000 6000 8000 10000火车迭代0 1 2 3 4 5 6 7 8 9 10 11 12 13 13 14 15 16 17 18 19 20 21 22 23 23 24 25 26 27 27 DIT-XL中的层指数图11。

--------------------------------------------------

[476] 7:14 Pruning Decisions els to pruned student models.
[476] 7:14修剪决策对修剪的学生模型。

--------------------------------------------------

[477] The RepKD approach aligns the output predictions and intermediate hidden states of the pruned and teacher models, with further details provided in the following section.
[477] REPKD方法将放置预测和修剪和教师模型的中间隐藏状态保持一致，并在下一节中提供了更多细节。

--------------------------------------------------

[478] Additionally, as Exponential Mov- ing Averages (EMA) are updated and used during image generation, an excessively small learning rate can weaken EMA’s effect, leading to suboptimal outcomes.
[478] 此外，随着指数移动平均值（EMA）在图像生成过程中进行了更新和使用，因此学习率过多会削弱EMA的效果，从而导致次优结果。

--------------------------------------------------

[479] To address this, a progressive learning rate scheduler was implemented to gradually halve the learning rate throughout training.
[479] 为了解决这个问题，实施了渐进式学习率调度程序，以逐步将整个培训的学习率逐渐减半。

--------------------------------------------------

[480] The 1
[480] 1

--------------------------------------------------

[481] Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Recoverability Estimation Local Block Joint Opt.
[481] 变压器层变压器层变压器层变压器层变压器层变压器层层变压器层恢复性估计局部块接头选择。

--------------------------------------------------

[482] 𝐒𝐡𝐚𝐫𝐞𝐝 𝚫𝚽  (LoRA/Full) 𝚽 Diff.
[482] 𝐒𝐡𝐚𝐫𝐞𝐝（lora/full）𝚽差异。

--------------------------------------------------

[483] Sampling Transformer Layer Transformer Layer Winner Decision 𝐦𝐢𝐧𝓛(𝚽+ 𝚫𝚽) Update Update Categorical Distribution ~ Transformer Layer Transformer Layer Differentiable Sampling of Candidate Solutions Figure 12.
[483] 采样变压器层变压器层获奖者决策𝐦𝐢𝐧𝓛（𝚽+ 𝚫𝚽）更新更新分类分布〜变压器层变压器层的候选解决方案可区分采样图12。

--------------------------------------------------

[484] Learnable depth pruning on a local block Transformer Block Transformer Block Transformer Block Transformer Block Learning the optimal sub-layers Transformer Block Transformer Block DiT TinyDiT Masked Distillation Massive Activation ( 𝑥> 𝑘⋅𝜎𝑥)  mask mask Hidden States Hidden States Figure 13.
[484] 可学习的深度修剪在本地块变压器块变压器块变压器块变压器块中学习最佳子层变压器块变压器块dit tinydit tinydit掩盖蒸馏大量激活（𝑥>𝑥>𝑘·庇护）掩码掩码隐藏状态隐藏状态图13。

--------------------------------------------------

[485] Masked knowledge distillation with 2:4 blocks.
[485] 带有2：4块的掩盖知识蒸馏。

--------------------------------------------------

[486] details of each hyperparameter are provided in Table 6.
[486] 表6中提供了每个超参数的详细信息。

--------------------------------------------------

[487] 7.
[487] 7。

--------------------------------------------------

[488] Visualization of Pruning Decisions Figures 9, 10 and 11 visualize the dynamics of pruning de- cisions during training for the 1:2, 2:4, and 7:14 pruning schemes.
[488] 修剪决策的可视化图9、10和11可视化在1：2、2：2：4和7:14修剪方案的训练过程中修剪效果的动力学。

--------------------------------------------------

[489] Different divisions lead to varying search spaces, which in turn result in various solutions.
[489] 不同的划分导致搜索空间不同，这又导致各种解决方案。

--------------------------------------------------

[490] For both the 1:2 and 2:4 schemes, good decisions can be learned in only one epoch, while the 7:14 scheme encounters optimization diffi- culty.
[490] 对于1：2和2：4方案，只能在一个时代中学习好的决定，而7:14方案遇到优化的构成。

--------------------------------------------------

[491] This is due to the  14 7  =3,432 candidates, which is too huge and thus cannot be adequately sampled within a single epoch.
[491] 这是由于14 7 = 3,432个候选人，这太大了，因此无法在单个时期内进行充分采样。

--------------------------------------------------

[492] Therefore, in practical applications, we use the 1:2 or 2:4 schemes for learnable layer pruning.
[492] 因此，在实际应用中，我们使用1：2或2：4的方案进行修剪。

--------------------------------------------------

[493] 8.
[493] 8。

--------------------------------------------------

[494] Details of Masked Knowledge Distillation Training Loss.
[494] 掩盖知识蒸馏培训损失的细节。

--------------------------------------------------

[495] This work deploys a standard knowledge distillation to learn a good student model by mimicking the pre-trained teacher.
[495] 这项工作通过模仿预训练的老师来部署标准知识蒸馏来学习一个好的学生模型。

--------------------------------------------------

[496] The loss function is formalized as: L = αKD · LKD + αDiff · LDiff + β · LRep (8) Here, LKD denotes the Mean Squared Error between the outputs of the student and teacher models.
[496] 损耗函数被形式化为：l =αkd·lkd +αdiff·ldiff +β·lrep（8），LKD表示学生和教师模型的输出之间的平均平方误差。

--------------------------------------------------

[497] LDiff repre- sents the original pre-training loss function.
[497] LDIFF代表原始的预训练损失函数。

--------------------------------------------------

[498] Finally, LRep corresponds to the masked distillation loss applied to the hidden states, as illustrated in Figure 13, which encourages alignment between the intermediate representations of the pruned model and the original model.
[498] 最后，LREP对应于应用于隐藏状态的掩盖蒸馏损失，如图13所示，这鼓励了修剪模型的中间表示与原始模型之间的一致性。

--------------------------------------------------

[499] The corresponding hyperparameters αKD, αDiff and αRep can be found in Ta- ble 6.
[499] 相应的超参数αKD，αDIFF和αREP可以在Table 6中找到。

--------------------------------------------------

[500] Hidden State Alignment.
[500] 隐藏的状态对齐。

--------------------------------------------------

[501] The masked distillation loss LRep is critical for aligning the intermediate representations of the student and teacher models.
[501] 掩盖的蒸馏损失LREP对于使学生和教师模型的中间表示至关重要。

--------------------------------------------------

[502] During the recovery phase, each layer of the student model is designed to repli- cate the output hidden states of a corresponding two-layer local block from the teacher model.
[502] 在恢复阶段，学生模型的每个层都旨在从教师模型中补充相应两层本地块的输出状态。

--------------------------------------------------

[503] Depth pruning does not alter the internal dimensions of the layers, enabling direct alignment without additional projection layers.
[503] 深度修剪不会改变层的内部维度，从而无需其他投影层即可直接对齐。

--------------------------------------------------

[504] For mod- els such as SiTs, where hidden state losses are more pro- nounced due to their unique interpolant-based architecture, a smaller coefficient β is applied to LRep to mitigate poten- tial training instability.
[504] 对于诸如SITS之类的模块化，由于其独特的基于插入式的架构，隐藏的状态损失更加呈现，因此将较小的系数β应用于LREP，以减轻潜在的训练不稳定性。

--------------------------------------------------

[505] The gradual decrease in β through- out training further reduces the risk of negative impacts on convergence.
[505] β跨训练的逐渐减少进一步降低了对收敛的负面影响的风险。

--------------------------------------------------

[506] Iterative Pruning and Distillation.
[506] 迭代修剪和蒸馏。

--------------------------------------------------

[507] Table 7 assesses the effectiveness of iterative pruning and teacher selection strategies.
[507] 表7评估了迭代修剪和教师选择策略的有效性。

--------------------------------------------------

[508] To obtain a TinyDiT-D7, we can either directly prune a DiT-XL with 28 layers or craft a TinyDiT-D14 first and then iteratively produce the small models.
[508] 要获得TinyDit-D7，我们可以直接修剪带有28层的DIT-XL，或者先制作TinyDit-D14，然后迭代产生小型模型。

--------------------------------------------------

[509] To investi- gate the impact of teacher choice and the method for obtain- ing the initial weights of the student model, we derived the initial weights of TinyDiT-D7 by pruning both a pre-trained model and a crafted intermediate model.
[509] 为了投资教师选择的影响以及获得学生模型的初始权重的方法，我们通过修剪预训练的模型和制作的中间模型来得出TinyDit-D7的初始权重。

--------------------------------------------------

[510] Subsequently, we used both the trained and crafted models as teachers for the pruned student models.
[510] 随后，我们将训练有素和制作的模型都用作修剪的学生模型的老师。

--------------------------------------------------

[511] Across four experimental set- tings, pruning and distilling using the crafted intermedi- ate model yielded the best performance.
[511] 在四个实验性设置中，使用精心设计的中介模型进行修剪和蒸馏产生了最佳性能。

--------------------------------------------------

[512] Notably, models pruned from the crafted model outperformed those pruned from the pre-trained model regardless of the teacher model employed in the distillation process.
[512] 值得注意的是，从制作模型中修剪的模型优于从预训练模型中修剪的模型，而不管蒸馏过程中使用的教师模型如何。

--------------------------------------------------

[513] We attribute this su- 2
[513] 我们归因于这个su-2

--------------------------------------------------

[514] Model Optimizer Cosine Sched.
[514] 模型优化器余弦计划。

--------------------------------------------------

[515] Teacher αKD αGT β Grad.
[515] 老师AKD AGT B毕业。

--------------------------------------------------

[516] Clip Pruning Configs DiT-D19 AdamW(lr=2e-4, wd=0.0) ηmin = 1e-4 DiT-XL 0.9 0.1 1e-2 →0 1.0 LoRA-1:2 DiT-D14 AdamW(lr=2e-4, wd=0.0 ηmin = 1e-4 DiT-XL 0.9 0.1 1e-2 →0 1.0 LoRA-1:2 DiT-D7 AdamW(lr=2e-4, wd=0.0) ηmin = 1e-4 DiT-D14 0.9 0.1 1e-2 →0 1.0 LoRA-1:2 SiT-D14 AdamW(lr=2e-4, wd=0.0) ηmin = 1e-4 SiT-XL 0.9 0.1 2e-4 →0 1.0 LoRA-1:2 MAR-D16 AdamW(lr=2e-4, wd=0.0) ηmin = 1e-4 MAR-Large 0.9 0.1 1e-2 →0 1.0 LoRA-1:2 Table 6.
[516] 剪辑修剪配置DIT-D19 ADAMW（LR = 2E-4，WD = 0.0）ηmin= 1E-4 DIT-XL 0.9 0.9 0.1 1E-2→0 1.0 Lora-1：2 DIT-DIT-D14 ADAMW（LR = 2E-4，WD = 2e-4，WD = 0.0ηmin= 0.0ηmin= 1e-4 dit-4 Dit-4 Dit-4 Dit-4 Dit-4 Dit-4 Dit-4 Dit-4 Dit-loror lor lor lor。 DIT-D7 ADAMW（LR = 2E-4，WD = 0.0）ηmin= 1e-4 Dit-D14 0.9 0.9 0.1 1E-2→0 1.0 Lora-1：2 Sit-D14 Adamw（LR = 2E-4，WD = 0.0） Adamw（LR = 2E-4，WD = 0.0）ηmin= 1E-4 Mar-large 0.9 0.1 1E-2→0 1.0 Lora-1：2表6。

--------------------------------------------------

[517] Training details and hyper-parameters for mask training Teacher Model Pruned From IS FID sFID Prec.
[517] 培训细节和超参数用于掩盖培训的教师模型，是FID SFID PREC。

--------------------------------------------------

[518] Recall DiT-XL/2 DiT-XL/2 29.46 56.18 26.03 0.43 0.51 DiT-XL/2 TinyDiT-D14 51.96 36.69 28.28 0.53 0.59 TinyDiT-D14 DiT-XL/2 28.30 58.73 29.53 0.41 0.50 TinyDiT-D14 TinyDiT-D14 57.97 32.47 26.05 0.55 0.60 Table 7.
[518] Recall DiT-XL/2 DiT-XL/2 29.46 56.18 26.03 0.43 0.51 DiT-XL/2 T​​inyDiT-D14 51.96 36.69 28.28 0.53 0.59 TinyDiT-D14 DiT-XL/2 28.30 58.73 29.53 0.41 0.50 TinyDiT-D14 TinyDiT-D14 57.97 32.47 26.05 0.55 0.60表7。

--------------------------------------------------

[519] TinyDiT-D7 is pruned and distilled with different teacher models for 10k, sample steps is 64, original weights are used for sampling rather than EMA.
[519] TinyDit-D7用不同的教师模型进行修剪和蒸馏，用于10K，样本步骤为64，原始权重用于采样而不是EMA。

--------------------------------------------------

[520] 100 200 300 400 500 Steps 3.0 3.5 4.0 4.5 5.0 5.5 FID Masked KD Finetune DiT-L/2 Scratch Figure 14.
[520] 100 200 300 400 500步3.0 3.5 4.0 4.5 5.0 5.0 5.5 FID蒙版KD Finetune Dit-L/2刮擦图14。

--------------------------------------------------

[521] FID and training steps.
[521] FID和培训步骤。

--------------------------------------------------

[522] perior performance to two factors: first, the crafted model’s structure is better adapted to knowledge distillation since it was trained using a distillation method; second, the reduced search space facilitates finding a more favorable initial state for the student model.
[522] 对两个因素的统治性能：首先，制作的模型的结构更好地适应了知识蒸馏，因为它是使用蒸馏方法训练的；其次，减少的搜索空间有助于为学生模型找到更有利的初始状态。

--------------------------------------------------

[523] 9.
[523] 9。

--------------------------------------------------

[524] Analytical Experiments Training Strategies Figure 14 illustrates the effective- ness of standard fine-tuning and knowledge distillation (KD), where we prune DiT-XL to 14 layers and then ap- ply various fine-tuning methods.
[524] 分析实验培训策略图14说明了标准微调和知识蒸馏（KD）的有效性，我们在其中将DIT-XL降至14层，然后将各种微调方法降至14层。

--------------------------------------------------

[525] Figure 3 presents the FID scores across 100K to 500K steps.
[525] 图3显示了在100k至500k步长的FID得分。

--------------------------------------------------

[526] It is evident that the standard fine-tuning method allows TinyDiT-D14 to achieve performance comparable to DiT-L while offering faster in- ference.
[526] 显然，标准的微调方法允许TinyDit-D14实现与DIT-L相当的性能，同时提供更快的信息。

--------------------------------------------------

[527] Additionally, we confirm the significant effective- ness of distillation, which enables the model to surpass DiT- L at just 100K steps and achieve better FID scores than the 500K standard fine-tuned TinyDiT-D14.
[527] 此外，我们确认了蒸馏的显着有效性，这使该模型能够在仅100k步骤中超过Dit-L，并且比500K标准的微型TinyDit-D14获得了更好的FID得分。

--------------------------------------------------

[528] This is because the distillation of hidden layers provides stronger supervision.
[528] 这是因为隐藏层的蒸馏提供了更强的监督。

--------------------------------------------------

[529] Further increasing the training steps to 500K leads to sig- nificantly better results.
[529] 进一步将训练步骤提高到500K，从而取得了更好的结果。

--------------------------------------------------

[530] Learning Rate IS FID sFID Prec.
[530] 学习率是FID SFID PREC。

--------------------------------------------------

[531] Recall lr=2e-4 207.27 3.73 5.04 0.8127 0.5401 lr=1e-4 194.31 4.10 5.01 0.8053 0.5413 lr=5e-5 161.40 6.63 6.69 0.7419 0.5705 Table 8.
[531] 回忆LR = 2E-4 207.27 3.73 5.04 0.8127 0.5401 LR = 1E-4 194.31 4.10 5.01 0.801 0.8053 0.5413 LR = 5E-5 161.40 6.63 6.63 6.69 6.69 0.7419 0.7419 0.5705表8。

--------------------------------------------------

[532] The effect of Learning rato for TinyDiT-D14 finetuning w/o knowledge distillation Learning Rate.
[532] 通过知识蒸馏学习率，学习拉托对TinyDit-D14的finetuning的效果。

--------------------------------------------------

[533] We also search on some key hyperparam- eters such as learning rates in Table 8.
[533] 我们还搜索了表8中的一些关键超帕拉姆语（例如学习率）。

--------------------------------------------------

[534] We identify the ef- fectiveness of lr=2e-4 and apply it to all models and exper- iments.
[534] 我们确定LR = 2E-4的效率，并将其应用于所有模型和实验。

--------------------------------------------------

[535] 10.
[535] 10。

--------------------------------------------------

[536] Visulization Figure 15 and 16 showcase the generated images from TinySiT-D14 and TinyMAR-D16, which were compressed from the official checkpoints.
[536] 可见的图15和16显示了从官方检查点压缩的Tinysit-D14和Tinymar-D16的生成图像。

--------------------------------------------------

[537] These models were trained using only 7% and 10% of the original pre-training costs, respectively, and were distilled using the proposed masked knowledge distillation method.
[537] 这些模型仅使用原始预训练成本的7％和10％培训，并使用拟议的蒙版知识蒸馏方法进行蒸馏。

--------------------------------------------------

[538] Despite compression, the models are capable of generating plausible results with only 50% of depth.
[538] 尽管有压缩，这些模型仍能够产生仅50％深度的合理结果。

--------------------------------------------------

[539] 11.
[539] 11。

--------------------------------------------------

[540] Limitations In this work, we explore a learnable depth pruning method to accelerate diffusion transformer models for conditional image generation.
[540] 这项工作的局限性，我们探索了一种可学习的深度修剪方法，以加速有条件图像生成的扩散变压器模型。

--------------------------------------------------

[541] As Diffusion Transformers have shown significant advancements in text-to-image generation, it is valuable to conduct a systematic analysis of the impact of layer removal within the text-to-image tasks.
[541] 由于扩散变压器在文本到图像生成方面已显示出显着的进步，因此对文本到图像任务中层拆卸的影响进行系统分析是有价值的。

--------------------------------------------------

[542] Additionally, there exist other interesting depth pruning strategies that need to be studied, such as more fine-grained pruning strate- gies that remove attention layers and MLP layers indepen- dently instead of removing entire transformer blocks.
[542] 此外，还有其他需要研究的有趣的深度修剪策略，例如更细粒度的修剪策略，这些策略会消除注意力层和MLP层，而不是删除整个变压器块。

--------------------------------------------------

[543] We leave these investigations for future work.
[543] 我们将这些调查留给未来的工作。

--------------------------------------------------

[544] 3
[544] 3

--------------------------------------------------

[545] Figure 15.
[545] 图15。

--------------------------------------------------

[546] Generated images from TinySiT-D14 Figure 16.
[546] 来自Tinysit-D14的产生图像图16。

--------------------------------------------------

[547] Generated images from TinyMAR-D16 4
[547] 蒂尼马尔-D16 4的产生图像4

--------------------------------------------------
