[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wangâ€  National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization, resulting in considerable infer- ence overhead in real-world applications.
[001] TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wangâ€  National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg Abstract Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with ex- cessive parameterization,å¯¼è‡´åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¤§é‡çš„æ¨è®ºå¼€é”€ã€‚

--------------------------------------------------

[002] In this work, we present TinyFusion, a depth pruning method designed to re- move redundant layers from diffusion transformers via end- to-end learning.
[002] åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TinyFusionï¼Œè¿™æ˜¯ä¸€ç§æ·±åº¦ä¿®å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æœ€ç»ˆå­¦ä¹ å°†å†—ä½™å±‚ä»æ‰©æ•£å˜å‹å™¨ä¸­ç§»åŠ¨ã€‚

--------------------------------------------------

[003] The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning.
[003] æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒåŸç†æ˜¯åˆ›å»ºå…·æœ‰é«˜å¯æ¢å¤æ€§çš„ä¿®å‰ªæ¨¡å‹ï¼Œä»è€Œä½¿å…¶åœ¨å¾®è°ƒåæ¢å¤äº†å¼ºåŠ²çš„æ€§èƒ½ã€‚

--------------------------------------------------

[004] To accom- plish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized pa- rameter to simulate future fine-tuning.
[004] ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯åŒºåˆ†çš„æŠ½æ ·æŠ€æœ¯ï¼Œä»¥ä½¿ä¿®å‰ªå¯å­¦ä¹ ï¼Œå¹¶ä¸åˆä½œçš„Pa-Rameteré…å¯¹ï¼Œä»¥æ¨¡æ‹Ÿæœªæ¥çš„å¾®è°ƒã€‚

--------------------------------------------------

[005] While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning perfor- mance of pruned models.
[005] è™½ç„¶å…ˆå‰çš„å·¥ä½œé‡ç‚¹æ˜¯æœ€å¤§ç¨‹åº¦åœ°å‡å°‘ä¿®å‰ªåçš„æŸå¤±æˆ–é”™è¯¯ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•æ˜¾å¼æ¨¡å‹å¹¶ä¼˜åŒ–äº†ä¿®å‰ªæ¨¡å‹çš„é¢„å…ˆè°ƒæ•´åè°ƒèŠ‚æ€§èƒ½ã€‚

--------------------------------------------------

[006] Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing exist- ing importance-based and error-based methods.
[006] å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¯å­¦ä¹ çš„èŒƒå¼ä¸ºæ‰©æ•£å˜å‹å™¨çš„å±‚ä¿®å‰ªæä¾›äº†å¯è§‚çš„å¥½å¤„ï¼Œè¶…è¿‡äº†åŸºäºé‡è¦æ€§å’ŒåŸºäºé”™è¯¯çš„æ–¹æ³•çš„å­˜åœ¨ã€‚

--------------------------------------------------

[007] Addition- ally, TinyFusion exhibits strong generalization across di- verse architectures, such as DiTs, MARs, and SiTs.
[007] è¡¥å……è¯´ï¼Œå¾®å°çš„çŒæ³¨è¡¨ç°å‡ºè·¨ä¸ªæ€§ä½“ç³»ç»“æ„ï¼ˆä¾‹å¦‚DITï¼ŒMARSå’ŒSITSï¼‰çš„å¼ºçƒˆæ¦‚æ‹¬ã€‚

--------------------------------------------------

[008] Ex- periments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre- training cost, achieving a 2Ã— speedup with an FID score of 2.86, outperforming competitors with comparable effi- ciency.
[008] DIT-XLçš„æ¦‚å¿µè¡¨æ˜ï¼ŒTinyFusionå¯ä»¥ä»¥ä¸åˆ°é¢„è®­ç»ƒæˆæœ¬çš„7ï¼…çš„7ï¼…æ¥åˆ¶ä½œæµ…æ‰©æ•£å˜å‹å™¨ï¼Œä»¥2.86çš„FIDå¾—åˆ†è¾¾åˆ°2Ã—åŠ é€Ÿï¼Œä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œè¡¨ç°ä¼˜äºç›¸å½“çš„æ•ˆç‡ã€‚

--------------------------------------------------

[009] Code is available at https://github.com/ VainF/TinyFusion 1.
[009] ä»£ç å¯åœ¨https://github.com/ vainf/tinyfusion 1ä¸­æ‰¾åˆ°ã€‚

--------------------------------------------------

[010] Introduction Diffusion Transformers have emerged as a cornerstone ar- chitecture for generative tasks, achieving notable success in areas such as image [11, 26, 40] and video synthe- sis [25, 59].
[010] å¼•è¨€æ‰©æ•£å˜å‹å™¨å·²æˆä¸ºç”Ÿæˆä»»åŠ¡çš„åŸºçŸ³ - åœ¨å›¾åƒ[11ã€26ã€40]å’Œè§†é¢‘ç»¼åˆ[25ã€59]ç­‰é¢†åŸŸå–å¾—äº†æ˜¾ç€æˆåŠŸã€‚

--------------------------------------------------

[011] This success has also led to the widespread availability of high-quality pre-trained models on the Inter- net, greatly accelerating and supporting the development of various downstream applications [5, 16, 53, 55].
[011] è¿™ä¸€æˆåŠŸè¿˜å¯¼è‡´äº†äº’è”ç½‘ä¸Šé«˜è´¨é‡é¢„è®­ç»ƒçš„æ¨¡å‹çš„å¹¿æ³›å¯ç”¨æ€§ï¼Œæå¤§åœ°åŠ é€Ÿå’Œæ”¯æŒäº†å„ç§ä¸‹æ¸¸åº”ç”¨ç¨‹åºçš„å¼€å‘[5ï¼Œ16ï¼Œ53ï¼Œ55]ã€‚

--------------------------------------------------

[012] However, pre-trained diffusion transformers usually come with con- *Equal contribution â€ Corresponding author Transformer Layer Transformer Layer Transformer Layer Transformer Layer Differentiable Sampling of Layer Mask ğ–’  Recoverability Estimation with ğš«ğš½  1 0 1 0 Local Block Joint Opt.
[012] ä½†æ˜¯ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨é€šå¸¸å…·æœ‰ç›¸ç­‰çš„è´¡çŒ®â€ ç›¸åº”çš„ä½œè€…å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å±‚å±‚æ©æ¨¡çš„å¯æ¢å¤æ€§ä¼°è®¡çš„å¯æ¢å¤æ€§ä¼°è®¡ï¼Œğš«ğš½1 0 1 0å±€éƒ¨å—æ¥å¤´é€‰æ‹©ã€‚

--------------------------------------------------

[013] Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer Transformer Layer ğš«ğš½  (LoRA/Full) ğ¦ğ¢ğ§ğ–’,ğš«ğš½ğ“›(ğ’™, ğš½+ ğš«ğš½, ğ–’) ğ“ğŸ ğ“ğŸ ğ“ğŸ‘ ğ“ğŸ’ Figure 1.
[013] å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚ğš«ğš½ï¼ˆlora/fullï¼‰ğ¦ğ¢ğ§ğ–’ï¼Œğš«ğš½ğ“›ï¼ˆğ’™ï¼Œğš½+ ğš«ğš½ï¼Œğ–’ï¼Œğ–’ï¼‰ğ“ğŸ ğ“ğŸ ğ“ğŸ‘ ğ“ğŸ‘å›¾1ã€‚

--------------------------------------------------

[014] This work presents a learnable approach for pruning the depth of pre-trained diffusion transformers.
[014] è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºä¿®å‰ªé¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨çš„æ·±åº¦ã€‚

--------------------------------------------------

[015] Our method simulta- neously optimizes a differentiable sampling process of layer masks and a weight update to identify a highly recoverable solution, en- suring that the pruned model maintains competitive performance after fine-tuning.
[015] æˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶ä¼˜åŒ–äº†å±‚æ©æ¨¡çš„å¯åŒºåˆ†é‡‡æ ·è¿‡ç¨‹å’Œé‡é‡æ›´æ–°ï¼Œä»¥è¯†åˆ«é«˜åº¦å¯æ¢å¤çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ç¡®ä¿ä¿®å‰ªæ¨¡å‹åœ¨å¾®è°ƒåä¿æŒç«äº‰æ€§èƒ½ã€‚

--------------------------------------------------

[016] siderable inference costs due to the huge parameter scale, which poses significant challenges for deployment.
[016] ç”±äºå·¨å¤§çš„å‚æ•°é‡è¡¨è€Œå¯¼è‡´çš„æ¨ç†æˆæœ¬å¾ˆå¤§ï¼Œè¿™ç»™éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚

--------------------------------------------------

[017] To re- solve this problem, there has been growing interest from both the research community and industry in developing lightweight models [12, 23, 32, 58].
[017] ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶ç¤¾åŒºå’Œè¡Œä¸šå¯¹å¼€å‘è½»è´¨æ¨¡å‹çš„å…´è¶£è¶Šæ¥è¶Šå¤§[12ï¼Œ23ï¼Œ32ï¼Œ58]ã€‚

--------------------------------------------------

[018] The efficiency of diffusion models is typically influ- enced by various factors, including the number of sampling steps [33, 43, 45, 46], operator design [7, 48, 52], compu- tational precision [19, 30, 44], network width [3, 12] and depth [6, 23, 36].
[018] æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡é€šå¸¸å—åˆ°å„ç§å› ç´ çš„å½±å“ï¼ŒåŒ…æ‹¬é‡‡æ ·æ­¥éª¤çš„æ•°é‡[33ï¼Œ43ï¼Œ45ï¼Œ46]ï¼Œæ“ä½œå‘˜è®¾è®¡[7ï¼Œ48ï¼Œ52]ï¼Œç»„åˆç²¾åº¦[19ï¼Œ30ï¼Œ44]ï¼Œç½‘ç»œå®½åº¦[3ï¼Œ12] [3ï¼Œ12]å’Œæ·±åº¦[6ï¼Œ23ï¼Œ36]ã€‚

--------------------------------------------------

[019] In this work, we focus on model compres- sion through depth pruning [36, 54], which removes entire layers from the network to reduce the latency.
[019] åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ·±åº¦ä¿®å‰ª[36ï¼Œ54]ä¸“æ³¨äºæ¨¡å‹çš„ç»„åˆï¼Œè¯¥æ¨¡å‹ä»ç½‘ç»œä¸­åˆ é™¤äº†æ•´ä¸ªå±‚ä»¥å‡å°‘å»¶è¿Ÿã€‚

--------------------------------------------------

[020] Depth prun- ing offers a significant advantage in practice: it can achieve a linear acceleration ratio relative to the compression rate on both parallel and non-parallel devices.
[020] æ·±åº¦ä¿®å‰ªåœ¨å®è·µä¸­æä¾›äº†é‡è¦çš„ä¼˜åŠ¿ï¼šç›¸å¯¹äºå¹¶è¡Œè®¾å¤‡å’Œéå¹³è¡Œè®¾å¤‡çš„å‹ç¼©ç‡ï¼Œå®ƒå¯ä»¥è¾¾åˆ°çº¿æ€§åŠ é€Ÿåº¦æ¯”ç‡ã€‚

--------------------------------------------------

[021] For example, as will be demonstrated in this work, while 50% width prun- ing [12] only yields a 1.6Ã— speedup, pruning 50% of the layers results in a 2Ã— speedup.
[021] ä¾‹å¦‚ï¼Œæ­£å¦‚è¿™é¡¹å·¥ä½œæ‰€è¯æ˜çš„é‚£æ ·ï¼Œå®½åº¦ä¸º50ï¼…çš„å®½åº¦[12]ä»…äº§ç”Ÿ1.6å€çš„åŠ é€Ÿï¼Œä¿®å‰ª50ï¼…çš„å±‚å¯¼è‡´2å€åŠ é€Ÿã€‚

--------------------------------------------------

[022] This makes depth pruning a flexible and practical method for model compression.
[022] è¿™ä½¿å¾—æ·±åº¦ä¿®å‰ªäº†æ¨¡å‹å‹ç¼©çš„çµæ´»ä¸”å®ç”¨çš„æ–¹æ³•ã€‚

--------------------------------------------------

[023] This work follows a standard depth pruning frame- work: unimportant layers are first removed, and the pruned model is then fine-tuned for performance recovery.
[023] è¿™é¡¹å·¥ä½œéµå¾ªæ ‡å‡†çš„æ·±åº¦ä¿®å‰ªæ¡†æ¶ - é¦–å…ˆè¦åˆ é™¤ä¸é‡è¦çš„å±‚ï¼Œç„¶åå¯¹ä¿®å‰ªæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ€§èƒ½æ¢å¤ã€‚

--------------------------------------------------

[024] In the literature, depth pruning techniques designed for dif- fusion transformers or general transformers primarily fo- cus on heuristic approaches, such as carefully designed importance scores [6, 36] or manually configured pruning 1 arXiv:2412.01199v1  [cs.CV]  2 Dec 2024
[024] åœ¨æ–‡çŒ®ä¸­ï¼Œè®¾è®¡ç”¨äºå·®å¼‚å˜å‹å™¨æˆ–é€šç”¨å˜å‹å™¨è®¾è®¡çš„æ·±åº¦ä¿®å‰ªæŠ€æœ¯ä¸»è¦æ˜¯åŸºäºå¯å‘å¼æ–¹æ³•ï¼Œä¾‹å¦‚ç²¾å¿ƒè®¾è®¡çš„é‡è¦æ€§å¾—åˆ†[6ï¼Œ36]æˆ–æ‰‹åŠ¨é…ç½®çš„ä¿®å‰ª1 arxivï¼š2412.011999v1 [cs.cv] [cs.cv] [cs.cv] 2024å¹´12æœˆ2æ—¥2024å¹´12æœˆ2æ—¥

--------------------------------------------------

[025] schemes [23, 54].
[025] æ–¹æ¡ˆ[23ï¼Œ54]ã€‚

--------------------------------------------------

[026] These methods adhere to a loss min- imization principle [18, 37], aiming to identify solutions that maintain low loss or error after pruning.
[026] è¿™äº›æ–¹æ³•éµå®ˆæŸå¤±æœ€å°åŸåˆ™[18ï¼Œ37]ï¼Œæ—¨åœ¨è¯†åˆ«åœ¨ä¿®å‰ªåä¿æŒä½æŸå¤±æˆ–é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚

--------------------------------------------------

[027] This paper investigates the effectiveness of this widely used principle in the context of depth compression.
[027] æœ¬æ–‡åœ¨æ·±åº¦å‹ç¼©çš„èƒŒæ™¯ä¸‹ç ”ç©¶äº†è¯¥å¹¿æ³›ä½¿ç”¨åŸç†çš„æœ‰æ•ˆæ€§ã€‚

--------------------------------------------------

[028] Through experiments, we examined the relationship between calibration loss ob- served post-pruning and the performance after fine-tuning.
[028] é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æ£€æŸ¥äº†æ ¡å‡†æŸå¤±æ¸—é€åçš„åå»¶æœŸä¸å¾®è°ƒåçš„æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚

--------------------------------------------------

[029] This is achieved by extensively sampling 100,000 models via random pruning, exhibiting different levels of calibra- tion loss in the searching space.
[029] è¿™æ˜¯é€šè¿‡é€šè¿‡éšæœºä¿®å‰ªè¿›è¡Œå¹¿æ³›é‡‡æ ·100,000æ¬¾æ¨¡å‹æ¥å®ç°çš„ï¼Œåœ¨æœç´¢ç©ºé—´ä¸­è¡¨ç°å‡ºä¸åŒæ°´å¹³çš„ç¢³çº¤ç»´æŸå¤±ã€‚

--------------------------------------------------

[030] Based on this, we analyzed the effectiveness of existing pruning algorithms, such as the feature similarity [6, 36] and sensitivity analysis [18], which indeed achieve low calibration losses in the solution space.
[030] åŸºäºæ­¤ï¼Œæˆ‘ä»¬åˆ†æäº†ç°æœ‰ä¿®å‰ªç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚ç‰¹å¾ç›¸ä¼¼æ€§[6ï¼Œ36]å’Œçµæ•åº¦åˆ†æ[18]ï¼Œå®ƒä»¬ç¡®å®åœ¨è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­å®ç°äº†ä½æ ¡å‡†æŸå¤±ã€‚

--------------------------------------------------

[031] However, the performance of all these models after fine- tuning often falls short of expectations.
[031] ä½†æ˜¯ï¼Œç²¾ç»†è°ƒæ•´åæ‰€æœ‰è¿™äº›æ¨¡å‹çš„æ€§èƒ½é€šå¸¸éƒ½æ²¡æœ‰æœŸæœ›ã€‚

--------------------------------------------------

[032] This indicates that the loss minimization principle may not be well-suited for diffusion transformers.
[032] è¿™è¡¨æ˜æœ€å°åŒ–åŸç†å¯èƒ½ä¸é€‚åˆæ‰©æ•£å˜å‹å™¨ã€‚

--------------------------------------------------

[033] Building on these insights, we reassessed the underly- ing principles for effective layer pruning in diffusion trans- formers.
[033] åœ¨è¿™äº›è§è§£çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†åœ¨æ‰©æ•£å¼ä¼ è¾“ä¸­ä¿®å‰ªæœ‰æ•ˆå±‚çš„åŸºæœ¬åŸç†ã€‚

--------------------------------------------------

[034] Fine-tuning diffusion transformers is an extremely time-consuming process.
[034] å¾®è°ƒæ‰©æ•£å˜å‹å™¨æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶çš„è¿‡ç¨‹ã€‚

--------------------------------------------------

[035] Instead of searching for a model that minimizes loss immediately after pruning, we propose identifying candidate models with strong recoverability, en- abling superior post-fine-tuning performance.
[035] æˆ‘ä»¬æ²¡æœ‰åœ¨ä¿®å‰ªåç«‹å³å¯»æ‰¾å°†æŸå¤±ç«‹å³æœ€å°åŒ–çš„æ¨¡å‹ï¼Œè€Œæ˜¯æè®®è¯†åˆ«å…·æœ‰å¼ºå¤§å¯æ¢å¤æ€§ï¼Œä¼˜åŠ¿è¾ƒé«˜åè°ƒèŠ‚æ€§èƒ½çš„å€™é€‰æ¨¡å‹ã€‚

--------------------------------------------------

[036] Achieving this goal is particularly challenging, as it requires the in- tegration of two distinct processes, pruning and fine-tuning, which involve non-differentiable operations and cannot be directly optimized via gradient descent.
[036] å®ç°è¿™ä¸€ç›®æ ‡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹ä¸¤ä¸ªä¸åŒçš„è¿‡ç¨‹è¿›è¡Œä¿®å‰ªå’Œå¾®è°ƒï¼Œè¿™æ¶‰åŠéä¸åŒçš„æ“ä½œï¼Œå¹¶ä¸”ä¸èƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜åŒ–ã€‚

--------------------------------------------------

[037] To this end, we propose a learnable depth pruning method that effectively integrates pruning and fine-tuning.
[037] ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æ·±åº¦ä¿®å‰ªæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆæ•´åˆä¿®å‰ªå’Œå¾®è°ƒã€‚

--------------------------------------------------

[038] As shown in Figure 1, we model the pruning and fine- tuning of a diffusion transformer as a differentiable sam- pling process of layer masks [13, 17, 22], combined with a co-optimized weight update to simulate future fine-tuning.
[038] å¦‚å›¾1æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†æ‰©æ•£å˜å‹å™¨çš„ä¿®å‰ªå’Œç²¾ç»†è°ƒæ•´ä¸ºå±‚æ©æ¨¡çš„å¯åŒºåˆ†çš„Samå›ºå®šè¿‡ç¨‹[13ï¼Œ17ï¼Œ22]ï¼Œå¹¶ç»“åˆäº†åˆä½œçš„æƒé‡æ›´æ–°ï¼Œä»¥æ¨¡æ‹Ÿæœªæ¥çš„å¾®è°ƒã€‚

--------------------------------------------------

[039] Our objective is to iteratively refine this distribution so that networks with higher recoverability are more likely to be sampled.
[039] æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¿­ä»£åœ°å®Œå–„æ­¤åˆ†å¸ƒï¼Œä»¥ä¾¿æ›´æœ‰å¯èƒ½é‡‡æ ·å…·æœ‰è¾ƒé«˜å¯æ¢å¤æ€§çš„ç½‘ç»œã€‚

--------------------------------------------------

[040] This is achieved through a straightforward strat- egy: if a sampled pruning decision results in strong recover- ability, similar pruning patterns will have an increased prob- ability of being sampled.
[040] è¿™æ˜¯é€šè¿‡ç›´æ¥çš„ç­–ç•¥æ¥å®ç°çš„ï¼šå¦‚æœé‡‡æ ·ä¿®å‰ªå†³ç­–ä¼šå¯¼è‡´å¼ºå¤§çš„æ¢å¤èƒ½åŠ›ï¼Œé‚£ä¹ˆç›¸ä¼¼çš„ä¿®å‰ªæ¨¡å¼å°†å…·æœ‰æé«˜é‡‡æ ·çš„æ¦‚ç‡èƒ½åŠ›ã€‚

--------------------------------------------------

[041] This approach promotes the ex- ploration of potentially valuable solutions while disregard- ing less effective ones.
[041] è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†å¯¹æ½œåœ¨æœ‰ä»·å€¼çš„è§£å†³æ–¹æ¡ˆçš„æå‡ºï¼ŒåŒæ—¶åˆæ— è§†æ•ˆç‡è¾ƒä½çš„è§£å†³æ–¹æ¡ˆã€‚

--------------------------------------------------

[042] Additionally, the proposed method is highly efficient, and we demonstrate that a suitable solu- tion can emerge within a few training steps.
[042] æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œæˆ‘ä»¬è¯æ˜å¯ä»¥åœ¨å‡ ä¸ªè®­ç»ƒæ­¥éª¤ä¸­å‡ºç°åˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚

--------------------------------------------------

[043] To evaluate the effectiveness of the proposed method, we conduct extensive experiments on various transformer- based diffusion models, including DiTs [40], MARs [29], SiTs [34].
[043] ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹åŸºäºå˜å‹å™¨çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬DITS [40]ï¼ŒMARS [29]ï¼Œä½äº[34]ã€‚

--------------------------------------------------

[044] The learnable approach is highly efficient.
[044] å¯å­¦ä¹ çš„æ–¹æ³•éå¸¸æœ‰æ•ˆã€‚

--------------------------------------------------

[045] It is able to identify redundant layers in diffusion transform- ers with 1-epoch training on the dataset, which effectively crafts shallow diffusion transformers from pre-trained mod- els with high recoverability.
[045] å®ƒèƒ½å¤Ÿé€šè¿‡æ•°æ®é›†ä¸Šçš„1ä¸ªä¸Šè¿°è®­ç»ƒæ¥è¯†åˆ«æ‰©æ•£å˜æ¢ä¸­çš„å†—ä½™å±‚ï¼Œä»è€Œæœ‰æ•ˆåœ°ä»å…·æœ‰é«˜å¯æ¢å¤æ€§çš„é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­åˆ¶ä½œäº†æµ…æ‰©æ•£å˜å‹å™¨ã€‚

--------------------------------------------------

[046] For instance, while the models pruned by TinyFusion initially exhibit relatively high cal- ibration loss after removing 50% of layers, they recover quickly through fine-tuning, achieving a significantly more competitive FID score (5.73 vs. 22.28) compared to base- line methods that only minimize immediate loss, using just 1% of the pre-training cost.
[046] ä¾‹å¦‚ï¼Œå°½ç®¡åˆ é™¤50ï¼…çš„å±‚åï¼Œæœ€åˆè¢«TinyFusionä¿®å‰ªçš„æ¨¡å‹æœ€åˆè¡¨ç°å‡ºç›¸å¯¹è¾ƒé«˜çš„calæŸå¤±ï¼Œä½†ä¸ä»…ä½¿ç”¨é¢„å…ˆåŸ¹è®­çš„1ï¼…çš„åŸºæœ¬æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä»¬é€šè¿‡å¾®è°ƒå¾—åˆ†è¿…é€Ÿæ¢å¤ï¼Œè·å¾—äº†æ›´å…·ç«äº‰åŠ›çš„FIDå¾—åˆ†ï¼ˆ5.73 vs 22.28ï¼‰ï¼ˆ5.73å¯¹22.28ï¼‰ã€‚

--------------------------------------------------

[047] Additionally, we also ex- plore the role of knowledge distillation in enhancing re- coverability [20, 23] by introducing a MaskedKD variant.
[047] æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¼•å…¥maskedkdå˜ä½“æ¥è¡¨è¾¾çŸ¥è¯†è’¸é¦åœ¨å¢å¼ºå¯è¦†ç›–æ€§[20ï¼Œ23]ä¸­çš„ä½œç”¨ã€‚

--------------------------------------------------

[048] MaskedKD mitigates the negative impact of the massive or outlier activations [47] in hidden states, which can signifi- cantly affect the performance and reliability of fine-tuning.
[048] MaskEDKDå‡è½»äº†éšè—çŠ¶æ€ä¸­å¤§è§„æ¨¡æˆ–å¼‚å¸¸æ¿€æ´»çš„è´Ÿé¢å½±å“[47]ï¼Œè¿™å¯èƒ½ä¼šæ˜¾ç€å½±å“å¾®è°ƒçš„æ€§èƒ½å’Œå¯é æ€§ã€‚

--------------------------------------------------

[049] With MaskedKD, the FID score improves from 5.73 to 3.73 with only 1% of pre-training cost.
[049] å€ŸåŠ©MaskedKDï¼ŒFIDå¾—åˆ†ä»5.73æé«˜åˆ°3.73ï¼Œä»…å åŸ¹è®­å‰æˆæœ¬çš„1ï¼…ã€‚

--------------------------------------------------

[050] Extending the training to 7% of the pre-training cost further reduces the FID to 2.86, just 0.4 higher than the original model with doubled depth.
[050] å°†åŸ¹è®­æ‰©å¤§åˆ°7ï¼…çš„åŸ¹è®­å‰æˆæœ¬å°†FIDè¿›ä¸€æ­¥é™ä½åˆ°2.86ï¼Œä»…æ¯”åŸå§‹æ¨¡å‹é«˜åº¦å¢åŠ äº†0.4ã€‚

--------------------------------------------------

[051] Therefore, the main contribution of this work lies in a learnable method to craft shallow diffusion transformers from pre-trained ones, which explicitly optimizes the re- coverability of pruned models.
[051] å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•ï¼Œå¯ä»¥ä»é¢„è®­ç»ƒçš„æ–¹æ³•ä¸­åˆ¶ä½œæµ…æ‰©æ•£å˜å‹å™¨ï¼Œè¯¥æ–¹æ³•æ˜ç¡®ä¼˜åŒ–äº†ä¿®å‰ªæ¨¡å‹çš„å¯è¦†ç›–æ€§ã€‚

--------------------------------------------------

[052] The method is general for various architectures, including DiTs, MARs and SiTs.
[052] è¯¥æ–¹æ³•æ˜¯å„ç§æ¶æ„ï¼ŒåŒ…æ‹¬DITï¼ŒMARSå’ŒSITSçš„ä¸€èˆ¬æ–¹æ³•ã€‚

--------------------------------------------------

[053] 2.
[053] 2ã€‚

--------------------------------------------------

[054] Related Works Network Pruning and Depth Reduction.
[054] ç›¸å…³å·¥ä½œç½‘ç»œä¿®å‰ªå’Œæ·±åº¦å‡å°‘ã€‚

--------------------------------------------------

[055] Network prun- ing is a widely used approach for compressing pre-trained diffusion models by eliminating redundant parameters [3, 12, 31, 51].
[055] ç½‘ç»œä¿®å‰ªæ˜¯ä¸€ç§é€šè¿‡æ¶ˆé™¤å†—ä½™å‚æ•°æ¥å‹ç¼©é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›ä½¿ç”¨æ–¹æ³•[3ï¼Œ12ï¼Œ31ï¼Œ51]ã€‚

--------------------------------------------------

[056] Diff-Pruning [12] introduces a gradient- based technique to streamline the width of UNet, fol- lowed by a simple fine-tuning to recover the performance.
[056] DIFF-PRUNING [12]å¼•å…¥äº†ä¸€ç§åŸºäºæ¢¯åº¦çš„æŠ€æœ¯ï¼Œä»¥ç®€åŒ–UNETçš„å®½åº¦ï¼Œä»¥é€šè¿‡ç®€å•çš„å¾®è°ƒæ¥æ¢å¤æ€§èƒ½ã€‚

--------------------------------------------------

[057] SparseDM [51] applies sparsity to pre-trained diffusion models via the Straight-Through Estimator (STE) [2], achieving a 50% reduction in MACs with only a 1.22 in- crease in FID on average.
[057] Sparsedm [51]é€šè¿‡ç›´é€šä¼°è®¡é‡ï¼ˆSteï¼‰[2]å°†ç¨€ç–æ€§åº”ç”¨äºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨MACä¸­é™ä½äº†50ï¼…ï¼Œå¹³å‡FIDä»…1.22ä¸ªæŠ˜ç—•ã€‚

--------------------------------------------------

[058] While width pruning and spar- sity help reduce memory overhead, they often offer lim- ited speed improvements, especially on parallel devices like GPUs.
[058] è™½ç„¶ä¿®å‰ªå’Œå®½åº¦æœ‰åŠ©äºå‡å°‘å†…å­˜å¼€é”€ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šæä¾›é™åˆ¶çš„é€Ÿåº¦æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯¸å¦‚GPUä¹‹ç±»çš„å¹³è¡Œè®¾å¤‡ä¸Šã€‚

--------------------------------------------------

[059] Consequently, depth reduction has gained signifi- cant attention in the past few years, as removing entire lay- ers enables better speedup proportional to the pruning ra- tio [24, 27, 28, 36, 54, 56, 58].
[059] å› æ­¤ï¼Œåœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œæ·±åº¦çš„é™ä½å¼•èµ·äº†æ˜¾ç€å…³æ³¨ï¼Œå› ä¸ºåˆ é™¤æ•´ä¸ªå¤–è¡Œå¯ä»¥æ›´å¥½åœ°åŠ é€Ÿä¸ä¿®å‰ªra-tioæˆæ­£æ¯”[24ã€27ã€27ã€28ã€36ã€54ã€56ã€58]ã€‚

--------------------------------------------------

[060] Adaptive depth reduction techniques, such as MoD [41] and depth-aware transform- ers [10], have also been proposed.
[060] è¿˜æå‡ºäº†è‡ªé€‚åº”æ·±åº¦è¿˜åŸæŠ€æœ¯ï¼Œä¾‹å¦‚MOD [41]å’Œæ·±åº¦æ„ŸçŸ¥çš„è½¬åŒ–[10]ã€‚

--------------------------------------------------

[061] Despite these advances, most existing methods are still based on empirical or heuris- tic strategies, such as carefully designed importance crite- ria [36, 54], sensitivity analyses [18] or manually designed schemes [23], which often do not yield strong performance guarantee after fine-tuning.
[061] å°½ç®¡å–å¾—äº†è¿™äº›è¿›æ­¥ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶åŸºäºç»éªŒæˆ–å¯å‘å¼ç­–ç•¥ï¼Œä¾‹å¦‚ç²¾å¿ƒè®¾è®¡çš„é‡è¦æ€§è¿¹è±¡[36ï¼Œ54]ï¼Œæ•æ„Ÿæ€§åˆ†æ[18]æˆ–æ‰‹åŠ¨è®¾è®¡çš„æ–¹æ¡ˆ[23]ï¼Œè¿™äº›æ–¹æ¡ˆé€šå¸¸ä¸ä¼šåœ¨å¾®è°ƒåäº§ç”Ÿå¼ºå¤§çš„æ€§èƒ½ä¿è¯ã€‚

--------------------------------------------------

[062] Efficient Diffusion Transformers.
[062] æœ‰æ•ˆçš„æ‰©æ•£å˜å‹å™¨ã€‚

--------------------------------------------------

[063] Developing efficient diffusion transformers has become an appealing focus within the community, where significant efforts have been made to enhance efficiency from various perspectives, in- cluding linear attention mechanisms [15, 48, 52], compact architectures [50], non-autoregressive transformers [4, 14, 38, 49], pruning [12, 23], quantization [19, 30, 44], feature 2
[063] å¼€å‘æœ‰æ•ˆçš„æ‰©æ•£å˜å‹å™¨å·²æˆä¸ºç¤¾åŒºä¸­çš„ä¸€ä¸ªå¸å¼•äººçš„é‡ç‚¹ï¼Œåœ¨å„ç§è§’åº¦ï¼Œå·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›æ¥æé«˜æ•ˆç‡ï¼ŒåŒ…æ‹¬çº¿æ€§æ³¨æ„æœºåˆ¶[15ï¼Œ48ï¼Œ52]ï¼Œç´§å‡‘å‹å»ºç­‘[50]ï¼Œéè‡ªåŠ¨æ€§å˜å‹å™¨[4ï¼Œ14ï¼Œ38ï¼Œ49]ï¼Œpruning [4,14,38,49]ï¼Œpruns [12ï¼Œ23]ï¼Œé‡åŒ–[19ï¼Œ30ï¼Œ30ï¼Œ44]

--------------------------------------------------

[064] Transformer Layer Transformer Layer Transformer Layer Transformer Layer 1:2 Local Blocks ğ“ğŸ ğ“ğŸ ğ“ğŸ‘ ğ“ğŸ’ 0 1 0 1 0 1 0 1 âŠ• Weight Update Weight Update Weight Update Weight Update Î”ğœ™4 â‹…ğ”ª4 Î”ğœ™3 â‹…ğ”ª3 Î”ğœ™2 â‹…ğ”ª2 Î”ğœ™1 â‹…ğ”ª1 Retained Layer Retained Layer ğ¦ğ¢ğ§ğ–’,ğš«ğš½ğ“›(ğ’™, ğš½+ ğš«ğš½, ğ–’) Confident Sampling â‡’ Good solution identified 1:2 Local Blocks ğ”ª1 ğ”ª2 ğ”ª3 ğ”ª4 âŠ• âˆ¼ Mixed Sampling â‡’ Exploration still in Progress Diff.
[064] å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚å˜å‹å™¨å±‚1ï¼š2å±€éƒ¨å—ğ“ğŸ 0 1 0 1 0 1 0 1 0 1 0 1âŠ•é‡é‡æ›´æ–°é‡é‡æ›´æ–°é‡é‡æ›´æ–°é‡é‡æ›´æ–°é‡é‡æ›´æ–°é‡é‡æ›´æ–°é‡é‡æ›´æ–°Î”4Î”3Î”3Î”2Î”2Î”2Î´ğœ™2Î”ğœ™1Î”ğœ™1Âµ1Â·ğ”ª1Â·ğ”ª1Â·ğ”ª1Âµ1ä¿ç•™å±‚ä¿ç•™å±‚ï¼Œè‰¯å¥½çš„å±‚è¯†åˆ«å±‚ï¼ˆè‰¯å¥½çš„è§£å†³æ–¹æ¡ˆï¼‰ï¼ŒèŒƒå›´2ï¼Œğš«ğš½ğ“›ï¼Œğš«ğš½ğ“›ï¼Œğ’™+ ğš«ğš½+ ğš«ğš½ï¼Œğš½+ ğš«ğš½ï¼Œğ–’+ ğš«ğš½ï¼ŒSamplingï¼ŒSampl complativeã€‚ ğ”ª1ğ”ª2ğ”ª3ğ”ª4âŠ•4ã€œæ··åˆé‡‡æ ·â‡’æ¢ç´¢ä»åœ¨è¿›è¡Œä¸­ã€‚

--------------------------------------------------

[065] Sampling Learnable Distribution âˆ¼ Diff.
[065] æŠ½æ ·å¯å­¦ä¹ çš„åˆ†å¸ƒã€œå·®å¼‚ã€‚

--------------------------------------------------

[066] Sampling Figure 2.
[066] é‡‡æ ·å›¾2ã€‚

--------------------------------------------------

[067] The proposed TinyFusion method learns to perform a differentiable sampling of candidate solutions, jointly optimized with a weight update to estimate recoverability.
[067] æ‹Ÿè®®çš„TinyFusionæ–¹æ³•å­¦ä¼šäº†å¯¹å€™é€‰è§£å†³æ–¹æ¡ˆè¿›è¡Œå¯åŒºåˆ†çš„é‡‡æ ·ï¼Œå…±åŒä¼˜åŒ–äº†é‡é‡æ›´æ–°ä»¥ä¼°ç®—å¯æ¢å¤æ€§ã€‚

--------------------------------------------------

[068] This approach aims to increase the likelihood of favorable solutions that ensure strong post-fine- tuning performance.
[068] è¿™ç§æ–¹æ³•æ—¨åœ¨å¢åŠ æœ‰åˆ©è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ï¼Œä»è€Œç¡®ä¿å¼ºå¤§çš„ç»“æŸåè¡¨ç°ã€‚

--------------------------------------------------

[069] After training, local structures with the highest sampling probabilities are retained.
[069] è®­ç»ƒåï¼Œä¿ç•™äº†é‡‡æ ·æ¦‚ç‡æœ€é«˜çš„æœ¬åœ°ç»“æ„ã€‚

--------------------------------------------------

[070] caching [35, 57], etc.
[070] ç¼“å­˜[35ï¼Œ57]ï¼Œç­‰ã€‚

--------------------------------------------------

[071] In this work, we focus on compress- ing the depth of pre-trained diffusion transformers and in- troduce a learnable method that directly optimizes recover- ability, which is able to achieve satisfactory results with low re-training costs.
[071] åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå‹ç¼©é¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨çš„æ·±åº¦ï¼Œå¹¶èµ‹äºˆä¸€ç§å¯å­¦ä¹ çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç›´æ¥ä¼˜åŒ–æ¢å¤èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡ä½é‡æ–°è®­ç»ƒæˆæœ¬è·å¾—ä»¤äººæ»¡æ„çš„ç»“æœã€‚

--------------------------------------------------

[072] 3.
[072] 3ã€‚

--------------------------------------------------

[073] Method 3.1.
[073] æ–¹æ³•3.1ã€‚

--------------------------------------------------

[074] Shallow Generative Transformers by Pruning This work aims to derive a shallow diffusion transformer by pruning a pre-trained model.
[074] é€šè¿‡ä¿®å‰ªè¿™é¡¹å·¥ä½œï¼Œæµ…å±‚ç”Ÿæˆå˜å‹å™¨æ—¨åœ¨é€šè¿‡ä¿®å‰ªé¢„è®­ç»ƒçš„æ¨¡å‹æ¥å¾—å‡ºæµ…æ‰©æ•£å˜å‹å™¨ã€‚

--------------------------------------------------

[075] For simplicity, all vectors in this paper are column vectors.
[075] ä¸ºç®€å•èµ·è§ï¼Œæœ¬æ–‡ä¸­çš„æ‰€æœ‰å‘é‡éƒ½æ˜¯åˆ—å‘é‡ã€‚

--------------------------------------------------

[076] Consider a L-layer trans- former, parameterized by Î¦LÃ—D = [Ï•1, Ï•2, Â· Â· Â· , Ï•L]âŠº, where each element Ï•i encompasses all learnable param- eters of a transformer layer as a D-dim column vector, which includes the weights of both attention layers and MLPs.
[076] è€ƒè™‘ä¸€ä¸ªç”±Lå±‚trans-å‰è€…ï¼Œç”±Ï†lÃ—d = [Ï•1ï¼ŒÏ•2ï¼ŒÂ·ï¼ŒÏ•L]âŠºè¿›è¡Œå‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ Ï•iåŒ…å«å˜å‹å™¨å±‚çš„æ‰€æœ‰å¯å­¦ä¹ çš„å‚æ•°ä½œä¸ºD-DIMæŸ±å‘é‡ï¼Œå…¶ä¸­åŒ…æ‹¬æ³¨æ„å±‚å’ŒMLPçš„é‡é‡ã€‚

--------------------------------------------------

[077] Depth pruning seeks to find a binary layer mask mLÃ—1 = [m1, m2, Â· Â· Â· , mL]âŠº, that removes a layer by: xi+1 = miÏ•i(xi) + (1 âˆ’mi)xi = ( Ï•i(xi), if mi = 1, xi, otherwise, (1) where the xi and Ï•i(xi) refers to the input and output of layer Ï•i.
[077] æ·±åº¦ä¿®å‰ªè¯•å›¾æ‰¾åˆ°äºŒè¿›åˆ¶æ©ç mlÃ—1 = [m1ï¼Œm2ï¼ŒÂ·Â·çº§ï¼Œml]âŠºï¼Œè¯¥å±‚é€šè¿‡ï¼šxi + 1 = miDarticeï¼ˆxiï¼‰ +ï¼ˆ1 -miï¼‰xi =ï¼ˆÏ•iï¼ˆxiï¼‰=ï¼ˆÏ•iï¼ˆxiï¼ˆxiï¼ˆxiï¼‰ï¼Œå¦‚æœmi = 1ï¼Œxiï¼‰ï¼Œxi = 1ï¼Œxiï¼Œï¼ˆxiï¼‰ï¼Œxiå’Œxi ylyï¼ˆxiï¼‰ï¼Œï¼ˆxiï¼‰å’Œxi liverï¼Œï¼ˆxiï¼‰codiï¼ˆxiï¼‰å’Œxiï¼ˆxiï¼‰codiï¼ˆxiï¼‰å’Œï¼ˆxi liverï¼Œï¼ˆ1ï¼‰ Ï•iã€‚

--------------------------------------------------

[078] To obtain the mask, a common paradigm in prior work is to minimize the loss L after pruning, which can be formulated as minm Ex [L(x, Î¦, m)].
[078] ä¸ºäº†è·å¾—æ©æ¨¡ï¼Œå…ˆå‰å·¥ä½œä¸­çš„ä¸€ä¸ªå¸¸è§èŒƒå¼æ˜¯å°†ä¿®å‰ªåçš„æŸè€—læœ€å°åŒ–ï¼Œå¯ä»¥å°†å…¶ä½œä¸ºminm ex [lï¼ˆxï¼ŒÏ†ï¼Œmï¼‰]é…åˆ¶ã€‚

--------------------------------------------------

[079] However, as we will show in the experiments, this objective â€“ though widely adopted in discriminative tasks â€“ may not be well-suited to pruning diffusion transformers.
[079] ä½†æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨å®éªŒä¸­æ˜¾ç¤ºçš„é‚£æ ·ï¼Œå°½ç®¡åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­å¹¿æ³›é‡‡ç”¨äº†è¿™ä¸ªç›®æ ‡ï¼Œä½†å¯èƒ½ä¸é€‚åˆä¿®å‰ªæ‰©æ•£å˜å‹å™¨ã€‚

--------------------------------------------------

[080] Instead, we are more inter- ested in the recoverability of pruned models.
[080] å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹ä¿®å‰ªæ¨¡å‹çš„å¯æ¢å¤æ€§æ›´åŠ æ„Ÿå…´è¶£ã€‚

--------------------------------------------------

[081] To achieve this, we incorporate an additional weight update into the optimization problem and extend the objective by: min m min âˆ†Î¦ Ex [L(x, Î¦ + âˆ†Î¦, m)] | {z } Recoverability: Post-Fine-Tuning Performance , (2) where âˆ†Î¦ = {âˆ†Ï•1, âˆ†Ï•2, Â· Â· Â· , âˆ†Ï•M} represents appro- priate update from fine-tuning.
[081] ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å°†é¢å¤–çš„æƒé‡æ›´æ–°çº³å…¥äº†ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æ‰©å±•äº†ç›®æ ‡ï¼šmin m min âˆ†Ï† ex [lï¼ˆxï¼ŒÏ† + âˆ†Ï†ï¼Œmï¼‰] | {z}å¯æ¢å¤æ€§ï¼šæ¢å¤æ€§èƒ½åçš„æ€§èƒ½ï¼Œï¼ˆ2ï¼‰å…¶ä¸­âˆ†Ï† = {âˆ† Ï•1ï¼Œâˆ† Ï•2ï¼ŒÂ·Â·Â·ï¼Œâˆ† Ï•m}è¡¨ç¤ºæ¥è‡ªå¾®è°ƒçš„é€‚å½“æ›´æ–°ã€‚

--------------------------------------------------

[082] The objective formulated by Equation 2 poses two challenges: 1) The non-differentiable nature of layer selection prevents direct optimization us- ing gradient descent; 2) The inner optimization over the retained layers makes it computationally intractable to ex- plore the entire search space, as this process necessitates se- lecting a candidate model and fine-tuning it for evaluation.
[082] å…¬å¼2æå‡ºçš„ç›®æ ‡æå‡ºäº†ä¸¤ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰å±‚é€‰æ‹©çš„éå·®å¼‚æ€§è´¨é˜»æ­¢äº†ç›´æ¥ä¼˜åŒ–çš„æ¢¯åº¦ä¸‹é™ï¼› 2ï¼‰åœ¨ä¿ç•™å±‚ä¸Šçš„å†…éƒ¨ä¼˜åŒ–ä½¿å¾—åœ¨è®¡ç®—ä¸Šæ£˜æ‰‹å¯ä»¥é˜æ˜æ•´ä¸ªæœç´¢ç©ºé—´ï¼Œå› ä¸ºæ­¤è¿‡ç¨‹éœ€è¦å°†å€™é€‰æ¨¡å‹åˆ—ä¸ºå€™é€‰æ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œè¯„ä¼°ã€‚

--------------------------------------------------

[083] To address this, we propose TinyFusion that makes both the pruning and recoverability optimizable.
[083] ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ä¿®å‰ªå’Œå¯æ¢å¤æ€§ä¼˜åŒ–çš„å¾®å°çŒæ³¨ã€‚

--------------------------------------------------

[084] 3.2.
[084] 3.2ã€‚

--------------------------------------------------

[085] TinyFusion: Learnable Depth Pruning A Probabilistic Perspective.
[085] å°å‹çŒæ³¨ï¼šå¯å­¦ä¹ çš„æ·±åº¦ä¿®å‰ªæ¦‚ç‡çš„è§‚ç‚¹ã€‚

--------------------------------------------------

[086] This work models Equa- tion 2 from a probabilistic standpoint.
[086] è¿™é¡¹å·¥ä½œä»æ¦‚ç‡çš„è§’åº¦æ¨¡æ‹Ÿäº†æ–¹ç¨‹2ã€‚

--------------------------------------------------

[087] We hypothesize that the mask m produced by â€œidealâ€ pruning methods (might be not unique) should follow a certain distribution.
[087] æˆ‘ä»¬å‡è®¾ç”±â€œç†æƒ³â€ä¿®å‰ªæ–¹æ³•ï¼ˆå¯èƒ½ä¸æ˜¯å”¯ä¸€çš„ï¼‰äº§ç”Ÿçš„é¢è†œåº”éµå¾ªä¸€å®šçš„åˆ†å¸ƒã€‚

--------------------------------------------------

[088] To model this, it is intuitive to associate every possible mask m with a probability value p(m), thus forming a categori- cal distribution.
[088] ä¸ºäº†å¯¹æ­¤è¿›è¡Œå»ºæ¨¡ï¼Œå°†æ¯ä¸ªå¯èƒ½çš„æ©ç Mä¸æ¦‚ç‡å€¼Pï¼ˆMï¼‰ç›¸å…³è”æ˜¯ç›´è§‚çš„ï¼Œä»è€Œå½¢æˆåˆ†ç±»åˆ†å¸ƒã€‚

--------------------------------------------------

[089] Without any prior knowledge, the assess- ment of pruning masks begins with a uniform distribution.
[089] æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†ï¼Œä¿®å‰ªå£ç½©çš„è¯„ä¼°å§‹äºå‡åŒ€çš„åˆ†å¸ƒã€‚

--------------------------------------------------

[090] However, directly sampling from this initial distribution is highly inefficient due to the vast search space.
[090] ä½†æ˜¯ï¼Œç”±äºåºå¤§çš„æœç´¢ç©ºé—´ï¼Œè¯¥åˆå§‹åˆ†å¸ƒç›´æ¥è¿›è¡Œé‡‡æ ·æ•ˆç‡å¾ˆé«˜ã€‚

--------------------------------------------------

[091] For in- stance, pruning a 28-layer model by 50% involves evalu- ating 