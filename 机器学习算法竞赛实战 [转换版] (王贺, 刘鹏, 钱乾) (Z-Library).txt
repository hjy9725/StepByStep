TURING 图灵原创

☑

Kaggle

阿里天池

腾讯

广告算法大赛

腾讯广告

算法大赛冠军、 Kaggle Grandmaster打造 王贺

刘鹏钱乾著机器学习算

法竞赛实战中国工信出

版集团 人民邮电出版社

POSTS &

TELECOM PRESS

版权信息

书名：机器学习

算法竞赛实战

作者：王贺

刘鹏 钱乾

ISBN：978-7-115-56959-2

本书由北京图

灵文化发展有限公司发

行数字版。版权所有，侵权

必究。

您购买的图灵电子

书仅供您个人使用，未经

授权，不得以任何方式复

制和传播本书内容。

我们

愿意相信读者具有这样

的良知和觉悟，与我们共

同保护知识产权。

如果购

买者有侵权行为，我们可

能对该用户实施包括但

不限于关闭该帐号等维

权措施，并可

能追究法律

责任。

091507240605ToBeReplacedWithUserId

前言

算法竞赛时代

本书缘起

本书特色

本书

的读者对象

欢迎交流

致

谢

第一部分 磨刀事半，砍

柴功倍

第 1 章

初见竞赛

1.1 竞

赛平台

1.1.1 Kaggle

1.1.2

天池

1.1.3 DF

1.1.4 DC

1.1.5

Kesci

1.1.6 JDATA

1.1.7 企业网站

1.2

竞

赛流程

1.2.1 问题建模

1.2.2 数据探

索

1.2.3

特征工程

1.2.4 模型训练

1.2.5 模

型融合

1.3

竞赛类型

1.3.1 数据类

型

1.3.2 任务类型

1.3.3

应用场景

1.4 思

考练习

第 2 章

问题建模

2.1 赛

题理解

2.1.1 业务背景

2.1.2

数据理

解

2.1.3 评价指标

2.2 样本选择

2.2.1

主

要原因

2.2.2 准确方法

2.2.3 应用场

景

2.3

线下评估策略

2.3.1 强时序

性问题

2.3.2 弱时序性问题

2.4

实

战案例

2.4.1 赛题理解

2.4.2 线下验

证

2.5

思考练习

第 3 章 数据探

索

3.1

数据初探

3.1.1 分析思路

3.1.2 分

析方法

3.1.3

明确目的

3.2 变量分

析

3.2.1 单变量分析

3.2.2

多变量分

析

3.3 模型分析

3.3.1 学习曲线

3.3.2

特

征重要性分析

3.3.3 误差分析

3.4 思考练习

第 4

章 特征工程

4.1 数据预处理

4.1.1 缺失值处理

4.1.2 异常值处理

4.1.3 优化内存

4.2 特

征变换

4.2.1 连续变量无量纲

化

4.2.2 连续变量数据变换

4.2.3 类

别特征转换

4.2.4 不规则特征

变换

4.3 特征提取

4.3.1 类别相关

的统计特征

4.3.2 数值相关的

统计特征

4.3.3 时间特征

4.3.4 多值

特征

4.4 特征选择

4.4.1 特征关联

性分析

4.4.2 特征重要性分析

4.4.3 封装方法

4.5

实战案例

4.5.1 数据

预处理

4.5.2 特征提取

4.5.3

特征选

择

4.6 思考练习

第 5 章

模型训

练

5.1 线性模型

5.1.1 Lasso 归

5.1.2 Ridge 归

5.2 树模型

5.2.1 随机森林

5.2.2 梯度提升树

5.2.3 XGBoost

5.2.4 LightGBM

5.2.5 CatBoost

5.2.6 模

型深入对比

5.3 神经网络

5.3.1 多

层感知机

5.3.2 卷积神经网络

5.3.3 循环神经网络

5.4

实战案例

5.5 思考练习

第 6 章 模型融合

6.1

构建多样性

6.1.1 特征多样性

6.1.2 样本多样性

6.1.3 模型多样性

6.2

训练过程融合

6.2.1 Bagging

6.2.2 Boosting

6.3

训练结果

融合

6.3.1 加权法

6.3.2 Stacking 融合

6.3.3 Blending融合

6.4 实

战案例

6.5 思考练习

第二部

分 物以类聚，人以群分

第

7 章 用户画像

7.1

什么是用户

画像

7.2 标签系统

7.2.1 标签分类

方式

7.2.2

多渠道获取标签

7.2.3 标

签体系框架

7.3 用户画像数

据特征

7.3.1

常见的数据形式

7.3.2 文本挖掘算法

7.3.3 神奇的嵌

入表示

7.3.4 相似度计算方法

7.4

用户画像的应用

7.4.1 用户分

析

7.4.2 精准营销

7.4.3

风控领域

7.5 思

考练习

第 8 章

实战案例：Elo Merchant Category Recommendation

8.1 赛

题理解

8.1.1 赛题背景

8.1.2 赛题数

据

8.1.3 赛题任务

8.1.4 评价指标

8.1.5 赛

题 FAQ

8.2

数据探索

8.2.1 字段类别含

义

8.2.2 字段取值状况

8.2.3

数据分

布差异

8.2.4 表格关联关系

8.2.5 数

据预处理

8.3

特征工程

8.3.1 通用

特征

8.3.2 业务特征

8.3.3

文本特征

8.3.4 特征选择

8.4 模型训练

8.4.1 随机

森林

8.4.2 LightGBM

8.4.3 XGBoost

8.5 模型融合

8.5.1 加权融合

8.5.2 Stacking 融合

8.6 高效提分

8.6.1 特征优化

8.6.2 融合技巧

8.7 赛题总结

8.7.1

更多

方案

8.7.2 知识点梳理

8.7.3 延伸学

习

第三部分

以史为鉴，未

来可期

第 9 章 时间序列

9.1

什

么是时间序列

9.1.1 简单定义

9.1.2 常见问题

9.1.3 交叉验证

9.1.4 基本

规则方法

9.2 时间序列模式

9.2.1 趋势性

9.2.2

周期性

9.2.3 相关性

9.2.4 随

机性

9.3

特征提取方式

9.3.1 历史

平移

9.3.2 窗口统计

9.3.3

序列熵特

征

9.3.4 其他特征

9.4 模型的多样

性

9.4.1

传统的时序模型

9.4.2 树模

型

9.4.3 深度学习模型

9.5

思考练

习

第 10 章 实战案例：全球城

市计算 AI

挑战赛

10.1 赛题理解

10.1.1 背景介绍

10.1.2 赛题数据

10.1.3 评价

指标

10.1.4 赛题 FAQ

10.1.5

baseline 方案

10.2 数据探索

10.2.1 数据初探

10.2.2

模式分析

10.3 特征

工程

10.3.1 数据预处理

10.3.2

强相关

性特征

10.3.3 趋势性特征

10.3.4 站点

相关特征

10.3.5

特征强化

10.4 模型

训练

10.4.1 LightGBM

10.4.2

时序模型

10.5 强化学习

10.5.1 时序 stacking

10.5.2

Top 方案解析

10.5.3 相关赛题

推荐

第 11

章 实战案例：Corporación Favorita Grocery Sales Forecasting

11.1 赛题

理解

11.1.1 背景介绍

11.1.2 赛题数据

11.1.3

评价指标

11.1.4 赛题 FAQ

11.1.5 baseline

方案

11.2 数据

探索

11.2.1 数据初探

11.2.2

单变量分

析

11.2.3 多变量分析

11.3 特征工程

11.3.1 历史平移特征

11.3.2 窗口统计

特征

11.3.3 构造粒度多样性

11.3.4 高

效特征选择

11.4 模型训练

11.4.1 LightGBM

11.4.2 LSTM

11.4.3 Wavenet

11.4.4 模

型融合

11.5 赛题总结

11.5.1 更多方

案

11.5.2 知识点梳理

11.5.3 延伸学习

第四部分

精准投放，优化

体验

第 12 章 计算广告

12.1

什么

是计算广告

12.1.1 主要问题

12.1.2 计

算广告系统架构

12.2

广告类

型

12.2.1 合约广告

12.2.2 竞价广告

12.2.3

程

序化交易广告

12.3 广告召回

12.3.1 广告召模块

12.3.2 DSSM

语义召回

12.4 广

告排序

12.4.1 点击率预估

12.4.2

特征

处理

12.4.3 常见模型

12.5 广告竞价

12.6 思考练习

第 13 章 实战案例

：2018 腾讯广告算法大赛——相似

人群拓展

13.1

赛题理解

13.1.1 赛题

背景

13.1.2 赛题数据

13.1.3

赛题任务

13.1.4 评价指标

13.1.5 赛题 FAQ

13.2

数据探索

13.2.1 竞赛的公开数据集

13.2.2 训练

集与测试集

13.2.3 广告属性

13.2.4 用

户信息

13.2.5 数据集特征拼接

13.2.6 基本建模思路

13.3

特征工程

13.3.1 经典特征

13.3.2 业务特征

13.3.3 文本

特征

13.3.4 特征降维

13.3.5 特征存储

13.4 模型训练

13.4.1

LightGBM

13.4.2 CatBoost

13.4.3 XGBoost

13.5

模型融合

13.5.1 加权

融合

13.5.2 Stacking融合

13.6

赛题总结

13.6.1 更多

方案

13.6.2 知识点梳理

13.6.3

延伸学

习

第 14 章 实战案例：TalkingData AdTracking

Fraud Detection Challenge

14.1 赛题理

解

14.1.1

背景介绍

14.1.2 赛题数据

14.1.3 评

价指标

14.1.4

赛题 FAQ

14.1.5 baseline 方案

14.2

数据探

索

14.2.1 数据初探

14.2.2 单变量分析

14.2.3 多变量分析

14.2.4 数据分布

14.3 特

征工程

14.3.1 统计特征

14.3.2 时间差

特征

14.3.3 排序特征

14.3.4 目标编码

特征

14.4 模型训练

14.4.1 LR

14.4.2 CatBoost

14.4.3 LightGBM

14.4.4 DeepFM

14.5 赛题总结

14.5.1

更多方案

14.5.2 知识点梳理

14.5.3 延

伸学习

第五部分

听你所

说，懂你所写

第 15 章 自然语

言处理

15.1

自然语言处理的

发展历程

15.2 自然语言处理

的常见场景

15.2.1 分类、归任务

15.2.2 信息检索、文本匹配等任

务

15.2.3 序列对序列、序列标注

15.2.4 机器阅读

15.3 自然语言处理

的常见技术

15.3.1

基于词袋模

型、TF-IDF 的特征提取

15.3.2 N-Gram 模型

15.3.3

词嵌

入模型

15.3.4 上下文相关预训

练模型

15.3.5 常用的深度学习

模型结构

15.4

思考练习

第 16 章

实战案例：Quora Question Pairs

16.1 赛题理解

16.1.1 赛题

背景

16.1.2 赛题数据

16.1.3 赛题任务

16.1.4 评价指标

16.1.5 赛题 FAQ

16.2 数据探索

16.2.1 字段类别含义

16.2.2 数据集基

本量

16.2.3

文本的分布

16.2.4 词的数

量与词云分析

16.2.5 基于传统

手段的文本数据预处理

16.2.6 基于深度学习模型的文

本数据预处理

16.3 特征工程

16.3.1 通用文本特征

16.3.2 相似度特

征

16.3.3

词向量的进一步应用

——独有词匹配

16.3.4 词向量的进

一步应用——词与词的两两

匹配

16.3.5 其他相似度计算方

式

16.4

模型训练

16.4.1 TextCNN 模型

16.4.2 TextLSTM

模型

16.4.3 TextLSTM with Attention 模

型

16.4.4 Self-Attention 层

16.4.5 Transformer 和

BERT 类模型

16.4.6 基于 representation 和基

于

interaction 的深度学习模型的差

异

16.4.7 一种特殊的基于 interaction 的深

度学习模型

16.4.8 深度学习文

本数据的翻译增强

16.4.9 深度

学习文本数据的预处理

16.4.10 BERT 模型的训练

16.5 模型融合

16.6 赛

题总结

16.6.1 更多方案

16.6.2 知识点

梳理

16.6.3 延伸学习

作者简介

前言

算法竞赛时代

2010 年，全

球著名算法竞赛平台 Kaggle 举

办了第一场竞赛 Forecast Eurovision

Voting，奖金

为

1000 美元。2015 年，国内第一场算法

竞赛在天池举办，比赛题

目是阿里移动推荐算法

，

奖金为 30

万元人民币，吸引

了 7000 多人参加。虽然国内的

算法竞赛起步时间晚于

国外，但

从 2015 年开始，在全球

举办的一共

1000 多场赛事中

，中国就举办了 400 多场，并且

场次的

年均增长率高达

108.8%，有累计超过 120

万人参加，奖

金累计达到 2.8 亿元人民币

。在算法

竞赛的举办场次

拥有如此高增长率的情

况下，其技术价值、业务价

值和创新价值自然不容

小

觑。

本书缘起

说起本书

，便要追溯到 2019 年 4 月 19

日人民

邮电出版社的策划编辑

陈兴璐在知乎上发给我

的一则信息，其中讲到她

看过我很多有关算法竞

赛的文章，而且多次在算

法竞赛中获奖，因

此期待

我能出版一本关于算法

竞赛的图书。大概在 2018 年年

初，我就已经创建了专栏

，开

始分享竞赛相关的文

章，一路走来持续输出，目

前的文章总浏览量达到

百万。这次收到来信

以及

希望出版算法竞赛图书

的邀请，是对我分享竞赛

知识和已取得成绩的莫

大认可，我欣然

答应了写

作邀请，并确定以《机器学

习算法竞赛实战》作为书

名。

为了完成本书，我邀请

了我的竞赛老队员刘鹏

（国内多次竞赛的冠亚军

），陈兴璐编辑向我

推荐了

钱乾（Kaggle 竞赛平台的 Grandmaster，国内最

早一批竞赛选手之一）。另

外，考虑

到每个人擅长的

点不同，我们进行了明确

的章节分工，以保证每个

章节的质量。

本书特色

对

于本书的章节架构，我们

除了进行仔细的讨论外

，还采纳了国内多名顶尖

竞赛选手的建

议。算法竞

赛本身涵盖的范围是很

大的，我们的理念是剖析

其最本质的内容，然后结

合多个

领域模块进行实

战讲解，这也是本书的一

大特色。本书分为以下五

个部分。

第一部分——磨刀事

半，砍柴功倍。这部分以算

法竞赛的通用化流程为

主，介绍竞赛中各

个部分

的核心内容和具体工作

，且每章都配有具体的实

战部分，以便加深理解。

第

二部分——物以类聚，人以群

分。这部分主要介绍用户

画像相关的问题，构建完

善的标

签体系是用户画

像的核心，也是解决用户

画像类赛题的关键，比如

个性化推荐和金融风控

等

问题都需要以用户画

像作为支撑。为了帮助读

者加快对此类竞赛问题

的学习、理解，会讲解

具体

的竞赛案例，即 Kaggle 平台的

Elo Merchant Category Recommendation。

第

三部分——以史为鉴，未来可

期。这部分以时间序列预

测问题为主，先讲述这类

问题的

常见解题思路和

技巧，然后分析两个具体

的实战案例，分别是天池

平台的全球城市计算

AI

挑

战赛和 Kaggle 平台的 Corporación Favorita

Grocery Sales Forecasting。

第四部分

——精准投放，优化体验。计算

广告相关的业务大多是

很好的竞赛题目，这部分

主要介绍了计算广告的

核心技术和业务，包括广

告召回、广告排序和广告

竞价。实战案例部

分则包

括两道赛题，分别是 2018

腾讯

广告算法大赛——相似人群

拓展，以及 Kaggle 平台的

TalkingData AdTracking Fraud

Detection Challenge。

第五部

分——听你所说，懂你所写。这

部分基于自然语言处理

相关的内容进行讲解，包

括

常见任务和常见技术

，实战案例部分是 Kaggle 平台上

的经典竞赛

Quora Question Pairs。

本书是算法

竞赛领域一本系统性介

绍竞赛的书，不仅包含竞

赛的基本理论知识，还结

合多个

方向和案例详细

阐述了竞赛中的上分思

路和技巧。

本书的读者对

象

本书的目标读者可以

分为以下三类。

对算法竞

赛感兴趣的人。兴趣是最

大的驱动力，为了让算法

竞赛变得更加有趣和

更

加多样性，本书增加了很

多扩展与探索性的内容

，从多个方向、多个领域进

行介

绍和实战。

想要研究

机器学习或深度学习算

法实战的人。实战的最佳

方式之一是参加一场算

法竞赛，加深对理论知识

的理解，这也是本书的核

心思想。

计算机相关专业

的人。机器学习或深度学

习算法作为目前计算机

行业一个火热的就

业方

向，值得去深入研究。本书

提供了很好的实战讲解

，帮助读者知其然，并知其

所以然。

欢迎交流

鉴于作

者水平有限，难免存在有

纰漏的地方，如果你在阅

读过程中遇到任何问题

，欢迎跟我

们联系，我们的

联系方式如下。

微信公众

号：Coggle 数据科学

知乎 ID：鱼遇雨

欲语与余

邮箱：fish_ml@foxmail.com

由于篇幅

限制，书中只放了部分代

码，完整的代码资源欢迎

大家到图灵社区（iTuring.cn）本

书主

页“随书下载”中获取。

致谢

本书的写作过程并不轻

松，我利用的基本上是晚

上下班之后的时间，还要

定期和刘鹏、钱乾

进行线

上会议，讨论近期的写作

进度，以及相互审阅内容

。这里我非常感谢刘鹏和

钱乾所做

的巨大贡献，他

们具备的丰富的竞赛经

验也是促使本书能够更

加高质量完成的一个重

要因

素。

此外，本书的成稿

还离不开其他很多人的

帮助，虽然这些人没有成

为作者之一，但也对本书

做出了很大的贡献，在此

一并表示感谢。

在此，还要

特别感谢人民邮电出版

社图灵公司的编辑陈兴

璐、王军花和王彦，她们不

仅给我

们充分的时间完

成这本书，还提出了很多

宝贵的建议，她们对这本

书的成功出版功不可没

。

最后，感谢我的妻子，她在

我写书的过程中给予了

我很多支持和照顾。

谢谢

你们！

王贺

2021 年 5

月

第一部分

磨刀事半，砍柴功倍

第 1 章

初见竞赛

第

2 章 问题建模

第 3 章 数据探索

第 4 章 特征

工程

第 5

章 模型训练

第 6 章

模型融合

第

1 章 初见竞赛

随着互联网时代的到来

，以及计算机硬件性能的

提升，人工智能在近几年

可以说是得到了爆

发式

的增长。互联网时代带来

了大量的信息，这些信息

是名副其实的大数据。另

外，性能极

佳的硬件也使

得计算机的计算能力大

大增强，这二者结合到一

起，人工智能的蓬勃兴盛

就变

成了自然而然的事

情。机器学习作为一种传

统的、可解释性较强的算

法，在人工智能三驾马

车

之算法中也占有一席之

地。本书几经商榷，最终定

名为《机器学习算法竞赛

实战》，意在

帮助机器学习

初学者通过实战的方法

从虽然优美但是略显枯

燥的各种公式和理论当

中脱离出

来，感受机器学

习在实际应用中的奥秘

，而竞赛则是一种最特殊

的实战。

之所以强烈推荐

用竞赛作为机器学习实

战的重要方式，是因为它

实在是一个快速入门机

器学

习的极佳方式。对于

初学者来说，他们的水平

并不足以支撑他们直接

进到企业接触实际的应

用场景，而从书里得来的

知识终究有些浅薄。提起

竞赛，大家总是不免会想

起高中时期的各

种数学

、物理、化学竞赛，这些竞赛

门槛极高且国内国外都

有，能拿到好名次的同学

甚至能

够直接保送国内

外知名院校，因此竞赛二

字总有一种令人望而生

畏的感觉。但是近几年，人

工智能的兴起催生出的

各种算法竞赛则相对友

好许多，也有意思许多。在

时代的洪流之下，

各行各

业都在寻求生存之道，利

用先进的技术完成转型

则是一个很好的办法，有

些企业就开

始寻求人工

智能的助力，开始向社会

征求优秀的算法解决方

案；此外，在学术领域的研

究者

们也渴望获得企业

的场景和数据用于算法

研究，这就催生出了各种

竞赛平台。本书主要给读

者介绍机器学习相关算

法的竞赛经验。

对于有志

于进军机器学习相关领

域从事研究或者相关工

作的初学者来说，竞赛是

性价比极高

的一个实战

选择，可以说是零门槛，任

何人都能参加。当然，一般

主办方自己的员工是不

允

许参赛的，即使参赛，也

不能参与排名。各种各样

的竞赛可以说能够覆盖

很多行业的典型应

用场

景，让参赛者不仅能够得

到实战的锻炼，也能体会

到机器学习在各个行业

下沉应用带来

的魔力，甚

至还能在竞赛中见到很

多行业大佬，结交一些志

同道合的朋友。

本章主要

从竞赛平台、竞赛流程以

及竞赛类型这三个部分

给大家做竞赛的相关介

绍。1.1 节

旨在介绍国内外知

名的算法竞赛平台，以帮

助读者快速了解竞赛渠

道。1.2

节则讲述了完成

一次

机器学习算法竞赛的大

致流程，以及每个模块的

功能和作用，更详细的内

容会在第 2 章

到第 6

章中给

出。1.3 节则会为读者介绍常

见的竞赛类型，帮助读者

了解机器学习算法竞赛

适用的场景以及业界需

求。

1.1 竞赛平台

我们参加的

各种竞赛都是由大大小

小的竞赛平台发布的，如

国际的 Kaggle

和国内的阿里云

天池竞赛平台，你可以在

这些平台上找到自己擅

长或喜欢的赛题。

1.1.1 Kaggle

机器学

习领域的大牛吴恩达老

师曾经讲过，机器学习在

大多数时候就只是数学

统计，数据相

关的特征工

程直接决定了模型的上

限，而算法只是不断地去

逼近这个上限而已。在机

器学习

领域，有一个十分

生动的比喻，建模的过程

就好比做饭，数据代表食

材，算法则代表烹饪过

程

，最终饭菜的可口程度就

是模型的效果。观看众多

的美食影像，比如央视著

名的美食纪录

片《舌尖上

的味道》《风味人间》等可以

发现，这些影像都用众多

的篇幅来讲述如何获取

新

鲜食材，而且古话里也

讲巧妇难为无米之炊，由

此可见食材的重要性。类

比到机器学习算法

竞赛

，数据的重要性不言而喻

，这也就是下面将介绍的

国际竞赛平台 Kaggle 对自身的

定

位：数据科学之家。

打开

Kaggle 的网站首页，点击上方的

Compete，出现的界面如图 1.1 所示，左

侧边栏除

Home

和 More 外有 5 个主要

部分，即 Compete（竞赛单元）、Data（数据集

）、Code（代

码笔记）、Communities（社区讨论）以及

Courses（在线课程）。作为全世界最

受欢迎的数

据科学竞赛

网站，其首页也介绍了这

里你可以找到完成数据

科学工作需要的所有数

据与代

码。截至 2020 年 10

月 5 日，这

里已经有 50 000 份以上的数据

集以及超过

400 000 个公开代

码

笔记。本书将着重介绍 Kaggle 的

竞赛单元。点击

Compete，这里从上

到下罗列着历史上所

有

进行过的竞赛，最上面那

部分永远都存在着正在

进行的各种竞赛，随便点

开其中一个，同

样可以看

见竞赛的相关信息，大致

有 Overview（概况）、Data（数据）、Code（代码笔

记）、Discussion（社

区讨论）、Leaderboard（排行榜）、Rules（竞赛规则

）等。接下来，

我们将以竞赛

Microsoft

Malware Prediction 为例介绍一场竞赛的主

要内容，其赛题主页见图

1.2。

图 1.1 Kaggle

页面

图 1.2 Microsoft Malware Prediction

赛题主页

Overview。即概

况，这里会简要介绍这场

竞赛，其中包括四个部分

：

Description（描述）、Evaluation（评分）、Prizes（奖项）和 Timeline（时间轴

）。

Description：竞赛的背景介绍以及主

办方信息。竞赛 Microsoft

Malware

Prediction 中写到，恶

意软件致力于规避传统

的安全措施。一旦计算机

受到恶

意软件的感染，犯

罪分子就会从很多方面

伤害消费者和企业。微软

拥有超过

10 亿的企业和消

费者客户，所以非常重视

这个问题，并且投入了大

量资金来

提高安全性。作

为其整体安全策略的一

部分，微软正在挑战数据

科学界开发

技术，以预测

一台机器是否很快会受

到恶意软件的攻击。与之

前的恶意软件

挑战（Malware Challenge，2015）一样

，微软正在向 Kaggle 提供一个前

所未

有的恶意软件数据

集，以鼓励其在预测恶意

软件的有效技术方面取

得开源进

展。你能帮助保

护超过 10 亿台机器不受损

害吗？

Evaluation：这里会列出本次竞

赛的评判标准以及提交

文件的格式，竞赛

Microsoft

Malware Prediction 采用的

是预测概率与真实标签

的 ROC 曲线下面

积（即

AUC）作为模

型得分，因此本次竞赛是

一个二分类问题。

Prizes：这里展

示了本次竞赛的总奖金

是 25 000 美元，其中冠军的奖励

12

000

美元几乎占了一半。通常

来说，25 000 美元是 Kaggle 有奖竞赛里

面比较

常见的金额，多的

能有

100 000 美元。需要注意的是

，这个竞赛对获奖者会

有

一定的要求，竞赛结束后

获奖者需要在规定时间

内提交建模方案文档，且

不允许微软内部员工参

赛。这些基本上也是大多

数竞赛会要求的。

Timeline：主要对

竞赛的时间线进行介绍

，比较关键的是组队截止

时间和提交

截止时间。一

般竞赛周期在两个月到

三个月之间，合理安排时

间是非常必要

的。

Data。了解了

竞赛的背景与任务之后

，参赛者就可以开始熟悉

数据了，通常的数据

格式

都会是 CSV 宽表形式。Data 部分有

个单独的

Data Description，这里通常会给

出

所有表格的数据信息

，包括采集来源、任务说明

以及详细的各个字段含

义等。以竞

赛 Microsoft Malware

Prediction 为例，其 Data Description 如下

。

这场竞赛的目标是根据

一台

Windows 机器的不同特性，预

测它被各种恶意软件

家

族感染的概率。主办方结

合微软的端点保护解决

方案——Windows Defender

收集的心跳和威胁

报告，生成包含相关属性

和机器感染的遥测数据

集。此数据

集中的每一行

各对应一台机器，由

MachineIdentifier 唯一

标

识。HasDetections 则是机器的标签，表

明是否在机器上检测到

了恶意软

件。参赛者需要

利用 train.csv

中的信息和标签训

练集，预测 test.csv 中每台计算

机

的 HasDetections 值。用于创建此数据集

的采样方法旨在满足某

些业务限

制，包括用户隐

私以及机器运行的时间

段。检测恶意软件本质上

是一个时间序

列预测问

题，但由于引入新机器、在

线和离线机器、接收补丁

的机器、接收新

操作系统

的机器等，它变得更加复

杂。而这里提供的数据集

已经大致按时间划

分好

，以上提到的复杂性和抽

样要求意味着你可能会

看到自己的交叉验证、公

开榜和私人榜单分数不

完全一致！此外，这个数据

集并不能代表微软客户

的机

器，因为它是经过抽

样的，包含更大比例的恶

意软件机器。

参赛者在参

加竞赛时首先应该做到

的就是熟悉题目与数据

，这里面往往会包含很多

重要的细节信息。拿上面

的题目为例，数据看上去

非常简单，主办方已经分

清楚了

训练集与测试集

、标准的特征字段与标签

字段。赛题任务也很明确

，就是预测机器

是否会被

恶意软件感染。但不可忽

视的一点是介绍中提到

的，这个题目本质上是一

个时间序列预测问题，训

练集与测试集只是大致

按照时间进行了划分，而

为了突出

恶意软件，机器

更是对正样本进行了一

定的升采样。因此，这种复

杂性与抽样性会

给建模

带来极大的不确定性，导

致交叉验证、公开榜和私

人榜单分数的上下波动

不

会完全一致，这一点在

竞赛结束后的私人榜单

上就有所体现，相比公开

榜单，其排

名波动异常剧

烈。

Code。这部分是本次竞赛的

开源社区所在，Kaggle 能成为全

球最大的数据竞赛平台

之一，其开源的特性与讨

论的氛围功不可没。在这

里，你可以看到各式各样

的数据

探索（EDA）、特征工程、建

模方法以及截然不同的

代码风格与个人偏好，有

的代

码标题下方甚至会

显示本代码在榜单上的

得分成绩。在这里，参赛者

可以尽情学习

各种工具

和代码写法，为了达成同

样的目的，参赛者也会在

这里发现更为简洁、优

雅

、快速的实现方式，同时可

以将各种建模方法进行

融合，博采众长。甚至竞赛

圈

里流传着：只要开源融

合得好，得一块银牌不是

问题。

Discussion。和承担代码笔记功

能的 Code 不同，Discussion 是参赛者之间

真正交流

讨论的地方，这

里代码很少，有各种

QA（问答

）以及对赛事的理解、发现

。在这里

参赛者可以自由

地和全世界的数据科学

爱好者共同讨论竞赛的

相关心得，甚至是对

理论

的探索验证，可以见到各

种 Master 乃至 Grandmaster

的身影，他们之间

的互动

也十分精彩。

Leaderboard。用于

展示排行榜，所有成功提

交过结果文件的参赛者

都可以在这里找

到自己

的位置。榜单实时刷新，对

于争分夺秒挠破头皮的

参赛者来说，可以说十分

刺激。Kaggle 的竞赛通常分为 Public

Leaderboard 和

Private Leaderboard，即竞赛圈子

里常说的 A 榜

和

B 榜。这也展现了机器学

习领域中非常重要的一

个概念，就是模型

泛化性

。实时榜单的存在虽然方

便了参赛者，使他们可以

不停地验证自己的想法

，

并对比出不同方案的得

分，但这只是在公开榜上

的成绩。机器学习建模很

注重的一

点就是模型的

泛化性能，也可以说是它

的健壮性，健壮性强的模

型才可以在未来的

预测

中始终保持良好的效果

，这对于实际应用来说非

常重要，因此才有了

A 榜、B

榜

这样的划分。一般来讲，把

同一批数据切分成两份

，一份用来评估 A 榜分数，另

一份用来评估 B

榜分数。参

赛者通常需要在竞赛的

第一阶段不断根据 A 榜的

得分来

修正并改善建模

方案，最后有两次机会可

以选择用于计算 B 榜得分

的结果文件，最

终排名依

据的是 B 榜得分。机器学习

建模的泛化性往往是一

个难点、痛点，对于有

些竞

赛，其 A 榜、B

榜的排名可以说

会发生翻天覆地的变化

，经常会有参赛者的模

型

过拟合 A 榜，也就是模型会

对 A 榜以外的数据表现得

十分差劲，这也是大家有

时

会将机器学习等人工

智能建模称为丹炉炼丹

或者玄学的原因所在。

Rules。这

部分给出了本次竞赛的

相关规则，是比 Overview 部分更加

详细的补充，

通常需要关

注其中几个重要的时间

点，比如竞赛开始时的 A

榜

评测开放时间、队伍

合并

的截止时间以及 B 榜的切

换时间。此外，还有对队伍

人数、队伍提交总次数的

限制，关于竞赛作弊方式

的判定以及其他不允许

发生的行为等。建议参赛

者不仅要

熟悉竞赛背景

、竞赛内容，更要对竞赛规

则了如指掌，以免不小心

违反竞赛规则而

导致努

力白费。

1.1.2 天池

天池是国内

较大的大数据众智平台

，面向社会开放高质量脱

敏数据集（阿里数据及第

三方授

权数据）和计算资

源，吸引全球高水平人才

创造优秀解决方案，有效

帮助行业、政府解决业

务

痛点，并为企业招聘输送

人才。作为中国 AI

产业的排

头兵，天池提供集品牌、生

态、人

才、算力为一体的数

据智能解决方案，为产业

创造价值。2014 年至今，天池已

成功运作 400

余场高规格数

据类竞赛，覆盖全球 98

个国

家和地区的 60 万数据开发

者。天池平台上的竞赛

课

题以解决实际场景中的

业务痛点为主，实战性和

应用性极强，除了本书涉

及的机器学习算

法竞赛

外，还有创新应用大赛、程

序设计大赛等，奖励也非

常丰厚。天池大数据竞赛

平台见

图

1.3。

图 1.3 天池大数据

竞赛平台

注册

和大多数

的竞赛网站一样，为了防

止建立小号等作弊行为

，保证比赛公平、公正，

天池

等平台都需要通过邮箱

或者手机号注册，并且需

要上传个人证件进行实

名认

证。

赛制

天池同样由

赛题介绍、开源社区等板

块组成，也都设有 A 榜、B

榜。和

Kaggle 不同

的是，天池通常会设

置初赛和复赛，且各自有

A 榜、B 榜。天池的 B

榜通常是换

数

据测试，并且会持续几

天，因此相比于 A 榜，它只是

时间上有所缩短，而 Kaggle 的

测

试数据是预先全部给出

，只是在评分的时候只计

算 A 榜部分，最后选定两个

作为

B 榜计算的结果文件

。此外，天池在初赛，也就是

离线赛阶段，是固定时间

点评

测；在复赛，也就是平

台赛阶段，选手在本地调

试算法并完成模型训练

，提交推断

过程的 Docker 镜像，由

镜像产生预测结果，将会

进行实时评测。天池也会

限制参赛

者每一天的提

交次数，限制次数一方面

是为了缩短不同参赛者

的资源配置之间的差

距

，防止有些参赛者凭借其

强大的计算资源获取不

当优势，另一方面是为了

避免参

赛者过多地依赖

于测试结果进行建模，导

致模型陷入过拟合的泥

沼，使得模型的泛

化性较

弱，做许多无用功。

积分

天

池设计有积分规则，根据

积分或者条件为参赛者

设计了五个等级称号，从

低到高

分别是数据新手

、数据极客、数据大神、数据

科学家以及数据大师，天

池会显示出

积分排名前

一百的选手，这也是天池

的一个特别做法。其他各

个竞赛平台基本只显

示

排行榜的前一百名。

1.1.3 DF

DF（DataFountain）是 CCF（即

中国计算机学会）指定的

专业大数据及人工智能

竞赛平台，

与学术界联系

紧密。DF 平台按照技术（比如

数据挖掘、自然语言处理

和计算机视觉等）以

及行

业（比如金融、医疗、互联网

、安全、电力、娱乐、交通、智慧

城市、通信、工业、零

售、社会

、汽车、教育、物流、地产、大数

据等）对竞赛进行区分，将

学术界和工业界紧密

联

系在一起，虽然奖金金额

可能比不上大平台，但其

对行业的细分理解以及

落地场景的多样

化还是

非常诱人的。

1.1.4 DC

DC 竞赛平台的

全名为 DataCastle，即数据城堡，是一

家坐落于成都的公司。其

网站架构和

竞赛举办方

式与 Kaggle、天池相似，独特的一

点在于其专门设有政企

办赛的部分。通常，参

赛者

可以在

DC 平台上看见许多

由政府、国企、央企扶持的

相关创业竞赛项目。除了

本书专

注的算法竞赛之

外，还有创意赛等。

1.1.5 Kesci

Kesci

的中文

名为和鲸社区，是每年中

国高校计算机大赛——大数

据挑战赛的战略合作平

台。相比于 DF 平台和 DC 平台，Kesci 还

能提供在线的

notebook 训练环境

，这点对一些没

有足够硬

件资源的参赛者来说比

较友好。

1.1.6 JDATA

JDATA

智汇平台是京东

旗下的竞赛平台，其板块

设置大多和天池、Kaggle 类似，只

是细节

方面有所不同。每

年的春季是京东自家出

赛题的高峰时段。有意思

的是，京东的竞赛主要涉

及电商以及物流，它们通

常会自定义一些评价指

标，参赛者在拿到宽表数

据之后需要自行考

虑建

模方案，包括搭建训练集

与测试集、选取样本标签

等，数据质量和赛题难度

都极高。当

然，奖金也不少

，而且在校大学生还有获

得校招绿色通道的机会

。

1.1.7

企业网站

除了上述列举

的国内外主流竞赛平台

外，有些企业会自己举办

竞赛，他们不与平台合作

，而

是自己单独建立一个

简易网站，比如腾讯的社

交广告算法大赛，虽然只

是有个网站，但此竞

赛依

然十分火爆。此外，还有 FlyAI、AI Challenger

等

。参赛者不必面面俱到地

了解，可以

通过关注一些

公众号来了解赛事的最

新相关信息，如 Coggle 数据科学

、Kaggle 竞赛宝典、麻

婆豆腐

AI 等。

1.2 竞

赛流程

要想成功完成一

次竞赛，一共需要几个步

骤呢？答案是 3

个。首先下载

数据，其次用代码运

行出

结果，最后提交结果。当然

，这只是仿照把大象关进

冰箱需要几步编的一个

笑话，搏君

一笑而已。机器

学习算法竞赛也逃脱不

了所谓的套路，我们通过

总结大量的实战经验后

，将

完成一次竞赛的整个

流程大致分为 5 个部分，即

问题建模、数据探索、特征

工程、模型训

练、模型融合

，如图 1.4 所示。当然，赛前还有

一些准备工作要做，比如

注册账号、完善个

人信息

甚至实名认证，然后点击

想参加的竞赛进行报名

。本节只简单介绍各个竞

赛流程，详

细内容会在本

书第 2

章到第 6 章中讲到。

图

1.4 竞赛流程

1.2.1

问题建模

我们

相信大家都还记得高考

前期，老师们耳提面命地

强调审题的重要性，理解

题目永远是最

先也是最

重要的一步。准确理解题

目想要表达的意思能够

避免我们走许多弯路。在

机器学习

的问题建模中

，并不是所有数据都是特

征加标签这种已经可以

直接加入模型训练的形

式，很

多时候还需要分析

数据进而抽象出建模目

标与方案。虽然通常来说

竞赛的目标明确，但也不

是所有竞赛的数据都是

那种可以直接加入训练

的形式。有些竞赛（如 JDATA

智汇

平台）就

常常会有一些不

同于一般分类和回归评

价指标的评估方式，参赛

者往往需要根据对赛题

的理

解自行利用主办方

提供的数据构造训练集

与测试集，这种竞赛极大

地考验参赛者的问题建

模

水平，这也是这类竞赛

的难点所在。此时问题建

模方式的选取在很大程

度上影响着参赛者的

成

绩好坏。

1.2.2

数据探索

数据探

索是机器学习领域最重

要的概念之一，习惯上被

大家称为 EDA（Exploratory Data

Analysis，探索性数据分

析）。在理解赛题并大致知

道了问题建模的方式之

后，就需要结合

对赛题背

景业务的理解去看看数

据长什么样子、数据是否

和描述相符、数据包含哪

些信息、

数据质量如何等

。首先，要对数据有一个清

晰的认知，主要是宽表中

各个字段的取值含义、

范

围和数据结构等。然后更

深层次的是要结合标签

分析特征的分布状态、训

练集与测试集的

同分布

情况、特征之间的业务关

联以及隐含信息表征等

。总的来说，数据探索是承

上启下的

一步，可以帮助

参赛者更好地理解问题

建模，并为接下来将进行

的特征工程做好准备。

1.2.3 特

征工程

同 EDA 一样，特征工程

（Feature Engineering）也是机器学习领域一个

重要的概念，由其命

名就

可以看出这是一项可以

被称为工程的模块。机器

学习泰斗吴恩达老师在

他著名的斯坦福

大学

CS229 机

器学习课程上曾经说过

，机器学习大多数时候是

在进行特征工程，特征决

定

了机器学习预测效果

的上限，而算法只是不断

地去逼近这个上限而已

，由此可见特征工程的

重

要性。事实上，无论是在竞

赛中还是在实际应用中

，特征工程都是花费时间

最多的模块，

会占去建模

者的大部分精力。

1.2.4

模型训

练

根据问题建立好模型

方案后，根据业务理解进

行相关的数据探索，继而

逐步完善特征工程，

就可

以得到标准的训练集与

测试集结构，接下来就可

以考虑如何进行模型训

练了。在一般的

机器学习

算法竞赛中，参赛者大多

偏爱 GDBT 类的树模型。当然，这

也是由于它们的效果

确

实好，常使用的树模型主

要有 XGBoost 和 LightGBM，这两种模型都有

scikit-learn 的接口

函数，非常便于使

用。此外，有时参赛者需要

用到

LR、SVM 和 RF 等算法，有时需要

用到

DNN、CNN、RNN 等深度学习模型以

及它们的衍生模型，以及

广告领域流行的

FFM 等。

如果

说之前的步骤花费的是

参赛者本人的时间与精

力，那么这一步则主要依

赖于参赛者的计

算资源

。当然，如果不是特别大量

级的数据，模型训练一般

会很快。模型训练这个模

块除了

选择合适的模型

之外，还有一部分需要花

时间的就是参数调优。虽

然只要参数不是设置得

很

离谱，效果都差别不太

大，但对于众多参赛者来

说，即使是一点点的成绩

提升，也可能意味

着排名

的上升。

1.2.5 模型融合

经过前

期烦琐艰辛的各种尝试

之后，终于可以来到喜闻

乐见的模（寻）型（觅）融（队）合

（友）阶段了。每一种算法都

有其自身的优势和局限

性，扬长避短，综合各个算

法的优势可

以使得模型

的效果更好。模型融合有

许多种办法，诸如 Stacking、加权投

票等，第

6 章会详

细介绍这

部分内容。之所以将模型

融合称作寻觅队友，是因

为在竞赛当中，不同参赛

者之间

的个人差异很大

，涉及问题建模、特征工程

、模型训练等流程时都会

有差异，这就导致不同

参

赛者之间的方案存在着

巨大差异，而差异带来的

模型融合效果却是极佳

的，并且差异越

大，效果提

升就越大。这里也建议各

位参赛者如果没有特别

熟悉的队友，可以先自己

做，一

个人走完全部流程

，这也是对自己的一种锻

炼，到后期实在没有想法

了，就可以考虑找成绩

相

近的参赛者进行组队。团

队力量在竞赛当中的重

要性不言而喻，而且后期

组队相当于队伍

前期是

各自提交结果的，变相意

味着多了验证思路的机

会。合理地利用规则进行

竞赛是被允

许和提倡的

。

1.3 竞赛类型

各种眼花缭乱

的竞赛令人跃跃欲试，门

类众多的数据竞赛可以

满足众多参赛者的不同

需求，

同时也促进 AI + 行业的

发展，让社会各界积极探

索人工智能。因此，有必要

介绍一下当今

常见的数

据竞赛类型。下面将分别

从数据类型、任务类型以

及应用场景这三个方面

展开。

1.3.1 数据类型

人工智能

领域大致可以分为计算

机视觉（CV）、自然语言处理（NLP）和

数据挖掘（DM）

三个主要方向

。从数据类型的角度看，又

可以对三者进行简单区

分，计算机视觉领域多是

处

理图像方面的数据，当

然这其中也包括视频；自

然语言处理多是文本数

据，涉及各种语言的

分词

等。这二者都是近几年随

着计算机硬件性能的不

断提升，以及宽带网络的

快速发展而得

到了学术

界以及工业界的共同关

注。Kaggle 上面的竞赛会在题目

下方给出数据类型，如图

片

数据、音频数据、文本数

据、宽表数据等，本书将着

重介绍传统宽表数据类

型的相关竞赛。

在传统宽

表的数据中，通常匹配有

样本的唯一 id 索引以及特

征列。根据含义，特征又可

分

为类别特征（如用户性

别）和数值特征（如年龄、身

高、体重等）。上述这些特征

的形式都

是单值特征，此

外还有多值特征，如用户

的兴趣爱好这列可以同

时包含健身、跑步和摄影

等。针对这种特殊的多值

特征，我们有特别的处理

技巧，这部分也将在第 13 章

中为大家进

行详细讲解

。

1.3.2

任务类型

机器学习相关

竞赛以算法为主，偶尔也

有方案创新设计赛等，本

书将专注于讲解机器学

习算

法相关竞赛，主要是

监督学习的相关内容，即

根据任务要求，通过已有

的、带标签的训练集

数据

进行建模，从而对测试集

数据进行预测并给出相

应标签的结果，从而进行

得分评价。任

务类型按照

问题类型大致可分为分

类以及回归，第 2

章会具体

给出相应任务的评价指

标。

1.3.3 应用场景

提起应用场

景，我们自然而然想到的

是机器学习在各个行业

的应用、行业的需求和痛

点。纵

观各大竞赛平台，涉

及的主要有医学、制造业

产品线、金融、电商、互联网

等。其中互联网

行业用户

数据的丰富性和多样性

，以及较少遇到的如医学

方面的伦理道德等挑战

产生出了很

多应用场景

，如广告、搜索和推荐等，都

是当今人工智能涉足较

多的场景。

1.4 思考练习

01. 请在

Kaggle、天池、DF、DC、Kesci 以及

JDATA 等网站分别注

册账号并浏览，体会

本章

介绍的内容。

02. 完整的竞赛

流程包括哪几个主要部

分，每个部分在流程中的

角色是怎么样的？

03.

以日常

生活接触到的场景为例

，列举 5 项可能使用了机器

学习算法的应用。

第 2 章

问

题建模

当参赛者拿到竞

赛题目的时候，首先应该

考虑的事情就是问题建

模，同时完成基线

（baseline）模型的

管道（pipeline）搭建，从而能够第一

时间获得结果上的反馈

，帮助后续

工作的进行。此

外，竞赛的存在都依赖于

真实的业务场景和复杂

的数据，参赛者通常对此

会

有很多想法，但是线上

提交结果验证的次数往

往有限。因此，合理地切分

训练集和验证集，

以及构

建可信的线下验证就变

得十分重要，这也是保障

模型具有泛化性的基础

。

竞赛中的问题建模主要

可以分为赛题理解、样本

选择、线下评估策略三个

部分，本章先从这

三个部

分介绍问题建模的对应

工作，同时给出相应的使

用技巧和应用代码。然后

带领读者进

行一个实战

案例的演练，以帮助读者

理解和应用本章内容。

2.1 赛

题理解

赛题理解其实是

从直观上梳理问题，分析

问题可解的方法、赛题背

景、赛题的主要痛点。厘

清

一道赛题要从赛题背景

引发的赛题任务出发，理

解其中的业务逻辑以及

可能对赛题有意义

的外

在数据有哪些，并对于赛

题数据有一个初步的了

解，如知道现在和任务相

关的数据有哪

些，其中数

据之间的关联逻辑是什

么样的。通常，赛题任务会

给出赛题背景、赛题数据

和评

价指标。赛题理解的

这一部分工作会成为竞

赛的重要组成部分和先

决条件，通过对赛题的理

解，对真实业务的分析，我

们可以用自身的先验知

识进行初步分析，很好地

为接下来的部分

做出铺

垫。

2.1.1

业务背景

深入业务

竞

赛本身是因特定场景而

存在的，同时很多操作也

会因场景的不同而大不

一样。这

里提到的场景指

的就是业务，那么该如何

分析业务呢？

比如，分析用

户的购买行为，这里就需

要知道用户购买的目的

、所购买产品能够吸

引用

户的地方、公司能够提供

的产品、产品与用户需求

是否一致、目标用户定位

、

用户复购情况、用户的购

买能力和支付方式。简而

言之，就是把自己当作商

家或者

用户来换位思考

和梳理这个过程。

接下来

，进行一次更直观的业务

理解，以此展示出实际竞

赛中的分析过程，如图 2.1

所

示。这是一个以互联网金

融信贷业务为背景的赛

题，目标是预测用户的还

款金额

与日期，也就是预

测用户的还款情况。如果

从商家的角度考虑，用户

的还款意愿和

还款能力

将成为影响还款情况的

关键因素。接下来，我们一

起串一下业务线。首

先，用

户去借贷，商家就会考虑

用户借贷的金额、用户是

否有欺诈倾向和历史借

贷

情况；然后，用户借贷成

功，商家要考虑用户当前

的负债情况、距离还款时

间以及

工资日；最后，用户

成功还款，商家要分析用

户逾期情况、所剩欠款和

当前期数

等。这样就模拟

出了基础的业务线。

图

2.1 用

户还款情况预测业务

上

文介绍了如何进行业务

理解，在接下来的内容中

，我们将阐述如何将赛题

目标与

业务紧密联合起

来，为竞赛成绩带来收益

。

明确目标

真实业务涵盖

的内容通常来说比竞赛

涉及的更加广泛，因此赛

题目标只是其中的一

部

分，真实业务还包括主办

方提供的数据。在上面的

例子中，赛题目标是预测

用户

还款金额与日期，那

么参赛者可以先根据此

目标来分析相关业务，即

影响用户还款

的因素等

；然后再将业务中的信息

反馈到赛题目标中，即工

资日、总借款次数等。

将赛

题目标与真实业务紧密

连接在一起的是数据，有

了具体的数据，才能根据

业务

提取出特征来显性

表示用户还款的情况，所

以为了进一步深入理解

赛题，还需要对

数据有一

个初步的认识。

2.1.2 数据理解

我们可以将数据理解分

为两个部分，分别是数据

基础层和数据描述层。当

然，在问题建模阶

段，并不

需要对数据有特别深的

理解，只需要做基本的分

析即可。在后面的数据探

察阶段，

再深入理解数据

，从数据中发现关键信息

。

数据基础层。各种竞赛主

办方提供的原始数据质

量良莠不齐，数据形态如

数据类

型、存储格式等也

是多种多样。为了进一步

分析和建模，往往需要对

原始数据进行

清洗、加工

和计算等处理。数据基础

层重点关注的是每个数

据字段的来源、生产过

程

、取数逻辑、计算逻辑等，了

解了这些才能正确地理

解、选取并使用每一个原

始

字段，从而加工计算得

出所需的更多衍生字段

，数据最终的呈现方式通

常是数据表

格。

数据描述

层。数据描述层主要是在

处理好的数据基础层上

进行统计分析和概括描

述，这个层面的重点在于

尽可能地通过一些简单

统计量（如均值、最值、分布

、增

幅、趋势等）来概括整体

数据的状况，也使得参赛

者能够清晰地知晓数据

的基本情

况。然而具体使

用哪些统计量，并没有统

一的标准，这要根据数据

呈现的具体场景

而定。比

如，对于时间序列问题，可

以统计其增幅、趋势和周

期；对于常规的数值

特征

，则可以观察其均值、最值

和方差等统计量；对于存

在多类别的样本集合，则

可以使用分布、分位点等

进行描述。

基于以上这两

个层面的数据探索，参赛

者可以对数据有一个基

本的认识，这些理解将会

对之

后进行的数据预处

理、特征提取等起到关键

性的作用。

2.1.3

评价指标

01. 分类

指标

分类问题不仅是竞

赛中常出现的一种核心

问题，也是实际应用中常

见的一种机器学

习问题

。评价分类问题的效果要

比评价回归问题的效果

困难很多，这两类问题都

包

含各式各样的评价指

标。本书将会撇开传统的

介绍方式，结合实际应用

出发，总结

评价指标的特

性和优缺点，帮助参赛者

在竞赛中获得一定收益

。

竞赛中常见的分类指标

包括错误率、精度、准确率

（precision，也称查准率）、召回

率（recall，也称

查全率）、F1-score、ROC 曲线、AUC 和对数损失

（logloss）等。

其实这些指标衡量的

都是模型效果的好坏程

度，且相互之间是有关系

的，只是各自

的侧重点不

同。在我们理解了各指标

的定义后，就能找出它们

的区别与联系。下面

将对

上述指标进行简单的介

绍，并给出一个例子来解

释这些指标。

错误率与精

度

在分类问题中，错误率

是分类结果错误的样本

数占样本总数的比例，精

度则

是分类结果正确的

样本数占样本总数的比

例。即，错误率 =

1–精度。

准确率

与召回率

以最简单的二

分类为例，图 2.2 给出了混淆

矩阵的定义来源，其中 TP、

FN、FP、TN 分

别表示各自群体的样本

数量。

图 2.2 混淆矩阵

其中基

本的逻辑是，对模型预测

出的概率值给定一个阈

值。若概率值超过阈

值，则

将样本预测为 1（Positive，正类），否则

预测为 0（Negative，负

类）。

True Positive（TP）：预测类别为

1，真实类别为

1，预测正确。

False Positive（FP）：预

测类别为 1，真实类别为 0，预

测错误。

True

Negative（TN）：预测类别为 0，真实

类别为 0，预测正确。

False Negative（FN）：预测类

别为 0，真实类别为

1，预测错

误。

准确率 是指被分类器

判定为正类的样本中真

正的正类样本所占的比

重，

即被分类器判为正类

的所有样本中有多少是

真正的正类样本，其公式

定义见

式 (2-1)：

由此易知，如果

只做一个单独的正样本

预测，并且预测类别正确

，则通过此

公式可得到 100% 的

准确率。但这没有什么意

义，这会使得分类器忽略

除正

样本之外的数据，因

此还需考虑另一个指标

，即召回率 。

召回率是指被

分类器正确判定的正类

样本占总的正类样本的

比重，即所有正

类样本中

有多少被分类器判为正

类样本，定义如式 (2-2)：

准确率

和召回率反映了分类器

性能的两个方面，单依靠

其中一个并不能较为

全

面地评价一个分类器的

性能。一般来说，鱼与熊掌

不可兼得，你的准确率

越

高，召回率越低；反之，召回

率越高，准确率越低。继而

为了平衡准确率

和召回

率的影响，较为全面地评

价一个分类器，便有了 F1-score 这

个综合了

这两者的指标

。

F1-score

很多机器学习分类问题

都希望准确率和召回率

能同时都高，所以可以考

虑使

用调和平均公式，以

均衡这两个指标，从而避

免在使用算术平均时，由

于其

中一个较高，另一个

较低，出现均值虚高的现

象。F1-score 就能起到这样一

个作

用，其定义如式 (2-3)：

我们很容

易看出，其最大值是

1，最小

值是 0。构建一个计算准确

率、召回

率和 F1-score 的评价代码

也很简单，具体实现代码

如下：

from

sklearn.metrics import precision_score, recall_scroe, f1_score

precision

= precision_score(y_train, y_pred)

recall = recall_scroe(y_train,

y_pred)

f1 = f1_score(y_train, y_pred)

ROC

曲线

除了上述几种

评价指标之外，还有一种

常用于度量分类中的非

均衡性的工

具，即 ROC 曲线（接

受者操作特征曲线）。ROC 曲线

用于绘制采用不同分

类

阈值时的 TP 率与 FP 率。降低分

类阈值会导致更多样本

被归为正类别，从

而增加

假正例和真正例的个数

。图

2.3 是一个比较典型的 ROC 曲

线。另外，

ROC 曲线与

AUC 常被用来

评价一个二值分类器的

优劣，那么这里就有一个

问题：既然已经有了这么

多评价指标，那么为什么

还要使用 ROC 曲线呢？

图 2.3

不同

分类阈值下的 TP率与 FP率

在

实际的数据集中，经常会

出现正负样本不均衡的

现象，即负样本比正样本

多很多（或者相反），而且测

试集中正负样本的分布

也可能随着时间发生变

化。ROC 曲线有一个很好的特

质，那就是在这种情况下

，它依然能够保持不

变。不

过

ROC 曲线在竞赛中倒是不

常见，反而 AUC 可以说是我们

的老朋

友，在分类问题中

经常出现。

AUC

在互联网的搜

索、推荐和广告的排序业

务中，AUC 是一个极其常见的

评价指

标。它定义为 ROC 曲线

下的面积，因为 ROC

曲线一般

都处于 这条

直线的上方

，所以取值范围在 0.5 和 1

之间

。之所以使用 AUC 作为评价指

标，是因为 ROC 曲线在很多时

候并不能清晰地说明哪

个分类器的效果更

好，而

AUC

作为一个数值，其值越大

就代表分类器的效果越

好。

值得一提的是 AUC 的排序

特性。相对于准确率、召回

率等指标，AUC 指标

本身和模

型预测的概率绝对值无

关，它只关注样本间的排

序效果，因此特别

适合用

作排序相关问题建模的

评价指标。AUC 是一个概率值

，我们随机挑选

一个正样

本和一个负样本，由当前

的分类算法根据计算出

的分数将这个正样

本排

在负样本前面的概率就

是 AUC 值。所以，AUC

值越大，当前的

分类算

法就越有可能将

正样本排在负样本值前

面，即能够更好地分类。

深

度思考

既然 AUC 与模型预测

的分数值无关，那这为何

是很好的特性？假设你

采

用的是准确率、F1-score 等指标，而

模型预测的分数是个概

率值，那

么就必须选择一

个阈值来决定把哪些样

本预测为 1，哪些预测为 0。阈

值的选择不同，准确率、召

回率与 F1-score

的值就会不同，而

AUC 可

以直接使用模型预测

分数本身，参考的是相对

顺序，更加好用。在竞赛

任

务中，参赛者更是省去了

试探阈值的麻烦。

对数损

失

该指标可用于评价分

类器的概率输出。对数损

失通过惩罚错误的分类

来实现

对分类器的准确

度的量化。最小化对数损

失基本等价于最大化分

类器的准确

度。为了计算

对数损失，分类器必须提

供概率结果，即把输入样

本喂入模型

后，预测得到

每个类别的概率值（0 和 1 之

间），而不只是预测最可能

的类

别。对数损失函数 的

函数标准形式见式 (2-4)：

对于

样本点 来说， 是真实标签

，在二分类问题中，其取值

只可能

为 0 或 1。假设某个样

本点的真实标签为 ，该样

本点取 的概率为

，则该样

本点的损失函数如式 (2-5)：

我

们不妨想想，AUC 同样是只需

要给出模型预测的概率

值，就能计算并衡量

模型

效果，那么对数损失与它

的区别在哪里呢？

对数损

失主要是评价模型预测

的概率是否足够准确，它

更关注和观察数据的

吻

合程度，而 AUC 评价的则是模

型把正样本排到前面的

能力。由于两个指

标评价

的侧重点不一样，因此参

赛者考虑的问题不同，所

选择的评价指标就

会不

同。对于广告 CTR

预估问题，如

果考虑广告排序效果，就

可以选择

AUC，这样也不会受

到极端值的影响。此外，对

数损失反映了平均偏差

，更

偏向于将样本数量多

的那类划分准确。

深度思

考

在各种数据竞赛的分

类问题中，AUC 和对数损失基

本是最常见的模型评

价

指标。通常来说，AUC 和对数损

失比错误率、精度、准确率

、召回

率、F1-score 更常用，这是为什

么呢？因为很多机器学习

模型对分类问

题的预测

结果都是概率值，如果要

计算上述这些指标，就需

要先把概率

转化成类别

，这需要人为设置一个阈

值，如果对一个样本的预

测概率高

于这个阈值，就

把这个样本判到相应类

别里面；如果低于这个阈

值，则

1

放进另一个类别里

面。所以阈值的选取在很

大程度上影响了分值的

计

算，不利于准确评价参

赛者的模型效果，而使用

AUC 或者对数损失则

可以避

免把预测概率转换成类

别的麻烦。

02. 回归指标

平均

绝对误差

首先考虑一个

问题，如何去衡量一个回

归模型的效果呢？大家自

然而然地会

想到采用残

差（真实值与预测值的差

值）的均值，即式 (2-6)：

可是这里

会存在一个问题，当真实

值分布在拟合曲线两侧

时，对于不同样本

而言，残

差有正有负，直接相加就

会相互抵消，因此考虑采

用真实值和预测

值之间

的距离来衡量模型效果

，即平均绝对误差（MAE，Mean Absolute

Error），又被称

为 L1

范数损失，其定义如式

(2-7)：

平均绝对误差虽然解决

了残差加和的正负抵消

问题，能较好衡量回归模

型的

好坏，但是绝对值的

存在导致函数不光滑，在

某些点上不能求导，即平

均绝

对误差不是二阶连

续可微的，同时二阶导数

总为 0。

扩展学习

在 XGBoost 里面，可

以使用平均绝对误差作

为损失函数进行模型训

练，

但是由于其存在局限

性，大家通常会选择 Huber 损失

进行替换。那为何

要使用

Huber 损失呢？由于平均绝对误

差并不是连续可导的（在

0 处不

可导），所以需要使用

可导目标函数来逼近平

均绝对误差。而对于下述

将会讲到的均方误差（MSE），梯

度又会随着损失的减小

而减小，使预

测结果更加

精确。在这种情况下，Huber 损失

就非常有用，它会由于梯

度的减小而落在最小值

附近。比起均方误差，Huber

损失

对异常点更具

健壮性。因

此，Huber 损失结合了平均绝对

误差和均方误差的优点

。但

是，Huber 损失的问题可能需

要我们不断地调整超参

数 delta。

均方误差

和平均绝对

误差对应的还有均方误

差（MSE，Mean Squared Error），又被称

为 L2

范数损失，其

定义如式 (2-8)：

由于均方误差

与数据标签的量纲不一

致，因此为了保证量纲一

致性，通常需

要对均方误

差进行开方，这也就出现

了均方根误差（RMSE）。

深度思考

那么平均绝对误差与均

方误差有何区别呢？均方

误差对误差（真实值–

预测

值）取了平方，因此若误差

＞

1，则均方误差会进一步增

大误差。

此时如果数据中

存在异常点，那么误差值

就会很大，而误差的平方

则会

远大于误差的绝对

值。因此，相对于使用平均

绝对误差计算损失，使用

均方误差的模型会赋予

异常点更大的权重。也就

是说，均方误差对异常

值

敏感，平均绝对误差对异

常值不敏感。

均方根误差

均方根误差用来评价回

归模型的好坏，是对均方

误差进行开方，缩小了误

差

的数值，其定义如式

(2-9)：

上

面介绍的几种衡量标准

的取值大小都与具体的

应用场景有关，因此很难

定

义统一的规则来衡量

模型的好坏。同样，均方根

误差也存在一定的局限

性。

例如，在计算广告的应

用场景中，需要预测广告

的流量情况时，某些离群

点

可能导致均方根误差

指标变得很差，即使模型

在 95%的数据样本中，都能预

测得很好。如果我们不选

择过滤掉离群点，就需要

找一个更合适的指标来

评

价广告流量的预测效

果。下面将介绍平均绝对

百分百误差（MAPE），它是

比均方

根误差更加健壮的评价

指标，相当于把每个点的

误差进行了归一化，

降低

了个别离群点对绝对误

差带来的影响。

平均绝对

百分比误差

平均绝对百

分比误差（MAPE）与平均绝对误

差的二阶导数都是不存

在

的。但不同于平均绝对

误差，平均绝对百分比误

差除了考虑预测值与真

实值

的误差，还考虑了误

差与真实值之间的比例

，比如 2019 腾讯广告算法大

赛

，虽然预测值与真实值的

差值是一样的，但是由于

使用了平均绝对百分比

误差来评测，因此其真实

值越大，误差反而越小，平

均绝对百分比误差的定

义如式 (2-10)：

1如无特殊说明，本

书中的

log 函数表示以 e 为底

的对数

2.2 样本选择

即使是

在实际的竞赛当中，主办

方提供的数据也有可能

存在令参赛者们十分头

疼的质量问

题。这无疑会

对最终预测结果造成很

大的影响，因此需要考虑

如何选择出合适的样本

数据进

行训练，那么如何

才能够选择出合适的样

本呢？在回答这个问题之

前，先来看看影响结果的

具体原因又是什么，这里

总结出四个主要原因：分

别是数据集过大严重影

响了模型的性能，

噪声和

异常数据导致准确率不

够高，样本数据冗余或不

相关数据没有给模型带

来收益，以及

正负样本分

布不均衡导致数据存在

倾斜。

2.2.1

主要原因

数据集过

大

机器学习算法相关竞

赛由于涉及各行各业的

应用场景，数据量也是多

寡不一。类似

搜索推荐以

及广告等相关竞赛的数

据量级达到千万级甚至

亿级，过大的数据集会严

重影响各种特征工程和

建模方式的快速验证。在

大多数情况下，我们的计

算资源有

限，需要考虑数

据采样处理，然后在小数

据集上建模分析。此外，在

特定的业务场

景下，也许

可以过滤掉一些对建模

没有意义的数据，这样可

以帮助提高模型性能。

数

据噪声

数据的噪声主要

有两种来源，一种是采集

数据时操作不当导致信

息表征出现错误，

另一种

则是数据本身的特性存

在合理范围内的抖动导

致噪声与异常。数据噪声

的存

在具有两面性，一方

面，噪声的存在会导致数

据质量变低，影响模型的

效果；但另

一方面，我们也

能通过在训练集中引入

噪声数据的方法使得模

型的健壮性更强。此

外，若

是噪声数据的来源为第

一种，则需要对应地去看

是否能够解码出正确数

据，

这种情况有时会极大

地提升建模效果。因此，当

需要处理噪声数据的时

候，首先考

虑是否为采集

错误导致的，其次再去权

衡模型的泛化性和模型

的当前效果。有时候

去噪

反而会导致模型的泛化

性能变差，在换了数据集

之后，模型的效果无法得

到很

好的保证。

要去噪，首

先要识别出噪声，然后采

取直接过滤或者修改噪

声数据等多种办法。噪

声

数据可能是特征值不对

，比如特征值缺失、超出特

征值域范围等；也可能是

标注

不对，比如二分类问

题的正样本标注成了负

样本。数据去噪很多是检

测和去除训练

数据中标

注带噪声的实例。3.1 节将展

示去除噪声或异常数据

的具体处理办法。

数据冗

余

通常来说，数据冗余与

数据集过大是不同的概

念。提到数据集，会自然而

然地想到

是样本的集合

，它的大小通常表示纵向

的样本数量，而数据冗余

则侧重于描述数据

特征

的冗余。数据中存在的冗

余不仅会影响到模型性

能，更会引入噪声与异常

，有

可能为模型带来反效

果。数据冗余的一个典型

解决方案就是进行特征

选择，这部分

会在 4.4 节中着

重进行讲解。

正负样本分

布不均衡

在二分类正负

样本不均衡的机器学习

场景中，数据集往往是比

较大的，为了让模型

可以

更好地学习数据中的特

征，让模型效果更佳，有时

需要进行数据采样，这同

时

也避免了因为数据集

较大而导致计算资源不

足的麻烦。这是比较浅层

的理解，更本

质上，数据采

样就是模拟随机现象，根

据给定的概率分布去模

拟一个随机事件。另

外，有

一种说法是用少量的样

本点去近似一个总体分

布，并刻画总体分布中的

不确

定性。大多数竞赛提

供的数据都是主办方从

真实完整的数据中提取

出来的一部分，

并且会保

证数据分布的一致性，降

低竞赛难度，保证效率。更

进一步，也可以从总

体样

本数据中抽取出一个子

集（训练集）来近似总体分

布，然后训练模型的目的

就

是最小化训练集上的

损失函数，训练完成后，需

要用另一个数据集（测试

集）来评

价模型。数据采样

也有一些高级用法，比如

对样本进行过采样或者

欠采样，或者在

目标信息

保留不变的情况下，不断

改变样本的分布来适应

模型训练与学习，这常用

于解决样本不均衡的问

题。

2.2.2

准确方法

在竞赛中，若

是在拿到数据后发现数

据存在数据集过大以及

正负样本不均衡这两种

情况，则

需要在一开始就

针对性地给出解决方案

。即如何处理以下两个问

题：在数据量非常大的情

况

下，为了降低成本，如何

提高模型训练速度；针对

正负样本分布不均衡的

场景，如何通过数

据采样

解决这类问题。

首先，针对

第一个问题，主要推荐以

下两种解决办法。

简单随

机抽样。这里分为无放回

和有放回两种，做法均比

较简单，不做具体介绍。

分

层采样。该方法分别对每

个类别进行采样。这是从

一个可以分成不同子集

（或称

为层、类别）的数据集

中，按规定的比例从不同

类别中随机抽取样本的

方法。其优

点是样本的代

表性比较好，抽样误差比

较小；缺点是抽样过程比

简单随机抽样要繁

杂些

。

针对第二个问题，则主要

有以下三种解决方法。

评

分加权处理。分布不均衡

的问题时有出现，包括欺

诈交易识别和垃圾邮件

识别

等，其正负样本的数

量分布差距极大。图 2.4 给出

了某个竞赛正负样本的

分布情况，

正样本的比例

只有 2%

左右。考虑到正样本

的重要性高于负样本，在

模型训练以及

评价的时

候就可以设计相应的得

分权重，使得模型能够学

习到需要获得关注的部

分。评分加权处理的办法

是比较常见的一种。当然

，在不同的应用场景中可

以选择

不同的加权方式

，例如多分类问题中的 Micro Fscore 指

标以及

KDD CUP 2019 竞赛

中采用的 Weighted

Fscore 指

标，这两种评价指标对不

同类别的权重是不一样

的，通

过对不同类别进行

加权的方式可以提升模

型的预测效果。

图 2.4 目标变

量分布

此方法具体的操

作步骤是，首先遍历所有

样本，根据样本是否满足

某个要求来给予

其权重

。例如，在不均衡的二分类

中，如果样本的标签为 1，我

们就将其权重设置为

（自

定义）；如果样本标签为 0，就

将其权重设置为 （自定义

）。然后将样

本权重代入模

型进行训练和测试。

加权

的直观含义从业务上理

解就是认为一个正样本

的价值大于多个负样本

的，因此

希望模型在训练

的时候能更多地从正样

本身上学到关键信息，当

它学得不好的时

候，就要

对它加大惩罚力度。

欠采

样。就是从数量较多的一

类样本中随机选取一部

分并剔除，使得最终样本

的目

标类别不太失衡。常

用的方法有随机欠采样

和

Tomek Links，其中 Tomek Links 先是

找出两个各

项指标都非常接近的相

反类样本，然后删除这类

样本中标签（label）占

比高的，这

类算法能够为分类器提

供一个非常好的决策边

界。

过采样。主要是对样本

较少的类别进行重新组

合，构造新样本。常用的方

法有随机

过采样和 SMOTE 算法

。SMOTE 算法并不是简单复制已

有的数据，而是在原有数

据

的基础上，通过算法产

生新生数据。欠采样与过

采样的示意图见图 2.5。

图 2.5 欠

采样与过采样

2.2.3

应用场景

那么，在什么样的场景下

需要处理样本的不均衡

问题呢？下面给出了一些

具体的场景，以帮

助参赛

者更好地应对这类问题

。

如果竞赛任务对于召回

有特别大的需求，也就是

说对每个正样本的预测

都远远比对

负样本的预

测更重要，那么这个时候

如果不做任何处理，就很

难取得比较好的建模

结

果。

如果竞赛的评价指标

是

AUC，那么参赛者在实战过

程中就会发现，这个时候

处理和

不处理样本不均

衡问题的差别没那么大

。但这也好比一个参数的

波动，将处理后的

结果和

不处理的结果进行融合

后，评价指标一般都是有

细微提升的。

如果在竞赛

任务中正样本和负样本

是同等重要的，即预测正

确一个正样本和预测正

确一个负样本是同等重

要的，那么其实不做其他

的处理也影响不大。

2.3 线下

评估策略

通常在数据竞

赛中，参赛者是不能将全

部数据都用于训练模型

的，因为这会导致没有数

据集

对该模型的效果进

行线下验证，从而无法评

估模型的预测效果。为了

解决这一问题，就需要

考

虑如何对数据进行切分

，构建合适的线下验证集

。针对不同类型的问题，需

要不同的线下

验证方式

，本书将这些问题大致分

为强时序性和弱时序性

两类，然后以此来确定线

下验证方

式。

2.3.1

强时序性问

题

对于含有明显时间序

列因素的赛题，可将其看

作强时序性问题，即线上

数据的时间都在离线

数

据集之后，这种情况下就

可以采用时间上最接近

测试集的数据做验证集

，且验证集的时间

分布在

训练集之后。图 2.6 为时间序

列分割验证方式。

图 2.6 时间

序列分割验证

例如，天池

平台上的“乘用车零售量

预测”竞赛，初赛提供 2012 年

1 月

至 2017 年 10 月盐城

分车型销量

配置数据，需要参赛者预

测 2017 年 11 月的盐城分车型销

量数据。这是一个很明

显

的含时间序列因素的问

题，那么我们可以选择数

据集的最后一个月作为

验证集，即

2017

年 10 月的数据。

2.3.2 弱

时序性问题

这类问题的

验证方式主要为 折交叉

验证（ -fold Cross Validation），根据 的取值不

同，会

衍生出不同的交叉验证

方式，具体如下。

(1) 当 时，这是

最简单的 折交叉验证，即

2 折交叉验证。这个时候将

数据集分

成两份：D1 和 D2。首先

，D1 当训练集，D2 当验证集；然后

，D2 当训练集，D1

当验证

集。2 折交

叉验证存在很明显的弊

端，即最终模型与参数的

选取将在极大程度上依

赖于事先

对训练集和验

证集的划分方法。对于不

同的划分方式，其结果浮

动是非常大的。

(2) 当

时，也就

是 折交叉验证，被称作留

一验证（Leave-one-out Cross

Validation）。具体做法是只用

一个数据作为测试集，其

他数据都作为训练集，并

重复 次

（

为数据集的数据

量）。这种方式的优点和缺

点都是很明显的。其优点

在于，首先它不

受测试集

和训练集划分方法的影

响，因为每一个数据都单

独做过测试集；其次，它用

了

个数据训练模型，也几

乎用到了所有的数据，从

而保证模型的偏差更小

。同时，

其缺点也很明显，那

就是计算量过大。如果数

据集是千万级的，那么就

需要训练千万次。

(3) 为了解

决

(1) 和 (2) 的缺陷，我们一般取

或 10，作为一种折中处理，这

也是最

常用的线下验证

方式。比如，

，如图 2.7 所示，我们

把完整的训练数据分为

5 份，

用其中的 4

份数据来训

练模型，用剩余的 1 份来评

价模型的质量。然后将这

个过程在 5 份数

据上依次

循环，并对得到的

5 个评价

结果进行合并，比如求平

均值或投票。下面给出通

用的

交叉验证代码，其中

参数 NFOLDS 用来控制折数，具体

代码如下：

from

sklearn.model_selection import KFold

NFOLDS = 5

folds = KFold(n_splits= NFOLDS, shuffle=True, random_state=2021)

for trn_idx, val_idx in folds.split(X_train, y_train):

train_df, train_label = X_train.iloc[trn_idx, :] ,

y_train[trn_idx]

valid_df, valid_label = X_train.iloc[val_idx, :]

, y_train[val_idx]

图 2.7 五折交叉验

证

2.4

实战案例

作为本章的

总结，下面将引领读者对

本章内容学以致用，进行

一个 Kaggle 经典入门竞赛实

战

——房价预测，赛题主页如图

2.8 所示。本节包含赛题理解

和线下验证。希望读者在

理解

本章内容、认真分析

业务、完善线下评估工作

后，能够快速搭建起一个

baseline（基线），

并得到令自己满意

的结果。

图 2.8 赛题主页展示

2.4.1 赛题理解

首先，熟悉下赛

题的业务背景，本赛题要

求预测最终的房价，其数

据集中有 79 个变量，几

乎涵

盖了爱荷华州艾姆斯（Ames, Iowa）住

宅的方方面面，可以看到

影响房价的因素有很

多

，具体如图

2.9 所示。

图 2.9 房屋价

格的影响因素

在上面影

响房价的因素中，有如下

几个关键的价值影响因

素。

房屋位置。位置是高估

值的关键，比如靠近大型

商圈、重点学校或者接近

市中心的

位置，一般房价

是比较高的。

形状尺寸。房

子包含的空间、房间和土

地越多，估价就越高。

内部

组成。最新的公用设施和

附加设施（如车库）是非常

可取的价值影响因素。

这

些初步的认识和业务分

析对于第 4

章有很大的帮

助。接下来，我们将导入数

据，并观察数

据的基本信

息，以得到一个基本的认

识，这些都将对之后的数

据预处理、特征提取等起

到关

键性的作用。

首先，导

入基本的模块：

import numpy

as np

import pandas as pd

from sklearn.model_selection import KFold

from sklearn.metrics

import mean_squared_error

from sklearn.preprocessing import OneHotEncoder

import lightgbm as lgb

接着，加载

数据：

train

= pd.read_csv('train.csv')

test = pd.read_csv('test.csv')

然后，看下数据的基

本信息：

train.describe()

最后，对数据进行

基本处理：

all_data = pd.concat((train,test))

all_data

= pd.get_dummies(all_data)

# 填充缺失值

all_data =

all_data.fillna(all_data.mean())

# 数

据切分

X_train = all_data[:train.shape[0]]

X_test = all_data[train.shape[0]:]

y = train.SalePrice

本赛题使用均方

误差作为评价指标，其计

算方式如式 (2-11)：

2.4.2 线下验证

验

证的代码如下：

#

K 折交叉验

证

from sklearn.model_selection import KFold

folds = KFold(n_splits= 5, shuffle=True, random_state=2021)

# 模型参数

params = {'num_leaves': 63,

'min_child_samples': 50, 'objective': 'regression',

'learning_rate': 0.01,

'boosting_type': 'gbdt', 'metric': 'rmse'}

for trn_idx,

val_idx in folds.split(X_train, y):

trn_df, trn_label

= X_train.iloc[trn_idx, :], y[trn_idx]

val_df, val_label

= X_train.iloc[val_idx, :], y[val_idx]

dtrn =

lgb.Dataset(trn_df, label = trn_label)

dval =

lgb.Dataset(val_df, label = val_label)

bst =

lgb.train(params,dtrn, num_boost_round=1000,valid_sets=[dtrn, dval],

early_stopping_rounds=100, verbose_eval=100)

至此就完成

了基本的问题建模，我们

不仅对赛题有了初步的

理解，同时还搭建出了

baseline，可

以快速地反馈预测结果

。根据初步的结果对模型

进行不断的优化就是之

后的重

要工作。在接下来

的章节中，我们将带着大

家进行数据探索，发现数

据特征，从中挖掘更多

有

用的信息。

2.5 思考练习

01.

对于

多分类问题，可否将其看

作回归问题进行处理，对

类别标签又有什么要求

？

02. 目前给出的都是已有的

评价指标，那么这些评价

指标（分类指标和回归指

标）的损

失函数如何实现

？

03. 解决样本分布不均衡问

题时，尝试用代码实现样

本加权、类别加权和采样

算法等几

种方式，并对比

使用权重前后的分数变

化。

04. 在对不均衡的数据集

进行采样时，是否会影响

训练集和测试集之间的

独立同分布关

系？

05. 在进行

折交叉验证的时候，对于

值的选取，是否越大越好

呢？

06. 在大多数情况下，我们

会选择使用 折交叉验证

，那么 折交叉验证为什么

能够

帮助提升效果呢？

第

3

章 数据探索

数据探索，是

竞赛的核心模块之一，贯

穿竞赛始终，也是很多竞

赛胜利的关键。那么，数据

探索又是什么呢？可以解

决哪些问题？首先应该明

确三点，即如何确保自己

准备好竞赛使用

的算法

模型，如何为数据集选择

最合适的算法，如何定义

可用于算法模型的特征

变量。

数据探索可以帮助

回答以上这三点，并能确

保竞赛的最佳结果。它是

一种总结、可视化和熟

悉

数据集中重要特征的方

法。一般而言，数据探索可

以分为三个部分：首先是

赛前数据探索

（即数据初

探），帮助我们对数据有个

整体性的认识，并发现数

据中存在的问题，比如缺

失

值、异常值和数据冗余

等；其次是竞赛中的数据

探索，通过分析数据发现

变量的特点，帮助

提取有

价值的特征，这里可以从

单变量、多变量和变量分

布进行分析；最后是模型

的分析，

可以分为特征重

要性分析和结果误差分

析，帮助我们从结果发现

问题，并进一步优化。

数据

探索有利于我们发现数

据的一些特征、数据之间

的关联性，有助于后续的

特征构建。本

章将结合实

际竞赛案例来讲解数据

探索。

3.1 数据初探

数据初探

可以看作赛前数据探索

，主要包含分析思路、分析

方法和明确目的。通过系

统化的

探索，我们可以加

深对数据的理解。

3.1.1 分析思

路

在实际竞赛中，最好使

用多种探索思路和方法

来探索每个变量并比较

结果。在完全理解数据

集

后，就可以进入数据预处

理阶段和特征提取阶段

了，以便根据所期望的业

务结果转换数据

集。此步

骤的目标是确信数据集

已准备好应用于机器学

习算法。

3.1.2 分析方法

数据探

索的分析主要采用以下

方法。

单变量可视化分析

：提供原始数据集中每个

字段的摘要统计信息。

多

变量可视化分析：用来了

解不同变量之间的交互

关系。

降维分析：有助于发

现数据中特征变量之间

方差最大的字段，并可以

在保留最大信

息量的同

时减少数据维度。

通过这

些方法，可以验证我们在

竞赛中的假设，并确定尝

试方向，以便理解问题和

选择模

型，并验证数据是

否是按预期方式生成的

。因此，可以检查每个变量

的分布，定义一些丢失

值

，最终找到替换它们的可

能方法。

3.1.3 明确目的

在竞赛

中跳过数据探索阶段将

会是一个很不理智的决

定。由于急于进入算法模

型阶段，很多

选手往往要

么完全跳过数据探索过

程，要么只做一项非常肤

浅的分析工作，这是大多

数选手

的一个非常严重

且常见的错误。这种不考

虑因素的行为可能会导

致数据倾斜，出现异常值

和

过多的缺失值。对竞赛

来说，这样会产生如下一

些糟糕的结果。

生成不准

确的模型。

在错误的数据

上生成精确的模型。

为模

型选择错误的变量。

资源

的低效利用，包括模型的

重建。

熟知可能会产生的

不好影响有助于我们明

确数据探索的主要目的

。一方面，数据探索用于回

答问题，测试业务假设，生

成进一步分析的假设。另

一方面，你也可以使用数

据探索来准备

建模数据

。这两者有一个共同点，那

就是使你对你的数据有

一个很好的了解，要么得

到你需

要的答案，要么发

展出一种直觉来解释未

来建模的结果。

更进一步

，将数据探索的目的具象

化。这里整理出来了数据

探索阶段必须要明确的

7 件事

情，具体如下。

(1)

数据集

基本情况：比如数据有多

大，每个字段各是什么类

型。

(2) 重复值、缺失值和异常

值：去除重复值，缺失值是

否严重，缺失值是否有特

殊含义，

如何发现异常值

。

(3) 特征之间是否冗余：比如

身高的单位用

cm 和 m 表示就

存在冗余。我们可以通过

特征间

相似性分析来找

出冗余特征。

(4)

是否存在时

间信息：当存在时间信息

时，通常要进行相关性、趋

势性、周期性和异常点

的

分析，同时还有可能涉及

潜在的数据穿越问题。

(5) 标

签分布：对于分类问题，是

否存在类别分布不均衡

。对于回归问题，是否存在

异常

值，整体分布如何，是

否需要进行目标转换。

(6)

训

练集与测试集的分布：是

否有很多在测试集中存

在的特征字段在训练集

中没有。

(7) 单变量 / 多变量分

布：熟悉特征的分布情况

，以及特征和标签的关系

。

当我们知道了为什么要

进行数据探索，了解了数

据探索必须要明确的事

情后，会让数据探索

变得

更有目的性。

要开始探索

数据，首先需要导入基本

的库，然后加载给出的数

据集。你可能已经知道了

这样

去做，但是不知从何

下手。多亏 pandas 库，这变成了一

个非常简单的任务，首先

将包导入

为

pd，然后使用 read_csv() 函

数，并将数据所在的路径

和参数传递给该函数，其

中的参

数可用于确保函

数可以正确读取数据，数

据的第一行不会被解释

为数据的列名。

数据探索

最基本的步骤之一是获

取对数据的基本描述，通

过获取对数据的基本描

述从而获得

对数据的基

本感觉。下面的一些方法

用于帮助我们认识数据

。

DataFrame.describe()：查看数据的基本分布，具

体是对每列数据进行统

计，统

计值包含频次、均值

、方差、最小值、分位数、最大

值等。它有助于我们快速

了解

数据分布，并发现异

常值等信息。

DataFrame.head()：可以直接加

载数据集的前五行。

DataFrame.shape：得到

数据集的行列情况。

DataFrane.info()：可以

快速获得对数据集的简

单描述，比如每个变量的

类型、

数据集的大小和缺

失值情况。

以上方法可以

帮助我们了解数据的基

本信息。接下来，我们将通

过具体的操作来展现这

些方

法的强大功能。首先

，通过一段代码展示 nunique 和缺

失值的情况：

stats

= []

for col in train.columns:

stats.append((col, train[col].nunique(),

train[col].isnull().sum() * 100 /

train.shape[0],

train[col].value_counts(normalize=True,

dropna=False).values[0] * 100, train[col].dtype))

stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values',

'Percentage

of missing values',

'Percentage of values

in the biggest category', 'type'])

stats_df.sort_values('Percentage

of missing values', ascending=False)[:10]

图 3.1

展示了经

过上述代码生成的数据

基本信息，我们从中找到

特殊变量进行细致分析

，这

里选择 nunique 值低和缺失值

多的变量进行观察。一般

而言，nunique 为 1

是不具备任何

意

义的，表示所有值都一样

，不存在区分性，需要进行

删除。可以发现，有些变量

的缺失值

很多，比如缺失

比例达到 95% 以上，我们可以

考虑将其删除。

图

3.1 数据的

基本分布

如图 3.2 所示，用柱

状图的形式可以更加直

观地展现变量的缺失值

分布情况。下面是变量缺

失值可视化图的具体生

成代码：

missing

= train.isnull().sum()

missing = missing[missing >

0]

missing.sort_values(inplace=True)

missing.plot.bar()

图 3.2 变量的缺失值

分布

3.2 变量分析

接下来，我

们再进行更细致的分析

，不单是针对每个变量，更

是分析变量之间的联系

以及变

量和标签的相关

性，并进行假设检验，帮助

我们提取有用特征。

3.2.1 单变

量分析

单变量可以分为

标签、连续型和类别型。

标

签

毫无疑问，标签是最重

要的变量，也是一次竞赛

所追求的目标，我们首先

应该观察

标签的分布情

况。对于房屋价格预测，其

标签 SalePrice 为连续型变量，对该

标签

的基本描述见图 3.3。基

本信息生成代码如下：

train['SalePrice'].describe()

图

3.3 对标签 SalePrice

的基本描述

在图

3.3 中，SalePrice 看起来还蛮正常的。接

下来，我们通过可视化的

方式更细

致地观察 SalePrice

的分

布情况，相关代码如下：

plt.figure(figsize=(9, 8))

sns.distplot(train['SalePrice'], color='g', bins=100,

hist_kws={'alpha': 0.4})

得

到的结果如图 3.4 所示。

图

3.4 SalePrice 的

分布情况

在图 3.4 中可以看

到，SalePrice

呈偏离正态分布，属于

向右倾斜类型，存在峰值

状态，一些异常值在 500 000 以上

。我们最终会想办法去掉

这些异常值，得出能够让

算法模型很好地学习的

、符合正态分布的变量。下

面的代码将对 SalePrice 进行对

数

转换，并生成可视化图，转

换结果如图 3.5 所示：

plt.figure(figsize=(9, 8))

sns.distplot(np.log(train['SalePrice']),

color='b', bins=100, hist_kws={'alpha': 0.4})

图 3.5

对 SalePrice 进

行对数转换

连续型

这里

我们可以先观察连续型

变量的基本分布，如图 3.6

所

示。

图 3.6 连续型变量的分布

类似于标签的查看方式

，这里主要使用直方图这

种可视化方式观察值的

分布、每个

值出现的频率

等。下面为连续型变量的

分布可视化图的生成代

码：

df_num

= train.select_dtypes(include = ['float64', 'int64'])

df_num

= df_num[df_num.columns.tolist()[1:5]]

df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8,

ylabelsize=8)

接下来进行更加科学

的分析，首先是相关性分

析。值得注意的是，相关性

分析只能

比较数值特征

，所以对于字母或字符串

特征，需要先进行编码，并

将其转换为数

值，然后再

看特征之间到底有什么

关联。在实际竞赛中，相关

性分析可以很好地过

滤

掉与标签没有直接关系

的特征，并且这种方式在

很多竞赛中均有很好的

效果。

当我们看一个相关

性分析的可视化图时，需

要了解这个图代表什么

，能够从中获取

什么信息

。先学习最基本的概念：正

相关和负相关。

正相关：如

果一个特征增加导致另

一个特征增加，则它们呈

正相关。值 1 表

示完全正相

关。

负相关：如果一个特征

增加导致另一个特征减

少，则它们呈负相关。值–1

表

示完全负相关。

现在假设

特征 A 和特征 B 完全正相关

，这意味着这两个特征包

含高度相似的信息，

信息

中几乎没有或者完全没

有差异。这称为多重线性

，因为两个特征包含几乎

相同

的信息。

在搭建或训

练模型时，如果同时使用

这两个特征，可能其中一

个会是多余的。我们

应该

尽量消除冗余特征，因为

它会使训练时间变长，同

时其他一些优势也会变

没。

下述代码用于生成有

关 SalePrice

的相似性矩阵图：

corrmat = train.corr()

f, ax

= plt.subplots(figsize=(20, 9))

sns.heatmap(corrmat, vmax=0.8, square=True)

生成

的相似性矩阵图如图 3.7 所

示，从中可以找出与房价

相关性强的变量，其中

OverallQual（总

评价）、GarageCars（车库）、TotalBsmtSF（地下室面

积）、GrLivArea（生

活面积）等特征与 SalePrice

呈正相

关，这也非常符合我

们对

业务的直觉。从相似性矩

阵中，不仅能够发现房价

与变量的关系，还能发现

变

量之间的关系，那么如

何利用相似性矩阵进行

分析就成为了关键。

图 3.7 相

似性矩阵

类别型

要知道

，数据探索的目的是帮助

我们了解数据并且构建

有效特征。比如，我们找到

了与标签有着强相关的

特征，那么就可以围绕这

个强相关特征进行一系

列的扩展，

具体可以进行

交叉组合，比如强相关加

弱相关、强相关加强相关

等组合，挖掘更高

维度的

潜在信息。

首先，观察类别

型变量的基本分布情况

，即观察每个属性的频次

。根据频次，我们

不仅能够

很快地发现热点属性和

极少出现的属性，还可以

进一步分析出现这种情

况

的原因，比如淘宝网的

女性用户多于男性用户

，这主要是由于平台在服

饰和美妆业

务方面拥有

强大的影响力。这是从业

务的角度考虑，当然也有

可能是数据采样的原

因

。对部分类别型变量的分

布进行可视化展示，如图

3.8 所示。

图 3.8

类别型变量的基

本分布

3.2.2 多变量分析

单变

量分析太过单一，不足以

挖掘变量之间的内在联

系，获取更加细粒度的信

息，所以多变

量分析就成

了必须。分析特征变量与

特征变量之间的关系有

助于构建更好的特征，同

时降低

构建冗余特征的

概率。此处我们选取本赛

题中需要特别关注的特

征变量进行分析。

从上面

的相似性矩阵中，我们了

解到房屋评价与 SalePrice 呈正相

关。进一步扩展分析，

考虑

房屋评价和房屋位置是

否存在某种联系呢？接下

来，我们将通过可视化的

方式来展现这

两者的联

系，具体实现代码如下：

plt.style.use('seaborn-white')

type_cluster = train.groupby(['Neighborhood','OverallQual']).size()

type_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'PuBu',

figsize=(13,11),

grid=False)

plt.xlabel('OverallQual', fontsize=16)

plt.show()

图

3.9

给出了不同房屋位置的

评价分布条状图，我们可

以发现颜色越深代表评

价越

高，NoRidge、NridgHt 和 StoneBr 都有不错的评

价。

图

3.9 不同房屋位置的评

价分布条状图

通过图 3.10，我

们能够进一步看看不同

位置房屋的 SalePrice 如何。

图 3.10 不同

房屋位置对应的 SalePrice 箱形图

完全符合我们的直觉，高

评价位置（NoRidge、NridgHt 和

StoneBr）对应高

SalePrice，这也

说明房屋位置评价与房

屋售价有比较强的相关

性。除了能通过这样的分

析证明原始特征与 SalePrice 强相

关外，如何通过分析来构

建新的特征呢？

既然房屋

位置和房屋评价的组合

能够出现更高售价的房

屋，那么我们可以构造这

两个类别特

征的交叉组

合特征来进行更细致的

描述，也可以构造这个组

合特征下的房屋均价等

。

3.3 模型分析

3.3.1 学习曲线

学习

曲线是机器学习中被广

泛使用的效果评估工具

，能够反映训练集和验证

集在训练迭代中

的分数

变化情况，帮助我们快速

了解模型的学习效果。我

们可以通过学习曲线来

观察模型是

否过拟合，通

过判断拟合程度来确定

如何改进模型。

学习曲线

广泛应用于机器学习中

的模型评估，模型会随着

训练迭代逐步学习（优化

其内部参

数），例如神经网

络模型。这时用于评估学

习的指标可能会最大化

（分类准确率）或者最小

化

（回归误差），这也意味着得

分越高（低）表示学习到的

信息越多（少）。接下来，一起

看一下在学习曲线图中

观察到的一些常见形状

。

欠拟合学习曲线

欠拟合

是指模型无法学习到训

练集中数据所展现的信

息，这里可以通过训练损

失的

学习曲线来确定是

否发生欠拟合。在通常情

况下，欠拟合学习曲线可

能是一条平坦

的线或有

着相对较高的损失，这也

就表明该模型根本无法

学习训练集。

图 3.11 展示了两

类常见的欠拟合学习曲

线，左图表示模型的拟合

能力不够，右图表

示需要

通过进一步训练来降低

损失。

图 3.11 欠拟合学习曲线

过拟合学习曲线

过拟合

是指模型对训练集学习

得很好，包括统计噪声或

训练集中的随机波动。过

拟

合的问题在于，模型对

训练数据的专业化程度

越高，对新数据的泛化能

力就越差，

这就会导致泛

化误差增加。泛化误差的

增加可以通过模型在验

证集上的表现来衡

量。如

果模型的容量超出了问

题所需的容量，而灵活性

又过多，则会经常发生这

种

情况；如果模型训练时

间过长，也会发生这种情

况。

如图 3.12 所示，左图展现的

是过拟合学习曲线，可以

看出验证集损失曲线减

小到一

个点时又开始增

加，训练集的损失却在不

停地减少。右图则是一个

正常的学习曲

线，既不存

在欠拟合，也不存在过拟

合。训练集和验证集的损

失都可以降低到稳定

点

，并且两个最终损失值之

间的差距很小，从而可以

确定拟合程度良好。

图 3.12 学

习曲线对比

3.3.2

特征重要性

分析

通过模型训练可以

得到特征重要性。对于树

模型（比如 LightGBM 和 XGBoost），通过计

算特

征的信息增益或分裂次

数得到特征的重要性得

分。对于模型

LR 和 SVM，则是使用

特

征系数作为特征重要

性得分，例如 LR，每个特征各

对应一个特征系数 ，

越大

，那么

该特征对模型预测

结果的影响就会越大，就

可以认为该特征越重要

。我们假定特征重要性得

分和特征系数 都是在衡

量特征在模型中的重要

性，都可以起到特征选择

的作用。

对特征重要性的

分析可用于业务理解，有

些奇怪的特征在模型中

起着关键的作用，可以帮

助

我们更好地理解业务

。同时如果有些特征反常

规，那么我们也可以看出

来，可能它们就是过

拟合

的特征等。图

3.13 是 Lasso 模型训练

中每个特征的系数情况

，可以看出既有特征系数

高

的，也有特征正负相关

的。

图

3.13 Lasso 模型训练中的特征

系数

在图 3.13 中，与

SalePrice 具有最高

正相关的特征是 GrLivArea（房屋居

住面积），这

是非常符合我

们直觉的，房屋面积越大

，售价自然也就越高。还有

一些位置特征，比如街道

StoneBr、NridgHt 和 NoRidge

也起到了正向作用。当

然，也存在很多负向的特

征，这些

消极的特征是没

有太多意义的，通常可以

直接剔除掉。

3.3.3 误差分析

误

差分析是我们通过模型

预测结果来发现问题的

关键。一般而言，在回归问

题中就是看预测

结果的

分布，在分类问题中就是

看混淆矩阵等。这么做可

以帮助我们找到模型对

于哪些样本

或哪类样本

预测能力不够从而导致

结果不准确，然后分析造

成结果误差的可能因素

，最终修

正训练数据和模

型。

在真实的问题中，误差

分析会变得更加细致。比

如，在进行一个用户违约

预估的二分类任务

中，验

证集结果中有 200 个错误分

类样本，进一步分析发现

有

70% 的错误分类样本是由

于

大量特征缺失而导致

的误判，这时就需要进行

调整，既可以通过挖掘更

多能够描述这些误判

样

本的特征信息帮助增强

模型的预测能力，还可以

在模型训练中赋予这些

误判样本更高的权

重。

3.4

思

考练习

01. 本章只绘制了部

分变量的可视化图，请尝

试绘制其余变量的分布

图，并进行观察。

02. 数据可视

化的图表类型繁多，选择

适合某种数据特点或分

析目标的图表就变得有

些

困难，这里面包括趋势

分析、多类比较、数据联系

、数据分布等不同的分析

目标，

可以试着总结下不

同图表所针对的数据特

点和所展现的目标。

03. 如何

绘制混淆矩阵的图像？怎

么发现错误高的类别？

第

4 章 特征工程

在本章中，我

们将向大家介绍在算法

竞赛中工作量最大、决定

参赛者能否拿到较好名

次的关

键部分——特征工程

（Feature Engineering）。吴恩达老师在斯坦福大

学 CS229 机器学习的课

程中曾

提到一句话：“机器学习在

本质上还是特征工程，数

据和特征决定了机器学

习的上

限，而模型和算法

只是逼近这个上限而已

。”由此可见特征工程的重

要性，基本上参赛者会

将

80% 的时间和精力用来搭建

特征工程。在机器学习应

用中，特征工程介于数据

和算法之

间，特征工程是

将原始数据转化为特征

，进而使我们能够从各种

各样新的维度来对样本

进行

刻画。特征可以更好

地向预测模型描述潜在

的问题，从而提高模型对

未见数据进行预测分析

的准确性。高质量的特征

有助于提高模型整体的

泛化性能，特征在很大程

度上与基本问题相

关联

。特征工程既是一门科学

，也是一门有趣的艺术，这

也是数据科学家在建模

之前要把大

量时间花在

数据准备上的原因，并且

乐此不疲。

特征工程主要

分为数据预处理、特征变

换、特征提取、特征选择这

四个部分。在本章中，我

们

也将从这四个部分介绍

特征工程的对应工作，同

时给出使用技巧和应用

代码。

4.1 数据预处理

在算法

竞赛中，我们拿到的数据

集可能包含大量错漏信

息，既可能因为人工录入

错误导致异

常点存在使

得数据变“脏”，也可能有些

样本信息没办法在实际

中采集到，这些错漏信息

非常

不利于模型训练，会

使模型没办法从数据集

中学到较为准确的规律

。所以在大多数情况下，

初

始数据基本上是不能直

接用于模型训练的，或者

即使用了，也只能得到一

个比较糟糕的结

果。如果

主办方提供的数据足够

好，那么参赛者真的是十

分幸福了。数据质量直接

决定了模

型的准确性和

泛化能力的高低，同时在

构造特征时也会影响其

顺畅性。因此，在竞赛提供

的

数据质量不高的情况

下，就需要对数据进行预

处理，对各种脏数据进行

对应方式的处理，从

而得

到标准的、干净的、连续的

数据，供数据统计、数据挖

掘等使用。同时，我们也要

视情

况尝试对缺失值进

行处理，比如是否需要进

行填补，如果填补的话，是

填补均值还是中位数

等

。此外，有些竞赛提供的数

据以及对应的存储方式

可能使得需要占用超过

参赛者本身硬件

条件的

内存，因此有必要进行一

定的内存优化，这也有助

于在有限的内存空间中

对更大的数

据集进行操

作。

4.1.1

缺失值处理

无论是在

竞赛还是在实际问题中

，经常会遇到数据集存在

数据缺失的情况。例如，信

息无法

收集、系统出现故

障或者用户拒绝分享他

们的信息等导致数据缺

失。面对数据缺失问题，除

了 XGBoost 和 LightGBM

等算法在训练时可

以直接处理缺失值以外

，其他很多算法（如

LR、DNN、CNN、RNN 等）并不

能对缺失值进行直接处

理。在数据准备阶段，要比

构建算

法阶段花费更多

的时间，因为像填补缺失

值这样的操作需要细致

处理，以免在处理过程中

出

现错误并影响模型训

练效果。

区分缺失值

首先

，参赛者需要找到缺失值

的表现形式。缺失值的表

现除了 None、NA 和 NaN 这

些，还包括其

他用于表示数值缺失的

特殊数值，例如使用–1

或者

–999 来填充的缺失

值。还有一

种是看着像缺失值，却有

实际意义的业务，这种情

况就需要特殊对待。

例如

，没有填写“婚姻状态”这一

项的用户可能对自己的

隐私比较敏感，应为其单

独

设为一个分类，比如用

值 1

表示已婚，值 0 表示未婚

，值–1 表示未填；没有填写“驾

龄”这一项的用户可能是

没有车，为其填充 0 比较合

理。当找出缺失值后，就需

要根

据不同应用场景下

缺失值可能包含的信息

进行合理填充。

处理方法

数据缺失可以分为类别

特征的缺失和数值特征

的缺失两种，它们的填充

方法存在很

大的差异。对

于类别特征，通常会填充

一个新类别，可以是 0、–1、负无

穷等。对于

数值特征，最基

本的方法是均值填充，不

过这个方法对异常值比

较敏感，所以可以

选择中

位数进行填充，这个方法

对异常值不敏感。另外，就

是在进行数据填充的时

候，一定要考虑所选择的

填充方法会不会影响数

据的准确性。对填充方法

的总结如

下。

对于类别特

征：可以选择最常见的一

类填充方法，即填充众数

；或者直接

填充一个新类

别，比如 0、–1、负无穷。

对于数值

特征：可以填充平均数、中

位数、众数、最大值、最小值

等。具

体选择哪种统计值

，需要具体问题具体分析

。

对于有序数据（比如时间

序列）：可以填充相邻值 next 或

者

previous。

模型预测填充：普通的

填充只是一个结果的常

态，并未考虑其他特征之

间

相互作用的影响，可以

对含有缺失值的那一列

进行建模并预测其中缺

失值的

结果。虽然这种方

法比较复杂，但是最后得

到的结果直觉上比直接

填充要

好，不过在实际竞

赛中的效果则需要具体

检验。

4.1.2 异常值处理

在实际

数据中，常常会发现某个

或某些字段（特征）根据某

个变量（比如时间序列问

题中的

时间）排序后，经观

察发现存在一些数值远

远高于或低于其一定范

围内的其他数值。还有一

些不符合常态的存在，例

如广告点击用户中出现

年龄为

0 或超过 100 的情况。这

些我们都可

以当作异常

值，它们的存在可能会给

算法性能带来负作用。

寻

找异常值

在处理异常值

之前，首先需要找出异常

值，这里我们针对数值特

征的异常值总结了

两种

常用的方法。

第一种是通

过可视化分析的方法来

发现异常值。简单使用散

点图，我们能很清晰地

观

察到异常值的存在，严重

偏离密集区域的点都可

以当作异常值来处理，如

图 4.1 所

示。

图 4.1 数据散点图可

视化

第二种是通过简单

的统计分析来发现异常

值，即根据基本的统计方

法来判断数据是

否存在

异常。例如，四分位数间距

、极差、均差、标准差等，这种

方法适合于挖掘

单变量

的数值型数据，如图 4.2 所示

。

扩展考虑

离散型异常值

（离散属性定义范围以外

的所有值均为异常值）、知

识型异常值

（比如身高

10 米

）等，都可以当作类别缺失

值来处理。

图 4.2 四分位数间

距箱形图

处理异常值

删

除含有异常值的记录。这

种办法的优点是可以消

除含有异常值的样本带

来的不确定性，缺点是减

少了样本量。

视为缺失值

。将异常值视为缺失值，利

用缺失值处理的方法进

行处理。这种

办法的优点

是将异常值集中化为一

类，增加了数据的可用性

；缺点是将异常

值和缺失

值混为一谈，会影响数据

的准确性。

平均值（中位数

）修正。可用对应同类别的

数值使用平均值修正该

异常

值，优缺点同“视为缺

失值”。

不处理。直接在具有

异常值的数据集上进行

数据挖掘。这种办法的效

果好坏

取决于异常值的

来源，若异常值是录入错

误造成的，则对数据挖掘

的效果会

产生负面影响

；若异常值只是对真实情

况的记录，则直接进行数

据挖掘能够

保留最真实

可信的信息。

4.1.3 优化内存

在

参加机器学习相关竞赛

时，赛题涉及的数据往往

较大，并且参赛者自身的

计算机硬件条件

有限，所

以常常会因为内存不够

导致代码出现 memory error，给参赛者

带来困扰。因此，有

必要介

绍一些有助于优化内存

的方法，最大限度地运行

代码。这里我们将介绍 Python

的

内

存回收机制和数值类

型优化这两种常见方法

。

内存回收机制。在 Python 的内存

回收机制中，gc 模块主要运

用“引用计数”来跟踪

和回

收垃圾。在引用计数的基

础上，还可以通过“标记清

除”解决容器对象可能产

生

的循环引用问题，通过

“隔代回收”以空间换取时

间来进一步提高垃圾回

收的效率。

一般来说，在我

们删除一些变量时，使用

gc.collect() 来释放内存。

数值类型优

化。竞赛中常使用的数据

保存格式是 csv

以及 txt，在进行

处理时，需要

将其读取为

表格型数据，即 DataFrame 格式。这就

需要利用 pandas

工具包进行操

作，pandas 可以在底层将数值型

数据表示成 NumPy 数组，并使之

在内存中连续存

储。这种

存储方式不仅消耗的空

间较少，并且允许我们快

速访问数据。由于 pandas

使用相

同数量的字节来表示同

一类型的每一个值，并且

NumPy 数组存储着这些值的

数

量，所以 pandas 能够快速、准确地

返回数值型列消耗的字

节量。

pandas

中的许多数据类型

具有多个子类型，它们可

以使用较少的字节表示

不同数据。比

如，float 类型有 float16、float32 和

float64 这些子类型。这些类型名

称的数字部

分表明了这

种类型使用多少比特来

表示数据。一个 int8 类型的数

据使用 1B（8bit）存储一

个值，可以

表示 256（

）个二进制数值，这意

味着我们可以用这种子

类型去表示–128 和

127（包括 0）之间

的数值。

我们可以用 np.iinfo

类来

确认每一个 int 型子类型的

最小值和最大值，代码如

下：

import numpy as

np

np.iinfo(np.int8).min

np.iinfo(np.int8).max

然后，我们可以通过选

择某列特征的最小值和

最大值来判断特征所属

的子类型，代码如下：

c_min =

df[col].min()

c_max = df[col].max()

if c_min

> np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:

df[col] = df[col].astype(np.int8)

此外

，在不影响模型泛化性能

的情况下，对于类别型的

变量，若其编码 ID 的数字较

大、极

不连续而且种类较

少，则可以重新从 0 开始编

码（自然数编码），这样也能

减少变量的内存

占用。而

对于数值型的变量，常常

由于存在浮点数使得内

存占用过多，可以考虑先

将其最小

值和最大值归

一化，然后乘以 100、1000

等，之后取

整，这样不仅可以保留同

一变量之间的

大小关系

，还极大地减少了内存占

用。

4.2 特征变换

数据预处理

结束之后，有时参赛者还

需要对特征进行一些数

值变换，且在实际竞赛中

，很多

原始特征并不能直

接使用，这时就需要进行

一定的调整，以帮助参赛

者更好地构造特征。

4.2.1 连续

变量无量纲化

无量纲化

指的是将不同规格的数

据转换到同一规格。常见

的无量纲化方法有标准

化和区间缩

放法。标准化

的前提是特征值服从正

态分布，标准化后，特征值

服从标准正态分布。区间

缩

放法利用了边界值信

息，将特征的取值区间缩

放到某个特定的范围，例

如 [0,

1]。

单特征转换是构建一

些模型（如线性回归、KNN、神经

网络）的关键，对决策树相

关模型

没有影响，这也是

决策树及其所有衍生算

法（随机森林、梯度提升）日

益流行的原因之一。

还有

一些纯粹的工程原因，即

在进行回归预测时，对目

标取对数处理，不仅可以

缩小数据范

围，而且压缩

变量尺度使数据更平稳

。这种转换方式仅是一个

特殊情况，通常由使数据

集适

应算法要求的愿望

驱动。

然而，数据要求不仅

是通过参数化方法施加

的。如果特征没有被规范

化，例如当一个特征的

分

布位于 0 附近且范围不超

过 (–1, 1)，而另一个特征的分布

范围在数十万数量级时

，会导致

分布位于 0 附近的

特征变得完全无用。

举一

个简单的例子：假设任务

是根据房间数量和到市

中心的距离这两个变量

来预测公寓的成

本。公寓

房间数量一般很少超过

5 间，而到市中心的距离很

容易达到几千米。此刻，使

用线

性回归或者 KNN 这类模

型是不可以的，需要对这

两个变量进行归一化处

理。

标准化。最简单的转换

是标准化（或零–均值规范

化）。标准化需要计算特征

的均值

和标准差，其公式

表达如式 (4-1)：

其中 是均值， 是

标准差。

区间缩放。区间缩

放的思路有多种，常见的

一种是利用两个最值进

行缩放，可使所

有点都缩

放在预定的范围内，即 [0,

1]。区

间缩放的公式表达如式

(4-2)：

4.2.2 连续变量数据变换

log 变换

进行 log

变换可以将倾斜数

据变得接近正态分布，这

是因为大多数机器学习

模型不能

很好地处理非

正态分布的数据，比如右

倾数据。可以应用 变换来

修

正倾斜，其中加 1 的目的

是防止数据等于

0，同时保

证 都是正的。取对数不会

改

变数据的性质和相关

关系，但是压缩了变量的

尺度，不仅数据更加平稳

，还削弱了

模型的共线性

、异方差性等。

扩展学习

cbox-cox变

换——自动寻找最佳正态分

布变换函数的方法。此方

法在竞赛中并不

常用，有

兴趣的读者可以了解一

下。

连续变量离散化

离散

化后的特征对异常数据

有很强的健壮性，更便于

探索数据的相关性。例如

，把

年龄特征离散化后的

结果是：如果年龄大于 30，则

为 1，否则为

0。如果此特征没

有

离散化，那么一个异常

数据“年龄 300 岁”会给模型造

成很大的干扰。离散化后

，我

们也能对特征进行交

叉组合。常用的离散化分

为无监督和有监督两种

。

无监督的离散化。分桶操

作可以将连续变量离散

化，同时使数据平滑，即

降

低噪声的影响。一般分为

等频和等距两种分桶方

式。

等频。区间的边界值要

经过选择，使得每个区间

包含数量大致相等

的变

量实例。比如分成 10 个区间

，那么每个区间应该包含

大约 10%

的实例。这种分桶方

式可以将数据变换成均

匀分布。

等距。将实例从最

小值到最大值，均分为 等

份，每份的间距是相

等的

。这里只考虑边界，每等份

的实例数量可能不等。等

距可以保持

数据原有的

分布，并且区间越多对数

据原貌保持得越好。

有监

督的离散化。这类方法对

目标有很好的区分能力

，常用的是使用树模

型返

回叶子节点来进行离散

化。在图 4.3 所示的 GBDT + LR

经典模型

中，就

是先使用 GDBT 来将连续

值转化为离散值。具体方

法：用训练集中的所有连

续值和标签输出来训练

LightGBM，共训练两棵决策树，第一

棵树有 4 个叶

子节点，第二

棵树有 3 个叶子节点。如果

某一个样本落在第一棵

树的第三个

叶子节点上

，落在第二棵树的第一个

叶子节点上，那么它的编

码就是 0010

100，一共

7 个离散特征

，其中会有两个取值为 1 的

位置，分别对应每棵树

中

样本落点的位置。最终我

们会获得 num_trees*num_leaves

维特征。

图 4.3 GBDT + LR

模型

4.2.3 类别特征转换

在实际数

据中，特征并不总是数值

，还有可能是类别。对于离

散型的类别特征进行编

码，一

般分为两种情况：自

然数编码（特征有意义）和

独热编码（特征没有意义

）。

自然数编码。一列有意义

的类别特征（即有顺序关

系）可以用自然数进行编

码，利

用自然数的大小关

系可以保留其顺序关系

。此外，当特征不是用数字

而是用字母或

符号等表

示时，是无法直接被“喂”到

模型里作为训练标签的

，比如年龄段、学历

等，这时

候就需要先把特征取值

转换成数字。如果一列类

别特征里有 个取值，

那么

经过自然数编码后，可以

得到取值为 的数字，

即给

每一个类别分别分配一

个编号。这样做的优点是

内存消耗小、训练时间快

，缺

点是可能会丢失部分

特征信息。下面将给出两

种自然数编码的常用方

式。

调用 sklearn 中的函数：

from sklearn

import preprocessing

for f in columns:

le = preprocessing.LabelEncoder()

le.fit(data[f])

自定义

实现（速度快）：

for

f in columns:

data[f] = data[f].fillna(-999)

data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))

独热编码。当

类别特征没有意义（即没

有顺序关系）时，需要使用

独热编码。例

如，红色＞蓝色

＞绿色不代表任何东西，进

行独热编码后，每个特征

的取值对应一

维特征，最

后得到的是一个样本数

×类别数大小的 0~1 矩阵。可以

直接调用 sklearn

中的

API。

4.2.4 不规则特

征变换

除了数值特征与

类别特征之外，还有一类

不规则特征可能包含样

本的很多信息，比如身份

证

号。由百度百科查得，根

据《中华人民共和国国家

标准 GB

11643—1999》中有关公民身份

号

码的规定，公民身份号码

是特征组合码，由十七位

数字本体码和一位数字

校验码组成，排

列顺序从

左至右依次为：六位数字

地址码、八位数字出生日

期码、三位数字顺序码和

一位数

字校验码。其中顺

序码的奇数分给男性，偶

数分给女性。校验码是根

据前面十七位数字码，

按

照 ISO

7064:1983.MOD 11-2 校验码计算出来的检

验码。因此，我们可以从身

份证号获得

用户的出生

地、年龄、性别等信息。当然

，身份证号涉及用户隐私

，在竞赛中主办方不可能

提供这个信息，在此仅作

为举例。

4.3 特征提取

机器学

习模型很难识别复杂的

模式，特别是很难学习到

由不同特征组合交互的

信息，所以我

们可以基于

对数据集的直觉分析和

业务理解创建一些特征

来帮助模型有效学习。下

面我们将

介绍结构化数

据的特征提取方式。（结构

化数据由明确定义的数

据类型组成，非结构化数

据

由音频、视频和图片等

不容易搜索的数据组成

。）

4.3.1 类别相关的统计特征

类

别特征又可以称为离散

特征，除了每个类别属性

的特定含义外，还可以构

造连续型的统计

特征，以

挖掘更多有价值的信息

，比如构造目标编码、count、nunique 和 ratio 等

特征。另

外，也可以通过类

别特征间的交叉组合构

造更加细粒度的特征。

目

标编码

目标编码可以理

解为用目标变量（标签）的

统计量对类别特征进行

编码，即根据目

标变量进

行有监督的特征构造。如

果是分类问题，可以统计

正样本个数、负样本个

数

或者正负样本的比例；如

果是回归问题，则可以统

计目标均值、中位数和最

值。

目标编码方式可以很

好地替代类别特征，或者

作为新特征。

使用目标变

量时，非常重要的一点是

不能泄露任何验证集的

信息。所有基于目标编

码

的特征都应该在训练集

上计算，测试集则由完整

的训练集来构造。更严格

一点，

我们在统计训练集

的特征时，需要采用 折交

叉统计法构造目标编码

特征，从而

最大限度地防

止信息泄露。如图 4.4 所示，我

们将样本划分为五份，对

于其中每一份

数据，我们

都将用另外四份来计算

每个类别取值对应目标

变量的频次、比例或者均

值，简单来说就是未知的

数据（一份）在已知的数据

（四份）里面取特征。

图 4.4 五折

交叉统计构造特征

目标

编码方法对于基数较低

的类别特征通常很有效

，但对于基数较高的类别

特征，

可能会有过拟合的

风险。因为会存在一些类

别出现频次非常低，统计

出来的结果不

具有代表

性。一般我们会加入平滑

性来降低过拟合风险。在

处置妥当的情况下，无

论

是线性模型，还是非线性

模型，目标编码都是最佳

的编码方式和特征构造

方式。

为了帮助大家更好

地理解，下面给出五折交

叉统计的代码实现：

folds = KFold(n_splits=5,

shuffle=True, random_state=2020)

for col in columns:

colname = col+'_kfold'

for fold_, (trn_idx,

val_idx) in enumerate(folds.split(train, train)):

tmp =

train.iloc[trn_idx]

order_label = tmp.groupby([col])['label'].mean()

train[colname] =

train[col].map(order_label)

order_label = train.groupby([col])['label'].mean()

test[colname] =

test[col].map(order_label)

count、nunique、ratio

这三

类是竞赛中类别特征经

常使用的构造方式。count（计数

特征）用于统计类别

特征

的出现频次。nunique 和 ratio

的构造相

对复杂一些，经常会涉及

多个类别特

征的联合构

造，例如在广告点击率预

测问题中，对于用户 ID 和广

告 ID，使用

nunique

可以反映用户对

广告的兴趣宽度，也就是

统计用户 ID 看过几种广告

ID；

使用 ratio 可以反映用户对某

类广告的偏好程度，也就

是统计用户

ID 点击某类广

告 ID 的频次占用户点击所

有广告 ID 频次的比例。当然

，这也适用于其他问题，比

如恶意攻击、反欺诈和信

用评分这类需要构造行

为信息或分布信息描述

的问题。

类别特征之间交

叉组合

交叉组合能够描

述更细粒度的内容。对类

别特征进行交叉组合在

竞赛中是一项非常

重要

的工作，这样可以进行很

好的非线性特征拟合。如

图 4.5 所示，用户年龄和用户

性别可以组合成“年龄 _

性

别”这样的新特征。一般我

们可以对两个类别或三

个类别

特征进行组合，也

称作二阶组合或三阶组

合。简单来说，就是对两个

类别特征进行

笛卡儿积

的操作，产生新的类别特

征。在实际数据中，可能会

有很多类别特征。如

果有

10 种类别特征并考虑所有

的二阶交叉组合，则能够

产生 45

种组合。

图 4.5 类别特征

之间交叉组合

并非所有

组合都是需要考虑的，我

们会从两个方面进行分

析。首先是业务逻辑方

面

，比如用户操作系统版本

与用户所在城市的组合

是没有实际意义的。然后

是类别

特征的基数，如果

基数过大，那么可能导致

很多类别只会出现一次

，在一轮训练

中，每个类别

只会被训练一次，显然特

征对应权重的置信度是

很低的。

4.3.2 数值相关的统计

特征

这里所说的数值特

征，我们认为是连续的，例

如房价、销量、点击次数、评

论次数和温度

等。不同于

类别特征，数值特征的大

小是有意义的，通常不需

要处理就可以直接“喂”给

模型

进行训练。除了在前

面对数值特征进行各种

变换外，还存在一些其他

常见的数值特征构造方

式。

数值特征之间的交叉

组合。不同于类别特征之

间的交叉组合，一般对数

值特征进

行加减乘除等

算术操作类的交叉组合

。这需要我们结合业务理

解和数据分析进行构

造

，而不是一拍脑袋式的暴

力构造。例如给出房屋大

小（单位为平方米）和售价

，

就可以构造每平方米的

均价；又或者给出用户过

去三个月每月的消费金

额，就可以

构造这三个月

的总消费金额和平均消

费金额，以反映用户的整

体消费能力。

类别特征和

数值特征之间的交叉组

合。除了类别特征之间和

数值特征之间的交叉

组

合外，还可以构造类别特

征与数值特征之间的交

叉组合。这类特征通常是

在类别

特征的某个类别

中计算数值特征的一些

统计量，比如均值、中位数

和最值等。

按行统计相关

特征。这种方式有点类似

特征交叉，都是将多列特

征的信息组合起

来。但是

行统计在构造时会包含

更多的列，直接对多列按

行进行统计，例如按行统

计 0

的个数、空值个数和正

负值个数，又或是均值、中

位数、最值或者求和等。多

列特征可能是每个月的

消费金额、用电量，在工业

数据上可以是化学实验

各阶段的

温度、浓度等。对

于这些含有多列相关特

征的数据，我们都需要分

析多列数值的变

化情况

，并从中提取有价值的特

征。

4.3.3 时间特征

在实际数据

中，通常给出的时间特征

是时间戳属性，所以首先

需要将其分离成多个维

度，比

如年、月、日、小时、分钟

、秒钟。如果你的数据源来

自不同的地理数据源，还

需要利用时

区将数据标

准化。除了分离出来的基

本时间特征以外，还可以

构造时间差特征，即计算

出各

个样本的时间与未

来某一个时间的数值差

距，这样这个差距是 UTC 的时

间差，从而将时间

特征转

化为连续值，比如用户首

次行为日期与用户注册

日期的时间差、用户当前

行为与用户

上次行为的

时间差等。

4.3.4 多值特征

在实

际竞赛中，可能会遇到某

一列特征中每行都包含

多个属性的情况，这就是

多值特征。例

如 2018

腾讯广告

算法大赛中的兴趣（interest）类目

，其中包含 5 个兴趣特征组

，每个兴趣

特征组都包含

若干个兴趣 ID。对于多值特

征，通常可以进行稀疏化

或者向量化的处理，这

种

操作一般出现在自然语

言处理中，比如文本分词

后使用

TF-IDF、LDA、NMF 等方式进

行处理

，这里则可以将多值特征

看作文本分词后的结果

，然后做相同的处理。

如图

4.6 所示，对多值特征最基本

的处理办法是完全展开

，即把这列特征所包含的

个属

性展开成

维稀疏矩

阵。使用 sklearn 中的 CountVectorizer 函数，可以方

便地将多值特

征展开，只

考虑每个属性在这个特

征的出现频次。

图 4.6 多值特

征处理方法

还有一种情

况，比如在 2020 腾讯广告算法

大赛中，需要根据用户的

历史点击行为预测用户

的属性标签。这时候用户

的点击序列尤为重要，当

我们构造好用户对应的

历史点击序列后，

除了使

用上述的 TF-IDF 等方法外，还可

以提取点击序列中商品

或广告的嵌入表示，比如

用

Word2Vec、DeepWalk 等方法获取 embedding向量表示

。因为要提取用户单个特

征，所以可

以对序列中的

嵌入向量进行聚合统计

，这种方法本质上是假设

用户点击过的商品或广

告同等

重要，是一种比较

粗糙的处理方式。我们可

以引入时间衰减因素，或

者使用序列模型，如

RNN、LSTN、GRU，套用

NLP 的方法进行求解。

到目前

，已经给出了基本类型特

征的构造方式。当然，还有

很多类型没有讲到，比如

空间特

征、时间序列特征

和文本特征，以及聚类和

降维等方法，我们将在后

面的章节中结合具体问

题再详细介绍。

4.4 特征选择

如图 4.7 所示，当我们添加新

特征时，需要验证它是否

确实能够提高模型预测

的准确度，以

确定不是加

入了无用的特征，因为这

样只会增加算法运算的

复杂度，这时候就需要通

过特征

选择算法自动选

择出特征集中的最优子

集，帮助模型提供更好的

性能。特征选择算法用于

从

数据中识别并删除不

需要、不相关以及冗余的

特征，这些特征可能会降

低模型的准确度和性

能

。特征选择的方法主要有

先验的特征关联性分析

以及后验的特征重要性

分析。

图 4.7 特征选择过程

4.4.1

特

征关联性分析

特征关联

性分析是使用统计量来

为特征之间的相关性进

行评分。特征按照分数进

行排序，要

么保留，要么从

数据集中删除。关联性分

析方法通常是针对单变

量的，并且独立考虑特征

或

者因变量。常见的特征

关联性分析方法有皮尔

逊相关系数、卡方检验、互

信息法和信息增益

等。这

些方法的速度非常快，用

起来也比较方便，不过忽

略了特征之间的关系，以

及特征和

模型之间的关

系。

皮尔逊相关系数（Pearson correlation coefficient）。这种

方法不仅可以衡量变量

之间

的线性相关性，解决

共线变量问题，还可以衡

量特征与标签的相关性

。共线变量是

指变量之间

存在高度相关关系，这会

降低模型的学习可用性

、可解释性以及测试集

的

泛化性能。很明显，这三个

特性都是我们想要增加

的，所以删除共线变量是

一个

有价值的步骤。我们

将为删除共线变量建立

一个基本的阈值（根据想

要保留的特征

数量来定

），然后从高于该阈值的任

何一对变量中删除一个

。

下面的代码用于解决特

征与标签不具有相关性

的问题，根据皮尔逊相关

系数的计算

提取 top300 的相似

特征：

def feature_select_pearson(train, features):

featureSelect = features[:]

# 进行皮尔逊相关性

计算

corr = []

for

feat in featureSelect:

corr.append(abs(train[[feat, 'target']].fillna(0).corr().values[0][1]))

se

= pd.Series(corr, index=featureSelect).sort_values(ascending=False)

feature_select = se[:300].index.tolist()

# 返回特征选择后的

训练集

return train[feature_select]

卡方检验。它用于

检验特征变量与因变量

的相关性。对于分类问题

，一般假设与标

签独立的

特征为无关特征，而卡方

检验恰好可以进行独立

性检验，所以适用于特征

选择。如果检验结果是某

个特征与标签独立，则可

以去除该特征。卡方公式

如式

(4-

3)：

互信息法。互信息是

对一个联合分布中两个

变量之间相互影响关系

的度量，也可以

用来评价

两个变量之间的相关性

。互信息法之所以能用于

特征选择，可以从两个角

度进行解释：基于 KL散度和

基于信息增益。互信息越

大说明两个变量相关性

越高。

互信息公式如式

(4-4)：

这

里的 、 和 都是从训练集上

得到的。想把互信息直接

用于

特征选择其实不是

太方便，其主要原因有以

下两点。

它不属于度量方

式，也没有办法归一化，无

法对不同数据集上的结

果进行比

较。

对于连续变

量的计算不是很方便（ 和

都是集合， 、 都是离散的取

值），通常连续变量需要先

离散化，而互信息的结果

对离散化的方式很敏

感

。

4.4.2 特征重要性分析

在实际

竞赛中，经常用到的一种

特征选择方法是基于树

模型评估特征的重要性

分数。特征的

重要性分数

越高，说明特征在模型中

被用来构建决策树的次

数越多。这里我们以 XGBoost

为例

来介绍树模型评估特征

重要性的三种计算方法

（weight、gain 和 cover）。

（LightGBM 也可以返回特征重要

性。）

weight

计算方式。该方法比较

简单，计算特征在所有树

中被选为分裂特征的次

数，

并将此作为评估特征

重要性的依据，代码示例

如下：

params = {

'max_depth':

10,

'subsample': 1,

'verbose_eval': True,

'seed':

12,

'objective':'binary:logistic'

}

xgtrain = xgb.DMatrix(x,

label=y)

bst = xgb.train(params, xgtrain, num_boost_round=10)

importance = bst.get_score(fmap = '',importance_type='weight')

gain

计算方式。gain 表示平均

增益。在进行特征重要性

评估时，使用 gain 表示

特征在

所有树中作为分裂节点

的信息增益之和再除以

该特征出现的频次。代码

示例

如下：

importance = bst.get_score(fmap = '',importance_type='gain')

cover

计算方式。cover 较复

杂些，其具体含义是特征

对每棵树的覆盖率，即特

征

被分到该节点的样本

的二阶导数之和，而特征

度量的标准就是平均覆

盖率值。代码

示例如下：

importance =

bst.get_score(fmap = '',importance_type='cover')

使

用技巧

虽然特征重要性

可以帮助我们快速分析

特征在模型训练过程中

的重要性，但不能将其当

作绝对的参考依据。一般

而言，只要特征不会导致

过拟合，我们就可以选择

重要性高的

特征进行分

析和扩展，对于重要性低

的特征，可以考虑将之从

特征集中移除，然后观察

线下效果，再做进一步判

断。

4.4.3 封装方法

封装方法是

一个比较耗时的特征选

择方法。可以将对一组特

征的选择视作一个搜索

问题，在

这个问题中，通过

准备、评估不同的组合并

对这些组合进行比较，从

而找出最优的特征子

集

。搜索过程可以是系统性

的，比如最佳优先搜索；也

可以是随机的，比如随机

爬山算法，

或者启发式方

法，比如通过向前和向后

搜索来添加和删除特征

（类似前剪枝和后剪枝算

法）。下面介绍两种常用的

封装方法。

启发式方法。启

发式方法分为两种：前向

搜索和后向搜索。前向搜

索说白了就是每

次增量

地从剩余未选中的特征

中选出一个并将其加入

特征集中，待特征集中的

特征

数量达到初设阈值

时，意味着贪心地选出了

错误率最小的特征子集

。既然有增量

加，就会有增

量减，后者称为后向搜索

，即从特征全集开始，每次

删除其中的一个

特征并

评价，直到特征集中的特

征数量达到初设阈值，就

选出了最佳的特征子集

。

我们还可以在此基础上

进行扩展。因为启发式方

法会导致局部最优，所以

加入模拟

退火方式进行

改善，这种方式不会因为

新加入的特征不能改善

效果而舍弃该特征，

而是

对其添加权重后放入已

选特征集。

这种启发式方

法在竞赛中尝试过，是比

较耗时、耗资源的操作，一

般而言可以在线

上线下

增益一致且数据集量级

不大的情况下使用。

递归

消除特征法。递归消除特

征法使用一个基模型来

进行多轮训练，每轮训练

都会

先消除若干权值系

数的特征，再基于新特征

集进行下一轮训练。可以

使用

feature_selection 库的 RFE 类来进行特征

选择。代码示例如下：

from sklearn.feature_selection

import RFE

from sklearn.linear_model import LogisticRegression

# 递归

消除特征法，返回特征选

择后的数据

# 参数estimator 为基模

型

#

参数n_features_to_select 为选择的特征个

数

RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(data, target)

使用技巧

在使用封装

方法进行特征选择时，用

全量数据训练并不是最

明智的选择。应先对大数

据

进行采样，再对小数据

使用封装方法。

上述三种

特征选择方法需要根据

实际问题选择或者组合

使用，建议优先考虑特征

重要性，其

次是特征关联

性。另外，还有一些不常见

的特征选择方法，比如 Kaggle 上

非常经典的 null

importance 特征选择方

式。

模型有时其实很蠢，很

多和目标标签根本没有

关联的特征，它也可以将

之和目标标签关联

上，这

种被虚假关联到测试集

上的特征会导致过拟合

，从而产生负面影响。之后

特征重要性

分析就会变

得不那么可靠，那么该如

何在特征重要性分析中

区分某个特征是否有用

呢？

null

importance 的思想其实很简单，就

是将构建好的特征和正

确的标签喂入树模型得

到一个

特征重要性分数

，再将特征和打乱后的标

签喂入树模型得到一个

特征重要性分数，然后对

比

这两个分数，如果前者

没有超过后者，那么这个

特征就是一个无用的特

征。

4.5 实战案例

有了第 2 章和

第 3 章的内容铺垫，接下来

我们就可以进行特征工

程部分的实战操作了。这

里

主要是对本章所学内

容的简单回顾，但也存在

很多平时用不到的特征

工程技巧，不过请放

心，在

后面的竞赛实战环节，我

们会进行更加详细的应

用演练。

4.5.1 数据预处理

本阶

段的主要工作是数据清

洗，对缺失值和异常值进

行及时处理。执行下面的

代码进行基本

的数据读

取，删除缺失值比例大于

50% 的特征列，并对

object 型的缺失

特征进行填充：

test = pd.read_csv("../input/test.csv")

train

= pd.read_csv("../input/train.csv")

ntrain = train.shape[0]

ntest

= test.shape[0]

data = pd.concat([train, test],

axis=0, sort=False)

# 删除缺失

值比例大于50% 的特征列

missing_cols

= [c for c in data

if data[c].isna().mean()*100 > 50]

data =

data.drop(missing_cols, axis=1)

# 对

object 型的缺失特征进行填充

object_df =

data.select_dtypes(include=['object'])

numerical_df = data.select_dtypes(exclude=['object'])

object_df =

object_df.fillna('unknow')

接下来，对数值型特征用

中位数进行填充：

missing_cols = [c for

c in numerical_df if numerical_df[c].isna().sum() >

0]

for c in missing_cols:

numerical_df[c]

= numerical_df[c].fillna(numerical_df[c].median())

对于特

征中属性的分布极其不

均衡，比如存在某个属性

占比超过 95%，这时也要考虑

是否将

其删除。图 4.8

所示的

街道（Street）就是这一类特征，其

他的比如有 Heating、RoofMatl、

Condition2 和 Utilities。

图

4.8 街道分

布展示

下面为特征删除

代码：

object_df = object_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)

4.5.2 特征提取

特征提取

阶段将从多个角度进行

具体的特征构造，并且构

造的每个特征都具有实

际的意义。

基本特征构造

房屋建筑年龄也是影响

房屋价格的一个因素，在

构建这个特征时，发现存

在销售日

期（YrSold）小于建造日

期（YearBuilt）的数据，针对这种异常

情况需要进行调

整。具体

地，将异常数据的销售日

期改为数据集中销售日

期的最大年份（2009）。

下面是构

建特征的具体代码：

numerical_df.loc[numerical_df['YrSold'] < numerical_df['YearBuilt'], 'YrSold' ]

= 2009

numerical_df['Age_House']= (numerical_df['YrSold'] - numerical_df['YearBuilt'])

接下

来构造业务相关特征，比

如对原始特征中的浴池

（BsmtFullBath）和半浴池

（BsmtHalfBath）进行求和，对全

浴（FullBath）和半浴（HalfBath）求和，

对一楼面

积（1stFlrSF）和二楼面积（2ndFlrSF）以及地下

室面积求和，来表示

房屋

的结构信息。代码如下：

numerical_df['TotalBsmtBath'] =

numerical_df['BsmtFullBath'] +

numerical_df['BsmtHalfBath']*0.5

numerical_df['TotalBath'] = numerical_df['FullBath']

+ numerical_df['HalfBath']*0.5

numerical_df['TotalSA'] = numerical_df['TotalBsmtSF'] +

numerical_df['1stFlrSF'] +

numerical_df['2ndFlrSF']

特

征编码

由于 object

类特征不能

直接参与模型训练，需要

先进行编码处理，而类别

特征的

编码方法非常多

，因此如何选择编码方法

就成为了关键。

首先，需要

区分类别特征：对于存在

大小关系的序数特征，可

以进行 的映

射转换，即自

然数编码；对于没有大小

关系的特征，则可以进行

独热编码（onehot）处理，或者频次

（count）编码。特征提取的代码如

下：

bin_map

= {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,

'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,

'IR3':0,"None" :

0,"No" : 2, "Mn" : 2,

"Av": 3,"Gd" : 4,"Unf" : 1,

"LwQ": 2,

"Rec" : 3,"BLQ" :

4, "ALQ" : 5, "GLQ" :

6}

object_df['ExterQual'] = object_df['ExterQual'].map(bin_map)

object_df['ExterCond'] =

object_df['ExterCond'].map(bin_map)

object_df['BsmtCond'] = object_df['BsmtCond'].map(bin_map)

object_df['BsmtQual'] =

object_df['BsmtQual'].map(bin_map)

object_df['HeatingQC'] = object_df['HeatingQC'].map(bin_map)

object_df['KitchenQual'] =

object_df['KitchenQual'].map(bin_map)

object_df['FireplaceQu'] = object_df['FireplaceQu'].map(bin_map)

object_df['GarageQual'] =

object_df['GarageQual'].map(bin_map)

object_df['GarageCond'] = object_df['GarageCond'].map(bin_map)

object_df['CentralAir'] =

object_df['CentralAir'].map(bin_map)

object_df['LotShape'] = object_df['LotShape'].map(bin_map)

object_df['BsmtExposure'] =

object_df['BsmtExposure'].map(bin_map)

object_df['BsmtFinType1'] = object_df['BsmtFinType1'].map(bin_map)

object_df['BsmtFinType2'] =

object_df['BsmtFinType2'].map(bin_map)

PavedDrive = {"N" : 0,

"P" : 1, "Y" : 2}

object_df['PavedDrive'] = object_df['PavedDrive'].map(PavedDrive)

# 选择剩余的object 特征

rest_object_columns = object_df.select_dtypes(include = ['object'])

#

进行

one-hot 编码

object_df = pd.get_dummies(object_df, columns

= rest_object_columns.columns)

data = pd.concat([object_df, numerical_df],

axis=1, sort=False)

我们并没有进行各

式各样的暴力提取操作

，主要是带着大家从业务

中提取特征，目

的是使大

家掌握特征提取的技巧

和使用方法，在后面的案

例中会有更多特征提取

的

方式。

4.5.3

特征选择

这部分

将使用相关性评估的方

式进行特征选择，这种方

式是相关性分析的一种

，可以过滤掉

相似性大于

一定阈值的特征，减少特

征冗余。下面创建了一个

相关性评估的辅助函数

：

def correlation(data, threshold):

col_corr = set()

corr_matrix = data.corr()

for i in range(len(corr_matrix.columns)):

for j

in range(i):

if abs(corr_matrix.iloc[i, j]) >

threshold: # 相似性分数与阈值对比

colname = corr_matrix.columns[i] #

获取列名

col_corr.add(colname)

return col_corr

all_cols =

[c for c in data.columns if

c not in ['SalePrice']]

corr_features =

correlation(data[all_cols], 0.9)

对于阈值的确

定一般很难量化，可以从

总特征量和整体相似分

数这两个角度进行考虑

，比如

我们的机器不允许

使用太多特征，就可以以

此为依据确定特征保留

量，这也是个相对灵活的

值。可以保留不同阈值下

的特征集进行训练，辅助

提升模型融合结果。

4.6 思考

练习

01. 在对数值型特征进

行缺失值填充时，该如何

从平均数、中位数和众数

中选择？

02. 在特征选择中，还

有一类是基于惩罚项的

特征选择法，里面包含 L1 正

则和

L2 正

则，那么这两种正

则有什么区别？该如何选

择？

03. 整个特征工程主要分

为几个部分？每个部分主

要包含哪些内容？

04.

数值型

特征、类别型特征与不规

则特征各自是怎么定义

的？请举例说明。

05. 在数据清

洗过程中，应该如何选择

异常值处理方式？

06. 为什么

要优化内存，可供选择的

优化方式是什么？

07.

请举例

说明 count、nunique、ratio 三种统计特征的含

义。

08. 在什么情况下，我们需

要进行特征变换？

09.

在进行

特征选择时，该选择特征

关联性分析方法还是特

征重要性分析方法呢？

第

5 章 模型训练

本章将向大

家介绍算法竞赛中的常

用模型，好的模型能够帮

我们逼近分数的上限。可

选择的

模型主要分为线

性模型、树模型和神经网

络三种。有一句话是没有

最好的模型，只有最适合

的模型，所以在本章中，我

们将分别介绍不同模型

适合的应用场景，同时给

出使用技巧和应

用代码

。

5.1 线性模型

本节将介绍两

种线性缩减方法：Lasso 回归和

Ridge 回归。这两种线性回归模

型的区别仅在

于如何进

行惩罚、如何解决过拟合

问题。然后对两种模型的

数学形式、优缺点和应用

场景进

行相应介绍。

5.1.1 Lasso 归

Lasso

的

全称是 Least absolute shrinkage and selection

operator，是对普通的线性

回归使用 L1

正则进行优化

，通过惩罚或限制估计值

的绝对值之和，可以使某

些系数为零，从而达到特

征

系数化和特征选择的

效果。当我们需要一些自

动的特征、变量选择，或者

处理高度相关的预

测因

素时，这是很方便的，因为

标准回归的回归系数通

常太大。Lasso 回归的数学形式

如式

(5-1)：

其中， 是正则项（惩罚

项）的系数，通过改变 的值

，基本上可以控制惩罚项

，即 的

L1

范数。当 的值越高，惩

罚项的影响程度越大，反

之越小。

代码实现

可以直

接调用 sklearn 库来实现

Lasso 回归，这

里选择的 L1 正则参数是 0.1。

from

sklearn.linear_model import Lasso

lasso_model = Lasso(alpha

= 0.1, normalize = True)

5.1.2

Ridge 归

Ridge 回归是对普通的线性回

归使用 L2 正则进行优化，对

特征的权重系数设置了

惩罚项。

其数学形式如式

(5-2)：

与 Lasso 回归的损失函数基本

一致，只是将惩罚项修改

成了 的 L2 范数。

代码实现

可

以直接调用 sklearn 库来实现 Ridge 回

归：

from sklearn.linear_model import Ridge

Ridge_model =

Ridge(alpha=0.05, normalize=True)

问题讨论

现在我们已

经对 Lasso 回归和

Ridge 回归有了基

本的了解，接下来我们考

虑一个例

子。假设现在有

一个非常大的数据集，其

中包含 10 000 个特征。这些特征

中仅有部

分特征是相关

的。然后思考一下，应该用

哪个回归模型进行训练

，是 Lasso 回归还

是 Ridge 回归？

如果我

们用 Lasso 回归进行训练，那么

遇到的困难主要是当存

在相关特征时，Lasso

回归只保

留其中一个特征，而将其

他相关特征设置为零。这

可能会导致一些信息丢

失，从而降低模型预测的

准确性。

如果选择 Ridge

回归，虽

然能降低模型的复杂性

，但并不会减少特征的数

量，因为

Ridge 回归从不会使系

数为零，而只会使系数最

小化，可是这并不利于特

征缩减。在

面对 10 000

个特征时

，模型仍然会很复杂，因此

可能会导致模型性能不

佳。

综合考虑，这个问题的

解决办法是什么呢？可以

选择 Elastic Net Regression 进行扩展

学习。

5.2 树模

型

本节将介绍竞赛中常

见的树模型，这些模型简

单易用，能够带来高收益

。可将树模型分为随

机森

林（Random Forest，RF）和梯度提升树（GBDT），这两者

最大的差异是前者并行

、

后者串行。在梯度提升树

部分我们将介绍如今竞

赛中大火的三种树模型

：XGBoost、

LightGBM 和 CatBoost。能够灵活运用这三种

模型是竞赛中的必备技

能。接下来将详细介绍

各

种树模型的数学形式、优

缺点、使用细节和应用场

景。

5.2.1

随机森林

简言之，随机

森林算法就是通过集成

学习的思想将多个决策

树集成在一起。这里的决

策树可

以是一个分类器

，各个决策树之间没有任

何关联，随机森林算法对

多个决策树的结果进行

投

票得到最终结果，这也

是最简单的 Bagging思想。随机森

林是一个基于非线性树

的模型，通

常可以提供准

确的结果。

随机森林的构

造过程

随机森林的构造

过程如图 5.1 所示。

图 5.1

随机森

林的构造过程

具体过程

如下。

(1) 假设训练集中的样

本数为 。有放回的随机抽

取 个样本，即样本可以重

复，然后用得到的这

个样

本来训练一棵决策树。

(2) 如

果有 个输入变量（特征），则

指定一个远小于 的数字

（列采

样），以便在每次分裂

时，都从

个输入变量中随

机选出 个。然后从选出的

这 个变量中选择最佳分

裂变量（分裂依据信息增

益、信息增益比、基尼指数

等）。

(3) 重复步骤 (2)，对每个节点

都进行分裂，并且不修剪

，使节点最大限度地生长

（停止分裂的条件为节点

的所有样本均属于同一

类）。

(4) 通过步骤 (1) 到 (3) 构造大量

决策树，然后汇总这些决

策树以预测新数据（对分

类

问题进行投票选择，对

回归问题进行均值计算

）。

随机森林的优缺点

随机

森林优点非常明显：不仅

可以解决分类和回归问

题，还可以同时处理类别

特征

和数值特征；不容易

过拟合，通过平均决策树

的方式，降低过拟合的风

险；非常稳

定，即使数据集

中出现了一个新的数据

点，整个算法也不会受到

过多影响，新的数

据点只

会影响到一棵决策树，很

难对所有决策树都产生

影响。

很多缺点都是相对

而言的，随机森林算法虽

然比决策树算法更复杂

，计算成本更

高，但是其拥

有天然的并行特性，在分

布式环境下可以很快地

训练。梯度提升树需

要不

断地训练残差，所以结果

准确度更高，但是随机森

林更不容易过拟合，更加

稳

定，这也是因为其 Bagging的特

性。

代码实现

from sklearn.ensemble import RandomForestClassifier

rf =

RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)

5.2.2 梯度提升树

梯度提升树（GBDT）是基于

Boosting改进

而得的，在 Boosting算法中，一系列

基学习器都

需要串行生

成，每次学习一棵树，学习

目标是上棵树的残差。和

AdaBoost 一样，梯度提升

树也是基

于梯度下降函数。梯度提

升树算法已被证明是 Boosting算

法集合中最成熟的算法

之

一，它的特点是估计方

差增加，对数据中的噪声

更敏感（这两个问题都可

以通过使用子采样

来减

弱），以及由于非并行操作

而导致计算成本显著，因

此要比随机森林慢很多

。

梯度提升树作为 XGBoost、LightGBM 和 CatBoost

的基

础，这里将对其原理进行

简单介

绍。我们知道梯度

提升树是关于 Boosting的加法模

型，由 个模型组合而成，其

形式如式

(5-3)：

一般而言，损失

函数描述的是预测值

与

真实值 之间的关系，梯度

提升树是基于残差（

， 为前

一个模型）来不断拟合训

练集的，这里使用平方损

失函数。那么对于

个样本

来说，则可以写成式 (5-4)：

更进

一步，目标函数可以写成

式 (5-5)：

其中 代表基模型的复

杂度，若基模型是树模型

，则树的深度、叶子节点数

等指标均可以

反映树的

复杂度。

对于

Boosting 来说，它采用

的是前向优化算法，即从

前往后逐渐建立基模型

来逼近目标函

数，具体过

程如式 (5-6)：

那么在逼近过程

的每一步中，如何学习一

个新的模型呢，答案的关

键还是在梯度提升树的

目

标函数上，即新模型的

加入总是以优化目标函

数为目的。重写目标函数

如式

(5-7)：

将式 (5-6) 泰勒展开到二

阶如式 (5-8)：

移除常数项，得到

式

(5-9)：

之所以要移除常数项

，是因为函数中的常量在

函数最小化的过程中不

起作用。这样一来，梯

度提

升树的最优化目标函数

就变得非常统一，它只依

赖于每个数据点在误差

函数上的一阶导

数和二

阶导数，然后根据加法模

型得到一个整体模型。

5.2.3 XGBoost

XGBoost 是

基于决策树的集成机器

学习算法，它以梯度提升

（Gradient Boost）为框架。在

SIGKDD 2016 大会上，陈天奇

和

Carlos Guestrin 发表的论文“XGBoost: A Scalable Tree

Boosting System”在整个机

器学习领域都引起了轰

动，并逐渐成为 Kaggle 和数据科

学界的主

导。XGBoost 同样也引入

了

Boosting 算法。

XGBoost 除了在精度和计

算效率上取得成功的性

能外，还是一个可扩展的

解决方案。由于

对初始树

Boost GBM

算法进行了重要调整，因

此 XGBoost 代表了新一代的 GBM 算法

。

主要特点

采用稀疏感知

算法，XGBoost 可以利用稀疏矩阵

，节省内存（不需要密集矩

阵）和节省计算时间（零值

以特殊方式处理）。

近似树

学习（加权分位数略图），这

类学习方式能得到近似

的结果，但比完

整的分支

切割探索要省很多时间

。

在一台机器上进行并行

计算（在搜索最佳分割阶

段使用多线程），在多台机

器上进行类似的分布式

计算。

利用名为核外计算

的优化方法，解决在磁盘

读取数据时间过长的问

题。将数

据集分成多个块

存放在磁盘中，使用一个

独立的线程专门从磁盘

读取数据并

加载到内存

中，这样一来，从磁盘读取

数据和在内存中完成数

据计算就能并

行运行。

XGBoost 还

可以有效地处理缺失值

，训练时对缺失值自动学

习切分方向。基

本思路是

在每次的切分中，让缺失

值分别被切分到决策树

的左节点和右节

点，然后

通过计算增益得分选择

增益大的切分方向进行

分裂，最后针对每个

特征

的缺失值，都会学习到一

个最优的默认切分方向

。

代码实现

输入：训练集 X_train，训

练集标签 y_train

验证集 X_valid，验证集

标签 y_valid

测试集 X_test

输出：训练好

的模型

model，测试集结果 y_pred

import xgboost as xgb

params = {'eta': 0.01, 'max_depth': 11,'objective':

'reg:linear', 'eval_metric': 'rmse' }

dtrain =

xgb.DMatrix(data=X_train, label=y_train)

dtest = xgb.DMatrix(data=X_valid, label=y_valid)

watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]

model=xgb.train(param, train_data,

num_boost_round=20000,

evals=watchlist,

early_stopping_rounds=200,

verbose_eval=500)

y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)

5.2.4 LightGBM

LightGBM 是微

软的一个团队在 Github 上开发

的一个开源项目，高性能

的 LightGBM 算法具

有分布式和可

以快速处理大量数据的

特点。LightGBM 虽然基于决策树和

XGBoost 而生，

但它还遵循其他不

同的策略。XGBoost 使用决策树对

一个变量进行拆分，并在

该变量上探

索不同的切

割点（按级别划分的树生

长策略），而

LightGBM 则专注于按叶

子节点进行拆

分，以便获

得更好的拟合（这是按叶

划分的树生长策略）。这使

得 LightGBM 能够快速获得

很好的

数据拟合，并生成能够替

代

XGBoost 的解决方案。从算法上

讲，XGBoost 将决策树

所进行的分

割结构作为一个图来计

算，使用广度优先搜索（BFS），而

LightGBM 使用的是

深度优先搜索

（DFS）。

主要特点

比 XGBoost 准确性更高

，训练时间更短。

支持并行

树增强，即使在大型数据

集上也能提供比 XGBoost

更好的

训练速

度。

通过使用直方

图算法将连续特征提取

为离散特征，实现了惊人

的快速训练速

度和较低

的内存使用率。

通过使用

按叶分割而不是按级别

分割来获得更高精度，加

快目标函数收敛过

程，并

在非常复杂的树中捕获

训练数据的底层模式。使

用

num_leaves 和

max_depth 超参数控制过拟合

。

代码实现

import

lightgbm as lgb

params = {'num_leaves':

54,'objective': 'regression','max_depth': 18,

'learning_rate': 0.01,'boosting': 'gbdt','metric':

'rmse','lambda_l1': 0.1}

model = lgb.LGBMRegressor(**params, n_estimators

= 20000, nthread = 4, n_jobs

= -1)

model.fit(X_train, y_train,

eval_set=[(X_train, y_train),

(X_valid, y_valid)],

eval_metric='rmse',

verbose=1000, early_stopping_rounds=200)

y_pred

= model.predict(X_test, num_iteration=model.best_iteration_)

5.2.5 CatBoost

CatBoost

是由俄罗斯搜

索引擎 Yandex 在 2017 年 7

月开源的一

个 GBM 算法，它最强大的一

点

是能够采用将独热编码

和平均编码混合的策略

来处理类别特征。

CatBoost 用来对

类别特征进行编码的方

法并不是新方法，是均值

编码，该方法已经成为一

种

特征工程方法，被广泛

应用于各种数据科学竞

赛中，如 Kaggle。均值编码，也称为

似然编

码、影响编码或目

标编码，可将标签转换为

基于它们的数字，并与目

标变量相关联。如果是

回

归问题，则基于级别典型

的平均目标值转换标签

；如果是分类问题，则仅给

定标签的目标

分类概率

（目标概率取决于每个类

别值）。均值编码可能看起

来只是一个简单而聪明

的特征

工程技巧，但实际

上它也有副作用，主要是

过拟合，因为会把目标信

息带入预测中。

主要特点

支持类别特征，因此我们

不需要预处理类别特征

（例如通过 label encoding 或

独热编码）。事

实上，CatBoost 文档中讲到不要在

预处理期间使用独热编

码，因为“这会影响训练速

度和结果质量”。

提出了一

种全新的梯度提升机制

（Ordered Boosting），不仅可以减少过拟合

的

风险，也大大提高了准确

性。

支持开箱即用的 GPU 训练

（只需设置

task_type="GPU"）。

训练中使用了

组合类别特征，利用了特

征之间的联系，极大丰富

了特征维

度。

深入理解特

征组合

CatBoost 另一个强大的功

能是在树分裂选择节点

的时候能够将所有类别

特征之间的

组合考虑进

来，即能够对两个类别特

征进行组合。具体做法是

，第一次分裂时不考

虑类

别特征的组合，之后分裂

时会考虑类别特征之间

的组合，使用贪心算法生

成最

佳组合，然后将组合

后的类别特征转化为数

值型特征。CatBoost 会把分裂得到

的两

组值作为类别型特

征参与后面的特征组合

，以实现更细粒度的组合

。

代码实现

from catboost import CatBoostRegressor

params =

{'learning_rate': 0.02,'depth': 13,'bootstrap_type': 'Bernoulli',

'od_type': 'Iter',

'od_wait': 50, 'random_seed': 11}

model =

CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)

model.fit(X_train, y_train, eval_set=(X_valid,

y_valid),

cat_features=[], use_best_model=True, verbose=False)

y_pred =

model.predict(X_test)

每类树模型都

其与众不同的地方，接下

来将从决策树的生长策

略、梯度偏差、类别

特征处

理和参数对比四个方面

深入理解这些树模型，帮

助参赛者更好地将它们

应用

到竞赛中。

更多功能

CatBoost 目前还支持输入文本特

征，因此不需要像以前那

样先进行烦琐的操作获

得

标准化输入，再喂给模

型。文本特征跟类别特征

的标记方式一样，只需在

训练时把

文本变量名的

列表赋给 text_features 即可。那么 CatBoost 内部

是怎么处理文本

特征的

呢？操作其实非常常规，CatBoost 内

部将输入的文本特征转

化为了数值特

征，具体过

程是分词、创建字典、将文

本特征转化为多值的数

值特征，接下来的处

理方

法可选择项就比较多了

，比如完全展开成布尔型

0/1 特征，或者进行词频统计

等。

5.2.6

模型深入对比

XGBoost、LightGBM 和 CatBoost 是三

个非常核心的树模型，本

节将对它们进行分析，因

为

三者之间有着千丝万

缕的关系，只有厘清其中

的关系，才能更好地运用

这三个模型。

决策树生长

策略

图 5.2 列出了三种决策

树的生长方式。

图 5.2

决策树

生长方式

XGBoost 使用的是 Level-wise 按层

生长，可以同时分裂同一

层的叶子，从而进行多

线

程优化，不容易过拟合，但

很多叶子节点的分裂增

益较低，会影响性能。

LightGBM 使用

的是 Leaf-wise 分裂方式，每次都从

当前叶子中选择增益最

大的结点

进行分裂，循环

迭代，但会生长出非常深

的决策树，从而导致过拟

合，这时可以调

整参数

max_depth 来

防止过拟合。

CatBoost 使用的是 oblivious-tree（对

称树），这种方式使得节点

是镜像生长的。相

对于传

统的生长策略，oblivious-tree

能够简单

拟合方案，快速生成模型

，这种树结构

起到了正则

化的作用，因此并不容易

过拟合。

梯度偏差（Gradient bias）

XGBoost 和

LightGBM 中的

提升树算法都是有偏梯

度估计，在梯度估计中使

用的数

据与目前建立的

模型所使用的数据是相

同的，这样会导致数据发

生泄漏，从而产生

过拟合

。

CatBoost 改进了提升树算法，将原

来的有偏梯度估计转换

为了无偏梯度估计。具体

做法是利用所有训练集

（除第

条）建立模型 ，然后使

用第 1 条到第 条

数据来建

一个修正树

，累加到原来

的模型 上。

类别特征处理

XGBoost 并不能处理类别特征，因

此需要我们根据数据实

际情况进行独热编码、

count 编

码和目标编码。

LightGBM 直接支持

类别特征，不需要独热展

开。这里使用 many-vs-many 的切分方

式

来处理类别特征，并且可

以把搜索最佳分割点的

时间复杂度控制在线性

级别，和

原来

one-vs-other 方式的时间

复杂度几乎一致。该算法

先按照每个类别对应的

标签均

值（即 avg(y)=Sum(y)/Count(y)）进行排序，然

后根据排序结果依次枚

举最优分割

点。和数值型

特征的切分方式不同，它

是将某一类别当作一类

，然后将其余所有类

别作

为一类。

CatBoost 在处理类别特征

方面做了更细致的操作

。或许在使用 LightGBM 时，还需

要对

类别特征进行更多的编

码方式，但对于 CatBoost，则可以选

择不进行多余的编

码方

式。

具体实现流程是首先

对输入的样本集随机排

序，然后针对类别特征中

的某个取值，

在将每个样

本的该特征转换为数值

型时，都基于排在该样本

之前的类别标签取均

值

。对所有的类别特征值结

果都进行如式 (5-10) 所示的运

算，使之转化为数值结

果

。

其中 [ ] 是指示函数，当方括

号内两个元素相等时取

1，反之取 0。

（ ）是

先验值 的权重

，添加先验值是一种常见

的做法，这有助于减少从

低频类别中获得

的噪声

、降低过拟合。对于回归问

题，将标签的平均值作为

先验值；对于分类问

题，将

正类出现的概率作为先

验值。

参数对比

如图 5.3 所示

，从三个方面对树模型的

参数进行对比，分别是用

于控制过拟合、用于

控制

训练速度和调整类别特

征的三类参数。这里只是

枚举一些重要的参数，还

有大

量有用的参数就不

一一进行介绍了。

图 5.3 核心

参数对比

5.3 神经网络

如果

想在竞赛的道路上走得

更远，那么神经网络也是

必须要掌握的模型。一般

而言，随着我

们拥有的数

据量不断增加，神经网络

战胜传统传统机器学习

模型的可能性也会加大

。

首先举一个房屋价格的

例子来展示神经网络的

功能细节，具体要根据房

屋的某些功能估算房

屋

的价格。如果提供的是房

屋大小、位置和卧室数量

等详细信息，要求估算房

屋价格，那么

此时神经网

络将是最合适的方法。

如

图 5.4

描述了神经网络的简

单结构，它具有三种不同

类型的层：输入层、隐藏层

和输出

层。每个隐藏层可

以包含任意数量的神经

元（节点），输入层中节点的

数量等于预测问题中

使

用的特征数量（上面示例

中有 3 个特征，即房屋大小

、位置和卧室数量），输出层

中节点

的数量等于要预

测的值的数量（上面示例

中要预测的值有

1 个，即房

屋价格）。接下来，我

们尝试

更深入一点，了解每个节

点中正在发生的事情。

图

5.4 神经网络

如图

5.5 所示，在输

入层的各个节点中，将输

入要素 、权重 和偏置 作为

输入，计算

并输出

。将这些

值构造成矩阵，可使计算

变得更加容易和高效。什

么是权重（weight）和

偏置（bais）？这两个

值首先是从高斯分布中

随机初始化的值，用于计

算输入节点的输出，

我们

通过调整这些值来让神

经网络拟合输入的数据

。

图 5.5 输入层节点的结构细

节

如图 5.6 所示，在计算出值

之后，我们对 使用激活函

数 。激活函数用于向模型

引入

一些非线性。如果我

们不应用任何激活函数

，则输出结果只能是线性

函数，并且可能无法成

功

地将复杂输入映射到输

出。

图 5.6 对输入层结果使用

激活函数

隐藏层中节点

的输入是上一层节点的

输出。最后的输出层会预

测一个值，对该值与已知

值

（真实值）进行比较，就可

计算出损失。从直觉上讲

，损失表示预测值与真实

值之间的误

差。

整个过程

先是计算出每个变量以

及每层的权重并计算误

差（前向传播），然后通过反

向传播

遍历每个层来测

量每个连接的误差贡献

，最后稍微调整连接器的

权重和偏置来减少误差

，以

确保对输出结果进行

正确的预测。

到目前为止

，我们对神经网络已经有

了基本的认识。接下来将

介绍多层感知机、卷积神

经网

络和循环神经网络

。

5.3.1 多层感知机

多层感知机

（MLP）也可以称作深度神经网

络（Deep Neural Networks，DNN），就是含有

多个隐藏层

的神经网络。单个感知机

尚且具有一定的拟合能

力，多层感知机必将拥有

更强大

的拟合能力，可以

用于解决更为复杂的问

题。

如图 5.7 所示，给出了多层

感知机最基本的结构，主

要分为输入层、隐藏层和

输出层。多层

感知机的不

同层之间均是全连接的

（全连接：指的是第 层中任

意神经元一定与第

层中

任意神经元相连接）。接下

来还需要详细学习三个

主要参数：权重、偏置和激

活函数。

图 5.7 多层感知机的

结构

权重与偏置

权重用

于表示神经元之间的连

接强度，权重的大小表示

可能性大小。设置偏置是

为

了正确分类样本，这是

模型中一个重要的参数

，即保证通过输入算出的

输出值不能

被随便激活

。

激活函数

激活函数可以

起到非线性映射的作用

，可以将神经元的输出幅

度限制在一定范围

内，一

般在 (–1,

1) 或 (0, 1) 之间。常用的激活

函数有 sigmoid、tanh、ReLU

等，其中

sigmoid 函数可将

(–∞, +∞) 之间的数映射到 (0,

1) 范围内

，其余的函数这里不做过

多

介绍。

代码实现

def create_mlp(shape):

X_input = Input((shape, ))

X =Dense(256,

activation='relu')(X_input)

X = Dense(128, activation='relu')(X)

X

= Dense(64, activation='relu')(X)

X = Dense(1,

activation='sigmoid')(X)

model = Model(inputs=X_input, outputs=X)

model.compile(optimizer='adam',

loss='binary_crossentropy', metrics=['accuracy'])

return model

mlp_model =

create_mlp(x_train.shape[0])

mlp_model.fit(x=x_train, y=y_train, epochs=30, batch_size=512)

5.3.2

卷积神

经网络

卷积神经网络（CNN）类

似于多层感知机，两者的

区别在于网络结构不同

。卷积神经网络

的提出是

受生物处理过程的启发

，其结构与动物视觉皮层

具有相似性。卷积神经网

络广泛应

用于计算机视

觉领域，比如人脸识别、自

动驾驶、图像分割等，并在

各种竞赛案例上取得了

优异的成绩。卷积神经网

络有两大特点。

能够有效

地将大数据量的图片降

维成小数据量，简化复杂

问题。在大部分场景下，

降

维并不会影响结果。比如

一张图片中有只猫，将该

图片的像素从

1000 缩小成 200

后

，即使肉眼看，也不会将其

中的猫认成狗，机器也是

如此。

能够有效保留图像

特征，并且符合图像处理

的原则。对图像进行翻转

、旋转或者变

换位置的操

作时，卷积神经网络能够

有效识别出哪些是类似

的图像。

如图 5.8 所示，给出了

卷积神经网络最基本的

结构，主要分为三层：卷积

层、池化层（采样

层）和全连

接层。三层各司其职，卷积

层负责提取特征，池化层

负责特征选择，全连接层

负

责分类。全连接层就是

我们前面所讲到的神经

网络，所以接下来只对卷

积层和池化层进行详

细

介绍。

图 5.8 卷积神经网络的

结构

卷积层（convolutional layer）

卷积层用于

提取图像中的局部特征

，可以有效降低数据维度

。如图

5.9 所示假设我们

的输

入的 RGB 图像大小为 32×32，因为含

3

种颜色的通道，所以输入

的实际大小是

32×32×3。然后选择

5×5 的卷积核进行卷积计算

，加之包含三个通道，所以

每次提取

5×5×3 大小的方块，将

提取步长（stride）设置为 1，并且不

向外侧进行填充

（padding=0），最后可

以得到 28×28×1 的特征图。

图 5.9 卷积

层操作

池化层（pooling layer）

池化层用

于特征选择，相比卷积层

可以更有效地降低数据

维度，不但能大大减少运

算量，还能有效避免过拟

合。例如，当数据经过卷积

层得到 28×28×1 的特征图后，

我们

设置步长为

2，卷积核大小

是 2×2，然后经过最大池化层

/ 平均池化层，得到

14×14×1 的新特

征图，如图 5.10

所示。有了上面

对基本结构的介绍，我们

就可以使

用 keras 来构建自己

的卷积神经网络了。

图 5.10

最

大 / 平均池化层（卷积核大

小为 2×2，步长为 2）

代码实现

def create_cnn():

X_input = Input((28,28,1))

X

= Conv2D(24,kernel_size=5,padding='same',activation='relu')(X_input)

X = MaxPooling2D()(X)

X

= Conv2D(48,kernel_size=5,padding='same',activation='relu')(X_input)

X = MaxPooling2D()(X)

X

= Flatten()(X)

X = Dense(128, activation='relu')(X)

X = Dense(64, activation='relu')(X)

X =

Dense(1, activation='sigmoid')(X)

model = Model(inputs=X_input, outputs=X)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

return model

cnn_model

= create_cnn()

cnn_model.fit(x=x_train, y=y_train, epochs=30, batch_size=64)

5.3.3 循

环神经网络

循环神经网

络（RNN）是神经网络的一种扩

展，更擅长对序列数据进

行建模处理。对于传

统的

前馈神经网络，输入一般

是一个定长的向量，无法

处理变长的序列信息，即

使通过一些

方法把序列

处理成定长的向量，模型

也很难捕捉序列中的长

距离依赖关系。循环神经

网络通

过将神经元串行

起来的方式处理序列化

的数据。由于每个神经元

都能用其内部变量保存

之前

输入的序列信息，因

此整个序列被浓缩成抽

象的表示，并可以据此进

行分类或生成新的序

列

。

对序列数据进行处理以

及用序列数据完成分类

决策或回归估计时，循环

神经网络是非常有效

的

，它通常用于解决与序列

数据相关的任务，主要包

括自然语言处理、语音识

别、机器翻

译、时间序列预

测等。当然，循环神经网络

也可以用于非序列数据

。

如图

5.11 所示为循环神经网

络的基本结构，它由一个

神经元接收输入，产生一

个输出，并

将输出返回给

自己，如图 5.11 中（1）所示。在每个

时间步 （也称为一个帧），循

环神经

元接收输入 以及

它自己的前一时间步长

。我们可以按照时间轴将

（1）推移展开成

网络，如图 5.11 中

（2）所示。

图

5.11 循环神经元（1），按时

间序列展开（2）

用公式表示

如下：

其中， 是隐藏层到输

出层的权重矩阵， 是输入

层到隐藏层的权重矩阵

，

也是权

重矩阵，表示隐藏

层上一次的值作为这一

次输入的权重。另外， 为输

入层， 为隐藏

层， 为输出层

。

由循环神经网络扩展的

序列相关模型，有 LSTM、GRU 等，在后

面章节也都会有相关介

绍

和应用。

代码实现

不同

于其他模型，循环神经网

络的输入数据包含序列

信息，例如，我们要对一个

电

影评论分类，需要进行

预处理，如果文本长度不

一致，需要利用 pad_sequences

进行截断

，保证每个样本序列的长

度一致。下面给出简单的

实现代码，其中

X_train 是已经预

处理好的训练集。

def

create_rnn():

emb = Embedding(10000, 32) #

10000 为总单

词个数，32 为输出维度

X = SimpleRNN(32)(emb)

X = Dense(256, activation='relu')(X)

X =

Dense(128, activation='relu')(X)

X = Dense(1, activation='sigmoid')(X)

model = Model(inputs=X_input, outputs=X)

model.compile(optimizer='adam', loss='binary_crossentropy',

metrics=['accuracy'])

return model

rnn_model = create_rnn()

rnn_model.fit(x=x_train, y=y_train, epochs=30, batch_size=64)

5.4 实战

案例

本节仅需选择多个

模型运行出结果即可，上

文给出的模型并不完整

，这里也将添加验证方

式

，让结果变得更加可靠。对

于多个模型的结果可以

对比着进行分析，以助模

型融合部分的

优化。

# 接第

5 章实战案例代码，构造训

练集和测试集

x_train = data[:ntrain][all_cols]

x_test = data[ntrain:][all_cols]

# 对售价进

行log 处理

y_train = np.log1p(data[data.SalePrice.notnull()]['SalePrice'].values)

XGBoost

这里使用比较常

规的五折交叉验证，

import xgboost as xgb

from sklearn.model_selection import KFold

kf =

KFold(n_splits=5, shuffle=True, random_state=2020)

for i, (train_index,

valid_index) in enumerate(kf.split(x_train, y_train)):

trn_x, trn_y,

val_x, val_y = x_train.iloc[train_index], y_train[train_index],

x_train.iloc[valid_index],

y_train[valid_index]

params = {'eta': 0.01, 'max_depth':

11,'objective': 'reg:linear', 'eval_metric': 'mae' }

dtrain

= xgb.DMatrix(data=trn_x, label=trn_y)

dtest = xgb.DMatrix(data=val_x,

label=val_y)

watchlist = [(dtrain, 'train'), (dtest,

'valid_data')]

model=xgb.train(params, dtrain,

num_boost_round=20000,

evals=watchlist,

early_stopping_rounds=200,

verbose_eval=500)

多层

感知机

在构建多层感知

机前，需要确保数据中没

有缺失值，并且要进行归

一化处理。这里

使用数据

集随机切分的方式进行

线下验证。

from sklearn.preprocessing

import StandardScaler

x_train = x_train.fillna(0)

x_train

= StandardScaler().fit_transform(x_train)

from sklearn.model_selection import train_test_split

trn_x, val_x, trn_y, val_y = train_test_split(x_train,

y_train, random_state = 2020)

def create_mlp(shape):

X_input = Input((shape,))

X = Dropout(0.2)(BatchNormalization()(Dense(256,

activation='relu')(X_input)))

X = Dropout(0.2)(BatchNormalization()(Dense(128, activation='relu')(X)))

X

= Dropout(0.2)(BatchNormalization()(Dense(64, activation='relu')(X)))

X = Dense(1)(X)

model = Model(inputs=X_input, outputs=X)

model.compile(optimizer='adam', loss='mse',

metrics=['mae'])

return model

mlp_model = create_mlp(trn_x.shape[1])

mlp_model.fit(x=trn_x, y=trn_y, validation_data = (val_x, val_y),

epochs=30, batch_size=16)

目前为止给出

的模型都是比较容易实

现的，这有助于快速反馈

出结果。对比

XGBoost（结果取对数

后平均绝对误差：0.08x）和多层

感知机（平均绝对误差：

0.21x）的

线下结果，发现后者的效

果差了很多，2000 多条的训练

数据很难让多层感

知机

取得一个较好的结果。

5.5 思

考练习

01. 在 Lasso

回归和 Ridge 回归部

分，我们知道 L1 和 L2

能够减少

过拟合的风险，那这个

参

数究竟取多大合适？

02. 树模

型在分裂的时候其实可

以看着特征的交叉组合

阶段，那么还有必要构造

交叉特

征喂入树模型吗

？

03.

本章介绍了树模型的核

心参数，还有很多没有介

绍到，请尝试分析参数之

间的关

系，以及具体参数

在算法中的哪个步骤中

出现，加深对参数的理解

。

04. 常用的激活函数还是蛮

多的，在进行深度学习相

关模型训练时，不同的激

活函数对

于结果的影响

还是蛮大的，尝试整理 sigmoid、tanh、ReLU、leaky

ReLU、SELU 和

GELU 等激活函数的优缺点以

及适用场景。

第 6 章

模型融

合

本章将向大家介绍在

算法竞赛中提分的关键

步骤，这也是最后阶段的

惯用方法，即模型融合

（或

者集成学习），通过结合不

同子模型的长处进行模

型融合，当然这是在理想

状态下。

本章主要分为构

建多样性、训练过程融合

和训练结果融合三部分

。模型融合常常是竞赛取

得

胜利的关键，相比之下

具有差异性的模型融合

往往能给结果带来很大

提升。虽然并不是每次

使

用模型融合都能起很大

的作用，但是就平时的竞

赛经验而言，我们得到的

一条结论是：模

型融合在

绝大多数情况下会带来

或多或少的帮助，在竞赛

中，尤其是在最终成绩相

差不大的

情况下，模型融

合的方法往往会成为取

胜的关键之一，在不同类

型的竞赛中，我们也很难

保

证哪一种方法就一定

会比另外一种好，评价指

标往往还得从线上结果

来看，只是说了解的模

型

融合方法越多，最后取胜

的概率就会越高。所以在

本章，我们也将从这三个

部分介绍不同

模型融合

方法的应用场景，同时给

出使用技巧和应用代码

。

6.1

构建多样性

本节将介绍

三种模型融合中构建多

样性的方式，分别是特征

多样性、样本多样性和模

型多样

性。其中多样性是

指子模型之间存在着差

异，可以通过降低子模型

融合的同质性来构建多

样

性，好的多样性有助于

模型融合效果的提升。

6.1.1 特

征多样性

构建多个有差

异的特征集并分别建立

模型，可使特征存在于不

同的超空间（hyperspace），

从而建立的

多个模型有不同的泛化

误差，最终模型融合时可

以起到互补的效果。在竞

赛中，

队友之间的特征集

往往是不一样的，在分数

差异不大的情况下，直接

进行模型融合基本会获

得不错的收益。

另外，像随

机森林中的 max_features、XGBoost 中的

colsample_bytree 和 LightGBM

中的

feature_fraction 都是用来对训练集中的

特征进行采样的，其实本

质上就是构建特

征的多

样性。

6.1.2 样本多样性

样本多

样性也是竞赛中常见的

一种模型融合方式，这里

的多样性主要来自不同

的样本集。具

体做法是将

数据集切分成多份，然后

分别建立模型。我们知道

很多树模型在训练的时

候会进

行采样（sampling），主要目的

是防止过拟合，从而提升

预测的准确性。

有时候将

数据集切分成多份并不

是随机进行的，而是根据

具体的赛题数据进行切

分，需要考

虑如何切分可

以构建最大限度的数据

差异性，并用切分后的数

据分别训练模型。

例如，在

天池“全球城市计算 AI 挑战

赛”中，竞赛训练集包含从

2019 年 1

月 1 日到 1 月 25

日共 25 天的地

铁刷卡数据记录，要求预

测 1 月 26

日每个地铁站点每

十分钟的平均出入客流

量（2019 年 1 月 26 日是周六）。显然，工

作日和周末的客流量分

布具有很大差异，这时会

面临一个问题，若只保留

周末的数据进行训练，则

会浪费掉很多数据；若一

周的数据全部保

留，则会

对工作日的数据产生一

定影响。这时候就可以尝

试构建两组有差异性的

样本分别训

练模型，即整

体数据保留为一组，周末

数据为一组。当然，模型融

合后的分数会有很大提

升。

6.1.3 模型多样性

不同模型

对数据的表达能力是不

同的，比如 FM

能够学习到特

征之间的交叉信息，并且

记忆

性较强；树模型可以

很好地处理连续特征和

离散特征（如 LightGBM 和 CatBoost），并且对

异

常值也具有很好的健壮

性。把这两类在数据假设

、表征能力方面有差异的

模型融合起来肯

定会达

到一定的效果。

对于竞赛

而言，传统的树模型（XGBoost、LightGBM、CatBoost）和神

经网络都需要尝试

一遍

，然后将尝试过的模型作

为具有差异性的模型融

合在一起。

更多多样性的

方法

除了本节所讲，还有

很多其他构建多样性的

方法，比如训练目标多样

性、参数多样性和

损失函

数选择的多样性等，这些

都能产生非常好的效果

。

6.2 训练过程融合

模型融合

的方式有两种，第一种是

训练过程融合，比如我们

了解到的随机森林和

XGBoost，基

于这两种模型在训练中

构造多个决策树进行融

合，这里的多个决策树可

以看作

多个弱学习器。其

中随机森林通过 Bagging的方式

进行融合，XGBoost

通过 Boosting的方式

进

行融合。

6.2.1 Bagging

Bagging的思想很简单，即

从训练集中有放回地取

出数据（Bootstrapping），这些数据构成

样

本集，这也保证了训练集

的规模不变，然后用样本

集训练弱分类器。重复上

述过程多次，

取平均值或

者采用投票机制得到模

型融合的最终结果。上述

流程的示意图如图 6.1 所示

。

图 6.1

Bagging 流程

当我们在不同的

样本集上训练模型时，Bagging通

过减小误差之间的差来

减少分类器的方

差。换言

之，Bagging可以降低过拟合的风

险。Bagging 算法的效率来自于训

练数据的不同，

各模型之

间存在着很大的差异，并

且在加权融合的过程中

可使训练数据的错误相

互抵消。当

然，这里可以选

择相同的分类器进行训

练，也可以选择不同的分

类器。另外，基于 Bagging

的算法有

Bagging meta-estimator 和随机森林。

6.2.2

Boosting

毫不夸张地

讲，Boosting的思想其实并不难理

解，首先训练一个弱分类

器，并把这个弱分类

器分

错类的样本记录下来，同

时给予这个弱分类器一

定的权重；然后建立一个

新的弱分类

器，新的弱分

类器基于前面记录的错

误样本进行训练，同样，我

们也给予这个分类器一

个权

重。重复上面的过程

，直到弱分类器的性能达

到某一指标，例如当再建

立的新弱分类器并不

会

使准确率显著提升时，就

停止迭代。最后，把这些弱

分类器各自乘上相应的

权重并全部加

起来，就得

到了最后的强分类器。其

实，基于 Boosting的算法是比较多

的，有 Adaboost、

LightGBM、XGBoost 和 CatBoost

等。

6.3 训练结果融合

模型融合的第二种方式

是训练结果融合，主要分

为加权法、Stacking和 Blending，这些方法

都

可以有效地提高模型的

整体预测能力，在竞赛中

也是参赛者必须要掌握

的方法。

6.3.1

加权法

加权法对

于一系列任务（比如分类

和回归）和评价指标（如 AUC、MSE 或

Logloss）都是很

有效的，比如我们

有 10

个算法模型并都预测

到了结果，直接对这 10 个结

果取平均值或者给

予每

个算法不同的权重，即得

到了融合结果。加权法通

常还能减少过拟合，因为

每个模型的

结果可能存

在一定的噪声，加权法能

够平滑噪声，提高模型的

泛化性。

分类问题

对于分

类问题，需要注意不同分

类器的输出结果范围一

致，因为输出的预测结果

可

以是 0/1 值，也可以是介于

0 和 1

之间的概率。另外，投票

法（Voting）也是一种特

殊的加权

法，假设三个模型分别输

出三组结果：

只要保证这

三个结果的权重一致，不

论是投票法（少数服从多

数），还是加权法

（固定 0.5 为阈

值），最终得到的融合结果

均为

1110110011。

回归问题

对于回归

问题，如果使用加权法，则

会非常简单。这里主要介

绍算法平均和几何平

均

，那么为什么有两种选择

呢，主要还是因为评价指

标。在 2019 腾讯广告算法大赛

中，选择几何平均的效果

远远好于选择算术平均

，这是由于评分规则是平

均绝对百

分比误差（SMAPE），此时

如果选择算术平均则会

使模型融合的结果偏大

，这不符

合平均绝对百分

比误差的直觉，越小的值

对评分影响越大，算术平

均会导致出现更

大的误

差，所以选择几何平均，能

够使结果偏向小值。

算术

平均。基于算术平均数的

集成方法在算法中是用

得最多的，因为它不仅

简

单，而且基本每次使用该

算法都有较大概率能获

得很好的效果。其公式如

式 (6-2)：

几何平均。根据很多参

赛选手的分享，基于几何

平均数的加权法在算法

中使

用得还不是很多，但

在实际情况中，有时候基

于几何平均数的模型融

合效果

要稍好于基于算

术平均数的效果。

排序问

题

一般推荐问题中的主

要任务是对推荐结果进

行排序，常见的评价指标

有 mAP（mean

Average Precision）、NDCG（Normalized Discounted Cumulative Gain）、

MRR（Mean

Reciprocal Rank）和 AUC，这里主要介绍 MRR 和 AUC。

(1) MRR

给

定推荐结果 ，如果 在推荐

序列中的位置是 ，那么

就

是 。

可以看出，如果向用户

推荐的产品在推荐序列

中命中，那么命中的位置

越靠前，得

分也就越高。显

然，排序结果在前在后的

重要性是不一样的，因此

我们不仅要进行

加权融

合，还需要让结果偏向小

值。这时候就要对结果进

行转换，然后再用加权法

进行融合，一般而言使用

的转换方式是 log变换。其基

本思路如下。

首先，输入三

个预测结果文件，每个预

测结果文件都包含 条记

录，每条记录各

对应 个预

测结果，最终输出三个预

测结果文件的整合结果

。内部的具体细节可

以分

为以下两步。

第一步：统计

三个预测结果文件中记

录的所有推荐商品（共

个

商品）出现的位

置，例如商

品 A，在第一份文件中的推

荐位置是 1，在第二个文件

的推荐位置是 3，

在第三个

文件中未出现，此时我们

计算商品

A 的得分为

，此处

我们用 来表示未出现，即

在

个推荐商品中是找不

到商品 A

的，所以只能是 。

第

二步：对每条记录中的商

品按计算得分由小到大

排序，取前 个作为这条记

录

的最终推荐结果。

(2)

AUC

AUC 作为

排序指标，一般使用排序

均值的融合思路，使用相

对顺序来代替原先的概

率值。很多以 AUC 为指标的比

赛均取得了非常不错的

成绩，如下两步为一种使

用过

程。

第一步：对每个分

类器中分类的概率进行

排序，然后用每个样本排

序之后得到的排

名值（rank）作

为新的结果。

第二步：对每

个分类器的排名值求算

术平均值作为最终结果

。

6.3.2 Stacking 融合

使用加权法进行融

合虽然简单，但需要人工

来确定权重，因此可以考

虑更加智能的方式，通

过

新的模型来学习每个分

类器的权重。这里我们假

设有两层分类器，如果在

第一层中某个特

定的基

分类器错误地学习了特

征空间的某个区域，则这

种错误的学习行为可能

会被第二层分

类器检测

到，这与其他分类器的学

习行为一样，可以纠正不

恰当的训练。上述过程便

是

Stacking 融合的基本思想。

这里

需要注意两点：第一，构建

的新模型一般是简单模

型，比如逻辑回归这样的

线性模型；

第二，使用多个

模型进行 Stacking 融合会有比较

好的结果。

Stacking 融合使用基模

型的预测结果作为第二

层模型的输入。然而，我们

不能简单地使用完

整的

训练集数据来训练基模

型，这会产生基分类器在

预测时就已经“看到”测试

集的风险，因

此在提供预

测结果时出现过度拟合

问题。所以我们应该使用

Out-of-Fold 的方式进行预测，

也就是

通过 折交叉验证的方式

来预测结果。这里我们将

Stacking融合分为训练阶段和测

试阶段两部分，将并以流

程图的形式展示每部分

的具体操作。如图 6.2

所示为

训练阶段。

图 6.2 Stacking 融合的训练

阶段

在图

6.2 中，我们对每个

模型都使用五折交叉验

证的方式，然后可以获取

完整验证集的预测

概率

结果，最终将得到的 列概

率结果和训练集标签拼

接成第二层的训练样本

，这样就可

以训练第二层

的模型。之后，我们将五折

交叉验证时训练的模型

（如模型 1，可以训练得到

5 个

不一样的模型 1）用作测试

集的训练。

如图 6.3 所示，测试

阶段将使用训练阶段训

练好的模型，首先使用模

型

1 得到的 5 个模型分

别对

测试集进行预测，然后将

5 个概率结果通过加权平

均得到一个概率结果概

率

1。然后对

模型 2 到模型 也

依次进行以上操作，最后

得到 个概率结果。将这

个

结果作为第

二层的测试

样本，然后使用第二层训

练得到的模型对第二层

测试样本预测得到最终

结果。

图 6.3 Stacking 融合的测试阶段

扩展学习

请思考特征加

权的线性堆叠，可参考相

应论文“Feature-Weighted Linear Stacking two layer

stacking”，其实就是对传统

的

Stacking融合方法在深度上进

行扩展。通过传统的 Stacking

融合

方法得到概率值，再将此

值与基础特征集进行拼

接，重新组成新的特征集

，进行新

一轮训练。

6.3.3 Blending

融合

不

同于 Stacking 融合使用 折交叉验

证方式得到预测结果，Blending融

合是建立一个

Holdout

集，将不相

交的数据集用于不同层

的训练，这样可以在很大

程度上降低过拟合的风

险。假设我们构造两层 Blending，将

训练集按 5∶5 的比例分为两

部分（train_one 和

train_two），测试集为

test。

第一层

用 train_one 训练多个模型，将此模

型对 train_two 和

test 的预测结果合并

到

原始特征集中，作为第

二层的特征集。第二层用

train_two 的特征集和标签训练新

的模

型，然后对 test

预测得到

最终的融合结果。

6.4 实战案

例

本节将带大家一起完

成 Stacking融合的操作，这需要构

造多个模型的预测结果

，一般是 3

个

以上，这里选择

ExtraTreesRegressor、RandomForestRegressor、Ridge 和 Lasso

作为基分类器，Ridge 作为最

终分类器。首先导入些新

的包：

from sklearn.ensemble import ExtraTreesRegressor

from sklearn.ensemble

import RandomForestRegressor

from sklearn.metrics import mean_squared_error

from sklearn.linear_model import Ridge, Lasso

from

math import sqrt

# 依然采用五折交叉

验证

kf

= KFold(n_splits=5, shuffle=True, random_state=2020)

然后构建一个 sklearn

中模

型的功能类，初始化参数

然后训练和预测。这段代

码可复用性很

高，建议大

家不断完善搭建，构造自

己的一套“弹药库”。

class SklearnWrapper(object):

def __init__(self,

clf, seed=0, params=None):

params['random_state'] = seed

self.clf = clf(**params)

def train(self, x_train,

y_train):

self.clf.fit(x_train, y_train)

def predict(self, x):

return self.clf.predict(x)

之后封

装交叉验证函数，这段代

码的可复用性也非常高

：

def get_oof(clf):

oof_train

= np.zeros((x_train.shape[0],))

oof_test = np.zeros((x_test.shape[0],))

oof_test_skf

= np.empty((5, x_test.shape[0]))

for i, (train_index,

valid_index) in enumerate(kf.split(x_train, y_train)):

trn_x, trn_y,

val_x, val_y = x_train.iloc[train_index], y_train[train_index],

x_train.iloc[valid_index],

y_train[valid_index]

clf.train(trn_x, trn_y)

oof_train[valid_index] = clf.predict(val_x)

oof_test_skf[i, :] = clf.predict(x_test)

oof_test[:] =

oof_test_skf.mean(axis=0)

return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)

接下来是基分类器训练

和预测的部分代码，可预

测四个模型的验证集结

果和测试集结果，并

辅助

最后一步的 Stacking融合操作：

et_params = {

'n_estimators': 100,

'max_features': 0.5,

'max_depth': 12,

'min_samples_leaf': 2,

}

rf_params = {

'n_estimators': 100,

'max_features': 0.2,

'max_depth': 12,

'min_samples_leaf': 2,

}

rd_params={'alpha': 10}

ls_params={'alpha':

0.005}

et = SklearnWrapper(clf=ExtraTreesRegressor, seed=2020, params=et_params)

rf = SklearnWrapper(clf=RandomForestRegressor, seed=2020, params=rf_params)

rd

= SklearnWrapper(clf=Ridge, seed=2020, params=rd_params)

ls =

SklearnWrapper(clf=Lasso, seed=2020, params=ls_params)

et_oof_train, et_oof_test =

get_oof(et)

rf_oof_train, rf_oof_test = get_oof(rf)

rd_oof_train,

rd_oof_test = get_oof(rd)

ls_oof_train, ls_oof_test =

get_oof(ls)

最

后就是 Stacking 部分，使用的 Ridge 模型

，当然也可以尝试树模型

这类更加复杂的模

型：

def stack_model(oof_1, oof_2, oof_3, oof_4,

predictions_1, predictions_2,

predictions_3, predictions_4, y):

train_stack

= np.hstack([oof_1, oof_2, oof_3, oof_4])

test_stack

= np.hstack([predictions_1, predictions_2, predictions_3, predictions_4])

oof

= np.zeros((train_stack.shape[0],))

predictions = np.zeros((test_stack.shape[0],))

scores

= []

for fold_, (trn_idx, val_idx)

in enumerate(kf.split(train_stack, y)):

trn_data, trn_y =

train_stack[trn_idx], y[trn_idx]

val_data, val_y = train_stack[val_idx],

y[val_idx]

clf = Ridge(random_state=2020)

clf.fit(trn_data, trn_y)

oof[val_idx] = clf.predict(val_data)

predictions += clf.predict(test_stack)

/ 5

score_single = sqrt(mean_squared_error(val_y, oof[val_idx]))

scores.append(score_single)

print(f'{fold_+1}/{5}', score_single)

print('mean: ',np.mean(scores))

return

oof, predictions

oof_stack , predictions_stack =

stack_model(et_oof_train, rf_oof_train,

rd_oof_train, ls_oof_train, et_oof_test, rf_oof_test,

rd_oof_test,

ls_oof_test, y_train)

对

比一下最终效果，Stacking融合后

的为 0.13157，基分类器最优的为

0.13677，大约有五个

千分点的提

升，说明模型融合还是有

一定效果的。另外我们也

进行了普通的加权平均

融合方

案，分数只有 0.13413，可以

看出效果相对差些。

6.5 思考

练习

01. 还有很多构建多样

性的方法，比如训练目标

多样性、参数多样性和损

失函数的选择

等，都能产

生非常好的效果，请对更

多方法进行梳理归纳。

02. 直

觉上 Stacking融合都能带来很好

的收益，可为什么有时候

Stacking融合之后的效果

会变差

，是基模型选择的问题，还

是层数不够，请分析有哪

些因素会影响最终融合

结果。

03.

尝试搭建 Stacking 融合的框

架，并使其可复用，便于参

赛者在竞赛中灵活调用

。

第二部分 物以类聚，人以

群分

第

7 章 用户画像

第 8 章

实战案例：Elo

Merchant Category Recommendation

第 7 章

用户画像

古语有云，千人千面。世界

上的每个人都是唯一的

个体，就像世界上找不到

完全相同的两片

树叶那

样，世界上也找不到同样

的人。从出生起，人就开始

拥有自己独属的标签，比

如姓甚

名谁、父母何许人

也、家在哪里、何时来到世

界、即将开启怎样的人生

旅程等。经历得越

多，人的

独特性就愈发明显，即使

是一母同胞，也总有各自

分开的时候，差异也就开

始产

生。孤独是人一生中

一个永恒的主题，是对心

理状态的一种描述，只要

寻一灵魂伴侣便可变

得

不再孤单，然而除了孤字

，还有独一无二的独字。由

于个体间存在着巨大的

差异性和个体

独特性使

得分析和研究个体变得

十分复杂，甚至没法做到

，而且也不太必要，因此心

理学、

社会学探讨较多的

都是群体特征，偶有对个

体异常行为的分析也不

过是基于事后结果去追

根

溯源罢了。

即使是在互

联网信息爆炸到号称大

数据人工智能时代的今

天，从一个人产生的数字

记录中能

得到的信息依

然只是其生命中的一部

分，别人根本没办法完全

知道他在想什么、未来会

做什

么。当然也没必要太

过悲观，如果能够利用好

记录到的信息也是可以

在一定程度上了解一个

人的。因此只基于某个层

面的数据便可以产生部

分个体画像，可用于从特

定角度形容群体乃

至个

体的大致差异。说了这么

多，那这些和本章要探讨

的用户画像有什么关系

呢？首先要明

确用户画像

里的用户是谁：数据收集

方（即产品提供方）往往开

发出某件产品供人使用

，这

些使用者便是数据收

集方的用户，数据收集方

为了推广产品同时持续

维护和改善用户体验便

需要对由用户操作而产

生的数据进行挖掘，以期

从中发现群体乃至个体

的行为偏好，形成数

据层

面上的所谓画像。更广泛

一点，对任何群体都可以

形成画像，比如一个地区

、一个年

代、一个社群的群

体等。

智能手机普及以后

，吸引了人们大部分闲暇

时间的目光，各式各样的

App

产生了大量对用

户操作

行为的记录数据，这就为

形成用户画像打下了基

础，因此基于用户画像的

机器学习算

法便拥有了

众多应用场景，此类场景

的竞赛也占据着一部分

江山。本章将从为什么是

用户画

像、标签系统、用户

画像数据特征、用户画像

的应用以及思考练习五

个部分进行介绍。

7.1 什么是

用户画像

不可否认，当想

到一个人时无论在表面

上还是潜意识里，其实都

会对这个人有一个大致

印

象，比如身材面相、社会

属性、性格修养、兴趣爱好

等，这个脑海里的大致印

象虽然也可以

被看作一

种具有相对主观意识的

画像，但这显然不是本节

要讲的用于商业分析和

数据挖掘的

用户画像。

在

机器学习中提到的用户

画像通常是基于给定的

数据对用户属性以及行

为进行描述，然后提

取用

户的个性化指标，再以此

分析可能存在的群体共

性，并落地应用到各种业

务场景中。在

互联网时代

，面向用户的产品多如牛

毛，且数据采集相对容易

，由此推动了机器学习在

面向

用户方面的应用，用

户画像便是其中最重要

的一环。在各式各样的机

器学习算法竞赛中，对

于

用户数据的挖掘始终占

有一席之地，因此用户画

像时常出没在竞赛中，扮

演着不可或缺的

角色，接

下来将向大家详细介绍

用户画像的组成以及如

何在竞赛中使用用户画

像。

7.2 标签系统

用户画像的

核心其实就是给用户“打

标签”，即标签化用户的行

为特征，企业通过用户画

像中

的标签来分析用户

的社会属性、生活习惯、消

费行为等信息，然后进行

商业应用。构建一个

标签

系统成为企业赋能更多

业务的关键，标签系统也

是本节要详细介绍的内

容，具体从三个

方面来展

开，分别是标签分类方式

、多渠道获取标签和标签

体系框架。

7.2.1 标签分类方式

如图

7.1 所示，通过分析一个

用户的特征来展示标签

分类方式。

图 7.1 标签分类方

式

7.2.2

多渠道获取标签

根据

标签获取方式可以把标

签分为事实类标签、规则

类标签和模型类标签三

种。标签获取方

式也可以

看作特征获取方式，借助

这三种方式从原始数据

中提取能够表现用户特

点、商品特

点或者数据特

点的特征。

事实类标签

事

实类标签最容易获取，直

接来源于原始数据，比如

性别、年龄、会员等级等字

段。当然也可以对原始数

据进行简单的统计后提

取事实标签，比如用户行

为次数、

消费总额。

规则类

标签

规则类标签使用广

泛，是由运营人员和数据

人员经过共同协商而设

定的多个规则生

成的标

签，其特点是直接有效灵

活、计算复杂度低和可解

释度高，主要用作较为直

观和清晰的用户相关标

签，例如地域所属、家庭类

型、年龄层等。使用到的技

术知

识主要是数理统计

类知识，例如基础统计、数

值分层、概率分布、均值分

析、方差

分析等。

如图 7.2 所示

，主要对单一或者多种指

标进行逻辑运算、函数运

算等运算生成最终的

规

则类标签，这里分为用户

行为、用户偏好和用户价

值三部分，感兴趣的话，可

对

用户进行更深层次的

刻画。

图

7.2 规则类标签分类

模型类标签

模型类标签

是经过机器学习和深度

学习等模型处理后，二次

加工生成的洞察性标

签

。比如预测用户状态、预测

用户信用分、划分兴趣人

群和对评论文本进行分

类

等，经过这些处理得到

的结果就是模型类标签

。其特点是综合程度高、复

杂程度

高，绝大部分标签

需要先有针对性地构建

相应的挖掘指标体系，然

后依托经典数学

算法或

模型进行多指标间的综

合计算方能得到模型类

标签，常常需要多种算法

一起

组合来建模。

如图 7.3 所

示，基于模型类标签可以

使用 RFM

模型来衡量用户价

值和用户创利能

力、对用

户行为信息建模预测用

户生命周期变化情况、通

过模型预测用户信用评

分、使用图嵌入或用户分

层模型划分兴趣人群。除

此之外还有很大通过模

型来得到

标签的方法。

图

7.3 模型类标签分类

7.2.3

标签体

系框架

对标签的分类和

获取有了初步了解后，我

们可以将其串联起来形

成基本的标签体系框架

，包

括从底层数据的提取

到业务应用赋能。

如图 7.4 所

示，可以看出整个标签体

系框架分为四个部分：数

据源、标签管理、标签层级

分

类和标签服务赋能。

图

7.4 标签体系框架

7.3 用户画像

数据特征

无论是构建用

户画像还是进行算法竞

赛，数据都是产生效益的

核心。一般而言，用户画像

的

数据来源为用户数据

、商品数据和渠道数据，比

如从电商网站和微信平

台可以获取用户的交

易

数据、行为数据等，从平台

用户系统可以获取用户

属性数据。这些数据的存

在形式多样，

通过对数据

形式的理解可以进行统

计、编码和降维等处理来

提取有效特征，然后用这

些特征

构造我们需要的

标签。本节我们将介绍常

见的数据形式以及用户

画像相关竞赛的一些特

征提

取方法。

7.3.1

常见的数据

形式

在各式各样的竞赛

当中，数据的形态和格式

是多种多样的，本节以用

户画像为例将数据的有

关字段大致分为数值型

变量、类别型变量、多值型

变量以及文本型变量四

种常见的数据形

式，每种

变量都有对应的处理方

式。需要强调的是这些变

量都是针对用户层面的

，即所有样

本数据以用户

为唯一主键进行区分，且

每个用户只有一条记录

，之所以这样举例是因为

通常

基于用户画像的机

器学习模型所需的数据

是以用户池的形式呈现

的，对用户的标签进行对

应

的特征学习。实际竞赛

中给的数据可能十分复

杂，甚至是以打点记录的

方式描述用户的行

为，这

时候往往还需要参赛者

构建提取用户特征，这涉

及更深的应用技巧。

数值

型变量

最常见的一种数

值型变量是连续型变量

，指的是具有数值大小含

义的变量，比如图

7.5 所示的

年龄、身高、体重等，其他如

消费金额、流量累计等。

图

7.5

连续型变量

类别型变量

类别型变量是指具有类

别标识的变量，比如性别

、籍贯、所在城市等，这类变

量记

录了用户的固有属

性，如图 7.6 所示。

图

7.6 类别型变

量

多值型变量

多值型变

量是指用户在某个维度

具有多个取值的变量，比

如兴趣爱好、穿衣风格、

看

过的电影等，这类变量由

于其特殊结构无法直接

应用到模型中，需要借助

特别的

数据结构如稀疏

矩阵进行处理，如图

7.7 所示

。

图 7.7 多值型变量

文本型变

量

文本型变量（如图 7.8 所示

）是指利用文本记录的变

量，比如用户对某件商品

或者某

次购物的评论等

。处理这类变量需要用到

自然语言处理的一些工

具，比如中文分词

工具 jieba

等

。

图 7.8 文本型变量

接下来将

介绍一些常见的特征提

取方法，以使读者在面对

用户画像相关竞赛时能

够

更好地解决问题。具体

介绍文本挖掘算法、神奇

的嵌入表示以及相似度

计算方法。

7.3.2 文本挖掘算法

对于基础的原始数据，比

如经常出现的用户标签

集合、购物评价等，除了常

见的统计特征

外，还能够

基于文本挖掘算法进行

特征提取，同时对原始数

据进行预处理和清洗，以

达到匹

配和标识用户数

据的效果。本节将会对常

见的文本挖掘算法 LSA、PLSA 和

LDA 进

行介

绍，这三种均为无监

督学习方法。

LSA

LSA（潜在语义分

析）是一种非概率主题模

型，与词向量有关，主要用

于文档的话

题分析，其核

心思想是通过矩阵分解

的方式来发现文档与词

之间基于话题的语义关

系。具体地，将文档集表示

为词–文档矩阵，对词–文档

矩阵进行

SVD（奇异值分

解），从

而得到话题向量空间以

及文档在话题向量空间

的表示。

LSA 的具体使用也非

常简单，我们以 2020 腾讯广告

算法大赛中的数据为例

，首先构

造用户点击的广

告素材 ID 序列（creative_id），然后进行 TF-IDF 计

算，最后经

过

SVD 得到结果，实

现代码如下：

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.decomposition import TruncatedSVD

from sklearn.pipeline

import Pipeline

# 提取用户点

击序列

docs=data_df.groupby(['user_id'])['creative_id'].agg(lambda x:"

".join(x)).reset_index()['creative_id']

# tfidf + svd

tfidf

= TfidfVectorizer()

svd = TruncatedSVD(n_components=100)

svd_transformer

= Pipeline([('tfidf', tfidf), ('svd', svd)])

lsa_matrix

= svd_transformer.fit_transform(documents)

PLSA

PLSA（概率潜在语义分

析）模型其实是为了克服

LSA 模型潜在存在的一些缺

点而提

出的。PLSA

模型通过一

个生成模型来为 LSA 赋予概

率意义上的解释。该模型

假设

每一篇文档都包含

一系列可能的潜在话题

，文档中的每一个单词都

不是凭空产生

的，而是在

潜在话题的指引下通过

一定的概率生成的。

LDA

LDA（潜在

狄利克雷分布）是一种概

率主题模型，与词向量无

关，可以将文档集中

每篇

文档的主题以概率分布

的形式给出。通过分析一

批文档集，抽取出它们的

主题

分布，就可以根据主

题分布进行主题聚类或

文本分类。同时，它是一种

典型的词袋

模型，即一篇

文档由一组相互独立的

词构成，词与词之间没有

先后顺序关系。

7.3.3 神奇的嵌

入表示

毫不夸张地说，任

何可以形成网络结构的

东西，都可以有嵌入表示

（Embedding），并且嵌

入表示可以将高

维稀疏特征向量转换成

低维稠密特征向量来表

示。嵌入表示这个概念最

初在

NLP 领域被广泛应用，现

在已经扩展到了其他应

用，比如电商平台。电商平

台将用户的行为

序列视

作由一系列单词组成的

句子，比如用户点击序列

和购买商品序列，经过训

练后得到关

于商品的嵌

入向量。这里主要介绍经

典的

Word2Vec 以及网络表示学习

中的 DeepWalk 方

法。

词嵌入

Word2Vec

Word2Vec 在竞赛

中被经常使用，能够带来

意想不到的效果，掌握其

原理非常关键。

Word2Vec 根据上下

文之间的关系去训练词

向量，有两种训练模式，分

别为 SkipGram（跳字模型）和

CBOW（连续词

袋模型），两者的主要区别

为在于输入层和输

出层

的不同。简单来说，Skip-Gram 是用一

个词语作为输入，来预测

它的上下文；

CBOW 是用一个词

语的上下文作为输入，来

预测这个词语本身。

如图

7.9

所示，Word2Vec 实质上是只有一个

隐藏层的全连接神经网

络，用来预测与

给定单词

关联度大的单词。模型词

汇表的大小是 ，每个隐藏

层的维度是 ，相

邻神经元

之间的连接方式是全连

接。

图 7.9 中的输入层是把一

个词被转化成的独热向

量，即给定一个单词，然后

把此单词

转化为 序列，这

个序列中只有一个值为

1，其他均为

0；通过输入层和

隐藏层之间的权重矩阵

，将序列在隐藏层进行简

单的映

射；隐藏层和输出

层之间存在一个权重矩

阵 ，通过计算得出词汇表

中每个

单词的得分，最后

使用 softmax激活函数输出每个

单词的概率结果。接下来

看看

Skip-Gram 和

CBOW 的具体模型结构

。

图 7.9 简单版 Word2Vec

模型

如图 7.10 所示

，Skip-Gram 根据当前单词预测给定

的序列或上下文。假设输

入的目

标单词为

，定义的

上下文窗口大小为 ，对应

的上下文为

，这些 是相互

独立的。

图 7.10

Skip-Gram

如图 7.11 是 CBOW 的模型

结构，定义上下文窗口大

小为

，上下文单词为

，通过

上下文单词来对当前的

单词进行预测，对应的目

标单

词为 。

图 7.11

CBOW

Word2Vec 使用起来非

常方便，直接调用 gensim 包即可

，需要注意的是几个具体

参

数，比如窗口大小、模型

类型选择、生成词向量长

度等。

对于 Skip-Gram 和 CBOW 的选择，大致

可以基于以下三点：CBOW 在训

练时要比

Skip-Gram 快很多，因为 CBOW 是

基于上下文来预测这个

单词本身，只需把窗口内

的其他单词相加作为输

入进行预测即可，不管窗

口多大，都只需要一次计

算；相较

于 Skip-Gram，CBOW

可以更好地表

示常见单词；Skip-Gram 在少量的训

练集中也可

以表示稀有

单词或者短语。

图嵌入 DeepWalk

在

很多场景下，数据对象除

了存在序列关系，还存在

图或者网络的结构关系

，而

Word2Vec 仅能提取序列关系下

的嵌入表示，不能跨越原

本的序列关系获取图相

关的

信息。为了能从“一维

”关系跨越到“二维”关系，图

嵌入（Graph Embedding）成为了

新的的研究

方向，其中最具影响的是

DeepWalk。

如图

7.12 所示，DeepWalk 算法主要包括

三个部分，首先生成图网

络，比如通过用户

点击序

列或购买商品序列来构

造商品图网络，就是把序

列中前后有关联的商品

用线

连接起来并作为图

的边，商品则作为点，这样

某个商品点就可以关联

大量的领域商

品；然后基

于图网络进行随机游走

（random

walk）获取网络中节点与节点

的共现关

系，根据游走长

度生成商品序列；最后把

产生的序列当作句子输

入到 Skip-Gram 进

行词向量的训练

，得到商品的图嵌入表示

。

图

7.12 DeepWalk 流程

为了方便理解，下

面给出 DeepWalk 的代码描述：

def deepwalk_walk(walk_length, start_node):

walk = [start_node]

while len(walk) <= walk_length:

cur =

walk[-1]

try:

cur_nbrs = item_dict[cur]

walk.append(random.choice(cur_nbrs))

except:

break

return walk

def simulate_walks(nodes,

num_walks, walk_length):

walks = []

for

i in range(num_walks):

random.shuffle(nodes)

for v

in nodes:

walks.append(deepwalk_walk(walk_length=walk_length, start_node=v))

return walks

if __name__ == "__main__":

# 第一

步：生成图网络（省略）

# 构建

item_dict 保存图中的节点关系，即

字典结构存储，key 为节点，value 为

领域

#

第二步：通过DeepWalk 生成商

品序列

nodes = [k for

k in item_dict] # 节点集合

num_walks

= 5 # 随机游

走轮数

walk_length =

20 # 随机游走长度

sentences = simulate_walks(nodes,

num_walks, walk_length) # 序

列集合

# 第三步：通过Word2Vec

训练

商品词向量

model = Word2Vec(sentences, size=64, window=5,

min_count=3, seed=2020)

扩展学习

对

于 Word2Vec 的衍生

Item2Vec 以及更多图嵌

入方法，比如 LINE、Node2Vec 和

SDNE 都是很值

得研究的。从传统的

Word2Vec 到推

荐系统中的嵌入表示，再

到

如今逐渐向图嵌入过

渡，这些嵌入方式的应用

都非常广泛。

7.3.4 相似度计算

方法

基于相似度计算的

特征提取有欧式距离、余

弦相似度、Jaccard

相似度等，有助

于提取用

户、商品和文本

的相似度。当已经获取了

用户和商品的嵌入表示

、文本的分词表示及各类

稀

疏表示后，可以对些向

量表示进行相似度的计

算。基于相似度计算在用

户分层聚类、个性化

推荐

或广告投放等应用中一

直被广泛使用。

欧式距离

欧式距离是最易于理解

的一种距离计算方式，是

二维、三维或多维空间中

两点之间

的距离公式。在

维空间中，对于向量

，

，其公

式为式 (7-1)：

欧式距离的代码

实现如下：

def EuclideanDistance(dataA,

dataB):

# np.linalg.norm 用于范数计算

，默认是二范数，相当于平

方和开根号

return 1.0

/ ( 1.0 + np.linalg.norm(dataA -

dataB))

余弦相似度

首先，样本数据的夹角余

弦并不是真正几何意义

上的夹角余弦，实际上前

者只不过

是借用后者的

名字，变成了代数意义上

的“夹角余弦”，用于衡量样

本向量之间的差

异。夹角

越小，余弦值越接近于 1，反

之则趋近于–1。上面的向量

和向量 之

间的夹角余弦

见式 (7-2)：

余弦相似度的代码

实现如下：

def Cosine(dataA, dataB):

sumData = np.dot(dataA, dataB)

denom =

np.linalg.norm(dataA) * np.linalg.norm(dataB)

# 归一化到[0,1] 区间

return

( 1 - sumData / denom

) / 2

Jaccard 相似度

Jaccard

相似度一般用于

度量两个集合之间的差

异大小，其思想为两个集

合共有的元

素越多，二者

就越相似。为了控制其取

值范围，我们可以增加一

个分母，也就是两

个集合

拥有的所有元素。集合 、集

合 的 Jaccard

相似度公式见式 (7-3)：

Jaccard 相

似度的代码实现如下：

def Jaccard(dataA,

dataB):

A_len, B_len = len(dataA), len(dataB)

C = [i for i in

dataA if i in dataB]

C_len

= len(C)

return C_len / (

A_len + B_len - C_len)

扩

展学习

更多相似度计算

方法有皮尔逊相关系数

（Pearson correlation coefficient）、修正

余弦相似度（Adjusted Cosine Similarity）、汉明距

离（Hamming

distance）、曼

哈顿距离（Manhattan distance）、莱文斯坦

距离（Levenshtein distance）等。

7.4 用户画像的应用

用户画像之所以值得去

研究学习，是因为其拥有

广阔的应用场景，且互联

网时代的用户行为

能够

产生大量可供分析建模

的数据，这也为用户画像

提供了良好的条件。虽然

很多企业做用

户画像考

虑的侧重点不一样，但全

部抽象出来进行分析后

，主要可以分为如图 7.13 所示

的

几类。

图

7.13 用户画像的应

用场景

结合以上用户画

像的多个主要目的，本节

将从用户分析、精准营销

和风控领域三个主要层

面

对用户画像的应用进

行介绍。

7.4.1 用户分析

日活、月

活是时常能听到的两个

词语，通常用于形容每天

或者每月的在线活跃人

数，是互联

网产品的重要

评价指标。虽然每个产品

在设计之初都有或多或

少地定位目标用户群，但

上线

之后产品总还是会

面临各种各样的挑战，比

如用户的拉新、促活、留存

，新增用户有什么特

征，核

心用户的属性是否变化

等都是需要分析研究的

难题，因此需要不停地做

用户画像分

析，提炼人群

特征，继而不断优化产品

性能、UI 交互，对用户阶段的

划分如图

7.14 所示。

图 7.14 划分用

户阶段的分析

(1)

京东 JDATA 平台

2019 年的“用户对品类下店铺

的购买预测”竞赛赛题提

供来自用户、商

家、商品等

多方面的数据信息，包括

商家和商品自身内容的

信息、评论信息以及用户

与商家

丰富的互动行为

。需要选手通过数据挖掘

技术和机器学习算法构

建用户购买商家商品中

相关

品类的预测模型，输

出用户和店铺、商品品类

的匹配结果，为用户拉新

提供高质量的目标群

体

。本质上是货找人的匹配

工作，为潜在用户提供合

适的商品，提高用户类型

转化率，促进

平台 GMV 提升。

(2)

腾

讯广告“2020 腾讯广告算法大

赛”的赛题主要也是围绕

用户的行为信息而展开

，即把用

户在广告系统中

的交互行为作为输入来

预测用户的人口统计学

属性（比如年龄、性别、职业

等）。例如，对于缺乏用户信

息的实践者来说，基于其

自有系统的数据来推断

用户属性可以

帮助其在

更广的人群上实现智能

定向或者受众保护。虽然

用户填写的信息可能存

在造假，但

是用户的行为

习惯是很难造假的，充分

挖掘用户的交互行为有

助于验证用户的属性特

征。

7.4.2

精准营销

智能手机下

载量排前十的 App 中总少不

了电商的身影，网购的方

便快捷以及商品的琳琅

满

目极大增强了用户的

购物体验。利用用户的历

史消费行为进行用户画

像，能够展示用户的消

费

偏好，使得在用户需要添

置物品的时候电商平台

能够迅速、准确地响应用

户的需求，同时

商家也能

容易地找到自家的种子

用户。除了电商搜索外，推

荐系统和广告投放也属

于精准营

销的范畴。在机

器学习的各大竞赛平台

上，有关面向用户精准营

销的竞赛层出不穷，以下

对

用户画像在精准营销

方面的算法竞赛实例进

行简单介绍。

(1) 本书王贺、刘

鹏二位作者竞赛生涯的

首冠在 DC

竞赛平台举行的

“2018 科大讯飞 AI 营销

算法大赛

”取得。主办方是讯飞 AI

营销

云，隶属于科大讯飞股份

有限公司，基于深耕多年

的

人工智能技术和大数

据积累，赋予营销智慧创

新的大脑，以健全的产品

矩阵和全方位的服务

帮

助广告主用 AI 实现营销效

能的全面提升，打造数字

营销新生态。讯飞 AI

营销云

在高速

发展的同时，积累

了海量的广告数据和用

户数据，如何有效利用这

些数据去预测用户的广

告

点击概率，是大数据应

用在精准营销中的关键

问题，也是所有智能营销

平台必须具备的核心

技

术。这次大赛提供了讯飞

AI 营销云的海量广告投放

数据，参赛选手需要通过

人工智能技

术构建预测

模型来预估用户的广告

点击概率，即在给定与广

告点击相关的广告、媒体

、用

户、上下文内容等信息

的条件下预测广告点击

概率。虽然这场竞赛也属

于广告投放领域，但

是二

位作者通过给出的用户

标签集对用户画像深入

挖掘，准确地将用户与广

告联系在一起，

取得了不

错的成绩。

(2) 腾讯广告“2018 腾讯

广告算法大赛”是以

Look-alike（相似

人群扩展）为背景，基于种

子用

户，即广告主已有的

消费者，通过一定的算法

评估模型，找出和已有消

费者相似的潜在消费

者

，以此有效地帮助广告主

挖掘新客户、拓展业务。本

题目将为参赛选手提供

几百个种子人

群、海量候

选人群对应的用户特征

以及种子人群对应的广

告特征。参赛选手需要预

测种子包

候选用户是否

属于该种子包的得分，得

分越高说明候选用户是

某个包的潜在扩展用户

的可能

性越大。在挖掘相

似人群的过程中，Look-alike

主要依

据用户基本属性及其拥

有的行为信

息，这就需要

庞大的数据存量作为分

析源头。

7.4.3 风控领域

除了前

面两节讲到的应用之外

，用户画像还有一类特殊

的应用场景，就是金融领

域的风控问

题。这类场景

主要关注用户的经济状

况，结合征信等维度评估

用户的偿贷能力。在移动

支付

盛行的时代，电子流

水取代了现金流通，同时

使得人们的各种消费行

为能够被自动记录下

来

，这就产生了大量的交易

流水，为评估消费者的消

费能力和信用提供了支

撑。相关的竞赛

案例有如

下几个。

(1) DF 竞赛平台的“消费

者人群画像–信用智能评

分”。在社会信用标准体系

快速发展的大背

景下，作

为通信运营商的中国移

动拥有海量、广泛、高质量

、高时效的数据。打破传统

的信

用评分方式，如何基

于丰富的大数据对客户

进行智能评分是中国移

动和新大陆科技集团目

前

攻关的难题。中国移动

福建公司提供了 2018 年某月

的样本数据（已脱敏处理

），包括客户

的各类通信支

出、欠费情况、出行情况、消

费场所、社交、个人兴趣等

多维度数据，需要参

赛者

通过分析建模，运用机器

学习和深度学习算法，准

确评估用户消费信用分

值。

(2) 拍拍贷“第四届魔镜杯

大赛”。本次竞赛以互联网

金融信贷业务为背景，考

虑到在互联网

金融信贷

业务中单个资产标的金

额小且复杂多样，给出借

人或机构带来巨大的资

金管理压

力，参赛选手需

要利用提供的数据预测

资产组合在未来一段时

间内每日的回款金额。该

赛题

涵盖了信贷违约预

测、现金流预测等金融领

域的常见问题，同时又是

复杂的时序问题和多目

标预测问题。主办方提供

借款用户基础信息、用户

画像标签列表、用户借还

款行为日志、借

款的属性

表等信息。

上面两个竞赛

分别是关于信用评分和

现金流预测的典型风控

问题，风控领域的问题特

点非常

明显。第一，业务对

模型的解释性要求偏高

，同时对时效性有一定要

求，这要求参赛者在实

际

建模中要学会权衡模型

复杂度与精度，并且适当

地优化算法内核；第二，业

务模型多样，

每一个模型

都和业务目标有着非常

高的联系，经常需要根据

业务搭建合适的模型；第

三，负

样本占比极少，是均

衡学习算法的主战场之

一。

7.5 思考练习

01. 你觉得用户

画像是想体现用户的共

性还是个性，为什么？

02. 就你

日常使用的

App，思考其算法

与运营团队会如何给你

画像呢？

03. 文本挖掘算法也

是非常多的，尝试整理这

些算法调用方法，并且结

合原理去熟悉参

数的设

置。

04. 嵌入方式被广泛应用

，除了

Word2Vec 和 DeepWalk 以外，还有哪些嵌

入算法，具体

原理是什么

样的？

05.

相似度计算方法非

常多，但要从大量数据中

检索出最为相似的或者

相似度排前

位的并不是

件容易的事情，所以有什

么好的检索算法吗？

第 8 章

实战案例：Elo Merchant

Category

Recommendation

本章将以 Kaggle 平台

2019 年的

Elo Merchant Category Recommendation 竞赛（如图 8.1

所

示）为例

讲解与用户画像相关的

实战，端到端地讲解完整

的实战流程与注意事项

。本章主要

分为如下几个

部分：赛题理解、数据探索

、特征工程、模型选择、模型

融合、高效提分和赛

题总

结，这些是本书所有实战

案例章节共同的组织结

构，也是一场竞赛的重要

组成流程。相

信在本书的

指引下，读者能够快速地

熟悉竞赛流程，并进行实

战。

图

8.1 Elo Merchant Category Recommendation 竞赛

8.1 赛题理解

所谓

磨刀不误砍柴工，竞赛前

应对赛题相关信息进行

充分了解，理解其背后的

需求，进而达

到正确审题

的目的。

8.1.1 赛题背景

想象一

下，当你在一个不熟悉的

地方饿着肚子想要找好

吃的东西时，你是不是会

得到基于你

的个人喜好

而被专属推荐的餐馆，且

该推荐还附带着你的信

用卡提供商为你提供的

附近餐馆

的折扣信息。

目

前，巴西最大的支付品牌

之一 Elo 已经与商家建立了

合作关系，以便向顾客提

供促销或折

扣活动。但这

些促销活动对顾客和商

家都有益吗？顾客喜欢他

们的活动体验吗？商家能

够看

到重复交易吗？要回

答这些问题，个性化是关

键。

Elo 建立了机器学习模型

，以了解顾客生命周期中

从食品到购物等最重要

方面的偏好。但到

目前为

止，那些学习模型都不是

专门为个人或个人资料

量身定做的，这也就是这

场竞赛举办

的原因。

在这

场竞赛中，需要参赛者开

发算法，通过发现顾客忠

诚度的信号，识别并为个

人提供最相

关的机会。你

的意见将改善顾客的生

活，帮助 Elo 减少不必要的活

动，为顾客创造精准正确

的体验。

8.1.2 赛题数据

为了保

证隐私与信息安全，本次

竞赛的所有数据都是模

拟与虚构数据或经过脱

敏的数据，并

非真实的顾

客数据。具体包含下列数

据文件。

train.csv：训练集。

test.csv：测试集。

sample_submission.csv：正

确与规范的提交文件示

例，含有需要参赛者预测

的所有的

card_id。

historical_transactions.csv：信用卡（card_id）在给定

商家的历史交易记录，对

于每

张信用卡，最多包含

其三个月的交易记录。

merchants.csv：数

据集中所有商家（商家 id）的

附加信息。

new_merchant_transactions.csv：每张信用卡在

新商家的购物数据，最多

包含两个月的

数据。

Data_Dictionary.xlsx：数据

字典的说明文件，提供了

上述各表的字段含义，包

括对

train、historical_transactions、new_merchant_period 和 merchant

的相应说明。相信

参赛者跟笔者一样疑惑

这个 new_merchant_period

又是什么东

西，且继

续往下读。

8.1.3 赛题任务

通过

顾客的历史交易记录以

及顾客和商家的信息数

据进行模型训练，最终预

测测试集里面所

有信用

卡的忠诚度分数。

8.1.4 评价指

标

本次竞赛采用均方根

误差（RMSE）作为评价指标，用来

计算参赛者提交结果的

成绩，具体

计算方式如式

(8-1)：

其中 是参赛者对每个信

用卡预测的忠诚度分数

，而

是对应信用卡的真实

忠诚度分

数。

8.1.5 赛题 FAQ

Q

竞赛提

供了这么多数据文件，至

少需要哪些才能完成建

模？

A 至少需要 train.csv 和 test.csv，这两个文

件包含所有将会被用来

进行训练与测试的信用

卡

card_id。另外 historical_transactions.csv 和 new_merchant_transactions.csv 包含每张信用

卡

的交易记录。

Q 参赛者如

何能够将其余的数据利

用上呢？

A train.csv 和 test.csv

包含所有信用

卡的 card_id 和信用卡本身的信

息（比如卡激活的第

一个

月是何时等）。此外 train.csv 还包含

部分顾客的目标值，即提

供了这部分顾客确定的

忠

诚度分值。historical_transactions.csv 和 new_merchant_transactions.csv 设计为与

train.csv、

test.csv 和

merchants.csv 结合在一起，因为如上

所述，这两个文件包含每

张信用卡的交易记

录，所

以将交易记录与商家结

合在一起可以提供额外

的商家级别等信息。

8.2 数据

探索

相信很多参赛者与

笔者一样，即使读完

8.1 节的

所有内容依然感觉有些

迷惑。老话说得

好，“Talk is cheap，show me

the data”（能说不

算什么，有本事给我看数

据）。千言万语都

不如直接

理解数据来得实在，相信

很多问题通过观察和分

析数据就能解决。在数据

挖掘领域

有个专有名词

叫作探索性数据分析（EDA，本

书中称为“数据探索”），这不

仅能帮助参赛者

理解赛

题题目的真实含义，了解

数据概况，还能对接下来

的特征工程与建模思路

起到引导作

用，进一步加

强参赛者对业务的理解

和技术的应用，因此我们

首先需要做的便是对数

据集进

行探索。读到这里

也许有的读者已经开始

跃跃欲试打算开动“码力

”，在写代码之前我想建

议

，可以的话，借助 Excel 这一强大

的表格工具，先打开竞赛

提供的各种文件以获得

一个直

观的感受，本题目

就可以直接查看 train.csv、test.csv、sample_submission.csv

和

Data_Dictionary.xlsx。一般

来讲，超过 50 MB 的文件就不方

便直接用 Excel

打开了，因为容

易造成电脑卡顿。当然要

注意 Excel 本身的数据格式也

会对文件呈现带来影响

，比如科学计

数法、文本以

及日期等。

8.2.1 字段类别含义

在进行数据探索前，参赛

者首先应该明确对各数

据文件的介绍以及文件

中字段的含义，以便

理解

赛题和搭建分析逻辑。参

考赛题主办方提供的字

段信息表 Data_Dictionary.xlsx可知，五

个数据

文件中的字段及含义如

下。

train.csv与 test.csv中的字段及含义

card_id：独

一无二的信用卡标识，即

信用卡

id，例如

C_ID_92a2005557；

first_active_month：首次使用信

用卡购物的月份，格式为

YYYY-MM，

例如 2017-04；

feature_1/2/3：匿名的信用卡离散

特征

1/2/3，例如 3；

target：Loyalty numerical score calculated

2 months after historical and

evaluation

period，忠诚度分数目

标列，例如 0.392913。

通过查看上述

字段的含义可知，三个 feature 都

是匿名的信用卡离散字

段，还有一

个首次购物的

月份，而

target 是在历史和评估

时期后的两个月进行量

化计算得到

的忠诚度分

数。需要注意的是，这里的

evaluation period 应该是指

new_merchant_transactions.csv

中的信息，同时

也是对应 Data_Dictionary.xlsx 里面的

new_merchant_period 字段。同

时校验一下数据的正确

性就可以发现训练集与

测

试集的

card_id 均为唯一值，且

训练集与测试集中的 card_id 不

重复。

historical_transactions.csv和 new_merchant_transaction.csv中的字段及含义

card_id：独一无二的信用卡标识

，即信用卡

id，例如

C_ID_415bb3a509；

month_lag：距离参考

日期的月份，例如 [-12, -1]、[0, 2]；

purchase_date：购物日

期（时间），例如 2018-03-11 14:57:36；

category_3：匿名类别特

征 3，例如 A/B/C/D/E；

installments：购买商品的数量

，例如 1；

category_1：匿名类别特征 1，例如

Y/N；

merchant_category_id：商品种类 id（经过了匿名处

理），例如

307；

subsector_id：商品种类群 id（经过

了匿名处理），例如 19；

merchant_id：商品 id（经

过了匿名处理），例如

M_ID_b0c793002c；

purchase_amount：标准

化的购物金额，例如 -0.557574；

city_id：城市

id（经过了匿名处理），例如 300；

state_id：州

id（经过了匿名处理），例如

9；

category_2：匿

名类别特征 2，例如 1。

merchants.csv 中的字

段与含义

merchant_id：独一无二的商

品标识，即商品 id，例如

M_ID_b0c793002c；

merchant_group_id：商品

组（经过了匿名处理），例如

8353；

merchant_category_id：商品种类 id（经过了匿名处

理），例如

307；

subsector_id：商品种类群 id（经过

了匿名处理），例如 19；

numerical_1/2：匿名数

值特征 1/2，例如

-0.057471；

category_1：匿名类别特

征 1，例如 N/Y；

most_recent_sales_range：在最近活跃月份

的销售额等级，例如

A、B、C、D、E（等级

依次降低）；

most_recent_purchases_range：在最近活跃月

份的交易数量等级，例

如

A、B、C、D、E（等级依次降低）；

avg_sales_lag3/6/12：过去 3、6、12 个月

的月平均收入除以上一

个活

跃月份的收入，例如

-82.13；

avg_purchases_lag3/6/12：过去 3、6、12 个月的月平均交易

量除以上

一个活跃月份

的交易量，例如 9.6667；

active_months_lag3/6/12：过去

3、6、12 个月

内活跃月份的数量，例

如

3；

category_4：匿名类别特征 4，例如 Y/N。

8.2.2 字段

取值状况

在梳理完各个

表格文件的字段及含义

以后，参赛者可以具体查

看每个表格每个字段的

具体取

值状况，通常来说

除了字段含义以外，还要

结合字段含义判定字段

的取值类型，类型主要分

为字符（object）和数值（int、float）两种，需要

注意字段含义的离散与

否和字段取值是否

为数

值不存在必然的联系。因

为离散型字段的取值可

能是数值，比如 city_id

字段，虽然

它的取值都是数值类型

，但相互之间并没有大小

关系；数值型字段的取值

也可能是字符，比

如 most_recent_sales_range 字段

，它的取值虽然是字符类

型，但可以明显感觉到相

互

之间存在大小关系。

无

论字段是什么类型，参赛

者都需要主要关心两点

，一是缺失值状况，二是字

段大概的取值

范围与分

布。离散特征的关注点在

于特征值的数量分布，而

数值特征则需关注其取

值范围和

异常值、离群点

等。这里以本次赛题的目

标列为例，目标列为连续

值，可采用

pandas.series 的 describe 方法分析其

取值范围和区间。

有趣的

是，若同时采用分析离散

特征分布的 value_counts 方法，则参赛

者可以惊喜地发

现目标

列有一个极端异常值–33.219281，占

比约 1%，在后面的建模任务

中参赛者会逐渐体

会到

这一发现的重要性与特

殊性。

8.2.3 数据分布差异

机器

学习领域有三个特别的

数据集称谓，分别是训练

集、验证集与测试集，模型

从训练集中

学习特征与

标签之间的关联关系，同

时利用验证集进行评估

，以避免发生过拟合和欠

拟合，

等学习到合适的程

度后就可以把模型应用

到测试集上进行预测。要

使模型的预测效果优异

，

其中一个前提就是训练

集、验证集与测试集的数

据分布要相似，尤其是特

征与标签的联合分

布一

致，这样模型学习到的关

联关系才是可以进行泛

化的。验证集根据建模任

务的不同有多

种选取方

式，一般而言不涉及时间

先后顺序的建模任务就

可以把一份数据集随机

划分为训练

集和验证集

；测试集由于无从得知标

签也就是目标列，因此只

能通过一些特征的分布

或是联

合分布进行划分

。本赛题将以 train.csv 与

test.csv 为例来探

索分析数据分布的差异

。

如图 8.2 所示，对 train.csv

和 test.csv 中的

first_active_month、feature_1、feature_2 和 feature_3

几

个字段进行单变量分

布

展示，可以看出训练集与

测试集在所有单变量上

的绝对数量分布形状都

极其相似，需要进

一步查

看相对占比分布才能得

到更准确的结论。继续对

这 4 个字段进行单变量占

比分布展

示，如图

8.3 所示。

图

8.2 训练集和测试集中特征

分布差异

图 8.3

训练集和测

试集中特征密度分布

由

图 8.3 可以看出，训练集与测

试集在所有单变量上的

相对占比分布形状基本

一致，由此猜

想训练集与

测试集的生成方式一样

，可以继续验证联合分布

作为加强此猜想的事实

依据。

需要注意的是，上面

通过画图进行分析有一

个不严谨之处，即训练集

与测试集的单变量取值

范围可能不完全一样，因

此将两根线画在同一张

图上有可能会出错，比如

发生偏移等。有兴

趣的读

者可以自行验证二者的

横坐标是否完全一样。如

果不一样，运行同样的画

图代码会发

生什么？在下

面的联合分布验证中，我

们将会解决这一遗漏之

处。

在查看多变量的联合

分布时，通常来说可以使

用散点图，但这里的四个

字段均是离散特征，

而散

点图不太适合延续上面

画单变量图的思想，因此

参赛者可以把两个变量

拼接到一起将多

变量的

联合分布转变为单变量

的分布，结果如图 8.4

所示。

图

8.4 验证训练集和测试集分

布

修正遗漏后，参赛者可

以发现训练集与测试集

的两变量联合分布也基

本保持一致，由此基本

可

以判定训练集与测试集

的生成方式基本一模一

样，即训练集与测试集是

将同一批数据随机

划分

后的结果，有兴趣的参赛

者可以继续验证三变量

和四变量分布。假定关于

训练集与测试

集的这一

猜想成立，则会极大地增

添参赛者后续进行特征

工程的信心，对建模方式

也会有一

个整体把握。

8.2.4 表

格关联关系

由上述探索

可以看出，train.csv 和

test.csv 帮助参赛者

明确了训练集和测试集

以及建模目

标；historical_transactions.csv 和 new_transactions.csv 具有相

同的字段，只是二者时间

上有所

区别，给参赛者提

供了丰富的顾客交易信

息；merchants.csv 则描述了商家的经营

状况。参赛

者需要结合商

家基本信息表与顾客交

易记录表对用户的消费

行为进行数据挖掘，找到

尽可能

丰富的目标列相

关信息，从而达到优异的

预测效果。

8.2.5 数据预处理

为

了方便后续的特征提取

以及保持数据的整洁性

，在进行数据探索的同时

参赛者可以完成相

应的

数据清洗，这方面对于不

同数据，其处理技巧也是

多种多样，但目的都是为

后续的特征

提取扫清障

碍。这里只给出详细步骤

，具体代码请见本书附带

资源中的 eda.ipynb。

train.csv 和

test.csv

这两个表格

只有 test.csv 中的 first_active_month 字段有一个缺

失值，总体来说

只有一个

缺失值的影响不大，且这

个字段是字符型，因此需

要对其进行编码处理，

考

虑到其实质上具有先后

顺序关系，采用字典排序

进行编码即可。

merchants.csv

处理步骤

如下：

(1) 根据业务含义划分

离散字段

category_cols 与连续字段 numeric_cols；

(2) 对

字符型的离散字段进行

字典排序编码；

(3)

为了更方

便统计，对缺失值进行处

理，对离散字段统一用 –1 进

行填充；

(4) 探查离散字段发

现有正无穷值，这是特征

提取以及模型不能接受

的，因此需要对

无穷值进

行处理，此处采用最大值

进行替换；

(5) 对离散字段的

缺失值进行处理的方式

有很多种，这里先使用平

均值进行填充，后

续有需

要再进行优化处理；

(6) 去除

与交易记录表格重复的

列以及对 merchant_id

的重复记录。

new_merchant_transactions.csv和

historical_transactions.csv

处理步骤如下：

(1) 为了统一

处理，首先将这两张表格

拼接起来，后续可以通过

month_lag>=0 这个条

件进行区分；

(2) 划分

离散字段、连续字段以及

时间字段；

(3) 可仿照 merchants.csv

的处理

方式对字符型离散字段

进行字典排序编码以及

对缺失

值进行填充；

(4) 对时

间段进行处理，简单起见

，提取月份、星期几（工作日

与周末）以及时间段

（上午

、下午、晚上、凌晨）信息；

(5)

对新

生成的购买月份离散字

段进行字典排序编码；

(6) 处

理完商家信息和交易记

录的表格后，为了方便特

征的统一计算将这几个

表格合

并，然后重新划分

相应的字段种类。

8.3 特征工

程

经过基本的数据探索

之后，相信参赛者对数据

以及对赛题任务都有了

一个良好的理解，本赛

题

的重点便是挖掘用户的

各种交易行为与目标列

的关系，进而达到良好的

模型学习效果，使

模型能

够准确预测测试集用户

的忠诚度分数。因此这是

一个关注信用卡用户局

部消费偏好画

像的题目

，通过找到相似的训练集

用户来类推测试集用户

的忠诚度分数，进而对高

价值人群

进行区分，给商

家与信用卡银行提供决

策支持，同时也能够提升

消费者的购物体验，因此

特

征工程可集中于用户

的交易行为画像，即用户

在各个维度上购物行为

的量化，比如最近一个

月

的消费金额与购买数量

等。

在评估用户价值的画

像领域，有个经典的 RFM 理论

，即 Recent、Frequency（频次）和

Money（金钱）。结合前面

的数据探索，参赛者应该

能够明确这一理论的可

行性。这里将用

购买数量

模拟 Frequency，把消费金额作为 Money。本

赛题不仅在建模目标上

具有广泛性，

其数据结构

也具有典型的特点，即主

要利用用户的行为记录

表格

（historical_transactions.csv、merchants. csv、以及

new_merchant_transactions.csv）进行信息

挖掘

。接下来将分别介绍特征

提取的两种办法，一种是

借助 python 的原生字典结构进

行

通用特征的提取，另一

种则借助 pandas

这一强大的数

据处理工具的统计函数

进行业务特征

的提取。

8.3.1 通

用特征

字典的键值结构

很好地提供了便于使用

的映射关系，这里的特征

提取可以把用户作为第

一层

键值，把特征字段作

为第二层键值，统计完成

后再将字典转换成

pandas.DataFrame 格

式

；简单来说，就是想知道用

户在每个类别字段的每

个取值下的购买数量与

消费金额。

首先，创建一个

字典以存储生成的统计

特征，并给每个 card_id 赋值：

features = {}

card_all = train['card_id'].append(test['card_id']).values.tolist()

for card in card_all:

features[card] =

{}

其次

，记录好每个字段的索引

以便按行处理的时候直

接获取目标值：

columns = transaction.columns.tolist()

idx

= columns.index('card_id')

category_cols_index = [columns.index(col) for

col in category_cols]

numeric_cols_index = [columns.index(col)

for col in numeric_cols]

然后，按行

进行相应字段的特征提

取和更新：

#

记录运行时间

s = time.time()

num = 0

for i in range(transaction.shape[0]):

va =

transaction.loc[i].values

card = va[idx]

for cate_ind

in category_cols_index:

for num_ind in numeric_cols_index:

col_name = '&'.join([columns[cate_ind], va[cate_ind], columns[num_ind]])

features[card][col_name]

= features[card].get(col_name, 0) + va[num_ind]

num

+= 1

if num%1000000==0:

print(time.time()-s, "s")

del transaction

gc.collect()

最后，将字典转换成特征

DataFrame 表格结构，并且重置表格

的列名。

df

= pd.DataFrame(features).T.reset_index()

del features

cols =

df.columns.tolist()

df.columns = ['card_id'] + cols[1:]

在表格生成后，就

可以拼接训练集与测试

集，进行后续的模型训练

。为区别于后续特征，此

处

将特征集命名为 dict，完整代

码见代码资源中的 dict.ipynb。

8.3.2 业务

特征

基于字典结构的通

用特征提取，其优势在于

可以按行读取及处理，无

论速度还是内存都有一

定的保障，还可以面面俱

到地量化到每个子类下

的用户行为。但其缺点也

比较明显，即需要

固定的

数据结构，同时会产生较

高维度的结果。另一种方

案是使用 pandas 工具的 groupby

方法进

行统计，这种方式简单很

多，但对内存性能要求较

高，因为需要加载全部数

据。需要

注意的是，这里为

了符合 pandas 的统计需要，不再

对缺失值以及离散型字

段进行转化。

同时增加两

个特征，这两个特征与用

户两次购买行为之间的

时间间隔有关，分别从日

和月方

面进行刻画，代码

如下：

transaction['purchase_day_diff']

= transaction.groupby("card_id")['purchase_day'].diff()

transaction['purchase_month_diff'] = transaction.groupby("card_id")['purchase_month'].diff()

首先，根据字段的种

类设置相应想获取的统

计量，并给定相应的字段

列表，为后续的计算做

准

备，这种方式逻辑清晰，特

征构造更加全面：

aggs = {}

for col

in numeric_cols:

aggs[col] = ['nunique', 'mean',

'min', 'max','var','skew', 'sum']

for col in

categorical_cols:

aggs[col] = ['nunique']

aggs['card_id'] =

['size', 'count']

cols = ['card_id']

for

key in aggs.keys():

cols.extend([key+'_'+stat for stat

in aggs[key]])

然后，针

对 new_merchant_transactions.csv、historical_transactions.csv 以及全时间段分别进

行

计算和统计，获取多角

度下的统计特征：

df = transaction[transaction['month_lag']<0].groupby('card_id').agg(aggs).reset_index()

df.columns = cols[:1]

+ [co+'_hist' for co in cols[1:]]

df2 = transaction[transaction['month_lag']>=0].groupby('card_id').agg(aggs).reset_index()

df2.columns = cols[:1]

+ [co+'_new' for co in cols[1:]]

df = pd.merge(df, df2, how='left',on='card_id')

df2

= transaction.groupby('card_id').agg(aggs).reset_index()

df2.columns = cols

df

= pd.merge(df, df2, how='left',on='card_id')

可以看

出，利用 groupby

方法统计出的特

征数量会少很多，集中为

用户各种行为的统计

量

，为区别于后续特征，将此

处特征集命名为 groupby。

8.3.3 文本特

征

除去上述常规的特征

之外，本赛题还可以对一

类特征进行提取，就是基

于

CountVector 和

NLP 领域的 TF-IDF 向量特征，不

同于前面的

dict 和 groupby，这里只针

对部分离散字段进

行词

频统计。CountVector 与 dict

部分的特征比

较像，而 TF-IDF 则是对多变量联

合分布的补

充。

首先将相

应字段处理成标准的输

入格式，然后调用 sklearn

中的相

关方法进行计算，需要注

意这部分特征采用的是

scipy 的 sparse 稀疏矩阵结构，因此在

处理上与 dict 和

groupby 有

所不同。

8.3.4 特

征选择

常见的特征选择

方法主要分两种，一种是

过滤式选择，另一种是特

征重要性选择。前者利用

一些统计学上的相关性

系数进行过滤，后者通过

模型评估过程中的特征

重要性进行选择。一

般来

讲，特征选择的功能主要

出于提升模型训练速度

与精度两个方面的考虑

，在 8.4 节将会

针对不同的特

征选择方法进行模型训

练，并对比最终的线下、线

上结果。

8.4 模型训练

在准备

好基础特征后，参赛者就

可以开始尝试模型训练

与预测的全流程，为尽可

能多地给参

赛者介绍一

些处理技巧，本节将会介

绍三种模型（随机森林、LightGBM 和

XGBoost）的全

流程，同时组合不同

的特征选择与参数调优

方法。

8.4.1 随机森林

首先是 sklearn 库

里的随机森林模型，本模

型的全流程分为四个模

块：读取数据、特征选

取、参

数调优以及训练预测。模

型的要素组成为 8.3.4 节中的

dict

和 groupby 两部分，特征

选取方面

采用基于皮尔逊相关系

数计算的 Filter 方法取前

300 个特

征，参数调优方面使用

sklearn 库

的网格搜索（GridSearch）。

首先，读取已

经提前构造好的指定特

征集和测试集并且进行

数据集的拼接，具体代码

如下：

def

read_data(debug=True):

NROWS = 10000 if debug

else None

train_dict = pd.read_csv("preprocess/train_dict.csv", nrows=NROWS)

test_dict = pd.read_csv("preprocess/test_dict.csv", nrows=NROWS)

train_groupby =

pd.read_csv("preprocess/train_groupby.csv", nrows=NROWS)

test_groupby = pd.read_csv("preprocess/test_groupby.csv", nrows=NROWS)

# 去除重复列

for co in train_dict.columns:

if co in train_groupby.columns and co!='card_id':

del train_groupby[co]

for co in test_dict.columns:

if co in test_groupby.columns and co!='card_id':

del test_groupby[co]

train = pd.merge(train_dict, train_groupby,

how='left', on='card_id').fillna(0)

test = pd.merge(test_dict, test_groupby,

how='left', on='card_id').fillna(0)

return train, test

然后采

用基于皮尔逊相关系数

计算的

Filter 方法取前 300 个特征

进行选取，这里的 300 是

随意

取的一个数字，参赛者可

以多试几个数字以选出

效果最佳的，具体代码如

下：

def feature_select_pearson(train, test):

features =

[f for f in train.columns if

f not in ["card_id","target"]]

featureSelect =

features[:]

# 去掉缺失值比例超过

99% 的

for fea

in features:

if train[fea].isnull().sum() / train.shape[0]

>= 0.99:

featureSelect.remove(fea)

# 进行皮尔逊相关系数

计算

corr

= []

for fea in featureSelect:

corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))

se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)

feature_select = ['card_id'] + se[:300].index.tolist()

return

train[feature_select + ['target']], test[feature_select]

接着就是基于网格

搜索的参数调优。网格搜

索实际上是不同参数、不

同取值的排列集合，有

可

能需要根据调优结果多

次手动迭代参数空间，当

然每次迭代都是在上一

次最佳参数的基础

上增

加未搜索过的参数区域

，具体代码如下：

def param_grid_search(train):

features = [f

for f in train.columns if f

not in ["card_id","target"]]

parameter_space = {

"n_estimators": [80],

"min_samples_leaf": [30],

"min_samples_split": [2],

"max_depth": [9],

"max_features": ["auto", 80]

}

# 配置为mse 的

参数调优

clf = RandomForestRegressor(

criterion="mse",

min_weight_fraction_leaf=0.,

max_leaf_nodes=None,

min_impurity_decrease=0.,

min_impurity_split=None,

bootstrap=True,

oob_score=False,

n_jobs=4,

random_state=2020,

verbose=0,

warm_start=False)

grid

= GridSearchCV(clf, parameter_space, cv=2, scoring="neg_mean_squared_error")

grid.fit(train[features].values,

train['target'].values)

print("best_params_:")

print(grid.best_params_)

means = grid.cv_results_["mean_test_score"]

stds = grid.cv_results_["std_test_score"]

for mean, std,

params in zip(means, stds, grid.cv_results_["params"]):

print("%0.3f

(+/-%0.03f) for %r"% (mean, std *

2, params))

return grid.best_estimator_

最后根据参数

调优的最佳结果进行模

型训练与预测，这里选择

五折交叉验证，注意保存

训练

集的交叉预测结果

以及测试集的预测结果

，便于

8.5 节使用。

def train_predict(train, test, best_clf):

features = [f for f in

train.columns if f not in ["card_id","target"]]

prediction_test = 0

cv_score = []

prediction_train = pd.Series()

kf = KFold(n_splits=5,

random_state=2020, shuffle=True)

for train_part_index, eval_index in

kf.split(train[features], train['target']):

best_clf.fit(train[features].loc[train_part_index].values,

train['target'].loc[train_part_index].values)

prediction_test +=

best_clf.predict(test[features].values)

eval_pre = best_clf.predict(train[features].loc[eval_index].values)

score =

np.sqrt(mean_squared_error(train['target'].loc[eval_index].values,

eval_pre))

cv_score.append(score)

print(score)

prediction_train =

prediction_train.append(pd.Series(

best_clf.predict(train[features].loc[eval_index]),index=eval_index))

print(cv_score, sum(cv_score) / 5)

pd.Series(prediction_train.sort_index().values).

to_csv("preprocess/train_randomforest.csv", index=False)

pd.Series(prediction_test / 5).to_csv("preprocess/test_randomforest.csv",

index=False)

test['target'] = prediction_test / 5

test[['card_id', 'target']].to_csv("result/submission_randomforest.csv",

index=False)

return

这里最后一

步采用的是五折交叉验

证，一方面可以避免模型

对训练集的过拟合，另一

方面可

使模型对测试集

的预测结果更具健壮性

，还有一个顺带的好处是

可生成用于

Stacking 融合的

特征

，即训练集的交叉预测结

果和测试集的模型预测

结果，将这两者保留下来

为后续模型融

合做准备

，总共需要保存三个文件

：train_randomforest.csv、test_randomforest.csv 和

submission_randomforest.csv。

预测结果出来以后，提

交测试，得到具体分数，交

叉验证分数为 3.68710936，其中提交

得分

为 Public Score（公开榜，俗称 A

榜）是

3.75283（2867/4127），Private Score（隐藏榜，俗

称 B 榜）是 3.65493（2814/4127）。

8.4.2 LightGBM

采用和

随机森林模型相同的特

征集进行 LightGBM 建模，LightGBM 模型和随

机森林模型的

全流程四

个模块是相同的，读取数

据阶段完全一致，不同的

是这里在特征选取阶段

采用

wrapper 方法，参数调优阶段

选择 hyperopt 框架。

特征选取

这里

主要采用特征重要性选

取前 300 个特征进行建模训

练，同样可以根据建模效

果

改变这个数字，具体代

码如下：

def feature_select_wrapper(train,

test):

label = 'target'

features =

[f for f in train.columns if

f not in ["card_id","target"]]

# 配置模型的训练

参数

params_initial = {

'num_leaves': 31,

'learning_rate':

0.1,

'boosting': 'gbdt',

'min_child_samples': 20,

'bagging_seed':

2020,

'bagging_fraction': 0.7,

'bagging_freq': 1,

'feature_fraction':

0.7,

'max_depth': -1,

'metric': 'rmse',

'reg_alpha':

0,

'reg_lambda': 1,

'objective': 'regression'

}

ESR = 30

NBR = 10000

VBE = 50

kf = KFold(n_splits=5,

random_state=2020, shuffle=True)

fse = pd.Series(0, index=features)

for train_part_index, eval_index in kf.split(train[features], train[label]):

train_part = lgb.Dataset(train[features].loc[train_part_index],

train[label].loc[train_part_index])

eval =

lgb.Dataset(train[features].loc[eval_index],

train[label].loc[eval_index])

bst = lgb.train(params_initial, train_part,

num_boost_round=NBR,

valid_sets=[train_part, eval],

valid_names=['train', 'valid'],

early_stopping_rounds=ESR,

verbose_eval=VBE)

fse += pd.Series(bst.feature_importance(), features)

feature_select

= ['card_id'] + fse.sort_values(ascending=False).index.tolist()[:300]

print('done')

return

train[feature_select + ['target']], test[feature_select]

参数调优

Hyperopt

是一个 sklearn 的

Python 库，它在搜索空间上进行

串行和并行优化，搜索

空

间可以是实值、离散值和

条件维度，提供了传递参

数空间和评估函数的接

口，目

前支持的优化算法

有随机搜索（random

search）、模拟退火（simulated annealing）和

TPE（Tree of Parzen Estimators）算法。相较于网格搜索，hyperopt 往

往能够在相

对较短的时

间内获取更优的参数结

果。具体代码如下：

def params_append(params):

params['objective'] = 'regression'

params['metric'] = 'rmse'

params['bagging_seed'] = 2020

return params

def param_hyperopt(train):

label =

'target'

features = [f for f

in train.columns if f not in

["card_id","target"]]

train_data = lgb.Dataset(train[features], train[label], silent=True)

def hyperopt_objective(params):

params = params_append(params)

print(params)

res = lgb.cv(params, train_data, 1000, nfold=2,

stratified=False, shuffle=True,

metrics='rmse', early_stopping_rounds=20, verbose_eval=False,

show_stdv=False,

seed=2020)

return min(res['rmse-mean'])

# 设置参

数的空间范围

params_space

= {

'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1),

'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),

'feature_fraction': hp.uniform('feature_fraction',

0.5, 1),

'num_leaves': hp.choice('num_leaves', list(range(10, 300,

10))),

'reg_alpha': hp.randint('reg_alpha', 0, 10),

'reg_lambda':

hp.uniform('reg_lambda', 0, 10),

'bagging_freq': hp.randint('bagging_freq', 1,

10),

'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))

}

params_best = fmin(

hyperopt_objective,

space=params_space,

algo=tpe.suggest,

max_evals=30,

rstate=RandomState(2020))

return params_best

对于结果

的输出，网格搜索和

hyperopt 有着

很大的不同，前者输出的

是含参数的

最佳分类器

，后者输出的是最佳参数

字典。

训练预测

最后按照

同样的方式进行 LightGBM

模型的

训练与预测，同样使用五

折交叉验证的

方法获得

最终的训练集的预测结

果和测试集的预测结果

如下：

def train_predict(train, test, params):

label = 'target'

features = [f

for f in train.columns if f

not in ["card_id","target"]]

params = params_append(params)

kf = KFold(n_splits=5, random_state=2020, shuffle=True)

prediction_test

= 0

cv_score = []

prediction_train

= pd.Series()

ESR = 30

NBR

= 10000

VBE = 50

for

train_part_index, eval_index in kf.split(train[features], train[label]):

train_part

= lgb.Dataset(train[features].loc[train_part_index],

train[label].loc[train_part_index])

eval = lgb.Dataset(train[features].loc[eval_index],

train[label].loc[eval_index])

bst = lgb.train(params, train_part, num_boost_round=NBR,

valid_sets=[train_part, eval],

valid_names=['train', 'valid'],

early_stopping_rounds=ESR, verbose_eval=VBE)

prediction_test += bst.predict(test[features])

prediction_train = prediction_train.append(pd.Series(

bst.predict(train[features].loc[eval_index]), index=eval_index))

eval_pre = bst.predict(train[features].loc[eval_index])

score

= np.sqrt(mean_squared_error(train[label].loc[eval_index].values,

eval_pre))

cv_score.append(score)

print(cv_score, sum(cv_score)

/ 5)

pd.Series(prediction_train.sort_index().values).to_csv(

"preprocess/train_lightgbm.csv", index=False)

pd.Series(prediction_test

/ 5).to_csv("preprocess/test_lightgbm.csv", index=False)

test['target'] = prediction_test

/ 5

test[['card_id', 'target']].to_csv("result/submission_lightgbm.csv", index=False)

return

同样保存 train_lightgbm.csv、test_lightgbm.csv 和 submission_lightgbm.csv 三个文

件，

并记录下

CV 得分与线上

提交得分，交叉验证分数

为 3.6773062，Public Score 为

3.73817（2786/4127），Private

Score 为 3.64490（2719/4127）。得分相对于 8.4.1 节

有

了一定提升，接下来将会

进行更多的优化，以便在

分数上实现更多突破。

8.4.3 XGBoost

前

面两个模型都只用到了

dict 和 groupby 两组特征，本节的

XGBoost 模型

将尝试把 nlp 特

征也加入训

练，同时略过特征选取阶

段考虑使用特征全集进

行建模，参数调优框架换

成

beyesian（贝叶斯）。

首先依然是读

取数据，不同之处在于需

要把之前的特征集与 nlp 特

征合并成 sparse 稀疏矩

阵，具体

代码如下：

def read_data(debug=True):

print("read_data...")

NROWS = 10000

if debug else None

# 读取特征工程

阶段得到的dict

和groupby 两组特征

train_dict = pd.read_csv("preprocess/train_dict.csv", nrows=NROWS)

test_dict

= pd.read_csv("preprocess/test_dict.csv", nrows=NROWS)

train_groupby = pd.read_csv("preprocess/train_groupby.csv",

nrows=NROWS)

test_groupby = pd.read_csv("preprocess/test_groupby.csv", nrows=NROWS)

#

去除重复列

for co in train_dict.columns:

if

co in train_groupby.columns and co!='card_id':

del

train_groupby[co]

for co in test_dict.columns:

if

co in test_groupby.columns and co!='card_id':

del

test_groupby[co]

train = pd.merge(train_dict, train_groupby, how='left',

on='card_id').fillna(0)

test = pd.merge(test_dict, test_groupby, how='left',

on='card_id').fillna(0)

features = [f for f

in train.columns if f not in

["card_id","target"]]

# 读取特征工

程阶段得到的nlp 相关特征

train_x = sparse.load_npz("preprocess/train_nlp.npz")

test_x = sparse.load_npz("preprocess/test_nlp.npz")

train_x = sparse.hstack((train_x,

train[features])).tocsr()

test_x = sparse.hstack((test_x, test[features])).tocsr()

print("done")

return train_x, test_x

然后是参数调优阶段，不

同于 hyperopt，beyesian 调参通过最大化评

估分值进行优化，

而评估

指标均方根误差应该是

越小越好，因此采用负值

的均方根误差作为优化

目标。具体代

码如下：

def params_append(params):

params['objective'] =

'reg:squarederror'

params['eval_metric'] = 'rmse'

params["min_child_weight"] =

int(params["min_child_weight"])

params['max_depth'] = int(params['max_depth'])

return params

def param_beyesian(train):

train_y = pd.read_csv("data/train.csv")['target'].values

train_data

= xgb.DMatrix(train, train_y, silent=True)

def xgb_cv(colsample_bytree,

subsample, min_child_weight, max_depth,

reg_alpha, eta, reg_lambda):

params = {'objective': 'reg:squarederror',

'early_stopping_round': 50,

'eval_metric': 'rmse'}

params['colsample_bytree'] = max(min(colsample_bytree, 1),

0)

params['subsample'] = max(min(subsample, 1), 0)

params["min_child_weight"] = int(min_child_weight)

params['max_depth'] = int(max_depth)

params['eta'] = float(eta)

params['reg_alpha'] = max(reg_alpha,

0)

params['reg_lambda'] = max(reg_lambda, 0)

print(params)

cv_result = xgb.cv(params, train_data, num_boost_round=1000,

nfold=2,

seed=2, stratified=False, shuffle=True,

early_stopping_rounds=30, verbose_eval=False)

return

-min(cv_result['test-rmse-mean'])

xgb_bo = BayesianOptimization(

xgb_cv,

{'colsample_bytree':

(0.5, 1),

'subsample': (0.5, 1),

'min_child_weight':

(1, 30),

'max_depth': (5, 12),

'reg_alpha':

(0, 5),

'eta':(0.02, 0.2),

'reg_lambda': (0,

5)}

)

# init_points 表示

初始点，n_iter 表示迭代次数（即

采样数）

xgb_bo.maximize(init_points=21, n_iter=5)

print(xgb_bo.max['target'], xgb_bo.max['params'])

return xgb_bo.max['params']

最后，是 XGBoost 模型的训

练预测部分，按照同样的

方式进行五折交叉训练

，并同时保存

train_xgboost.csv、test_xgboost.csv 和 submission_xgboost.csv

三个文件

。具体代码如下：

def train_predict(train, test, params):

train_y

= pd.read_csv("data/train.csv")['target']

test_data = xgb.DMatrix(test)

params

= params_append(params)

kf = KFold(n_splits=5, random_state=2020,

shuffle=True)

prediction_test = 0

cv_score =

[]

prediction_train = pd.Series()

ESR =

30

NBR = 10000

VBE =

50

for train_part_index, eval_index in kf.split(train,

train_y):

train_part = xgb.DMatrix(train.tocsr()[train_part_index, :],

train_y.loc[train_part_index])

eval = xgb.DMatrix(train.tocsr()[eval_index, :], train_y.loc[eval_index])

bst

= xgb.train(params, train_part, NBR, [(train_part, 'train'),

(eval, 'eval')],

verbose_eval=VBE, maximize=False, early_stopping_rounds=ESR, )

prediction_test += bst.predict(test_data)

eval_pre = bst.predict(eval)

prediction_train = prediction_train.append(pd.Series(eval_pre, index=eval_index))

score =

np.sqrt(mean_squared_error(train_y.loc[eval_index].values, eval_pre))

cv_score.append(score)

print(cv_score, sum(cv_score) /

5)

pd.Series(prediction_train.sort_index().values).to_csv(

"preprocess/train_xgboost.csv", index=False)

pd.Series(prediction_test /

5).to_csv("preprocess/test_xgboost.csv", index=False)

test['target'] = prediction_test /

5

test[['card_id', 'target']].to_csv("result/submission_xgboost.csv", index=False)

return

8.5

模型融合

在完成特征工程与模型

训练后，参赛者可能会发

现自己的得分仍旧不尽

如人意。本章为了尽

可能

简而泛地介绍机器学习

竞赛的相关技巧，多使用

较通用或者比赛圈常说

的一把梭办法，

并没有针

对赛题制定极致细化的

方案，那并不是本书想要

达到的目的，由此可见单

模型的分

数显得并不高

，本节将分别尝试模型加

权融合与 Stacking融合以提升分

数。这里还有一点想

要告

诉参赛者，在机器学习竞

赛中团队与开源的力量

极其强大，一个人的思路

、时间和精力

往往都是有

限的，而不同人之间的建

模方法存在着极大的差

异，因此往往能够带来极

大的融

合收益。此外，大部

分竞赛尤其是 Kaggle 的竞赛，允

许参赛者自由讨论甚至

开源代码，这

同样是上分

的一个很好的资源，可以

融合参赛者自身的算法

与开源方案，从而获得更

好的分

数。

8.5.1 加权融合

本书

第 6 章已经将结果加权融

合的原理阐述清楚，可按

照分数与相关性分别给

8.4 节三种单

模型得到的结

果赋予权值然后提交，提

交后可得到具体分数：Public Score 为

3.73135（2741/4127）和 Private Score 为

3.63741（2646/4127）。

具体的权重计算方

式为 data['randomforest']*0.2 + data['lightgbm']*0.3 +

data ['xgboost']*0.5。

8.5.2 Stacking 融合

在训练前面的

三个模型时，顺带生成了

对应模型的

Stacking特征，也就是

训练集与测试集的

模型

预测结果，可以把这个结

果看作对特征集信息的

提炼压缩，结合开源的一

个较高分数的

方案（注意

此开源代码有几处 Bug，更改

后不影响使用）进行特征

拼接，集成建模。将前面

三

个模型中表现最好的模

型 XGBoost

对应的 Stacking特征加入训练

过程可以得到一个分数

尚可的模型，分数具体是

Public Score 为 3.68825（878/4127）和 Private

Score 为

3.60871（90/4127）。

由于篇幅有限，本

节只罗列了分数低的加

权融合结果以及分数高

的 Stacking融合结果，有兴

趣的读

者可以自己尝试并对比

在同等条件下加权融合

与

Stacking融合的结果。由于本次

赛题

为回归问题且存在

异常值，因此 Stacking融合是要优

于加权融合的。Stacking 融合代码

可以

参考后面 8.6.2

节的融合

技巧。

8.6 高效提分

到目前为

止得到的分数依然不是

很理想，我们更多的是要

学习一些重要的工具，如

特征选择

的方法、参数调

优的方式和一些核心的

树模型。在本节将对特征

部分和最终结果进行优

化处

理，以便在分数上有

更多的突破。

8.6.1 特征优化

在

最终方案中，主要围绕 new_merchant_transactions.csv 和

historical_transactions.csv 提取特

征。特征包含五部

分：基础统计特征、全局 card_id 特

征、最近两月的 card_id、二阶特

征

和补充特征。这些特征基

本包含了大部分选手设

计的特征工程中所包含

的特征，接下来详

细介绍

对每部分特征所做的工

作。

基础统计特征

本部分

特征主要以 card_id 为 key 进行聚合

（groupby）统计，分别从

new_transactions. csv、historical_transactions.csv（authorized_flag 为 1）和

historical_transactions.csv（authorized_flag 为

0）的数

据集提取此部分特征，具

体聚合方式与统计维度

如下面的代码所示：

def aggregate_transactions(df_, prefix):

df =

df_.copy()

df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30

df['month_diff'] = df['month_diff'].astype(int)

df['month_diff'] += df['month_lag']

df['price'] = df['purchase_amount'] / df['installments']

df['duration']

= df['purchase_amount'] * df['month_diff']

df['amount_month_ratio'] =

df['purchase_amount'] / df['month_diff']

df.loc[:, 'purchase_date'] =

pd.DatetimeIndex(df['purchase_date']).

astype(np.int64) * 1e-9

agg_func =

{

'category_1': ['mean'],

'category_2': ['mean'],

'category_3':

['mean'],

'installments': ['mean', 'max', 'min', 'std'],

'month_lag': ['nunique', 'mean', 'max', 'min', 'std'],

'month': ['nunique', 'mean', 'max', 'min', 'std'],

'hour': ['nunique', 'mean', 'max', 'min', 'std'],

'weekofyear': ['nunique', 'mean', 'max', 'min', 'std'],

'dayofweek': ['nunique', 'mean'],

'weekend': ['mean'],

'year':

['nunique'],

'card_id': ['size','count'],

'purchase_date': ['max', 'min'],

'price': ['mean','max','min','std'],

'duration': ['mean','min','max','std','skew'],

'amount_month_ratio':['mean','min','max','std','skew'],

}

for col in ['category_2','category_3']:

df[col+'_mean'] =

df.groupby([col])['purchase_amount'].transform('mean')

agg_func[col+'_mean'] = ['mean']

agg_df =

df.groupby(['card_id']).agg(agg_func)

agg_df.columns = [prefix + '_'.join(col).strip()

for col in agg_df.columns.values]

agg_df.reset_index(drop=False, inplace=True)

return agg_df

全局

card_id 特征

分别对 new_transactions.csv、historical_transactions.csv（authorized_flag

为 1）

和 historical_transactions.csv（authorized_flag 为 0）的数

据集提取此部分特征。

主

要包含与用户行为时间

相关的统计，比如最近一

次交易与首次交易的时

间差、信

用卡激活日与首

次交易的时间差；以 card_id 为 key 聚

合统计

authorized_flag

和 month_diff 的统计量（mean/sum）；以 card_id 为

key

聚合统计

state_i、city_id、installments、merchant_id、merchant_category_id

等的 nunique，并构造 card_id 频

次与上述得到的

nunique 的比值

特征，以此反

映用户 card_ id 的行

为纯度（分散范围）；以

card_id 为 key 聚

合统计

purchase_amount 相关变量的统计

量（mean/sum/std/median）；除此之外还

构造了一

些 Pivot 相关的特征。

最近两月

的 card_id

仅对

historical_transactions.csv 的数据集提取此

部分特征。此部分与全局

card_id 特

征有很多类似特征，主

要差别在于时间范围不

同，此处更加注重用户近

期的行为变

化情况。

二阶

特征

仅对 historical_transactions.csv 的数据集提取

此部分特征，前提是要先

构建一阶特征

（nunique、count、sum 等），具体提

取结构如下：

for

col_level1,col_level2 in tqdm_notebook(level12_nunique):

# 一阶提取nunique 特

征

level1 = df.groupby(['card_id',col_level1])[col_level2].

nunique().to_frame(col_level2 + '_nunique')

level1.reset_index(inplace =True)

# 以card_id 为key 构造nunique

的聚合统计

特征（二阶特征）

level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean',

'max', 'std'])

level2 = pd.DataFrame(level2)

level2.columns

= [col_level1 + '_' + col_level2

+ '_nunique_' + col for col

in

level2.columns.values]

level2.reset_index(inplace = True)

cardid_features

= cardid_features.merge(level2, on='card_id', how='left')

补充特征

此部分特征大多数具有

业务意义，比如为了更好

地发现异常值（即标签为

–

33.219281）而构建关于预测目标是

否为异常值的均值编码

特征，还有一些是关于

hist 和

new 系列特征的交叉统计特

征。

train['outliers'] = 0

train.loc[train['target'] < -30, 'outliers'] = 1

train['outliers'].value_counts()

for f in ['feature_1','feature_2','feature_3']:

colname

= f+'_outliers_mean'

order_label = train.groupby([f])['outliers'].mean()

for

df in [train, test]:

df[colname] =

df[f].map(order_label)

# hist 和new 系列特征的交叉统

计特征，下面是部分展示

df['card_id_total'] =

df['hist_card_id_size']+df['new_card_id_size']

df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']

df['card_id_cnt_ratio'] =

df['new_card_id_count']/df['hist_card_id_count']

8.6.2 融合技巧

单模结果

在经

过特征优化后，使用交叉

验证的方式进行线下验

证，并分别使用 LightGBM、

XGBoost 和 CatBoost 模型进

行训练及结果预测，得到

线下验证分数、Public Score 和

Private Score。这里将

LightGBM、XGBoost 和 CatBoost 模型封装到一个函数

中，代

码如下：

def train_model(X, X_test, y, params, folds,

model_type='lgb', eval_type='regression'):

oof = np.zeros(X.shape[0])

predictions

= np.zeros(X_test.shape[0])

scores = []

#

进行五折交

叉验证

for fold_n, (trn_idx, val_idx) in

enumerate(folds.split(X, y)):

print('Fold', fold_n, 'started at',

time.ctime())

# 根据model_type 确定所选择

的模型

if model_type

== 'lgb':

trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])

val_data = lgb.Dataset(X[val_idx], y[val_idx])

clf =

lgb.train(params, trn_data, num_boost_round=20000,

valid_sets=[trn_data, val_data],

verbose_eval=100,

early_stopping_rounds=300)

oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)

predictions

+= clf.predict(X_test, num_iteration=clf.best_iteration) /

folds.n_splits

if

model_type == 'xgb':

trn_data = xgb.DMatrix(X[trn_idx],

y[trn_idx])

val_data = xgb.DMatrix(X[val_idx], y[val_idx])

watchlist

= [(trn_data, 'train'), (val_data, 'valid_data')]

clf

= xgb.train(dtrain=trn_data, num_boost_round=20000,

evals=watchlist, early_stopping_rounds=200,

verbose_eval=100,

params=params)

oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]),

ntree_limit=clf.best_ntree_limit)

predictions

+= clf.predict(xgb.DMatrix(X_test),

ntree_limit=clf.best_ntree_limit) / folds.n_splits

#

对于CatBoost 模型，回归任

务和分类任务的代码有

很大不同，需要分开进行

if (model_type == 'cat') and

(eval_type == 'regression'):

clf = CatBoostRegressor(iterations=20000,

eval_metric='RMSE', **params)

clf.fit(X[trn_idx], y[trn_idx],

eval_set=(X[val_idx], y[val_idx]),

cat_features=[], use_best_model=True, verbose=100)

oof[val_idx] = clf.predict(X[val_idx])

predictions += clf.predict(X_test) / folds.n_splits

if

(model_type == 'cat') and (eval_type ==

'binary'):

clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)

clf.fit(X[trn_idx], y[trn_idx],

eval_set=(X[val_idx], y[val_idx]),

cat_features=[], use_best_model=True,

verbose=100)

oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]

predictions +=

clf.predict_proba(X_test)[:,1] / folds.n_splits

print(predictions)

if eval_type

== 'regression': # 进行回归评分

scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)

if eval_type == 'binary': # 进行分类

评分

scores.append(log_loss(y[val_idx], oof[val_idx]))

print('CV mean score: {0:.4f},

std: {1:.4f}.'.format(np.mean(scores),

np.std(scores)))

return oof, predictions,

scores

有了上面的函数，使

用 LightGBM、XGBoost 和 CatBoost 模型就会变得非常

方便。

其中输入参数 model_type 决定

选择哪个模型，eval_type 决定是二

分类任务还

是回归任务

。下面看看具体的使用代

码：

lgb_params

= {'num_leaves': 63, 'min_data_in_leaf': 32, 'objective':'regression',

'max_depth': -1,

'learning_rate': 0.01, "min_child_samples": 20,

"boosting": "gbdt",

"feature_fraction": 0.9, "bagging_freq": 1,

"bagging_fraction": 0.9,

"bagging_seed": 11, "metric": 'rmse',

"lambda_l1": 0.1, "verbosity": -1}

folds =

KFold(n_splits=5, shuffle=True, random_state=4096)

# 按特征列提取训练集

和测试集

X_train

= train[fea_cols].values

X_test = test[fea_cols].values

#

使用LightGBM 模型进行

回归预测

oof_lgb, predictions_lgb, scores_lgb =

train_model(X_train, X_test, y_train,

params=lgb_params, folds=folds, model_type='lgb',

eval_type='regression')

由此可以训练

XGBoost 模型和 CatBoost 模型，并得到预测

结果，oof 前缀代表验证

集预

测结果，predictions 前缀代表测试集

结果，这些结果将用于模

型融合。

加权融合

使用加

权融合作为基础融合方

式，如下式，给三个单模结

果分配相同权值：

Weighted_average =

(LightGBM + XGBoost + CatBoost) /3

Stacking 融合

这

里仅对三个模型（LightGBM、XGBoost、CatBoost）的结果

进行 Stacking融合，

当然这是远远

不够的，在实际竞赛中可

以构造更多具有差异性

的结果进行融合，不

限于

模型差异，存在特征差异

也是可以的。单模型以及

两种融合方式下的结果

对比

如图 8.5 所示。

图 8.5 多种方

案结果对比

可以看出，加

权融合的 Private Score 效果最佳，不过

Public Score 并没有得到改善。

相对稳

定的是 Stacking 融合，Public Score 和 Private

Score 均有很大

的提升。目前，加

权融合的

结果在隐藏榜最优能到

82 名，当然这个排名还可以

提高。

具体对 LightGBM

模型、XGBoost 模型、CatBoost 模

型的预测结果进行 Stacking融

合

的代码如下：

def

stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3,

y,

eval_type='regression'):

train_stack = np.vstack([oof_1, oof_2]).transpose()

test_stack = np.vstack([predictions_1, predictions_2]).transpose()

from sklearn.model_selection

import RepeatedKFold

folds = RepeatedKFold(n_splits=5, n_repeats=2,

random_state=2020)

oof = np.zeros(train_stack.shape[0])

predictions =

np.zeros(test_stack.shape[0])

for fold_, (trn_idx, val_idx) in

enumerate(folds.split(train_stack, y)):

print("fold n° {}".format(fold_+1))

trn_data,

trn_y = train_stack[trn_idx], y[trn_idx]

val_data, val_y

= train_stack[val_idx], y[val_idx]

print("-" * 10

+ "Stacking " + str(fold_) +

"-" * 10)

clf = BayesianRidge()

clf.fit(trn_data, trn_y)

oof[val_idx] = clf.predict(val_data)

predictions

+= clf.predict(test_stack) / (5 * 2)

if eval_type == 'regression':

print('mean: ',np.sqrt(mean_squared_error(y,

oof)))

if eval_type == 'binary':

print('mean:

',log_loss(y, oof))

return oof, predictions

其中输入参

数

oof_1、oof_2 和 oof_3 分别对应三个模型

的验证集预测结

果，predictions_1、predictions_2 和

predictions_3 分

别对应三个模型的

测试

集预测结果。

上述代码可

以同时针对回归任务的

融合和分类任务的融合

，具体需要对输入参数

eval_type 进

行设置。模型方面则使用

BayesianRidge

作为最终模型，因为模型

构

造非常简单，不容易过

拟合。

Trick 融合

先构建两个模

型，一个是预测某值是否

为异常值的分类模型（若

等于极端异常值 –

33.219281，则为 1，表

示是异常值；否则为 0，表示

不是），一个是去除异常值

后的

回归模型。当然也可

以先单模构建这两类模

型，然后使用 Stacking融合得到最

终结

果。

接下来基于以上

是否为异常值的分类模

型、全量数据的回归模型

和非异常值的回归

模型

进行最终的融合方案，并

且以上这三类模型均由

Stacking 融合得到。

方案一：分类模

型结果×（–33.219281）+（1–分类模型结果）×非

异常值的回归

模型结果

。

方案二：方案一的结果×0.5

+ 全

量数据的回归模型×0.5。

这两

种方案的分数均有很大

提升，其中方案一的 Public Score 为

3.67542（150/4127），Private Score 为

3.60636（59/4127）；方案二的 Public Score 为

3.67415（136/4127），Private Score 为 3.60414（35/4127）。

8.7 赛题总结

8.7.1

更多方案

Top1 方案

Top1 方案介绍

的是冠军队使用的是 Trick

融

合，这在本地 CV 中能够直接

提升 0.015

的分数。最初是在 Kaggle

的

Discussion 部分，有人发现建模的均

方根误差有一半以

上的

损失都是极端异常值 –33.219281 造

成的，因此可以考虑将含

有极端异常值的样

本去

除后训练一个回归模型

，同时再将数据是否为极

端异常值作为预测目标

构建一

个二分类模型，这

也是冠军最后采用的方

案，主要以如下公式融合

两类模型的预测

结果：

train['final'] = train['bin_predict'] *

(-33.21928) + (1 - train['bin_predict']) *

train['no_outlier']

其

中 train['bin_predict'] 为数据是极端异常值

–33.219281 的概

率，no_outlier

为去除异常值后

的回归模型的预测结果

。具体理解为利用二分类

结果是异常值的概率乘

以异常值，再加上二分类

结果不是异常值的概率

与模型在无

异常值下的

预测结果的乘积。

当然，还

有一种针对异常值比较

直接的处理方式，即在预

测完结果后，直接对其中

的最小值进行先改值后

处理，不过这种方式相对

来说有风险，且没有实际

的利用价

值，因此不推荐

采用，有兴趣的读者可以

尝试看看，了解一下效果

。

Top5 方案

第 5 名的团队介绍了

详细的方案，以及每类模

型和模型的组合方式。该

团队首先创

建了上千个

特征，在特征选择后剩余

100 多个。如图 8.6

所示为第 5 名团

队的整体方

案框架。

图 8.6

整

体方案框架

下面详细介

绍框架中涉及的内容。

回

归模型，在训练中使用全

量数据。

用于预测数据是

否为极端异常值的二分

类模型（测试集中没有重

复交易的银

行卡），将对该

模型的预测结果按阈值

0.015 进行划分，用以创建第二

层的

2 个回归模型（低概率

和高概率）。

低概率回归模

型——基于上一层二分类模

型的预测结果，按低于 0.015 阈

值

的预测结果进行回归

建模。由于这是比较稀疏

的一部分预测结果，所以

最终

融合的权值取 0.4。

高概

率回归模型——基于上一层

二分类模型的预测结果

，按高于 0.015 阈值

的预测结果

进行回归建模。该模型的

主要特征来自于上一层

二分类模型和回

归模型

预测得到的结果，再加上

一些其他特征。该模型有

助于避免进行后

理。

最终

提交，先将来自低概率回

归模型和高概率回归模

型的预测结果融合在一

起，再融合第 1 步中回归模

型的结果。

8.7.2

知识点梳理

特

征工程

本章用到的特征

主要有三类，即 RFM、groupby 以及 nlp

特征

，分别使用字

典、pandas.DataFrame 以及稀疏

矩阵进行提取。RFM 模型只模

拟利用了 F 与

M，对于近期相

关的信息有待读者进行

思考调研。在某种程度上

，第二种方案

groupby 包含了部分

R 信息，而 nlp 主要采用了

CountVector 和 TF-IDF。需

要注意的

是三者各自需

要的数据结构与类型有

所不同，所以导致前期预

处理有些许差别。在

特征

选择方面，尝试了基于皮

尔逊相关系数的 Filter

方法以

及基于模型特征重要

性

的 wrapper 方法。最后进入建模时

有采用 pandas.DataFrame 与

scipy.sparse 两种方式。

参数

调优

整体来讲，本赛题是

一个比较标准的数据挖

掘与机器学习建模问题

，训练集与测试

集的分布

高度耦合使得参赛者只

需要专注于刻画用户的

自身消费行为，然后通过

机

器学习算法进行训练

预测。本章的实战使用了

三种参数调优框架，分别

是网格搜

索、hyperopt 以及 beyesian，它们各

有特点，为了快速找寻到

较为不错的参数

组合，参

赛者可以只随机采样部

分数据进行调试。

8.7.3 延伸学

习

鉴于篇幅有限，本章没

有介绍 CatBoost 与 Word2Vec 两种算法，CatBoost 也是

一种决策树模

型，特别之

处在于它直接支持离散

字段与文本字段的建模

计算，从而省去了大量的

预处理时

间。而 Word2Vec 也是 NLP 领域

一个经典的算法，同样是

将文本处理成模型可以

理解的数值

向量。

除此之

外，本节接下来将推荐些

类似赛题作为延伸部分

的学习内容，以便加深对

此类赛题的

理解。

Santander Product Recommendation

该赛题

的主页如图 8.7 所示。

图 8.7 Santander

Product Recommendation

为了

满足一系列财务决策的

需求，Santander 银行通过个性化产

品推荐为客户提供贷

款

服务，根据他们过去的行

为以及类似客户的行为

，预测现有客户下个月将

使用哪

些产品。

赛题根据

西班牙 Santander 银行前 17 个月的商

品购买记录以及用户属

性去预测 2016

年 6 月每个用户

最可能购买的商品（要求

给出预测的 7 个商品，并按

可能性大小排

列，评测指

标用

MAP@7）。这是个很典型的推

荐竞赛，根据用户的历史

行为信息

构建模型，这些

行为信息是对用户的兴

趣习惯进行刻画性描述

的关键信息，也可以

用来

构建丰富的用户标签。

基

本思路：这类问题提取的

主要是历史特征，比如产

品滞后（lag）类特征、产品存

在

的时长、产品平均购买时

间差、上次购买产品后的

时间、上个月 20

种产品的价

值、上个月购买产品的数

量等，还有部分原始特征

。模型使用基本的 XGBoost 和

LightGBM（2016 年还

没有出现）就行，计算用户

对每种产品的新购买概

率，即

row_num

* n_classes，最终对概率进行排

序并提取每个用户排前

7 的产品即可。

这样可以得

到一个 baseline 的方案，接下来主

要从特征扩增、如何进行

建模（线下验

证方案）、模型

选择和后处理等方向进

行优化。

WSDM - KKBox's Music Recommendation

Challenge

该赛题的主页如

图 8.8 所示。

图 8.8

WSDM - KKBox's Music Recommendation Challenge

这是第十一届

ACM 网络搜索和数据挖掘国

际会议（WSDM 2018）挑战赛，采用

KKBOX 提供

的数据集构建更好的音

乐推荐系统。此赛题要求

选手预测当用户在一

个

时间窗口内第一个可观

察到的收听事件被触发

后，重复收听此歌曲的可

能性。如

果在用户的第一

个可观察的收听事件后

一个月内触发了重复的

收听事件，则将其目

标标

记为 1，否则标记为 0。这个规

则同样适用于测试集。

基

本思路：冠军团队介绍到

赛题任务与点击率（CVR）预测

类似，重新收听就像是

购

买，第一次收听就像是点

击。与“推荐”相比，“点击率预

测”是一个更准确的关键

字（因为

CTR 预测和 CVR 预测共享

相似的模型），因此像基于

潜在因子或基于邻

域的

协同过滤等方法并不是

解决点击率预测的最佳

方法。

模型使用常见的树

模型和

NN，这些均没问题，然

后将数据集最后 20% 的数据

用作

验证集。特征部分为

目标编码、计数特征、上次

听某歌曲的时间（类别于

msno、msno + genre_ids、msno

+ composer 等特征组合）、下次听到某

歌曲

的时间、最近一次听

到某歌曲与现在的时间

差，同时考虑不同粒度的

特征组合构

造。

第三部分

以史为鉴，未来可期

第 9 章

时间序列

第 10 章

实战案例

：全球城市计算 AI 挑战赛

第

11 章 实战案例：Corporaciòn

Favorita Grocery Sales Forecasting

第 9

章 时间序

列

关于时间序列分析有

一个很古老的故事，在 7000 年

前的古埃及，人们把尼罗

河的涨落情况

逐天记录

下来，从而构成一个时间

序列。对这个时间序列长

期观察后，人们发现尼罗

河的涨

落非常有规律。由

于掌握了涨落的规律，古

埃及的农业迅速发展。这

种从观测序列得到直观

规律的方法即为描述性

分析方法。在时间序列分

析方法的发展历程中，经

济、金融、工程等

领域的应

用始终起着重要的推动

作用，时间序列分析的每

一步发展都与应用密不

可分。

时间序列分析一直

备受关注，最为经典的赛

事“Makridakis Competitions”专注于时间序列预

测

问题，目前已经举办了五

届，第一次于 1982

年举办，第五

次于 2020 年举办，相隔近 40

年。这

项挑战旨在评估和比较

不同预测方法的准确度

，解决时间序列预测问题

。

在竞赛中，经常会遇到与

时间序列分析相关的任

务，比如指标在下一天

/ 周

/ 月将发生什

么，更具体点

，预测接下来两周的广告

曝光流量、预测用户购买

商品的时间和股票交易

时间

等。当然远不止这些

，我们可以通过总结归纳

，根据所需的预测质量、预

测周期的长度，使

用不同

的方法来处理这些预测

任务。

本章将分为四个部

分，分别为介绍时间序列

分析、时间序列模式、特征

提取方式和模型的多

样

性。

9.1 什么是时间序列

本节

将对时间序列分析进行

简单的介绍，分别为简单

定义、常见问题、交叉验证

和基本规则

方法，帮助大

家对时间序列分析有个

基本的认识和大致的解

题思路。

9.1.1 简单定义

我们首

先从时间序列分析的定

义开始，时间序列是按时

间顺序索引（或列出或图

示）的一系

列数据点。因此

，组成时间序列的数据由

相对确定的时间戳组成

，与随机样本数据相比，从

时间序列数据中能够提

取更多附加信息（如时间

趋势、变化信息等）。

与大多

数其他统计数据中讨论

的随机观测样本分析不

同，对时间序列的分析基

于以下假设：

数据文件中

标签的数据值表示以等

间隔时间进行的连续测

量值，如一小时内的流量

、一天内

的销量等；假设数

据存在相关性，然后通过

建模找到对应的相关性

，并利用它预测未来的数

据走向。

9.1.2 常见问题

与时间

序列预测相关的竞赛作

为最常出现的赛题之一

，可以细分出很多问题，通

过对以往竞

赛的总结，首

先可以从变量角度将这

些问题归纳为单变量时

间序列和多变量时间序

列，

其次可以从预测目标

角度将这些问题归纳为

单步预测和多步预测。

单

变量时间序列和多变量

时间序列

单变量时间序

列仅具有单个时间相关

变量，所以仅受时间因素

的影响。这类问题重

点在

于分析数据的变化特点

，受相关性、趋势性、周期性

和循环性等因素的影响

。

这类问题相对较少，一般

可以看作多变量时间序

列的一部分，即仅考虑时

间对标签

的影响。对其举

例见图 9.1。

图 9.1 单变量时间序

列举例

多变量时间序列

具有多个时间相关变量

，除了受时间因素的影响

，还受其他变量的

影响，比

如商品销量预测可能会

受到品类、品牌、促销情况

等一系列变量的影响。

这

类问题比较常见，需要考

虑更多的因素，所以挑战

也更大。对其举例见图

9.2。

图

9.2 多变量时间序列举例

单

步预测和多步预测

单步

预测问题比较基础，仅在

训练集的时间基础上添

加一个时间单位便可作

为测试

集，其实就是普通

的回归问题，只不过输入

变量不再是独立的特征

变量，而是随着

时间变化

会受历史数据影响的特

征变量。如图 9.3 所示，如果测

试集部分只有

时刻，那么

就是单步预测；如果是 至

时刻，则为多步预测。

图

9.3 数

据集划分

多步预测问题

比较复杂，是在训练集的

时间基础上添加多个时

间单位作为测试集。

这种

问题的解决方法也比较

多：第一种，以单步预测为

基础，每次将预测的值作

为

真实值加入到训练集

中对下一个时间单位进

行预测，这样会导致误差

累加，尤其是

如果刚开始

就存在很大的误差，那么

预测效果会变得越来越

差；第二种，直接预测

出所

有测试集结果，即看作多

输出的回归问题，这样虽

然避免了误差累加的问

题，

但是会增加模型学习

的难度，因为这需要模型

学习出一个多对多的系

统，从而加大

训练难度。

9.1.3 交

叉验证

在开始建立模型

之前，应该考虑如何进行

线下验证，为了结果的稳

定性，我们选择交叉验

证

，那么如何对时间序列进

行交叉验证呢？由于时间

序列中包含时间结构，因

此一般在保留

这种结构

的同时要注意不能在折

叠中出现数据穿越的情

况。如果进行随机化的交

叉验证，那

么标签值之间

的所有时间相关性都将

丢失，并且会导致数据穿

越。所幸，还是有合适的方

法

用于处理时间序列问

题的，这种方法叫作滚动

交叉验证，如图 9.4 所示。

图 9.4 滚

动交叉验证

这种交叉验

证方法相当简单，我们首

先用初始时间到 时刻的

数据来训练模型，然后用

从

到

时刻的数据进行线

下验证，并计算评价指标

的分数；接下来，将训练样

本扩展到

时刻，用从 到 时

刻的数据进行验证；不断

重复这个过程，直到达

到

最后一个可用的标签值

。验证次数可以自由控制

，最后对多次验证结果计

算平均值得到最

终的线

下验证结果。

9.1.4 基本规则方

法

在时间序列相关竞赛

中经常会用规则的方法

来解决问题，由于数据中

存在噪声，或由于发生

了

某些突发状况，导致模型

不能学习到所有信息。这

时规则方法可能会带来

帮助。这里我们

主要介绍

两种常见的规则预测方

法：加权平均和指数平滑

。

加权平均

加权平均就是

先获取数据中最近 个时

间单位的值，如果数据存

在强周期性（周

期为天、周

、月、季节等），也可以考虑环

比提取，即昨天、上周、上月

、上个季

节对应单位的值

；然后对提取出的子集进

行简单的加权计算，通常

离当前时间点越

近的数

据重要性越高。对于如何

选择 值，一般考虑短期的

历史数据，因为短期

内数

据的相关性更高。加权平

均的计算公式为式 (9-1)：

其中

为计算周期， 为每个时间

单位的权重， 为当前时间

单位的数值。

对于

值的选

择和权重的确定是比较

困难的事情，因为存在的

可能性太多，尤其

是权重

。所以我们可以通过线下

验证，进行简单的线性搜

索，来确定 值和权

重。如下

式 (9-2) 为线下验证进行最优

化搜索方式：

指数平滑

在

时间序列预测问题中，距

离预测单位越近的时间

点重要性越大。比如现在

有近 10

天的销量数据，那么

对预测第 11 天销量最有影

响的就是第

10 天的数据，另

外数据

离测试集越远，其

权重越接近 0。将每个时间

单位的权重按照指数级

进行衰减，并进

行最终的

加权计算，这种方式称为

指数平滑。其公式为式 (9-3)：

其

中 是第 个时间点上经过

指数平滑得到的值， 为第

个时间点上

的实际值。 为

可调节的超参数值，在

0 和

1 之间取值，也可称之为记

忆衰减因

子，通过式 (9-3) 可以

看出，

值越大，模型对历史

数据“遗忘”得就越快。

从某

种程度来说，指数平滑法

就像是拥有无限记忆（平

滑窗口足够大）且权值呈

指

数级递减的移动平均

法。一次指数平滑得到的

计算结果可以在数据集

及范围之外进

行扩展，因

此也就可以用来预测。

扩

展学习

思考一下上面介

绍的两种规则方法，会发

现预测的结果既不会高

于历史最高

值，也不会低

于历史最低值，这显然会

有不切实际的时候，比如

售车行业变得

不景气，乘

用车销量逐年降低，那么

今年的销量肯定要低于

去年。究其原因，

是没有考

虑到趋势性。因此，我们可

以使用二次指数平滑线

性趋势法来预测乘

用车

销量，同样还有三次指数

平滑，可以对同时含有趋

势性和季节性的时间序

列进行预测，是基于一次

指数平滑和二次指数平

滑的算法。

9.2 时间序列模式

解决时间序列问题首先

需要了解关键数据模式

，然后通过提取特征来表

现这些模式。这里主

要介

绍四类时间序列模式：趋

势线、周期性、相关性和随

机性。通过对这些模式的

理解，可

以找到特征提取

和模型选择的方向。

9.2.1 趋势

性

趋势性就是在很长一

段时间内呈现的数据持

续上升或持续下降的变

动，当然这不仅局限于线

性上升或者下降，也可以

是周期性的上升或者下

降。趋势性出现在多类时

间序列预测问题

中，比如

销量预测、各类流量预测

、金融相关的预测等。

那么

如何用特征来表现趋势

性呢，主要围绕数据的变

化进行特征构造，通常可

以从一阶趋势

和二阶趋

势进行构造。一阶趋势主

要为相邻时间单位的数

据差分、比例等，用来反映

相邻时

间单位数据变化

程度；二阶趋势则是在一

节趋势的基础上进一步

构造，可以反映一阶趋势

的

变化快慢。

如图 9.5

所示，展

示了 Google 和 Microsoft 从 2006

年到 2018 年的股价

，横坐标为时间

（date），纵坐标为

股价（share price）。从整体上看，两个公

司的股价都存在一定的

上升趋

势，只不过

Google 的更加

明显。

图 9.5 Google 和

Microsoft 的股价变化趋

势

9.2.2 周期性

周期性指的是

在一段时间序列内重复

出现的波动，是气候条件

、生产条件、节假日和人们

的

风俗习惯等各种因素

影响的结果。很多时间序

列预测问题都存在周期

性，比如温度变化预测

会

受季节性周期的影响，地

铁流量预测会受早、晚高

峰或工作日、周末的周期

影响。

周期性可以通过环

比表现，即用上月同时期

、上周同时期的数据值作

为特征，这是因为如果

存

在周期性，那么相同时期

的标签值可能更加相近

。既然存在周期性，那么时

间特征也是能

够表现周

期性的，比如构造当前时

间在所处周期内的位置

、当前时间距所处周期内

峰值的时

间差等特征。

如

图

9.6 所示，横坐标为时间（date），纵

坐标为观看广告流量（traffic）展

现的是真实的手

机游戏

数据，用来调查每小时观

看的广告流量和每天的

游戏内货币支出情况，可

以明显地看

出数据以一

天为周期反复循环。

图 9.6

广

告观看流量

9.2.3 相关性

时间

序列中的相关性又称自

相关性，描述的是在某一

段序列往往存在正相关

或负相关，前后

时间点会

有很大的关联，正因为有

了这种关联，才使未来成

为可预测的。

相关性的表

现可以通过临近时刻的

标签值来描述，比如将上

个时刻、上两个时刻等标

签值直

接作为特征，此外

还可以用历史多个时刻

的统计值作为特征，来反

映近期的变化。

9.2.4 随机性

随

机性描述的是除上述三

种模式之外的随机扰动

。由于时间序列的不确定

性，总会产生一些

意外或

者噪声，导致时间序列表

现出无规律的波动。比如

股市就是一个典型的具

有巨大随机

性的场景，存

在复杂的历史依赖性与

非线性的时间序列。随机

性很难预测，是带来误差

最大

的地方。如图 9.7 所示，是

Kaggle 的 M5 Forecasting

- Accuracy 竞赛中对某个商品的

需求量

进行可视化的结

果，可以明显看到有四处

异常点，相较于整体需求

量而言，这些点是难以预

测的。

图 9.7

商品的需求量变

化

针对随机性，可以通过

简单的异常标注来解决

，比如特殊日期、活动等。还

可以进行预处理

改变原

始数据，首先将这些因为

随机性产生的异常值剔

除，其次进行修正处理。

9.3 特

征提取方式

时间序列相

关的竞赛有不一样的特

征提取方式，主要围绕时

间序列的延迟（即历史信

息数

据）展开。可进一步将

此处的特征提取方式分

为历史平移、窗口统计两

种。除此之外还将介

绍序

列熵特征，以及额外补充

的其他特征。

9.3.1 历史平移

时

间序列数据存在着前后

关系，比如昨天的销量很

有可能影响今天的销量

，明天的天气温度

会受今

天的温度影响。换言之，时

间序列中越是相近的标

签，其相关性越高。我们可

以借助

这个特性构造历

史平移特征，即直接将历

史记录作为特征。

具体地

，如果当前时刻为 ，那么可

以将 、 、…、 时刻的值作为特

征

，这个值可以是标签值，也

可以是与标签值存在关

联性的值。比如，预测目标

为乘用车销

量，那么与乘

用车销量存在关联性的

乘用车产量和 GDP 都可以作

为特征。

如图 9.8

所示，对于单

位 时刻而言，直接将 时刻

的值作为特征，可以直接

使用

shift() 来完成平移操作，shift(1) 表

示向右平移

1 个单位，shift(-1) 表示

向左平移

1 个单位。这样一

来，第二行对应时刻的值

都将作为第一行对应时

刻的特征。

图

9.8 历史平移

9.3.2 窗

口统计

不同于历史平移

从单个序列单位中提取

特征，窗口统计是从多个

序列单位中提取特征。窗

口

统计可以反映区间内

序列数据的状况，比如窗

口内的最大值、最小值、均

值、中位数和方差

等。

窗口

大小并不是固定的，可以

进行各种尝试，如果是以

天为单位的时间序列，那

么选择 3

天、5 天、7 天、14

天作为窗

口大小进行统计是个不

错的决定。如图 9.9 所示，选择

3 作为

窗口大小，那么对于

单位 时刻而言，就是以

到

时刻作为窗口进行统计

。

图 9.9 窗口统计

9.3.3 序列熵特征

熵这个概念最早应用于

热力学中，用来衡量一个

系统能量的不可用程度

。熵越大，能量的不

可用程

度就越高，反之越低。它的

物理意义是对系统中混

乱程度或者复杂程度的

度量。同

样，在时间序列分

析中，熵可以用来描述序

列的确定性与不确定性

，如图 9.10 给出了两段

时间序

列的可视化展示，这两段

序列分别为（1，2，1，2，1，2，1，2，1，2，1）和（1，

1，2，1，2，2，2，2，1，1，1）。

图 9.10 时间序

列可视化

如果分别统计

两段序列的均值、方差、中

位数等结果，会发现都是

相等的，所以统计特征对

这两种时间序列并没有

区分性，很难挖掘序列的

稳定性。正因为如此，才引

入熵的概念来描

述序列

，计算公式如式 (9-4)：

9.3.4 其他特征

除了历史平移、窗口统计

和序列熵特征外，还有很

多常会用到的特征，我们

将之总结为两

类：时间特

征与统计特征。

时间特征

：比如小时、天、周、月，一天中

的某个时段（如上午、中午

），距离某日还有几

天，是否

为节假日等。

统计特征：基

本的统计特征有最大值

（max）、最小值（min）、均值（mean）、中位数

（median）、方差

（variance）、标准差（standard variance）、偏度（skewness）和峰度

（kuriosis）等，另

外还有一阶差分、二阶差

分、比例相关等特征。

9.4 模型

的多样性

时间序列预测

可以尝试的模型还是比

较多的，本节将介绍传统

的时序模型、树模型、深度

学

习模型。如果不出意外

，把这些方法在竞赛中都

尝试一遍，首先可以找到

好的单模，其次可

以为最

终融合做准备。

9.4.1 传统的时

序模型

ARIMA（autoregressive integrated

moving average model）是一种常见的时

序模型，被称作差

分自回

归滑动平均模型。同时 ARIMA 由

AR、MA

和 I 三部分组成，包含三个

关键参数

p、q 和 d。其中

AR 是自回

归模型，p 是自回归项数，表

示模型中包含的滞后观

测次数；

MA 是移动平均模型

，q 是移动平均窗口的大小

，d

是成为平稳序列所进行

的差分次数；

I（代表“积分”）表

示数据值已被当前值和

先前值之间的差值替换

（并且该微分过程可能已

经执行了不止一次）。

之前

提到的三次指数平滑是

基于数据趋势性和季节

性的描述，而 ARIMA 模型主要描

述的是

数据之间的相互

关系。ARIMA

中三部分的目的都

是使模型更加容易拟合

历史数据，能够在

较大时

间序列范围内获得较高

的准确性。如下代码简单

实现了 ARIMA 模型：

from statsmodels.tsa.arima_model

import ARIMA

model = ARIMA(train, order=(p,d,q)

)

arima = model.fit()

pred =

arima.predict(start= len(train), end= len(train)+L)

可以看出 ARIMA

模

型的使用非常方便，代码

分为三个部分；创建模型

、训练模型和进行预

测。除

了准备好一维序列标签

值外，还需要确定参数 p、d 和

q。另外预测部分的 L 表示预

测的长度单位。

参数确定

p、d 和 q 均为非负整数，对于 p 和

q

的选择可以根据 ACF（自相关

系数）图和 PACF（偏

自相关系数

）图进行判断。首先进行 d 阶

差分，将时间序列数据转

换为平稳时间序列，然后

分别求得平稳时间序列

的

ACF 图和 PACF 图，通过分析这两

个图，得到最佳的阶层 p 和

阶

数 q。

扩展学习

SARIMA（季节性差

分自回归滑动平均模型

）是对 ARIMA 的扩展，该模型可使

用包含

趋势性和季节性

的单变量数据进行时间

序列预测。

9.4.2 树模型

树模型

（XGBoost、LightGBM 等）是比较通用的模型，即

使在很多与时间序列相

关的竞赛

中，也能够展现

强大威力。当趋势性和季

节性相对稳定，并且噪声

较小的时候，树模型是非

常适用的。当然，可以通过

一些处理来降低时间序

列的趋势性，使之变得平

稳。在第

10 章

会使用树模型

作为 baseline 方案完成基本的预

测。

扩展学习

将时间序列

转换为平稳序列的方法

有对数处理、一阶差分、季

节性差分等。一般而言，

先

进行平稳性调整，然后再

训练，最后结果做转换即

可。这三种方法可以单独

使用，也

可以组合使用。

9.4.3 深

度学习模型

深度学习模

型可以给时间序列预测

更多的可能性，例如对时

间依赖性进行自动学习

以及对趋

势性和季节性

等时间结构进行自动处

理。深度学习模型还可以

处理大量数据、多个复杂

变量

和多步操作，并且从

输入数据中提取序列模

型，这些都能给时间序列

预测提供很大帮助。这

里

主要介绍如何使用卷积

神经网络和长短期记忆

网络来解决时间序列预

测问题，并给出具体

的实

现代码。同时，本节还会介

绍更多深度学习在时间

序列预测问题中的尝试

和应用。

卷积神经网络

卷

积神经网络（CNN）是一种旨在

有效处理图像数据的神

经网络，它能够自动从原

始输入数据中提取特征

，这项能力可以应用于时

间序列预测问题，即把一

系列观察

结果视为一个

一维图像，然后读取该图

像并将其提取为最显著

的元素。卷积神经网

络还

支持多变量输入和多变

量输出，并且能够学习复

杂的函数关系，但不需要

模型

直接从滞后观察中

学习。相反，模型可以从与

预测问题最相关的输入

序列中学习特

征表示形

式，即从原始输入数据中

自动识别、提取和精炼显

著的特征，这些特征与

要

建模的预测问题直接相

关。

如图

9.11 所示，是基于一维

卷积神经网络解决时间

序列预测问题的结构图

，首先初

始化一个 的矩阵

，其中 表示时间片的长度

， 表示特征个数；然后第一

层为卷积隐藏层，定义多

个过滤器用来提取特征

；接着使用最大池化层，以

减少输

出数据的复杂度

和防止数据过拟合；最后

通过一个全连接层得到

输出结果。

图 9.11 基于一维卷

积神经网络的时间序列

预测结构图

下面是用基

于 keras

版的卷积神经网络解

决时间序列预测问题的

参考代码：

import numpy as np

import

pandas as pd

from sklearn.model_selection import

train_test_split

from keras import optimizers

from

keras.models import Sequential, Model

from keras.layers.convolutional

import Conv1D, MaxPooling1D

from keras.layers import

Dense, LSTM, RepeatVector, TimeDistributed, Flatten

#

数据准备，按时

间前后划分训练集和验

证集

# data 表示数据集，features 表示特

征集，label 表示标签

X_train, X_valid, y_train, y_valid = train_test_split(data[features],

data[label],

test_size=0.2, random_state=2020, shuffle=True)

# 输入数据

的格式为[

样本，时间步，特

征]

X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))

X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))

#

网络设计

# 使用一个卷

积隐藏层和一个最大池

化层

# 然后，在被全连接层

解释和输出预测结果之

前，使滤波器映射平稳化

model_cnn =

Sequential()

model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu',

input_shape=(X_train.shape[1], X_train.shape[2])))

model_cnn.add(MaxPooling1D(pool_size=2))

model_cnn.add(Flatten())

model_cnn.add(Dense(50, activation='relu'))

model_cnn.add(Dense(1))

model_cnn.compile(loss='mean_squared_error',

optimizer='adam')

# 拟合网络

model_cnn.fit(X_train, y_train, validation_data=(X_valid,

y_valid), epochs=20, verbose=1)

长短期记忆网

络

长短期记忆网络（LSTM）是一

种特殊的循环神经网络

（RNN），更适用于较长的

时间序

列，由一组具有记忆数据

序列特征的单元格组成

，这些单元格具有存储数

据

序列的功能。长短期记

忆网络适合处理时序数

据，可以捕获当前观测值

与历史观测

值之间的依

赖关系。

如图 9.12 所示，是长短

期记忆网络的结构示意

图，其中每个节点的输出

将会作为其

他节点的输

入，箭头表示信号传递（数

据传递），蓝色圆圈表示

pointwise

operation（即

逐点运算），比如节点求和

，绿色框表示用于学习的

network layer（神

经网络层），合并的两条

线表示连接，分开的两条

线表示信息被复制成两

个副本，

并传递到不同的

位置。具体地，输入部分由

（ 时刻的卷积隐藏层）和

（ 时

刻的特征向量）组成；输出

部分为 ；主线部分由 和 组

成。

图

9.12 长短期记忆网络结

构示意图

下面是用基于

keras 版的长短期记忆网络解

决时间序列预测问题的

参考代码：

# 网络设计

model_lstm = Sequential()

model_lstm.add(LSTM(50, activation='relu',

input_shape=(X_train.shape[1],

X_train.shape[2])))

model_lstm.add(Dense(1))

model_lstm.compile(loss='mean_squared_error', optimizer='adam')

# 拟合

网络

model_lstm.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, verbose=1)

9.5 思考练习

01. 在进行时

间序列预测时，什么情况

下规则方法的效果要好

于模型效果？

02. 如何将规则

方法与模型结合得更好

？

03. 相关性特征提取主要是

提取最近的标签直接作

为特征，那么如何来确定

提取的时间

区间呢？

04. 为什

么长短期记忆网络比循

环神经网络更能解决长

时间序列依赖问题？

第

10 章

实战案例：全球城市计算

AI 挑战赛

本章将把 2019 年天池

竞赛中的一道赛题（即“全

球城市计算

AI 挑战赛：地铁

乘客流量预

测”，如图 10.1 所示

）作为时间序列分析相关

问题的实战案例，主要包

括赛题理解、数据探

索、特

征工程和模型训练。特别

地，在实战部分，我们除了

提供一个通用的解题思

路之外，

更重要的是引导

大家去学习在不同赛题

类型中的思考过程，最终

进行知识点的梳理和进

一步

延伸，即赛题总结。

图

10.1 全球城市计算 AI 挑战赛：地

铁乘客流量预测

10.1 赛题理

解

本节旨在让读者快速

了解这次实战案例的基

本内容，除了常见的背景

介绍、赛题数据和评价

指

标外，还包含两个独特的

部分：赛题 FAQ 和

baseline 方案。在初次

面对一类问题时，提

出问

题和假设有助于发现问

题的核心内容和难点。baseline 方

案则能让我们快速获取

线下、

线上反馈，然后进行

不断尝试和优化。

10.1.1

背景介

绍

2019 年，杭州市公安局联合

阿里云智能启动首届全

球城市计算 AI 挑战赛，本次

挑战赛的题

目最终选定

为“地铁乘客流量预测”。目

前，地铁是城市出行的主

要交通工具之一，地铁站

突

发乘客流量的增加极

易引起拥塞，引发大客流

对冲，造成安全隐患。因此

，地铁运营部门和

公安机

关亟需借助流量预测技

术提前部署相应的安保

策略，保障人民安全出行

。

竞赛以“地铁乘客流量预

测”为赛题，参赛者需通过

分析地铁站的历史刷卡

数据来预测站点未

来的

乘客流量变化，预测结果

能够帮助乘客选择更合

理的出行路线，从而规避

交通堵塞。这

可以帮助地

铁运营部门和公安机关

提前部署站点安保措施

等，最终实现用大数据和

人工智能

等技术助力未

来城市安全出行。

10.1.2 赛题数

据

大赛开放了从 2019 年 1

月 1 日

至 2019 年 1

月 25 日共 25 天的地铁刷

卡数据，共涉及 3

条线

路、81 个

地铁站、约 7000 万条数据。这些

数据作为训练数据（Metro_train.zip），供选

手

搭建地铁站点乘客流

量预测模型。将训练数据

解压后，可以得到

25 个 csv 文件

，这些 csv

文件分别存储着每

天的刷卡数据，文件名以

record

为前缀。比如 2019 年 1 月 1

日的所

有线

路所有站点的刷卡

数据存储在 record_2019-01-01.csv 文件中，以此

类推。大赛还提供了路网

地图，即各地铁站之间的

连接关系表，它们存储在

Metro_roadMap.csv 文件中供选手使用。

在测

试阶段，大赛将提供某天

所有线路所有站点的刷

卡数据记录，选手需预测

未来一天

00

时至 24 时（以 10 分钟

为单位）各时段各站点的

进站和出站人次。

在预选

赛阶段，大赛将提供 2019 年 1 月

28 日的刷卡数据记录作为

测试集

A（testA_record_2019-01-28.csv），选手需预测 2019 年 1 月

29 日全天各地铁站（以

10 分

钟

为单位）的乘客流量。在淘

汰赛和决赛阶段，又将分

别更新一批数据作为测

试集 B 和测

试集

C。

用户刷卡

数据表（record_2019-01-xx.csv）

在 record_2019-01-xx.csv 文件中，除第一

行外，其余各行分别包含

一条用户

的刷卡记录。

对

于 userID，在 payType 为 3 时无法唯一标识

用户身份，即此

userID 可

能被多

人使用，但在一次进出站

期间可以视为同一用户

。对于其他取值的

payType，对应的

userID 可以唯一标识一个用户

。

路网地图（Metro_roadMap.csv）

大赛提供了各

地铁站之间的连接关系

表，相应的邻接矩阵存储

在 roadMap.csv 中，

其中包含一个 81×81 的矩

阵roadMap。在

roadMap.csv 文件中，首行和首列

均表示

地铁站 ID（stationID），列取值为

0~80，行取值为 0~80。其中，roadMap[i]

[j]

= 1 表示 stationID 为 i

的

地铁站 和 stationID 为 j 的地铁站直

接相

连，roadMap[i][j] = 0 表示 stationID 为

i 的地铁站

和 stationID 为 j 的

地铁站不相连。

10.1.3 评

价指标

评价指标用以评

判选手的预测是否准确

，此处先采用平均绝对误

差分别对进站人数和出

站人

数的预测结果进行

评价，再对两者取平均得

到最终评分。

10.1.4

赛题 FAQ

Q 本次竞

赛的标签需要自己构建

，如何建模能使我们在给

定的数据集上达到尽可

能

大的预测准确性？

A

构建

进出地铁站的流量标签

是本次竞赛的首要任务

。在初次观察数据后，会发

现存在或

多或少的问题

，比如出现了凌晨进出地

铁站流量不为 0 的记录，不

同地铁站的数据存在一

定

的差异性。为了保证结

果的稳定性，或许可以尝

试对进出地铁站的流量

按地铁站进行处理

（归一

化、标准化等）。

Q 地铁站的流

量存在太多影响因素，比

如多辆地铁同时到站、突

发情况、盛大活动

等，所以

该如何处理异常值，保证

模型稳定呢？

A 特殊因素带

来的影响非常多，针对这

类问题，常见的方法有异

常去除、异常标记和异常

平滑。我们可以对这些方

法进行一一尝试，并对比

优劣。

10.1.5

baseline 方案

了解了前几节

的内容，我们就可以开始

基本的建模了，baseline 方案不需

要太复杂，只要能

给出一

个正确的结果即可。这其

实也可以看作先建立一

个简单的框架，然后在后

面的环节中

不断填充和

优化。这里以

1 月 29 日的刷卡

数据作为测试集，1 月 1

日至

1 月 25 日以及 1 月

28 日的刷卡数

据作为训练集进行建模

，1 月 26 日和 1

月 27 日（周末）两天的

数据官方并未

提供。

数据

准备

下面为数据读取和

时间单位转换的具体代

码：

import numpy as np

import pandas

as pd

from tqdm import tqdm

Import lightgbm as lgb

# 读取数据

path ='./input/'

for i in tqdm(range(1,26)):

if i < 10:

train_tmp =

pd.read_csv(path + 'Metro_train/record_2019-01-0' + str(i) +

'.csv')

else:

train_tmp = pd.read_csv(path +

'Metro_train/record_2019-01-' + str(i) + '.csv')

if

i == 1:

data = train_tmp

else:

data = pd.concat([data, train_tmp],axis=0,ignore_index=True)

Metro_roadMap

= pd.read_csv(path + 'Metro_roadMap.csv')

test_A_record =

pd.read_csv(path + 'Metro_testA/testA_record_2019-01-28.csv')

test_A_submit = pd.read_csv(path

+ 'Metro_testA/testA_submit_2019-01-29.csv')

data = pd.concat([data, test_A_record],axis=0,ignore_index=True)

# 将数据转化

为以10 分钟为单位

def trans_time_10_minutes(x):

x_split

= x.split(':')

x_part1 = x_split[0]

x_part2

= int(x_split[1]) // 10

if x_part2

== 0:

x_part2 = '00'

else:

x_part2 = str(x_part2 * 10)

return

x_part1 + ':' + x_part2 +

':00'

data['time'] = pd.to_datetime(data['time'])

data['time_10_minutes'] =

data['time'].astype(str).apply(lambda x: trans_time_10_minutes(x))

接下来

，构造进站流量（inNums）和出站流

量（outNums），直接进行聚合即可：

data_inNums =

data[data.status == 1].groupby(['stationID','time_10_minutes']).

size().to_frame('inNums').reset_index()

data_outNums =

data[data.status == 0].groupby(['stationID','time_10_minutes']).

size().to_frame('outNums').reset_index()

核

心部分——构建训练集

训练

集构造起来还是很麻烦

的，仔细观察数据可以发

现，如果某站在某段时间

段没

有客流量，那么这个

时间段的数据就是缺失

的，需要选手进行填充。构

造过程如

下：

stationIDs = test_A_submit['stationID'].unique()

times

= []

days = [i for

i in range(1,26)] + [28, 29]

for day in days:

if day

< 10:

day_str = '0' +

str(day)

else:

day_str = str(day)

for

hour in range(24):

if hour <

10:

hour_str = '0' + str(hour)

else:

hour_str = str(hour)

for minutes

in range(6):

if minutes == 0:

minutes_str = '0' + str(minutes)

else:

minutes_str = str(minutes * 10)

times.append('2019-01-'

+ day_str + ' ' +

hour_str +':' + minutes_str + ':00')

# 求笛卡儿积

from itertools import product

stationids_by_times

= list(product(stationIDs, times))

# 构建新的数据集

df_data

= pd.DataFrame()

df_data['stationID'] = np.array(stationids_by_times)[:,0]

df_data['startTime']

= np.array(stationids_by_times)[:,1]

df_data = df_data.sort_values(['stationID','startTime'])

df_data['endTime']

= df_data.groupby('stationID')['startTime'].shift(-1).values

def filltime(x):

x_split =

x.split(' ')[0].split('-')

x_part1_1 = x_split[0] +'-'+x_split[1]+'-'

x_part1_2 = int(x_split[2]) + 1

if

x_part1_2 < 10:

x_part1_2 = '0'

+ str(x_part1_2)

else:

x_part1_2 = str(x_part1_2)

x_part2 = ' 00:00:00'

return x_part1_1

+ x_part1_2 + x_part2

# 填充缺

失值

df_data.loc[df_data.endTime.isnull(), 'endTime'] =

df_data.loc[df_data.endTime.isnull(), 'startTime'].apply(lambda x:

filltime(x))

df_data['stationID'] = df_data['stationID'].astype(int)

经过上面的操作，数

据已经变得非常整齐，以

10 分钟为单位。这也有助于

后续提取

特征，完整的训

练集如图 10.2 所示。

图 10.2 完整的

训练集

接下来，我们将对

出进站流量数据进行合

并：

data_inNums.rename(columns={'time_10_minutes':'startTime'}, inplace=True)

data_outNums.rename( columns={'time_10_minutes':'startTime'}, inplace=True)

df_data = df_data.merge(data_inNums, on=['stationID', 'startTime'], how='left')

df_data = df_data.merge(data_outNums, on=['stationID', 'startTime'], how='left')

df_data['inNums'] = df_data['inNums'].fillna(0)

df_data['outNums'] = df_data['outNums'].fillna(0)

特征提取

在 baseline 部分，仅提

取些基础特征即可。对于

时间序列预测问题，主要

提取简单的

时间特征和

历史平移特征。下面是提

取时间相关特征的具体

代码：

#

时间相关特征

df_data['time'] = pd.to_datetime(df_data['startTime'])

df_data['days'] =

df_data['time'].dt.day

df_data['hours_in_day'] = df_data['time'].dt.hour

df_data['day_of_week'] =

df_data['time'].dt.dayofweek

df_data['ten_minutes_in_day'] = df_data['hours_in_day'] * 6

+ df_data['time'].dt.minute // 10

del df_data['time']

用于

描述当前时间在所处周

期内位置信息的特征是

非常具有套路性的，其作

用也非

常大。比如星期特

征（day_of_week）有助于发现相同星期

数具有的相似性，类似

周

期性和相关性描述。下面

是提取历史平移特征的

具体代码：

# 历史平移特征

df_data['bf_inNums'] =

0

df_data['bf_outNums'] = 0

for i,

d in enumerate(days):

If d ==

1:

continue

df_data.loc[df_data.day==d, bf_inNums] = df_data.loc[df_data.day==days[i-1],

inNums]

df_data.loc[df_data.day==d, bf_outNums] = df_data.loc[df_data.day==days[i-1], outNums]

模型训练

为了快速生成

一个可靠稳定的结果，我

们选择使用 LightGBM 模型，线下验

证方式

采用时序验证策

略，用 1

月 28 日的刷卡数据作

为验证集。模型训练的代

码如下：

# 训练集和验证集

准备

cols

= [f for f in df_data.columns

if f not in ['startTime','endTime','inNums','outNums']]

df_train

= df_data[df_data.day<28]

df_valid = df_data[df_data.day==28]

X_train

= df_train[cols].values

X_valid = df_valid[cols].values

y_train_inNums

= df_train['inNums'].values

y_valid_inNums = df_valid['inNums'].values

y_train_outNums

= df_train['outNums'].values

y_valid_outNums = df_valid['outNums'].values

#

开始训练

params = {'num_leaves': 63,'objective': 'regression_l1','max_depth':

5,

'learning_rate': 0.01,'boosting': 'gbdt','metric': 'mae','lambda_l1': 0.1}

model = lgb.LGBMRegressor(**params, n_estimators = 20000,

nthread = 4, n_jobs = -1)

model.fit(X_train, y_train_inNums,

eval_set=[(X_train, y_train_inNums), (X_valid, y_valid_inNums)],

eval_metric='mae',

verbose=100, early_stopping_rounds=200)

这里仅训

练了进站流量，出站流量

也是相同的操作。这样就

能得到基本的分数结果

（进出站对应的平均绝对

误差分数为 19.6167 和

19.0041）。能够优化

的点还是非常

多的，在后

面的工作中将逐步发现

这些优化点，并更新 baseline 方案

的结构和分数，

最终带着

突破使分数靠前。

10.2

数据探

索

在数据探索部分，对不

同业务问题的分析方法

有着明显的差异。在时间

序列预测问题中，数

据分

析的关键在于对时间序

列模式（趋势性、周期性、相

关性和随机性）的分析，在

多种模

式中发现数据的

特点。

10.2.1 数据初探

流量数据

图 10.3 展示的是基础流量数

据，这是可以直接用于训

练的干净数据，可从多个

维度

构造时间相关特征

。

图 10.3

基础流量数据

既然是

时间序列预测问题，数据

跟时间自然存在强相关

性。图 10.4 和图 10.5 分别按

时间序

列展示了 stationID 从 0 到 9

的地铁站

进站和出站流量变化情

况，具体选

择的是 2019 年 1 月

1 日

（星期二）的数据。

图 10.4 2019 年

1 月 1 日

进站流量展示

图 10.5

2019 年 1 月 1 日

出站流量展示

其实 2019 年 1 月

1 日的数据存在一些特殊

性，虽然是工作日，但当天

是元旦节，属

于节假日。在

后面的分析中，我们可以

观察其与其他数据的差

异。

路网地图

大赛提供了

各地铁站之间的连接关

系表，如图 10.6 所示，除“Unnamed:0”列外是

一个

81×81

的矩阵。

图 10.6 路网地图

数据

这里可以进行一些

初步的假设。对于地铁换

乘点，尤其是邻接站比较

多（比如三邻

接站和四邻

接站）的站点，假设其流量

较高；对于只有一个邻接

站的站点，可以直

接确定

其为始发（终点）站，一般是

较偏的地方，流量相对较

低。

10.2.2 模式分析

在第 9 章中讲

过，要解决时间序列问题

，首先需要了解关键数据

模式，然后通过提取特征

来

表现这些模式。此外，我

们还介绍了 4 种模式，分别

为趋势性、周期性、相关性

和随机性，

本节也将围绕

这 4 个方面进行数据分析

。

趋势性。在时间序列中，趋

势性是经常见到的一种

模式，现实生活中很多事

情都包

含趋势性的变化

。杭州地铁流量的变化也

不例外，比如从早上开始

运营到上班早高

峰这一

时间段的流量，又比如接

近春节时的流量。

周期性

。我们尝试从数据中发现

周期性的特征，如图 10.7 所示

，横纵坐标分别表示

时间

索引（date，单位：秒）和进站流量

（inNums，单位：人次），按时间序列展

示了 stationID 从 0 到 9

的地铁站的进

站流量随时间变化的情

况，这里选择的是

从 2019 年 1 月

1

日到 2019 年 1 月 28

日的数据。

图 10.7 周

期性可视化展示

仔细观

察图 10.7

中的紫线和蓝线（即

stationID 为 4 和 stationID 为

9），会发

现这两条线

每次达到进站流量峰值

的时间间隔正好都是一

天，这表示一天是最明显

的周期。需要注意一开始

的 1 月 1 日为节假日，然后紧

接着的

3 天是工作日。这进

一步可以得出结论：工作

日和周末的流量数据分

布是不同的，在具体建模

时需要特

别注意这一点

。

由于图 10.7 有些密集，因此接

下来进行更细致的分析

，选择

stationID 为 4 的地

铁站，详细对

比其工作日和周末的进

站流量。下面的代码用于

生成周五、周六不同

时刻

的流量对比图：

tmp = df_data.loc[(df_data.day.isin([4,5]))]

tmp.loc[tmp.stationID == 4].pivot_table(index='hours_in_day',

columns='day',values='inNums').plot(style='o-')

生成结果

如图 10.8 所示，横纵坐标分别

表示时间（单位：小时）和进

站流量。对比

了周五（day=4）和周

六（day=5）每个时刻的进站流量

后，很容易发现周五明显

区别于周六的时间段是

从 7

点到 8 点以及从 17 点到 19

点

，恰好是进站流量高峰

期

。因此带来差异的主要是

工作日的早、晚高峰，从整

体来看，也就是周期性的

变

化。

图 10.8 周五、周六不同时

刻的流量对比

相关性。一

般来讲，相关性突出表现

在两个相近的时间单位

，比如在不受周期性影

响

的情况下，时间间隔越短

，进出站的流量就越相近

。如图 10.9 所示，横坐标为时

间

（单位：小时），纵坐标为进站

流量，展示的是周四和周

五这两天的地铁进站流

量。两条线的吻合度非常

高，非常符合短期相关性

的概念。另外，对比一天内

两个

相近的时刻，会发现

如果不考虑特殊因素（早

、晚高峰），也是越相近的时

刻流量

越相似。

图 10.9 周四、周

五不同时刻的流量对比

随机性。随机性的数据变

化是不容易确定的，突发

状况、特殊日期等均可导

致随机

性，比如元旦那天

的地铁进站流量就很难

预测。由图 10.10

可以看出，元旦

当天的地

铁进站流量与

其余日期的差异是非常

大的，这会给建模带来困

难，需要特殊对待。

图 10.10 元旦

当天流量的特殊性

10.3

特征

工程

这节的内容非常重

要，其中所有操作均依托

于我在竞赛中的实际操

作，层次清晰且便于优

化

。

10.3.1 数据预处理

在正式提取

特征前，先剔除与测试集

数据分布差异大的数据

（即周末和元旦的数据），从

而

保证整体数据的分布

一致性，在最终方案中也

是先剔除这部分数据。具

体代码如下：

# 剔除周末和

元旦的数据

df_data = df_data.loc[((df_data.day_of_week

< 5) & (df_data.day != 1))].copy()

# 保留日期

retain_days = list(df_data.day.unique())

#

重

新计算rank，方便我们后续提

取特征

days_relative = {}

for i,d

in enumerate(retain_days):

days_relative[d] = i +

1

df_data['days_relative'] = df_data['day'].map(days_relative)

#### 可视化代码

####

dt = [r for r

in range(df_data.loc[df_data.stationID==0, 'ten_minutes_in_day'].shape[0])]

fig = plt.figure(1,figsize=[12,6])

plt.ylabel('inNums',fontsize=14)

plt.xlabel('date',fontsize=14)

for i in range(0,10):

plt.plot(dt, df_data.loc[df_data.stationID==i, 'inNums'], label = str(i)+'stationID'

)

plt.legend()

# 生成

矢量图

plt.savefig("inNums_of_stationID.svg", format="svg")

运行上述代码，生

成的可视化图如图 10.11 所示

，具体是剔除了周末的流

量数据，这样剩下

的就都

是流量分布相似的周内

数据。

图 10.11

只保留工作日数

据的可视化展示

10.3.2 强相关

性特征

强相关性信息主

要产生于不同天的同一

时段，所以我们分别构造

了 10 分钟粒度和

1 小时粒

度

的进出站流量特征。考虑

到前后时间段流量有波

动，又添加了前某个时段

和后某个时段，

或者前某

两个时段和后某两个时

段的流量特征。此外，我们

还构造了前 天对应时段

的流

量。更进一步，考虑到

相邻站点的强相关性，添

加相邻两站对应时段的

流量特征。相关代码

如下

：

def time_before_trans(x,dic_):

if x in

dic_.keys():

return dic_[x]

else:

return np.nan

df_feature_y['tmp_10_minutes'] = df_feature_y['stationID'].values * 1000 +

df_feature_y['ten_minutes_in_day'].values

df_feature_y['tmp_hours'] = df_feature_y['stationID'].values * 1000

+

df_feature_y['hours_in_day'].values

for i in range(1,

n): # 遍历前n 天

d =

day - i

df_d = df.loc[df.days_relative

== d].copy() # 当天的数据

# 特

征1：过去该时间段（一样的

时间段，10

分钟粒度）的进出

站流量

df_d['tmp_10_minutes'] = df['stationID'] * 1000

+ df['ten_minutes_in_day']

df_d['tmp_hours'] = df['stationID'] *

1000 + df['hours_in_day']

# 这里以sum 为统计量

，可进一步考虑mean、median、max、min、std

等统计量

dic_innums = df_d.groupby(['tmp_10_minutes'])['inNums'].sum().to_dict()

dic_outnums = df_d.groupby(['tmp_10_minutes'])['outNums'].sum().to_dict()

df_feature_y['_bf_' + str(day-d) + '_innum_10minutes'] =

df_feature_y['tmp_10_minutes'].map(dic_innums).values

df_feature_y['_bf_' + str(day-d) + '_outnum_10minutes']

=

df_feature_y['tmp_10_minutes'].map(dic_outnums).values

# 特征2：过去在该时间段（1 小

时粒度）的进出站流量

dic_innums

= df_d.groupby(['tmp_hours'])['inNums'].sum().to_dict()

dic_outnums = df_d.groupby(['tmp_hours'])['outNums'].sum().to_dict()

df_feature_y['_bf_'

+ str(day-d) + '_innum_hour'] =

df_feature_y['tmp_hours'].map(dic_innums).values

df_feature_y['_bf_' + str(day-d) + '_outnum_hour'] =

df_feature_y['tmp_hours'].map(dic_outnums).values

# 特

征3：前10 分钟的进出站流量

df_d['tmp_10_minutes_bf'] = df['stationID']

* 1000 + df['ten_minutes_in_day'] - 1

df_d['tmp_hours_bf'] = df['stationID'] * 1000 +

df['hours_in_day'] - 1

# sum 统计量

dic_innums = df_d.groupby(['tmp_10_minutes_bf'])['inNums'].sum().to_dict()

dic_outnums = df_d.groupby(['tmp_10_minutes_bf'])['outNums'].sum().to_dict()

df_feature_y['_bf1_' + str(day-d) + '_innum_10minutes'] =

df_feature_y['tmp_10_minutes'].agg(lambda x:

time_before_trans(x,dic_innums)).values

df_feature_y['_bf1_' + str(day-d)

+ '_outnum_10minutes'] =

df_feature_y['tmp_10_minutes'].agg(lambda x:

time_before_trans(x,dic_outnums)).values

# 特征4：前1 小时的进

出站流量

dic_innums = df_d.groupby(['tmp_hours_bf'])['inNums'].sum().to_dict()

dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].sum().to_dict()

df_feature_y['_bf1_' + str(day-d)

+ '_innum_hour'] =

df_feature_y['tmp_hours'].map(dic_innums).values

df_feature_y['_bf1_' +

str(day-d) + '_outnum_hour'] =

df_feature_y['tmp_hours'].map(dic_outnums).values

for

col in ['tmp_10_minutes','tmp_hours']:

del df_feature_y[col]

return

df_feature_y

仔细观察代码

，在构造历史某天特征的

时候，已经包含了周期性

相关的特征，比如前几周

对

应时刻的进出站流量

、前几周对应时刻 10 分钟（1 小

时）粒度的统计特征等。

那

么考虑一个问题，10.1.5 节没有

对周期性带来的分布差

异问题进行处理，这在构

造特征时

会带来很大的

影响，本身相邻两天的数

据是具有相关性的，又受

工作日和周末的影响，因

此

会提取很多噪声特征

。面对这一问题，我们的建

模就有了多种思路，比如

去除周末数据以保

证一

致性，保留周末数据增强

特征相关的描述，也可以

融合考虑多种建模结果

。

扩展思考

数据只要存在

即合理，没有哪种建模方

式一定是好的，我们更多

的是平衡数据对建模结

果带来的影响。虽然数据

分布存在差异，但那是因

为受到了特征之间关系

的影响，新日

期会带来新

的特征组合，又会导致新

的建模结果。

10.3.3 趋势性特征

挖掘趋势性也是我们提

取特征的关键，我们主要

构造的趋势性特征定义

如下：

或

即前后时段数据

的差值，这里的数据既可

以是进站流量，也可以是

出站流量。同样，我们考

虑

了每天对应的当前时段

、对应的上个时段等。当然

，我们也可以考虑比值，其

定义如下：

或

这类特征在

实际竞赛中的作用也很

强，主要用来辅助模型学

习趋势性的变化。一般而

言，会

构造一阶趋势特征

和二阶趋势特征，其中一

阶趋势特征是相邻时间

单位数据的差值或比

值

，反映趋势的变化情况；二

阶趋势特征是一阶趋势

特征的差值，反映趋势变

化的快慢。

10.3.4

站点相关特征

既然要预测地铁站点（stationID）的

流量，那么可以从站点本

身来挖掘更多信息，这里

主要挖掘不同站点以及

站点与其他特征组合的

热度。这类特征主要用于

描述实体信息，在实

际竞

赛中也是不可或缺的。下

面是构造站点相关特征

的具体实现代码，主要构

造频次特征

（count）和类别数特

征（nunique）：

def get_stationID_fea(df):

df_station

= pd.DataFrame()

df_station['stationID'] = df['stationID'].unique()

df_station

= df_station.sort_values('stationID')

# nunique 相关

tmp1

= df.groupby(['stationID'])['deviceID'].nunique().

to_frame('stationID_deviceID_nunique').reset_index()

tmp2 = df.groupby(['stationID'])['userID'].nunique().

to_frame('stationID_userID_nunique').reset_index()

df_station = df_station.merge(tmp1,on ='stationID', how='left')

df_station = df_station.merge(tmp2,on ='stationID', how='left')

#

与stationID 进行组合，获取

count 特征

for pivot_cols in

tqdm_notebook(['payType','hour', 'days_relative','ten_minutes_in_day']):

tmp = df.groupby(['stationID',pivot_cols])['deviceID'].count().

to_frame('stationID_'+pivot_cols+'_cnt').reset_index()

df_tmp = tmp.pivot(index = 'stationID', columns=pivot_cols,

values='stationID_'+pivot_cols+'_cnt')

cols = ['stationID_'+pivot_cols+'_cnt' + str(col)

for col in df_tmp.columns]

df_tmp.columns =

cols

df_tmp.reset_index(inplace = True)

df_station =

df_station.merge(df_tmp, on ='stationID', how='left')

return df_station

10.3.5 特征强化

对构造出

来的特征进行强化是一

项非常重要的工作，比如

在 2019 腾讯广告算法大赛中

，要

求对新的统计特征做

进一步扩展，试想下如果

把新构造的特征看作一

个非真实的值，然后将

其

与真实值进行交叉，那么

理论上可以得到接近真

实的值。这只是一个方向

，我们还可以对

新构造的

特征进行交叉组合或者

聚合统计，得到更深层次

的特征描述。

下面将进行

具体的特征强化，在相关

性特征的基础上，选择不

同大小的窗口进行了求

和和均

值统计，还对窗口

统计特征进行了差分特

征的提取，以便获取趋势

性相关的特征：

columns =

['_innum_10minutes','_outnum_10minutes','_innum_hour','_outnum_hour']

# 对过去n 天

的流量计算总和、均值

for i

in range(2,left):

for f in columns:

colname1 = '_bf_'+str(i)+'_'+'days'+f+'_sum'

df_feature_y[colname1] = 0

for d in range(1,i+1):

df_feature_y[colname1] =

df_feature_y[colname1] + df_feature_y['_bf_'+

str(d) +f]

colname2

= '_bf_'+str(d)+'_'+'days'+f+'_mean'

df_feature_y[colname2] = df_feature_y[colname1] /

i

# 过

去n 天流量均值的差分特

征

for i

in range(2,left):

for f in columns:

colname1 = '_bf_'+str(d)+'_'+'days'+f+'_mean'

colname2 = '_bf_'+str(d)+'_'+'days'+f+'_mean_diff'

df_feature_y[colname2] = df_feature_y[colname1].diff(1)

# 对一天的第一个时刻

进行处理

df_feature_y.loc[(df_feature_y.hours_in_day==0)&

(df_feature_y.ten_minutes_in_day==0), colname2] = 0

10.4 模型训练

本节

将从模型侧优化方案。作

为时间序列预测问题，可

选择的模型还是蛮多的

，比如传统的

时序模型、树

模型和深度学习模型。调

参也是模型选择的一部

分，本节会对 LightGBM 模型

进行参

数优化，以便进一步提高

分数。

10.4.1

LightGBM

这里使用与 baseline 方案一

样的 LightGBM 模型，方便对比效果

。优化前后的主要差别在

于

多角度特征提取和对

模型个别参数（learning_rate 和 feature_fraction）的调整

。

相对于 baseline 方案中进出站对

应的平均绝对误差分数

为

19.6167 和 19.0041，目前优化后的

方案

进出站平均绝对误差分

数为 12.6477 和

13.1619。两者均有大幅度

的提升，主要原因有

三点

，分别是数据预处理、特征

提取和模型调参。如果从

重要程度来看，特征提取

对结果影

响最大，模型调

参仅是锦上添花。

特征重

要性反馈

我们知道树模

型可以反馈特征的重要

性得分，因此接下来一起

看看 LightGBM

模型

的重要性（importance）得分

。执行下述代码，对特征重

要性得分进行可视化展

示：

import matplotlib.pyplot as plt

import seaborn as sns

import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)

feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,cols)),

columns=['Value','Feature'])

plt.figure(figsize=(20, 10))

sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value",

ascending=False)[:20])

plt.title('LightGBM Features Importance')

plt.tight_layout()

plt.show()

生成的结果如图

10.12 所示

，按重要性由高到低进行

排序。

图 10.12 LightGBM 特征重要性得分

基于重要性得分的特征

选择

特征重要性得分除

了能衡量特征的重要性

外，还可以依此进行特征

选择，保留重要

性得分高

的特征。接下来，对比下这

类特征选择方式的效果

。这里提取重要性为

top100 的特

征，然后重新训练模型，对

比进出站流量的平均绝

对误差分数：

new_cols =

feature_imp.sort_values(by="Value",

ascending=False)[:100]['Feature'].values.tolist()

经过实验反

馈，进行特征选择后的方

案进出站平均绝对误差

分数为 12.6248 和

13.1509。相较之前并没

有太多提升，不过在特征

量上从

262 个缩小到了 100 个，剔

除了大量冗余特征，整体

性能有很大的提升。

10.4.2 时序

模型

在时间序列问题中

，使用循环神经网络、LSTM 和 GRU 这

类时序模型是再合适不

过的选

择，这类模型可以

自动提取时序相关信息

，减少由人工构造大量时

序特征这种费时的工作

。

这里我们选择使用

LSTM 模型

，然后经过多层全连接层

。为了让模型更具泛化性

，还添加

了 Batch Normalization（批归一化）和 Dropout。一

般在进行深度学习相关

建模时，都会考

虑添加这

两个部分，让模型变得更

具健壮性。对于 Batch Normalization 而言，我们

能够通过

规范化的手段

，将越来越偏的分布拉回

到标准化分布，使得梯度

变大，从而加快模型学习

的

收敛速度，避免梯度消

失问题。建模代码如下：

from keras.models import Sequential

from keras.layers.core

import Dense, Dropout, Activation

from keras.layers.normalization

import BatchNormalization

from keras.layers import LSTM

from keras import callbacks

from keras

import optimizers

from keras.callbacks import ModelCheckpoint,

EarlyStopping, ReduceLROnPlateau

def build_model():

model =

Sequential()

model.add(LSTM(512, input_shape=(X_train.shape[1],X_train.shape[2])))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(256))

model.add(Activation(activation="relu"))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(64))

model.add(Activation(activation="relu"))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(16))

model.add(Activation(activation="relu"))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(1))

return model

上

述代码也是非常具有通

用性的，在一般的时间序

列预测问题中能够展现

不错的效果。接下

来，我们

再看看如何进行模型编

译和训练，此处同样能够

像树模型那样进行早停

：

# 编译部分

model = build_model()

model.compile(loss='mae', optimizer=optimizers.Adam(lr=0.001), metrics=['mae'])

# 回调函数

reduce_lr = ReduceLROnPlateau(

monitor='val_loss',

factor=0.5, patience=3, min_lr=0.0001, verbose=1)

earlystopping =

EarlyStopping(

monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='min')

callbacks = [reduce_lr, earlystopping]

# 训练

部分

model.fit(X_train, y_train_inNums, batch_size = 256, epochs

= 200, verbose=1,

validation_data=(X_valid,y_valid_inNums), callbacks=callbacks )

上述简单清晰的代

码就能得到一个还不错

的分数，如图 10.13 所示，线下进

站流量的平均绝

对误差

分数为 13.2967。这里可以优化的

地方也是非常多的，比如

调整网络结构和参数，我

们就简单地调整个参数

吧。注意在训练部分，有一

个特别重要的参数 shuffle，它用

于设

置在每轮（epoch）迭代之前

是否对数据进行打散操

作，默认情况下为 False，如果将

其设

置为 True，那么线下进站

流量的平均绝对误差分

数能到 12.9747。

图

10.13 LSTM 模型训练过程

的展示

10.5 强化学习

学无止

境在本书中应该有所体

现，简单地解决问题不是

最终目标，能够考虑不同

的优化方

案、学习不一样

的解题思路，并且能够在

不同的赛题中做到“举一

反三”才是本书的核心目

标。

10.5.1 时序 stacking

这类建模方式在

时间序列预测问题中虽

然很少被用到，但其威力

是非常明显的，这其中少

不

了建模方式的合理性

。具体地，因为历史数据中

存在一些未知的奇异值

，例如某些大型活动

会导

致某些站点在某些时刻

流量突然增加，这些数据

造成的影响是很大的，又

比如和当前时

间相差较

远的数据与当前数据的

分布存在差异性。

为了减

小奇异值数据带来的影

响，我们采用时序 stacking的方式

进行解决。如果模型预测

结

果和真实结果差距较

大，那么此类数据就是异

常的。接下来，一起学习一

下时序 stacking的

建模思路，发掘

其中的价值。首先给出方

案的结构，如图 10.14 所示，通过

下面的操作，线

下和线上

都能得到稳定的提升。

图

10.14 在时序

stacking 中构造中间特征

图 10.14 是时序 stacking中构建中间特

征的部分，为了保证验证

集不存在数据穿越的问

题，

首先使用

1 月 28 日之前的

数据进行模型训练；然后

用完整数据进行预测，得

到中间特征

（时序 stacking特征）；最

后将中间特征与其余特

征相拼接，并选择距离测

试集较近的数据

作为训

练集（1 月 10 日至 1 月

28 日）。具体的

实现代码如下：

# 训练集准

备

y_inNums =

df_data[df_data.day<28]['inNums'].values

y_outNums = df_data['outNums'].values

df_train =

df_data[df_data.day<28][cols].values

df_data = df_data[cols].values

# 模型训练

params = {'num_leaves': 63,'objective': 'regression_l1','max_depth': 5,

'feature_fraction': 0.9, 'learning_rate': 0.05,'boosting': 'gbdt','metric':

'mae','lambda_l1':

0.1}

model = lgb.LGBMRegressor(**params, n_estimators =

1500, nthread = 4)

model.fit(df_train, y_inNums)

# 中间特征

inNums_stacking = model.predict(df_data)

这

段代码分成两部分，分别

是中间特征获取和合并

中间特征并进行训练。

在

不调整任何参数的情况

下，合并中间特征后进站

流量的平均绝对均值可

以得到 12.5814 的

分数。本赛题的

时间区间并不算大，相信

随着数据集时间区间的

扩大，时序 stacking这种优

化策略

最终的效果也会越来越

好。

特别注意

此处构造中

间特征的过程是简化版

的，还可以使用 折交叉验

证的方式得到中间特

征

，然后拼接其余特征确定

最终训练集区间，进行最

终训练。

10.5.2 Top

方案解析

本章能

够详细介绍的内容毕竟

是有限的，希望读者在未

来能够学习到更多优秀

的思路和方

案，本节将对

更多 Top 方案进行介绍，以扩

展思路。

Top1

方案

除了传统的

LightGBM 模型外，冠军选手还使用

了 NMF 模型，其主要作用是将

乘

客在各个站点间流入

和流出的关系也考虑进

来。注意，这里如果以

10 分钟

为间隔的

话，那么同一时

段的客流流入矩阵和客

流流出矩阵是不同的。但

是如果以一天作为

时间

间隔，则全天的客流流入

矩阵和客流流出矩阵基

本一样。然后对 NMF 模型进

行

改造，利用非负矩阵分解

获取 W 和 H 两个矩阵，其中隐

变量可以理解为算法学

习

到的各自站点的表征

，同时添加了学习趋势的

矩阵

A 和 B，并通过优化损失

函数训

练 W、H、A 和

B 的参数。

提取

的特征主要分为常规特

征、位置判断特征和地铁

路网特征三类，其中位置

判断

特征是由工作时间

、休息时间、上下班高峰期

、周末休闲时间地铁客流

变化得出的

站点位置特

征，比如某站点是否属于

工作地、是否属于居住地

等。地铁路网特征主

要是

参考了

SNA（social network analysis）中的一些思想，对

centrality、

betweenness、closeness 等进行了改造，不但衡量

地铁站点之间的位置和

物理关系，

还衡量乘客在

各个车站间的流动关系

。由于时间紧迫且对杭州

地铁不甚熟悉，因此

这里

并没有将主办方给出的

road_map 站点对应到真实的杭州

地铁站点，建议参考其

他

队伍的攻略，参考杭州地

铁的真实站点来提取特

征或引入周边 POI 数据、相关

POI 对应的事件数据等，以提

高最终预测精度。

Top2 方案

亚

军队伍在模型方面仅使

用了 LightGBM 模型，不过考虑到本

题数据具有周期性强

的

特点，构造了两个模型，分

别是全量数据模型和前

两天数据模型，最后对结

果进

行融合。我还设计了

时间序列加权回归模型

，引入不同日期的相似度

计算来确定加

权时的权

重。最终结果由 LightGBM 模型和时

间序列加权回归模型的

结果加权而

得。

在特征方

面，我们也进行了更加细

致的尝试，主要分为原始

特征、统计特征和衍生

特

征。在衍生特征部分构造

了进出站高峰时间的时

间差、前两天人流量变化

趋势、

通过用户 ID 得到每天

固定时间范围内进出站

的固定人数（该特征能够

有效排除固定

人数对模

型的影响，让训练模型更

加关注随机出行的人数

，减少误差），其余特征

都采

取常规。

10.5.3 相关赛题推荐

时

间序列预测相关的赛题

还是蛮多的，存在着固定

的套路，比如对时间序列

的模式分析和多

角度提

取特征的方式，不过经常

会因为业务和数据不同

带来一些不一样的操作

。接下来，我

们就推荐几个

经典的赛题，相信在深入

对比和学习多个赛题后

，定能轻松应对此类问题

。

2019

腾讯广告算法大赛：广告

曝光预估

本次算法大赛

的题目源于腾讯广告业

务中一个面向广告主服

务的真实业务产品——

广告

曝光预估。广告曝光预估

的目的是在广告主创建

新广告和修改广告设置

时，为

广告主提供未来的

广告曝光效果参考。通过

这个预估的参考，广告主

能避免盲目的

优化尝试

，有效缩短广告的优化周

期，降低试错成本，使广告

效果尽快达到自己的

预

期范围。

本次竞赛提供历

史 天的曝光广告数据（在

特定流量上采样而得），包

括每次曝光

对应的流量

特征（用户属性和广告位

等时空信息）以及曝光广

告的设置和竞争力分

数

；测试集是新一批的广告

设置（有的是全新的广告

ID，也有的是对老广告 ID 修

改

了设置），要求预估这批广

告的日曝光。

基本思路：此

赛题的方案还是蛮多的

，并且每类方案都是可以

拿到前面的名次。这

里给

出三种，分别是传统树模

型、深度学习模型和规则

策略。特征方面主要围绕

时

间序列相关进行预测

，提取思路主要从两部分

来考虑：历史信息和整体

信息，更细

致些就是前 1

天

、最近 5 天、五折交叉统计和

除当天外所有天的统计

特征。在具体

比赛中，我们

会发现测试集里出现了

大量的新广告 ID。新广告是

没有历史信息的，

所以如

何构造新广告的特征、对

新广告进行历史和整体

性的描述成为了提分的

关

键。

我们在这里进行模

糊的特征构造，虽然不知

道新广告的历史信息，但

是知道广告账

户 ID 下所包

含的旧广告的历史信息

。因此，对广告账户 ID

与旧广

告的广告竞胜率

进行组

合，可以构造出广告账户

ID 下广告竞胜率的均值、中

位数等。这样就得到了

新

广告在广告账户 ID 下广告

竞胜率的统计值。

Kaggle 赛题：Web Traffic Time Series Forecasting

本

次赛题（赛题主页如图 10.15 所

示）属于多步时间序列预

测问题，并且时间跨度非

常大，这个问题一直是时

间序列领域最具挑战性

的问题之一。具体来说，该

题目提

供了过去一年多

时间内，大约 145 000

篇维基百科

文章每天的访问流量情

况，要求选

手预测未来三

个月这些文章的访问情

况。

图 10.15 Web Traffic

Time Series Forecasting 赛题主页

这次比赛

分为两个阶段，并且将包

括对未来实际事件的预

测。在第一阶段，排行榜

将

基于历史数据进行打分

；在第二阶段，将基于真实

的未来事件对参赛者的

提交结

果进行评分。

训练

集数据由约 145 000 个时间序列

组成，包含从 2015

年 7 月 1 日到 2016

年

12 月

31 日的数据，每个时间序

列代表不同维基百科文

章的每日浏览次数。第一

阶段的排

行榜是基于 2017

年

1 月 1 日至 2017 年

3 月 1 日的访问流

量而得。第二阶段将使用

截止到 2017 年

9 月 1 日的训练数

据。比赛的最终排名将基

于数据集中每一篇文章

在

2017 年

9 月 13 日至 2017 年

10 月 13 日期间

内每日文章浏览量进行

预测。

基本思路：本次比赛

仅使用规则的方式就可

以拿到银牌的分数，当然

规则方式要考

虑得非常

细致（周期性、趋势性和相

似性的表示）。对于大多数

时间序列预测问

题，规则

方式都能起到一定的作

用，即使不作为最终方案

，也可以作为提取特征的

思路之一。模型方案大致

分为三类：RNN seq2seq、卷积神经网络

和预测中位数法

（将预测

目标转为预测中位数）。

第

11 章 实战案例：Corporación

Favorita Grocery Sales

Forecasting

本章基于 2018

年

Kaggle 竞赛平台中一道经典的

商品销量预测赛题展开

，即如图 11.1 所示的

Corporación Favorita

Grocery Sales Forecasting，这也是时

间序列分析相关问题的

第二个实战

案例，同样内

容主要包括赛题理解、数

据探索、特征工程、模型训

练。作为一道国际赛题，

有

很多内容值得深挖，本章

除了给出通用的解题思

路外，更重要的是引导大

家进行不一样的

尝试，避

免思维定势，最终梳理知

识点并做进一步延伸，即

赛题总结。

图 11.1 Corporación Favorita Grocery Sales

Forecasting 赛题主页

11.1 赛

题理解

11.1.1 背景介绍

在实体

杂货店里，销量预测和顾

客采购量之间的关系总

是很微妙。如果销量预测

得多，而顾

客采购得少，那

么杂货店的商品就会积

压过多，尤其对易腐商品

的影响较大；如果销量预

测

较少，而顾客采购量较

大，那么商品很快就会卖

光，短时间内顾客的体验

会变差。

随着零售商不断

增加新地点、新产品，以及

季节性口味的变化多样

和产品营销的不可预测

，

问题变得更加复杂。位于

厄瓜多尔的大型杂货零

售商 Corporación

Favorita 也非常清楚这点，

其

经营着数百家超市，售卖

的商品超过 20 万种。

于是

Corporación Favorita 向

Kaggle 社区提出了挑战，要求其

建立一个可以准确预测

商品销

量的模型。Corporación Favorita

目前依

靠主观预测来备份数据

，很少通过自动化工具执

行计

划，他们非常期待通

过机器学习实现在正确

的时间提供足够正确的

商品，来更好地让顾客满

意。

11.1.2 赛题数据

本竞赛要求

预测厄瓜多尔 Corporación

Favorita 零售商的

不同商店出售的数千种

商品的单位销

量。赛题提

供的训练数据包括日期

（date）、商店（store_nbr）、商品

（item_nbr）、某商品是否参

与促销（onpromotion，文件中大约有 16％的

值是缺失

的）以及单位销

量（unit_sales）；其他文件包括补充信

息，这些信息对建模可能

有用。

赛题包含的文件比

较多，下面对各个文件进

行简单介绍。

stores.csv：商店的详细

信息，比如位置和类型。

items.csv：商

品信息，比如类别、商品是

否易腐等。需要特别注意

，易腐商品

（perishable）的评分权重高

于其他商品。

transactions.csv：各个商店在

不同日期（仅包含训练数

据时间范围内的日期）的

交易

量。

oil.csv：每日油价。此数据

与销量相关，因为厄瓜多

尔是一个依赖石油的国

家，其经

济的健康状况极

易受到油价的影响。

holidays_events.csv：厄瓜

多尔的假期数据。其中一

些假期可能会转移到另

一天（从

周末到工作日），类

似于补假。

11.1.3 评价指标

根据

标准化加权均方根对数

误差（NWRMSLE）对提交内容进行评

价，计算方法如式 (11-

1)：

对于测

试集的第 行， 是商品的预

测单位销量，

是真实单位

销量， 是测试集的总行

数

，权重 可以在文件 items.csv 中找到

。易腐商品的权重为

1.25，其他

物品的权重为

1.00。

这个回归

评价指标和常见的平均

绝对误差（MAE）、均方误差（MSE）是不

一样的，式

(11-1) 对销量进行了

转换，并且给不同商品赋

予不一样的权重，最后计

算了结果的平方根。

11.1.4

赛题

FAQ

Q 在时间序列预测问题中

会遇到数据穿越的现象

，对此该如何防止？

A 在与时

间相关的建模问题中，最

需要注意的就是数据穿

越。很多时候我们稍不注

意就会

把未来的信息当

作特征加入建模过程中

，这会导致严重的过拟合

，使得线上线下的分数差

异

变大，而且经常会出现

线上和线下评测结果不

一致的情况。此处列举两

个最典型的穿越案

例：

(1) 假

设我们需要预测用户是

否会去观看某视频，测试

集中需要预测 4 月

16 日 10 点 10 分

用

户观看视频 B 的概率，但

是通过训练集中的数据

发现该用户 4 月 16

日 10 点 09 分在

观看视

频

A，10 点 11 分也在观看

视频 A，那么很明显 10

点 10 分该

用户有很大的概率不看

视频

B，通过未来的信息很

容易就能判断出在 4 月

16 日

10 点 10 分，该用户并未观看视

频 B；

(2) 假设我们需要预测用

户 8 月 17 日某银行卡的消费

金额，但是训练集中已经

给出了用户

8

月 16 日以及 8 月

18

日的银行卡余额，那么我

们就很容易就能知道用

户在 8 月 17 日的消费

金额是

多少。以上两例便是明显

的数据穿越情况，这时我

们应该过滤掉未来的数

据信息，仅

用历史数据训

练模型。

Q 如何解决或者预

判训练集与测试集中的

噪声问题？

A 在时序类问题

的训练集中，或多或少都

会存在噪声，这些噪声会

对建模造成极大的影

响

。此处我们将噪声细分为

三类：随机噪声、局部明显

噪声

/ 局部奇异值、长时间

带来的噪

声。测试集中的

噪声和训练集中的是类

似的，本赛题的测试集数

据涵盖的时间范围相对

较

大，有 10 多天，而且包含一

些特殊时间，例如月末（发

工资的时间）等，这些可能

会给预

测带来一定困难

。

Q 如何理解变量 onpromotion，该变量可

能会带来哪些影响？

A 我们

发现该赛题中有一个奇

怪的变量

onpromotion。第一，这是一个

穿越变量，因为

它包含未

来的促销信息；第二，这个

变量存在 16% 的缺失数据，而

且训练集中

unit_sales 为

0 的行全部

省略了，也就是说有很大

一部分的 onpromotion 信息也丢失

了

，但是只要出现了 onpromotion

信息，很

多时候我们的模型就会

认为 unit_sales 不

为 0，这样模型会有

偏差。

11.1.5

baseline 方案

有了上面的了

解，就可以开始基本的建

模了，baseline 方案不需要太复杂

，能给出一个正确

的结果

即可。其实也可以将这个

过程看作先建立一个简

单的框架，之后再不断填

充和优化。

数据读取

读取

数据集的相关代码如下

：

import pandas as pd

import

numpy as np

from sklearn.metrics import

mean_squared_error

from sklearn.preprocessing import LabelEncoder

import

lightgbm as lgb

from datetime import

date, timedelta

path = './input/'

df_train

= pd.read_csv(path+'train.csv',

converters={'unit_sales':lambda u: np.log1p(float(u)) if

float(u) > 0 else 0},

parse_dates=["date"])

df_test = pd.read_csv(path + "test.csv",parse_dates=["date"])

items

= pd.read_csv(path+'items.csv')

stores = pd.read_csv(path+'stores.csv')

#

类型转换

df_train['onpromotion'] = df_train['onpromotion'].astype(bool)

df_test['onpromotion'] =

df_test['onpromotion'].astype(bool)

在上述代码中

，首先对 unit_sales 进行 log1p() 预处理，这样

做的好处是可以

对偏度

比较大的数据进行转化

，将其压缩到一个较小的

区间，最后 log1p() 预处理

能起到

平滑数据的作用。另外在

评价指标部分也是对 unit_sales 进

行同样的处

理，这部分操

作也是预处理。

另一个操

作是对 date 进行处理，将表格

文件中的时间字符串转

换成日期格式。提

前处理

不仅有便于后续操作，还

能减少代码量。

数据准备

数据集包含从

2013 年到 2017 年的

数据，时间跨度非常大，四

年的发展过程中会产

生

很多的不确定性。在利用

太久远的数据对未来进

行预测时会产生一定的

噪声，并

且会存在分布上

的差异，这一点在

11.2 节也可

以发现。另外出于对性能

的考虑，最

终仅使用 2017 年的

数据作为训练集。执行下

述代码过滤 2017

年之前的数

据：

df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]

del df_train

接下来进行基本的数

据格式转换，并最终以店

铺、商品和时间为索引，构

造是否促

销的数据表，以

便进行与促销或者未促

销相关的统计，这样的构

造方式有利于之后

的特

征提取。相关代码如下：

promo_2017_train = df_2017.set_index(["store_nbr",

"item_nbr",

"date"])[["onpromotion"]].unstack(level=-1).fillna(False)

promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)

promo_2017_test

= df_test.set_index(["store_nbr", "item_nbr",

"date"])[["onpromotion"]].unstack(level=-1).fillna(False)

promo_2017_test.columns =

promo_2017_test.columns.get_level_values(1)

promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)

df_2017 = df_2017.set_index(["store_nbr", "item_nbr",

"date"])[["unit_sales"]].unstack(level=-1).fillna(0)

df_2017.columns

= df_2017.columns.get_level_values(1)

特

征提取

历史平移特征和

窗口统计特征是时间序

列预测问题的核心特征

，这里仅简单地使用

历史

平移特征（一个单位）和不

同窗口大小的窗口统计

特征作为基础特征。下面

实

现的是提取特征的通

用代码：

def get_date_range(df, dt, forward_steps, periods, freq='D'):

return df[pd.date_range(start=dt-timedelta(days=forward_steps), periods=periods,

freq=freq)]

接下来的特征提

取主要围绕刚实现的 get_date_range

函

数进行，该函数非常通

用

，其入口参数 df、dt、forward_steps、periods、freq 分别为窗口

提取的

数据来源方式、起

始时间、历史跨度、周期和

频率。特征提取的具体代

码如下：

def

prepare_dataset(t2017, is_train=True):

X = pd.DataFrame({

#

历史平移特征，前

1、2、3 天的销量

"day_1_hist": get_date_range(df_2017, t2017, 1,

1).values.ravel(),

"day_2_hist": get_date_range(df_2017, t2017, 2, 1).values.ravel(),

"day_3_hist": get_date_range(df_2017, t2017, 3, 1).values.ravel(),

})

for i in [7, 14, 21,

30]:

# 窗口统计特征

，销量diff/mean/meidan/max/min/std

X['diff_{}_day_mean'.format(i)] = get_date_range(df_2017,

t2017, i,

i).diff(axis=1).mean(axis=1).values

X['mean_{}_day'.format(i)] = get_date_range(df_2017,

t2017, i,

i).mean(axis=1).values

X['median_{}_day'.format(i)] = get_date_range(df_2017,

t2017, i,

i).mean(axis=1).values

X['max_{}_day'.format(i)] = get_date_range(df_2017,

t2017, i,

i).max(axis=1).values

X['min_{}_day'.format(i)] = get_date_range(df_2017,

t2017, i,

i).min(axis=1).values

X['std_{}_day'.format(i)] = get_date_range(df_2017,

t2017, i,

i).min(axis=1).values

for i in

range(7):

# 前4、10 周每周的平均销

量

X['mean_4_dow{}_2017'.format(i)] =

get_date_range(df_2017, t2017, 28-i, 4,

freq='7D').mean(axis=1).values

X['mean_10_dow{}_2017'.format(i)]

= get_date_range(df_2017, t2017, 70-i, 10,

freq='7D').mean(axis=1).values

for i in range(16):

# 未来16

天是否为促销日

X["promo_{}".format(i)] = promo_2017[str(t2017 +

timedelta(days=i))].values.astype(np.uint8)

if

is_train:

y = df_2017[pd.date_range(t2017, periods=16)].values

return

X, y

return X

从上述代码可以看到，baseline 方

案中仅提取了历史平移

、窗口统计、前

周统计

特征

、未来 16 天是否为促销日和

类型特征等，整体结构非

常简单。

其中需要特别注

意的是前 周统计特征，尤

其是

get_date_range 函数的参数部

分，freq='7D' 表

示提取距离（频率）为 7 天，periods

为

4 表示提取 4 个周

期，28-i（i 的取值

有

0、1、2、3、4、5、6）是历史跨度。当 i 取 1 时，表

示计

算日期为

2017-06-08、2017-06-15、2017-06-22 和 2017-06-29 四天的

销量均值。

接下来确定提

取特征的区间，以及介绍

训练集、验证集和测试集

分别如何提取特

征：

# 以7 月

5 日后的第16 天作为最后一

个训练集窗口，向前依次

递推14 周得到14

个训练窗口

的训练数据

X_l, y_l = [], []

t2017 = date(2017, 7, 5)

n_range

= 14

for i in tqdm(range(n_range)):

delta = timedelta(days=7 * i)

X_tmp,

y_tmp = prepare_dataset(t2017 - delta)

X_l.append(X_tmp)

y_l.append(y_tmp)

X_train = pd.concat(X_l, axis=0)

y_train

= np.concatenate(y_l, axis=0)

del X_l, y_l

# 验证集取7 月

26 日到8 月10 日的数据

X_val, y_val = prepare_dataset(date(2017, 7, 26))

# 测试集

取8 月16 日到8 月31 日的数据

X_test = prepare_dataset(date(2017, 8, 16), is_train=False)

在

时间序列预测问题中，如

何选择验证集很重要。测

试集包含的是 2017 年 8 月 16

日到

8 月 31 日的数据，起始时间是

周三，因为验证集不仅要

在时间上和测试集接

近

，并且还要符合周期性分

布，所以选择 2017

年 7 月 26 日到 8

月

10 日的数据作为

验证集是

最合适的，即起始时间为

周三，终止时间为周四。另

外考虑到验证集的稳

定

性，还可以进行多轮滚动

验证，即间隔一周或若干

周选取一个验证集，比如

7 月

19

日到 8 月 3 日的数据。

还有

一个值得考虑的问题是

为什么选择以

7 天为一个

周期来构建训练数据并

提取特

征呢？这样做会不

会浪费很多数据？为了明

确以 7 天作为一个周期的

合理性，可以

选择不同周

期进行实验对比，比如将

周期改为

1 天，就是提取 7×16 天

的数据，会发

现这样不仅

大幅度增加了模型训练

的时间，分数也没从前好

。针对这个问题，合理

的解

释是以

7 天为一个周期可

以很好地保证数据集的

周期性，同时数据集与验

证

集、测试集具有相同的

分布。

模型训练

这里使用

LightGBM 作为基础模型，预测未来

16

天的商品单位销量。对于

多步预测

的时间序列预

测问题可以有多种建模

方式：第一种，以单步预测

为基础，将预测的

值作为

真实值加入到训练集中

进行下一个单位的预测

，但这样会导致误差累加

，如

果刚开始就存在很大

的误差，那么效果会变得

越来越差；第二种，直接预

测出所有

测试集的结果

，即看作多输出的回归问

题，这样虽然可以避免误

差累加的问题，但

是会增

加模型学习的难度，因为

需要模型学习出一个多

对多的系统，这会加大训

练

的难度。我们暂时选择

第一种建模方式来搭建

baseline 方案：

params = {

'num_leaves':

2**5 - 1,

'objective': 'regression_l2',

'max_depth':

8,

'min_data_in_leaf': 50,

'learning_rate': 0.05,

'feature_fraction':

0.75,

'bagging_fraction': 0.75,

'bagging_freq': 1,

'metric':

'l2',

'num_threads': 4

}

MAX_ROUNDS =

500

val_pred = []

test_pred =

[]

for i in range(16):

print("======

Step %d ======" % (i+1))

dtrain

= lgb.Dataset(X_train, label=y_train[:, i])

dval =

lgb.Dataset(X_val, label=y_val[:, i], reference=dtrain)

bst =

lgb.train(

params, dtrain, num_boost_round=MAX_ROUNDS,

valid_sets=[dtrain, dval],

verbose_eval=100)

val_pred.append(bst.predict(X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))

test_pred.append(bst.predict(X_test,

num_iteration=bst.best_iteration or MAX_ROUNDS))

至此，基础的 baseline 方案已

经搭建好了，在特征提取

和模型训练方面并没有

考虑太

复杂的操作，最终

分数 Public Score 为 0.51837（721/1624），Private Score

为

0.52798（695/1624）。在真实的业务

场景中，这样的方案已经

可以达到不错的预测

效

果了，剩下的工作就是不

断地数据分析、特征提取

和模型调优。

11.2 数据探索

11.2.1

数

据初探

本次比赛的数据

表格还是蛮多的，本节主

要带大家一一了解各个

表格的结构和基本情况

。对

基础表格进行分析是

整个数据分析的初始工

作，有助于厘清表格与表

格之间的联系。

train 数据表

下

面的代码将展示训练集

的基本信息情况，其中包

括每个特征的属性个数

（nunique）、缺失值占比、最大属性占

比和特征类型。

stats = []

for col in

train.columns:

stats.append((col, train[col].nunique(),

round(train[col].isnull().sum() * 100

/ train.shape[0], 3),

round(train[col].value_counts(normalize=True, dropna=False).

values[0]

* 100,3), train[col].dtype))

stats_df = pd.DataFrame(stats,

columns=[' 特征', ' 属性

个数', ' 缺失值占比',

' 最大属

性占比', ' 特征类型'])

stats_df.sort_values(' 缺失值

占比',

ascending=False)[:10]

如图 11.2 所示，可以对 train.csv 中

的基本信息有个大致的

了解，其中属性个数是指

特征所包含的类别个数

、最大属性占比是指出现

频次最高的属性占总数

据量的比

重。另外 train.csv 文件的

大小为高于 5.6 GB，共包含 115

497 040 条数

据。

图 11.2 train.csv中的基本信息

test.csv

如图

11.3 所示，test.csv 中不存在缺失值，其

大小大于 106.1 MB，共包含

3 370 464

条数据

。

图 11.3

test.csv中的基本信息

transactions.csv

如图 11.4 所

示，transactions.csv 中不存在缺失值，其大

小大于

1.9 MB，共包含 83

488 条数据。

图

11.4

transactions.csv 中的基本信息

items.csv

如图 11.5 所示

，items.csv

中不存在缺失值，其大小

大于 118.2 KB，共包含 4100 条数

据。

图 11.5 items.csv中

的基本信息

stores.csv

如图 11.6

所示，stores.csv 中

不存在缺失值，其大小大

于 2.2 KB，共包含 54 条数

据。

图 11.6 stores.csv 中的

基本信息

oil.csv

此数据表中的

信息为每日油价，包括从

2013 年 1 月 1 日到

2017 年 8 月 31 日的数

据

。

holidays_events.csv

此数据表中的信息为厄

瓜多尔的假期数据，共包

含 350 条记录。如图 11.7

所示，其

中

locale 表示假期涉及的地区、description 表

示与假期相关的描

述、transferred 表

示假期是否转移。

图 11.7 holidays_events.csv 中的

基本信息

11.2.2 单变量分析

train.csv：date

图

11.8 展示的是每天的销量，具

体分为三部分：总单位销

量（Total unit sales）、促

销的销量（On

Promotion）、非促销的

销量（Not On Promotion）。可以看到总单位

销

量每年都有明显增长，这

可能是因为公司在成长

。此外，可以发现基本每天

都有

促销和非促销的销

量记录，还有部分记录为

空，约在 2014

年第二季度之前

。

图 11.8 销量随日期的变化展

示

还有部分数据呈断崖

式的降低或增长，这可能

是受其他特殊因素的影

响，比如石油

价格、假期或

者自然灾害等，在具体构

造特征时也应该考虑这

些因素。

train.csv：store_nbr

接下来分析训练

集文件中的 store_nbr（商店编号），如

图 11.9 所示，交易频次最

高的

商店编号是

44，其频次接近

350 万，编号为 52 的商店交易频

次最低，这些可能

是商店

类型、营业地点、营业时间

或者促销力度造成的。

图

11.9

训练集中商店的交易频

次

train.csv：item_nbr

图 11.10 通过折线图的方式

展现不同商品的交易频

次分布，可以发现商品的

交易次数

差距较大，最多

的有

80 000 多次，少的只有几次

。事实上，这也符合直觉，例

如快

消品的销量一般较

好，而不常用且昂贵的商

品则销量会差很多。

图 11.10

训

练集中商品的交易频次

items.csv：perishable

现在观察易腐商品（Perishable）和非

易腐商品（Non Perishable）之间的平衡关

系，

如图 11.11 所示。目前仅是简

单了解单变量的情况，之

后会分析不同家庭或者

不同商

店的易腐商品分

布情况。

图 11.11 是否为易腐商

品的分布

oil.csv：dcoilwtico

这是一个非常

有趣的数据集，里面包含

每日石油价格。由于厄瓜

多尔是一个石油依

赖国

，因此我们可以试图了解

商品销量与石油价格之

间的关系，这里蕴含的知

识在

很大程度上与经济

学有关。

图 11.12 展示了 2013

年到 2017 年

的石油价格（Oil Price）变化，除了少

量日期缺少

油价记录外

，整体油价呈多个阶段的

大幅度变化，比如 2015

年年初

的低谷、2016 年

年初的低谷、2013 年

到 2014 年上半年的油价在

80~100 元

之间、2015 年年初到 2017

年下半年

油价大多在 50

元以内。另外

可以清楚地看到 2017 年的石

油价格变化基本

稳定，因

此考虑在建模阶段仅选

取 2017 年的数据作为训练集

。

图 11.12 石油价格的变化

stores.csv：state

图 11.13

通

过竖状图的方式展示了

每个州（State）的商店数量分布

，共包含 16 个

州，其中 Guayas 和

Pichincha 的商

店是较多的，最多的 Pichincha 有 19 个

店铺，

Guayas 有 11 个店铺，其他州的

商店最多不超过 3 家。

图 11.13 每

个州的商店数量分布

stores.csv：city

city 也

是非常重要的特征，其本

身具有独特的实体信息

。如图

11.14 所示，stores.csv

文件中共包含

22 个不同的城市，其中 Guayaquil 和

Quito 的

商店是较多的。可以看

出

不同城市的商店数目（store count）差

异还是蛮大的，可能是因

为不同地区的经济

发展

水平不同。

图

11.14 各个城市的

商店数量分布

11.2.3 多变量分

析

本节主要分析变量和

变量与变量和标签，一方

面挖掘变量之间的分布

关系，另一方面探索变

量

与标签是否存在区分性

，或者说是否存在某种与

我们的直觉有差异的特

性。

变量和变量

首先分析

holidays_events.csv 文件，图 11.15 展示的是不同假

期类型的地区分布，可

以

看到

Holiday 类型出现的次数最

多，且多数 Holiday 都是发生在 Local。其

余假期

类型多发生在

National。

图

11.15 不同假期类型的地区分

布

变量和标签（本赛题标

签为销量）

首先来看下商

店（stroe_nbr）与销量的关系，由图 11.16

可

以看出，不同商店对

应的

总销量不一样，例如商店

44 和商店 45 销量非常高，而商

店 22

和商店 52 的销

量就很低

。参考图 11.9，可以发现大部分

交易频次高的商店，其销

量也是比较高

的，当然还

可能受商品本身价格、促

销活动和经济因素的影

响。

图 11.16 不同商店对应的总

销量分布

由于商品（item_nbr）的类

别数过多，因此这里通过

箱形图的方式展示商品

与销量

的关系，如图 11.17

所示

。图中忽略具体的商品，仅

考虑将商品销量聚合后

的分布情

况，不同商品的

销量差异非常大，80% 以上是

低于 1 000 000

的，只有少量在 1 000

000 以上

。

图

11.17 不同商品的销量分布

评价指标中有个重要的

参数影响着商品在评分

阶段的重要程度，即是否

易腐

（perishable）特征，其中易腐商品

的权重为 1.25，其他物品的权

重为 1.00。接下里

看看两类商

品随着时间的变化，销量

会有何不同。

由图 11.18 可以看

出非易腐（Non Perishable）商品的销量更

高，易腐（Perishable）

商品的稳定性更

好、抖动程度更低，两者整

体的增长或降低趋势基

本一致。不过易

腐商品的

权重更高，因此在评价阶

段单个易腐商品比单个

非易腐商品对分数的影

响

更大，可以考虑添加易

腐特征或者在模型训练

阶段添加样本权重。

图 11.18 易

腐与非易商品的销量分

布

扩展学习

在数据分析

这一部分并没有展示对

所有变量、所有变量和变

量的组合，以及变

量和标

签的组合的可视化分析

。不过大体可以发现，很多

分析还是符合我们的

先

验判断的，另外也存在一

些只有在进行完可视化

分析后才能了解到的信

息。

对于时间序列预测问

题，最关键的是目标随时

间变化的情况，所以在扩

展学习

部分希望大家继

续探索不同变量随时间

的变化、分析单位销量的

变化趋势，看

是否会出现

难以解释的现象。

11.3

特征工

程

本节内容非常重要，是

本次竞赛获得高分的关

键，将从特征提取的思路

开始介绍，最终讲解

特征

的强化方法，层次清晰且

便于优化。另外，还将带着

读者进行高效特征选择

方法的实

践，以提高整体

训练的性能。

图 11.19

是对 item_nbr 为 502331 的

商品进行的日销量可视

化展示，其中 2017

年 8 月 15

日之后

是要预测的部分。从此折

线图中也可以看出时间

序列预测问题的三大核

心模式，即周

期性、趋势性

和相似性。销量变化的周

期是一周，这在图中的呈

现还是非常明显的，所以

多

提取与七的倍数有关

的特征是有意义的，比如

上周销量、上上周销量、最

近三周中第 天

的均值销

量等。当然，还要更加精准

地预测以及考虑趋势性

和相似性的变化，通过周

期性确

定此刻的大概销

量，通过趋势性调整未来

销量的增减程度，通过相

似性确保近期的销量情

况。

图 11.19

特征提取的核心思

路

在时间序列预测问题

中还应该考虑微观和宏

观的变化，如果 item_nbr 销量是微

观角度的

销量描述，那么

store_nbr、class 和

city 对应的销量聚合就可

以看作宏观角度的销量

描述，如果今年一个商店

整体不景气，那这和其售

卖的每个商品都相关，一

线城市的高销量

预示着

商品销量也是非常高的

。宏观的销量变化并不会

受到局部商品异常的影

响，而且可以

很好地反映

整体的变化趋势。所以在

具体的特征提取阶段会

进行更多的扩展，从微观

到宏观

逐步对销量进行

描述。

11.3.1

历史平移特征

历史

平移特征是提取具有相

似性的信息，基本的历史

平移是将 个时间单位的

销量用

作第 个时间单位

的特征。一般而言特征提

取的时间区间是一个月

之内，因为在具体构造特

征时，时间相隔太远的数

据不仅不具有相似性，还

会带来噪声。

for

i in range(1,31):

# 历史平移特

征，前N 天的销量

X["day_{}_hist".format(i)] = get_date_range(df_2017, t2017, i, 1).values.ravel()

11.3.2 窗口统计

特征

这里构造两大类窗

口统计特征，第一种是构

造特征日前 天的窗口统

计，第二种是构造特

征日

前 周中每周第

天的窗口

统计。其中第二种比较难

理解，为什么以周为单位

呢？为

什么只选取每周的

第 天呢？这里可以从周期

性角度进行解释，完整的

周期性能够保证统计

具

有实际意义，对每周都选

取相同的第 天能够保证

一个周期中时间单位的

高度相似性。

窗口内除了

统计传统的均值、中位数

、最值和方差外，还可以进

行窗口中序列前后的差

值计

算并统计均值。另外

一个需要介绍的是幂函

数衰减加权，这与指数加

权平均类似，权重随着

时

间的流逝逐渐衰减，这些

方法也常作为规则策略

。

构造特征日前 天的窗口

统计

下面给出基本代码

：

X['before_diff_{}_day_mean'.format(i)]

= get_date_range(df_2017,

t2017-timedelta(days=d), i, i).diff(1,axis=1).mean(axis=1).values

X['after_diff_{}_day_mean'.format(i)]

= get_date_range(df_2017,

t2017-timedelta(days=d), i, i).diff(-1,axis=1).mean(axis=1).values

#

指数衰减求和

X['mean_%s_decay_1' % i] = (get_date_range(df_2017,

t2017-timedelta(days=d),

i, i) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values

# mean/meidan/max/min/std

X['mean_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d),

i, i).mean(axis=1).values

X['median_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d),

i, i).median(axis=1).values

X['max_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d),

i, i).max(axis=1).values

X['min_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d),

i, i).min(axis=1).values

X['std_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d),

i, i).std(axis=1).values

其中需要

注意 i 和 d

这两个参数，i 表示

窗口大小，d 表示向历史方

向跨 天。翻

译过来是，首先

向历史方向跨 天到达某

个时间单位，然后统计这

个时间单位过去

第 天的

特征值。在实际操作中为

了保证周期性，d 的取值有

0、7、14，i 的取值

有 3、4、5、6、7、10、14、21、30、90、110、140、356。

构造特征日前

周中每周的第 天的窗口

统计

下面给出基本代码

：

for i in

range(7):

# 前N 周中每周第i 天的销量

for periods

in [5,10,15,20]:

steps = periods *

7

X['before_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017,

t2017, steps-i,

periods, freq='7D').diff(1,axis=1).mean(axis=1).values

X['after_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017,

t2017,

steps-i, periods, freq='7D').diff(-1,axis=1).mean(axis=1).values

X['mean_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017,

t2017,

steps-i, periods, freq='7D').mean(axis=1).values

X['median_{}_dow{}_2017'.format(periods,i)] =

get_date_range(df_2017, t2017,

steps-i, periods, freq='7D').median(axis=1).values

X['max_{}_dow{}_2017'.format(periods,i)]

= get_date_range(df_2017, t2017,

steps-i, periods, freq='7D').max(axis=1).values

X['min_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017,

steps-i, periods,

freq='7D').min(axis=1).values

X['std_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017,

steps-i,

periods, freq='7D').std(axis=1).values

其中需要注意 i、periods 和 steps

这三个

参数，i 表示选取每周的第

天，periods 表示前 个周期，steps-i 表示起

始日期。

11.3.3

构造粒度多样性

粒度多样性表达的是不

同阶数键的组合构造，本

次竞赛的目标是 store_item 下的销

量预

测，但在实际的特征

提取中除了构造商店与

商品的组合，还考虑了

item_nbr、store_nbr、city、class、store_class、city_class 等

一阶和二阶特

征的组合

情况，这些粒度的特征构

造都能对最终效果起到

正向作用。

那么如何确定

两个特征是否可以进行

组合呢？依据的主要是稀

疏程度和层级关系。如果

两个

特征组合后每个属

性都是唯一的，那么这个

特征组合就非常的稀疏

，这种特征是没有构造意

义的；如图 11.20 所示为 stores.csv

中各字

段的层级关系图，其中 state 和

city 字段具有

层级关系，并且

将两者组合后的属性个

数与 city

属性的个数一致，因

此 state 和 city 的

组合是没意义的

。

图 11.20 层级关系

11.3.4 高效特征选

择

在时间序列预测问题

中可以构造大量特征。对

于历史平移特征，可以考

虑以任意可行的单位

进

行平移；对于窗口统计特

征，可以考虑以不同窗口

大小进行统计，这样构造

下来，整体的

特征可能会

达到上千个，这么多特征

多少会导致特征冗余，而

且可能会构造出含有噪

声的特

征。为了解决这个

问题，我们选择使用树模

型生成特征重要性的特

征选择方法，并且进行线

上和线下分数验证。

通过

树模型生成特征具有一

定的可解释性，这也是竞

赛圈和实际工作中一种

常用的方法。本

部分也将

通过实验探索根据特征

重要性选择特征个数的

效果如何。此处仅提取 6

个

训练窗口

的训练数据，分

别对比 top500、top1000、top2000、tail2000 特征以及完整特

征的线下分数

对比，并且

仅看验证集中第一天的

线下分数。执行下述代码

生成特征重要性的得分

可视化

图：

import matplotlib.pyplot as plt

fig, ax

= plt.subplots(figsize=(10,10))

lgb.plot_importance(bst, max_num_features=20, ax=ax,importance_type='gain')

plt.show()

生成结果如图

11.21 所示，该图有助于我们快

速了解特征在模型训练

中的重要性。

图 11.21 特征重要

性得分

实验代码如下：

# 特

征重要性排序

imps = sorted(zip(X_train.columns, bst.feature_importance("gain")),

key=lambda x: x[1], reverse=True)

# 提取top500

特征

top_500 = [items[0] for items in

imps[:500]]

# 验证集中第一天的线下

分数

dtrain = lgb.Dataset(X_train[top_500],

label=y_train[:, 0], weight=train_weight)

dval = lgb.Dataset(X_val[top_500],

label=y_val[:, 0], reference=dtrain,

weight=val_weight)

bst =

lgb.train(params, dtrain, num_boost_round=MAX_ROUNDS,

valid_sets=[dtrain, dval], verbose_eval=100)

经过多轮实验的对

比，如图 11.22 所示，可以看到提

取 top2000 特征的效果最好（分数

为

0.277435），提取

tail2000 特征的效果最差

（分数为 0.318104），这明显说明通过

树模型获

取特征的重要

性得分是有效果的。

图 11.22

提

取不同部分特征集的分

数展示

11.4 模型训练

本节将

从模型侧对方案进行优

化，对于时间序列预测问

题，可选择的模型还是蛮

多的，比如

传统的时间序

列模型、树模型和深度学

习模型。本节选择 LightGBM、LSTM

和 Wavenet 作

为

方案的最终模型，还会学

习模型融合，以使最终的

排名更上一层楼。

11.4.1 LightGBM

虽然这

里和 baseline 方案使用的模型是

一样的，但这里进行了优

化，首先由于评价指标会

受

perishable 的影响而为每个样本

设置了权重；然后加入了

11.3 节提到的历史平移特征

、

窗口统计特征和粒度多

样性特征，基本包含了时

间序列预测问题可以提

取的大部分特征。

# 样本权

重构造部分

item_perishable_dict = dict(zip(items['item_nbr'],items['perishable'].values))

train_weight = [] # 训练集权重

val_weight =

[] # 验证集权重

items_ = df_2017.reset_index()['item_nbr'].tolist()

* n_range

for item in items_:

train_weight.append(item_perishable_dict[item] * 0.25 + 1)

items_

= df_2017.reset_index()['item_nbr'].values

for item in items_:

val_weight.append(item_perishable_dict[item] * 0.25 + 1)

#

添加样本权

重和指定类别特征

dtrain = lgb.Dataset(X_train, label=y_train[:, i],

weight=train_weight)

dval = lgb.Dataset(X_val, label=y_val[:, i],

reference=dtrain, weight=val_weight)

bst = lgb.train(params, dtrain,

num_boost_round=MAX_ROUNDS,

valid_sets=[dtrain, dval], verbose_eval=100)

相对

于 baseline

方案的 Public Score 为 0.51837（721/1624），Private Score

为

0.52798（695/1624），在经过更

加细致的特征提取、建模

策略调整、添加样本权重

以及指定

类别特征后，模

型的分数有了很大的改

进，具体讲，Public Score 为 0.51319（497/1624）和

Private Score 为 0.51571（13/1624）。

11.4.2 LSTM

在时间

序列预测问题中，作者依

旧选择最为流行的 LSTM 模型

，和 LightGBM 模型相比，

LSTM

模型提取的

特征数量和粒度会有很

大程度的缩减，主要是因

为它具有从历史数据中

提

取序列信息的特性，而

像 LightGBM 这样的树模型本身并

不能提取历史信息，需要

人工构造

大量特征。

本节

首先给出基础的网络结

构，在训练过程中还会使

用

BatchNormalization 和

Dropout 方法，有助于提高模

型的泛化性。下面是构建

LSTM 模型的具体代码实现：

def

build_model():

model = Sequential()

model.add(LSTM(118, input_shape=(X_train.shape[1],X_train.shape[2])))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(64))

model.add(Activation(activation="relu"))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(16))

model.add(Activation(activation="relu"))

model.add(BatchNormalization())

model.add(Dropout(0.2))

model.add(Dense(1))

model.compile(loss='mse',

optimizer=optimizers.Adam(lr=0.001), metrics=['mse'])

return model

LSTM 的

建模方式与之前的基本

一样，也是进行

16 次训练得

到测试集中 16 个单位的结

果，

在训练时仅添加了训

练样本的权重。另外需要

注意的是对标签进行了

减去标签均值的转换，

主

要是为了缩放处理，减弱

预测结果的抖动。下面是

16

次训练和 16 次预测的实现

代码：

for i in

range(16):

y_mean = y_train[:, i].mean()

#

编译部分

model = build_model()

# 回调函数

reduce_lr

= ReduceLROnPlateau(

monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001,

verbose=1)

earlystopping = EarlyStopping(

monitor='val_loss', min_delta=0.0001,

patience=3, verbose=1, mode='min')

callbacks = [reduce_lr,

earlystopping]

# 训练部分

model.fit(X_train, y_train[:, i]-y_mean,

batch_size =4096, epochs = 50, verbose=1,

sample_weight=np.array(train_weight),

validation_data=(X_val, y_val[:, i]-y_mean),

callbacks=callbacks, shuffle=True)

val_pred.append(model.predict(X_val)+y_mean)

test_pred.append(model.predict(X_test)+y_mean)

LSTM 模型的分数也

是很不错的，Public Score 为

0.51431（557/1624），Private Score 为

0.52067（116/1624），可以用

于模型融合。

在模型部分

其实还有很多优化方向

，最基本的就是确定深度

神经网络的隐藏层数。理

论上层

数越深，拟合函数

的能力越强，模型预测效

果按理说会更好，但实际

上更深的层数可能会导

致过拟合，同时增加训练

难度，使模型难以收敛。当

然，为了更好地确定层数

，接下来进行

了简单的实

验对比。

实验对比了 8 层和

4 层的深度神经网络结构

（为了快速得到实验结果

，仅对比验证集中第一

天

的分数），其中

8 层的线下评

估分数为 0.2790，4 层的线下评估

分数为 0.2795，两者基本

一致，但

在运行时间上，8

层是 4 层的

三倍多。鉴于此，使用 4 层的

深度神经网络结构不仅

能使分数达到与 8

层同等

的效果，运行时间也是非

常高效的。

扩展学习

除了

确定隐藏层数，还可以选

择神经元数量。在隐藏层

中使用太少的神经元会

导致欠拟

合。使用过多的

神经元又可能会导致过

拟合，当神经网络的节点

数量过多（信息处理能

力

过高）时，训练集中包含的

有限信息不足以训练隐

藏层中的所有神经元，就

会导致过

拟合。另外，即使

训练集包含足够多的信

息，隐藏层中过多的神经

元也会增加训练时

间，从

而难以达到预期的效果

。显然，选择合适的隐藏层

神经元数量至关重要。

11.4.3 Wavenet

Wavenet 模

型虽然在本次竞赛中基

本没有出现，但亚军选手

使用此模型能够取得第

2 名的好

名次。既然有如此

大的威力，我们不妨来认

识下这个模型。

2016 年，谷歌 DeepMind 在

ISCA 上发表了“WaveNet:

A generative model for raw audio”。

Wavenet 模型是一种序

列生成模型，最初用于语

音生成建模。相较于传统

的 ARIMA、

Prophet、LightGBM 或者 LSTM，Wavenet

模型在解决时间

序列预测问题时有着独

特优势，

它是基于卷积神

经网络的时间序列模型

，其核心是扩张的因果卷

积层（Dilated Casual

Convolutions），如图 11.23 所示。

图 11.23 因果卷

积层

Wavenet 模型能够正确处理

时间顺序，并且可以处理

长期的依赖，从而避免了

模型爆炸。具

体实现代码

如下：

def build_model(shape_):

def wave_block(x, filters, kernel_size,

n):

dilation_rates = [2**i for i

in range(n)]

x = Conv1D(filters =

filters, kernel_size = 1, padding =

'same')(x)

res_x = x

for dilation_rate

in dilation_rates:

tanh_out = Conv1D(filters =

filters,

kernel_size = kernel_size, padding =

'same',

activation = 'tanh', dilation_rate =

dilation_rate)(x)

sigm_out = Conv1D(filters = filters,

kernel_size = kernel_size, padding = 'same',

activation = 'sigmoid', dilation_rate = dilation_rate)(x)

x = Multiply()([tanh_out, sigm_out])

x =

Conv1D(filters = filters,

kernel_size = 1,

padding = 'same')(x)

res_x = Add()([res_x,

x])

return res_x

inp = Input(shape

= (shape_))

x = wave_block(inp, 32,

3, 8)

x = wave_block(x, 64,

3, 4)

x = wave_block(x, 118,

3, 1)

out = Dense(1, name

= 'out')(x)

model = models.Model(inputs =

inp, outputs = out)

return model

模型训练部分与之

前基本一致，不过需要调

整标签的格式，同时为了

缩短训练时间，对

batch_size 和 epochs 也进

行了调整。代码如下：

model.fit(X_train,

y_train[:, i].reshape((y_train.shape[0], 1, 1)),

batch_size =

4096, epochs = 5, verbose=1,

sample_weight=np.array(train_weight),

validation_data=(X_val, y_val[:, i].reshape((y_val.shape[0], 1, 1))),

callbacks=callbacks,

shuffle=True)

相较

于之前的 LSTM 模型，如果不使

用 GPU 进行训练，则会花费非

常多的时间。不过最终

结

果也还不错，如果想获取

更好分数的话，还得继续

优化。

11.4.4 模型融合

此部分使

用简单的加权平均即可

，这也是本次竞赛中绝大

多数排名靠前的团队使

用的融合方

法，不仅效果

明显，还简单直观。具体地

，将 LightGBM

模型的最优成绩和 LSTM 模

型的

最优成绩进行融合

，权重按线上成绩进行大

致确定，即最终结果为 0.7×LightGBM 模

型的最

优成绩 + 0.3×LSTM 模型的最

优成绩。最终，Public Score 为

0.51089（119/1624），Private

Score 为 0.51456（6/1624），可见模

型融合后的分数有了非

常明显的提高。如果再加

上

Wavenet 模型的成绩，并且完善

三个模型的特征部分，分

数还会有更大的提高。

11.5 赛

题总结

11.5.1 更多方案

Top1 方案

在

模型方面，冠军团队使用

的是 LightGBM 和 NN 模型，并且都构造

了很多个，这

些模型存在

特征或者样本选择方面

的差异。在数据方面，冠军

团队仅采用

2017 年的

数据来

提取特征和构建样本，具

体地，训练集采用 2017 年 5

月 31 日

至 2017 年 7

月

19 日或 2017 年 6

月 14 日至 2017 年

7 月

19 日的数据（不同模型采

用不同的数据

集），验证集

采用 2017 年 7

月 26 日至 2017 年 8

月 10 日的

数据。

冠军团队构造的特

征大体分为基本特征和

统计特征，其中基本特征

包含类别特征、

促销特征

和周期相关特征等；统计

特征则作为主要提分特

征，使用一些方法（均

值、最

值、标准差、差值等）来统计

不同时间窗口中不同键

（item_nbr、store_nbr、store_nbr_class

等）的某些目标值（销量、促

销

情况等）。

Top2 方案

亚军团队

使用的是 Wavenet

模型，并对训练

集和验证集的划分方式

进行了介绍，把

随机采样

所得的长度为 128 的序列小

批量喂入模型，然后随机

选择目标日期的开

始，这

样一来，可以说模型在每

次训练迭代中都将看到

不同的数据。因为总数据

集

约为

170 000（seq）×365 天，我们认为这种

方式训练的 Wavenet 模型能够很

好地处

理过拟合问题。他

们采取的验证方法是分

步进行的，并且保留最近

16

天的验证数

据。

亚军团队

还发现在 2015 年 7

月 1 日、2015 年 9 月

1 日

、2015 年 10 月 1

日和 2016

年 11 月 27

日，很多物

品开始有 unit_sales 的记录，但难点

在于不知道测试数据

内

的大多数新商品从哪一

天开始有这个记录。通过

检查促销信息，发现仅在

8 月 30

日和 8 月 31 日的数据中带

有新商品的促销信息。通

过查看 2015

年 10 月 1 日或 2016

年 11 月 27 日

的数据，那些“旧的新商品

”会在促销当天（而不是在

此之前）显示。

因此，亚军团

队认为大多数新商品将

在

8 月 30 日和 8 月

31 日开始具有

unit_sales 的记录。在此之前，可能只

售出了一些新商品。对此

，不使用模型来

预测以防

止发生过拟合，而仅使用

一些规则值来对那些“旧

的新商品”进行损失计算

验证。

Top3 方案

季军团队构建

了三个模型，分别是 LightGBM、CNN 和 GRU 模

型，在模型融合时

三者的

权重几乎相等。如果只看

单个模型，GRU

模型的预测效

果要比其他模型好一

些

。

在上述的方案分享中，季

军团队着重提到了在时

间序列预测问题中，两个

重要的事

情是验证（validation）和装

袋（bagging）。通过正确的验证，即模

拟对训练集和测

试集进

行拆分，从而避免数据拆

分导致的未来信息泄露

问题。

鉴于此，在具体的拆

分方式中，对训练集只记

录每个“item

和 store 对”80 个相应销售

日的历史记录，并将接下

来的 16 个销售日作为验证

集。训练集和验证集的划

分始终

是从星期二到星

期三，保证与原始测试集

划分方式一样，旨在捕获

每周动态。

在上述划分方

式下，训练模型并估计模

型的最佳迭代次数，然后

拼接验证集对模型

进行

重新训练，以便使用最新

信息。

关于装袋，季军团队

对每个模型都会进行 10 次

训练，每次都初始化不同

的权重，因

此结果也不同

，对它们取均值有助于改

善解决方案，尤其是在处

理不确定的未来

时。另外

一种袋装方式是在每个

训练时期（包括最开始的

时期）之后，都预测目

标，然

后对这些预测结果取平

均，这也会极大地改善最

终结果。

11.5.2 知识点梳理

梳理

知识点也是竞赛结束后

一项重要的工作，大致分

为梳理重点方案和梳理

核心代码。本次

竞赛的核

心在于特征工程，如果这

部分的功夫做足，那么成

绩将会很不错；梳理代码

部分主

要是对代码进行

优化，提高其可读性和使

其模块化，便于之后的比

赛复用。本节主要对特征

工程中的特征提取方式

进行梳理，尽量展现最佳

、完整的时间序列特征提

取思路。

时间特征

年、季度

、月、星期、日、时等为基本的

时间特征，当然还可以将

日划分为上午、

中午、下午

、晚上、深夜和凌晨等。

还有

一类时间特征是某个时

间区间的记录，比如间隔

某段时间、距离某天的第

几

天、上 次和下 次做某个

行为的时间差等。

时序特

征

时序特征可以分为历

史平移和窗口统计两类

，这也是时间序列预测问

题中最为核心

的部分。

历

史平移仅需简单的平移

，比如将历史第 1、2、3、7、14 天的销量

作为当天的特

征。

窗口统

计首先是确定窗口大小

，然后进行聚合统计，具体

统计方式为均值、中位

数

、最值、分位数、偏差、偏度和

峰度等。时间窗口内还可

以进行一阶差分或二阶

差分，然后对差分值进行

聚合。

交叉特征

交叉特征

一般分为三类，分别是类

别特征和类别特征组合

、类别特征和连续特征组

合、连续特征和连续特征

组合。类别特征和类别特

征的组合相当于进行笛

卡儿积，

比如将天和小时

组合得到具体一天的某

个小时；类别特征和连续

特征进行组合一般

是进

行聚合操作；连续特征和

连续特征的组合包含同

比和环比、一阶差分和二

阶差

分等。

高级特征

这类

特征一般通过传统的时

序模型得到，比如 AR、ARMA、ARIMA、Prophet

等，这类

模型仅基于历史目标变

量拟合预测结果，可以将

预测结果视为高级特征

与

最终特征集进行合并

。

11.5.3 延伸学习

本节将推荐些

有关商品销量相关的竞

赛作为延伸部分的学习

内容，以便加深对此类型

赛题的

理解，并了解和发

现在不同商品销量问题

中有哪些坑需要注意、有

哪些解题套路需要掌握

。

Kaggle 平台的 M5 Forecasting

- Accuracy - Estimate the unit

sales of Walmart

retail goods

本次竞赛（如图

11.24 所

示）是预测沃尔玛零售商

品的单位销量，竞赛提供

了数据的

层次结构信息

（state、store、dept 和 item）以及 2011

年 1 月 29 日到 2016

年

6 月 19 日

商品的销售情况，目标是

预测未来 28

天不同商品的

销售量。

图 11.24 M5 Forecasting -

Accuracy - Estimate the unit sales

of Walmart retail

goods 赛题主页

基本

思路：赛题的建模方案基

本分为递归和非递归两

类，第一类是循环预测未

来

28

天中每一天的销量，将

已预测的销量（第 天）也合

并到训练集中继续预测

接

下来的销量（第 天），也可

以把已预测的销量作为

特征；第二类将不会对训

练集进行扩展，在预测第

天的销量时，仅使用第 start

天

到第 天的销量作

为训练

数据进行训练并预测。

Kaggle 平

台的 Predict

Future Sales

Predict Future Sales 竞赛（如图

11.25 所示）的数

据是由日常销售数据组

成的时间序

列数据集，该

数据集由俄罗斯某公司

提供。其中包括商店、商品

、价格和日销量等

连续 34 个

月的数据，要求预测第

35 个

月的各商店各商品的销

量，评价指标为均方

根误

差。

图 11.25 Predict

Future Sales 赛题主页

基本思路

：本赛题为常规的商品销

量预测问题，一共涉及 60 个

商店，这些商店坐

落在 31 个

城市。主要工作围绕数据

探索和特征工程进行，在

构造特征时需考虑不同

细粒度的特征组合，这与

本章所讲的赛题非常相

似。

IJCAI-17 口碑商家客流量预测

随着移动定位服务的流

行，阿里巴巴和蚂蚁金服

逐渐积累了来自用户和

商家的海量

线上线下交

易数据。蚂蚁金服的

O2O 平台

“口碑”用这些数据为商家

提供了包括交易

统计、销

售分析和销售建议等后

端商业智能服务。举例来

说，口碑致力于为每个商

家提供销售预测。基于预

测结果，商家可以优化运

营、降低成本，并改善用户

体

验。

在这次竞赛中，提供

的数据是蚂蚁金服的支

付数据，具体提供用户的

浏览和支付历

史，以及商

家相关的信息，通过给出

店铺（实体店）过去每天的

客流量，预测店铺

未来 14 天

每天的客流量。

基本思路

：此赛题如果放在今天，就

是一道比较常规的题，不

过在 2017 年这是道

非常新颖

的赛题，没有太多类似方

案作为参考，排名靠前的

选手们给出的解题思路

也是多种多样。冠军选手

使用的是对时间序列权

重模型与树模型的结果

加权融合，

其中时间序列

权重模型是规则方法，考

虑了常量因素、时间衰减

因素、星期因素和

天气因

素的结合，即使放在今天

也是非常细致的规则方

法；树模型则使用 XGBoost

和随机

森林。特别需要注意的是

选手对双十一客流量的

预测结果进行了 1.1

倍的扩

增。

第四部分 精准投放，优

化体验

第 12 章

计算广告

第

13 章 实战案例：2018 腾讯广告算

法大赛——相似人群拓展

第

14

章 实战案例：TalkingData AdTracking Fraud Detection Challenge

第 12 章 计算广

告

在中国互联网兴盛的

早期，江湖流传着神秘的

BAT 三巨头，分别是百度（B）、阿里

巴巴

（A）和腾讯（T），它们占据着

搜索引擎、电商和社交软

件的山头。近来又出现了

TMDJ（今日头条、美团、滴滴、京东

）。在 20 世纪八九十年代，香港

娱乐圈占据着国内

娱乐

圈的大半江山，各种天王

、天后、巨星风起云涌，这都

得益于电视、广播和海报

等传播

媒介。从 2010

年开始，随

着 4G 移动互联网、智能终端

的出现和影视产业的发

展，人民群

众追星的方式

也发生了改变，不知道从

什么时候开始著名的演

员、歌手被叫作明星，并且

被

分成了一线、二线……十八

线，在这日新月异变化之

中，流量逐渐成为衡量明

星受欢迎程度

的一个标

准，并且流量明星区别于

传统明星形成了一个新

的群类，顶级流量更是对

一线明星

的赞誉。流量之

所以受到如此重视，是因

为可以进行商业变现，具

有巨大的潜在价值。

12.1 什么

是计算广告

想象一下，在

一个日常准备去上班的

早晨，进电梯后发现左右

两侧分别贴着编程猫与

年货节

的海报，出门骑的

共享单车上贴着某 App

的牌

子，地铁走廊两边的大屏

上清一色地展示着

某著

名品牌刚推出的旗舰机

，等等。在商业文明极度发

达的今天，有人活动的地

方就会有广

告出现，因为

有人就代表有人流量。不

论广告类型和广告素材

是否存在某种差异，都是

能够

带来流量变现的。纵

然市场经济下的广告花

样越发繁多，然而究其本

质依然是建立在流量之

上，比如商家请明星代言

是借助明星的流量，明星

自身可以通过收取广告

费实现流量变现；

广告的

内容与形式也是一种流

量，这种流量则是基于目

标消费者的大众共识或

流行文化。另

外，广告投放

的渠道会影响流量的大

小，继而影响变现效果。移

动互联网催生出了许多

新名

词，其中区别于传统

销售方式的一对名词便

是线下与线上，本章介绍

的大部分内容适用于线

上场景，广告是借助流量

实现变现，而计算广告则

是让广告接触更多的流

量。

计算广告是指借助大

数据的分析建模，使得广

告能够覆盖广泛区域和

实现消费者的多跨度精

准曝光，让同一份广告尽

可能接触到更多有效的

流量和更多对广告感兴

趣的人，从而用同样

的成

本，让广告效果尽可能更

好，使产品与服务获得更

多商业上的成功。而随着

大数据、人

工智能与物联

网在人类社会的逐渐发

展，计算广告的重要性与

可能性也会越来越高。

12.1.1 主

要问题

计算广告的目的

是在控制一定成本的基

础上，找到尽可能多的流

量渠道与目标消费者，从

而

进行商业变现，核心之

处在于借助大数据与人

工智能进行广告的定向

投放，其中涉及计算广

告

的三要素（如图 12.1 所示）即广

告主、平台与消费者之间

的互相作用。对于广告主

来

说，是想依靠平台宣传

自己的产品，通过投入一

定的成本对产品进行推

广从而提升销量与营

业

额；平台则收取广告主的

推广费用，以便更好地建

设自身，同时更好地服务

于消费者；消

费者则可以

过滤掉自己不感兴趣的

广告类型，同时享受平台

的其他免费服务。其实本

质上还

是消费者养活了

广告主与平台。

图 12.1 计算广

告的三要素

因此，计算广

告需主要解决的问题就

是如何协调三方的利益

，即广告主、平台与消费者

之间

的利益。针对这个问

题涌现出了很多核心技

术，比如广告主与广告主

、广告主与平台的竞价

策

略；预测用户点击率或转

化率的技术，以使平台投

放合适的广告给用户；优

化广告排期、

控制预算的

技术，用于支持广告主和

平台的相关操作等。

12.1.2 计算

广告系统架构

虽然不同

公司或者不同业务的计

算广告系统存在很大的

细节差异，但就这些计算

广告系统的

架构而言，还

是存在通用部分的，这里

主要介绍三大部分，即在

线投放引擎、分布式计算

平

台（离线）和流式计算平

台（在线），如图 12.2 所示。在线投

放引擎根据 Web

服务端的广

告请求所对应的用户和

上下文等相关信息进行

广告检索、广告排序和收

益管理，最后将相关

记录

转移到分布式计算平台

和流式计算平台；分布式

计算平台周期性地以批

处理方式加工过

去一段

时间内的数据，得到离线

用户标签和 CTR 模型与特征

，然后将这些存放到数据

库

中，供制定在线投放决

策时使用；流式计算平台

负责加工处理最近一小

段时间内的数据，得

到实

时用户标签以及模型参

数，并且也把这些存放到

数据库中，供制定在线投

放决策时使

用。

图 12.2 计算广

告系统的架构

在线投放

引擎

广告检索：当 Web 服务端

发来广告请求时，系统根

据该广告位的页面标签

或

者用户标签从广告索

引列表中查找符合条件

的广告。广告检索阶段主

要以召

回率作为评价指

标，高召回率意味着可以

避免漏掉有可能被用户

点击的广

告。

广告排序：当

出现多个广告主抢夺一

个广告位的情况时，需要

对投放各个广

告可能会

产生的效益分别进行预

估，即计算 eCPM 值，之后按该值

的大小对

广告主从高到

低进行排序。

分布式计算

平台

行为定向：该模块用

于挖掘广告投放日志中

的用户行为属性，给用户

赋予各

式各样的标签并

存放在结构化标签库中

，供后续投放广告时使用

。

点击率建模：该模块的功

能是在分布式计算平台

上训练并得到点击率模

型的

参数和相应特征，然

后加载到缓存中，用以辅

助广告投放系统进行决

策。

流式计算平台

实时受

众定向：该模块的功能是

将最近一段短时间内发

生的用户行为和广告

投

放日志及时地加工成实

时用户标签，用以辅助广

告检索模块。对于在线计

算广告系统来说，这部分

对效果提升的意义更为

重大。

实时点击反馈：该模

块同样是实时反馈用户

行为和广告投放日志的

变化，主

要生成实时点击

率相关特征，用以辅助广

告排序模块。在很多情况

下，捕捉

短时间内的行为

记录更能反映用户的偏

好信息，广告投放效果也

更加显著。

12.2 广告类型

为了

使利益达到最大化和不

断满足种类繁多的需求

，广告类型处于不断的更

新和迭代中，其

发展演变

历程如图 12.3 所示。本节将按

照广告商业模型的发展

对广告类型进行介绍，包

括

CPT 广告、定向广告、竞价广

告和程序化交易广告。

图

12.3 广告类型的发展演变

12.2.1 合

约广告

可将 CPT

广告和定向

广告统称为合约广告，合

约广告具体又可分为无

定向合约交易广告和

粗

粒度定向合约交易广告

。CPT 广告是按时间成本计费

，广告主以固定的价格买

断一段时

间内的广告位

来展示自己的广告，比如

开屏广告、富媒体广告或

应用市场的下拉关键词

等；

定向广告是广告主选

择自己要投放的兴趣标

签，然后算法为其匹配相

应的受众人群并进行广

告投放。

12.2.2

竞价广告

定向广

告产生后，市场朝着精细

化的方向发展，参与的广

告主越来越多，定向标签

也越来越

精准。媒体主为

了提高收益，引入了竞价

广告模式。在这种模式下

，媒体主不再以合约的形

式向广告主承诺展示量

，而是采用“价高者得”的方

案来决策每次展示哪个

广告，使得媒体主

可以实

时对不同广告进行比价

，从而最大化收益。这种模

式还催生出了 ADN（广告网

络

）、ATD（广告交易终端）等广告产

品。

12.2.3 程序化交易广告

竞价

广告的进一步发展催生

出了实时竞价模式（Real Time Bidding，RTB），这种

模式使得

广告主可以实

时地在每一次广告展示

中选择自己的目标受众

，并且参与竞价。后来以实

时竞

价为核心的一系列

广告交易逐渐演变为机

器与机器之间依靠程序

完成广告交易决策的模

式，

因此这类广告被称为

程序化交易广告，催生出

的相关广告产品有 DSP、SSP、ADX、DMP

等。

计

算广告技术是支撑广告

应用业务进行的关键，其

主要能够协调广告主、平

台和消费者之间

的利益

关系。计算广告中有三大

核心技术，即广告召回、广

告排序和广告竞价，这些

技术既

能保证将广告投

放到合适的人群中，也能

保证广告主和平台的利

益。

12.3 广告召回

广告召回就

是广告检索，这一阶段的

主要工作是根据用户或

商品属性以及页面上下

文属性从

广告索引（Ad index）中检

索符合投放条件的候选

广告，其中会用到的召回

（检索）方式也

是多种多样

，接下来先具体看看有哪

些召回方式。

12.3.1 广告召模块

这里将广告召回的模块

分成以下三个部分。

布尔

表达式召回：根据广告主

设置的定向标签组合成

布尔表达式。如图 12.4 所示，

在

庞大的定向标签体系中

，广告主根据用户的兴趣

、年龄和性别组成的布尔

表达式

来对广告的定向

受众人群进行召回。

图 12.4 布

尔表达式召回

向量检索

召回：这种技术可以分为

三种，第一种是通过传统

的

Word2Vec、Item2Vec

或 Node2Vec 等方式来获取广告

的向量表示，然后通过相

似度计算对受众人群进

行召

回，其特点是实现简

单、表达能力强等；第二种

是通过深度学习模型获

取广告的向

量表示，比如

YouTube

DNN 利用深度学习模型将广

告、用户等信息都映射为

向量，

然后通过向量的最

近邻检索算法对受众人

群进行召回；还有一种就

是经典的 DSSM

双塔模型（12.3.2 节会

详细介绍）。

基于 TDM（深度树匹

配模型）的召回：这是由阿

里妈妈精准定向广告算

法团队自主

研发的基于

深度学习的大规模（千万

级 + ）推荐系统算法框架。这

种技术通过结合

深度学

习模型与树结构搜索，来

解决召回问题中高性能

需求与使用复杂模型进

行全

局搜索之间的平衡

，可将召回问题转化为逐

层分类并筛选的过程，借

助树的层级检

索性质可

以将时间复杂度降低到

对数级别。假如目标推荐

个数为 ，总商品个数为

，那

么时间复杂度就是 。

如图

12.5

所示，深度树的每一个叶

节点均对应数据中的一

个 item，非叶节点则表示 item 的

集

合。这样一种层次化的结

构直观体现了粒度从粗

到细的 item

架构，此时推荐任

务便转换

成如何从深度

树中检索一系列叶节点

，并且将这些叶节点作为

用户最感兴趣的 item 返回的

问题。值得一提的是，虽然

图 12.5 中展示的树是一个二

叉树，但在实际应用中并

无此限

制。

图 12.5 TDM 深度树结构

当然，除了上述三种，还有

很多召回方式，比如经典

的系统过滤、基于图计算

的召回、基于

知识图谱的

召回等。另外，目前的召回

策略大多是多路召回与

权重检索相结合，在实际

业务

中常常是对十多路

的召回方式进行结合。

12.3.2 DSSM 语

义召回

本节将介绍一种

基于深度神经网络的语

义建模方式——DSSM（深度语义匹

配模型），这由

微软发表的

一篇关于

Query 和 Doc 的相似度计

算模型的论文提出。在广

告召回问题中，这

种多塔

的结构分别为用户侧特

征和广告侧特征构造不

同的塔，在经过多层全连

接之后，将最

后一个输出

层的

embedding 向量拼接在一起然

后输入到 softmax函数。而且输出

向量同处一

个向量空间

，这就可以直接通过点积

或者余弦函数计算 Query 和

Doc 的

相似度并进行广告

检索

。接下来一起看看 DSSM 的网络

结构，如图 12.6

所示。

图 12.6 DSSM 的网络

结构

(1)

首先输入层将 Query（或者

User）向量和 Doc（或者 Item）向量（one-hot 编码）转

化为

embedding

向量，原论文针对英

文输入提出了一种叫作

word hashing的特殊 embedding方法来

降低字典

规模。我们在针对中文进

行 embedding 时，使用

Word2Vec 类常规操作即

可。

(2) 接下来是表示层，完成

embedding 之后的词向量经过多层

全连接映射得到针对 Query

和

Doc 的语义特征向量表示。

(3) 最

后匹配层对 Query 向量和

Doc 向量

做余弦相似度计算得到

相似度，然后进行 softmax

归一化

，得到最终的指标后验概

率 ，训练目标针对点击的

正样本拟合 为

1，反之拟合

为 0。

这种方式目前也广泛

应用于搜索、推荐等领域

的召回和排序问题中。双

塔模型的最大特点就

是

用户侧和广告侧是两个

独立的子网络，两个塔可

以各自缓存，线上召回时

只需要取出缓存

中的向

量做相似度计算即可。在

向量检索匹配的时候，是

一件非常耗时的工作，可

以通过最

近邻搜索的方

法来提高检索效率，使用

Annoy、Faiss

等常用的 python 包可以轻松应

对这类

场景。

12.4 广告排序

广

告排序是计算广告中的

核心部分，其主要功能是

对广告召回模块送来的

广告候选集计算

eCPM（千次展

示收益），并按照所得值的

大小倒排序。对 eCPM 的计算依

赖于受众定向平

台离线

计算好的点击率，由于最

终投放的广告都是来自

于排序的结果，因此这一

模块至关重

要，也是各种

算法模型和策略大展身

手的地方。

12.4.1 点击率预估

点

击率（CTR）预估是帮助进行广

告投放最重要的算法模

块之一，同时在诸如信息

检索、推

荐系统、在线广告

投放系统等工业级的应

用中，点击率预估也是至

关重要的。在一定程度上

点击率代表着用户的体

验效果。例如，在电商平台

的推荐系统中，可以把一

个主要的排序目

标 GMV（商品

成交总额）拆解为流量×点

击率×转化率×客单价，可见

点击率是优化排序目

标

的重要因子。

可以把点击

率预估任务抽象成一个

二分类问题，即向用户投

放一个广告，然后预测用

户点击

广告的概率。当一

个广告展示在消费者面

前时，消费者对此广告的

链接产生点击行为的比

例，按照投放方式的不同

可以体现为两种指标，一

种是点击率：

点击率 = 点击

人数（次）/

曝光人数（次）

显然

，点击率越高代表广告的

投放效果越好。另一种是

转化率（CVR），这是对点击率的

进

一步延伸，代表的是指

消费者在点击广告链接

的基础上，是否进一步完

成了相应的转化行

为，即

转化率是付费交易或者

注册使用的消费者占点

击广告的消费者的比例

，其定义与点击

率相似：

转

化率

= 转化人数（次）/ 曝光人

数（次）

同样地，转化率越高

代表广告的投放效果越

好。

点击率相关问题也经

常出现在竞赛中，比如 IJCAI

2018 阿

里妈妈广告预测算法比

赛、腾讯

广告算法大赛、科

大讯飞 AI 营销算法大赛都

是围绕点击率展开的比

赛，这类比赛经常面临

大

量离散特征和特征组合

等问题。下面将针对这些

问题给出常用的解决方

式，并且对常用模

型进行

介绍。

12.4.2 特征处理

特征工程

在竞赛中一直备受关注

，其中当然也包括点击率

预估问题的特征工程。广

告业务中

的数据不仅丰

富、而且维度非常高，对精

度有极高的要求，除了模

型方面，在特征处理方面

也存在诸多技巧需要学

习。

特征交叉组合

在点击

率问题中存在大量类别

特征，如用户标签、广告标

签等，因此提取细粒度的

特征表达成为关键，如对

用户职业与广告类型进

行组合：程序员 _ 防脱发广

告。当

然还可以对三种类

别特征进行组合，构造更

细粒度的特征。如图 12.7 所示

，是

2019

腾讯广告算法大赛中

进行的特征交叉组合，同

时还进行了数值统计，充

分反映了广

告在不同粒

度下的变化情况，可以看

作一种宏观变化和微观

变化。

图 12.7 特征交叉组合

连

续型特征的处理

连续型

特征具有实际统计意义

，如用户行为次数、广告曝

光量等，虽然这些特征能

直接喂入模型进行训练

。但是，特征在不同区间的

重要程度可能是不一样

的，连续

型特征默认为特

征的重要程度和特征值

之间是线性关系，但实际

中两者往往存在非

线性

关系，即特征值在不同区

间的重要程度是不一样

的。

这里将介绍一种神经

网络模型——键值对存储（Key-Value Memory），实

现从浮点

数到向量的映

射。如图 12.8 所示，可以看到这

个模型的输入是一个稠

密特征 ，输

出是一个特征

向量 ，即实现了从一维到

多维的特征空间转换。

图

12.8 键值对存储模型结构

图

12.8 中的具体步骤如下。

Key addressing

部分

：寻址过程，此处会用到 softmax 函

数，如式 (12-1) 所示。计算

上述选

出的每个

memory 的概率值。

Attention 部分

：不同 Key addression

部分的特征重要性

不同，所以使用 Attention 给

予不同

的权重概率。

Value reading部分：在上一

个步骤的权重下进行加

权求和，得到答案信息。（

）

点

击率平滑

在构造点击率

相关特征时常常会因为

数据稀疏导致计算出现

偏差，比如一个广告投

放

了 100 次，有

2 次点击行为，那么

点击率就是 2%，但是当这个

广告的投放量达到

1000 次的

时候，点击行为只有 10

次，此

时点击率是 1%，这就相差了

一倍了。所以

会通过平滑

处理的方式来修正计算

结果，即把分子、分母都加

上一个比较大的常

数，这

样能缓解曝光量低的数

据、突出曝光量高的数据

，如热门广告及商品，还可

以填充冷启动的样本。

常

使用贝叶斯平滑进行处

理，基本思想是用以平滑

分布选出先验分布，然后

用先验

分布通过某种方

式求出最终的平滑分布

，如式

(12-2) 所示。

其中， 为点击次

数， 为曝光量， 和

是通过贝

叶斯平滑计算得到的。

向

量化表示

很多传统的特

征提取方法表征能力有

限，因此这里尝试用一些

嵌入表示方法（比如

Word2Vec、DeepWalk 等）或

通过深度学习模型来学

得嵌入向量表示，如图 12.9

所

示。例如，提取用户历史点

击广告序列，将所有用户

的序列组合成一个文本

输入到

Word2Vec 中进行广告向量

的训练，最后可以得到广

告向量化的表示。当然也

可以通

过广告曝光用户

序列来得到用户的向量

表示。

图 12.9

广告嵌入向量提

取过程

12.4.3 常见模型

点击率

预估在推荐系统和计算

广告领域中是极其重要

的一环，预估效果会直接

影响到用户体

验和广告

收益。为了能不断提高性

能，相关模型的更新迭代

也是非常快速的。由于这

个领域

具有数据量大、特

征高度稀疏等特点，因此

模型的改进也大多围绕

这些特点进行优化。这里

将对

FM、Wide&Deep、DeepFM 和 DIN 四个较为经典且

从不同方向演化的模型

进行介绍。

FM：Factorization Machines（2010）——隐向量学习提

升模型表达

FM（因子分解机

）可以说是推荐系统和计

算广告领域内非常经典

的一个算法，是在

LR 模型的

基础上改进而得，其模型

结构如图 12.10 所示。LR 模型是广

告点击率或转

化率问题

在早期使用的模型，传统

的 LR 模型并不能学习到特

征之间的交叉信息，而

只

能依靠大量的人工去构

造特征。面对这一困难，FM 应

运而生，它不仅能够直接

引入二阶特征组合，还可

以计算特征组合的权重

。

图

12.10 FM 模型结构

FM 通过学习潜

在特征空间中成对的特

征交互来解决上述的问

题。在特征空间中每

个特

征都有一个与之关联的

隐向量，两个特征之间的

相互作用是它们各自隐

向量的

内积。

在 FM 中，模型可

以表达为式 (12-3)：

其中

由于 FM 会

学习所有交叉组合特征

，其中肯定会包含很多无

用的组合，这些组合会

引

入噪声从而降低模型的

表现，因此一般而言，我们

不能直接将所有类别特

征都放

入模型，应该先进

行一定的特征选择，降低

引入噪声的风险。

另外，FM

在

进行特征组合时学习不

到特征域（Field）的信息，或者说

是感知不到

特征域的存

在，即特征组合均使用相

同的隐向量。很明显，这样

会显得非常粗糙，

属于同

一个特征域和不同特征

域下的特征在进行组合

时，应该使用不同的隐向

量。

为了对此进行改进，FFM（Field-aware Factorization Machine）诞

生了。

FFM 的思想是将原始潜

在空间划分为许多较小

的潜在空间，并根据特征

域使用其中

之一。比如“男

性”和“篮球”、“男性”和“化妆品

”，这两种特征组合潜在的

作用是不一

样的，引入特

征域的概念非常有必要

。在 FFM 中，模型可以表达为式

(12-4)。

其中， 是第 个特征所属的

特征域。如果隐向量的长

度为 ，那么 FFM 的二次

参数有

个，远多于 FM 模型的 个。此外

，由于隐向量与特

征域相

关，因此并不能化简 FFM

表达

式中的二次项，其预测复

杂度是 。

Wide&Deep：Wide and Deep Learning（2016）——记忆性与泛化性

的信息互补

Wide&Deep 模型的核心

思想是结合线性模型的

记忆能力和深度神经网

络模型的泛化

能力，提升

整体模型的性能。能够从

历史数据中学习到高频

共现的特征组合是模型

的记忆能力，模型的泛化

能力代表的则是模型能

够利用相关性、传递性去

探索历史

数据中从未出

现过的特征组合。Wide&Deep 模型兼

备记忆能力和泛化能力

，在

Google

Play store 的场景中成功落地，成

为一种经典模型。

如图 12.11 所

示，Wide&Deep

模型结构由 Wide 部分和 Deep 部

分组成。Wide 部分主

要是一个

广义线性模型（如 LR），线性模

型通常输入 one-hot 稀疏表示特

征或连续

型特征进行训

练，Wide 模型可以通过交叉特

征高效地实现记忆能力

，达到准确推荐

的目的；Deep 部

分简单理解就是嵌入向

量结合多层感知机（MLP）这种

常见的结

构，可以通过学

习到的低维稠密嵌入向

量实现模型的泛化能力

，即使是历史上没有

出现

过的商品，也能得到不错

的推荐。

图 12.11

Wide&Deep 模型结构

Wide&Deep 模型

的训练采用的是联合训

练，其训练误差会同时被

反馈到线性模型和

深度

神经网络模型中进行参

数更新，所以单个模型的

权重更新会受到 Wide

部分和

Deep 部分对模型训练误差的

共同影响。Wide&Deep 模型的数学表

示如式 (12-5)：

其中， 表示偏置，

表

示 Deep 模型的最后一层输出

， 表示原始的输入特

征，注

意还有一个 ，这个是原始

特征的特征交叉。Wide

和 Deep 部分

的输出

通过加权方式合

并到一起，并通过 logistic 损失函

数进行最终输出。

DeepFM：Deep Factorization Machines（2017）——在 FM 基础

上引入神经网络

隐式高

阶交叉信息

DeepFM 与 Wide&Deep 的相同点

是都从 Wide 和

Deep 两部分同时进

行考虑，不同点

是为了能

够更好地捕获交叉特征

信息，DeepFM 使用 FM 作为

Wide 部分的模

型。传

统的线性模型无法

提取高阶组合特征，然而

通过人工挖掘特征交叉

组合达到的效果

又非常

有限，所以使用 FM 作为

Wide 部分

的模型，充分利用其提取

特征交叉组合的

能力。

如

图 12.12 所示，FM

和 Deep 共享输入向量

和稠密嵌入向量，这使得

训练不但更

快，而且更加

准确。相比之下，Wide&Deep 模型中，输

入向量非常大，里面包含

大

量由人工构造的且成

对的组合特征，这无疑会

增加模型的计算复杂度

。

图 12.12 DeepFM 模型结构

DeepFM 论文对此模

型给出的物理意义是，FM

部

分负责一阶特征和二阶

交叉特

征，深度神经网络

部分负责二阶以上的高

阶交叉特征。式 (12-6) 给出了 FM 模

型的

输出公式，式 (12-7) 给出了

DeepFM 模型的预测结果表达式

：

DIN：Deep Interest

Network（2018）——融合 Attention 机制的深度学习模

型

DIN（Deep Interest Network，深度兴趣网络）通过设

计一个局部激活单元（Local

activation unit）来

自适应地从某一特定广

告的历史行为中学习对

用户兴趣的表示，这

大大

提高了模型的表达能力

。

DIN 模型结构如图 12.13

所示，一部

分为基础模型，另一部分

为加入 Attention（注

意力机制）后的

改进模型。基础模型就是

嵌入向量结合多层感知

机，首先将不同的

特征转

换为对应的嵌入向量表

示，然后将所有特征的嵌

入向量拼接到一起，最后

输

入到多层感知机中进

行计算。为了保证给多层

感知机的为固定长度的

输入，基础模

型采用

pooling 的方

式，一般为向量和或向量

平均两种方法，对用户历

史行为序列中

商品的嵌

入向量进行 sum、mean pooling操作。

图

12.13 DIN 模型

结构

但这存在很大的局

限性，对于要预测的每个

推荐商品来说，不管这个

商品是衣服、

化妆品，还是

电子产品等，用户的表示

向量都是确定不变的，这

会造成无差别推

荐。我们

知道在电商场景中，用户

的兴趣是多种多样的，随

着时间的迁移，或者其

他

情况，用户兴趣的转变和

不相关行为的存在都对

点击率预测起不到帮助

作用，所

以要考虑设置不

同的权重，比如根据时间

的变化来改变权重，但这

又不能完全解决

问题。

为

了解决上面提到的问题

，阿里妈妈的算法团队提

出了 DIN。该模型对用户点击

商

品序列的处理做了调

整，其他部分都没有变化

。核心思路是在

pooling部分，将与

推

荐相关的商品的权重

设置得大一些，与推荐不

相关的商品的权重则设

置得小一些，

这是一种 Attention 的

思想，使待推荐商品与点

击序列中的每个商品发

生交互来计算

注意力机

制得分。

DIN 的关键点在于局

部激活单元的设计，DIN 会计

算待推荐商品与用户最

近历史行

为序列中商品

的相关性权重，并将此权

重作为加权系数来对些

行为序列中商品的嵌

入

向量做 sum

pooling，用户兴趣正是由

这个加权求和后的向量

来表示的。用户兴趣

表示

的计算公式如式 (12-8)：

其中 是

用户的嵌入向量， 是待推

荐商品的嵌入向量，

是用

户第 次行为

的嵌入向量

， 就是 Attention，可以看到候选嵌入

向量会和用户行为序

列

中每个商品的嵌入向量

都计算一个权重，最后的

加权求和就是用户兴趣

表示。

看似完美的用户兴

趣表示其实还存在一些

缺点。用户的兴趣是不断

进化的，而 DIN

抽取的用户兴

趣之间是独立无关联的

，并没有捕获兴趣的动态

进化性；另外，通过

用户显

式的行为来表达用户隐

含的兴趣，其准确性无法

得到保证。对此阿里妈妈

的

算法团队提出了 DIEN（Deep

Interest Evolution Network，深度

兴趣演化网络），将

DIN 中的 Attention

与

sum pooling 部分换成了 sequential model 与

attention，不过由

于

网络结构的复杂性，会给

工程上的应用带来困难

。

12.5 广告竞价

提供同一产品

与服务的公司也许有成

百上千，甚至更多个，此时

如何确定它们之间的优

先级

呢？针对此，平台方基

于自身的利益发明了褒

贬不一的竞价排名，简单

来说就是谁出的钱多

就

优先展示谁的广告。这样

将搜索与竞价捆绑起来

的模式有好有坏，逻辑的

简单粗暴固然有

一定效

率，然而效果如何则要打

个问号。尤其是对所有搜

索进行一刀切的做法导

致有些特定

的行业风险

较高，比如某度上的的医

院广告被曝出丑闻，对其

带来的影响异常恶劣。对

于广

告主、平台以及消费

者这三方来说，消费者是

较为弱势的群体，搜索竞

价的策略忽视了广告

的

质量与适应性，平台的搜

索匹配只能达到关键词

的粒度，无法千人千面地

提供个性化服

务，而每个

人都是独一无二的个体

，采用群体性策略必然会

导致消费者体验不佳，同

时广告

主又会在竞价中

拉高成本，并变相转移到

消费者身上，平台纵然能

够一时广告费赚得飞起

，

但也难长久地保持下去

。

通常在广告竞拍机制中

，广告的实际曝光量取决

于广告的流量覆盖大小

和在竞争广告中的相

对

竞争力水平，其中前者取

决于广告的人群定向（匹

配对应特征的用户数量

）、广告素材尺

寸（匹配的广

告位）以及投放时段、预算

等设置项；而影响后者的

因素主要有出价、广告质

量（如 pCTR/pCVR

等），以及对用户体验

的控制策略等。通常来说

，基本竞争力可以用

eCPM = 1000×cpc_bid×pCTR = 1000×cpa_bid×pCTR×pCVR（cpc、cpa

分别

代表按点击付费

模式和

按转化付费模式）来表示

。综上，广告的流量覆盖大

小决定广告能参与竞争

的次数以

及竞争对象，相

对竞争力水平决定广告

在每次竞争中胜出的概

率，二者共同决定广告每

天的

曝光量。

如图 12.14

所示，对

检索到的广告通过 eCMP 计算

进行排序，然后进行多样

性过滤得到待投

放广告

。当然，eCMP 体现的仅是基本的

竞争力，一般还会结合广

告质量等因素进行最终

计

算。

图 12.14 广告竞拍流程

广

告曝光量，顾名思义就是

广告在消费者面前曝光

的数量，既可以按人数，也

可以按人次来

计算，但需

注意计算的维度要与后

续的点击率与转化率保

持一致。曝光量与广告投

放效果之

间的关系并不

是绝对的正比，如图

12.15 所示

，通常意义上来说适度曝

光的效果是最好的，

过少

和过多都不好。在 2019 腾讯广

告算法大赛中就是以预

估未来广告的日曝光量

为任务，

提供历史

天的广

告曝光数据（在特定流量

上采样），包括每次曝光对

应的流量特征（用

户属性

和广告位等时空信息）以

及曝光广告的设置和竞

争力分数。值得一提的是

，本书作者

王贺所的在团

队“鱼遇雨欲语与余”获得

了此次比赛的冠军，在第

13 章将会对这场比赛进行

深入的分析和讲解。

图 12.15

曝

光量与广告投放效果的

关系

本章以实际的计算

广告领域为背景，介绍了

广告系统框架、广告类型

和广告核心技术。从竞

赛

的角度来看计算广告，可

以围绕广告投放的量化

指标进行深入学习，从而

厘清广告主与平

台的成

本与收益，倒逼对模型进

行改善。由于本书着重讲

解机器学习竞赛相关实

战，因此主

要考虑计算广

告在线上的投放与应用

，而在这个领域有一些专

属的评价指标来评估投

放的效

果。一项产品和服

务，从概念的诞生到落地

到消费者身上，这一过程

相对来说较为漫长，牵

涉

众多。而与之相应的计算

广告主要分为三个阶段

，首先是要让广告展示在

消费者面前，也

就是所谓

的曝光，让消费者看到该

广告；其次对此广告感兴

趣的消费者会点击它，对

广告内

容即相应产品与

服务进行浏览；最后有购

买欲望的消费者会进行

相应的付费交易或者注

册使

用。为了便于广告主

和平台对广告投放的效

果进行分析统计，三个阶

段分别设有曝光量、点

击

率和转化率这三项指标

，虽然三者的定义不相同

，但都根植于进行相应操

作的消费者数

量。除了曝

光是消费者无法自主选

择的之外，点击和转化都

是消费者的自发行为。

12.6 思

考练习

01. 计算广告与普通

广告有哪些显著的异同

点？

02. 请简述搜索竞价模式

下，计算广告的优劣之处

。

03. 请思考平台应如何通过

广告投放的各项指标与

广告主协商广告费定价

策略？

04. 仔细观察你最常使

用的 10 个

App 给你推送的广告

页面，能找出它们之间的

共同点

吗？

05. GBDT 与

LR 的结合也是

点击率预估中的经典模

型，那么两者具体是如何

结合的？

06. 计算广告中还有

一个重要部分没有讲到

，即广告检测，这部分常常

伴随着作弊与反

作弊的

问题出现，那么具体的建

模方式又是什么样的？

第

13

章 实战案例：2018 腾讯广告算

法大赛——相似人

群拓展

本

章将以 2018

年第二届腾讯广

告算法大赛（如图 13.1 所示）为

例，对计算广告相关的实

战

案例进行分析，会详细

地讲解完整的实战流程

与注意事项。本章主要分

为六个部分，分别是

赛题

理解、数据探索、特征工程

、模型训练、模型融合和赛

题总结。这不仅是本书所

有实战

案例章节的组织

结构，也是一场竞赛的重

要组成流程。相信在本书

的指引下，你能够快速地

熟悉竞赛流程，并且进行

实战。

图 13.1 2018 腾讯广告算法大

赛

13.1 赛题理解

所谓磨刀不

误砍柴工，在竞赛前应该

充分了解与赛题相关的

信息，理解赛题背后的需

求，进

而达到正确审题的

目的。本次赛题的题目为

基于计算广告问题的相

似人群拓展。参赛者的幸

运之处在于竞赛主办方

拥有国内最大的社交平

台，无论是其提供数据的

质量还是举办竞赛的

专

业度都是无可挑剔的，本

节内容也多来自腾讯官

方给出的赛题说明。

13.1.1 赛题

背景

基于社交关系的广

告（即社交广告）已经成为

互联网广告行业中发展

最为迅速的广告种类之

一。腾讯社交广告平台是

依托于腾讯丰富的社交

产品，植根于腾讯海量的

社交数据，借助强

大的数

据分析、机器学习和云计

算能力打造出的一个服

务于千万商家和亿万用

户的商业广告

平台。腾讯

社交广告平台一直致力

于提供精准高效的广告

解决方案，而复杂的社交

场景、多

样的广告形态以

及庞大的用户数据给实

现这一目标带来了不小

的挑战。为攻克这些挑战

，腾

讯社交广告平台也在

不断地试图寻找更为优

秀的数据挖掘和机器学

习算法。

本次算法大赛的

题目源自腾讯社交广告

业务中一个真实的广告

产品——相似人群拓展（后面

称作 Lookalike）。该产品的目的是基

于广告主提供的目标人

群，从海量人群中找出和

目标

人群相似的其他人

群，已达到拓展人群的作

用，如图 13.2 所示。

图 13.2 相似人群

拓展

在实际的广告业务

应用场景中，Lookalike 能够基于广

告主已有的消费者，从目

标消费者中

找出和这些

已有消费者相似的潜在

消费者，以此有效地帮助

广告主挖掘新客户、拓展

业务。

目前，Lookalike 以广告主提供

的第一方数据以及广告

投放的效果数据（即后面

提到的种子

人群）为基础

，结合腾讯丰富的数据标

签，透过深度神经网络的

挖掘，实现了在线实时为

多

个广告主同时拓展具

有相似特征的高品质潜

在客户的功能。如图 13.3 所示

为 Lookalike 的工

作机制。

图 13.3 Lookalike 的工作

机制

13.1.2 赛题数据

本次竞赛

的比赛数据（已脱敏处理

）抽取的是某连续 30 天的数

据。总体而言，数据文件可

以分为训练集文件、测试

集文件、用户特征文件以

及种子包对应的广告特

征文件四部分，下

面分别

对这四部分进行介绍。

训

练集文件 train.csv：此文件中的每

一行分别代表一个训练

样本，且各字段之间以

逗

号分隔，格式为“aid,uid,label”。其中，aid 用于

唯一标识一个广告；uid 用

于

唯一标识一个用户；label 表示

样本标签，其取值为 +1

或 -1，+1 表

示种子用

户，-1 表示非种子

用户。为了简化问题，一个

种子包仅对应一个 aid，两者

为一一

对应的关系。

测试

集文件 test.csv：此文件中的每一

行分别代表一个测试样

本，且各字段之间以逗

号

分隔，格式为“aid,uid”。其中两个字

段的含义同训练集文件

。

用户特征文件 userFeature.data：此文件中

的每一行分别代表一个

用户的特征数

据，各字段

之间用竖线“|”分隔，格式为

“uid|features”。其中，features 由多

个 feature_group 组成，每个 feature_group

分

别代表一个特征组，多个

特征

组之间也以竖线“|”分

隔，格式

为“feature_group1|feature_group2|feature_group3|...”。若一个特征

组

包括多个值，则以空格分

隔，格式为“feature_group_name fea_name1

fea_name2

...”，里面的 fea_name 采用数

据编号的格式。

种子包对

应的广告特征文件 adFeature.csv：此文

件中每一行的格式为“aid,

advertiserId,

campaignId, creativeId, creativeSize,

adCategoryId, productId, productType”。其

中，第一个字段

aid 用于唯一

标识一个广告，其余字段

为广告特征，这些字段之

间以逗号分隔。出于对数

据安全

的考虑，我们对 uid、aid、用

户特征、广告特征按照如

下方式进行加密处理。

uid：对

每个 uid

进行从 1 到 的随机化

编号，生成一个不重复的

加密

uid， 为用户总数目（假设

用户数目为

100 万，将所有用

户随机打散排

列，把排列

后的序号作为用户的 uid，序

号的取值范围就是 [1, 100

万 ]）。

aid：参

考 uid 的加密方式，生成加密

后的 aid。

用户特征：参考 uid 的加

密方式，生成加密后的 fea_name。

广

告特征：参考 uid

的加密方式

，生成加密后的各字段。

接

下来对用户特征和广告

特征的取值进行说明。

用

户特征的取值说明

年龄

（age）：分段表示，每个序号表示

一个年龄分段；

性别（gender）：男、女

；

婚姻状况（marriageStatus）：单身、已婚等（多

个状态可以共存）；

学历（education）：博

士、硕士、本科、高中、初中、小

学；

消费能力（consumptionAbility）：高、低；

地理位

置（LBS）：每个序号分别代表一

个地理位置；

兴趣类目（interest）：对

不同数据源进行挖掘，得

到 5 个兴趣特征组，

分别以

interest1、interest2、interest3、interest4、interest5 表

示，每个兴趣特征组均

包含若干个兴趣 ID；

关键词

（keyword）：对不同数据源进行挖掘

，得到 3

个关键词特征组，分

别以 kw1、kw2、kw3 表示，每个关键词特

征组均包含若干个用户

感兴趣的

关键词，能比兴

趣类目更细粒度地表示

用户喜好；

主题（topic）：使用 LDA

算法

挖掘的用户喜好主题，具

体地，对不同数据

源进行

挖掘，得到 3 个主题特征组

，分别以 topic1、topic2、topic3 表

示；

App 近期安装行

为（appIdInstall）：包括最近 63 天内安装的

App，其

中每个

App 均用一个唯一

的 ID 表示；

活跃的 App（appIdAction）：用户活跃

度较高的

App 的 ID；

上网连接类

型（ct）：Wi-Fi、2G、3G、4G；

操作系统（os）：Android、iOS（不区分版本

号）；

移动运营商（carrier）：移动、联通

、电信、其他；

有房（house）：有房、没房

。

广告特征的取值说明

广

告 ID（aid）：广告 ID 对应的是具体广

告，广告是指广告主创建

的广告

创意（或称广告素

材）及广告展示相关的设

置，包含广告的基本信息

（广告

名称、投放时间等）、推

广目标、投放平台，投放的

广告规格、投放的广告

创

意、广告的受众（即广告的

定向设置）以及广告出价

等信息；

广告主 ID（advertiserId）：账户结构

分为账户、推广计划、广告

、素材

四级，广告主和账户

是一一对应的关系；

推广

计划 ID（campaignId）：推广计划是广告的

集合（类似于电脑的文件

夹功能），广告主可以将推

广平台、预算限额、是否匀

速投放等条件相同的

广

告放在同一个推广计划

中，以便管理；

素材 ID（creativeId）：直接展

示给用户的广告内容，一

条广告下可以有

多组素

材；

素材大小（creativeSize）：素材大小 ID，用

于标识广告素材的大小

；

广告类目（adCategoryId）：广告分类 ID，使用

广告分类体系；

商品 ID（productId）：推广

的商品

ID；

商品类型（productType）：广告投

放目标对应的商品类型

（比如京东对应

商品、App 对应

下载）。

13.1.3 赛题任务

Lookalike 基于广告

主提供的一个种子人群

（又称为种子包），自动计算

出候选人群中与之

相似

的人群，称为拓展人群。本

赛题将为参赛选手提供

几百个种子人群、海量候

选人群对应

的用户特征

以及种子人群对应的广

告特征。出于对业务数据

安全性的考虑，所有数据

均经过

了脱敏处理。整个

数据集分为训练集和测

试集。训练集中标定了候

选人群中属于种子人群

的

用户与不属于种子人

群的用户（即正负样本）。模

型预测时将检测参赛选

手的算法能否准确

标定

测试集中的用户是否属

于相应的种子包。训练集

和测试集对应的种子人

群完全一致。如

图 13.4 所示，为

人群分布情况。

图 13.4

人群分

布情况

为检验参赛选手

的算法是否能够很好地

学习用户以及种子人群

，本次大赛要求参赛者提

交的

结果中包含各种子

人群的候选用户属于该

种子人群的得分（得分越

高，候选用户是这个种子

人群潜在的拓展用户的

可能性就越大）。初赛和复

赛提供的种子人群除了

量级有所不同外，

其他设

置均相同。

13.1.4 评价指标

如果

对拓展后的相似用户进

行广告投放后产生了相

关的效果行为（比如点击

或者转化），则

认为其是正

例；如果没有产生效果行

为，则认为其是负例。每个

待评价的种子人群都会

提供

如下信息：种子人群

对应的广告 ID（aid）、广告特征以

及对应的候选人群集合

（包含每个

候选用户的 uid

及

用户特征）。参赛选手需要

为每个种子人群计算测

试集中用户的得分，

比赛

会据此计算每个种子人

群的 AUC 指标，并以所有待评

价的 个种子人群的平均

AUC

值作为最终评价指标，式

子如式

(13-1)：

其中 表示第 个种

子人群的 AUC 值。

13.1.5 赛题 FAQ

Q 此次赛

题的本质任务是什么？

A

赛

题的本质任务是通过以

前的广告推送与用户点

击记录，对未来的广告推

送进行精准用

户匹配，提

高用户对所推送广告的

点击率，进而提高转化率

，为广告主带来商业价值

，并收

取广告营销费用。

Q 互

联网广告方面的评价指

标有几个，这些指标之间

有什么关联？

A

第一个指标

是曝光量，指广告在用户

面前曝光的次数，即对多

少用户进行了广告的推

送

展示；第二个指标是用

户看到广告后的点击量

，即点击并进入广告页面

的用户有多少；第三

个指

标是转化量，若用户看到

广告并且购买了相应的

商品，则这部分用户的数

量就是转化

量。可以看出

，曝光量、点击量与转化量

呈一个倒金字塔结构，即

依次递减。当然，曝光广

告

除了能给广告主带来直

接的用户转化外，变相也

是对广告主的品牌与知

名度的营销。

13.2

数据探索

本

节将对竞赛提供的可用

信息与数据进行分析解

读，探索可能的建模思路

。通常来说，在内

存允许的

情况下，参赛者一般可以

借助 jupyter notebook、pandas 和

numpy 等常见的 python

三方开

源包进行数据探索，依据

分析需要的不同可以借

助不同的函数，其中 pandas 包常

用

的函数有 read_csv()、head()、describe()、value_counts()、plot()、shape

等。

13.2.1 竞赛的公

开数据集

以初赛数据为

例，提供的数据集文件有

train.csv（训练集）、test1.csv（测试集）、

test1_truth.csv（测试集标

签）、adFeature.data（广告基本属性）、userFeature.csv（用户基

本信息）。

13.2.2 训练集与测试集

训练集与测试集中只给

出了 ID 列与标签列，关于这

部分，腾讯公开提供的数

据集中也给出

了测试集

的真实标签，参赛者需要

明确这是一个双主键的

用户与广告匹配问题，因

此可以适

当查看训练集

与测试集中 aid 与 uid 的重叠情

况以判断训练集分布与

测试集分布的差异。

分布

差异

首先需要确认训练

集与测试集中均无缺失

值，且训练集中正样本的

比例为 4.8%，这

应该是经过一

定采样后得到的数据，实

际业务中的点击率很难

达到这个水平。然后

分别

合并、统计训练集与测试

集中 aid 和

uid 的去重唯一取值

数，如表 13.1 所示。

表 13.1

uid 和 aid 的分布

情况

train_nunique test_nunique

all_nunique duplicates inbag_ratio

uid 7883466 2195951

9686953 392464 18%

aid 173 173

173 173 100%

可以看出，测试集中

的 uid 只有不到

18% 出现在训练

集中，而测试集与训练集

中的

aid 出现得全部一样。其

实这也符合商业逻辑，即

在广告投放种类短时间

内保持一

致的情况下，通

过现有投放的点击效果

来对未推送过的用户进

行概率匹配预测，进

而提

升点击量，带来商业收益

。

检查完单主键的取值差

异之后，需要确认双主键

的取值也唯一，即确认 aid 与

uid

的组合是唯一的，这里的

唯一表示只有一个确定

的标签取值，代码验证如

下：

train_nunique =

train[['uid', 'aid']].drop_duplicates().shape[0]

test1_nunique = test1[['uid', 'aid']].drop_duplicates().shape[0]

all_nunique = test1[['uid', 'aid']].append(train[['uid',

'aid']]).drop_duplicates().shape[0]

assert

train_nunique == train.shape[0]

assert test1_nunique ==

test1.shape[0]

assert train_nunique + test1_nunique ==

all_nunique

最后，根据上述分析，还

缺一点逻辑闭环就是训

练集与测试集中投放的

广告 ID 分布

是否相同，验证

结果如图 13.5

所示。

图 13.5 训练集

与测试集中投放的广告

ID 分布情况

由图

13.5 可见，训练

集与测试集中的广告分

布基本一致。因此，重点需

要考察不同

用户对同一

广告的感兴趣程度，或者

说需要参赛者找出同一

广告的用户群特点，继

而

通过已有的点击数据挖

掘更多可能对该广告感

趣的用户，这便是本次竞

赛的主题

——相似人群拓展

。

13.2.3

广告属性

使用 pandas.DataFrame().head() 方法对基

本数据进行展示，如图 13.6 所

示，虽然数

据全部进行了

脱敏处理，但并不妨碍参

赛者理解各字段的含义

，13.1.2 节列出了详细的广告

特

征说明，参赛者可结合说

明自行查看数据。

图 13.6 广告

属性数据展示

13.2.4 用户信息

由于用户特征文件的格

式为 .data，不利于参赛者直接

进行分析统计，因此先将

其转换为 .csv

格式的文件，具

体操作代码如下：

#

判断文

件路径是否已存在

if os.path.exists('data/preliminary_contest_data/userFeature.csv'):

user_feature=pd.read_csv('data/preliminary_contest_data/userFeature.csv')

else:

userFeature_data

= []

with open('data/preliminary_contest_dataa/userFeature.data', 'r') as

f:

for i, line in enumerate(f):

line = line.strip().split('|')

userFeature_dict = {}

for each in line:

each_list =

each.split(' ')

userFeature_dict[each_list[0]] = ' '.join(each_list[1:])

userFeature_data.append(userFeature_dict)

if i % 1000000 ==

0:

print(i)

user_feature = pd.DataFrame(userFeature_data)

user_feature.to_csv('data/preliminary_contest_data/userFeature.csv',

index=False)

将原

始数据转换成 pandas.dataframe 格式后，分

析就变得很方便了，由于

字段过多，这里只

截图展

示部分字段，如图 13.7

所示。其

中除了用户标识 uid 之外，其

他字段均为用户属

性，用

户属性又分为单变量属

性和多变量属

性，age、gender、marriageStatus、education、consumptionAbility、LBS 是

每个

用户只有一个取值的单

变量属性，interest2、interest5、kw2 是每个用户会

有多

个取值的多变量属

性。对多变量属性的处理

会借助自然语言处理相

关的算法，这将在 13.3

节中着

重讲解。

图

13.7 用户基本特征

的展示

13.2.5 数据集特征拼接

熟悉了训练集、测试集、广

告属性、用户信息之后，参

赛者就可以明确了解这

几个表格文件

之间的关

系，即以训练集和测试集

的 ID

列为基础，对广告属性

与用户信息进行关联，形

成

常规意义上的带有 ID 列

与标签的特征宽表，除了

多变量属性特征可能还

需要额外处理之

外，其余

特征可以直接用于建模

。

由于原始数据相对较大

，对于部分刚入门的参赛

者来说，可能手上还没有

足够的算力资源，

因此为

了方便参赛者快速理解

并且运行成功 demo，本书在这

一环节对训练集与测试

集进行

了 1% 的随机采样，使

得大数据问题转换为小

数据问题，参赛者能够快

速进行相关的数据探

索

、特征工程以及模型搭建

。此处方案确定后，如果有

足够的资源，就可以进行

全量数据建

模。下面是随

机采样和数据拼接的代

码实现：

train = train.sample(frac=0.01, random_state=2020).reset_index(drop=True)

test1

= test1.sample(frac=0.01, random_state=2020).reset_index(drop=True)

test1['label'] = -2

# 从中取出现有训

练集与测试集的用户信

息

user_feature = pd.merge(train.append(test1), user_feature,

how='left', on='uid').reset_index(drop=True)

# 拼接广告信息

data =

pd.merge(user_feature, ad, how='left', on='aid')

# 进行标

签的转换，方便区分训练

集与测试集

data['label'].replace(-1, 0, inplace=True)

data['label'].replace(-2, -1, inplace=True)

同时，为了建

模方便，需要将样本标签

中代表负样本的 –1 用 0 代替

，同时记录测试集的真

实

标签，以便后续建模进行

验证对比，标签分布展示

见图

13.8。

图 13.8 标签分布展示

然

后根据特征的单变量、多

变量属性对特征类别进

行区分，区分方法如下：

cols

= train.columns.tolist()

cols.sort()

se = train[cols].dtypes

# 多

变量特征

text_features = se[se=='object'].index.tolist()

#

单变量特征

discrete_features = se[se!='object'].index.tolist()

discrete_features.remove('aid')

discrete_features.remove('uid')

discrete_features.remove('label')

最

后可以得出，数据集的多

变量特征 text_features 共有 16 个，分别为

appIdAction、appIdInstall、ct、interest1、interest2、interest3、interest4

单变量特征 discrete_features 则有 14 个，分别

为

LBS、adCategoryId、advertiserId、age、campaignId、carrier、consumptionAbility

由于对不同类型特征

的处理方式具有很大的

差异，因此进行简单的区

分有便于后面开展更加

高效的工作。

13.2.6 基本建模思

路

通过简单的数据探索

以及表格文件的拼接处

理，参赛者应该可以感知

到这个数据结构是十分

清晰的，其实就两类特征

，即多变量文本特征 text_features 与单

变量离散特征

discrete_features，因此本章

将考虑一种新颖的建模

思路，即引入可以直接支

持

text_features 的 CatBoost 模型进行建模。

13.3

特征

工程

本节将在数据探索

的基础之上，进行一些特

征提取，本次竞赛的数据

极具代表性，除了 ID

列和标

签列以外，其余列均为特

征列，而且这里的特征列

均为离散列，包含多变量

特征与单

变量特征两种

。这种数据组织形式与第

8 章的组织形式是两种典

型的场景，第

8 章的原始数

据是用户的一些行为记

录，在建模之前需要进行

相应的特征设计与提取

。当然并不是说本章

的实

战案例就不需要特征设

计与提取了，只是在这里

的特征工程会和第 8 章的

有一些不同。

本节将以

2018 腾

讯广告算法大赛的数据

为例，介绍另一套常用的

特征设计与提取方案，其

中经典特征与业务特征

均是对单变量字段的信

息进行提取处理，而文本

特征则针对的是多变

量

字段。

13.3.1 经典特征

直观来讲

，普通模型（比如

LR、RF、GDBT 等）在训练

时没办法区分和处理单

变量离散特

征，因此需要

对这类特征进行变换，使

其能够被具有大小意义

的连续列表征，继而采用

模型

进行量化区分与学

习。本节将介绍三类常见

统计特征的含义以及提

取方式。

count 特征

这是一种简

单的计数特征，可以衡量

某个单变量离散字段的

出现频次，表示样本的

某

个属性是属于大众还是

偏小众，通常使用 pandas.series 的 value_counts()

方法

进行频次统计，count

编码特征

与 13.3.3 节介绍的用于多变量

字段的

countVectorizer 相对应，在长尾分

布等数据取值分布中体

现得最为明显。体现在本

次

竞赛中，count

编码特征被称

为曝光量特征，既可以是

单个字段的曝光量，也可

以是

多个字段的组合的

高阶曝光量。下面以本次

数据集中的部分单变量

特征字段为例，

给出原始

输入数据以及输出的曝

光量特征。

如图 13.9 展示的是

部分输入数据：

图 13.9 训练集

原始输入数据

如图 13.10 展示

的是部分输出数据：

图 13.10 输

出的曝光量特征

以 exposure_age 字段

为例，年龄

age 的取值样例有

1、2、5，对应的数量分别为

26029、25245、26179，其余

字段含义相似。exposure_age_and_gender 则是

age 与

gender 的

组合计数。计算单变量的

曝光量特征只需用到一

个单变量特征字

段，计算

二阶以上的曝光量特征

则需要用到两个单变量

特征字段。这样，原本不具

有大小关系的取值被映

射成了数量值，在直观上

就能代表用户在比如年

龄这一维度

是属于大众

还是偏小众，参赛者应该

能想到在一定程度上这

可以反映用户的年龄区

分度，在此基础上甚至还

可以计算三阶及以上的

特征，当然容易导致维度

爆炸，这

也是自然语言处

理领域有关文本特征提

取的 N-Gram

算法的精髓。

nunique 特征

第

二类特征是属性值个数

特征 nunique，指的是两个单变量

字段相交后属性值的个

数，这两个单变量可以有

包含关系，也可以相互独

立。通常来说，当两个单变

量具

有包含关系且不同

分支之间的属性值个数

差异较大时，建模效果比

较明显。举个例

子，若用户

的地理位置属性，即 LBS 中含

有脱敏的城市 ID 与地铁线

路信息，由于

一般认为地

铁线路多的城市，经济更

繁华，人口可能也更多，因

此就可以给用户增

加额

外的由地理位置属性挖

掘出的信息。

输出的 nunique 特征

如图 13.11 所示，观察其中的

nunique_adCategoryId_in_LBS 特

征列，在某种程度上此列

能够反映不同 LBS

的 adCategoryId 分布范

围。这是在

adCategoryId 层面对 LBS 进行的

表征，反

过来也可以从 LBS

层

面对 adCategoryId 进行表征。然而这部

分信息同样只是一阶

表

达，不管是这类特征的包

含项，还是被包含项都可

以进行更高阶的延伸。

图

13.11 输出的

nunique 特征

ratio 特征

结合上

面在 count

编码特征部分提到

的构造二阶特征时两个

特征之间的交互，可以构

造 ratio 特征。count 编码特征与 nunique 特征

的计算结果取值均为整

数，和这两个不

同，计算 ratio 特

征得到的值为 0 和 1

之间的

小数。如果说 nunique 特征能够反

映特

征的分布范围影响

，那么 ratio 特征则可以反映占

比程度，或者说偏好程度

。

13.3.2 业务特征

在 13.3.1 节介绍了 count

编

码、nunique 和 ratio 三种经典特征，本节

将结合本次赛题介绍另

一种需要用到标签的统

计特征，即业务特征。这个

特征在分类模型里面都

有可能会用到，实

际上是

每个离散字段的不同取

值的标签分布比例，体现

到本次竞赛，便是点击率

。

点击率特征

在介绍点击

率特征之前，首先要明确

过拟合以及泄露这两个

概念。过拟合是指模型

在

训练时对训练集学习过

度，导致其泛化性能较差

，在训练集与测试集的分

布特别

是特征与标签的

联合分布差异较大时，泛

化性能尤其差。泄露是指

模型在训练时，

特征里面

掺杂了标签信息，导致在

某种程度上标签化为了

特征的一部分，因此模型

在学习时效果极佳，然而

问题是测试集的标签是

未知的且可能存在分布

差异，这同

样会导致模型

的泛化性能不佳甚至极

差，也会导致过拟合。因此

，在使用标签进行

相关特

征的提取和处理时要极

度小心，既要加强特征对

标签的表达，又不能使表

达

过度，这会导致标签信

息的过拟合和泄露。

为了

在一定程度上避免标签

泄露，可以借鉴五折交叉

验证的思想进行交叉点

击率统

计，这样得到的每

个样本的点击率特征就

未使用其标签的信息。具

体算法步骤如

下：

(1) 将训练

集随机分成

等份；

(2) 第 (1) 步得

到的每一份训练集对应

的点击率特征都由其余

份训练集进行

统计映射

，同时得到一个测试集的

点击率特征映射结果；

(3) 第

(2) 步完成后，可以得到整个

训练集对应的点击率特

征，同时对 次不同的

份训

练集对测试集的点击率

特征映射结果取均值，就

能得到测试集对应的点

击率特征。

接下来给出具

体的实现代码，需特别注

意此处仅给出了一阶点

击率特征，即直接对

原始

类别特征进行构造，没有

给出构造类别特征交叉

组合后的点击率特征。

# 步

骤1

n_parts = 5

train['part'] = (pd.Series(train.index)%n_parts).values

for co in

cat_features:

col_name = 'ctr_of_'+co

ctr_train =

pd.Series(dtype=float)

ctr_test = pd.Series(0, index=test.index.tolist())

#

步骤2

for i in range(n_parts):

se

= train[train['part']!=i].groupby(co)['label'].mean()

ctr_train = ctr_train.append(train[train['part']==i][co].map(se))

ctr_test

+= test[co].map(se)

train_df[col_name] = ctr_train.sort_index().fillna(-1).values

test_df[col_name]

= (ctr_test/5).fillna(-1).values

13.3.3 文本特征

前面两

节介绍了有关单变量离

散字段的一些特征提取

方式，然而本章介绍的竞

赛中还有一类

是多变量

离散字段，如兴趣、关键词

和主题等，如何对这类字

段进行处理和特征工程

也是非

常值得探讨的一

点，本节将引入自然语言

处理的相关算法，将这类

字段当作文本特征进行

处

理。图 13.12 展示了兴趣字段

。

图 13.12

多值特征 interest1

下面给出提

取文本特征前的基本准

备工作，主要是导入库和

初始化数据集：

from scipy import

sparse

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.preprocessing import OneHotEncoder,LabelEncoder

from sklearn.decomposition

import TruncatedSVD

train_sp = pd.DataFrame()

test_sp

= pd.DataFrame()

先介绍 scipy 库

的稀疏矩阵结构，这是一

种不同于 pandas.DataFrame()

的数据存储方

式，稀疏矩阵特点在于它

的总维度很高，但是每个

用户只在其中一小部分

存在取值，因此在

保持超

高维度的同时又不会占

用过多的内存。接下来将

从三个方面生成稀疏矩

阵特征。

OneHotEncoder

OneHotEncoder 也叫独热编码，是

指对单变量离散字段进

行编码处理，形成稀疏矩

阵

结构。简单来说，就是把

一个唯一值个数为

的单

变量离散字段变成 维的

0、1

向量，然后存储为稀疏矩

阵结构，在这里使用 DataFrame 的格

式进行代码实现：

ohe

= OneHotEncoder()

for feature in cat_features:

ohe.fit(train[feature].append(test[feature]).values.reshape(-1, 1))

arr = ohe.transform(train[feature].values.reshape(-1, 1))

train_sp = sparse.hstack((train_sp, arr))

arr =

ohe.transform(test[feature].values.reshape(-1, 1))

test_sp = sparse.hstack((test_sp, arr))

在经过

独热编码后，原本不具有

量化大小关系的单变量

离散字段就转换成了由

0、1

表示的多个连续字段，可

以直接用于逻辑回归模

型（LR）之类不直接支持离散

字段

的模型。

CountVectorizer

同样地，既然

单变量离散字段可以进

行 0、1

值连续特征的转换，那

么多变量离散字

段同样

有对应的转换方式，即 CountVectorizer。直

观上理解，就是对多变量

的每个字

段分别进行计

数，表示样本在某个取值

上的出现次数。当然，本次

竞赛数据由于单

个用户

在如 interest

特征上的多个取值

并不会重复，所以转换后

的取值依然只是

0 或 1，下面

给出具体的实现代码：

cntv=CountVectorizer()

for

feature in text_features:

cntv.fit(train[feature].append(test[feature]))

train_sp =

sparse.hstack((train_sp, cntv.transform(train[feature])))

test_sp = sparse.hstack((test_sp, cntv.transform(test[feature])))

TfidfVectorizer

TfidfVectorizer 是

一种和词频有关的统计

向量，和 CountVectorizer 的相同之处是特

征

维度一致，不同之处是

CountVectorizer

计算的是某个属性在不

同维度上的数量值，

而 TfidfVectorizer 计

算的是频率，某个属性的

重要性随着它在某个样

本中出现的次数

成正比

增加，但同时会随着它在

整个数据集中出现的频

率成反比下降。具体代码

如

下，需要特别注意

TfidfVectorizer() 中是

包含参数的，只不过这里

是默认参

数，即不做设定

。

tfd = TfidfVectorizer()

for feature in text_features:

tfd.fit(train[feature].append(test[feature]))

train_sp

= sparse.hstack((train_sp, tfd.transform(train[feature])))

test_sp = sparse.hstack((test_sp,

tfd.transform(test[feature])))

看到这里，参赛者可能自

然而然地会产生一个疑

问，那就是本节这样的处

理办法无

疑会产生超高

维度的特征，可能会造成

性能上的问题。针对这一

风险，除了采用稀

疏矩阵

作为数据的存储结构之

外，还有一种辅助办法便

是进行一定程度的降维

，去

除冗余的极度稀疏的

维度或者通过特征变换

将特征映射到低维空间

，从而实现对计

算速度与

内存占用的优化。

13.3.4 特征降

维

TruncatedSVD

sklearn（scikit-learn）是一个强大的机器学

习 Python 开源包，里面包含各种

常用的模

块，其中特征分

解模块含有多个针对特

征降维的算法，用于处理

不同种类和形式的

特征

。本书为了方便参赛者快

速熟悉算法流程和技巧

，事先对比赛数据（约 10W 量

级

的数据）进行了采样，然而

经过文本特征处理的参

赛者会发现其特征维度

爆炸到

了

25W +，这会给建模带

来极大的性能挑战，因此

可以考虑先进行一定程

度的降

维。sklearn 包中的 decomposition 模块就

有对稀疏矩阵结构进行

降维的

TruncatedSVD 算子，可以指定主

成分的特征数量进行矩

阵输出，其用法与文本特

征

的处理算子类似。下面

是 TruncatedSVD 使用方式的代码实现

：

svd

= TruncatedSVD(n_components=100, n_iter=50, random_state=2020)

svd.fit(sparse.vstack((train_sp, test_sp)))

cols = ['svd_'+str(k) for k in

range(100)]

train_svd = pd.DataFrame(svd.transform(train_sp), columns =

cols)

test_svd = pd.DataFrame(svd.transform(test_sp), columns =

cols)

除了 SVD 以外，还有很多可以

使用的降维方法，比如 PCA（Principal Components

Analysis，主

成分分析）、LDA（Linear Discriminant Analysis，线性判别分析

）和

NMF（Non-negative Matrix Factorization，非负矩阵分解）等，这些

方法在具体的降

维过程

中又存在很大的差异，说

明不同降维方法存在共

同使用的可能性。

13.3.5 特征存

储

需要注意，在竞赛中为

了取得更好的成绩，通常

会将测试集中的字段信

息加入对特征的计算

处

理，但在实际业务运用中

，这种做法是不可能实现

的，有的竞赛也会明确要

求不能使用测

试集的字

段信息进行特征工程。经

过前几节的特征处理之

后，除了原始的数据特征

，还生成

了其他三个特征

文件。如图 13.13 所示为对所有

特征文件的描述。

图 13.13 对特

征文件的描述

13.4 模型训练

13.4.1 LightGBM

LightGBM 模型在训练时可以支持

类别特征，但前提是需要

先进行 LabelEncoder

编码处理。

feature（特征）模

块包含单变量离散字段

的 LabelEncoder，SVD 是降维处理后的多变

量离散

字段的稀疏矩阵

特征，将这两者与 LightGBM

模型结

合起来，采用五折交叉验

证的方式训练

模型，最后

模型的验证集评价分数

是 0.67922（AUC 指标），测试集评价分数

为 0.61864。

可以明显看到训练集

有过拟合现象，即测试集

评价分数远低于验证集

评价分数，有可能是

feature 模块

里面的特征过拟合以及

SVD 降维后，信息丢失过于严

重造成的。

13.4.2 CatBoost

Catboost

也是最常用的

模型之一，因为它直接支

持文本特征即多变量字

段特征的处理和建

模，只

用原始数据便可以进行

训练和建模。同样是采用

五折交叉验证的方式训

练模型，其模

型的验证集

分数是 0.64900，测试集分数为 0.66501。

13.4.3

XGBoost

CatBoost 能

够直接支持文本特征（text_features）和

类别特征（cat_features）的原因是其在

模

型内部对这些字段进

行了稀疏处理。因此可先

在外层利用 sklearn 包的相关算

子进行处理，

之后再使用

XGBoost 进行建模，其模型的验证

集分数是 0.67905，测试集分数为

0.67671。

13.5 模型融合

13.5.1

加权融合

按照

测试集的分数进行简单

的加权融合，具体计算方

式为 RandomForest 结果×0.2 +

LightGBM

结果×0.3 + XGBoost 结果×0.5，其模

型的验证集分数是 0.68147，测试

集分数为

0.68208。可以看到加权

融合的效果还是比较明

显的，同时还不需要复杂

的操作。

13.5.2 Stacking 融合

Stacking结构可选择

性是比较多的，在本赛题

中，我们选择以 LightGBM 和

XGBoost 模型的

验证结果和预测结果作

为特征值，CatBoost 模型则作为最

终模型，进行训练、预测，这

是因

为 CatBoost 模型即使在只利

用原始特征的情况下，也

能获得不错的预测效果

，具备比较强

的预测能力

。Stacking融合作为经常被用到的

部分，下面将具体展现其

具备通用性的实现代

码

：

def stack_model(oof_1, oof_2, oof_3,

pred_1, pred_2, pred_3, y, eval_type='regression'):

#

oof_1、oof_2、oof_3 为三个模型的验证集结

果

# pred_1、pred_2、pred_3 为三个模型的测试集

结果

#

y 为训练集真实标签

，eval_type 为任务类型

train_stack = np.vstack([oof_1,

oof_2, oof_3]).transpose()

test_stack = np.vstack([pred_1, pred_2,

pred_3]).transpose()

from sklearn.model_selection import RepeatedKFold

folds

= RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)

oof =

np.zeros(train_stack.shape[0])

predictions = np.zeros(test_stack.shape[0])

for fold_,

(trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):

print("fold

n° {}".format(fold_+1))

trn_data, trn_y = train_stack[trn_idx],

y[trn_idx]

val_data, val_y = train_stack[val_idx], y[val_idx]

print("-" * 10 + "Stacking "

+ str(fold_) + "-" * 10)

clf = BayesianRidge()

clf.fit(trn_data, trn_y)

oof[val_idx]

= clf.predict(val_data)

predictions += clf.predict(test_stack) /

(5 * 2)

if eval_type ==

'regression':

print('mean: ',np.sqrt(mean_squared_error(y, oof)))

if eval_type

== 'binary':

print('mean: ',log_loss(y, oof))

return

oof, predictions

其中 oof_1、oof_2、oof_3 分别为

三个模型的验证集结果

，pred_1、pred_2、pred_3

为三个模型的测试集结

果。作为通用的

Stacking框架，这里

不对三个模型做具体约

束。将这

两部分分别拼接

起来得到，仅有三个特征

列的训练集和测试集，然

后将训练集喂入

BayesianRidge 模型中

训练，并预存最终结果。

最

终模型验证集分数为 0.70788，测

试集分数为

0.67445，可以看到 Stacking 融

合的线下分数

通常更高

，但是在测试集上往往因

为过拟合等原因无法得

到一致的提升结果。

13.6 赛题

总结

13.6.1 更多方案

GroupByMean

前面通过

组合单变量离散字段与

标签提取点击率特征，由

这点可以想到，将多变量

离散字段稀疏矩阵化后

得到的是类似于标签的

0、1 列，因此可以考虑统计单

变量离

散字段在某个多

变量离散字段中取值的

均值，即

groupby(cat_features)

[text_features].mean()，比如统计在 age 取值

为 5 的人群中，interest1

的

兴趣 ID 为 109 的

人数占比。

N-Gram

在提取 CountVectorizer 特征的

时候，本书采用了默认的

参数，即 ngram=(1,1)，还

没有尝试采取

更高阶的

N-Gram 进行统计，高阶

的 N-Gram 本质上是增加了一层

特征

组合，使得属于同一

个多变量离散字段的信

息捆绑在了一起，比如标

识出了哪些人

同时喜欢

跑步与骑自行车。

图嵌入

这类方式主要用来提取

uid 或者 aid 等类别的向量表征

，可以很好地从图结构中

挖

掘用户和广告信息，那

些在图中具有同质性或

者同构性的 uid

或者 aid 也能被

嵌入

向量表示出来。如图

13.14 所示，为 DeepWalk

的两种嵌入向量

提取方式。

图 13.14 提取嵌入向

量的流程

13.6.2 知识点梳理

特

征工程

在特征工程方面

，本章从经典特征、业务特

征以及文本特征三个方

面介绍了常见的

特征提

取办法。其中经典特征主

要是单变量离散字段之

间的交互统计，包括 count 编

码

、nunique

与 ratio 特征三种；业务特征部

分介绍了行业场景与领

域知识结合的点击

率特

征，这也是需要结合标签

去构建的特征；文本特征

部分介绍了几种不同的

稀疏

矩阵生成方式，这部

分在处理大规模单变量

与多变量离散字段时尤

其有用。

建模思路

本章的

赛题代表了一种典型的

数据组织形式与表格数

据结构，针对这类数据可

以抽

象出相对通用的特

征工程方法，这也是本书

采取这个赛题进行计算

广告的案例讲解

的一方

面原因。另一方面，本次赛

题是以用户和广告同时

作为建模对象，尝试对它

们进行合理地匹配，Lookalike 的原

理是通过以往广告的营

销结果，找到潜在的、和

点

击该广告的用户相似的

用户，从而达到广告的持

续曝光与点击，所以重点

要放在

寻找用户之间的

相似性上面，尤其是在各

个维度上的联合相似度

。遗憾的是，机器

学习受限

于特征工程，并不能取得

最好的效果，而深度学习

神经网络能在文本字段

上进行嵌套组合和非线

性函数拟合，因此本次竞

赛中，神经网络模型的表

现较佳。

13.6.3 延伸学习

本次大

赛要求参赛者在提交结

果中提供测试集中各种

子人群的候选用户属于

该种子人群的得

分（得分

越高说明候选用户是某

个种子人群潜在的拓展

用户的可能性越大），那么

是否可以

把用户点击某

个广告的概率看作点击

率预估问题。这与

2017 腾讯广

告算法大赛是非常相似

的，基本的特征构造方式

以及模型选择都是相同

的，不同之处在于 2018 腾讯广

告算法大赛

的用户行为

序列中没有时间相关的

信息，这就缺少了很多时

间相关的特征，当然这也

是

Lookalike

的业务所致。

2017 腾讯广告

算法大赛——移动 App 广告转化

率预估

计算广告是互联

网最重要的商业模式之

一，广告投放效果通常通

过曝光、点击和转

化环节

来衡量，大多数广告系统

受广告效果数据回流的

限制只能通过曝光或点

击作

为投放效果的衡量

标准开展优化。腾讯社交

广告（Tencent Social Ads）发挥其特有的

用户

识别和转化跟踪数据能

力，帮助广告主跟踪广告

投放后的转化效果，基于

广告

转化数据训练转化

率预估模型（pCVR，Predicted

Conversion Rate），在广告排序

中

引入 pCVR 因子优化广告投

放效果，提升 ROI。本赛题以移

动

App 广告为研究对

象，预测

App 广告点击后被激活的概

率：pCVR=P(conversion=1 |

Ad,User,Context)，即在给定广告、用户和

上下文的情况下预测广

告在点击后被

激活的概

率。业界一直比较重视对

广告点击转化（CTR）的研究，而

且目前应用相

对成熟，腾

讯本次竞赛对广告转化

率（CVR）进行预估算是独出心

裁，该次比赛无

论是在学

术研究还是业界应用领

域都有较高的研究价值

。

基本思路：2017 年的腾讯算法

大赛是比较早的 CTR

比赛，很

多方法都值得借鉴，

其中

不乏经典的操作。在模型

方面，大多数选手都是选

择树模型和 FFM 模型，然后

结

合各式各样的 Stacking

组合得到

最终的结果。在那个时代

，对于广告点击预测使

用

的模型还是比较单一的

，毕竟现在的 DeepFM、xDeepFM、AFM 等都是之后

才出

来的。

特征构造方面

，也都比较类似，比如都含

有基础特征、用户类别特

征、广告类特

征、上下文特

征、交互特征和其他特征

。这里的重点是其他特征

，可以称之为 trick

特征，具体包

含当天用户重复点击一

次转化、当天重复样本（特

征变量一致）的第

一条与

最后一条的时间差、当天

重复样本按时间排序。

冠

军选手的方案在模型方

面有很大的创新，除了树

模型、wide&deep 和

PNN 以外还

使用了改

进创新的 NFFM 模型，并且单模

成绩高于线上第三名的

成绩。最终使用的

模型融

合方式是加权平均融合

，不过是在进行了

logit 逆变化

后融合的。具体地，首

先将

各个模型的结果代入到

sigmoid 反函数中，然后得到均值

结果，最后对均值结果

使

用 sigmoid

函数。相较于普通的加

权平均，这种方法更适合

于结果具有较小差异的

情况。

# sigmoid 函数

def f(x):

res = 1 / ( 1

+ np.e ** ( -x )

)

return res

# sigmoid 反函数

def f_ver(x):

res = np.log( x

/ ( 1 - x )

)

return res

第 14 章

实

战案例：TalkingData AdTracking Fraud

Detection Challenge

本章将基于

2018 年 Kaggle 竞

赛平台中一道经典的广

告点击流量反欺诈赛题

，即 TalkingData

AdTracking

Fraud Detection Challenge 展开（如图 14.1 所示），这也将

作为计算广告相关问题

的第二道实战案例，主要

内容包括赛题理解、数据

探索、特征工程、模型训练

和赛题总结。

其实在挑选

赛题的时候，几位作者讨

论了很多次，因为广告领

域不仅核心技术点很多

，可选

择的赛题也是非常

之多。最终我们以数据质

量、可覆盖知识点、赛题热

度作为主要选择条

件，确

定了这道赛题。

图 14.1 TalkingData

AdTracking Fraud Detection Challenge 赛题主

页

14.1

赛题理解

14.1.1 背景介绍

欺

诈风险无处不在，对于在

网上做广告的公司而言

，则可能会发生大量点击

欺诈事件，从而

导致大量

异常点击数据并浪费金

钱，能够识别欺诈点击可

以大大降低成本。在中国

，每个月

有超过

10 亿的智能

移动设备在使用。

TalkingData 是比较

大的独立大数据服务平

台，覆盖全国 70％以上的移动

设备，每天处理 30

亿次点击

事件，其中 90％可能是具有欺

诈性的。当前该平台为 App 开

发人员提供的防止点

击

欺诈的方法是衡量用户

在其产品组合中的点击

过程，并对那些产生大量

点击事件但最终从

未安

装过

App 的 IP 地址进行标记。利

用这些信息，开发人员已

经建立了 IP 地址黑名单和

设

备黑名单。

TalkingData 还希望反欺

诈工作能够始终领先于

欺诈者的行为，于是向 Kaggle 社

区发起了算

法挑战赛，以

进一步开发解决方案。在

与

Kaggle 合作的第二场竞赛中

，选手面临的挑战是

要构

造一种算法，该算法可以

预测用户在点击 App 的广告

后是否会下载该 App。为支持

选

手建模，主办方提供了

一份涵盖 4 天，包含约 2 亿次

点击事件的数据集。

14.1.2

赛题

数据

本赛题提供了样本

量为近 1.9 亿的训练数据，包

括从 2017 年

11 月 6 日至 2017 年

11 月 9 日

之

间的数据，每条数据记录

均为一次广告点击事件

。训练集中包含的变量（特

征）如下。

ip：产生点击事件的

IP

地址；

app：广告商提供的 App ID；

device：用户

的移动设备 ID；

os：用户移动设

备的操作系统版本 ID；

channel：广告

投放渠道 ID；

click_time：点击时间（UTC 时间

），格式为

yyyy-mm-dd hh:mm:ss；

attributed_time：若用户点击后下

载了 App，那这就是下载 App 的时

间；

is_attributed：用户点击后是否下载

了 App，这是目标变量。

14.1.3 评价指

标

竞赛要求参赛者提交

用户最终下载 App

的概率，并

以此计算 AUC 值，作为评判标

准。

14.1.4 赛题 FAQ

Q 初步看到的原始

数据集大小达到了 4 GB，这会

给竞赛带来哪些难点呢

？

A 在实际竞赛中，数据集过

大往往会对操作造成限

制。如果内存配置不够，不

仅数据集难

以全部加载

成功，在构造特征时也得

十分节约地使用内存；在

写代码时要优化着写，不

然一

不小心就会爆内存

，或者代码运行了好几个

小时还没有结束；在训练

模型的时候也会受很大

限制，比如模型的参数不

能再调整得那么随意了

，学习率、迭代次数、早停部

分的确定都得

考虑时间

的损耗；还有就是验证方

式，五折交叉验证变得相

当费事，采用留一法会更

加高

效。

Q 若正负样本的分

布极其不平衡，该怎么办

？

A 这也是推荐广告领域一

个常见的问题，一般的解

决办法是进行数据采样

，其中最常用的

是随机采

样，细化到样本分布不平

衡问题，则是随机负采样

。具体地，随机采样一定百

分比

的负样本数据，然后

用这部分负样本数据和

完整的正样本数据来完

成特征提取与最终的模

型

训练。需要特别注意，在

模型融合阶段，先用不同

比例的负样本数据训练

模型并预测，再进

行集成

往往会得到意想不到的

效果。

14.1.5 baseline 方案

有了 14.1.2 节介绍的

初步属性，就可以开始基

本的建模工作，初期构造

的

baseline 方案不需

要太复杂，只

要能给出一个正确的结

果即可。

数据读取

读取数

据的相关代码如下：

import

gc

import time

import numpy as

np

import pandas as pd

from

sklearn.model_selection import train_test_split

import xgboost as

xgb

path = './input/'

train_columns =

['ip', 'app', 'device', 'os', 'channel', 'click_time',

'is_attributed']

test_columns = ['ip', 'app', 'device',

'os', 'channel', 'click_time', 'click_id']

dtypes =

{'ip' : 'uint32', 'app' : 'uint16',

'device' : 'uint16', 'os' : 'uint16',

'channel' : 'uint16', 'is_attributed' : 'uint8',

'click_id' : 'uint32'}

train = pd.read_csv(path+'train.csv',

usecols=train_columns, dtype=dtypes)

test = pd.read_csv(path+'test_supplement.csv', usecols=test_columns,

dtype=dtypes)

在读

取数据时有个技巧性的

操作，即优化内存。如果直

接读取表格数据，那么整

型

列默认为 int64，浮点型列默

认为 float64，对那些处在–128 和

127 之间

的整型数值来

说，这显然

是非常浪费空间的，这里

通过优化内存解决了这

个问题。

准备数据

首先合

并训练集和验证集，在这

之前删除多余的变量，以

保证合并的时候不会出

错，具体操作代码如下：

#

训

练集

y_train = train['is_attributed'].values

# 删除多余变量

del train['is_attributed']

sub = test[['click_id']]

del

test['click_id']

# 合并

训练集与测试集

nrow_train = train.shape[0]

data = pd.concat([train, test], axis=0)

del

train, test

gc.collect()

特别需

要注意的是，由于内存空

间有限，在训练集和测试

集合并后务必删除原始

训

练集和测试集，然后使

用 gc.collect()

释放内存。

特征提取

对

于本章所讲的这类问题

，简单的统计特征可以发

挥具大的作用，因为这些

特征具

有很强的业务意

义，比如 count 编码特征可以反

映热度或活跃程度、nunique

特征

可

以反映某类变量的广

泛程度，以及 ratio 特征可以描

述范围比例等。接下来简

单构造

count 编码特征作为基

本特征的一部分，还有点

击时间特征

click_time，可以对此

特

征进行多种类型的转换

，代码如下：

for f in ['ip','app','device','os','channel']:

data[f+'_cnts'] = data.groupby([f])['click_time'].transform('count')

data['click_time'] = pd.to_datetime(data['click_time'])

data['days'] = data['click_time'].dt.day

data['hours_in_day'] = data['click_time'].dt.hour

data['day_of_week'] = data['click_time'].dt.dayofweek

train = data[:nrow_train]

test = data[nrow_train:]

del data

gc.collect()

模型训练

训练

模型的相关代码如下：

params = { 'eta':

0.2,

'max_leaves': 2**9-1,

'max_depth': 9,

'subsample':

0.7,

'colsample_bytree': 0.9,

'objective': 'binary:logistic',

'scale_pos_weight':9,

'eval_metric': 'auc',

'random_state': 2020,

'silent': True

}

trn_x, val_x, trn_y, val_y =

train_test_split(train, y_train, test_size=0.2,

random_state=2020)

dtrain =

xgb.DMatrix(trn_x, trn_y)

dvalid = xgb.DMatrix(val_x, val_y)

del trn_x, val_x, trn_y, val_y

gc.collect()

watchlist = [(dtrain, 'train'), (dvalid, 'valid')]

model = xgb.train(params, dtrain, 200, watchlist,

early_stopping_rounds = 20,

verbose_eval=10)

作

为 baseline

方案，这里并没有构造

太多特征，在模型训练方

面，为了能够快速得到

结

果反馈，直接将数据按照

8∶2 分为了两部分，学习率也

调得比较高，同样是为了

能在短时间内得到结果

。图 14.2 为模型训练过程中的

分数反馈，需主要关注 validauc

的

分数，然后使用训练得到

的模型预测测试集结果

，最后得到线上分数。

图 14.2 XGBoost 训

练过程的展示

训练好模

型后，需要进行测试集结

果的预测，并提交最终结

果，实现代码如下：

dtest = xgb.DMatrix(test[cols])

sub['is_attributed'] = None

sub['is_attributed'] = model.predict(dtest, ntree_limit=model.best_ntree_limit)

sub.to_csv('talkingdata_baseline.csv', index=False)

最终线

上私榜分数（Private Score）为 0.96854，线上公榜

分数（Public Score）为

0.96566。目前 baseline

方案在私榜

的排名是 1995/3946，这样看来提升

空间还是非常

大的，因此

可以定个小目标：努力拿

到银牌。本章还将给出更

多可以尝试的方向，

我们

一起往金牌区晋升。

14.2 数据

探索

14.2.1 数据初探

基础展示

图 14.3 展示了训练集数据，便

于快速了解数据集内部

结构组成。

图

14.3 训练集数据

的展示

标签分布

构建标

签分布的条形图可视化

实现代码：

plt.figure()

fig,

ax = plt.subplots(figsize=(6,6))

x = train['is_attributed'].value_counts().index.values

y = train["is_attributed"].value_counts().values

sns.barplot(ax=ax, x=x, y=y)

plt.ylabel('Number of values', fontsize=12)

plt.xlabel('is_attributed value',

fontsize=12)

plt.show()

首先观察标签

分布，如图 14.4 所示，能够看出

训练集的标签正负样本

分布极其不平

衡，正样本

占比约为

0.247%，甚至连百分之

一都没有到。在 14.4 节会对标

签分布不

平衡的问题进

行优化处理，通常是对负

样本进行欠采样，对正样

本进行过采样。在

大规模

的数据集中，对负样本进

行欠采样是最常用的选

择，能够解决性能问题。

图

14.4

标签分布图

变量分布

接

下来观察下其余特征变

量的基本分布情况，如图

14.5 所示。此处主要展示特征

变

量 ip、app、device、os

和 channel 的唯一值个数，其

余的单变量分析在 14.2.2

节。

图

14.5

取对数后唯一值个数的

展示

14.2.2 单变量分析

单变量

的属性分布

了解单变量

的属性分布有助于我们

对特征建立基本认识。本

赛题涉及的主要为类别

特征，一般会观察每个属

性的 count

分布。

首先分析 IP 地址

，下面是生成下载量最高

的 10 个

IP 地址可视化图的实

现代码：

tmp = train.groupby('ip').is_attributed.sum()

data_plot

= tmp.nlargest(10).reset_index()

data_plot.columns=('IP', 'Downloads')

data_plot.sort_values('Downloads', ascending

= False)

plt.figure(figsize = (8,5))

sns.barplot(x

= data_plot['IP'], y = data_plot['Downloads'])

plt.ylabel('Downloads',

fontsize=16)

plt.xlabel('IP', fontsize=16)

plt.title('Top 10 bigest

downloader', fontsize = 15)

生成结果如图 14.6

所

示，可以看出下载量排前

10 的 IP 地址差距还是蛮大的

。

图 14.6

下载量最高的 10 个 IP地址

另外进行了简单的统计

，结果如图 14.7 所示。70%

的 IP 地址只

下载了一次

（once），18%的 IP 地址多次

下载（multiple

times），12% 的 IP 地址一次都没有

下

载过（no），呈很明显的长尾

分布。IP 地址对于模型预测

也能起到很大的帮助。

图

14.7 下载量分类占比

扩展考

虑

虽然下载量有明显的

差异性，可以作为一个不

错的特征，但下载量和下

载率

（预测目标）并不一定

是正相关的，比如点击 10

万

次、下载 1000 次和点击 200

次、下载

100 次，虽然前者下载量很高

，但下载率却远低于后者

。所以下载量

作为特征是

存在一部分偏差的，更为

全面的做法是构造点击

量和下载率特征，

用以辅

助模型更好的训练。

同样

地，也可以使用类似的方

法来分析单变量的其他

分布情况，并观察点击量

与下

载量的关系，或许存

在一些 App 平时很少被点击

，不过只要有用户点击此

App

基

本就会下载。

单变量与

标签的关系

对比单变量

与标签的关系是最能挖

掘变量价值的一个操作

，变量的属性之间如果存

在差异性，即如果 is_attributed 的分布

不一致，那么可把此变量

视为有价值的

特征，否则

需要继续深入进行挖掘

。接下来通过一段代码分

别实现特征变量

app、device、os 和 channel 与标

签 is_attributed

的密度分布关系，并进

行

可视化展示，如图 14.8 所示

。

cols =

['app','device','os','channel']

train1 = train[train['is_attributed'] == 1][train['day']

== 8]

t

r

a

i

n

0

=

t

r

a

i

n

[

t

r

a

i

n

['i

s

_

a

t

t

r

i

b

u

t

e

d']

=

=

0

]

[

t

r

a

i

n

['d

a

y']

=

=

8

]

s

n

s

.

s

e

t

_

s

t

y

l

e

('w

h

i

t

e

g

r

i

d')

p

l

t

.

f

i

g

u

r

e

(

)

f

i

g

,

a

x

=

p

l

t

.

s

u

b

p

l

o

t

s

(

2

,

2

,

f

i

g

s

i

z

e

=

(

1

6

,

1

6

)

)

i

=

0

f

o

r

c

o

l

i

n

c

o

l

s

:

i

+

=

1

p

l

t

.

s

u

b

p

l

o

t

(

2

,

2

,

i

)

s

n

s

.

d

i

s

t

p

l

o

t

(

t

r

a

i

n

1

[

c

o

l

]

,

l

a

b

e

l

=

"

i

s

_

a

t

t

r

i

b

u

t

e

d

=

1

"

)

s

n

s

.

d

i

s

t

p

l

o

t

(

t

r

a

i

n

0

[

c

o

l

]

,

l

a

b

e

l

=

"

i

s

_

a

t

t

r

i

b

u

t

e

d

=

0

"

)

p

l

t

.

y

l

a

b

e

l

('D

e

n

s

i

t

y

p

l

o

t',

f

o

n

t

s

i

z

e

=

1

2

)

p

l

t

.

x

l

a

b

e

l

(

c

o

l

,

f

o

n

t

s

i

z

e

=

1

2

)

p

l

t

.

s

h

o

w

(

)

图 14.8 单变量正负样本密度

分布图

由图 14.8 可以看出，四

个变量与标签的密度分

布存在差异，变量中不同

属性间正负

样本的占比

不同，这也说明特征变量

有一定的区分性。

除了以

上特征外，时间对于标签

的影响往往也是非常大

的，比如晚上的 App 下载率

和

中午的 App

下载率就可能不

同。不止是在本赛题中，类

似的推荐和广告场景也

都

会受时间的影响，导致

其 CTR、CVR 存在较大的差异。所以

，时间也是需要分析的

重

要对象。

如图

14.9 所示，首先提

取天（day）和小时（hour）特征，然后对

比

is_attributed 标签的密度分布差异

，可以看到天单位下正负

样本的密度分布完

全吻

合，小时单位的密度分布

则存在一定差异。

图

14.9 不同

时刻正负样本的密度分

布图

通过接下来的代码

绘制一幅热力图，能够更

加直观地展示下载概率

随时间发生的变

化：

grouped_df =

train.groupby(["day",

"hour"])["is_attributed"].aggregate("mean").reset_index()

grouped_df = grouped_df.pivot('day', 'hour',

'is_attributed')

plt.figure(figsize=(12,6))

sns.heatmap(grouped_df)

plt.show()

结果

如图 14.10

所示，其中横坐标为

小时单位，纵坐标为天单

位。可以明显看到不同

时

刻的下载率存在差异，比

如 13 点之后的下载率是比

较低的（颜色越深表示下

载率

越低）。另外，还能发现

6 日和

9 日的数据不完整，在

这种数据不完整的情况

下，

构造天单位的 count 等相关

特征需要进行特殊处理

，比如对数据量按比例修

正等。

图

14.10 不同时刻下载率

的展示

14.2.3 多变量分析

本节

将进行更加复杂的分析

工作，结合多个变量探索

数据中更多有价值的信

息，比如对 ip

与 app 进行组合，这

有助于了解不同 ip 下 app

的分

布情况。在对多个变量组

合之后构造

的统计特征

，比如 count 相关的特征，可以反

映同一个 IP 对不同

App 的偏好

程度，这里假

设相同组合

出现的频次越高，组合间

的联系就越密切。如果对

ip、app 和 os 进行组合，所

反映的信

息也会更加细致，此时的

数据粒度也是非常细的

。

对多个变量进行组合并

统计组合后多变量特征

的 count，结果如图 14.11 所示，其中横

坐标

为频次（count），纵坐标为密

度分布（distribution）。可以明显看出与

频次相关的多变量组

合

特征都服从长尾分布，并

且在密度分布上，正负样

本存在一定的差异。

图 14.11 多

变量组合后正负样本的

密度分布

14.2.4 数据分布

准确

了解数据的分布情况对

构造特征和数据建模都

有很大帮助，在竞赛中经

常会碰到线上线

下评估

分数不一致的情况，此时

首先要观察的就是数据

分布是否一致，这里的数

据具体是指

训练集（验证

集）和测试集。

先介绍一个

概念——协变量偏移，协变量

是指模型的输入变量（特

征），协变量偏移指的是

训

练集和测试集中的输入

变量具有不同的数据分

布，即数据变量发生了偏

移。然而我们所期

望的是

训练集和测试集的输入

变量是相同分布的，这也

有利于模型的预测。在真

实的场景中

却很难达到

这一点。

比如训练集中含

30% 的 app1、40% 的 app2

和 30% 的 app3，而测试集中含

10% 的

app1、20%

的 app2 和 70% 的 app3，很明显两个数

据集中

App 各类别所占的比

例不同，

即输入变量的分

布是不同的，这就是协变

量偏移。接下来通过可视

化的方式具体分析，如图

14.12 所示。

图 14.12

训练集和测试集

中的变量密度分布

由图

14.12 能够看出，各变量在训练

集上的比例和在测试集

上的比例均存在一定差

异，尤其

是变量 channel。另外，观察

变量的密度分布图，可以

发现训练集和测试集对

应的变量唯

一值个数（unique

count）也

是存在差异的，这说明变

量中有很多属性仅存在

于训练集中，

而测试集中

并没有。

如图 14.13 所示，天和小

时特征在训练集与测试

集上的分布也存在很大

的差异，因为两个数

据集

的时间区间不一致，这是

导致差异的主要原因，因

此在构造特征时要考虑

时间单位带来

的影响。

图

14.13 训练集与测试集中的天

和小时密度分布

14.3 特征工

程

对于

CTR、CVR 相关的问题，能联

想到的特征是非常多的

，比如不同粒度的统计特

征

（count、nunique、ratio、rank、lag等）、目标编码、embedding特征等，不

同粒度则是一

阶、二阶或

多阶的特征组合。虽然这

些丰富的特征各自有其

存在的意义和价值，但是

本赛题

的数据量过于庞

大，无法满足所有的特征

需求。因此如何从大量特

征中选择出重要性高的

就

显得尤为重要，本节将

介绍最核心的四类特征

，即统计特征、时间差特征

、排序特征和目标

编码特

征，这些特征都能对最终

分数起到巨大的正向作

用，同时也是参赛者在竞

赛中经常会

用到的特征

。

14.3.1 统计特征

此处将构造本

赛题的核心特征，即统计

特征 groupby。具体地，对五个原始

类别特征

（ip、os、app、channel、device）构造一阶和二

阶的组合，然后进行聚合

，获取

count、nunique 和 ratio 特征。下面是一阶

和二阶特征构造代码，其

中提取的一阶特征包含

count 和

ratio，二阶特征包含 count、nunique 和 ratio。

# 一阶

特征count、ratio

for f in tqdm(['ip','app','device','os','channel']):

data[f+'_cnts'] =

data.groupby([f])['click_time'].transform('count')

data[f+'_cnts'] = data[f+'_cnts'].astype('uint32')

data[f+'_ratio'] =

(data[f].map(data[f].value_counts()) / len(data) *

100).astype('uint8')

#

二阶特征count

cols = ['ip','app','device','os','channel']

for i

in tqdm(range(0, len(cols)-1)):

for j in

range(i+1, len(cols)):

f1, f2 = cols[i],

cols[j]

data[f1+'_'+f2+'_cnts'] =

data.groupby([f1,f2])['click_time'].transform('count')

data[f1+'_'+f2+'_cnts'] =

data[f1+'_'+f2+'_cnts'].astype('uint32')

# 二阶特征

nunique 和ratio

for f1

in tqdm(['ip','app','device','os','channel']):

for f2 in ['ip','app','device','os','channel']:

if f1 != f2:

data[f1+'_'+f2+'_nuni'] =

data.groupby([f1])[f2].transform('nunique')

data[f1+'_'+f2+'_nuni'] = data[f1+'_'+f2+'_nuni'].astype('uint32')

data[f1+'_'+f2+'_ratio'] =

(data.groupby([f1,f2])['click_time'].

transform('count') / data.groupby([f1])

['click_time'].transform('count') *

100).astype('uint8')

当然，还可以进行多阶

特征组合，以获取不同细

粒度的信息，比如对 ip、app 和 device

进

行三阶组合。不过这时需

要特别注意，构造的特征

粒度不能过细，一般太细

的特征是不能

直接放到

模型中的，可以称这类特

征为高维稀疏特征，这种

特征对应的权重的置信

度很低

（很多这样的特征

组合仅出现一次），一般需

要进行转换或者压缩处

理。

14.3.2 时间差特征

时间差（time-delta）特

征也是本次竞赛的核心

特征之一，具体地，可以抽

取每个点击事件的

前

次

点击和后 次点击之间的

时间差作为特征。时间差

特征的具体实现代码如

下：

for cols in [['ip','os','device','app'],['ip','os','device','app','day']]:

for i in range(1,6):

data['ct'] =

(data['click_time'].astype(np.int64)//10**9).astype(np.int32)

name = '{}_next_{}_click'.format('_'.join(cols), str(i))

data[name]

= (data.groupby(cols).ct.shift(-i)-data.ct).astype(np.float32)

data[name] = data[name].fillna(data[name].mean())

data[name]

= data[name].astype('uint32')

name = '{}_lag_{}_click'.format('_'.join(cols), str(i))

data[name] = (data.groupby(cols).ct.shift(i)-data.ct).astype(np.float32)

data[name] = data[name].fillna(data[name].mean())

data[name] = data[name].astype('uint32')

data.drop(['ct'],axis=1,inplace=True)

那么时间差特征为什

么能起作用？首先这种特

征可以反映用户的活动

频繁程度；其次从业务

方

面考虑，可以用用户点击

App

的时间差来反映用户下

载此 App 的可能性，如果在短

时间

内多次点击相同 App，那

么下载的可能性会比较

大。这类特征的提取非常

具有技巧性，可

以看作是

trick

特征，同时能在很多竞赛

中起作用。

根据这种时间

差特征，还能构造先后点

击的标记特征。用户点击

App 无非分三种：首次点

击、中

间点击和末次点击。可以

想象我们在浏览应用市

场时，往往最后一次点击

的下载可能

性最大。具体

实现代码如下：

subset

= ['ip', 'os', 'device', 'app']

data['click_user_lab']

= 0

pos = data.duplicated(subset=subset, keep=False)

data.loc[pos, 'click_user_lab'] = 1

pos =

(~data.duplicated(subset=subset, keep='first')) & data.duplicated(subset=subset,

keep=False)

data.loc[pos,

'click_user_lab'] = 2

pos = (~data.duplicated(subset=subset,

keep='last')) & data.duplicated(subset=subset,

keep=False)

data.loc[pos, 'click_user_lab']

= 3

14.3.3 排序特征

从字面上理解，排序特征

有较强的穿越性质，主要

是基于用户与 App 第几次交

互进行排

序。构造排序特

征的代码如下：

for cols in tqdm([['ip','os','device','app'],['ip','os','device','app','day']]):

name

= '{}_click_asc_rank'.format('_'.join(cols))

data[name] = data.groupby(cols)['click_time'].rank(ascending=True)

name

= '{}_click_dec_rank'.format('_'.join(cols))

data[name] = data.groupby(cols)['click_time'].rank(ascending=True)

14.3.4

目标编码

特征

目标编码特征是直

接展现实体信息的特征

，即基于目标的概率分布

特征。正因为与标签有直

接关系，所以在构造目标

编码特征时需要避免数

据穿越问题。在含有直接

时间序列信息的数

据集

中，统计历史信息作为当

前特征；在不含时间序列

信息的数据集中，使用常

见的 折

交叉统计即可。构

造目标编码特征的代码

如下：

for

cols in tqdm([['ip'], ['app'], ['ip','app'], ['ip','hour'],

['ip','os','device'],

['ip','app','os','device'], ['app','os','channel']]):

name = '_'.join(cols)

res = pd.DataFrame()

temp = data[cols

+ ['day', 'is_attributed']]

for period in

[7,8,9,10]:

mean_ = temp[temp['day']<period].groupby(cols)['is_attributed'].mean().

reset_index(name=name +

'_mean_is_attributed')

mean_['day'] = period

res =

res.append(mean_, ignore_index=True)

data = pd.merge(data, res,

how='left', on=['day']+cols)

14.4 模型训练

14.4.1 LR

LR 模型一直

是点击率预估问题的 benchmark 模

型，凭借其简单、易于并行

化实现、可解释

性强等优

点被广泛使用。然而由于

线性模型本身的局限，不

能处理特征和目标之间

的非线性

关系，因此模型

的预测效果严重依赖于

算法工程师的特征工程

经验。构建

LR 模型的代码如

下：

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(C=5, solver='sag')

model.fit(trn_x, trn_y)

val_preds = model.predict_proba(val_x)[:,1]

preds = model.predict_proba(test_x)[:,1]

最终，线上私榜分数（Private Score）为

0.82260，线上公榜分数（Public Score）为

0.79545。可以发

现在特征相同的情况下

，LR 模型的效果很难达到预

期要求。不过常把 LR

模型用

作 Stacking融合中的第二层学习

器，能够降低过拟合的风

险，同时学习特征和目标

之

间的线性关系。

14.4.2 CatBoost

起初，提

出

CatBoost 模型主要是为了解决

类别特征问题，是高质量

的类别编码和特征交叉

能力使其变得流行起来

。由于 CatBoost 模型出现得比较晚

，可以说是一个年轻的算

法，在

2017 年下半年才慢慢崭

露头角，因此在本次竞赛

中并不能看到

CatBoost 的身影。接

下来对

CatBoost 进行建模，构建代

码如下：

from catboost

import CatBoostClassifier

params = {'learning_rate':0.02, 'depth':13,

'l2_leaf_reg':10,

'bootstrap_type':'Bernoulli',

'od_type': 'Iter','od_wait': 50,'random_seed': 11,'allow_writing_files':

False}

clf = CatBoostClassifier(iterations=20000, eval_metric='AUC', **params)

clf.fit(trn_x,trn_y, eval_set=(trn_x, trn_y),

cat_features=categorical_features,

use_best_model=True,

early_stopping_rounds=20,

verbose=10)

最终，线下验证集

分数为 0.9850，线上私榜分数（Private Score）为

0.97538，线上公榜分数

（Public Score）为

0.97655。

14.4.3 LightGBM

本节选

择使用非常稳定的 LightGBM 模型

。另外，本赛题的正样本占

比为

0.247%，标签分

布极其不平

衡，因此本节将尝试进行

负采样处理，并从实践中

判断能否在保证分数基

本不变

的情况下，使整体

训练性能得到提升。

全量

数据

为了能够获取更高

的分数，将进行两次训练

，第一次使用第 7

天和第 8 天

的数据记

录作为训练集

，使用第 9 天的数据记录作

为验证集；第二次首先从

第一次训练中获

取到最

佳迭代次数，然后使用第

7 天、第 8 天和第 9 天的数据记

录作为训练集来训

练，并

进行最终的预测。具体实

现代码如下：

trn_x = data[data['day']<9][features]

trn_y =

data[data['day']<9]['is_attributed']

val_x = data[data['day']==9][features]

val_y =

data[data['day']==9]['is_attributed']

params = { 'min_child_weight': 25,

'subsample': 0.7,

'subsample_freq': 1,

'colsample_bytree': 0.6,

'learning_rate': 0.1,

'max_depth': -1,

'seed': 48,

'min_split_gain': 0.001,

'reg_alpha': 0.0001,

'max_bin': 2047,

'num_leaves': 127,

'objective': 'binary',

'metric': 'auc',

'scale_pos_weight': 1,

'n_jobs': 24,

'verbose': -1,

}

train_data = lgb.Dataset(trn_x.values.astype(np.float32), label=trn_y,

categorical_feature=categorical_features,

feature_name=features)

valid_data = lgb.Dataset(val_x.values.astype(np.float32), label=val_y,

categorical_feature=categorical_features,

feature_name=features)

clf = lgb.train(params, train_data, 10000,

early_stopping_rounds=30,

valid_sets=[test_data],

verbose_eval=10

)

接下来对训

练集和测试集进行合并

，因为训练集得到了扩充

，所以将最佳迭代次数

扩

大

1.1 倍，最终用训练好的模

型对测试集的结果进行

预测。

trn_x = pd.concat([trn_x, val_x],

axis=0, ignore_index=True)

trn_y = np.r_[trn_y, val_y]

# 这里是按列连接两

个矩阵，即把两矩阵上下

相加，要求列数相等，类似

于pandas 中的concat()

del val_x

del

val_y

gc.collect()

train_data = lgb.Dataset(trn_x.values.astype(np.float32), label=trn_y,

categorical_feature=categorical_features, feature_name=features)

trees = 400

clf

= lgb.train(params,

train_data,

int(trees * 1.1),

valid_sets=[train_data],

verbose_eval=10

)

负采样数据

负采

样优化时，应注意需在对

全部数据完成特征提取

之后再进行负采样，如果

在特

征提取前就进行负

采样将会影响原始数据

分布的真实描述，这样构

造出来的特征不

具备真

实意义。

下面给出随机负

采样代码，经过负样本采

样后，即可直接进行模型

训练。

# 对训练集进行负采

样

df_train_neg =

data[(data['is_attributed'] == 0)&(data['day'] < 9)]

df_train_neg

= df_train_neg.sample(n=1000000)

# 合并成新的数据集

df_rest =

data[(data['is_attributed'] == 1)|(data['day'] >= 9)]

data

= pd.concat([df_train_neg, df_rest]).sample(frac=1)

del df_train_neg

del

df_rest

gc.collect()

14.4.4 DeepFM

DeepFM 也

是

CTR、CVR 问题中的经典模型，其

结构是 FM 和深度神经网络

的组合，如图

14.14 所示。因此

DeepFM 不

仅有 FM 自动学习交叉特征

的能力，同时还引入了神

经网络的

隐式高阶交叉

信息。在 DeepFM

的具体实现部分

，主要分为 FM 层、DNN 层和 Liner 层三

部

分，最后将三部分结果拼

接起来，并输入到输出层

得到最终的结果。

图 14.14 DeepFM 的网

络结构图

接下来是

DeepFM 的具

体实现代码：

from tensorflow.keras.layers import *

import tensorflow.keras.backend as K

import tensorflow

as tf

from tensorflow.keras.models import Model

from keras.callbacks import *

def deepfm_model(sparse_columns,

dense_columns, train, test):

# 稀疏特征处

理部分

sparse_input

= []

lr_embedding = []

fm_embedding

= []

for col in sparse_columns:

_input = Input(shape=(1,))

sparse_input.append(_input)

nums =

pd.concat((train[col], test[col])).nunique()

embed = Embedding(nums, 1,

embeddings_regularizer=tf.

keras.regularizers.l2(0.5))(_input)

embed = Flatten()(embed)

lr_embedding.append(embed)

embed = Embedding(nums,10,embeddings_regularizer=tf.

keras.regularizers.l2(0.5))(_input)

reshape =

Reshape((10,))(embed)

fm_embedding.append(reshape)

# FM 处理层

fm_square

= Lambda(lambda x: K.square(x))(Add()(fm_embedding))

square_fm =

Add()([Lambda(lambda x:K.square(x))(embed) for embed in fm_embedding])

snd_order_sparse_layer = subtract([fm_square, square_fm])

snd_order_sparse_layer =

Lambda(lambda x: x * 0.5)(snd_order_sparse_layer)

#

数值特征

处理

dense_input = []

for col

in dense_columns:

_input = Input(shape=(1,))

dense_input.append(_input)

concat_dense_input = concatenate(dense_input)

fst_order_dense_layer = Activation(activation="relu")

(BatchNormalization()(Dense(4)(concat_dense_input)))

# 线性部分拼接

fst_order_sparse_layer = concatenate(lr_embedding)

linear_part = concatenate([fst_order_dense_layer, fst_order_sparse_layer])

# 把FM

嵌

入向量与数值特征拼接

起来后“喂" 入FC 部分

concat_fm_embedding = concatenate(fm_embedding)

concat_fm_embedding_dense = concatenate([concat_fm_embedding,

fst_order_dense_layer])

fc_layer =

Dropout(0.2)(Activation(activation="relu"

)(BatchNormalization()(Dense(128)(concat_fm_embedding_dense))))

fc_layer = Dropout(0.2)(Activation(activation="relu")

(BatchNormalization()(Dense(64)(fc_layer))))

fc_layer = Dropout(0.2)(Activation(activation="relu")

(BatchNormalization()(Dense(32)(fc_layer))))

# 输出层

output_layer

= concatenate([linear_part, snd_order_sparse_layer, fc_layer])

output_layer =

Dense(1, activation='sigmoid')(output_layer)

model = Model(inputs=sparse_input+dense_input, outputs=output_layer)

return model

接下来是最终训练阶段

的代码，分为数据转换、编

译部分、回调函数和训练

部分，整体结构

都很常见

。

train_sparse_x =

[trn_x[f].values for f in categorical_features]

train_dense_x

= [trn_x[f].values for f in numerical_features]

train_label = [trn_y]

valid_sparse_x = [val_x[f].values

for f in categorical_features]

valid_dense_x =

[val_x[f].values for f in numerical_features]

valid_label

= [val_y]

# 编译部分

model =

deepfm_model(categorical_features, numerical_features, trn_x, val_x)

model.compile(optimizer="adam",

loss="binary_crossentropy",

metrics=["binary_crossentropy", tf.keras.metrics.AUC(name='auc')])

# 回调函数

file_path =

"deepfm_model.h5"

checkpoint = ModelCheckpoint(

file_path, monitor='val_auc',

verbose=1, save_best_only=True,

mode='max', save_weights_only=True)

earlystopping =

EarlyStopping(

monitor='val_auc', min_delta=0.0001, patience=5, verbose=1, mode='max')

callbacks = [checkpoint, earlystopping]

hist =

model.fit(train_sparse_x+train_dense_x,

train_label,

batch_size=8192,

epochs=50,

validation_data=(valid_sparse_x+valid_dense_x, valid_label),

callbacks=callbacks,

shuffle=True)

以上

就是 DeepFM 模型完整且可运行

的代码，整体实现还是非

常简单的，并且能够得到

与

前两节的树模型具有

很大差异性的结果。

14.5 赛题

总结

在整个竞赛流程中

，我们尝试了很多种特征

提取方式和不一样的模

型，除了这些，更重要的

部

分就是赛题总结。本节将

介绍排名靠前的选手的

方案，梳理关键知识点，并

带大家一起学

习、认识更

多类似的竞赛。

14.5.1

更多方案

Top1 方案

冠军选手在模型方

面是采用多个 LightGBM 模型和神

经网络模型，并进行加权

融

合。不同于大多数选手

的是，冠军选手在模型训

练阶段进行了负采样处

理，即选取

全部的正样本

（is_attributed==1），和相同样本量的负样本

，这意味着抛弃了

99.8% 的负样

本。会发现模型的表现并

没有受太大影响（特征工

程部分是在全部数

据上

进行特征提取，而非仅在

采样之后的数据上进行

）。另外，先采样不同比例的

负样本分别训练模型，然

后进行模型融合，最终结

果会有不错的提升（即训

练 5 个

模型，每个模型采样

不同的随机种子），还能大

大降低模型训练的时间

。

在验证阶段，选取 11 月 7 日和

11

月 8 日的数据进行训练，用

11 月 9 日的数据进行

验证，得

到迭代次数等参数后重

新在 11 月 7 日到 11

月 9 日的数据

上训练模型。数

据集中总

共有 646 个特征，把

5 个 LightGBM 模型融

合后的分数是线上公榜

分数为

0.9833 和线上私榜分数

为

0.9842。最后的提交方案为基

于排序的加权平均，融合

了 7

个 LightGBM 模型和 1

个神经网络

模型，线上公榜分数为 0.9834。

在

特征工程方面，主要是聚

合统计、对 groupby 特征构造接下

来 1

小时和 6 小时的

点击数

（count 特征）、对 groupby

特征构造计算前

向和后向的点击时间差

、对

groupby 特征构造历史点击时

间的平均下载率。另外，对

分类变量的组合（总共 20

种

）尝试用 LDA、NMF、LSA

得到 embedding（嵌入）特征，将

n_component 设

置为 5，这样每种方法可

以得到 100

个特征，最后一共

是 300 个特征（LDA、

NMF、PCA）。

Top2 方案

亚军选手

在模型方面选择的也是

LightGBM 模型和神经网络模型。最

佳单模为

LightGBM，其线上私榜分

数为 0.9837，最佳神经网络模型

的 private lb

分数为

0.9834（对分类变量采

用 Dot_Product 层，将连续特征喂入 FC 层

）。在模型融合方

面，亚军队

伍的每个人都训练了一

个 LightGBM 和神经网络模型，总共

有 6 个模

型，对这

6 个模型的

结果直接进行简单的加

权平均即可。

在模型训练

和验证方面，亚军选手同

样进行了负采样处理，具

体负采样的比例并未

告

知。在全部数据上进行特

征抽取，然后将特征合并

到采样的样本上，即先构

建特

征再采样，最后采用

五折交叉验证进行线下

模型评价。

在特征工程方

面，主要是通过聚合构造

统计特征，比如

count、cumcount、nunique

和时间差

特征等，还有一类比较特

殊的特征是将 IP 地址分别

于 app、os、channel

组合并计算每个属性

的点击数，然后选取高频

属性直接作为类别特征

。

Top3 方案

在模型方面，季军选

手选择的依然是 LightGBM 模型和

神经网络模型，不同之处

在

于该团队构造了很多

结构不同的神经网络模

型来增加模型的多样性

，比如通过循环

神经网络

来对点击的时序信息进

行建模、在 FC 层上添加 res-link 等。在

模型融合阶

段使用的是

Stacking融合，将模型的输出结果

作为特征，结合更多新的

特征继续参

与训练。

在特

征工程方面，主要采用了

23 个特征，其中比较重要的

是时间差特征，抽取每个

点击事件的前 5 次与后 5

次

点击之间的时间差作为

特征。除了原始数据中的

app、device、os、channel 特征外，还有 hour（小时）特征和

统计特征。

14.5.2 知识点梳理

本

节将对计算广告中

CTR/CVR 相关

赛题的特征提取方式进

行详细梳理。在提取特征

时，不

仅需要面对非常丰

富且维度非常高的数据

，对结果也有极高的精度

要求。表 14.1 将对不同

的特征

类型进行描述。

表 14.1 描述不

同特征类型

特征类型 具

体特征 描述

用户信息

年

龄、性别、职业、用户等级、兴

趣偏

好

提取用户侧基本

的信息特征

广告信息

广

告类型、广告素材、广告主

、广告行

业

提取广告侧基

本的信息特征

上下文信

息

广告位类型、运行商、广

告位置 提取上下文信息

的基本特征

一阶特征

count、nunique、rank、target

encoding

最

基础的统计特征，都存在

构造的意义

二阶特征

对

count、ratio、groupby、target

encoding

进行交叉组合

对一节特

征进行交叉组合，获取更

细粒度的特征描述，

当然

还可以进行三阶或者更

高阶的交叉组合

时间相

关

时间特征（年、月、日、小时

）、时序

特征（历史统计）、时间

差特征

时间信息出现在

很多的日志数据集中，时

间相关的统计

往往能挖

掘很多有用的信息

embedding

特征

Word2Vec、图嵌入、TF-IDF 结合

PCA、LDA

等文本挖掘

算法

提取用户或者广告

的实体表征向量

14.5.3 延伸学

习

本次赛题可以看作 CTR/CVR

相

关的赛题，这类竞赛的重

点在特征工程和模型方

面。特征工

程方面主要是

特征提取，比如聚合统计

特征、目标编码特征和 embedding相

关特征。在模型

方面则有

多种多样的选择，主要是

与广告点击率预测相关

的模型，如 FM、FFM、

DeepFM、Wide&Deep

等。接下来将给

出类似的经典赛题，以助

进一步理解此类赛题，达

到

在解题中游刃有余的

效果。

IJCAI 2018 阿里妈妈搜索广告

转化预测

第一个竞赛题

目是“阿里妈妈搜索广告

转化预测”（赛题主页如图

14.15

所示），该赛

题需要通过人

工智能技术构建预测模

型预估用户的购买意向

，即提供与历史广告点

击

事件相关的用户（user）、广告商

品（ad）、检索词（query）、上下文信息

（context）、商

店（shop）等五类信息，要求预测

接下来的广告产生购买

行为的

概率（pCVR）。

图

14.15 IJCAI 2018 阿里妈妈

搜索广告转化预测赛题

主页

结合淘宝平台的业

务场景和不同的流量特

点，官方定义了以下两类

挑战：日常的转

化率预估

（初赛）和特殊日期的转化

率预估（复赛）。

基本思路：本

赛题的难点在于复赛时

的特殊日期转化率预估

，这不仅要求预估结果

的

准确性，还要求进行有效

的线下验证。初赛可以将

最后一天的数据作为验

证集，

其余数据作为训练

集。复赛由于需要预估特

殊日期的转化率，而特殊

日期 7 号和之

前差异比较

大，因此用

7 号上半天的数

据进行训练，将最后两小

时的数据作为验证

集。

具

体的建模方案大致分为

三类：

(1) 只用前

7 天的数据来

预测，最后 1 天的数据用作

线下验证，因为训练集和

测试集

的数据分布有差

异，所以这种方案效果比

较差。

(2)

只用最后半天的数

据来预测，这样会损失掉

前 7 天的数据信息。

(3) 冠军选

手使用迁移学习的方式

，首先用前 7

天的数据进行

训练并预测，将得到的

结

果合并到最后半天，然后

仅用最后半天的数据完

成最终的训练和预测，这

样不仅

能保留前 7 天的信

息，预测出来的结果也更

接近于最后一天。

此赛题

中构造的特征大致分为

原始特征、统计特征（count、nunique、mean

等）、时

间差特征（上 次点击和下

次点击之间）、分段特征（将

hour、socre、rate

等连续特征离散化）、概率

特征（转化率和占比计算

）等。

2018 科大讯飞 AI

营销算法大

赛

本次大赛提供了讯飞

AI 营销云的海量广告投放

数据，需要参赛选手通过

人工智能技

术构造预测

模型并预估用户点击广

告的概率，即提供与广告

点击事件相关的广告、

媒

体、用户、上下文信息等，要

求预测用户的广告点击

概率。

这也是一道关于

CTR 预

估的问题，对于这类问题

，广告是否被点击的主导

因素是用

户，其次才是广

告信息。所以我们要做的

是充分挖掘用户以及用

户行为的信息，然

后才是

广告主、广告等信息。本赛

题的评价指标为对数损

失。

基本思路：赛题的难点

在于 user_tags

多值特征的处理，因

为其中包含用户的属

性

信息，所以能够完美地表

达 user_tags（提取有效属性，减少冗

余）至关重要。

对多值特征

的处理，最基本的是利用

CountVectorizer 进行展开，然后使用卡方

检验

进行特征选择。另一

种更有效的方式是利用

LightGBM

特征重要性进行分析，提

取

top 标签，相对卡方检验，这

又有一定程度的提升。还

有就是点击率特征，因为

该特

征包含时间信息，所

以提取历史点击率作为

特征可以有效避免数据

过拟合和泄露。

另外，本次

竞赛数据中缺乏用户 ID（uid）这

一关键信息，难以清晰地

建立用户画

像，因此如何

充分挖掘用户标签中包

含的信息就至关重要。数

据中还存在一些匿名

化

的数据，这时需要对数据

进行充分理解和分析，甚

至尝试根据业务理解进

行反编

码，才能够为特征

工程指明方向。在建模过

程中充分考虑用户标签

与其他信息的交

互作用

，并采用 Stacking 抽取特征信息的

方式减少维度与内存的

使用，对广告与用

户交互

信息进行充分挖掘，都使

得模型可以在 A、B 榜测试中

保持相对稳定。

第五部分

听你所说，懂你所写

第 15

章

自然语言处理

第 16 章 实战

案例：Quora Question

Pairs

第 15 章 自然语言处理

随着社交平台、内容平台

的不断发展，以互联网为

载体进行内容传播变得

更加平常。推荐系

统、计算

广告、用户画像分析等相

关技术的应用使得对海

量信息流进行个性化筛

选成为了可

能。在所有数

据中，多模态数据（文本、图

像、音频、视频和结构化数

据等）的利用为更加

精准

的预测提供了更多可能

性。文本数据作为内容平

台的主要信息载体，也成

为上述技术中

一种不可

或缺的数据类型，而如何

利用文本数据就成为自

然语言处理需要解决的

核心问题。

15.1 自然语言处理

的发展历程

技术并非一

成不变，可以将自然语言

处理的发展历程分为三

个阶段：

(1) 1950 年到 1970 年，基于经验

、规则的阶段；

(2)

1970 年到 2008 年，基于

统计方法的阶段；

(3) 2008

年至今

，基于深度学习技术的阶

段。

在第 (1) 阶段（即早期阶段

），图灵测试被视为一种评

判人工智能程度的测试

标准，对自然

语言输入进

行识别作为图灵测试的

一部分，拉开了自然语言

处理相关研究的序幕。这

个阶段

中，基于经验主义

和人工规则的模板构建

、语法解析等技术成为发

展的主流。然而由于语言

的时效性与多变性，以及

定制规则对语言学家和

领域知识的极端依赖，固

定的规则往往并不

能覆

盖绝大多数通用场景的

语言识别。

在第 (2) 阶段，随着

计算机的普及、互联网的

发展，在大量数据累积的

背景下，统计方法作

为一

种新方案出现了。传统的

方案需要大量人工操作

来汇总知识，统计方法取

代了这种方

案，被应用于

各类工业自然语言处理

场景中，并获得了一定的

效果。然而这一阶段能够

采用

的统计方法，诸如贝

叶斯模型、词袋模型 /TF-IDF、N-Gram 语言

模型等，仅能近似一些不

是特别复杂的任务。对于

包含丰富语言信息、复杂

语言结构和上下文场景

的任务，这种方法

仍然表

现得十分无力。

在第 (3)

阶段

，大量深度学习算法被应

用于自然语言处理中。早

期的词嵌入模型以及随

后发

展的卷积神经网络

、循环神经网络在这一阶

段扮演了非常重要的角

色，使原本统计方法的精

准度基线大幅度上提，并

在不同领域（如翻译、语音

识别等任务）得到了更泛

化的效果。在

目前的最新

环境下，以 Transformer 结构等为代表

的 Self-Attention

机制模型，可以在海量

数

据中应用并生成训练

模型，这种能力使得自然

语言处理得到了进一步

发展，在部分任务上甚

至

达到超越人类的基线评

分。

15.2 自然语言处理的常见

场景

自然语言处理技术

的目标是通过以计算机

为代表的各种电子机器

识别人类语言，从而理解

人

类意图。自然语言处理

可以更好地在部分特定

领域，将人工从繁杂的任

务中解放出来。以淘

宝为

例，对话系统通过解析识

别客户的提问，定位客户

需求，提供对相应问题的

解答或者购

买流程的具

体操作，为商家节省大量

重复劳动力与时间成本

。在车载语音系统中，将语

音识

别系统与自然语言

处理系统相结合，解放司

机双手并提供相应服务

（诸如寻路、播放音乐

等）。

根

据任务场景的不同，自然

语言发展过程中采用的

技术也会不同，本节将介

绍常见的几种自

然语言

处理任务。

15.2.1 分类、归任务

这

类任务也是传统机器学

习最为通用的任务，如何

将自然语言特征向量化

，采用传统或者深

度学习

模型训练预测是这一类

任务主要关注的点。其代

表任务有语义分析、情感

分析、意图

识别等，通常涉

及文本的表征和模型的

选择。

15.2.2 信息检索、文本匹配

等任务

信息检索、文本匹

配、问答等任务需要对大

量问—问、问—答配对进行预

测。这一类问题的

重点在

于，如何采用数据的特征

构造以及选择合适的模

型来实现文本与文本之

间的快速检索

与匹配。常

见的基于关键词进行搜

索的任务实际上也是这

类任务的一个特殊子类

。而更加复

杂的基于语义

甚至基于逻辑判断的匹

配依然是当前工业界和

学术界都为之关注的问

题。

15.2.3 序列对序列、序列标注

这一类问题更加关注序

列的生成与标注，常见的

任务包括语音和文本互

转、机器翻译、命名

实体识

别等。采用的方式通常是

深度学习的 CNN、RNN、Transformer 结构等。另外

，在适

当的场景中，也会结

合传统的

CRF、MRF、HMM 等模型一起使

用。

15.2.4 机器阅读

机器阅读是

给定问题与文本，然后根

据问题从文本中找出符

合要求的答案的方法。过

往的传

统方法常常受限

于数据与技术手段，达不

到理想效果。随着深度学

习和预训练语言模型的

不

断发展，越来越多的最

新技术被应用于机器阅

读领域，通过挖掘上下文

的语义，采用

Attention 机制来识别

特定场景下的问题答案

。

自然语言处理技术通常

并不是被独立应用的，而

是综合其他特征数据，协

同其他媒体、结构

化数据

等实现多模态数据的预

测。

15.3

自然语言处理的常见

技术

针对不同的任务，自

然语言处理的特征生成

方案也会存在诸多不同

。根据需要解决的任务的

特性，首先要考虑如何选

择数据处理方法、模型。深

度学习虽拥有极高的预

测上限，但并不

意味着传

统的自然语言特征处理

在平时的应用中就会被

抛弃。相反，在一些对响应

时间、模

型复杂度和大小

、模型可解释性有较高要

求的任务中，传统的基于

统计的特征提取方式与

机

器学习模型会扮演极

其重要的角色。接下来，我

们将列出一些常见的文

本特征提取方法。

15.3.1

基于词

袋模型、TF-IDF 的特征提取

词袋

模型（Bag-of-words）是最简单、最直接的

特征提取方式，该模型常

被应用于信息检索

领域

。词袋模型通常会忽略词

在文中的上下文关系，假

设词与词之间是上下文

独立的。这样

的假设能够

在丢失一定预测精度的

前提下，很好地通过词出

现的频率来表征整个语

句的信

息。

通过构造语料

的字典，可以将原本离散

的词集合表征为一个具

有字典大小的稀疏向量

。

例如，当我们拥有下面两

个语句：

We have noticed a

new sign in to your Zoho

account.

We have sent back permission.

那么对这两句的

字典构造方式为：

{'We': 2, 'have': 2, 'noticed':

1, 'a': 1, 'new': 1, 'sign':

1, 'in': 1,

'to': 1, 'your':

1, 'Zoho': 1, 'account.': 1, 'sent':

1, 'back': 1,

'permission.': 1}

两个语

句生成的

BOW 特征分别为：[1，0，1，1，1，1，0，0，1，1，1，1，1] 和

[0，1，

1，0，0，0，1，1，0，0，1，0，0]。

通过上述方式将离散特

征向量化，使得之后可以

采用传统的机器学习模

型（比如逻辑回归模

型、神

经网络模型、树模型以及

SVM

等）进行训练。

词袋模型仅

考虑词在一个句子中是

否出现，却没有考虑词本

身在句子中的重要性，因

此又提

出了 TF-IDF 方法，使用 TF×IDF

的

值对每一个出现的词进

行加权，可使词在文本中

拥有更

好的表征能力。

TF 的

计算方式为：词在句子中

出现的次数 / 文档中词的

总数。

IDF 的计算方式为：log( 文档

总数 / 包含词的文档的总

数 )。

基于上述两个式子计

算出 TF 和 IDF 后，将两者相乘就

可以得到 TF-IDF。在通过构建稀

疏

矩阵来表征语句含义

的算法之中，词袋模型 BOW 和

TF-IDF 方法具有简单易用、速度

快的

优点，同时缺点也很

明显，即当文本语料稀少

、字典大小大于文本语料

大小时，由于特征构

建缺

少足够的语料，对于词的

表征能力缺乏统计信息

的依据，会导致模型在训

练过程中容易

发生过拟

合。

15.3.2 N-Gram 模型

在自然语言处理

中，句子的表征是一个重

要课题。早期基于统计的

方法提出过这样一个方

案：现有一个句子 ，其中

代

表句子中的词，要求计算

句

子的出现概率 ，其表达

式为

。在此公式的前提下

加入

马尔可夫假设，假设

当前词的出现概率只和

前 个词有关，则

N-Gram 模型可修

改为

。结合 N-Gram 模型和词

袋模

型的理念可以更进一步

提升文本特征的预测能

力。

BOW、TF-IDF 模型可以与 N-Gram 模型相结

合，通过构建 Bi-Gram、Tri-Gram 等生成额外

的稀疏特征向量，构建出

来的特征比使用

Uni-Gram 的 BOW、TF-IDF 特征

更具有表征能

力，且能够

获取一定的上下文信息

，但是依然无法较好地处

理长序列依赖情况。

15.3.3

词嵌

入模型

词袋模型中存在

一个尚待解决的问题：假

如近义词出现在不同文

本中，那么在计算这一类

文

本的相似度或者进行

预测时，如果训练数据不

含大量标注，就会出现无

法识别拥有相似上下

文

语义词的情况。

后来出现

的词嵌入模型很好地解

决了这一类问题，当前常

用的词向量算法包括 Word2Vec、

glove、fastText 等

。另外，针对中文词向量的

预训练，还有腾讯公开的

AI Lab 词向量。

词向量的一个先

验假设是当前词的信息

可以由上下文推断出来

，因此对于一些罕见词、多

义

词，甚至常见的误拼写

词等，它都具有很好的泛

化能力。以

Word2Vev 为例，常见的模

型训

练方式分为 CBOW 和 Skip-Gram

两种

算法，如图 15.1 所示。

图 15.1 CBOW和

Skip-Gram 算法

的结构

训练生成的词向

量矩阵将以查询表的形

式记录由每一个词训练

生成的向量，这些向量对

应不

同的任务，可以发挥

不同的特征提取作用，具

体如下。

在传统特征的提

取上，可以采用加权求和

的方式，对句子中所有词

的向量加权求

和，最终生

成可以用来表征句子的

句子向量。句子向量可以

用来计算文本与文本之

间的余弦夹角相似度等

。

作为深度学习自然语言

处理模型的词嵌入层初

始化参数，可以得到比使

用端对端方

式训练出的

模型更高的精度。

构造匹

配任务的聚合类相似度

特征，可用于词级别的相

似度计算，并基于不同维

度

分别计算其平均值、中

位数、最大值、最小值等统

计数值构造特征。

15.3.4 上下文

相关预训练模型

词嵌入

模型虽然能够解决同意

不同词的问题，但在实际

的自然语言处理任务场

景中，相同词

汇在不同上

下文场景中往往具有不

同的含义。词嵌入模型的

结果是词的静态向量，我

们往往

需要在此向量的

基础上使用深度学习模

型构造上下文关系，由此

引申出对上下文相关的

预训

练模型的研究，这一

类模型的发展和词向量

模型高度相关。从早期的

基于双向 Bi-LSTM 构

建的 ELMo 模型，到

后续引入 Multi-Head Attention 机制构建的

GPT、BERT 结

构模型，都

是通过构建 Seq2Seq 的

语言模型，使用海量文本

数据，采用语言模型或者

自编码器模式训练

词的

上下文语义，从而将海量

的文本语义信息编码并

压缩在序列模型中。

现有

的上下文相关的预训练

模型包括：ELMo、GPT、BERT、BERT-wwm、ERNIE_1.0、

XLNet、ERNIE_2.0、RoBERTa、ALBERT、ELECTRA。接下来，我们将

列举一些常见的序

列模

型。

ELMo 模型。ELMo 是一个采用自回

归语言模型方式训练的

语言模型。自回归语言

模

型的本质是通过输入的

文本序列，对下一个词进

行预测。通过不断优化预

测的准

确率，使模型逐步

学得上下文的语义关系

。ELMo 模型的结构包括正向 LSTM 层

和

反向

LSTM 层，通过分别优化

正向下一个词和反向下

一个词达到更好的预测

效果。

GPT 模型。GPT 在 Google

公布了 Multi-Head Attention 机制

与 Transformer 结构

后，将这两者应用

到了语言模型的预训练

上。GPT 模型采用正向 Transformer 结构，

去

除了其中的解码器，同 ELMo

模

型一样，采用自回归语言

模型的方式进行训练。

相

比 ELMo 预训练模型，基于海量

数据训练而得的 GPT 结构在

当时大幅超越了原先

的

基准。

BERT/RoBERTa 模型。与采用自回归

语言模型的 ELMo 模型和 GPT

模型

不同，

BERT 模型使用自编码器

模式进行训练，模型结构

中包含正向和反向 Transformer 结

构

。为了减少由双向

Transformer 结构和

自编码器模式造成的信

息溢出影响，BERT

在训练过程

中引入了 MLM（Masked Language Model，遮蔽语言模型

），预训练中

15% 的词条（token）会被遮

蔽掉。对于这 15% 的词条，有 80% 的

概率会使用

[MASK] 替换，10% 的概率

随机替换，10% 的概率保持原

样，这个替换策略在模型

训练过程中起正则作用

，能够防止 BERT 模型因其双向

Self-Attention

结构，在训练

时学习到“未

来”的词，从而引起过拟合

。

RoBERTa 模型。由 Facebook 提出，在

BERT 模型的基

础上移除了 NSP（Next

Sentence Prediction）机制，并且修

改了 MLM

机制，调整了其参数

，最终获得的精

度超越了

原始的 BERT 模型。

ERNIE模型。百度在

BERT 模型的基础上优化了对

中文数据的预训练，并且

在

BERT 的基础上增加了三个

层次的预训练：Basic-Level Masking（第一层）、PhraseLevel Masking（第

二层）和 Entity-Level Masking（第三层）。它们分别

从字、短

语、实体三个层次

上加入先验知识，提高模

型在中文语料上的预测

能力。

BERT/RoBERTa-wwm。该模型由哈工大讯

飞联合实验室发布，严格

意义上并不是一

个新模

型，wwm（whole word mask）是一种训练策略。在原

始 BERT

论文中提到的

MLM 策略中

，它具有一定的随机性，会

将原始词的 word pieces 遮蔽掉。而在

wwm

策略中，相同词所属的 word pieces 被

遮蔽后，其同属其他部分

也会一同被遮蔽，在

保证

词语完整性的同时又不

影响其独立性。

对于上下

文相关预训练模型的应

用，可以先采用海量数据

预训练语言模型，之后在

其下游任

务上进行 finetune（精调

）。与端对端的训练方式相

比，选择合适的预训练模

型通常能够达

到更好的

效果，但这并不意味着在

所有任务上预训练模型

都能达到相同的效果。最

终效果和

模型在预训练

过程中采用的训练语料

、训练策略和下游任务所

属的领域有着密切的关

系。对

于所属领域具有巨

大差异的上游模型和下

游任务来说，其 finetune

结果并不

能够达到预期的

效果，在

一些情况下甚至不如使

用词向量 + Bi-LSTM 得出的结果。

15.3.5

常

用的深度学习模型结构

预训练模型通常可以带

来很高的精度收益，但其

复杂的模型结构也为训

练和预测带来了额外

的

时间成本。相比 BERT 等复杂的

预训练模型，采用一般的

卷积神经网络、循环神经

网络训

练模型的方式在

实际中有着更广泛的应

用，这些模型结构更加简

单、参数量更少、训练与推

论的时间也更短。

下面将

介绍常见的深度学习模

型及对应结构。

TextCNN。该模型的

特点是模型结构简单，训

练和预测速度快，同时拥

有比传统模

型更高的精

度。其设计理念来源于 N-Gram 模

型，采用多尺度卷积核来

模拟 N-Gram

模型在文本上的操

作，最终合并后进行预测

。适合短文本以及有明显

短语结构的语

料。其结构

如图 15.2 所示。

图 15.2 TextCNN

模型结构

LSTM、Bi-LSTM/Bi-GRU+Attention。这

些是非常典型的双向循

环神经网络结构。

在循环

神经网络中，无论是 LSTM 层还

是 GRU

层，都具有非常好的时

序拟合能

力，而增加了 Attention 机

制后的模型对于不同时

间的状态值进行加权，又

能使模型

的预测能力得

到进一步提升，相对适合

具有复杂语义上下文的

文本。

LSTM

模块是一种循环神

经网络结构模块，其结构

图如图 15.3 所示。相比普通的

循

环神经网络模块，LSTM 缓解

了训练过程中的梯度消

失，其添加的“门”结构用于

控

制记忆、输出、遗忘状态

，有利于模型在长序列中

获得更好的效果。

图 15.3 LSTM 结构

图

通过将序列正向、反向

排布，并使用 Bi-LSTM（双向

LSTM），可以得

到 LSTM 无

法得到的反向编码

信息，增加了模型的拟合

能力。Attention 机制从原理上分析

，是

一种对词在句子中的

不同状态进行加权的操

作，从最原始的加权平均

，逐步发展到

当前最为热

门的 Self-Attention 等，其核心理念一直

是通过使用词的相似度

矩阵进行

计算，调整词在

句子中对应的权重，从而

允许将词的加权求和作

为输出或者下一层

的输

入。结合 Bi-LSTM

和 Attention 层构建的模型

，能够更好地对输入文本

进行训

练。

DPCNN。TextCNN 结构可以模拟

N-Gram

模型，具有提取词中短语

的能力。但面对

文本结构

复杂、语义丰富或者上下

文依赖性强的文本（诸如

具有长序列头尾依赖的

词或者词组）时，TextCNN 往往无法

取得很好的效果，这是其

浅层结构本身的容量

造

成的问题。因此，需要采用

一种更深的卷积神经网

络结构——DPCNN，它从

ResNet 结构中借鉴

了残差块（residual

block）的理念，模拟 CV 任

务中对于图像特征

进行

逐层提取的操作，相对适

合于长文本且有复杂语

法结构的文本。其结构如

图

15.4 所示。

图 15.4 DPCNN 结构

15.4 思考练习

01.

如果要设计一个对话系

统，应该设计成什么样，采

用什么模型？

02. 自然语言处

理技术核心解决的是什

么任务，哪些在应用中可

以考虑使用，哪些不

能？

03. 如

果限定任务的时效性、存

储介质和推论时间，应该

如何选择并设计模型？

第

16 章 实战案例：Quora Question Pairs

本章将以

Kaggle 竞

赛平台的 Quora Question Pairs 竞赛（如图

16.1 所示

）为例进行自然语

言处理

文本匹配案例的讲解。Quora 是

一个问答 SNS 网站，类似于知

乎，提供社交提问与回

答

服务。在 Quora 平台上有很多用

户提出的问题，这些问题

中往往包含大量的重复

内容，还

会出现关联提问

或者同类型问题被多次

提出的现象。在实际业务

的处理过程中，上述现象

经

常会分散高质量答案

的流量，因此需要在业务

上对相同问题进行匹配

，从而将重复问题归一

化

。

图 16.1 Quora Question Pairs 竞赛首页

16.1 赛题理解

自

然语言处理文本匹配类

的竞赛与传统的 ML问题虽

有相同之处，但也有自己

的特点，这种

竞赛的特征

构造和模型选择往往与

其他任务有所不同，需要

额外的特征工程储备知

识。

16.1.1

赛题背景

对于知乎、Quora 这

一类问答平台来说，能否

将高质量的答案整合起

来决定了其流量是否具

有集中性。对于重复的问

题，很难通过人工的方式

或者简单的关键词抽取

与匹配等完成答案

的整

合；对于包含复杂语义、语

法的问题，针对其特点往

往有意想不到的情况发

生，比如如

果采用句子分

词或者短语的匹配方式

，那么即便句子中有 90%

以上

的内容相同，只要有语

义

相反或者关键词不同，也

无法完成匹配。

考虑到这

些情况，应该对文本特征

做进一步的人工处理，或

者采用可以捕捉这类特

点的深度

学习模型来构

建整个方案。

16.1.2 赛题数据

从

官网下载数据压缩文件

并解压后，得到的具体文

件有 train.csv（训练集）、test.csv（测试

集）、sample_submission.csv（正确

与规范的提交示例文件

，含有需要参赛者预测的

所有的

card_id）、Train.csv.zip 和 Test.csv.zip。

16.1.3 赛题任务

预测

Quora 平台上的问题与问题之

间是否互相重复，即预测

test.csv 文件中 question1

与

question2 重复的概率。

16.1.4 评

价指标

本次竞赛采用对

数损失进行计算，具体的

计算代码如下：

import numpy as np

def log_loss(y,pred):

return -np.mean(y * np.log(pred) + (1

- y) * np.log(1 - pred))

16.1.5 赛题 FAQ

Q 如何

更好地掌握自然语言处

理相关的任务？

A

对于自然

语言处理相关的问题，可

以从两方面入手，一方面

是传统的 ML手段，即人工

提

取特征，基于统计和自然

语言处理领域相关的技

术进行模型构建和训练

；另一方面则可以

考虑采

用深度学习模型进行模

型的训练与优化，自动提

取特征，这可以省去大量

的人工预处

理环节。

Q

对于

自然语言处理问题，目前

深度学习正流行，是否还

有必要掌握传统方法？

A 有

必要，用深度学习解决自

然语言处理相关的问题

常常因为其高额的机器

性能依赖于响

应速度（模

型越复杂，训练和推论的

速度就越慢），而无法满足

一些瞬时响应需求，而在

竞

赛之外常常会遇到对

响应效率有严苛要求的

场景，以及粗排等对效率

要求大于精度要求的场

景，这时候传统的 ML加人工

提取特征的手段优先级

往往会更高。

16.2 数据探索

对

于文本类型的数据探索

，主要可以从如下几方面

进行考虑：

样本的数量。训

练集、验证集各自的样本

数量；

文本的最大、最小长

度，以及长度的分布图。这

方面决定了模型训练的

效率，在训

练深度学习模

型的过程中，并非总是需

要使用最长的样本长度

作为文本截断长度，

适当

根据文本长度分布选择

截断长度有助于提升模

型训练的速度，有时候还

能够提

升精度（减少过拟

合）；

文本词典中词的个数

，停用词分析，来判断使用

到的词向量或者深度学

习预训练模

型；

文本中的

关键词、词云等，可视化理

解文本的热点内容。

16.2.1

字段

类别含义

如图 16.2 所示，为数

据集中各自字段的含义

描述。

图 16.2

字段含义描述

16.2.2 数

据集基本量

首先分析数

据的总量，以及标签是否

分布均匀，这一块主要会

对模型的选择、训练是否

存在

潜在的过拟合情况

等造成影响。如果样本量

太小，则需要添加更多的

先验知识或者使用更好

的预训练模型来弥补模

型训练样本的缺失，还要

选择更好的数据增强手

段。具体代码如下：

print('Total

number of question pairs for training:

{}'.format(len(df_train)))

print('Total number of question pairs

for tes 的ting: {}'.format(len(df_test)))

print('Duplicate pairs:

{}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))

qids = pd.Series(df_train['qid1'].tolist() +

df_train['qid2'].tolist())

print('Total number of questions in

the training data: {}'.format(len(np.unique(qids))))

print('Number of

questions that appear multiple times:

{}'.format(np.sum(qids.value_counts()

> 1)))

结果

：

Total number of

question pairs for training: 404290

Total

number of question pairs for testing:

2345796

Duplicate pairs: 36.92%

Total number

of questions in the training data:

537933

Number of questions that appear

multiple times: 111680

从运行结果中可以看出

，数据中训练集和验证集

的量级都在 10 万级别以上

，这对于一个自

然语言处

理任务来说，并不算很多

。部分潜在的先验知识可

能无法从样本集中自动

识别出

来，需要使用预训

练模型或者人工构造特

征来弥补。通过分析我们

发现，训练集中的标签存

在一定的不均衡现象，重

复数据占总样本的 36.92%，这一

发现是否会影响到对数

损失函数

的验证，有待进

一步通过训练模型进行

验证。

16.2.3 文本的分布

本节首

先需要判断文本长度在

训练集和测试集中是否

保持分布一致，同时需要

找出文本最

长、最短的情

况。如果存在空文本，则需

要进行一定的处理。通过

观察文本长度的分布，尝

试找出有效且合理的文

本截断长度。

对于英文文

本来说，存在字符级别和

词级别的分布，下面我们

来看下着两种类型的分

布分别

是怎样的。

字符级

别的分布代码如下：

train_qs

= pd.Series(df_train['question1'].tolist() +

df_train['question2'].tolist()).astype(str)

test_qs =

pd.Series(df_test['question1'].tolist() +

df_test['question2'].tolist()).astype(str)

dist_train = train_qs.apply(len)

dist_test = test_qs.apply(len)

plt.figure(figsize=(15, 10))

plt.hist(dist_train,

bins=200, range=[0, 200], color=pal[2], normed=True,

label='train')

plt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True,

alpha=0.5,

label='test')

plt.title('Normalised histogram of character

count in questions', fontsize=15)

plt.legend()

plt.xlabel('Number

of characters', fontsize=15)

plt.ylabel('Probability', fontsize=15)

字符

文本在训练集和测试集

中的长度分布如图

16.3 所示

。

图 16.3 字符文本的长度分布

词级别的分布代码如下

：

dist_train

= train_qs.apply(lambda x: len(x.split(' ')))

dist_test

= test_qs.apply(lambda x: len(x.split(' ')))

plt.figure(figsize=(15,

10))

plt.hist(dist_train, bins=50, range=[0, 50], color=pal[2],

normed=True, label='train')

plt.hist(dist_test, bins=50, range=[0, 50],

color=pal[1], normed=True, alpha=0.5,

label='test')

plt.title('Normalised histogram

of word count in questions', fontsize=15)

plt.legend()

plt.xlabel('Number of words', fontsize=15)

plt.ylabel('Probability',

fontsize=15)

词文本在训练集和测试

集中的长度分布如图 16.4 所

示。

图 16.4

词文本的长度分布

由图 16.3 和图 16.4 可以基本得出

结论，文本长度在训练集

与测试集上基本保持分

布一致。

在上述分析过程

中，我们进行了最长长度

截断，在字符级别使用

200 作

为阈值，在词级别使

用 50 作

为阈值，可以看到，在分析

字符文本长度分布的过

程中，文本的最长长度超

出了

200，但这部分样本的数

量占比非常小，词文本同

样如此。同时还存在文本

长度接近

0 或者

为 0 的样本

，有兴趣的读者可以对这

部分样本做进一步分析

，将它们打印出来并可视

化，观

察这些特别短的样

本是什么？是噪声，还是一

种存在特殊的样本。另外

，需要考虑特别短的

文本

如果出现，是否会对后续

特征构造或者模型训练

造成影响，应提早做好缺

失值插值或者

填充，避免

代码出现错误。

16.2.4 词的数量

与词云分析

计算词的数

量有助于更好地理解文

本，可以初步判断文本中

的专有名词等，在 16.2.3

节中我

们已经分析了词文本的

分布，接下来我们将从规

模和内容方面着手对词

文本进行分析。代码

如下

：

txt_tmp = ' '.join(train_qs.values.tolist())+'

'.join(test_qs.values.tolist())

words = set(txt_tmp.lower().split(' '))

print('max

number of words is %s'%len(words))

#

得到结果

# max number of words

is 327537

从代码结果可

以看出，这个样本在训练

集和测试集上词的总量

为 32 万左右，相对来说处于

一个比较小的量级，差不

多是常用的 Word2Vec、glove、fastText

等预训练词

向量量级的在十

分之一

，因此可能需要使用更多

外部数据或者预训练词

向量来丰富文本内容的

语义。

生成词云的代码如

下：

import matplotlib.pyplot as

plt

from sklearn.datasets import fetch_20newsgroups #

导入sklearn 中自带的数据集

import jieba

from wordcloud import

WordCloud

newsgroups_train = fetch_20newsgroups(subset='train')

text =

newsgroups_train.data

text = ' '.join(text)

wc

= WordCloud(background_color='white',scale=32)

wc.generate(text)

plt.axis('off')

plt.imshow(wc)

plt.show()

通过词云可以简单地画

出热点词的集合，了解哪

些词在语料中最热门，这

部分词出现比较多

的样

本有可能会主导模型的

评价指标。生成的词云如

图 16.5 所示。

图 16.5

生成的词云

我

们会发现图 16.5 中出现了一

个有意思的现象，best、difference、will 等词出

现的频率比较

高，这类词

在后续模型预测中分类

是否准确可能会影响最

终的分值。

16.2.5 基于传统手段

的文本数据预处理

对于

文本数据来说，根据使用

传统手段和深度学习模

型方式的不同，预处理的

手段也会不

同，此处针对

不同场景分别列举一些

文本预处理手段。

针对文

本的特征构建，首先需要

去除停用词，比如 can、is、are，由于这

类词几乎出现在所

有句

子中，因此往往会对特征

构建造成影响，比如对构

建 TF-IDF 特征或者基于 TF-IDF 特

征构

建其他特征就会有一定

的影响，可以尝试去除停

用词以获得更好的效果

。针对英文停用

词的问题

，可以使用 nltk 包中自带的语

料。代码如下：

from nltk.corpus import

stopwords

stops = set(stopwords.words("english"))

接下来是可

选操作，由于英文语料还

存在各类时态问题等，因

此也可以尝试（不是必须

）使

用

nltk 包中自带的词干提

取器（stemer），nltk 包提供了两种不同

的词干提取器，具体

选用

哪一种可根据使用后的

效果来决定，不管使用哪

种，目标都是将不同时态

的英文单词转

换成相同

的词干。具体代码如下：

from

nltk.stem.porter import PorterStemmer

from nltk.stem.snowball import

SnowballStemmer

def stem_str(x,stemmer=SnowballStemmer('english')):

x = text.re.sub("[^a-zA-Z0-9]","

", x)

x = (" ").join([stemmer.stem(z)

for z in x.split(" ")])

x

= " ".join(x.split())

return x

同

时，针对部分单词的错误

还可使用人工手段进行

修正，对存在错拼、近义词

的场合，可以

使用编辑距

离对英文单词做简单召

回，然后人工重新标注或

者替换单词。对于一些重

要的单

词，在使用传统的

特征工程构造手段如 TF-IDF 时

，往往无法识别字符级别

的误拼，而这些

单词对于

预测准确度有很大的贡

献，需要特殊处理。

16.2.6

基于深

度学习模型的文本数据

预处理

基于深度学习模

型的文本数据预处理也

分两块，此处和 16.2.5 节考虑的

点不太一样，此处往

往不

需要考虑太多停用词或

者词干的处理方式。

首先

是使用词向量卷积神经

网络或者循环神经网络

的深度学习模型训练方

法，这类方法在前

几年获

得了长足的发展，基于模

型调整模型结构和选择

更好的预训练词向量是

这类方法获得

更好结果

的核心关注点，因此预处

理需要针对选择的预训

练模型来减小词中 oov（out of

vocabulary，未登

录词）所占的比例，尽可能

让所有词都出现在预训

练的词向量中。对于那

些

没有出现在预训练词向

量中的单词，可以考虑使

用自己训练的

Word2Vec 或者 glove 词向

量，通过比对词与词的相

似度，找到在词典中存在

且相似的词进行替换。此

类 oov 的替换使

得基于预训

练词向量的深度学习模

型方案有了非常大的提

升空间。

其次是基于预训

练类序列模型的训练方

法，近几年来，从 Transformer 发展开始

一直到 GPT3，基于 Transformer

的深度学习

模型在海量数据集上的

预训练、迁移使得自然语

言处理有了

很大的发展

。BERT、RoBERTa、GPT 这类模型由于内部有 Word Segment 机

制，在使用时很

少会出现

含 oov 的场景，因此不需要像

基于预训练词向量的深

度模型方案那样进行大

量的词

替换操作，这类模

型更多时候被应用在模

型的重新预训练、微调和

制定训练策略上。

另外，对

于中文 BERT

模型来说，其 Tokenizer（分词

器）选用的并非英文单词

中的 Word

Segment 机制，而是使用单个

汉字作为 token

进行训练。

16.3 特征

工程

本章所讲的 Quora Question

Pairs 是自然

语言处理大类下的一个

文本匹配任务。在文本匹

配

的场景下，参赛者除了

要考虑提取文本本身的

特征外，还要考虑构建文

本与文本之间的关

系，如

何表达“文本与文本在语

义上相似”则是围绕这一

命题的最大的特征工程

难点。

对传统机器学习模

型需要利用到的特征工

程来说，除了使用 TF-IDF\word

embedding构建

sentence embedding等

原始特征外，还需要考虑

如何通过构建文本词级

别的相似度、句子级

别的

相似度等实现语义匹配

的表达。下面将介绍一些

常见的针对文本匹配任

务的特征构建方

案。

16.3.1

通用

文本特征

最简单的特征

构建方式是利用 bag of word（bow）或者 TF-IDF

构

建稀疏特征进行模型训

练

和预测。

使用 TF-IDF 构建稀疏

特征的方法也可以参照

本书前几章提到过的 sklearn

包

，代码如下：

from sklearn.feature_extraction.text import TfidfVectorizer

len_train

= df_train.shape[0]

data_all =pd.concat([df_train,df_test])

max_features =

200000

ngram_range = (1,2)

min_df =

3

print('Generate tfidf')

feats= ['question1','question2']

vect_orig

= TfidfVectorizer(max_features=max_features,ngram_range=ngram_range,

min_df=min_df)

corpus = []

for f in feats:

data_all[f] =

data_all[f].astype(str)

corpus+=data_all[f].values.tolist()

vect_orig.fit(corpus)

for f in

feats:

train_tfidf = vect_orig.transform(df_train[f].astype(str).values.tolist())

test_tfidf =

vect_orig.transform(df_test[f].astype(str).values.tolist())

pd.to_pickle(train_tfidf,path+'train_%s_tfidf_v2.pkl'%f)

pd.to_pickle(test_tfidf,path+'test_%s_tfidf_v2.pkl'%f)

在上述代码中

，通过调节 max_features 的大小来限定

最大可使用的词的个数

（等价于稀

疏向量的维度

），在一些场景下，限定维度

可以起到降维并减少过

拟合发生的作用，具体需

要限定多少词则需要进

行超参的调试，此处默认

使用 200000；使用 pickle 对生成的特征

进行缓存，得到 question1 和

question2 的 TF-IDF 稀疏

特征。

同时，ngram_range =

(1,2) 是一个比较关

键的参数设置，对 ngram_range 参数的

设

置影响着 vectorizer

对 N-Gram 的选择，即

其构建的文本 TF-IDF 特征所能

使用的最大和最小

N-Gram

值。当

ngram_range 的上限设置为 2 时，会生成

unigram 和 bigram

term 特征；当

设置为 3 时，则会

生成 unigram、bigram

和 trigram 特征。ngram_range 参数的上限

值越

大，生成的候选特征

维度就越高。

对于自然语

言处理的文本分类而言

，一个稀疏

TF-IDF 特征再加上一

个线性模型（LR、

Linear SVM）就足够作为

一个 baseline 方案了。然而对于文

本匹配而言，只是简单地

把

question1 与 question2 的稀疏特征拼接在

一起，然后使用线性模型

训练并不能得到很

好的

结果，原因在于线性模型

无法捕捉 question1

和 question2 文本特征之

间的非线性

关系。

对于 XGBoost

和

LightGBM 这样的树模型而言，如果

训练集与测试集都是稀

疏的，那么无

论对模型训

练的效率还是精度，都将

是一个考验，因为树模型

的训练过程会涉及对叶

子结点

的分裂和增益的

计算。当稀疏矩阵维度过

高时，即便是 XGBoost 和

LightGBM 这类采用

并

行训练方式的模型，常

常也会因为维度爆炸而

导致训练效率低下。因此

更好的方法是通过构

造

文本与文本之间的关系

，生成相关的特征，再进行

后续的模型训练。

总而言

之，此处可以在构建 TF-IDF

特征

的基础上，先将稀疏矩阵

降维成稠密矩阵，这也也

非常适合喂入树模型进

行训练。具体的降维方法

则可以使用 LSI（潜在语义索

引，等价于

Truncated SVD），或者其他 Topic Model（主题

模型）。读者可以考虑采用

sklearn

包中提供

的 decomposition 模块来尝试

对 TF-IDF 进行降维，并尝试自己

操作一番，对比

TF-IDF

特征与降

维后数据模型的验证精

度。常见的 decomposition 模块包括使用

的 Truncated

SVD、NFM

和 LDA 等方法，在这里 LDA 既可

以作为一种 Topic

Model 提取 topic 特征，也

可以被看作一种降维手

段。

除了像 Topic

Model 这样的文本表

征方法外，另一种文本表

征方法是采用预训练词

向量模型

来构建句子级

别的特征，通过对词进行

加权平均或者求和运算

，得到维度固定的稠密特

征矩

阵，这类特征矩阵由

于其固定的维度（通常在

二百到三百维左右）可以

直接被树模型训练和

使

用，具体的方案会在 16.3.2

节讲

到。

16.3.2 相似度特征

无论是原

始的 TF-IDF 特征，还是降维后的

LSI

特征或者 Topic Model 特征，都无法很

好地

解决上面提到的关

联问题或者同类型问题

被多次提出的现象。因此

需要采用一种或多种文

本

相似度匹配的方法，来

构建文本在不同层面上

的相似度。这种相似度可

以是句法层面、关键

词集

合层面或者语义层面的

。

下面将列举一些相似度

计算的方法，从不同角度

出发来构建相似度特征

。

第一种相似度特征构建

方式

最简单的文本相似

度特征构建方法是基于

编辑距离或者集合相似

度计算。通常的做

法是计

算 jaccard

distance 和 dice distance，下面给计算代码：

def get_jaccard(seq1,

seq2):

"""Compute the Jaccard distance between

the two sequences



seq1



and



seq2



.

They should contain hashable items.

The return value is a float

between 0 and 1, where 0

means equal, and 1 totally

different.

"""

set1, set2 = set(seq1), set(seq2)

return 1 - len(set1 & set2)

/ float(len(set1 | set2))

def get_dice(A,B):

A, B = set(A), set(B)

intersect

= len(A.intersection(B))

union = len(A) +

len(B)

d = try_divide(2*intersect, union)

return

d

当然还有其他很多方式

。

基于集合相似度计算的

最大优点是高效，并且能

够很好地从句式层面表

达两个文本

之间的相似

度。对于 jaccard distance

和 dice distance 而言，在构建相

似度特征时，还

需要考虑

句式和停用词。当两种句

式存在大量相同的停用

词时，使用集合相似度构

建的特征会具有偏高的

相似度，造成特征的分辨

能力下降。因此在使用集

合相似度

的过程中，建议

的做法是首先对比原始

文本与去除停用词后的

相似度文本的特征，

在一

定情况下还可以同时使

用这两种特征来提高准

确度。

第二种相似度特征

构建方式

第二种方法是

基于 TF-IDF 特征构建向量夹角

或者欧式距离。采用 TF-IDF

特征

的稀

疏矩阵直接计算余

弦夹角或者欧式距离得

到的相似度在一定程度

上可以表征向量的

相似

程度。因为 TF-IDF 特征直接表达

了词级别的重要性，因此

与集合相似度相比，

其中

包含的语义信息更多，对

重要词和不重要词的区

分度也更高。可以利用我

们已

经生成的 TF-IDF 文件进行

构建，代码如下：

def calc_cosine_dist(text_a,text_b,metric='cosine'):

return

pairwise_distances(text_a, text_b, metric=metric)[0][0]

上面这段

代码可用来计算向量与

向量之间的相似度，相似

度类型可以是余弦夹角

或

者欧氏距离。在拥有了

计算相似度的函数后，我

们可以有如下操作：

train_question1_tfidf

= pd.read_pickle(path+'train_question1_tfidf.pkl')

test_question1_tfidf = pd.read_pickle(path+'test_question1_tfidf.pkl')

train_question2_tfidf

= pd.read_pickle(path+'train_question2_tfidf.pkl')

test_question2_tfidf = pd.read_pickle(path+'test_question2_tfidf.pkl')

train_tfidf_sim

= []

for r1,r2 in zip(train_question1_tfidf,train_question2_tfidf):

train_tfidf_sim.append(calc_cosine_dist(r1,r2))

test_tfidf_sim = []

for r1,r2

in zip(test_question1_tfidf,test_question2_tfidf):

test_tfidf_sim.append(calc_cosine_dist(r1,r2))

train_tfidf_sim = np.array(train_tfidf_sim)

test_tfidf_sim = np.array(test_tfidf_sim)

pd.to_pickle(train_tfidf_sim,path+"train_tfidf_sim.pkl")

pd.to_pickle(test_tfidf_sim,path+"test_tfidf_sim.pkl")

此处

基于

sklearn 包的 pairwise_distances 函数来构建特

征，这实际上是一个速

度

比较慢的特征构建函数

。更好的方法是直接使用

scipy 库的

sparse 函数进行点

积操作

，这样可以更快得到结果

而且避免了 for 循环带来的

的压力。读者们可以思考

一下如何使用 scipy

库来构建

自己的稀疏矩阵余弦相

似度计算方式。

第三种相

似度特征构建方式

第三

种方式依赖于词向量，对

于词向量来说，某种程度

上会出现在向量空间

内

，word_a+word_b 近似于 word_c+word_d

的情况，也就是说

，通过对词向量

求和或者

取平均值生成句子级别

的向量，再用句子向量求

夹角和在某种程度上也

能

够表征文本的相似程

度。

这个方法的好处在于

，基于词向量的语义信息

比 TF-IDF 特征更加丰富，由于有

外部

数据的预训练信息

，对于 TF-IDF 特征无法处理的一

词多义情况，基于 embedding向

量的

相似性拥有更强大的表

征能力。

使用词向量时，可

以有多种备选方案。首先

可以使用预训练模型，诸

如

Word2Vec，

glove，fastText 等常用且包含丰富信

息量的预训练模型；其次

可以使用现有手中

Quora 平台

的语料自行训练，以此捕

获一些在预训练场景中

可能被遗漏的上下文信

息；第三种也可以考虑对

多种 embedding

向量加权求和后计

算相似度的方式，这样可

以将多个 embedding 向量融合在一

起，得到的相似度信息相

对更加精准。

将生成的 embedding 矩

阵转换成字典，通过词与

embedding向量的映射关系进行维

护。在计算相似度前，可以

尝试用

IDF 值对词进行加权

，或者如果有预定义的加

权

系数也可以对需要强

调的词进行加权，在部分

场景中是基于 IDF 值加权的

。相似度

拥有更强的预测

能力，但并不

100% 保证预测正

确，还需要尝试。

对应的计

算代码如下：

def calc_w2v_sim(row,embedder,idf_dict=None,dim=300):

'''

Calc w2v similarities and diff of

centers of query\title

'''

a2 =

[x for x in row['question1'].lower().split() if

x in embedder.vocab]

b2 = [x

for x in row['question2'].lower().split() if x

in embedder.vocab]

vectorA = np.zeros(dim)

for

w in a2:

if w in

idf_dict:

coef = idf_dict[w]

else:

coef

= idf_dict['default_idf']

vectorA += coef*embedder[w]

if

len(a2)!=0:

vectorA /= len(a2)

vectorB =

np.zeros(dim)

for w in b2:

if

w in idf_dict:

coef = idf_dict[w]

else:

coef = idf_dict['default_idf']

vectorB +=

coef*embedder[w]

if len(b2)!=0:

vectorB /= len(b2)

return (vectorA,vectorB)

假定我们采

用的预训练词向量维度

为 300。此处构建了一个函数

calc_w2v_sim，

其参数 row

为包含 question1 和 question2 的原始

文本；参数 idf_dict

是一

个可选项

，表示一个字典，可以提前

算好该项，并用其存放每

一个词的 IDF 系数，

进行加权

后输出两个句子向量。

函

数返回的

vectorA 和 vectorB 可以直接作

为句子级别的语义特征

，拼接后参与

训练，通过计

算 A、B

这两个句子向量之间

的余弦夹角或者欧式距

离，得到具有表

征句子相

似能力的特征，根据经验

，这类特征通常具有比较

强的表征能力。是否两

种

距离度量都要计算取决

于特征构建出来后对于

模型的贡献，以及是否有

助于提升

评价指标。另外

，存在一定的风险就是两

种距离度量都使用后会

有过拟合的情况。

16.3.3 词向量

的进一步应用——独有词匹

配

除去上述基于词向量

的文本语句匹配外，词向

量的另一个用途是度量

文本细节语义的差异。

在

16.3.2 节中我们介绍了多种使

语句向量化并计算相似

度的方法，以便衡量句子

与句子之间

的差异度。然

而在通常情况下，衡量文

本语句之间差异的部分

恰恰是在语句中没有重

复词的

那部分。

例如：

Do you know apple?

Do you

know banana?

Do you know machine

learning?

在这

三个语句中，前两句的疑

问内容主体是水果，最后

一句的则是一门学科。当

文本段落句

式相同时，如

何判断剩下的词句中不

相同部分的相似程度是

文本匹配需要考虑的另

一个因

素。

去重代码如下

：

def

distinct_terms(lst1, lst2):

lst1 = lst1.split(" ")

lst2 = lst2.split(" ")

common =

set(lst1).intersection(set(lst2))

new_lst1 = ' '.join([w for

w in lst1 if w not

in common])

new_lst2 = ' '.join([w

for w in lst2 if w

not in common])

return (new_lst1,new_lst2)

使用

distinct_terms 函数可以将输入的

文本字符串中重复的部

分去除，只保留独有的词

并且保持其前后顺序。然

后再利用 16.3.2 节中的句子向

量特征计算方法再一次

计算并生成相

同的特征

。

16.3.4

词向量的进一步应用——词

与词的两两匹配

使用词

向量进行匹配的另一种

进阶做法是进行词与词

的两两匹配。存在这样一

种可能，句子

与句子之间

的相似度不取决于对整

句中所有词加权求和后

的句向量相似度，而是从

两个句子

的词与词之间

的相似度就可以得出。也

就是说，我们可以首先计

算出两个句子之间词的

笛卡

儿积，得到两个句子

之间的词相似度，然后使

用统计方式生成词相似

度的最大值、最小值、

平均

值和中位数等一系列特

征值。这一组特征通常能

够更好地表征句子整体

的相似度，相较

于句向量

相似度，它含有更多的互

补信息。

16.3.5 其他相似度计算

方式

除了常见的 Word2Vec 等词向

量算法外，还可以使用

Doc2Vec 直

接生成句子文本特征（在

gensim 包中留有 Doc2Vec 的接口），也可以

使用 simhash

方法来计算文本的

相似度。关

于 simhash 方法的更多

细节可以查看 Google 于

2007 年发布

的一篇论文“Detecting Nearduplicates for web crawling”。

总之，任何可

以作为度量句子与句子

之间相似度的方法，都可

以尝试作为传统机器学

习模型

用来训练文本匹

配场景的特征工程方案

。

16.4 模型训练

常见的机器学

习模型的训练和调参过

程在前几节中已经描述

过，这一节将会介绍更多

的深度

学习模型。在自然

语言处理的发展过程中

，出现过基于卷积神经网

络、循环神经网络、基于

Attention（注

意力机制）等多种多样的

深度学习模型。如今 BERT 类模

型凭借其超大的预训

练

数据集和权重参数，正不

断地刷新着自然语言处

理领域的各项 benchmark。

以文本匹

配为例，使用到的深度学

习模型又可以分为基于

representation

和基于 interaction 两

种。这两种模型

都可以使用上一章提到

的自然语言处理相关深

度学习模型结构作为骨

干模

型，通常来说基于 representation

的

模型训练效率更高，但最

终得到的结果比基于 interaction

的

模型差。

在介绍两者的差

异之前，我们先列举一些

常见的自然语言处理模

型，然后再向大家介绍基

于

representation 和基于

interaction 的两类模型在

结构上的一些差异。

16.4.1 TextCNN 模型

首先介绍的是浅层 TextCNN

模型

，这是自然语言处理场景

下一个最简单（也是工程

化场景

下使用率较高）的

模型。

一个浅层 TextCNN 模型包含

三个组成部分，分别是 embedding层

、shallow

cnn 层和输出

层。在常见的自

然语言处理场景中，不考

虑段落的前提下，通常使

用到的卷积层为一维卷

积。这里我们使用 PyTorch 来构建

我们的 TextCNN

模型，代码如下：

import torch

import torch.nn as

nn

import torch.nn.functional as F

from

torch.autograd import Variable

class TextCNN(nn.Module):

def

__init__(self, args):

super(TextCNN, self).__init__()

self.args =

args

self.embed = nn.Embedding(args.sequence_length, args.embed_dim)

self.convs1

= nn.ModuleList([nn.Conv2d(1, args.kernel_num,

(ks, args.embed_dim)) for

ks in args.kernel_sizes])

self.dropout = nn.Dropout(args.dropout)

self.fc1 = nn.Linear(len(args.kernel_sizes) * args.kernel_num, args.class_num)

def forward(self, x):

x = self.embed(x)

# (batch_size, sequence_length, embedding_dim)

if self.args.static:

x = torch.tensor(x)

x = x.unsqueeze(1)

# (batch_size, 1, sequence_length, embedding_dim)

#

# input size (N,Cin,H,W) output size

(N,Cout,Hout,1)

x = [F.relu(conv(x)).squeeze(3) for conv

in self.convs1]

x = [F.max_pool1d(i, i.size(2)).squeeze(2)

for i in x]

x =

torch.cat(x, 1) # (batch_size, len(kernel_sizes)*kernel_num)

x

= self.dropout(x)

logit = self.fc1(x)

return

logit

该

模型的结构非常简单，用

embedding层的输出连接多个具有

不同大小卷积核的卷积

层，卷

积层的输出通过最

大池化层后再拼接起来

连接输出层。这样一个模

型结构融合了深度学习

中

卷积层的操作，通过卷

积计算来模拟一种人为

提取 N-Gram 的操作，这里卷积核

的大小代表

卷积计算滑

窗的范围，在一定程度上

等价于取 N-Gram trem 生成一个新的

phrase（短语），

模型将通过更新参

数的方式自动学习这样

一种特征提取行为。

由于

只并行使用了一层卷积

神经网络，因此即使采用

多种不同大小的卷积核

依然不会对模型

的效率

造成影响，同时类似 N-Gram 的卷

积计算操作可以有效提

取文本局部的 phrase 特征，

因此

TextCNN

非常适用于需要快速响

应且对预测准确度有一

定要求（相比传统的 TF-IDF +

线性

模型更好）的场景。

16.4.2 TextLSTM

模型

这

也是自然语言处理场景

下使用最多、最简单的循

环神经网络模型，由于诸

如 LSTM、

GRU 等循环神经网络层具

有拟合序列的能力，而自

然语言处理又是一个非

常典型的序列数

据场景

，因此往往会把

TextLSTM（或 TextGRU）和 TextCNN 一起

作为一个深度学习模型

的 baseline 参照。

构建 TextLSTM 模型的代码

如下：

class TextLSTM(nn.Module):

def

__init__(self, args):

super(TextLSTM, self).__init__()

self.hidden_dim =

args.hidden_dim

self.batch_size = args.batch_size

self.embeds =

nn.Embedding(args.vocab_size, args.embedding_dim)

self.lstm = nn.LSTM(input_size=args.embedding_dim,

hidden_size=args.hidden_dim,

num_layers=args.num_layers,

batch_first=True, bidirectional=True)

self.hidden2label = nn.Linear(args.hidden_dim,

args.num_classes)

self.hidden = self.init_hidden()

def init_hidden(self):

h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))

c0

= Variable(torch.zeros(1, self.batch_size, self.hidden_dim))

return h0,

c0

def forward(self, sentence):

embeds =

self.embeds(sentence)

# x = embeds.view(len(sentence), self.batch_size,

-1)

lstm_out, self.hidden = self.lstm(embeds, self.hidden)

y = self.hidden2label(lstm_out[-1])

return y

在这个

TextLSTM 中，embedding 层接入 LSTM 层

，这里我们使用的 LSTM

层为一

层双向

LSTM，该层的具体代码

如下：

nn.LSTM(input_size=args.embedding_dim,

hidden_size=args.hidden_dim,

num_layers=args.num_layers,

batch_first=True,

bidirectional=True)

其中 bidirectional=True 意味着 LSTM 包含正

向和反向两种方向。双向

LSTM

相比单

向 LSTM 拥有更强的序

列拟合能力，通常反向序

列中含有在正向序列中

捕获不到的额外信

息。TextLSTM 模

型的决策层只使用双向

LSTM

的最后一个 state（状态）输出作

为输入。

在后面几节讲的

模型中针对如何让决策

层获得更好的输入表征

，会有其他对 TextLSTM 优化

的方案

。

16.4.3 TextLSTM with Attention 模型

无论是取

LSTM 最后一层

输出，还是对 LSTM 的所有 state 输出

做

pooling操作都无法很好

地捕

获不同时间态上特征的

重要程度，Attention（注意力机制）应

运而生。Attention 希望达

到的目标

是模型在训练过程中能

对句子的每一个词给予

不同的关注度，然后通过

加权的方式

增加或者减

少每个词的重要性，以此

来提升模型的总体精度

。相比 pooling操作或者取

LSTM 的最后

一个 state 输出，Attention 能够在不丢失

长期信息的前提下（使用

最后一个

state

会有丢失的情

况），更加突出重要词在模

型训练和预测过程中的

作用。这里将会采用

最简

单的加权求和 Attention 为例来介

绍一个简单的 Attention 层以及其

对

TextLSTM 模型进

行的改造，代码

如下：

class SimpleAttention(nn.Module):

def

__init__(self,input_size):

super(SimpleAttention,self).__init__()

self.input_size = input_size

self.word_weight

= nn.Parameter(torch.Tensor(self.input_size))

self.word_bias = nn.Parameter(torch.Tensor(1))

self._create_weights()

def _create_weights(self, mean=0.0, std=0.05):

self.word_weight.data.normal_(mean, std)

self.word_bias.data.normal_(mean, std)

def forward(self,inputs):

att =

torch.einsum('abc,c->ab',(inputs,self.word_weight))

att = att+self.word_bias

att =

torch.tanh(att)

att = torch.exp(att)

s =

torch.sum(att,1,keepdim=True)+1e-6

att = att / s

att = torch.einsum('abc,ab->ac',(inputs,att))

return att

上述代码中，构建了

一个权重矩阵，并将输出

的每个

state 维度都归一化到

(0, 1) 区间内，让

每一个时间态

（词的位置）上都拥有一个

对应的权重。在 PyTorch

程序中，我

们可以采用爱

因斯坦求

和的方式来实现对任意

维度张量的计算，简化我

们代码的复杂程度。

对应

的 TextLSTM 模型则可以改造为：

class

TextLSTMAtt(nn.Module):

def __init__(self, args):

super(TextLSTMAtt, self).__init__()

self.hidden_dim = args.hidden_dim

self.batch_size = args.batch_size

self.embeds = nn.Embedding(args.vocab_size, args.embedding_dim)

self.lstm =

nn.LSTM(input_size=args.embedding_dim,

hidden_size=args.hidden_dim, num_layers=1,

batch_first=True, bidirectional=True)

self.simple_att

= SimpleAttention(args.hidden_dim*2)

self.hidden2label = nn.Linear(args.hidden_dim*2, args.num_classes)

self.hidden = self.init_hidden()

def init_hidden(self):

h0

= Variable(torch.zeros(1, self.batch_size, self.hidden_dim))

c0 =

Variable(torch.zeros(1, self.batch_size, self.hidden_dim))

return h0, c0

def forward(self, sentence):

embeds = self.embeds(sentence)

# x = embeds.view(len(sentence), self.batch_size, -1)

lstm_out, self.hidden = self.lstm(embeds, self.hidden)

x

= self.simple_att(lstm_out)

y = self.hidden2label(x)

return

y

在

通常情况下，结合 Attention 能够使

模型产生更好的效果。

16.4.4 Self-Attention

层

Transformer 结构或者使用到 multi-head-attention 的模型

大多使用了 Self-Attention 层，并通过

Self-Attention 层

构建的 block 堆叠来获取拟合

更长文本的能力。Self-Attention 层本身

也可以

单独拿出来构建

深度

CNN/RNN 模型中的加权操作

。从本质上讲，Self-Attention 层先计算输

入 query 的词相似度矩阵，然后

使用此矩阵生成加权系

数对输入 query

重新加权，以此

来提

升自己捕捉词级别

交互信息的能力。在不使

用任何卷积神经网络或

者循环神经网络的前提

下，一层 Self-Attention 通常只能捕获任

意两个词之间的交互信

息，计算复杂度为

，与 16.4.3

节中

的加权求和 Attention 相比，其计算

效率较低。Transformer 等模型通过不

断

堆叠 Self-Attention

层的 block 来获取词组

之间的相似度矩阵，以提

升复杂度。

下面将列举一

些更为复杂的 Self-Attention 层结构：

class MatchTensor(torch.nn.Module):

def __init__(self,size_a,size_b,channel_size=8,max_len=10):

super(MatchTensor,self).__init__()

self.size_a=size_a

self.size_b=size_b

self.channel_size=channel_size

self.max_len = max_len

self.M

= nn.Parameter(torch.Tensor(channel_size,size_a,size_b).to(device))

self.W = nn.Parameter(

torch.Tensor(channel_size,size_a+size_b,max_len).to(device))

self.b = nn.Parameter(torch.Tensor(channel_size).to(device))

self._create_weights()

def _create_weights(self,

mean=0.0, std=0.05):

self.M.data.normal_(mean, std)

self.W.data.normal_(mean, std)

self.b.data.normal_(mean, std)

def forward(self, x1,x2):

matching_matrix

= torch.einsum('abd,fde,ace->afbc',[x1, self.M, x2])

tmp =

torch.cat([x1,x2],2)

linear_part = torch.einsum('abc,fcd->afbd',[tmp,self.W])

matching_matrix =

matching_matrix+linear_part+self.b.view(

self.channel_size,1,1)

matching_matrix = F.relu(matching_matrix)

return

matching_matrix

class SelfMMAttention(nn.Module):

def __init__(self,input_size,max_len=100,channel_size=3):

super(SelfMMAttention,self).__init__()

self.input_size = input_size

self.channel_size = channel_size

self.max_len = max_len

self.match_tensor = MatchTensor(input_size,input_size,channel_size,max_len)

self.V = nn.Parameter(torch.Tensor(max_len,channel_size).to(device))

self._create_weights()

def _create_weights(self,

mean=0.0, std=0.05):

self.V.data.normal_(mean, std)

def forward(self,inputs,mask=None,output_score=False):

batch_size,len_seq,embedding_dim = inputs.size()

x = self.match_tensor(inputs,inputs)

att_softmax = torch.tanh(x)

att_softmax = torch.softmax(att_softmax,dim=1)

x = torch.einsum('abc,adbe->adc',[inputs,att_softmax])

if output_score:

return

x,att_softmax

return x

在

上述代码中，MatchTensor 结构构造了

一个计算层，用于计算任

意两个输入序列之间的

相

似度矩阵，并且在后续

的

Self-Attention 层中使用计算层来加

权重。对于相似度矩阵的

计

算，通常可以有多种复

杂的方式，比如矩阵点积

、concat、欧氏距离和余弦夹角等

。

使用 Self-Attention 通常能够比使用加

权求和

Attention 获得更好的效果

，但也更花时间。

16.4.5 Transformer 和 BERT

类模型

在 Transformer 结构出现之前，ELMo 等序列

预训练模型被当作当时

的 SOTA 结构，通过半

监督方式

预训练 LSTM 层级结构而非只

预训练词向量，使得得到

的预训练模型能够捕获

不

同输入的上下文关系

，这是只预训练词向量无

法获得的效果。而 BERT、GPT、RoBERTa 等

模型

是通过堆叠 Transformer block 层作为 encoder 实现

的大规模海量语料预训

练模型。通过

使用大量的

预训练数据，将语义和语

法信息压缩表征在模型

的超大规模参数中。

对 BERT 模

型的使用相对还是比较

方便的，PyTorch 的第三方包提供

了完整的 BERT

模型实

现，代码

如下：

# coding: UTF-8

import

torch

import torch.nn as nn

from

pytorch_pretrained import BertModel, BertTokenizer

class Config(object):

""" 配置参数"""

def __init__(self, dataset):

self.model_name

= 'bert'

self.train_path = dataset +

'/data/train.txt' # 训练集

self.dev_path = dataset

+ '/data/dev.txt' # 验

证集

self.test_path =

dataset + '/data/test.txt' # 测试集

self.class_list

= [x.strip() for x in open(

dataset + '/data/class.txt').readlines()] # 类别名单

self.save_path

= dataset + '/saved_dict/' + self.model_name

+ '.ckpt'

# 模

型训练结果

self.device =

torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#

设备

print('device',self.device)

self.require_improvement = 1000 #

若超过

1000batch 效果还没提升，则提前结

束训练

self.num_classes = len(self.class_list)

print('num_classes',self.num_classes)

# 类别数

self.num_epochs = 10 #

epoch 数

self.batch_size = 32 #

mini-batch 的大小

self.pad_size = 100 # 把每句话处理成的长度

（短填长切）

self.learning_rate = 5e-5 # 学习率

self.bert_path

= './bert_pretrain'

self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)

self.hidden_size

= 768

class Model(nn.Module):

def __init__(self,

config):

super(Model, self).__init__()

self.bert = BertModel.from_pretrained(config.bert_path)

for param in self.bert.parameters():

param.requires_grad =

True

self.fc = nn.Linear(config.hidden_size, config.num_classes)

def

forward(self, x):

context = x[0] #

输入的

句子

mask = x[2]

# 对padding

部分进行mask，和句子

一个size，padding 部分用0 表示

# 如：[1, 1,

1, 1, 0, 0]

_, pooled

= self.bert(context, attention_mask=mask,

output_all_encoded_layers=False)

out =

self.fc(pooled)

return out

上述

代码就是一个最简单的

基于 BERT 完成分类或者回归

任务的模型。通过调用

pytorch_pretrained 库

中的 BertModel 包来加载预训练完

成的 BERT 模型参

数。BertModel 不仅能够

加载原生谷歌 BERT 模型，也能

够使用基于 RoBERTa、ERNIE 和

RoBERTa-wwm-ext 等多种虽

使用 BERT 结构但训练方案不

同的其他预训练模型。

16.4.6 基

于

representation 和基于 interaction 的深度学习模

型的差异

前几节介绍的

模型皆可用作训练文本

匹配模型的骨干模型，对

于本节将介绍的这两种

模型的

差异可以用图

16.6 中

的两张小图来解释。

图 16.6 模

型对比

基于

representation 的深度学习

模型通常是先将输入文

本接入循环神经网络层

或者卷积神经

网络层，然

后通过 pooling或者 flatten 操作得到一

维文本表征，再通过表征

匹配层计算得到

最终的

输出结果。通常情况下用

作一维特征向量抽取的

卷积神经网络层或者循

环神经网络层

的权重参

数可被复用，输入文本 1 和

输入文本 2 同时通过表征

层得到表征向量结果。

以

下是一个简单的基于 LSTM 与

Attention 相结合构建的 Siamese Network（孪生网络

，基于

representation 的深度学习模型的

一种）。代码如下：

class SpatialDropout1D(nn.Module):

def __init__(self,p=0.5):

super(SpatialDropout1D,self).__init__()

self.p = p

self.dropout2d =

nn.Dropout2d(p=p)

def forward(self,x):

# b,h,c

x

= x.permute(0,2,1)

# b,c,h

x =

torch.unsqueeze(x,3)

# b,c,h,w

x = self.dropout2d(x)

x = torch.squeeze(x,3)

x = x.permute(0,2,1)

return x

class LSTM_ATT(torch.nn.Module):

def __init__(self,embedding_dim=200,hidden_dim=128,

voacb_size=10000,target_size=66,embedding_matrix=None,

layer_num = 1,num_heads=40):

super(LSTM_ATT,self).__init__()

self.hidden_dim=hidden_dim

self.voacb_size=voacb_size

self.target_size=target_size

self.embedding_dim = embedding_dim

self.layer_num

= layer_num

self.num_heads = num_heads

self.embed_scale

= np.sqrt(embedding_dim)

if embedding_matrix is not

None:

self.emb = nn.Embedding.from_pretrained(

torch.FloatTensor(embedding_matrix),freeze=True)

print('use

pretrained embedding')

else:

self.emb = nn.Embedding(voacb_size,embedding_dim)

self.dropout = SpatialDropout1D(0.15)

embedding_dim = int(embedding_dim/2)

self.embedding_dim = embedding_dim

self.lstm=nn.LSTM(input_size=embedding_dim,

hidden_size=hidden_dim,batch_first=True,

bidirectional=True,num_layers=1)

self.simple_att = SimpleAttention(hidden_dim*2)

self.out = nn.Sequential(

nn.LayerNorm(hidden_dim*2),

nn.Linear(hidden_dim*2, target_size),

)

self.log_softmax=torch.nn.LogSoftmax(dim=1)

def

init_hidden(self,batch_size=None):

h0 = torch.zeros((2*self.layer_num,batch_size,

self.hidden_dim),dtype=torch.float32).to(device)

h0

= Variable(h0)

c0 = torch.zeros((2*self.layer_num,batch_size,

self.hidden_dim),dtype=torch.float32).to(device)

c0 = Variable(c0)

return (h0, c0)

def forward(self,x,mask=None):

x = self.emb(x)

x

= self.dropout(x)

x, _ = self.lstm(x)

x = self.simple_att(x)

x = self.out(x)

return x

我们首先

构建了一个 LSTM_ATT 模型，这是一

个标准的单文本输入模

型，有一个输入和输

出，可

用作回归或者分类任务

。为了将其改造、适配成一

个孪生神经网络（Siamese

Network），我们需

要继承其并复用其中的

部分属性。代码如下：

class LSTM_ATT_SIA(LSTM_ATT_AM):

def forward(self,x,x1,mask=None):

x

= self.emb(x)

x = self.dropout(x)

x,

_ = self.lstm(x)

x = self.simple_att(x)

x1 = self.emb(x1)

x1 = self.dropout(x1)

x1, _ = self.lstm(x1)

x1 =

self.simple_att(x1)

x = F.cosine_similarity(x, x1)

return

x

通过

复用 LSTM_ATT 类，实现了同时输入

x 和 x1，然后通过同一个

LSTM_ATT 模型

作为骨

干模型（框架）并将

其映射成一维的文本向

量，最终计算余弦相似度

并输出。

使用孪生神经网

络有其特有优势，比如构

建模型简单、可对文本分

类模型进行复用、训练效

率高等。和基于 interaction 的深度学

习模型相比，孪生神经网

络由于不需要在序列级

别进行

交互，因此可以非

常简单地让 GPU 进行并行训

练，适合在一些文本比较

简单、对匹配有时

效要求

且对精度没有太高要求

的场景下使用。

基于 interaction

的深

度学习模型拥有更复杂

的先验假设，对基于 representation 的深

度学习模

型而言，由于计

算相似度的操作发生在

最终的输出层，文本在进

行计算前会先被压缩成

向

量，在这个过程中，输入

文本 1

和输入文本 2 的词级

别序列之间是互相毫无

感知的，对于部

分文本而

言，词与词对位之间的交

互可能存在有用的信息

，基于 representation 的深度学习

模型将

无法构建这类信息的特

征。对于词级别相似度或

者交互信息的捕获，是基

于

interaction 的深度学习模型所关

注的点，其核心思想是通

过 Attention 机制，使输入文本 1

和

输

入文本 2 提前进行交互，所

生成的相似度矩阵可用

于后续的分类任务判断

文本是否匹配。

以深度相

关性匹配模型 DRMM

为例，DRMM 是一

种典型的基于 interaction 的深度学

习模

型，下面通过观察代

码仔细分解其中 Attention

机制的

构成逻辑，代码如下：

"""An implementation of DRMM Model."""

import typing

import keras

import keras.backend

as K

import tensorflow as tf

from matchzoo.engine.base_model import BaseModel

from matchzoo.engine.param

import Param

from matchzoo.engine.param_table import ParamTable

class DRMM(BaseModel):

"""

DRMM Model.

Examples:

>>> model = DRMM()

>>> model.params['mlp_num_layers']

= 1

>>> model.params['mlp_num_units'] = 5

>>> model.params['mlp_num_fan_out'] = 1

>>> model.params['mlp_activation_func']

= 'tanh'

>>> model.guess_and_fill_missing_params(verbose=0)

>>> model.build()

>>> model.compile()

"""

@classmethod

def get_default_params(cls)

-> ParamTable:

""":return: model default parameters."""

params = super().get_default_params(with_embedding=True,

with_multi_layer_perceptron=True)

params.add(Param(name='mask_value', value=-1,

desc="The value to be masked from

inputs."))

params['optimizer'] = 'adam'

params['input_shapes'] =

[(5,), (5, 30,)]

return params

def

build(self):

"""Build model structure."""

# Scalar

dimensions referenced here:

# B =

batch size (number of sequences)

#

D = embedding size

# L

=



input_left



sequence length

# R =



input_right



sequence length

# H = histogram

size

# K = size of

top-k

# Left input and right

input.

# query: shape = [B,

L]

# doc: shape = [B,

L, H]

# Note here, the

doc is the matching histogram between

original query

# and original document.

query = keras.layers.Input(

name='text_left',

shape=self._params['input_shapes'][0]

)

match_hist = keras.layers.Input(

name='match_histogram',

shape=self._params['input_shapes'][1]

)

embedding = self._make_embedding_layer()

# Process left

input.

# shape = [B, L,

D]

embed_query = embedding(query)

# shape

= [B, L]

atten_mask = tf.not_equal(query,

self._params['mask_value'])

# shape = [B, L]

atten_mask = tf.cast(atten_mask, K.floatx())

# shape

= [B, L, D]

atten_mask =

tf.expand_dims(atten_mask, axis=2)

# shape = [B,

L, D]

attention_probs = self.attention_layer(embed_query, atten_mask)

# Process right input.

# shape

= [B, L, 1]

dense_output =

self._make_multi_layer_perceptron_layer()(match_hist)

# shape = [B, 1,

1]

dot_score = keras.layers.Dot(axes=[1, 1])(

[attention_probs,

dense_output])

flatten_score = keras.layers.Flatten()(dot_score)

x_out =

self._make_output_layer()(flatten_score)

self._backend = keras.Model(inputs=[query, match_hist], outputs=x_out)

@classmethod

def attention_layer(cls, attention_input: typing.Any,

attention_mask:

typing.Any = None

) -> keras.layers.Layer:

"""

生成Attentioon 的输入 .

:param attention_input:

输入张量 .

:param attention_mask: 输

入张量的mask .

: 返回: 被mask 掉的张

量结果 .

"""

# shape = [B, L, 1]

dense_input = keras.layers.Dense(1, use_bias=False)(attention_input)

if attention_mask

is not None:

# Since attention_mask

is 1.0 for positions we want

to attend and

# 0.0 for

masked positions, this operation will create

a tensor

# which is 0.0

for positions we want to attend

and -10000.0 for

# masked positions.

# shape = [B, L, 1]

dense_input = keras.layers.Lambda(

lambda x: x

+ (1.0 - attention_mask) * -10000.0,

name="attention_mask"

)(dense_input)

# shape = [B,

L, 1]

attention_probs = keras.layers.Lambda(

lambda

x: tf.nn.softmax(x, axis=1),

output_shape=lambda s: (s[0],

s[1], s[2]),

name="attention_probs"

)(dense_input)

return attention_probs

基于 interaction 的深度学习

模型通常比基于 representation 的深度

学习模型精度更高，但是

效

率更低，在使用过程中

如何取舍还需要根据实

际情况进行判断，不过对

于模型融合来说，这

两个

都是可以尝试的候选模

型。

16.4.7 一种特殊的基于 interaction 的深

度学习模型

在训练

BERT 类模

型或者所有将 Self-Attention 作为核心

层的模型时，有一种特殊

操作可以

极大地简化模

型构建方法。具体地，在训

练过程中，可以简单将输

入文本 1

和输入文本 2 拼

接

成一个新的长文本 3，直接

将长文本 3

作为输入文本

放到模型中，这样整个任

务就从文本

匹配转换为

了文本二分类。只需要对

一个 BERT 类模型进行微调即

可达到文本匹配的效果

。

能够进行上述特殊操作

的原因是基于 interaction

的深度学

习模型从本质上讲是对

输入文本做

相互的 Attention 操作

。而 BERT 类模型由于其自身的

block

层中以 multi-head attention 这种

Self-Attention 作为基础，对

文本自身词与词级别做

相似度矩阵的计算，因此

可以将这一特殊

的模型

使用技巧近似地看作一

种基于 interaction 的深度学习模型

操作。且通常使用这种操

作

训练模型与采用 BERT 模型

作为骨干模型的孪生神

经网络相比，拥有更好的

精度和更高的效

率。

16.4.8 深度

学习文本数据的翻译增

强

通常来说对于图像任

务，既能通过各种旋转、偏

移和缩放等对图像进行

增强，也能通过

mixup 技巧等在

训练过程中实现数据增

强。增强背后的意义在于

增加模型的健壮性，通过

加

入更多似是而非的图

像使模型获得更好的泛

化能力。

对于文本数据来

说，通常可以进行截断、平

移和抽取等操作，但是由

于文本内容的长度的不

确定性，截断、平移和抽取

的阈值往往难以设定，很

可能无法达到预期的增

强效果。对此，

一种比较取

巧的方法是，通过使用开

源的机器翻译模型或者

公开接口，对需要训练的

语料进

行翻译和回翻。这

样获取到的文本可以在

满足一定语义信息不变

的基础上增加文本的多

样

性。注意在这个过程中

要尽可能使用热门的语

言和常用外语进行翻译

，使用冷门语言的增强

效

果较差，甚至会起反作用

。

16.4.9 深度学习文本数据的预

处理

为了使用深度学习

模型，需要将文本数据转

化为对应的词编码值，这

样才能在 embedding 层

获得其映射

关系的词向量。此处我们

以

BERT 模型的训练为例，展示

对其文本进行预处理的

代码：

def build_dataset(config):

def load_dataset(path,

pad_size=32):

contents = []

with open(path,

'r', encoding='UTF-8') as f:

for line

in tqdm(f):

lin = line.strip()

if

not lin:

continue

try:

content, label

= lin.split('\t')

label = int(label)

label

= config.le.transform([label])[0]

except Exception as e:

continue

token = config.tokenizer.tokenize(content)

token =

[CLS] + token

seq_len = len(token)

mask = []

token_ids = config.tokenizer.convert_tokens_to_ids(token)

if pad_size:

if len(token) < pad_size:

mask = [1] * len(token_ids) +

[0] * (pad_size - len(token))

token_ids

+= ([0] * (pad_size - len(token)))

else:

mask = [1] * pad_size

token_ids = token_ids[:pad_size]

seq_len = pad_size

contents.append((token_ids, int(label), seq_len, mask))

return contents

train = load_dataset(config.train_path, config.pad_size)

dev =

load_dataset(config.dev_path, config.pad_size)

test = load_dataset(config.test_path, config.pad_size)

return train, dev, test

class DatasetIterater(object):

def __init__(self, batches, batch_size, device,shuffle=False):

self.batch_size

= batch_size

self.batches = batches

self.n_batches

= len(batches) // batch_size

self.residue =

False # 记录batch 数量是否为整

数

self.shuffle=shuffle

if

len(batches) % self.n_batches != 0:

self.residue

= True

self.index = 0

self.device

= device

def _to_tensor(self, datas):

x

= torch.LongTensor([_[0] for _ in datas]).to(self.device)

y = torch.LongTensor([_[1] for _ in

datas]).to(self.device)

# pad 前的长度（若长度超过

pad_size，则设为pad_size）

seq_len =

torch.LongTensor([_[2] for _ in datas]).to(self.device)

mask

= torch.LongTensor([_[3] for _ in datas]).to(self.device)

return (x, seq_len, mask), y

def

__next__(self):

if self.residue and self.index ==

self.n_batches:

batches = self.batches[self.index * self.batch_size:

len(self.batches)]

self.index += 1

batches =

self._to_tensor(batches)

return batches

elif self.index >

self.n_batches:

self.index = 0

raise StopIteration

else:

batches = self.batches[self.index * self.batch_size:

(self.index + 1) *

self.batch_size]

self.index

+= 1

batches = self._to_tensor(batches)

return

batches

def __iter__(self):

if self.shuffle:

np.random.shuffle(self.batches)

return self

def __len__(self):

if self.residue:

return self.n_batches + 1

else:

return

self.n_batches

def build_iterator(dataset, config,shuffle=False):

iter =

DatasetIterater(dataset, config.batch_size, config.device,shuffle)

return iter

在上述代码中构

建了数据迭代类

DataIterator，其作用

是进行数据的迭代加载

。在实际训练

过程中，我们

首先将文本 1 和文本 2 拼接

起来，两者的拼接关系需

要使用一个特殊的符号

[sep]

分割开。数据量可能非常

大，无法一次性完全加载

到内存中，此时可以构建

数据迭代

器 DataIterator，进行小批次

数据加载的迭代训练。

16.4.10 BERT 模

型的训练

在模型训练中

，还有一个关于数据增强

的技巧。我们可以使用预

训练词向量模型，通过设

定

阈值的方式，在模型的

训练过程中设定一个随

机比例，在满足相似度阈

值的前提下，随机将

利用

词向量相似度计算得到

的最相似的词作为近义

词，进行句子级别的文本

替换，以此增加

文本的多

变性。代码如下：

def synonyms_augmentation(texts,tokenizer,aug_rate=0.2,sample_size=3):

candidate_texts = texts[:int(len(texts)*aug_rate)]

raw_texts = texts[int(len(texts)*aug_rate):]

new_texts = []

for idx,text in

enumerate(candidate_texts):

words = text.split(' ')

indices

= np.random.choice(len(words), size=sample_size)

flag=0

for idx

in indices:

word = words[idx]

res

= word_syn_dict.get(word,[])

if len(res)>0:

res_idx =

np.random.choice(len(res), size=1)[0]

syn_word,syn_score = res[res_idx]

if

syn_score>0.75 and syn_word in tokenizer.word_index:

words[idx]

= syn_word

flag+=1

text = '

'.join(jieba.cut(''.join(words)))

new_texts.append(text)

texts = list(new_texts)+list(raw_texts)

return

texts

然而这种

方法存在一定的误差，即

可能会因为选取的近义

词存在相反的语义而使

得模型学到

错误的关系

（把替换后不应该匹配上

的文本当作匹配的标签

进行训练），所以要严格把

控增

强率和阈值的取值

。另外，在训练过程中的增

加权重等手段可以作为

进一步的优化方案。

由于

我们无法直接在 word

token 级别对

模型进行语义增强，因此

可以对 BERT 预训练模型的

参

数进行对抗学习，通过在

训练过程中加入一定的

扰动、噪声误差使得模型

的泛化能力得到

进一步

提升。早在

2016 年，Goodfellow 就提出了 FGM，其

增加的扰动为：

新增的对

抗样本为：

通过增加对抗

样本，可以与

CV 任务训练类

中对图像进行变换操作

进行类比，从而实现数据

增强的效果。对应代码为

：

class FGM():

def __init__(self,

model):

self.model = model

self.backup =

{}

def attack(self, epsilon=0.9, emb_name=["word_embeddings"]):

#

emb_name 参数要换成你模型中embedding 的

参数名

for name, param

in self.model.named_parameters():

if param.requires_grad and any([p

in name for p in emb_name]):

self.backup[name] = param.data.clone()

norm = torch.norm(param.grad)

if norm != 0:

r_at =

epsilon * param.grad / norm

param.data.add_(r_at)

def restore(self, emb_name=["word_embeddings"]):

# emb_name 参数要换成你模

型中embedding

的参数名

for name, param in self.model.named_parameters():

if param.requires_grad and any([p in name

for p in emb_name]):

assert name

in self.backup

param.data = self.backup[name]

self.backup

= {}

模型的训

练函数代码如下：

def train(config, model,

train_iter, dev_iter, test_iter):

if hasattr(config,'loss_type'):

loss_type

= config.loss_type

else:

loss_type='cce'

if loss_type=='bce':

loss_function = nn.BCEWithLogitsLoss()

else:

loss_function =

nn.CrossEntropyLoss()

start_time = time.time()

model.train()

param_optimizer

= list(model.named_parameters())

no_decay = ['bias', 'LayerNorm.bias',

'LayerNorm.weight']

optimizer_grouped_parameters = [

{'params': [p

for n, p in param_optimizer if

not any(nd in n for nd

in no_decay)],

'weight_decay': 0.05},

{'params': [p

for n, p in param_optimizer if

any(nd in n for nd in

no_decay)],

'weight_decay': 0.0} ]

optimizer =

BertAdam(optimizer_grouped_parameters,

lr=config.learning_rate,

warmup=0.05,

t_total=len(train_iter) * config.num_epochs)

total_batch = 0 # 记录进

行到多少batch

dev_best_loss

= float('inf')

last_improve = 0 #

记录上次验证

集损失下降的batch 数

flag = False #

记录是

否很久没有得到效果提

升

model.train()

fgm = FGM(model)

for

epoch in range(config.num_epochs):

print('Epoch [{}/{}]'.format(epoch +

1, config.num_epochs))

for i, (trains, labels)

in enumerate(train_iter):

outputs = model(trains)

model.zero_grad()

if loss_type=='bce':

bs = labels.size()[0]

labels_one_hot

= torch.zeros(bs, config.num_classes).

to(config.device).scatter_(1, labels.view(-1,1), 1)

loss = loss_function(outputs,labels_one_hot)

else:

loss =

loss_function(outputs,labels)

loss.backward()

fgm.attack()

outputs_adv = model(trains)

loss_adv = loss_function(outputs_adv,labels)

loss_adv.backward()

fgm.restore()

optimizer.step()

if total_batch % 100 == 0:

# 每经过多少轮，输出一

次在训练集和验证集上

的效果，用来展示当前的

效果并缓存

# 模型权重

true =

labels.detach().cpu()

predic = torch.max(outputs.data, 1)[1].cpu()

train_acc

= metrics.accuracy_score(true, predic)

dev_acc, dev_loss =

evaluate(config, model, dev_iter)

if dev_loss <

dev_best_loss:

dev_best_loss = dev_loss

torch.save(model.state_dict(), config.save_path)

improve = '*'

last_improve = total_batch

else:

improve = ''

time_dif =

get_time_dif(start_time)

msg = 'Iter: {0:>6}, Train

Loss: {1:>5.2}, Train Acc: {2:>6.2%}, Val

Loss: {3:>5.2}, Val Acc: {4:>6.2%}, Time:

{5} {6}'

print(msg.format(total_batch, loss.item(), train_acc, dev_loss,

dev_acc, time_dif, improve))

model.train()

total_batch +=

1

if total_batch - last_improve >

config.require_improvement:

# 若

超过1000batch，验证集损失还没下

降，就结束训练

print("No optimization for

a long time, auto-stopping...")

flag =

True

break

if flag:

break

test(config,

model, test_iter)

在训练过

程中，使用 FGM 对词向量部分

进行对抗攻击的代码如

下：

gm.attack()

outputs_adv = model(trains)

loss_adv = loss_function(outputs_adv,labels)

loss_adv.backward()

fgm.restore()

在整个训练过程中要

尽可能关注如下内容：

no_decay = ['bias',

'LayerNorm.bias', 'LayerNorm.weight']

optimizer_grouped_parameters = [

{'params':

[p for n, p in param_optimizer

if not any(nd in n for

nd in no_decay)],

'weight_decay': 0.05},

{'params':

[p for n, p in param_optimizer

if any(nd in n for nd

in no_decay)],

'weight_decay': 0.0} ]

optimizer

= BertAdam(optimizer_grouped_parameters,

lr=config.learning_rate,

warmup=0.05,

t_total=len(train_iter) *

config.num_epochs)

这

一部分代码主要是为了

调整模型的 finetune 学习速率（learning rate）、warmup 的

百分比

和训练的轮次等

。

BERT 模型的参数在一定程度

上会影响模型的收敛效

果，因此需要进行一定的

调参，找出合

适的配置之

后再进行模型训练。主要

涉及的参数包括 weight_decay、learning_rate 和

warmup 这三

个，当 warmup 的百分比设置得不

到位时，甚至有可能导致

BERT 模型在训练

过程中无法

收敛。

此外，在 BERT 类模型的训

练过程中，必须注意训练

所使用的 batch_size 的大小，要根

据

不同的场景和文本长度

，适当减少或者增加

batch_size 的值

，使得最大限度地利用显

存容量。

一般而言，对于 BERT 类

Medium 模型，使用

2080ti 级别的单卡就

可以满足大部分训练场

景，但如果需要使用 large sale 的 BERT

类

模型，则需要考虑增加显

卡的数量，通过多卡训练

满足其对显存的依赖。

16.5 模

型融合

在通常情况下，自

然语言处理的传统模型

训练方案可以采用前几

章提到的融合策略，使用

加

权平均、bagging或者 Stacking等方案进

行模型融合，训练多个模

型进行元特征的构建并

训练

二层模型输出最终

结果。

对于深度学习模型

而言，由于其本身的模型

复杂程度和时间消耗，并

不建议采用 Stacking方

案进行模

型的多折交叉训练。此时

加权平均是一种常见的

对深度学习模型结果进

行融合的方

案。

16.6

赛题总结

16.6.1 更多方案

预训练模型的

其他方案

对于采用的深

度学习模型而言，不必局

限于 BERT 类模型，可以考虑包

括但不限于

RoBERTa-wwm-ext，GPT，GPT3 等最的新模

型。

人工特征提取的其他

方案

针对人工提取的传

统文本特征，还可以考虑

采用 simhash、wordnet 等方案来计算句

子

或者词之间的相似程度

，同时判断文本的长短差

距，统计文本中的标点符

号，提

前对文本构造 N-Gram term。在进

行 TF-IDF 特征相似度计算等方

案也可以增加模型

的泛

化能力。

问题的 pattern 挖掘

可以

使用 Python

代码进行问题的句

式挖掘，并且构造 one-hot 特征加

入模型中进行

训练。

传统

机器学习的其他模型

由

于

XGBoost、LightGBM 等树模型并不适合用

来训练稀疏特征，且用传

统线性模

型无法捕获特

征交互，因此可以考虑使

用 FM 模型（Factorization Machine）进行训

练，其既有

效结合了 polynomial degree 2 的特征交互，又

在一定程度上保证了自

己计

算的线性时间复杂

度

，其中 为 FM 模型的 hidden_dim。

16.6.2

知识点

梳理

特征工程

本章用到

的特征主要有三类，即文

本的原始特征、文本的统

计特征和文本的相似度

特征。

深度学习

本章列举

了多种常见的自然语言

处理深度学习模型的搭

建、基于 PyTorch

框架构建

模型以

及预训练 BERT 模型等。

建模思

路

整体来讲，本章所讲的

赛题是一个比较标准的

数据挖掘与机器学习建

模问题，其训

练集与测试

集的分布高度耦合，使得

参赛者只需要专注于刻

画用户自身的消费行为

即可，然后通过机器学习

算法进行模型的训练与

预测。

16.6.3 延伸学习

对于自然

语言处理模型，除了 Kaggle 平台

的

Quora Question Pairs 这个竞赛中遇到的问

题之

外，其实实际面临还

有诸多的问题。其中最常

摆在业界数据工程师眼

前的问题是如何搭建一

个既满足上线时效要求

，又满足精度要求的模型

。此时，类似于 BERT

这样的大规

模，且具

有超大参数的预

训练模型往往无法在给

定的硬件条件下完成毫

秒级别的响应，而能够实

现这

一时效要求的 TextCnn 模型

由于其本身的复杂度无

法在训练过程中捕获文

本背后更为深层的

语义

关系和信息，达不到精度

要求。

如何在模型的预测

能力和运行效率之间选

择一个合适的中间值，是

构建模型时常需考虑的

问

题。在不考虑堆硬件的

前提下，业界对如何提升

模型效率提出了很多常

用的解决方案，比如

对模

型进行剪裁，或者使用一

个相对更小的模型进行

模型蒸馏。

此处额外对模

型蒸馏的相关问题进行

展开。常见的模型蒸馏手

段有基于输出结果的蒸

馏和逐

层蒸馏。对于一些

对线上效率有要求，又需

要尽可能提高精度的任

务，可以考虑先利用

BERT

模型

进行训练，得到一个 teacher 模型

，然后构建一个卷积神经

网络或者循环神经网络

模型作为 student 模型进行蒸馏

。对于标签的蒸馏，往往由

于无法选择合适的评价

指标——

使用何种损失函数

来评价

teacher 模型和 student 模型是“相

似”的，而导致蒸馏效果不

尽如人

意。

因此存在另一

种基于

GAN 的蒸馏训练方案

，在这种方案中，不需要定

义用来评价相似程度

的

损失函数，而是让鉴别器

（discriminator）自行学得。以下是一段基

于 LSTM 模型，

对

BERT 预训练结果进

行蒸馏的代码，其中利用

了 DRAGAN（Deep Regret Analytic

GAN），改造了

GAN 的损失函数，获

得了更好的蒸馏效果：

if config.do_train:

early_stop_rounds =

20

best_loss = 999

patient_count =

0

for epoch in trange(int(200), desc="Epoch"):

tr_loss = 0

tr_loss_D = 0

nb_tr_examples, nb_tr_steps = 0, 0

lr_scheduler.step()

model.train()

tr_gen = test_batch_generator(data_tr,batch_size=batch_size,

shuffle=True,maxlen=config.MAX_SEQUENCE_LENGTH,

tokenizer=tokenizer,bert_pred

= bert_prediction_tr,

use_mlm=config.use_mlm,

use_synonym_aug=config.use_synonym_aug)

step =

0

fgm = FGM(model)

for batch

in tr_gen:

input_ids,label_ids,bert_pred_tr = batch[0],batch[1],batch[2]

input_ids

= torch.LongTensor(input_ids).to(device)

label_ids = torch.LongTensor(label_ids).to(device)

if

bert_pred_tr is not None:

bert_pred_tr =

torch.FloatTensor(bert_pred_tr).to(device)

bs = input_ids.size()[0]

if config.use_distill:

def train_discriminator(model,model_discriminator,

input_ids,label_ids,bert_pred_tr,

lambda_=0.25,K=5.0):

logits =

model(input_ids)

real_teacher_label = np.ones(bert_pred_tr.size()[0])

fake_student_label =

np.zeros(logits.size()[0])

bin_label = np.concatenate([real_teacher_label,

fake_student_label])

bin_label

= torch.FloatTensor(bin_label).to(device)

label_ids_class = torch.cat([label_ids,label_ids])

x

= torch.cat([bert_pred_tr,logits])

D_real,real_classification = model_discriminator(bert_pred_tr)

D_real_loss

= loss_function_discriminator(D_real,

torch.FloatTensor(real_teacher_label).to(device))

D_fake,fake_classification = model_discriminator(logits)

D_fake_loss = loss_function_discriminator(D_fake,

torch.FloatTensor(fake_student_label).to(device))

out_classification =

torch.cat([real_classification,

fake_classification])

loss_clf = loss_function_classification(out_classification,

label_ids_class)

""" DRAGAN Loss ( 梯

度惩罚项) """

alpha = torch.rand((bs, 1))

alpha =

alpha.to(device)

x_p = bert_pred_tr + 0.5

* bert_pred_tr.std() *

torch.rand(bert_pred_tr.size()).to(device)

differences =

x_p - bert_pred_tr

interpolates = bert_pred_tr

+ (alpha * differences)

interpolates.requires_grad =

True

pred_hat,_ = model_discriminator(interpolates)

gradients =

grad(outputs=pred_hat,

inputs=interpolates,

grad_outputs=torch.ones(pred_hat.size()).to(device),

create_graph=True, retain_graph=True, only_inputs=True)[0]

gradient_penalty = lambda_ * ((gradients.view(gradients.size()[0],

-1).norm(2,

1) - 1) ** 2).mean()

loss_bin

= (D_real_loss + D_fake_loss) + gradient_penalty

return loss_bin,loss_clf

loss_bin,loss_clf = train_discriminator(model,model_discriminator,

input_ids,label_ids,bert_pred_tr)

loss_discriminator = loss_bin*0.5+loss_clf*0.5

loss_discriminator.backward()

optimizer_D.step()

tr_loss_D

+= loss_discriminator.item()

def train_student(model,model_discriminator,

input_ids,label_ids,bert_pred_tr,K=5.0):

logits

= model(input_ids)

out_bin,out_classification = model_discriminator(logits)

real_student_label

= Variable(torch.ones(

bert_pred_tr.size()[0]).to(device))

if config.multi_bin:

label_ids_one_hot

= torch.zeros(bs, target_size).to(device).

scatter_(1, label_ids.view(-1,1), 1)

loss = loss_function(logits,label_ids_one_hot)

else:

loss =

loss_function(logits,label_ids)

loss2 = loss_function_distil(logits,bert_pred_tr)

loss_clf =

loss_function_classification(

out_classification,label_ids)

loss_bin = loss_function_discriminator(out_bin,real_student_label)

return

loss,loss2,loss_bin,loss_clf

loss,loss2,loss_bin,loss_clf = train_student(model,

model_discriminator,input_ids,label_ids,bert_pred_tr,)

loss

= loss+loss2+loss_bin*0.5-loss_clf*0.5

loss.backward()

if config.use_adv:

fgm.attack()

loss,loss2,loss_bin,loss_clf = train_student(model,

model_discriminator,input_ids,label_ids,bert_pred_tr)

loss_adv =

loss+loss2+loss_bin*0.5-loss_clf*0.5

loss_adv.backward()

fgm.restore()

else:

logits =

model(input_ids)

if config.multi_bin:

label_ids_one_hot = torch.zeros(bs,

target_size).to(device).

scatter_(1, label_ids.view(-1,1), 1)

loss =

loss_function(logits,label_ids_one_hot)

loss.backward()

else:

loss = loss_function(logits,label_ids)

loss.backward()

if config.use_adv:

fgm.attack()

logits_adv =

model(input_ids)

loss_adv = loss_function(logits_adv,label_ids)

loss_adv.backward()

fgm.restore()

tr_loss += loss.item()

torch.nn.utils.clip_grad_norm(model.parameters(),2)

optimizer.step()

nb_tr_examples

+= input_ids.size(0)

nb_tr_steps += 1

model.zero_grad()

if config.use_distill:

model_discriminator.zero_grad()

step+=1

tr_loss /=

nb_tr_steps

tr_loss_D /= nb_tr_steps

model.eval()

eval_loss,eval_acc

= evaluation(data_te,model)

if best_loss>eval_loss:

best_loss =

eval_loss

torch.save(model.state_dict(), dump_path+generator_file_name)

if use_distill:

torch.save(model_discriminator.state_dict(),

dump_path+discriminator_file_name )

patient_count = 0

else:

if patient_count>=early_stop_rounds:

break

else:

patient_count+=1

作者简介

王贺

（鱼遇雨欲语与余）

现任职

于小米商业算法部，从事

应用商店广告推荐的研

究和开发。从 2018 年至 2020 年多次

参加国内外算法竞赛，共

获得

5 次冠军和 5 次亚军，是

2019 年和 2020

年腾讯广告算法大

赛

的冠军。毕业于武汉大

学计算机学院，硕士学位

，研究方向为图数据挖掘

。

刘鹏

华为技术有限公司

算法工程师，从事电信运

营商和智能运维领域的

算法研究和开发工作。

2016 年

本科毕业于武汉大学数

学基地班，保研至中国科

学技术大学自动化系，硕

士期间研

究方向为复杂

网络与机器学习，2018 年起多

次获得机器学习相关竞

赛奖项。

钱乾

数程科技大

数据技术负责人，工作方

向为物流领域的智能算

法应用。本科就读于美国

佐治亚

理工大学，研究方

向包括机器学习、深度学

习、自然语言处理等。

看完

了

如果您对本书内容有

疑问，可发邮件至contact@turingbook.com，会有编

辑或作译者协助答

疑。也

可访问图灵社区，参与本

书讨论。

如果是有关电子

书的建议或问题，请联系

专用客服邮箱：ebook@turingbook.com。

在这里可

以找到我们：

微博 @图灵教

育

: 好书、活动每日播报

微

博 @图灵社区 : 电子书和好

文章的消息

微博 @图灵新

知 : 图灵教育的科普小组

微信 图灵访谈 :

ituring_interview，讲述码农

精彩人生

微信 图灵教育

: turingbooks

091507240605ToBeReplacedWithUserId
