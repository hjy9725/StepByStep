


































industrial practitioners that a

diffusion-based approach that only

sees training dataset videos

can effectively boost the

performance of the validation

set with layout and

captions.

行业从

业者可能会

对结果感兴

趣，即仅查看

训练数据集

视频的基于

扩散的方法

可以有效地

提高带有布

局和标题的

验证集的性

能。

Figure 8: Visualization

of instance and scene

editing. (a) shows the

instance-level control result, such

as the appearance attributes

of all vehicles. (b)

shows the scene-level control

result, including weather and

time.

图8:实例和

场景编辑的

可视化。(a)显示

实例级控制

结果，如所有

车辆的外观

属性。(b)显示场

景级控制结

果，包括天气

和时间。

8.5Ablation Studies

8.6消融

研究

We

perform ablation studies to

showcase the effectiveness of

our method.

我们进

行消融研究

来展示我们

方法的有效

性。

Ablating Sim-Real Gap. To

further evaluate the sim-

to-real gap, we train

the UniAD with different

portions of synthetic data.

At the top of

Table 3, we train

UniAD with purely generated

video clips, and the

collision rate increases from

0.34 to 0.50. This

indicates that the synthetic

data cannot yet fully

replace the real data.

By contrast, if we

consider the incremental learning

setting, while we train

UniAD with additional data,

using syn- thetic data

results in a much

better performance while using

additional real data deteriorates

the performance from 0.34

to 0.38.

消融虚拟

现实的鸿沟

。为了进一步

评估模拟与

真实之间的

差距，我们用

不同部分的

合成数据来

训练UniAD。在表3的

顶部，我们用

纯生成的视

频片段训练

UniAD，碰撞率从0.34增

加到0.50。这表明

合成数据还

不能完全取

代真实数据

。相比之下，如

果我们考虑

增量学习设

置，当我们用

额外的数据

训练UniAD时，使用

合成数据会

产生更好的

性能，而使用

额外的真实

数据会使性

能从0.34恶化到

0.38。

Table 3: Performance of

end-to-end model with real

and generated data. We

train the second stage

of UniAD [13] with

the officially released weights

of the first stage

as a starting point.

表3:使用真实

数据和生成

数据的端到

端模型的性

能。我们以官

方发布的第

一阶段的权

重作为起点

来训练UniAD [13]的第

二阶段。

Method	

1s	Col.

Rate(%)↓ 2s	3s

Avg. 

Real	0.07

0.24	0.70	0.34

Generated

0.17	0.37	0.97	0.50

Real+Real	0.03	0.33	0.77

0.38

Real+Generated	0.08	0.18

0.56	0.27

方法

1s

颜色变化率

(%)↓ 2s	3s

平均值。

真实

的	0.07	0.24

0.70	0.34

生成的	0.17

0.37	0.97	0.50

真

实+真实

0.03	0.33	0.77	0.38

真实

+生成	0.08	0.18	0.56

0.27

Ablating Scene and Instance

Editing. Table 4 demon-

strates the effectiveness of

data diversity for end-to-end

mod- els. Specifically, we

edited existing scenes in

two approaches: scene-level editing

and instance-level editing. This

fancy function allows us

to generate a large

amount of new data

from a limited amount

of existing data. As

shown in Table 4,

simultaneous editing of both

the scene and instances

yields the best performance.

Leveraging powerful precise control-

lability, Delphi maximizes end-to-end

model performance by generating

richer and more diverse

data.

消融场

景和实例编

辑。表4展示了

端到端模型

的数据多样

性的有效性

。具体来说，我

们用两种方

法编辑现有

场景:场景级

编辑和实例

级编辑。这个

奇特的功能

让我们可以

从有限的现

有数据中生

成大量的新

数据。如表4所

示，同时编辑

场景和实例

可以获得最

佳性能。利用

强大的精确

控制能力，Delphi通

过生成更丰

富、更多样的

数据，最大限

度地提高了

端到端模型

的性能。



Table

4: The effectiveness of

Delphi’s pre- cise controllability

on end-to-end models.

表4:德

尔福精确可

控性在端到

端模型上的

有效性。

0.10

0.20	0.71	0.34

✓

0.11	0.18	0.64	0.31

✓

✓	✓

0.05

0.08 	0.20

0.18 	0.62

0.56

0.29

0.27

0.10	0.20	0.71	0.34

✓	0.11	0.18	0.64

0.31

✓

✓	✓

0.05

0.08	0.20

0.18

0.62

0.56	0.29

0.27

Ablating

NRM and FTCM. In

Table 5, we validate

the two modules, the

Noise Reinitialization Module (NRM)

and the Feature-aligned Temporal

Consistency Module (FTCM). We

see an evident increase

in all metrics to

validate the effectiveness of

our proposed method. In

particular, the FTCM structure

improves FID from 22.85

to 19.81 while the

NRM further boosts it.

消融

NRM和FTCM。在表5中，我

们验证了两

个模块，噪声

重新初始化

模块(NRM)和特征

对齐的时间

一致性模块

(FTCM)。我们看到所

有指标都有

明显的提高

，证明了我们

提出的方法

的有效性。特

别是，FTCM结构将

FID从22.85提高到19.81，而

NRM进一步提高

了FID。





9Conclusion

10结论



Table 5:

Ablation study results for

our proposed NRM and

FTCM.



表5:我

们提议的NRM和

FTCM的消融研究

结果。

Delphi

15.08275.686.73 

德尔斐

15.08275.686.73

w/o

NRM	19.81	291.5	85.22

w/o NRM & FTCM

22.85	346.96	82.91

不含NRM

19.81	291.5	85.22

不含NRM和

FTCM

22.85	346.96	82.91

In summary,

we propose a novel

video generation method for

autonomous driving scenarios that

can synthesize up to

40 frames of videos

on nuScenes dataset. Surprisingly,

we show that with

a diffusion model trained

only with training split,

we are able to

improve the performance of

the end-to-end planning model

by a sample efficient

failure-case driven framework. We

hope to shed light

on addressing the data

scarcity problem for both

researchers and practitioners in

this field, and make

a solid step towards

making autonomous driving vehicles

safe on the road.

总之，我们提

出了一种新

的自动驾驶

场景视频生

成方法，可以

在nuScenes数据集上

合成多达40帧

的视频。令人

惊讶的是，我

们表明，利用

仅用训练分

割训练的扩

散模型，我们

能够通过样

本有效的故

障案例驱动

框架来提高

端到端规划

模型的性能

。我们希望为

该领域的研

究人员和从

业人员解决

数据稀缺问

题提供帮助

，并朝着自动

驾驶汽车安

全上路迈出

坚实的一步

。

局限性和社

会影响。我们

的Delphi将BEV布局作

为输入，以确

保控制能力

，即我们只能

丰富外观，不

能在合成过

程中改变布

局。这导致了

一个限制，即

我们的框架

只能用于开

环设置[2]，而不

能用于闭环

设置。然而，另

一个限制是

，当端到端模

型在训练数

据集中表现

完美时，我们

的失败案例

驱动的采样

不起作用。就

社会影响而

言，我们相信

我们的方法

可以用来提

高端到端模

型的性能，并

可能有助于

未来大规模

自动驾驶汽

车的部署。





B.1Detailed

implementation of failure case

driven framework

B.2故障案

例驱动框架

的详细实现

Collecting Failure

Cases. Initially, we assess

the performance of the

base end-to-end model on

the validation set .

For this evaluation, we

utilize the UniAD [13]

base model as our

starting point. We employ

a metric, wherein a

collision occurring within 3

seconds on the path

planned by the algorithm

qualifies a scenario as

a failure case. Additionally,

to gain further insights,

we visualize both the

perception results of the

3D boxes and the

planning outcomes derived from

the end-to-end model. Through

this process, we identify

and select failure cases

for further analysis.

收集失败案

例。最初，我们

在验证集上

评估基本端

到端模型的

性能。在本次

评估中，我们

利用UniAD

[13]基础模

型作为起点

。我们采用一

种度量标准

，其中在算法

规划的路径

上3秒钟内发

生的碰撞将

一个场景视

为失败案例

。此外，为了获

得进一步的

见解，我们将

3D盒子的感知

结果和从端

到端模型中

得出的规划

结果可视化

。通过这个过

程，我们识别

并选择失败

案例进行进

一步分析。





Analyzing

data pattern. We initially

anticipated that large visual-language

models would be able

to automatically pinpoint the

reasons behind algorithm failures.

However, our investigations revealed

that a straightforward inquiry

was insufficient for this

purpose. Consequently, we devised

a multi-round inquiry method

leveraging VLM [1]. This

approach enables a more

precise analysis of the

factors contributing to algorithm

failures, as well as

a detailed description of

the key elements leading

to such failures.

分

析数据模式

。我们最初预

计大型视觉

语言模型将

能够自动查

明算法失败

背后的原因

。然而，我们的

调查显示，一

个简单的调

查是不够的

。因此，我们设

计了一种利

用VLM的多轮调

查方法[1]。这种

方法能够更

精确地分析

导致算法失

败的因素，并

详细描述导

致这种失败

的关键因素

。

Specifically, we feed the

visualization outcomes from the

preceding step into VLM

and prompt it to

discern whether the primary

cause of failure stems

from perception or planning

issues. In cases where

perception is the culprit,

the reasons can be

further categorized into various

factors such as nighttime

darkness, challenges in recognizing

large nearby objects, or

the inability to identify

rare object categories. On

the other hand, if

planning is identified as

the source of failure,

VLM can differentiate between

scenarios like occlusion or

infrequent interactions, including running

a red light or

crossing the road. Ultimately,

based on the previously

established reasons for failure,

we prompt VLM to

offer a precise account

of the specific factors

that led to the

failure.

具体来说，我

们将前一步

的可视化结

果输入VLM，并提

示它辨别失

败的主要原

因是源于感

知还是计划

问题。在感知

是罪魁祸首

的情况下，原

因可以进一

步分为各种

因素，如夜间

黑暗、识别附

近大型物体

的挑战或无

法识别罕见

物体类别。另

一方面，如果

规划被确定

为失败的根

源，VLM可以区分

堵塞或不频

繁互动等场

景，包括闯红

灯或过马路

。最终，根据先

前确定的失

败原因，我们

要求VLM提供导

致失败的具

体因素的准

确说明。

Retrieving similar scenes. Using

the detailed image description,

we employ BLIP-2 [19]

to identify and retrieve

scenes from the train

set that closely correspond

to the reasons behind

the failure. This process

involves quantifying the cosine

similarity between embeddings extracted

from both the image

and the designated text

input using BLIP-2. Based

on this similarity measure,

we then select and

retrieve only the top-k

most relevant images.

检索

相似场景。使

用详细的图

像描述，我们

使用BLIP-2

[19]从列车

组中识别和

检索与故障

背后的原因

紧密对应的

场景。这个过

程包括使用

BLIP-2量化从图像

和指定文本

输入中提取

的嵌入之间

的余弦相似

性。基于这种

相似性度量

，我们然后只

选择和检索

前k个最相关

的图像。





Updating

Model. Based on the

identified potential failure scenarios,

we created an extensive

and varied image dataset

utilizing Delphi. We obtained

scene captions from VLM

using corresponding sample tokens

and employed these captions

as input to generate

analogous images with Delphi.

更新

模型。基于确

定的潜在故

障场景，我们

利用Delphi创建了

一个广泛多

样的图像数

据集。我们使

用相应的样

本令牌从VLM获

得场景字幕

，并使用这些

字幕作为输

入，用Delphi生成类

似的图像。

To augment data

diversity, we employed a

LLM to adjust the

captions inputted into Delphi.

This approach facilitated the

alteration of scene descriptions

to encompass various scene

conditions and instance conditions,

such as sunny, rainy,

cloudy, Night, suburban, changing

the color of the

cars. Consequently, feeding these

revised captions into Delphi

resulted in the generation

of a broader range

of images.

为

了增加数据

的多样性，我

们使用了一

个LLM来调整输

入到Delphi中的标

题。这种方法

有助于场景

描述的改变

，以包含各种

场景条件和

实例条件，例

如晴天、雨天

、阴天、夜晚、郊

区、改变汽车

的颜色。因此

，将这些修改

过的标题输

入到Delphi中导致

了更广泛的

图像的产生

。

However,

we discovered that directly

utilizing the generated failure

scenes for training could

result in overfitting. While

the trained model excelled

in the selected failure

instances, its performance suffered

in previously successful cases.

Therefore, to mitigate this

issue, we integrated our

generated data with the

complete train set for

each fine-tuning session. This

strategy proved effective in

optimizing the model’s overall

performance.

然而，我们发

现直接利用

生成的失败

场景进行训

练可能会导

致过度拟合

。虽然经过训

练的模型在

选定的失败

案例中表现

出色，但其性

能在以前的

成功案例中

受到影响。因

此，为了缓解

这个问题，我

们将生成的

数据与每个

微调会话的

完整训练集

相集成。这种

策略在优化

模型的整体

性能方面被

证明是有效

的。

Ultimately, we

trained the end-to-end model

using this combined dataset,

yielding a refined model

that marked the commencement

of the subsequent iteration

of the improvement cycle.

最终，我们

使用这个组

合的数据集

训练端到端

模型，产生一

个改进的模

型，它标志着

改进周期的

后续迭代的

开始。





CExperiments

D实验

D.1Metrics

D.2韵

律学

Metrics

about Quality and Controllability

of Generated Video. We

evaluate the quality of

the generated videos from

two aspects: quality and

controllability. Specifically, for quality,

we use Frechet Inception

Distance (FID) [7] to

assess the realism of

single-frame single-view images in

the generated videos, Frechet

Video Distance (FVD) [35]

to evaluate the temporal

consistency of

关于生

成视频的质

量和可控性

的指标。我们

从两个方面

评估生成视

频的质量:质

量和可控性

。具体来说，对

于质量，我们

使用弗雷歇

初始距离(FID) [7]来

评估生成的

视频中单帧

单视图图像

的真实性，弗

雷歇视频距

离(FVD)

[35]来评估





single-view

videos, and CLIP scores

(CLIP) [42] to assess

the spatial consistency of

single-frame multi-view images. For

controllability, we utilize the

popular BEV detection model

StreamPETR [37] and end-to-end

model [13] to evaluate

the generated data and

report the NDS score

and the Average Collision

Rate(Avg. Col. Rate) respectively,

which comprehensively reflects the

geometric alignment between the

generated images and the

BEV layout annotations. By

using these evaluation metrics,

we can ensure that

the generated results maintain

high standards in both

quality and controllability.

单

视图视频和

剪辑分数(CLIP)

[42]来

评估单帧多

视图图像的

空间一致性

。对于可控性

，我们利用流

行的BEV检测模

型StreamPETR [37]和端到端

模型[13]来评估

生成的数据

，并报告NDS分数

和平均碰撞

率(Avg。Col. Rate ),其全面反

映了所生成

的图像和BEV布

局注释之间

的几何对准

。通过使用这

些评估指标

，我们可以确

保生成的结

果在质量和

可控性方面

保持高标准

。

Metrics about Effectiveness

of the Generated Video

for End-to-End Model. To

evaluate the effectiveness of

our proposed failure-case driven

framework based upon the

Delphi for the end-to-end

model, we utilize the

generated diverse training data

to augment the end-to-end

model’s origin training data.

Specifically, we evaluate the

performance of the end-to-end

model by applying data

augmentation on the nuScenes

validation set and report

the average collision rate.

端到端模型

生成的视频

有效性的度

量。为了评估

我们提出的

基于德尔菲

法的端到端

模型的故障

案例驱动框

架的有效性

，我们利用生

成的不同训

练数据来扩

充端到端模

型的原始训

练数据。具体

来说，我们通

过在nuScenes验证集

上应用数据

扩充来评估

端到端模型

的性能，并报

告平均冲突

率。





D.3More

Experimental Details

D.4更多实验

细节

Experimental

Setting of the end-to-end

model. During the training

phase, we utilize the

model available on the

UniAD official repository as

our foundation for fine-tuning.

To enhance the training

process, we have decreased

the learning rate by

a factor of 10,

setting it to 2e-5.

Additionally, we maintain consistency

with the hyperparameters recommended

on the UniAD repository,

including the optimizer settings.

端到端

模型的实验

设置。在培训

阶段，我们利

用UniAD官方知识

库上的模型

作为微调的

基础。为了增

强训练过程

，我们将学习

率降低了10倍

，设置为2e-5。此外

，我们与UniAD存储

库上推荐的

超参数保持

一致，包括优

化器设置。





Computation

Efficiency and Hardware RequirementsWe

report the model complexity

of our two model

variants in Table 6.

We will further provide

the generated data on

the nuScenes training set

for the convenience of

data augmentation.

计

算效率和硬

件要求我们

在表6中报告

了两个模型

变体的模型

复杂性。我们

将进一步在

nuScenes训练集上提

供生成的数

据，以便于数

据扩充。

表6:模

型效率和硬

件要求。

Model	Parameter	Inference Memeory&GPU

Inference Time	Train config

模型

参数	推理存

储器和GPU	推理

时间	列车配

置

multi-view single-frame	0.5B

22GB(RTX3090)	4s / example

8×A100, 24 hours

multi-view

multi-frame	1.1B	39GB(A100 40G)

4s / example	8×A800,

72 hours

多视图单

帧	0.5亿

22GB(RTX3090)	4s /示例	8×A100，24小

时

多视图多

帧	1.1B	39GB(A100 40G)

4s /示例	8×A800，72小时



D.5Validating each components of

our failure-case driven framework

D.6验证我们的

失败案例驱

动框架的每

个组件

As in Table

2, we compare in

three aspects, data sampling

strategy, number of generating

cases and data engine

validation.

如表

2所示，我们从

三个方面进

行比较，数据

采样策略、生

成案例的数

量和数据引

擎验证。

Data Sampling Strategy. We

evaluated different data sampling

strategies, such as random

sampling and failure-case targeted

sampling. In the upper

part of Table 2,

we randomly selected various

proportions of data samples

from the training dataset

and used the corresponding

BEV layout and original

scene captions to generate

new data. In the

lower part of Table

2, we retrieved training

data with similar patterns

to failure cases from

the validation set and

generated diverse weather data

using the powerful control

capabilities of the generative

model. The newly generated

data was mixed with

the original data to

train the end-to-end model.

It was observed that

the end-to-end model, enhanced

through failure-case guided data

augmentation, achieved the best

performance. This demonstrates that

the end-to-end model is

under-trained in these failure

cases, and feeding it

more failure-case related training

data can achieve optimal

generalization performance with fewer

computational resources.

数据

采样策略。我

们评估了不

同的数据抽

样策略，如随

机抽样和失

败案例目标

抽样。在表2的

上半部分，我

们从训练数

据集中随机

选择各种比

例的数据样

本，并使用相

应的BEV布局和

原始场景字

幕来生成新

数据。在表2的

下半部分，我

们从验证集

中检索了与

故障案例模

式相似的训

练数据，并使

用生成模型

的强大控制

功能生成了

不同的天气

数据。新生成

的数据与原

始数据混合

，以训练端到

端模型。据观

察，通过故障

案例引导的

数据扩充增

强的端到端

模型实现了

最佳性能。这

表明，端到端

模型在这些

故障情况下

训练不足，向

其提供更多

故障情况相

关的训练数

据可以用更

少的计算资

源实现最佳

的泛化性能

。

Numbers of Cases. We

investigated the quantity of

data samples. We randomly

sampled 14,065 and 28,130

training samples (approximately 50%

and 100% of the

entire training set) from

the training set. The

results generated by the

configuration of the generative

model on these samples

were used for data

augmentation. As shown in

the upper part of

Table 2, the performance

of the end-to-end model

worsened as the number

of samples increased. This

indicates that using training

data with a style

similar to the original

training set can only

help the model to

a limited extent. Thus,

it prompted us to

consider increasing the diversity

of the training data.

案件数量。我

们调查了数

据样本的数

量。我们从训

练集中随机

抽取了14，065和28，130个

训练样本(大

约是整个训

练集的50%和100%)。由

生成模型在

这些样本上

的配置生成

的结果被用

于数据扩充

。如表2的上半

部分所示，端

到端模型的

性能随着样

本数量的增

加而恶化。这

表明使用与

原始训练集

风格相似的

训练数据只

能在有限的

程度上帮助

模型。因此，这

促使我们考

虑增加训练

数据的多样

性。





Data

Engine. We tested various

data generation engines, including

Delphi and other state-of-the-art

generative models Panacea [40],

to compare their effectiveness

in generating high-quality training

data for model enhancement.

From the three sets

of comparison experiments, it

can be seen that

the

数据引擎

。我们测试了

各种数据生

成引擎，包括

Delphi和其他最先

进的生成模

型万能药[40]，以

比较它们在

为模型增强

生成高质量

训练数据方

面的有效性

。从三组对比

实验可以看

出

data generated by Delphi

effectively improves the performance

of the end-to-end model

compared to other generative

models. This is due

to Delphi’s superior fine

control capabilities in scene

generation, leading to more

diverse training data for

model tuning.

与其他生

成模型相比

，Delphi生成的数据

有效地提高

了端到端模

型的性能。这

是由于Delphi在场

景生成方面

卓越的精细

控制能力，导

致模型调优

的训练数据

更加多样化

。

[28]

[29]
