









曼。Dreambooth:微调主题

驱动生成的

文本到图像

扩散模型。IEEE/CVF

计

算机视觉

和

模式识别会

议论文集，第

22500-22510 页，2023。

[30]

斋藤正树

，松本荣一和

斋藤顺太。具

有奇异值裁

剪的时态生

成对抗网。IEEE 计

算机视

觉国

际会议论文

集，第 2830-2839

页，2017 年。

11

[31]

辛

格，亚当·波亚

克，托马斯·海

斯，席音，解安

，，胡启元，杨海

瑞，奥龙·阿舒

尔，奥兰·加夫

尼，等。制作视

频:不需要文

本-视频数据

的文本-视频

生成。arXiv 预

印本

arXiv:2209.14792，2022。

[32]

宋 家 明 ，

孟 和

斯 特 凡

诺 埃

尔 蒙 。

去 噪 扩

散 隐

式 模 型

。 arXiv

预 印 本

arXiv:2010.02502，2020。

[33] Alexander Swerdlow、徐润

生和周。从鸟

瞰图布局生

成街景图像

。arXiv 预印本

arXiv:2301.04634，2023。

[34] 谢尔

盖·图利亚科

夫、刘明宇、杨

晓东和扬·考

茨。Mocogan:分解视频

生成的运动

和

内容。IEEE

计算

机视觉和模

式识别会议

论文集，第 1526-1535 页

，2018 年。

[35] 托马斯·安

特辛纳、斯约

尔德·范·斯廷

基斯特、凯罗

尔·库拉奇、拉

斐尔·马里尼

尔、

马钦·米哈

尔斯基和西

尔万·盖利。走

向精确的视

频生成模型

:新的度量和

挑战。arXiv 预

印本

arXiv:1812.01717，2018。

[36] 卡尔·冯德里

克、哈米德·皮

尔西亚瓦什

和安东尼奥

·托雷巴。用场

景动态生成

视频。

神经信

息处理系统

进展，29，2016。

[37] 王世豪

、刘英飞、王沺

裁、李颖和张

翔宇。探索以

对象为中心

的时间建模

，用于有效的

多

视图 3d

对象

检测。arXiv 预印本

arXiv:2303.11926，2023。

[38] 、郑竹、、陈新泽

和陆继文。Drivedreamer:走

向自动驾驶

的真实世界

驱动世界模

型。arXiv

预印本 arXiv:2309.09777，2023。

[39] 、王

、、吕凡、、陈云涛

、。驾驶未来:自

动驾驶世界

模型的多视

角视觉预测

和规

划。arXiv 预印

本 arXiv:2311.17918，2023。

[40]

余庆文，赵

玉成，，，王艳辉

，，，，，，和张翔宇。灵

丹妙药:自动

驾驶的全景

可控

视频生

成。arXiv 预印本 arXiv:2311.16813，2023。

[41] 周

杰伦吴，葛，王

，雷伟贤，顾，许

永利，，肖虎和

郑寿。视频调

谐:文本到视

频生成

的图

像扩散模型

的一次性调

谐。arXiv 预印本

arXiv:2212.11565，2022。

[42] 杨

，马恩慧，，彭，，郭

庆，，俞开成。Bevcontrol:通

过 bev

草图布局

，精确控制多

视角

一致性

的街景元素

。arXiv 预印本 arXiv:2308.01661，2023。

[43] 杨泽

通，，，李红阳。视

觉点云预测

实现可扩展

的自动驾驶

。 arXiv 预印本

arXiv:2312.17655，2023。

[44] 翟江

田、泽峰、杜金

浩、毛永强、江

、谭子昌、张毅

夫、、王京东。重

新思考

nuscenes

中端

到端自动驾

驶的开环评

测。arXiv 预印本 arXiv:2305.10430，2023。

[45]

北

辰张，潘章，董

晓义，臧余杭

，。长剪辑:释放

剪辑的长文

本功能。arXiv 预印

本

arXiv:2403.15378，2024。

[46]

张和马涅

什·阿格拉瓦

拉。向文本到

图像扩散模

型添加条件

控制。arXiv 预印本

arXiv:2302.05543，2023。

[47] 赵，，郑竹，陈新

泽，，鲍晓义，。drive

dreamer-2:Llm-增

强的世界模

型，用于多

样

化的驾驶视

频生成。arXiv 预印

本 arXiv:2403.06845，2024。

[48] 周大全，，颜

汉书，，，朱，冯家

实。Magicvideo:利用潜在

扩散模型的

高效视频生

成。arXiv 预印本 arXiv:2211.11018，2022。

12

13

A 方

法

A.1 故障案例

驱动框架的

详细实现

收

集失败案例

。最初，我们在

验证集上评

估基本端到

端模型的性

能。在本次评

估中，我们

利

用

UniAD [13]基础模型

作为起点。我

们采用一种

度量标准，其

中在算法规

划的路径上

3

秒钟内发生

的碰撞将一

个场景视为

失败案例。此

外，为了获得

进一步的见

解，我们将 3D

盒

子的感知结

果和从端到

端模型中得

出的规划结

果可视化。通

过这个过程

，我们识别并

选择

失败案

例进行进一

步分析。

分析

数据模式。我

们最初预计

大型视觉语

言模型将能

够自动查明

算法失败背

后的原因。然

而，我们的调

查显示，一个

简单的调查

是不够的。因

此，我们设计

了一种利用

VLM 的多轮

调查

方法[1]。这种方

法能够更精

确地分析导

致算法失败

的因素，并详

细描述导致

这种失败

的

关键因素。

具

体来说，我们

将前一步的

可视化结果

输入 VLM，并提示

它辨别失败

的主要原因

是源于感

知

还是计划问

题。在感知是

罪魁祸首的

情况下，原因

可以进一步

分为各种因

素，如夜间黑

暗、识别附近

大型物体的

挑战或无法

识别罕见物

体类别。另一

方面，如果规

划被确定为

失

败的根源

，VLM 可以区分堵

塞或不频繁

互动等场景

，包括闯红灯

或过马路。最

终，根据先

前

确定的失败

原因，我们要

求

VLM 提供导致

失败的具体

因素的准确

说明。

检索相

似场景。使用

详细的图像

描述，我们使

用 BLIP-2

[19]从列车组

中识别和检

索与故

障背

后的原因紧

密对应的场

景。这个过程

包括使用 BLIP-2 量

化从图像和

指定文本输

入中提

取的

嵌入之间的

余弦相似性

。基于这种相

似性度量，我

们然后只选

择和检索前

k 个最相关

的

图像。

更新模

型。基于确定

的潜在故障

场景，我们利

用

Delphi 创建了一

个广泛多样

的图像数据

集。我们使用

相应的样本

令牌从 VLM 获得

场景字幕，并

使用这些字

幕作为输入

，用

Delphi

生成类似

的图像。

为了

增加数据的

多样性，我们

使用了一个

LLM 来调整输入

到

Delphi 中的标题

。这种方法有

助于场景描

述的改变，以

包含各种场

景条件和实

例条件，例如

晴天、雨天、阴

天、夜晚、

郊区

、改变汽车的

颜色。因此，将

这些修改过

的标题输入

到 Delphi

中导致了

更广泛的图

像

的产生。

然

而，我们发现

直接利用生

成的失败场

景进行训练

可能会导致

过度拟合。虽

然经过训练

的模

型在选

定的失败案

例中表现出

色，但其性能

在以前的成

功案例中受

到影响。因此

，为了缓解

这

个问题，我们

将生成的数

据与每个微

调会话的完

整训练集相

集成。这种策

略在优化模

型的

整体性

能方面被证

明是有效的

。

最终，我们使

用这个组合

的数据集训

练端到端模

型，产生一个

改进的模型

，它标志着改

进

周期的后

续迭代的开

始。

B 实验

B.1 韵律

学

关于生成

视频的质量

和可控性的

指标。我们从

两个方面评

估生成视频

的质量 :质量

和可控

性。具

体来说，对于

质量，我们使

用弗雷歇初

始距离(FID) [7]来评

估生成的视

频中单帧单

视图图像的

真实性，弗雷

歇视频距离

(FVD)

[35]来评估

14

单视

图视频和剪

辑分数(CLIP) [42]来评

估单帧多视

图图像的空

间一致性。对

于可控性，我

们利用流行

的

BEV 检测模型

StreamPETR [37]和端到端模

型[13]来评估生

成的数据，并

报告

NDS

分数和

平均碰撞率

(Avg。Col. Rate ),其全面反映

了所生成的

图像和 BEV

布局

注释之间

的

几何对准。通

过使用这些

评估指标，我

们可以确保

生成的结果

在质量和可

控性方面保

持

高标准。

端

到端模型生

成的视频有

效性的度量

。为了评估我

们提出的基

于德尔菲法

的端到端模

型的

故障案

例驱动框架

的有效性，我

们利用生成

的不同训练

数据来扩充

端到端模型

的原始训练

数据。具体来

说，我们通过

在 nuScenes 验证集上

应用数据扩

充来评估端

到端模型的

性能，

并报告

平均冲突率

。

B.2 更多实验细

节

端到端模

型的实验设

置。在培训阶

段，我们利用

UniAD 官方知识库

上的模型作

为微调的基

础。为了增强

训练过程，我

们将学习率

降低了

10 倍，设

置为 2e-5。此外，我

们与 UniAD

存

储库

上推荐的超

参数保持一

致，包括优化

器设置。

计算

效率和硬件

要求我们在

表 6

中报告了

两个模型变

体的模型复

杂性。我们将

进一步在

nuScenes 训

练集上提供

生成的数据

，以便于数据

扩充。

表

6:模型

效率和硬件

要求。

模型 参

数 推理存储

器和

GPU 推理时

间 列车配置

多视图单帧

0.5

亿

22GB(RTX3090) 4s /示例 8×A100，24

小时

多视图多帧

1.1B 39GB(A100 40G) 4s

/示例 8×A800，72 小时

B.3

验

证我们的失

败案例驱动

框架的每个

组件

如表 2 所

示，我们从三

个方面进行

比较，数据采

样策略、生成

案例的数量

和数据引擎

验

证。

数据采

样策略。我们

评估了不同

的数据抽样

策略，如随机

抽样和失败

案例目标抽

样。在表 2

的上

半部分，我们

从训练数据

集中随机选

择各种比例

的数据样本

，并使用相应

的

BEV 布局和

原

始场景字幕

来生成新数

据。在表 2

的下

半部分，我们

从验证集中

检索了与故

障案例模式

相

似的训练

数据，并使用

生成模型的

强大控制功

能生成了不

同的天气数

据。新生成的

数据与原

始

数据混合，以

训练端到端

模型。据观察

，通过故障案

例引导的数

据扩充增强

的端到端模

型

实现了最

佳性能。这表

明，端到端模

型在这些故

障情况下训

练不足，向其

提供更多故

障情况

相关

的训练数据

可以用更少

的计算资源

实现最佳的

泛化性能。

案

件数量。我们

调查了数据

样本的数量

。我们从训练

集中随机抽

取了 14，065 和

28，130

个训

练样本(大约

是整个训练

集的 50%和 100%)。由生

成模型在这

些样本上的

配置生成的

结

果被用于

数据扩充。如

表 2 的上半部

分所示，端到

端模型的性

能随着样本

数量的增加

而恶

化。这表

明使用与原

始训练集风

格相似的训

练数据只能

在有限的程

度上帮助模

型。因此，

这促

使我们考虑

增加训练数

据的多样性

。

数据引擎。我

们测试了各

种数据生成

引擎，包括 Delphi 和

其他最先进

的生成模型

万能药

[40]，以比

较它们在为

模型增强生

成高质量训

练数据方面

的有效性。从

三组对比实

验可以

看出

与其他生成

模型相比，Delphi 生

成的数据有

效地提高了

端到端模型

的性能。这是

由于

Delphi

在场景

生成方面卓

越的精细控

制能力，导致

模型调优的

训练数据更

加多样化。

[15]

15
