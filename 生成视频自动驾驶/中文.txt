利用可控长

视频生成释

放端到端自

动驾驶的普

遍性

恩慧马

1，2，3∫李俊周 2∫陶唐

4，2 詹章

5，1 董翰 6，1 江

7，2

昆展 2

彭佳 2

仙

彭郎 2 海洋阳

光 2

号 狄林 3 凯

成

Yu1 1 西湖大学

2Li 汽车公司

3 天

津大学 4 中山

大学深圳校

区

5 东南大学

6 哈尔滨工程

大学 7

哈尔滨

工业大学(深

圳)

kyu}@westlake.edu.cn 马恩辉，zhoulijun@lixiang.com

图

1:我们方法的

概述。我们表

明:(

a)我们的 Delphi 可

以生成多达

40 帧的连续视

频，而(b)现有的

best

只能生成 8 帧

。(3)使用 Delphi

配备的

故障案例驱

动框架，(4)我们

可以用小得

多的成本显

著提

高端到

端模型的性

能。

摘要

使用

生成模型来

合成新数据

已经成为自

动驾驶中解

决数据稀缺

问题的事实

上的标准。尽

管现有方法

能够增强感

知模型，但我

们发现这些

方法无法提

高端到端自

动驾驶模型

的规划性能

，因为生成的

视频通常少

于

8 帧，并且

空

间和时间不

一致性不可

忽略。为此，我

们提出了 Delphi，一

种新的基于

扩散的长视

频生成方法

，具有跨多视

图的共享噪

声建模机制

以增加空间

一

致性，以及

特征对齐模

块以实现精

确的可控性

和时间一致

性。我们的方

法

可以生成

多达 40 帧的视

频而不损失

一致性，这比

最先进的方

法长约

5 倍。

我

们不再随机

生成新数据

，而是进一步

设计了一种

抽样策略，让

Delphi 生

成与那些

失败案例相

似的新数据

，以提高抽样

效率。这是通

过在预先训

练

的视觉语

言模型的帮

助下构建失

败案例驱动

的框架来实

现的。我们的

大量

实验表

明，我们的 Delphi

生

成了更高质

量的长视频

，超过了以前

最先进的

方

法。因此，仅生

成 4%的训练数

据集

*共同第

一作者

通讯

作者

预印本

。正在审查中

。

a

r

X

i

v:2

4

0

6.0

1

3

4

9

v

3

[

c

s。简

历

]

2

0

2

4

年

6

月

6

日

大小，我们的

框架能够超

越感知和预

测任务，据我

们所知，这是

第一次将

端

到端自动驾

驶模型的规

划性能提高

了 25%。请观看视

频演示

https://

westlake-autolab.github.io/delphi.github.io/。

1 介绍

端到端自动

驾驶最近受

到了越来越

多的关注[13，16，43]，它

直接从原始

传感器数据

中学习规

划

运动，减少了

对手工制作

的规则的严

重依赖，并避

免了级联模

块。然而，当前

的端到端模

型

在训练数

据的规模和

质量方面面

临着重大挑

战。数据多样

性不足会导

致模型过度

拟合[44]，例

如收

集的真实世

界轨迹主要

涉及“直行”动

作的直线，当

应用于更复

杂的场景如

“十字路口左

转”时，模型容

易失败。虽然

大规模和高

质量的注释

数据对于安

全和稳健的

端到端自动

驾驶系

统至

关重要，但不

幸的是，收集

这种数据带

来了挑战，特

别是在涉及

危险场景的

情况下，数据

收集可能很

困难或不安

全。

尽管最近

的生成模型

[5，40，42]在缓解感知

模型的数据

缺乏问题方

面取得了显

著的进展，

这

是通过采用

ControlNet [46]来控制具有

注入的 BEV

布局

的场景元素

的几何位置

并扩展到

视

图维度以生

成多视图图

像而实现的

。当应用于需

要长多视图

视频的端到

端自动驾驶

时，

会出现两

个主要挑战

:时空一致性

和精确可控

性。现有的生

成方法[5，40，38，39]简单

地

利用与先

前生成的帧

的跨帧注意

力来确保一

致性，这忽略

了图像生成

和视频生成

之间的噪

声

模式的差异

，以及跨帧注

意力中的特

征对准。因此

，只能在短视

频序列中保

持时间一致

性，例如 8 帧的

panacle[40]和 7

帧的 MagicDrive [5]。此外

，当前的方法

仅提供对生

成的

视频的

粗粒度控制

，限于修改简

单的全局属

性，例如用简

单的文本提

示改变天气

。它们无

法精

确控制场景

的整体建筑

风格或单个

对象的特定

外观属性。

为

此，我们提出

了一种新的

多视角长视

频生成方法

，称为 Delphi，以解决

这些限制。首

先，我们注意

到现有的方

法在两个方

面存在不足

:I)向不同的视

图添加独立

的噪声，并且

没

有考虑跨

视图的一致

性；ii)利用简单

的交叉注意

力来融合具

有不同接收

场的多个特

征空

间。然后

，我们提出了

两个组件，噪

声重新初始

化模块来模

拟跨帧的共

享噪声，以及

特征对

齐的

时间一致性

模块来解决

第二个挑战

。

利用 Delphi

作为数

据引擎，我们

进一步提出

了一个失败

案例驱动的

框架，它自动

增强了端

到

端模型的泛

化能力。具体

来说，这个框

架集成了几

个步骤，如图

1 所示:1)评估端

到端模

型，收

集失败案例

，

2)使用预先训

练的 VLMs 分析隐

含的数据模

式，3)从现有的

训练数据中

检索相似的

模式化数

据

，4)使用

Delphi 生成多

样化的训练

数据，然后更

新端到端模

型。为了研究

我们方法的

有

效性，我们

在大规模公

共数据集 nuScenes

[2]上

进行了大量

实验。首先，多

种指标的综

合

评价表明

，我们的 Delphi 生成

了高质量的

长多视点视

频，具有时空

一致性和精

确可控性。

此

外，所提出的

故障案例驱

动框架以较

低的成本显

著提高了端

到端模型的

泛化能力。

我

们的贡献可

以总结如下

:

• 我们介绍了

Delphi，这是一种新

颖的方法，可

以在自动驾

驶(AD)场景中生

成长达

12 秒

(40 帧

)的时间一致

的多视图视

频，比最先进

的视频生成

方法长

5 倍。此

外，Delphi 包

含了包

括对象和场

景级细节的

控制能力，以

丰富生成数

据的多样性

。

• 我们提出了

一个失败案

例驱动的框

架来大幅提

高采样效率

。我们表明，使

用纯粹在训

练

数据集上

训练的长期

视频生成方

法，我们能够

将 UniAD

的性能提

高 25%(冲突率从

0.34 降

低到

0.27)，而只

生成训练数

据集大小的

4% (972 例)。

2

• 与早期仅

使用合成数

据来提高感

知能力的工

作相比，就我

们所知，我们

是第一个展

示数

据引擎

可以超越感

知任务并自

动提高端到

端广告方法

的规划能力

的工作。我们

希望这能

有

助于缓解广

告车大规模

发展的长尾

问题。

2 相关著

作

端到端自

动驾驶。端到

端模型在自

动驾驶领域

获得了极大

的关注。这些

模型通过将

感知、

预测、决

策和规划集

成到一个学

习框架中，简

化了传统的

模块化管道

。TransFuser[25]将

视觉和激

光雷达输入

与基于变压

器的架构相

融合，以改善

感知和驾驶

决策。圣 P3[12]利用

时空特征学

习来改善感

知、预测和规

划任务。UniAD[13]有效

地结合了多

种感知和预

测任

务，以提

高规划性能

。VAD [16]探索了矢量

化场景表示

在规划和摆

脱密集地图

方面的潜

力

。VADv2[3]利用概率规

划来管理不

确定性，并将

多视图图像

序列转换为

环境标志嵌

入，

以预测和

采样车辆控

制动作。在本

文中，由于计

算资源的限

制，我们选择

利用众所周

知的

UniAD 作为我

们的下游模

型。

提升自动

驾驶性能的

生成模型。视

频生成是理

解视觉世界

的关键技术

。早期的方法

主要包

括变

分自动编码

器(VAEs) [4，14]，基于流的

模型[18]，以及生

成对抗网络

(GANs)

[23，30，34，36]。值得注意的

是，扩散模型

在图像生成

中的最新成

就[24，28，29]激发

了人

们对其在视

频生成中的

应用的兴趣

[6，10]。基于扩散的

方法已经显

著提高了真

实性、

可控性

和时间一致

性。由于其可

控的属性，基

于文本的条

件视频生成

已经获得了

越来越多

的

关注，导致了

许多方法的

出现[28，9，31，41，48]。特别是

流行的模型

，如基于扩散

的

模型[28，32，8，46]，使用

户能够生成

可控的图像

。受这种创新

的启发，一些

模型

[5，42]采用 ControlNet 通

过注入 BEV

布局

来控制场景

元素的几何

位置，并将这

种方法扩展

到视图维度

以生成多视

图图像。其他

模型[40，38，47]进一步

将其扩展到

时间维度以

生成

多视图

视频，这些视

频都是基于

预训练的图

像模型[28]来训

练的。BEVGen [33]专门基

于鸟

瞰图(BEV)布

局生成多视

图街道图像

。BEVControl

[42]提出了从 BEV 布

局创建图像

前景和

背景

的两阶段生

成流水线。DriveDreamer

[38]和

Panacea [40]引入了一个

布局调整的

视频

生成系

统，旨在为训

练感知模型

提供多样化

的数据源。GAIA-1 [11]和

ADriver-I

[15]将大

型语言

模型集成到

视频生成中

；并行工作 DriveDreamer-2 [47]提

出了仅采用

文本提示作

为

输入的交

通模拟流水

线，其可用于

生成用于驾

驶视频生成

的各种交通

状况，然而，其

需要

一帧输

入，并且在推

进规划任务

的设置中不

起作用。总而

言之，这些方

法只能生成

相当短

的视

频，最多 8

帧，而

我们的 Delphi 可以

生成长得多

的视频。

3

方法

在本节中，我

们首先介绍

Delphi，这是一种用

于生成自动

驾驶的长多

视图视频的

创新方

法。有

两个设计用

于生成时间

一致性视频

的核心模块

:第 3.1.1 节中的噪

声重新初始

化模块

(NRM)和第

3.1.2 节中的特征

对齐的时间

一致性模块

(FTCM)。最后，Sec。3.2 介绍了

一个失

败案

例驱动的框

架，以展示我

们如何利用

长期视频生

成能力，仅使

用来自训练

数据集的数

据来自动增

强端到端模

型的泛化。

3.1 Delphi:一

种可控的长

视频生成方

法

这里，我们

在图 2(a)中展示

了

Delphi 的架构。现

有模型倾向

于忽略跨时

间和空间维

度的噪

声公

式，导致较差

的长视频生

成质量。在…里

3

n

∈

∈

n n

n

4

图 2:(a)Delphi

的架构。它

将多视图视

频 z 和相应的

BEV(鸟瞰图)布局

序列作为输

入。每个视频

由 N

个帧和 V 个

视图组成。BEV 布

局序列首先

根据摄像机

参数被投影

到摄像机空

间，产生包括

前景和背景

布局的摄像

机布局。具体

来说，前景布

局包括边界

框的角坐标

、标题、实例

id 和

密集标题，而

背景

布局包

括不同颜色

的线来表示

道路走向。由

编码器处理

的布局嵌入

通过交叉关

注被注入到

U-Net 中，

以在生成

过程中实现

细粒度的布

局控制。此外

，我们利用 VLM [1]提

取输入场景

的密集字幕

，通过长

剪辑

[45]进行编码以

获得文本嵌

入，然后通过

文本交叉注

意将其注入

U-Net

以实现基于

文本的控制

。

我们进一步

设计了两个

关键模块，(b)噪

声重新初始

化模块，其包

含跨不同视

图共享噪声

，以及(c)特

征对

齐的时间一

致性模块，以

相应地确保

空间和时间

一致性。

相比

之下，我们提

出了两个关

键组件来解

决这些挑战

:噪声重新初

始化模块和

特征对齐的

时

间一致性

模块。

3.1.1 噪声重

新初始化模

块

多视图视

频在时间和

视图维度上

自然表现出

相似性。然而

，现有的方法

分为两组，I)并

行

单视图视

频生成方法

[22，27，26]不能直接应

用于室外多

视图场景；ii)多

视图生成模

型增

加了不

考虑交叉视

图一致性的

独立噪声[5，42，40]。这

里，我们计划

通过在这两

个维度上

引

入共享噪声

来解决这个

问题。具体来

说，如图 2(b)所示

，我们沿时间

维度引入共

享运动

噪声

m，沿视点维度

引入共享全

景噪声 p。这导

致多视图视

频的噪声版

本在时间和

视图维度

上

都是相关的

。合并共享噪

声的过程可

以表示如下

:

zv

=√αˊXV+√1αˊ(RV+mv+pn)， (1)

其中 zvR1×1×h×w

表示第

N 帧的视图 V 的

图像潜变量

，mRV

×1×h×w 是 V 视图的共

享运动噪声

，p

R1×N×h×w 是 N 帧的共享

全景噪声。为

了简单起见

，我们省略了

下标

t。

3.1.2 特征对

齐的时间一

致性

当生成

当前帧时，现

有方法[40，5，39]利用

简单的交叉

注意机制将

先前帧信息

融合到当前

视图中。然而

，他们往往忽

略了这样一

个事实，即位

于不同网络

深度的特征

拥有不同的

感

受域。因此

，这种粗糙的

特征交互方

法不能从前

一帧的不同

级别的感受

野捕获所有

信息，

导致次

优的视频生

成性能。

n

n1

n √

n1

d

n n1 √

d

k

1.收集

失败案例 2.分

析数据模式

分类 稀有类

别

端到端评

估

模型

感知

误差

失败案

例分类

附近

的大型物体

......

分类 是 规划

错误

列车组

冲突 > 0

遮挡场

景

罕见的互

动行为

场景

标题 展开标

题 给加说明

文字

编辑

场

景对象

场景

标题 1

一辆电

动自行车正

从“前方”穿过

马路。它位于

自动驾驶汽

车前方大约

10米处，略位于

车辆直接视

线的左侧。随

着电动自行

车的前进，它

的路径与自

动驾驶汽车

的轨迹相交

新建数据 德

尔斐 相似数

据 VLM

列车组

现

有数据

4.更新

模型 3.检索相

似场景

5

图 3:失

败案例驱动

框架概述。

为

此，我们提出

了一种更有

效的结构来

完全建立相

邻帧中相同

网络深度的

对齐特征之

间的

特征交

互，如图 2 (c)所示

。我们通过确

保全局一致

性和优化局

部一致性来

实现这一点

，结

合了两种

主要设计:场

景感知注意

力和实例感

知注意力。

场

景感知注意

力。为了充分

利用前一帧

中不同网络

深度特征的

丰富信息，我

们提出了场

景

级跨帧注

意机制。具体

来说，该模块

对相邻帧之

间相同网络

深度的特征

进行关注度

计算。

计算过

程可以表示

如下:

戚姬

t！

其

中 Qi(查询)是来

自特定网络

深度 I

处的当

前帧 n 的潜在

特征图，

I

n1

(关键

)和 V

i

(值)来自前

一帧 n1 的潜在

特征图是否

相同

网络深

度

I，dk 是密钥的

维数。并且 i =

1，...，I，其

中 I 表示总数

U 网块数。为了

简单起见，我

们省略了观

看频道。通过

应用这种场

景感知注意

机制，该模

块

有效地将全

局样式信息

从前一帧转

移到当前帧

，从而确保跨

帧的时间一

致性。

实例感

知注意力。为

了增强场景

中运动物体

的一致性，我

们提出了一

种实例感知

的跨帧注

意

机制。与场景

级注意力相

比，该模块使

用前景边界

框作为注意

力掩模来计

算相邻帧之

间

的局部区

域中的特征

交互。计算过

程可以表示

如下:

( 齐 M n

) ( K i

mn1)T

k

Qi =

Qi+Zero[Attn ins(Qi，Ki ，V

i

)]q， (4)

n n

n n1 n1

其中

Mn

和 mn1 分别是当

前帧和前一

帧的前景对

象的遮罩，聚

焦于由前景

边界框定义

的区

域，零表

示用值

0 初始

化的可训练

卷积层。

3.2 失败

案例驱动的

框架

为了利

用生成的数

据，常见的方

法将随机采

样训练数据

集的子集，然

后应用视频

生成模型

来

增加这些数

据，以增强下

游任务的性

能。我们假设

这个随机样

本不考虑

K

我

我

我 我

Attnscene(Qn，Kn 1，Vn

1)=

soft max

VN

1， (2)

我 我

我

我

Attnins(Qn，Kn 1，Vn 1)=

soft max

(Vn 1

Mn 1)， (3)

增量训

练

6

表 1:我们在

nuScenes 验证集上将

Delphi

与最先进的

方法进行了

比较。结果衡

量不同方法

的时空一致

性和可控性

。↓/↑表示指标值

越小/越大，性

能越好。

方法

空间一致性

时间一致性

模拟真实差

距

FID↓ 剪辑↑

FVD↓ NDS↑ 平均

值。Col.

Rate↓

4 帧 8

帧 40 帧

贝

夫根[33]

25.54 71.23 不适用

的 不适用的

失败

不适用

的 不适用的

饮料控制[42] 24.85 82.70

不

适用的 不适

用的 失败 不

适用的

不适

用的

司机梦

想家[38] 26.8 不适用

的

不适用的

353.2 失败 不适用

的 不适用的

魔法驱动[5]

16.20 不

适用的不适

用的 不适用

的 失败

不适

用的 不适用

的

魔法驱动

* [5]

46.18 82.47 617.2 不适用

的

失

败 34.56 3.87

灵丹妙药

[40] 16.96 不适用的 不

适用的

139.0 失败

32.10 不适用的

灵

丹妙药*

[40] 55.32 84.23 –

446.9 失败

29.41 1.35

Drive-WM

[39] 15.8 不适用的 不

适用的

122.7 失败

不适用的 不

适用的

德尔

福(我们的)

15.08 86.73 – 113.5

275.6 36.58 0.29

*结

果是使用官

方

github 发布代码

或验证集上

的渲染视频

计算的。

N/A 表示

模型或预训

练权重不是

开源的，因此

我们无法如

实再现。

现有

分布的长尾

情况下，是实

质性的进一

步优化。因此

，我们提出了

一个简单而

有效的失败

案例驱动框

架，利用四个

步骤来减少

计算成本。如

图 3 所示，我们

首先评估现

有的失败案

例

作为起点

，然后实施一

种基于可视

化语言的方

法来分析这

些数据的模

式，并检索相

似的场景

以

更深入地理

解上下文，然

后我们将场

景和实例编

辑的标题多

样化，以生成

具有不同外

观的

新数据

。最后，我们用

这样的附加

数据训练下

游任务几个

时期，以增加

泛化能力。

请

注意，所有这

些操作都是

在训练集上

进行的，以避

免验证信息

的任何潜在

泄漏。每个组

件的详细实

现请参见补

充材料。此外

，我们注意到

一个并行的

工作[20]利用了

类似的想

法

。然而，他们的

方法仅适用

于

2D 检测任务

，而我们的方

法能够提高

端到端的规

划能力。

4 实验

我们遵循流

行的方法[40，42]，使

用

nuScenes [2]并使用 FID [7]，FVD

[35]和

下游模型对

新

生成的数

据的性能来

评估图像，视

频和模拟到

真实的差距

。更多详情见

附录。

数据集

。我们在流行

的 nuScenes

[2]验证数据

集上进行了

大量实验，该

数据集包括

150 个

以密集交

通和复杂街

道驾驶场景

为标志的驾

驶场景。每个

场景包含大

约 40

帧。我们利

用十

个前景

类别(即，公共

汽车、汽车、自

行车、卡车、拖

车、摩托车)来

创建详细的

街道前景

对

象布局。从地

图扩展包获

得的四个背

景类用于生

成背景布局

。

超参数。我们

在

8 个 A800 80GB

GPUs 上训练

我们的模型

。使用 AdamW [17]优化器

以

5e-5

的学习速

率优化扩散

U-Net。我们将原始

图像的大小

从 1600 ×

900 调整到 512 ×

512。

在

训练过程中

，视频长度设

置为 10，我们以

流式方式顺

序生成视频

帧。为了进行

推断，我

们使

用配置有

50 个

采样步骤的

PLMS [21]采样器。视频

样本的空间

分辨率设置

为 512

×

512，帧长为 40。推

断长度不受

限制，可以是

40 或更长。我们

的模型在

nuScenes 数

据集上

进行

训练，交叉视

图模型有 50，000

步

，时间模型有

20，000 步。

4.1 比较

Delphi 和最

先进的视频

生成方法

我

们通过包括

定量和定性

两个方面的

综合评估来

评估视频生

成的质量，将

我们的方法

与以

前的方

法进行比较

。在表

1 中，我们

报告了 nuScenes 验证

集、空间和时

间一致性以

及

sim￾real gap 三个方面

的指标。简而

言之，我们的

方法在短视

频生成任务

上明显优于

最先进的

方

法，可以生成

高达

40 帧的视

频。相比之下

，其他方法崩

溃，这证明了

我们的方法

在长期

视频

生成的有效

性。我们展示

定性的

图

4:不

同生成模型

生成的局部

区域的可视

化比较。

图 Delphi 生

成的时空一

致性多视角

长视频。

结果

如图 4 所示，并

将视频质量

与相同剪辑

上的先前方

法进行比较

。我们的方法

在以前的

方

法失败的地

方保持一致

的空间和时

间外观。

Delphi 生成

的多视角长

视频的可视

化。我们在图

5 中演示了生

成的多视角

长视频。可见

，

我们的方法

具有生成时

空一致性长

视频的强大

能力。

不同模

型生成的多

视点视频的

可视化比较

。我们在图 6 中

展示了由不

同模型生成

的多视图

视

频的可视化

比较。可见，我

们的方法具

有生成时空

一致性长视

频的强大能

力。

4.2 我们的失

败案例驱动

框架提升了

端到端规划

模型

为了证

明我们的框

架的有效性

，我们比较了

表 2

中的三个

因素，即生成

案例的数量

、数据

引擎(视

频生成方法

)和数据源的

选择。总之，我

们发现，通过

仅生成 4%的训

练集大小数

据，我们的方

法可以将冲

突率从 0.33

降低

到 0.27，降低幅度

为 25%。然而，在相

同的设置

下

，如果我们使

用其他数据

引擎(如

Panasonic)来微

调 UniAD，冲突率会

增加。尽管如

此，

我们也为

两个数据引

擎利用随机

抽样，我们的

方法始终优

于基线。我们

还展示了我

们的框

架如

何修复图

7 中

的失败案例

。

如果我们从

验证集中抽

取布局样本

呢？由于我们

的 Delphi

只看到 nuScenes 的

训练集，一

个

自然的问题

是，我们能包

括验证集来

看看我们是

否能进一步

提高下游任

务的性能吗

？这

里，我们从

训练集和验

证集中收集

失败案例。请

注意，因为在

我们的框架

中只使用了

布局

和标题

，所以验证视

频剪辑不会

在任何训练

过程中暴露

。我们注意到

，冲突率从 0.33 降

低

到 0.26，仅生成

429 个案例，这只

是训练集大

小的 1.5%。这

7

抽样

策略 案例数

量 数据引擎

数据源

Col比率

(%)↓

1s 2s 3s平均值。

8

图 6:不

同模型生成

的多视角视

频的可视化

对比。

图

7:前后

四个例子的

可视化。(a)此处

，我们展示了

来自验证集

的四个硬示

例，“前方大型

物

体”和“交叉

口无保护左

转”。(b)我们的框

架能够在训

练期间不使

用这些数据

的情况下修

复这四个

示

例。

表

2:通过在

故障案例驱

动框架中应

用不同的数

据采样策略

、数据案例数

量、数据引擎

和数据源，从

UniAD 开源模型微

调而来的端

到端模型的

性能比较。基

线性能显示

在表格的第

一行。

基线(UniAD) –

– – 0.10 0.18

0.7

1

0.3

3

14065 (50%)

灵

丹妙

药

0.03 0.23 0.7

9

0.3

5

随机

抽样 德尔斐

训练集

0.08 0.20 0.5

8

0.29

0.43

28130 (100%)

灵丹

妙

药

0.08 0.22

0.9

8

德尔斐

0.07 0.29

0.6

5

0.3

3

故障案例驱

动采

样

972 (4%)

灵丹

妙

药

德尔斐

训练集 0.05

0.08

0.18

0.18

0.81

0.56

0.35

0.27

0.26

429

(1.5%)

德尔

斐 验证集

0.07 0.10 0.6

1

行

业从业者可

能会对结果

感兴趣，即仅

查看训练数

据集视频的

基于扩散的

方法可以有

效地

提高带

有布局和标

题的验证集

的性能。

9

10

事件

实例编辑编

辑Col比率 (%)↓

1s 2s

3s平均

值。

方法 FID FVD剪辑

图

8:实例和场

景编辑的可

视化。(a)显示实

例级控制结

果，如所有车

辆的外观属

性。(b)显示场景

级控制

结果

，包括天气和

时间。

4.3 消融研

究

我们进行

消融研究来

展示我们方

法的有效性

。

消融虚拟现

实的鸿沟。为

了进一步评

估模拟与真

实之间的差

距，我们用不

同部分的合

成数据来训

练 UniAD。在表 3

的顶

部，我们用纯

生成的视频

片

段训练 UniAD，碰

撞率从 0.34

增加

到 0.50。这表

明合

成数据还不

能完全取代

真实数据。相

比之

下，如果

我们考虑增

量学习设置

，当我们用额

外

的数据训

练 UniAD 时，使用合

成数据会产

生更好

的性

能，而使用额

外的真实数

据会使性能

从

0.34

恶化到 0.38。

表

3:使用真实数

据和生成数

据的端到端

模

型的性能

。我们以官方

发布的第一

阶段的

权重

作为起点来

训练 UniAD [13]的第二

阶

段。

方法

1s

颜

色变化率

(%)↓ 2s

3s

平

均

值 。

真实的

0.07 0.24

0.70 0.34

生成的 0.17

0.37 0.97 0.50

真实

+真实

0.03 0.33 0.7

7

0.38

真实+生

成 0.08 0.18

0.5

6

0.27

消融场景

和实例编辑

。表

4 展示了端

到端模型的

数据多

样性

的有效性。具

体来说，我们

用两种方法

编辑现有场

景:场景级编

辑和实例级

编辑。这个奇

特的功能让

我们

可以从

有限的现有

数据中生成

大量的新数

据。如表

4 所

示

，同时编辑场

景和实例可

以获得最佳

性能。利用强

大

的精确控

制能力，Delphi

通过

生成更丰富

、更多样的

数

据，最大限度

地提高了端

到端模型的

性能。

表 4:德尔

福精确可控

性在端到端

模型上的

有

效性。

0.10 0.20 0.7

1

0.3

4

✓

0.11 0.18 0.6

4

0.3

1

✓

✓

✓

0.05

0.08

0.20

0.18

0.62

0.56

0.29

0.27

消融 NRM 和

FTCM。在表

5 中，我们

验证了两个

模块，噪声

重

新初始化模

块 (NRM)和特征对

齐的时间一

致性模块

(FTCM)。我

们看到所有

指标都有明

显的提高，证

明了我们

提

出的方法的

有效性。特别

是，FTCM 结构将 FID

从

22.85

提高到 19.81，而 NRM

进

一步提高了

FID。

5 结论

表

5:我们

提议的 NRM 和 FTCM

的

消

融研究结

果。

德尔斐 15.08275.686.73

11

不

含 NRM 19.81

291.5 85.22

不含 NRM

和 FTCM 22.85 346.9

6

82.91

总

之，我们提出

了一种新的

自动驾驶场

景视频生成

方法，可以在

nuScenes 数据集上合

成多

达 40 帧的

视频。令人惊

讶的是，我们

表明，利用仅

用训练分割

训练的扩散

模型，我们能

够

通过样本

有效的故障

案例驱动框

架来提高端

到端规划模

型的性能。我

们希望为该

领域的研

究

人员和从业

人员解决数

据稀缺问题

提供帮助，并

朝着自动驾

驶汽车安全

上路迈出坚

实的

一步。

局

限性和社会

影响。我们的

Delphi 将

BEV 布局作为

输入，以确保

控制能力，即

我们只能丰

富外观，不能

在合成过程

中改变布局

。这导致了一

个限制，即我

们的框架只

能用于开环

设

置[2]，而不能

用于闭环设

置。然而，另一

个限制是，当

端到端模型

在训练数据

集中表现完

美时，我们的

失败案例驱

动的采样不

起作用。就社

会影响而言

，我们相信我

们的方法可

以

用来提高

端到端模型

的性能，并可

能有助于未

来大规模自

动驾驶汽车

的部署。

参考

[1] 乔希·阿齐姆

、史蒂文·阿德

勒、桑迪尼·阿

加瓦尔、拉马

·艾哈迈德、伊

尔格·阿

克卡

亚、弗洛伦西

亚·莱昂尼·埃

勒曼、迪奥戈

·阿尔梅达、扬

科·阿尔滕施

密特、

萨姆·奥

特曼、希亚马

尔·阿纳德卡

特等。arXiv

预印本

arXiv:2303.08774，2023。

[2] 霍尔格·凯撒

、瓦伦·班基蒂

、亚历克斯·H·朗

、苏拉布·沃拉

、威尼斯·艾琳

·莱

昂

、 徐 强 、

阿

努 什 · 克

里 希

南 、 潘

宇 、 贾 恩

卡

洛 · 巴 尔

丹

和 奥 斯 卡

· 贝

伊

邦。nuscenes:用于自

动驾驶的多

模态数据集

。IEEE/CVF 计算机视觉

和模式识别

会议

论文集

，第 11621-11631 页，2020 年。

[3] 、姜波

、、廖本成、、、黄昌

、、王兴刚。Vadv2:通过

概率规划的

端到端矢量

化自动驾

驶

。arXiv 预印本

arXiv:2402.13243，2024。

[4] 艾米

莉·丹顿和罗

布·费格斯。具

有学习先验

的随机视频

生成。在机器

学习国际会

议上，

第

1174-1183 页。PMLR，2018。

[5] 高

瑞元、、谢恩泽

、洪蓝青、、杨迪

燕和。Magicdrive:具有不

同

3d 几何控制

的街景

生成

。arXiv 预印本

arXiv:2310.02601，2023。

[6] 威廉

姆·哈维、赛义

德·纳德里帕

里兹、瓦登·马

斯拉尼、克里

斯蒂安·韦尔

巴赫和

弗兰

克·伍德。长视

频的柔性扩

散建模。神经

信息处理系

统进展，35:27953–

27965，2022。

[7] 马丁

·霍塞尔，休伯

特·拉姆绍尔

，托马斯·安特

辛纳，伯恩哈

德·奈斯勒和

赛普·

霍克雷

特。由双时标

更新规则训

练的

gan 收敛到

局部纳什均

衡。神经信息

处理系统进

展，30，2017。

[8] 乔纳森·何

，阿贾伊·贾恩

和彼得·阿比

勒。去噪扩散

概率模型。神

经信息处理

系统

进展，33:6840–6851，2020。

[9] 乔

纳森·何、、奇特

万·萨哈利亚

、黄杰、高瑞琪

、阿列克谢·格

里岑科、迪德

里克

·P·金马、本

·普尔、穆罕默

德·诺鲁齐、戴

维·J·弗利特等

。arXiv

预印本

arXiv:2210.02303，2022。

[10] 托比

亚斯·霍普。视

频预测和填

充的扩散模

型:为任意视

频完成任务

训练一个条

件视频

扩散

模型，2022。

[11] 安东尼

·胡，劳埃德·拉

塞尔，哈德森

·杨，扎克·穆雷

兹，乔治·费多

塞夫，亚历

克

斯·肯德尔，杰

米·肖顿和吉

安卢卡·科拉

多。Gaia-1:自动驾驶

的生成世界

模

型。arXiv 预印本

arXiv:2309.17080，2023。

[12] 胡、、、吴、李红阳

、严俊池、陶大

成。St-p3:通过时空

特征学习的

端到端基于

视觉

的自动

驾驶。欧洲计

算机视觉会

议(ECCV)，2022 年。

[13] 胡，，杨家

志，，，司马崇浩

，朱喜洲，柴思

齐，杜森耀，林

天伟，，等。面向

自动

驾 驶 的

规 划

。 IEEE/CVF 计 算

机

视 觉 和 模

式

识 别 会 议

论

文 集 ， 第

17853-17862

页，2023。

[14] Aapo

Hyvä rinen 和

Peter Dayan。通过分数匹

配估计非归

一化统计模

型。机器学习

研

究杂志，6(4)，2005。

12

[15] 、毛

伟新、、赵玉成

、温玉清、、和。自

动驾驶的通

用世界模型

。arXiv

预印本

arXiv:2311.13549，2023。

[16] 姜波

，，，廖本成，，周和

龙，，，黄昌，王兴

刚。Vad:有效自动

驾驶的矢量

化场景

表示

。2023 年 IEEE/CVF 计算机视

觉国际会议

(ICCV)，第

8306–8316 页，2023 年。

[17]

迪 德

里 克 ·P·

金 马 和

吉 米

· 巴 。 亚

当

: 一 种 随

机 优

化 方 法

。 arXiv 预 印

本

arXiv:1412.6980，2014。

[18] Manoj Kumar

、 Mohammad Babaeizadeh 、

Dumitru Erhan 、 Chelsea

Finn 、 Sergey

Levine、Laurent

Dinh 和 Durk Kingma。Videoflow:一个基

于流的视频

生成模型。arXiv

预

印本 arXiv:1903.01434，2(5):3，2019。

[19] 李，，西尔

维奥·萨瓦雷

塞和史蒂文

·霍伊。Blip-2:用冻结

图像编码器

和大型语言

模

型引导语

言图像预训

练。arXiv 预印本 arXiv:2301.12597，2023。

[20]

梁

明福、苏钟琦

、塞缪尔 ·舒尔

特、斯巴斯 ·加

格、赵、和曼莫

汉 ·钱德勒

克

。 Aide: 自 动

驾 驶 中

用 于

对 象 检

测 的

自 动 数

据 引

擎 。 arXiv 预

印

本

arXiv:2403.17373，2024。

[21] 刘、、、和。流形

上扩散模型

的伪数值方

法。arXiv

预印本 arXiv:2202.09778，2022。

[22] 、罗

、、陈、张颖雅、、、沈

、、、周和谭铁牛

。视频融合:高

质量视频生

成的分解扩

散模

型。IEEE/CVF 计算

机视觉和模

式识别会议

论文集，第 10209–10218 页

，2023。

[23] 迈克尔·马修

，卡米尔·库普

里和扬·勒昆

。超越均方误

差的深度多

尺度视频预

测。arXiv 预印本 arXiv:1511.05440，2015。

[24] Alex Nichol ，

Prafulla Dhariwal ， Aditya

Ramesh ， Pranav Shyam

， Pamela

Mishkin，Bob McGrew，Ilya

Sutskever 和

陈唐山。Glide:使用

文本引导扩

散模型实现

照

片级真实

感图像生成

和编辑。arXiv 预印

本

arXiv:2112.10741，2021。

[25] Aditya Prakash，Kashyap

Chitta 和 Andreas Geiger。用于端

到端自动驾

驶的多模式

融合变压器

。IEEE/CVF

计算机视觉

和模式识别

会议论文集

(CVPR)，2021 年。

[26] 邱浩南，，夏

，，何颖清，，，，和。Freenoise:通

过噪声重新

调度的免调

谐较长视

频

扩散。arXiv 预印本

arXiv:2310.15169，2023。

[27] 任伟明、杨海

瑞、、、杜新润、黄

建华和陈。Consisti2v:增

强图像到视

频生成的视

觉一致性。arXiv

预

印本 arXiv:2402.04324，2024。

[28] 罗宾·龙

巴赫、安德里

亚斯·布拉特

曼、张秀坤·洛

伦茨、帕特里

克·埃塞尔和

比约

恩·奥默

。用潜在扩散

模型合成高

分辨率图像

。IEEE/CVF 计算机视觉

和模式识别

会议

论文集

，第 10684-10695

页，2022。

[29] 纳坦尼

尔·鲁伊斯、李

元镇、瓦伦·詹

帕尼、雅艾尔

·普里奇、迈克

尔·温斯顿和

克菲尔

·阿伯

曼。Dreambooth:微调主题

驱动生成的

文本到图像

扩散模型。IEEE/CVF

计

算机视觉

和

模式识别会

议论文集，第

22500-22510 页，2023。

[30]

斋藤正树

，松本荣一和

斋藤顺太。具

有奇异值裁

剪的时态生

成对抗网。IEEE 计

算机视

觉国

际会议论文

集，第 2830-2839

页，2017 年。

11

[31]

辛

格，亚当·波亚

克，托马斯·海

斯，席音，解安

，，胡启元，杨海

瑞，奥龙·阿舒

尔，奥兰·加夫

尼，等。制作视

频:不需要文

本-视频数据

的文本-视频

生成。arXiv 预

印本

arXiv:2209.14792，2022。

[32]

宋 家 明 ，

孟 和

斯 特 凡

诺 埃

尔 蒙 。

去 噪 扩

散 隐

式 模 型

。 arXiv

预 印 本

arXiv:2010.02502，2020。

[33] Alexander Swerdlow、徐润

生和周。从鸟

瞰图布局生

成街景图像

。arXiv 预印本

arXiv:2301.04634，2023。

[34] 谢尔

盖·图利亚科

夫、刘明宇、杨

晓东和扬·考

茨。Mocogan:分解视频

生成的运动

和

内容。IEEE

计算

机视觉和模

式识别会议

论文集，第 1526-1535 页

，2018 年。

[35] 托马斯·安

特辛纳、斯约

尔德·范·斯廷

基斯特、凯罗

尔·库拉奇、拉

斐尔·马里尼

尔、

马钦·米哈

尔斯基和西

尔万·盖利。走

向精确的视

频生成模型

:新的度量和

挑战。arXiv 预

印本

arXiv:1812.01717，2018。

[36] 卡尔·冯德里

克、哈米德·皮

尔西亚瓦什

和安东尼奥

·托雷巴。用场

景动态生成

视频。

神经信

息处理系统

进展，29，2016。

[37] 王世豪

、刘英飞、王沺

裁、李颖和张

翔宇。探索以

对象为中心

的时间建模

，用于有效的

多

视图 3d

对象

检测。arXiv 预印本

arXiv:2303.11926，2023。

[38] 、郑竹、、陈新泽

和陆继文。Drivedreamer:走

向自动驾驶

的真实世界

驱动世界模

型。arXiv

预印本 arXiv:2309.09777，2023。

[39] 、王

、、吕凡、、陈云涛

、。驾驶未来:自

动驾驶世界

模型的多视

角视觉预测

和规

划。arXiv 预印

本 arXiv:2311.17918，2023。

[40]

余庆文，赵

玉成，，，王艳辉

，，，，，，和张翔宇。灵

丹妙药:自动

驾驶的全景

可控

视频生

成。arXiv 预印本 arXiv:2311.16813，2023。

[41] 周

杰伦吴，葛，王

，雷伟贤，顾，许

永利，，肖虎和

郑寿。视频调

谐:文本到视

频生成

的图

像扩散模型

的一次性调

谐。arXiv 预印本

arXiv:2212.11565，2022。

[42] 杨

，马恩慧，，彭，，郭

庆，，俞开成。Bevcontrol:通

过 bev

草图布局

，精确控制多

视角

一致性

的街景元素

。arXiv 预印本 arXiv:2308.01661，2023。

[43] 杨泽

通，，，李红阳。视

觉点云预测

实现可扩展

的自动驾驶

。 arXiv 预印本

arXiv:2312.17655，2023。

[44] 翟江

田、泽峰、杜金

浩、毛永强、江

、谭子昌、张毅

夫、、王京东。重

新思考

nuscenes

中端

到端自动驾

驶的开环评

测。arXiv 预印本 arXiv:2305.10430，2023。

[45]

北

辰张，潘章，董

晓义，臧余杭

，。长剪辑:释放

剪辑的长文

本功能。arXiv 预印

本

arXiv:2403.15378，2024。

[46]

张和马涅

什·阿格拉瓦

拉。向文本到

图像扩散模

型添加条件

控制。arXiv 预印本

arXiv:2302.05543，2023。

[47] 赵，，郑竹，陈新

泽，，鲍晓义，。drive

dreamer-2:Llm-增

强的世界模

型，用于多

样

化的驾驶视

频生成。arXiv 预印

本 arXiv:2403.06845，2024。

[48] 周大全，，颜

汉书，，，朱，冯家

实。Magicvideo:利用潜在

扩散模型的

高效视频生

成。arXiv 预印本 arXiv:2211.11018，2022。

12

13

A 方

法

A.1 故障案例

驱动框架的

详细实现

收

集失败案例

。最初，我们在

验证集上评

估基本端到

端模型的性

能。在本次评

估中，我们

利

用

UniAD [13]基础模型

作为起点。我

们采用一种

度量标准，其

中在算法规

划的路径上

3

秒钟内发生

的碰撞将一

个场景视为

失败案例。此

外，为了获得

进一步的见

解，我们将 3D

盒

子的感知结

果和从端到

端模型中得

出的规划结

果可视化。通

过这个过程

，我们识别并

选择

失败案

例进行进一

步分析。

分析

数据模式。我

们最初预计

大型视觉语

言模型将能

够自动查明

算法失败背

后的原因。然

而，我们的调

查显示，一个

简单的调查

是不够的。因

此，我们设计

了一种利用

VLM 的多轮

调查

方法[1]。这种方

法能够更精

确地分析导

致算法失败

的因素，并详

细描述导致

这种失败

的

关键因素。

具

体来说，我们

将前一步的

可视化结果

输入 VLM，并提示

它辨别失败

的主要原因

是源于感

知

还是计划问

题。在感知是

罪魁祸首的

情况下，原因

可以进一步

分为各种因

素，如夜间黑

暗、识别附近

大型物体的

挑战或无法

识别罕见物

体类别。另一

方面，如果规

划被确定为

失

败的根源

，VLM 可以区分堵

塞或不频繁

互动等场景

，包括闯红灯

或过马路。最

终，根据先

前

确定的失败

原因，我们要

求

VLM 提供导致

失败的具体

因素的准确

说明。

检索相

似场景。使用

详细的图像

描述，我们使

用 BLIP-2

[19]从列车组

中识别和检

索与故

障背

后的原因紧

密对应的场

景。这个过程

包括使用 BLIP-2 量

化从图像和

指定文本输

入中提

取的

嵌入之间的

余弦相似性

。基于这种相

似性度量，我

们然后只选

择和检索前

k 个最相关

的

图像。

更新模

型。基于确定

的潜在故障

场景，我们利

用

Delphi 创建了一

个广泛多样

的图像数据

集。我们使用

相应的样本

令牌从 VLM 获得

场景字幕，并

使用这些字

幕作为输入

，用

Delphi

生成类似

的图像。

为了

增加数据的

多样性，我们

使用了一个

LLM 来调整输入

到

Delphi 中的标题

。这种方法有

助于场景描

述的改变，以

包含各种场

景条件和实

例条件，例如

晴天、雨天、阴

天、夜晚、

郊区

、改变汽车的

颜色。因此，将

这些修改过

的标题输入

到 Delphi

中导致了

更广泛的图

像

的产生。

然

而，我们发现

直接利用生

成的失败场

景进行训练

可能会导致

过度拟合。虽

然经过训练

的模

型在选

定的失败案

例中表现出

色，但其性能

在以前的成

功案例中受

到影响。因此

，为了缓解

这

个问题，我们

将生成的数

据与每个微

调会话的完

整训练集相

集成。这种策

略在优化模

型的

整体性

能方面被证

明是有效的

。

最终，我们使

用这个组合

的数据集训

练端到端模

型，产生一个

改进的模型

，它标志着改

进

周期的后

续迭代的开

始。

B 实验

B.1 韵律

学

关于生成

视频的质量

和可控性的

指标。我们从

两个方面评

估生成视频

的质量 :质量

和可控

性。具

体来说，对于

质量，我们使

用弗雷歇初

始距离(FID) [7]来评

估生成的视

频中单帧单

视图图像的

真实性，弗雷

歇视频距离

(FVD)

[35]来评估

14

单视

图视频和剪

辑分数(CLIP) [42]来评

估单帧多视

图图像的空

间一致性。对

于可控性，我

们利用流行

的

BEV 检测模型

StreamPETR [37]和端到端模

型[13]来评估生

成的数据，并

报告

NDS

分数和

平均碰撞率

(Avg。Col. Rate ),其全面反映

了所生成的

图像和 BEV

布局

注释之间

的

几何对准。通

过使用这些

评估指标，我

们可以确保

生成的结果

在质量和可

控性方面保

持

高标准。

端

到端模型生

成的视频有

效性的度量

。为了评估我

们提出的基

于德尔菲法

的端到端模

型的

故障案

例驱动框架

的有效性，我

们利用生成

的不同训练

数据来扩充

端到端模型

的原始训练

数据。具体来

说，我们通过

在 nuScenes 验证集上

应用数据扩

充来评估端

到端模型的

性能，

并报告

平均冲突率

。

B.2 更多实验细

节

端到端模

型的实验设

置。在培训阶

段，我们利用

UniAD 官方知识库

上的模型作

为微调的基

础。为了增强

训练过程，我

们将学习率

降低了

10 倍，设

置为 2e-5。此外，我

们与 UniAD

存

储库

上推荐的超

参数保持一

致，包括优化

器设置。

计算

效率和硬件

要求我们在

表 6

中报告了

两个模型变

体的模型复

杂性。我们将

进一步在

nuScenes 训

练集上提供

生成的数据

，以便于数据

扩充。

表

6:模型

效率和硬件

要求。

模型 参

数 推理存储

器和

GPU 推理时

间 列车配置

多视图单帧

0.5

亿

22GB(RTX3090) 4s /示例 8×A100，24

小时

多视图多帧

1.1B 39GB(A100 40G) 4s

/示例 8×A800，72 小时

B.3

验

证我们的失

败案例驱动

框架的每个

组件

如表 2 所

示，我们从三

个方面进行

比较，数据采

样策略、生成

案例的数量

和数据引擎

验

证。

数据采

样策略。我们

评估了不同

的数据抽样

策略，如随机

抽样和失败

案例目标抽

样。在表 2

的上

半部分，我们

从训练数据

集中随机选

择各种比例

的数据样本

，并使用相应

的

BEV 布局和

原

始场景字幕

来生成新数

据。在表 2

的下

半部分，我们

从验证集中

检索了与故

障案例模式

相

似的训练

数据，并使用

生成模型的

强大控制功

能生成了不

同的天气数

据。新生成的

数据与原

始

数据混合，以

训练端到端

模型。据观察

，通过故障案

例引导的数

据扩充增强

的端到端模

型

实现了最

佳性能。这表

明，端到端模

型在这些故

障情况下训

练不足，向其

提供更多故

障情况

相关

的训练数据

可以用更少

的计算资源

实现最佳的

泛化性能。

案

件数量。我们

调查了数据

样本的数量

。我们从训练

集中随机抽

取了 14，065 和

28，130

个训

练样本(大约

是整个训练

集的 50%和 100%)。由生

成模型在这

些样本上的

配置生成的

结

果被用于

数据扩充。如

表 2 的上半部

分所示，端到

端模型的性

能随着样本

数量的增加

而恶

化。这表

明使用与原

始训练集风

格相似的训

练数据只能

在有限的程

度上帮助模

型。因此，

这促

使我们考虑

增加训练数

据的多样性

。

数据引擎。我

们测试了各

种数据生成

引擎，包括 Delphi 和

其他最先进

的生成模型

万能药

[40]，以比

较它们在为

模型增强生

成高质量训

练数据方面

的有效性。从

三组对比实

验可以

看出

与其他生成

模型相比，Delphi 生

成的数据有

效地提高了

端到端模型

的性能。这是

由于

Delphi

在场景

生成方面卓

越的精细控

制能力，导致

模型调优的

训练数据更

加多样化。

[15]

15
