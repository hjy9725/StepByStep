



PyTorch在学术界和产业界都

得到了广泛的应用，被用

于完成各种任务，例如图

像分类、自然语言处理、目

标检测等。在2019年，PyTorch被Google和OpenAI等机

构评选为机器学习框架

的首选，这也进一步证明

了

PyTorch在机器学习领域的重

要性。

关于本书

本书是一

本以PyTorch 2.0为框架的深度学习

实战图

书，以通俗易懂的

方式介绍深度学习的基

础内容与理

论，并以项目

实战的形式详细介绍PyTorch框

架的使

用。本书从单个API的

使用，到组合架构完成进

阶的项

目实战，全面介绍

使用PyTorch 2.0进行深度学习项

目

实战的核心技术和涉及

的相关知识，内容丰富而

翔

实。

同时，本书不仅仅是

一本简单的项目实战性

质的图

书，本书在讲解和

演示实例代码的过程中

，对

PyTorch 2.0的核心内容进行深入

分析，重要内容均

结合代

码进行实战讲解，围绕深

度学习的基本原理介

绍

大量案例，读者通过这些

案例可以深入掌握深度

学

习和PyTorch 2.0的相关技术及其

应用，并能提升使

用深度

学习框架进行真实的项

目实战的能力。

本书特点

（1）重实践，讲原理。本书立足

于深度学习，以实战

为目

的，以新版的PyTorch 2.0为基础框架

，详细介

绍深度学习基本

原理以及示例项目的完

整实现过程，

并提供可运

行的全套示例代码，帮助

读者在直接使用

代码的

基础上掌握深度学习的

原理与应用方法。

（2）版本新

，易入门。本书详细讲解PyTorch 2.0的

安装和使用，包括PyTorch 2.0的重大

优化和改进方

案，以及官

方默认使用的API和官方推

荐的编程方法与

技巧。

（3）作

者经验丰富，代码编写优

雅细腻。作者是长期

奋战

在科研和工业界的一线

算法设计和程序编写人

员，实战经验丰富，对代码

中可能会出现的各种问

题

和“坑”有丰富的处理经

验，使得读者能够少走很

多

弯路。

（4）理论扎实，深入浅

出。在代码设计的基础上

，本

书还深入浅出地介绍

深度学习需要掌握的一

些基本理

论知识，作者以

大量的公式与图示相结

合的方式进行

理论讲解

，是一本难得的好书。

（5）对比

多种应用方案，实战案例

丰富。本书采用了

大量的

实例，同时也提供了实现

同类功能的多种解决

方

案，覆盖使用PyTorch 2.0进行深度学

习开发常用

的知识。

配套

示例源代码、PPT课件等资源

下载

本书配套示例源代

码、PPT课件，需要用微信扫描

下面

的二维码获取。如果

阅读中发现问题或疑问

，请联系

booksaga@163.com，邮件主题写“PyTorch 2.0深度

学习从零开始学”。

本书读

者

·深度学习初学者

·PyTorch初学

者

·PyTorch深度学习项目开发人

员

·计算机技术、人工智能

、智能科学与技术、数据科

学与大数据技术等专业

的师生

作

者

2023年5月

第1章

PyTorch 2.0

——一

个新的开始

PyTorch是一个开源

的机器学习框架，提供了

动态计算

图的支持，让用

户能够自定义和训练自

己的神经网

络。它由Facebook的研

究团队开发，并于2017年首次

发布，从那时起，PyTorch迅速成为

机器学习领域最受

欢迎

的框架之一。

PyTorch在学术界和

产业界都得到了广泛的

应用，被用

于实现各种任

务，例如图像分类、自然语

言处理、目

标检测等。2019年，PyTorch被

Google和OpenAI等机构

评选为机器学

习框架的首选，这也进一

步证明了

PyTorch在机器学习领

域的重要性。

1.1 燎原之势的

人工智能

人工智能作为

当今信息科技最炙手可

热的研究领域之

一，近年

来得到了越来越多的关

注。然而，人工智能

并不是

一蹴而就的产物，而是在

不断发展、演变的过

程中

逐渐形成的。人工智能从

无到有是一个漫长而又

不断迭代的过程。

1.1.1 从无到

有的人工智能

人工智能

的发展可以追溯到20世纪

50年代，当时的计

算机技术

还非常落后，但是人们已

经开始思考如何让

计算

机具备人类的智能。于是

，在20世纪50年代末

期，人工智

能这个概念正式被提出

。最早的人工智能

技术主

要是基于规则的，即通过

编写一些规则来让计

算

机进行推理和决策。然而

，这种方法很快就被证明

是不够灵活的，无法应对

各种复杂的情况。

为了解

决这个问题，人们开始研

究机器学习。机器学

习是

一种让计算机从数据中

学习规律的方法。最早的

机器学习算法主要是基

于统计学的方法，如线性

回

归、逻辑回归等。这些算

法主要用于解决一些简

单的

问题，如分类、回归等

。

随着数据量的不断增大

和计算能力的提升，人们

开始

研究更加复杂的机

器学习算法，如神经网络

。神经网

络是一种模仿人

脑神经元的网络结构，可

以用于解决

各种复杂的

问题，如图像识别、语音识

别、自然语言

处理等。然而

，在早期的研究中，神经网

络还存在训

练时间长、容

易陷入局部最优解等问

题。

为了解决这个问题，人

们开始研究深度学习。深

度学

习是一种多层神经

网络的方法，通过层层抽

象可以获

取更加高级、更

加抽象的特征表示。深度

学习在图像

识别、语音识

别、自然语言处理等领域

都取得了非常

显著的成

果。

除了深度学习外，人们

还在不断探索其他的人

工智能

算法，如遗传算法

、强化学习等。这些算法在

不同的

领域都取得了一

定的成果。

随着互联网的

普及和大数据时代的到

来，数据变得越

来越容易

获取。同时，计算能力的提

升也为人工智能

的发展

提供了坚实的基础。GPU的出

现使得人们能够更

加高

效地训练深度学习模型

，云计算的发展使得人们

能够更加轻松地部署和

运行人工智能应用。

如今

，人工智能已经渗透到了

各个领域，如医疗、金

融、交

通、制造等。人工智能已经

成为许多企业的核

心竞

争力，也成为推动社会进

步的重要力量。

然而，人工

智能的发展依然面临着

许多挑战。一方

面，人工智

能的算法和技术还有很

大的提升空间，例

如如何

让人工智能具备更好的

理解能力、创造能力、

推理

能力等。另一方面，人工智

能的应用还需要面对

许

多社会、伦理、法律等方面

的问题。例如如何保障

人

工智能的安全、隐私和公

正性，如何处理人工智能

和人类的关系等。

总之，人

工智能从无到有是一个

漫长而又不断迭代的

过

程。虽然人工智能已经取

得了许多显著的成果，但

是还有很多挑战需要我

们去面对。相信在不久的

将

来，人工智能会继续发

展，成为推动人类社会进

步的

重要力量。

1.1.2 深度学习

与人工智能

深度学习作

为人工智能领域的一种

重要技术，正在引

领人工

智能的发展。它利用多层

神经网络模拟人脑的

处

理方式，可以实现很多人

类难以完成的任务，如图

像识别、语音识别、自然语

言处理等。在过去的几年

中，深度学习在各个领域

取得了巨大的成功，成为

人

工智能领域的一颗璀

璨的明珠。

深度学习的核

心是神经网络，它可以被

看作是由许多

个简单的

神经元组成的网络。这些

神经元可以接收输

入并

产生输出，通过学习不同

的权重来实现不同的任

务。深度学习的“深度”指的

是神经网络的层数，即

多

层神经元的堆叠。在多层

神经网络中，每一层的输

出都是下一层的输入，每

一层都负责提取不同层

次的

特征，从而完成更加

复杂的任务。

深度学习在

人工智能领域的成功得

益于其强大的表征

学习

能力。表征学习是指从输

入数据中学习到抽象的

特征表示的过程。深度学

习模型可以自动学习到

数据

的特征表示，并从中

提取出具有区分性的特

征，从而

实现对数据的分

类、识别等任务。

深度学习

在图像识别、语音识别、自

然语言处理等领

域都取

得了很好的效果。例如，在

图像识别领域，深

度学习

已经取代了传统的机器

学习方法成为主流，可

以

实现对复杂场景中的物

体进行精确识别和定位

。在

自然语言处理领域，深

度学习可以实现文本的

情感分

析、机器翻译等任

务。在语音识别领域，深度

学习可

以将语音转换为

文本，实现语音助手等应

用。

1.1.3 应用深度学习解决实

际问题

深度学习是一种

机器学习技术，它可以通

过大量的数

据来训练模

型，以解决现实生活中的

实际问题。深度

学习技术

在多个领域都有广泛应

用，例如自然语言处

理、计

算机视觉、智能控制等。接

下来将介绍几个使

用深

度学习解决实际问题的

案例，并探讨深度学习技

术的优势。

1. 人脸识别

人脸

识别是一种广泛应用的

技术，它可以用于安全认

证、人脸支付、人脸考勤等

。深度学习技术在人脸识

别中具有优势，因为它可

以从大量的数据中学习

特

征，使得识别准确率更

高。例如，FaceNet是一个基于

深度

学习的人脸识别系统，它

可以在不同光照和角度

下准确地识别同一个人

的脸。

2. 自然语言处理

自然

语言处理是一种重要的

人工智能应用，它可以用

于文本分类、情感分析、机

器翻译等。深度学习技术

在自然语言处理中也有

广泛应用。例如，Google的翻

译系

统就使用了深度学习技

术来提高翻译的准确率

。

此外，深度学习技术还可

以用于语音识别、语音合

成

等方面，以提高人机交

互的体验。

3. 智能控制

智能

控制是一种应用广泛的

技术，它可以用于自动化

控制、机器人控制、智能家

居等。深度学习技术在智

能控制中的应用也越来

越多。例如，深度强化学习

可

以用于机器人控制，通

过从环境中不断学习来

优化机

器人的决策。此外

，深度学习技术还可以用

于自动驾

驶、航空航天等

方面，以提高自主决策的

能力。

1.1.4 深度学习技术的优

势和挑战

深度学习技术

的优势主要体现在以下

几个方面。

1.

准确性更高

深

度学习技术可以通过大

量的数据来训练模型，从

而

学习到更多的特征，以

提高识别准确率。例如，传

统

的人脸识别技术可能

只能识别人脸的一部分

特征，而

深度学习技术可

以学习到更多的细节，从

而提高人脸

识别的准确

率。

2. 可以处理更复杂的问

题

传统的机器学习技术

在处理复杂问题时往往

受限于特

征工程的能力

，而深度学习技术可以通

过学习大量的

数据来自

动提取特征，从而可以处

理更复杂的问题。

3.

可以逐

步优化

深度学习技术可

以通过逐步优化来提高

模型的性能。

例如，可以通

过改变模型的结构、增加

数据量、调整

超参数等方

法来提高模型的性能。

4. 可

以适应不同的场景

深度

学习技术可以根据不同

的场景进行适应性调整

。

例如，对于不同的语音、图

像等数据，可以通过不同

的网络结构和训练方式

进行处理。

虽然深度学习

技术有着诸多优势，但也

存在一些挑

战。例如，深度

学习技术需要大量的数

据来训练模

型，这需要消

耗大量的计算资源和存

储空间。此外，

深度学习模

型的解释性也相对较差

，难以解释模型内

部的决

策过程。

总的来说，深度学

习技术在解决实际问题

方面具有广

泛的应用前

景。随着计算资源的不断

提升和算法的不

断优化

，深度学习技术在未来将

会发挥更加重要的作

用

。

1.2

为什么选择PyTorch 2.0

在PyTorch Conference 2022上，PyTorch官方正

式

发布了PyTorch

2.0，整场活动含“compiler”率

极

高，跟先前的1.x版本相比

，2.0版本有了颠覆式的变

化

。

PyTorch 2.0中发布了大量足以改变

PyTorch使用方式

的新功能，它提

供了相同的Eager模式和用户

体验，同

时通过torch.compile增加了一

个编译模式，在训练和

推

理过程中可以对模型进

行加速，从而提供更佳的

性

能以及对Dynamic Shapes和Distributed的支持。

1.2.1 PyTorch的

前世今生

PyTorch是一个Python开源机

器学习库，它可以提供强

大的GPU加速张量运算和动

态计算图，方便用户进行

快

速实验和开发。PyTorch由Facebook的人

工智能研究小

组于2016年发

布，当时它作为Torch的Python版，目的

是解决Torch在Python中使用的不便

之处。

Torch是另一个开源机器

学习库，它于2002年由

Ronan Collobert创建，主

要基于Lua编程语言。

Torch最初是

为了解决语音识别的问

题而创建的，但随

着时间

的推移，Torch开始被广泛应用

于其他机器学习

领域，包

括计算机视觉、自然语言

处理、强化学习

等。

尽管Torch在

机器学习领域得到了广

泛的应用，但是它

在Python中的

实现相对麻烦，导致它在

Python社区的

使用率不如其他

机器学习库（如TensorFlow）。这也

就迫

使了Facebook的人工智能研究小

组开始着手开发

PyTorch。

在2016年，PyTorch首

次发布了其Alpha版本，但是该

版本的使用范围比较有

限。直到2017年，PyTorch正式

发布了其

Beta版本，这使得更多的用户

可以使用

PyTorch进行机器学习

实验和开发。在2018年，

PyTorch 1.0版本正

式发布，这也标志着PyTorch开始

成为机器学习领域最受

欢迎的开源机器学习库

之一。

PyTorch在国际学术界和工

业界都得到了广泛的认

可，

并在实践得到广泛的

应用。同时，PyTorch持续更新和

优

化，使得用户可以在不断

的技术发展中获得更好

的

使用体验。

1.2.2 更快、更优、更

具编译支持——

PyTorch 2.0更好的未来

PyTorch 2.0的诞生使得PyTorch的性能进一

步提升，

并开始将PyTorch的部分

内容从C++阵营拉到Python阵

营中

。而其中最为人津津乐道

的新技术包括

TorchDynamo、AOTAutograd、PrimTorch以及

TorchInductor。

1. TorchDynamo

TorchDynamo可以

借助

Python Frame Evaluation Hooks（Python框架评

估钩子），安全

地获取PyTorch程序，这项重大创

新是

PyTorch过去5年来在安全图

结构捕获

（Safe Graph Capture）方面研发成果

的汇总。

2. AOTAutograd

AOTAutograd重载PyTorch

Autograd Engine

（PyTorch自动微分引

擎），作为一个

Tracing Autodiff，用于生成超

前的

Backward

Trace（后向追溯）。

3. PrimTorch

PrimTorch将2000多个PyTorch算

子归纳为约250个

Primitive Operator闭集（Closed

Set），开发

者

可以针对这些算子构

建一个完整的PyTorch后端。

PrimTorch大大

简化了编写PyTorch功能或后端

的流

程。

4. TorchInductor

TorchInductor是一个深度学习

编译器，可以为多个加

速

器和后端生成快速代码

。对于NVIDIA GPU，它使用

OpenAI Triton作为关键构

建模块。

TorchDynamo、AOTAutograd、PrimTorch和

TorchInductor是用Python编写的，并

且支持

Dynamic Shape（无须重新编译就

能发送不同大小的

向量

），这使得它们灵活且易学

，降低了开发者和供

应商

的准入门槛。

除此之外，PyTorch

2.0官

宣了一个重要特性——

torch.compile，这一

特性将PyTorch的性能推向了新

的高度，并将PyTorch的部分内容

从C++移回Python。

torch.compile是一个完全附加

的（可选的）特性，因

此PyTorch 2.0是100%向

后兼容的。

当然，PyTorch

2.0目前只推

出了改革的第一个版

本

，随着后续PyTorch社区以及维护

团队的修正和更

新，PyTorch一定

会迎来更好的未来。

1.2.3 PyTorch 2.0学习

路径——从零基础到项目

实

战

学习PyTorch的步骤可能因个

人情况而有所不同，以下

是一般的学习步骤：

（1）安装

PyTorch和Python：用户可以在PyTorch官方

网站

上找到安装说明，并选择

合适的版本和平台。

（2）学习

Python基础知识：如果用户还不

熟悉

Python，那么需要先学习Python基

础知识，包括变

量、数据类

型、控制流语句、函数和模

块等。

（3）了解深度学习基础

知识：学习深度学习前，需

要

先了解神经网络、损失

函数、优化算法和梯度下

降等

基础概念。

（4）学习PyTorch基础

知识：学习PyTorch张量

（Tensor）、自动微分

（Autograd）、模块

（Module）和优化器（Optimizer）等PyTorch基础知

识。

（5）实践PyTorch：尝试使用PyTorch进行一

些简单的

任务，比如创建

张量、定义神经网络模型

、计算损失

函数和梯度、优

化模型参数等。

（6）学习PyTorch高级

知识：深入学习PyTorch的高级

知

识，包括PyTorch的GPU加速、数据处理

、模型保存

和加载、模型微

调和迁移学习等。

（7）完成深

度学习实战项目：使用PyTorch完

成多个

深度学习项目，例

如图像分类、物体检测、文

本生成

等。这些项目可以

来自实际问题或公开的

数据集，目

的是将前面所

学的知识应用于实际情

况中。

最终总结一句话，前

途是光明的，道路是曲折

的。实

践出真知，学习PyTorch 2.0是一

个需要仔细钻研的

过程

，不仅要学习理论，还需要

自己动手实践才能了

解

其中的奥义。感谢读者选

择本书来进行

PyTorch 2.0的学习之

旅，希望沉下心来，认认真

真

地掌握这个领域的知

识。

1.3 本章小结

本章介绍了

人工智能技术发展简史

、深度学习与人工

智能的

关系、深度学习能解决的

实际问题，以及

PyTorch的发展历

程和最新技术，并给出了

PyTorch

2.0的学习路径——从零基础到

项目实战。

第2章

Hello PyTorch 2.0

——深度学习

环境搭建

工欲善其事，必

先利其器。第1章介绍了PyTorch与

深

度学习神经网络之间

的关系，本章将正式进入

PyTorch 2.0的学习过程。

首先读者需

要知道，无论是构建深度

学习应用程序，

还是应用

已完成训练的项目到某

项具体项目中，都需

要使

用编程语言完成设计者

的目的，本书使用Python

语言作

为开发的基本语言。

Python是深

度学习的首选开发语言

，很多第三方提供

了集成

大量科学计算库的Python标准

安装包，常用的

是Miniconda和Anaconda。Python是一

个脚本语言，如

果不使用

Miniconda或者Anaconda，那么第三方库的安

装会较为困难，各个库之

间的依赖性就很难连接

得很

好。因此，这里推荐使

用Miniconda，当然对Python语

言非常熟悉

的读者也可以直接使用

原生Python。

本章首先介绍Miniconda的完

整安装，然后完成一个练

习项目——生成可控的手写

体数字，这是一个入门程

序，可以帮助读者了解完

整的PyTorch项目的工作过

程。

2.1 安

装Python

2.1.1

Miniconda的下载与安装

1. 下载和

安装

进入Miniconda官方网站，打开

下载页面，如图2-1所

示。

读者

可以根据自己使用的操

作系统选择不同平台的

Miniconda下载，目前官网提供的是

最新集成了

Python 3.10版本的Miniconda。如果

读者目前使用的

是以前

的Python版本，例如Python 3.7，也是完全可

以的。从图中可以看到，使

用3.7～3.10版本的Python

都能支持PyTorch的使

用。读者可以根据自己的

操作系

统选择下载相应

的文件。

图2-1 下载页面

这里

推荐使用Windows Python 3.9版本，相对于

3.10版

本，3.9版本经过一段时间的

训练，具有一定的

稳定性

。当然，读者可根据自己的

喜好选择集成

Python 3.10版本的Miniconda。下

载页面如图2-2所

示。

图2-2 集成

Python

3.10版本的官方网站Miniconda3

下载页

面

注意：如果读者使用的

是64位操作系统，那么可以

选

择以Miniconda3开头、以64结尾的安

装文件，不要下

载错了。

下

载完成后得到的安装文

件是exe版本，直接运行即可

进入安装过程，安装比较

简单，按界面提示进行操

作

即可。安装完成后，出现

如图2-3所示的目录结构，说

明安装正确。

图2-3 Miniconda3安装目录

2. 打开控制台

依次单击“开

始”→“所有程序”

→Miniconda3→Miniconda

Prompt，打开

Miniconda Prompt窗口，它

与CMD控制台类似，输入

命令

就可以控制和配置Python。在Miniconda中

最常用

的是conda命令，该命令

可以执行一些基本操作

。

3.

验证Python

接下来在控制台中

输入python，若安装正确，则会打

印版本号和控制符号。在

控制符号下输入以下代

码：

print("hello Python")

输出结果如图2-4所示，可

以验证Miniconda Python

已经安装成功。

图

2-4 安装成功

4. 使用pip命令

使用

Miniconda的好处在于，它能够很方

便地帮助读者

安装和使

用大量第三方类库。查看

已安装的第三方类

库的

命令如下：

pip list

注意：如果此时

命令行还处于>>>状态，那么

可以输入

exit()退出。

在Miniconda Prompt控制台

输入pip list命令，结

果如图2-5所示

。

图2-5 已安装的第三方类库

Miniconda中使用pip进行的操作还有

很多，其中最重

要的是安

装第三方类库，命令如下

：

pip install name

这里的name是需要安装的第

三方类库名，假设需要安

装NumPy包（这个包已经安装过

），那么输入的命令如

下：

pip install numpy

结

果如图2-6所示。

图2-6 安装NumPy包

使

用Miniconda的一个好处是默认安

装了大部分深度学

习所

需要的第三方类库，这样

可以避免使用者在安装

和使用某个特定的类库

时出现依赖类库缺失的

情况。

2.1.2 PyCharm的下载与安装

和其

他语言类似，Python可以使用Windows自

带的控制

台进行程序编

写。但是这种方式对于较

为复杂的程序

工程来说

，容易混淆相互之间的层

级和交互文件，因

此在编

写程序工程时，建议使用

专用的Python编译器

PyCharm。

1. PyCharm的下载和

安装

（1）进入PyCharm官网的Download页面后

，可以选择

不同的版本，如

图2-7所示，有收费的专业版

和免费的

社区版。这里读

者选择免费的社区版即

可。

图2-7 选择PyCharm的免费版

（2）双击

运行后进入安装界面，如

图2-8所示。直接

单击Next按钮，采

用默认安装即可。

图2-8 安装

界面

（3）如图2-9所示，在安装PyCharm的

过程中需要选择

安装的

位数，这里建议读者选择

与已安装的Python相

同位数的

文件。

图2-9

选择安装的位数

（4）安装完成后，单击Finish按钮，如

图2-10所示。

图2-10 安装完成

2. 使用

PyCharm创建程序

（1）单击桌面上新

生成的

图标进入PyCharm程序界

面，首先是第一次启动的

定位，如图2-11所示。这里

是对

程序存储的定位，一般建

议选择第2个

Do not import settings。

图2-11 由PyCharm自动指

定

（2）单击OK按钮后进入PyChrarm配置

窗口，如图2-12

所示。

图2-12 界面配

置

（3）在配置窗口上可以对

PyCharm的界面进行配置，

选择自

己的使用风格。如果对其

不熟悉，直接使用默

认配

置即可，如图2-13所示。

图2-13 对PyCharm的

界面进行配置

（4）创建一个

新的工程，如图2-14所示。

图2-14 创

建一个新的工程

这里，建

议新建一个PyCharm的工程文件

，如图2-15所

示。

图2-15 新建一个PyCharm的

工程文件

之后右击新建

的工程名PyCharm，选择

New→Python File菜单，新建

一个helloworld.py文

件，内容如图2-16所示

。

图2-16 helloworld.py

输入代码并单击菜单

栏的Run→run…运行代码，或者

直接

右击helloworld.py文件名，在弹出的快

捷菜单中

选择run。如果成功

输出hello world，那么恭喜你，

Python与PyCharm的配

置就完成了。

2.1.3

Python代码小练习

：计算Softmax函数

对于Python科学计算

来说，最简单的想法就是

将数学

公式直接表达成

程序语言，可以说，Python满足了

这

个想法。本小节将使用

Python实现一个深度学习中最

为常见的函数——Softmax函数。至于

这个函数的作

用，现在不

加以说明，笔者只是带领

读者尝试实现其

程序的

编写。

Softmax的计算公式如下：

其

中，V是长度为j的数列V中的

一个数，代入Softmax

的结果其实

就是先对每一个V取e为底

的指数计算变成

非负，然

后除以所有项之和进行

归一化，之后每个V就

可以

解释成：在观察到的数据

集类别中，特定的V属于

某

个类别的概率，或者称作

似然（Likelihood）。

iiii

提示：Softmax用以解决概率

计算中概率结果大而占

绝

对优势的问题。例如函

数计算结果中的两个值

a和b，

且a>b，如果简单地以值的

大小为单位来衡量，那么

在

后续的使用过程中，a永

远被选用，而b由于数值较

小

而不会被选择，但是有

时也需要使用数值小的

b，

Softmax就可以解决这个问题。

Softmax按

照概率选择a和b，由于a的概

率值大于b，在

计算时a经常

会被取得，而b由于概率较

小，被取得的

可能性也较

小，但是也有概率被取得

。

Softmax的代码如下：

import

numpy

def softmax(inMatrix):

m,n = numpy.shape(inMatrix)

outMatrix = numpy.mat(numpy.zeros((m,n)))

soft_sum = 0

for idx in range(0,n):

outMatrix[0,idx] =

math.exp(inMatrix[0,idx]

)

soft_sum += outMatrix[0,idx]

for

idx in range(0,n):

outMatrix[0,idx] = outMatrix[0,idx]

/ sof

t_sum

return outMatrix

a

= numpy.array([[1, 2, 1, 2, 1,

1, 3]])

print(softmax(a))

可以看到，当

传入一个数列后，分别计

算每个数值所

对应的指

数函数值，之后将其相加

后计算每个数值在

数值

和中的概率。结果请读者

自行打印验证。

2.2 安装PyTorch 2.0

Python运行

环境调试完毕后，接下来

的重点就是安装

本书的

主角PyTorch 2.0。由于CPU版本的PyTorch相

对GPU版

本的PyTorch来说，运行速度较慢

，我们推荐

安装GPU版本的PyTorch。

2.2.1 Nvidia 10/20/30/40系

列显卡选择的GPU版

本

由于

40系显卡的推出，目前市场

上会有Nvidia 10、

20、30、40系列显卡并存的

情况。对于需要调用专用

编译器的PyTorch来说，不同的显

卡需要安装不同的依

赖

计算包，作者在此总结了

不同显卡的PyTorch版本以

及CUDA和

cuDNN的对应关系，如表2-1所示。

表

2-1

10/20/30/40系列显卡的版本对比

注

意：这里的区别主要在于

显卡运算库CUDA与cuDNN的

区别，当

在20/30/40系列显卡上使用PyTorch时，可

以

安装11.6以上版本以及cuDNN 8.1以

上版本的计算

包，而在10系

列版本的显卡上，建议优

先使用2.0版本

以前的PyTorch。

下面

以CUDA 11.7+cuDNN 8.2.0组合为例，演示完整

的

PyTorch 2.0

GPU Nvidia运行库的安装步骤，其

他

不同版本CUDA+cuDNN组合的安装过

程基本一致。

2.2.2 PyTorch 2.0

GPU Nvidia运行库的安

装

——以CUDA 11.7+cuDNN 8.2.0为例

从CPU版本的PyTorch开始

深度学习之旅完全是可

以

的，但不是作者推荐的

方式。相对于GPU版本的

PyTorch来说

，在运行速度方面CPU版本存

在着极大的

劣势，很有可

能会让读者的深度学习

止步于前。

如果读者的电

脑不支持GPU，可以直接使用

PyTorch 2.0 CPU版本的安装命令：

pip install numpy --

pre torch

torchvision torchaudio --force-reinstall

--extra-index￾url https://download.pytorch.org/whl/nightly/cpu

如果读

者的电脑支持GPU，则继续下

面本小节的重头

戏，PyTorch 2.0 GPU版本

的前置软件的安装。对于

GPU版本的PyTorch来说，由于调用了

NVIDA显卡作为其

代码运行的

主要工具，因此额外需要

NVIDA提供的运行

库作为运行

基础。

对于PyTorch

2.0的安装来说，最

好根据官方提供的

安装

代码进行安装，如图2-17所示

。在这里PyTorch官

方提供了两种

安装模式，分别对应CUDA 11.7与

CUDA 11.8。

图

2-17 PyTorch官网提供的配置信息

从

图中可以看到，这里提供

了两种不同的CUDA版本的

安

装，作者经过测试，无论是

使用CUDA 11.7还是

CUDA

11.8，在PyTorch 2.0的程序编写

上没有显著

的区别，因此

读者可以根据安装配置

自行选择。下面

以CUDA 11.7为例讲

解安装的方法。

（1）CUDA的安装。在

百度搜索

CUDA 11.7 download，进入官方下载

页面，选择合

适的操作系

统安装方式（推荐使用local本

地化安装方

式），如图2-18所示

。

图2-18

CUDA下载页面

此时下载的

是一个.exe文件，读者自行安

装，不要修

改其中的路径

信息，使用默认路径安装

即可。

（2）下载和安装对应的

cuDNN文件。cuDNN的下载需要

先注册

一个用户，相信读者可以

很快完成，之后直接

进入

下载页面，如图2-19所示。注意

：不要选择错误

的版本，一

定要找到对应的版本号

，另外，如果使用

的是Windows 64位的

操作系统，那么直接下载

x86版

本的cuDNN即可。

图2-19 cuDNN下载页面

下载的cuDNN是一个压缩文件

，将其解压到CUDA安装目

录，如

图2-20所示。

图2-20 解压cuDNN文件

（3）配置

环境变量，这里需要将CUDA的

运行路径加到

环境变量

Path的值中，如图2-21所示。如果cuDNN是

使

用.exe文件安装的，那这个

环境变量自动就配置好

了，读者只要验证一下即

可。

图2-21 配置环境变量

（4）安装

PyTorch及相关软件。从图2-17可以看

到，

对应CUDA 11.7的安装命令如下

：

conda

install pytorch torchvision torchaudio pyt

orch-cuda=11.7

-c pytorch -c

nvidia

如果读者直接安装Python，没有

按2.1.1节安装

Miniconda，则PyTorch安装命令如

下：

pip3 install torch torchvision torchaudio --

index-url

https://download.pytorch.org/whl/cu117

完成PyTorch 2.0 GPU版本的安装后，接

下来验证一

下PyTorch是否安装

成功。

2.2.3 PyTorch 2.0小练习：Hello PyTorch

打开CMD窗口依

次输入如下命令可以验

证安装是否成

功，代码如

下：

import torch

result = torch.tensor(1) +

torch.tensor(2.0)

result

结果如图2-22所示。

图2-22 验证

结果

或者打开前面安装

的PyCharm

IDE，新建一个项目，

再新建

一个hello_pytorch.py文件，输入如下代码

：

import torch

result =

torch.tensor(1) + torch.tensor(2.0)

print(result)

最终结果请读者自行验

证。

2.3

实战：基于PyTorch 2.0的图像去噪

为了给读者提供一个使

用PyTorch进行深度学习的总体

印象，这里准备了一个实

战案例，向读者演示进行

深

度学习任务所需要的

整体流程，读者可能不熟

悉这里

的程序设计和编

写，但是只要求了解每个

过程需要做

的内容以及

涉及的步骤即可。

2.3.1

MNIST数据集

的准备

HelloWorld是任何一门编程

语言入门的基础程序，读

者在开始学习编程时，打

印的第一句话往往就是

HelloWorld。在前面的章节中，我们也

带领读者学习

了PyCharm打印出

来的第一个程序HelloWorld。

在深度

学习编程中也有其特有

的HelloWorld，其编程

对象是一个图

片数据集MNIST，要求对数据集

中的图片

进行分类，因此

难度比较大。

对于好奇的

读者来说，一定有一个疑

问：MNIST究竟是

什么？

实际上，MNIST是

一个手写数字的图片数

据库，它有

60000个训练样本集

和10000个测试样本集。打开来

看，MNIST数据集如图2-23所示。

图2-23 MNIST数

据集

读者可直接使用本

书源码库提供的MNIST数据集

，文件

在dataset文件夹中，如图2-24所

示。

图2-24 dataset文件夹

之后使用NumPy工

具库进行数据读取，代码

如下：

import

numpy as np

x_train = np.load("./dataset/mnist/x_train.npy")

y_train_label = np.load("./dataset/mnist/y_train_label.npy"

)

读者也可以在百度

搜索MNIST的下载地址，直接下

载

train-images-idx3-ubyte.gz、train-labels-idx1-

ubyte.gz等，如图2-25所示。

图2-25 下载页

面

下载4个文件，分别是训

练图片集、训练标签集、测

试

图片集、测试标签集，这

些文件都是压缩文件，解

压

后，可以发现这些文件

并不是标准的图像格式

，而是

二进制文件，其中训

练图片集的部分内容如

图2-26所

示。

图2-26 训练图片集的

部分内容

MNIST训练集内部的

文件结构如图2-27所示。

图2-27

训

练集内部的文件结构

训

练集中有60000个实例，也就是

说这个文件包含

60000个标签

内容，每一个标签的值为

一个0～9的数。

这里先解析文

件中每一个属性的含义

。首先，该数据

是以二进制

格式存储的，我们读取的

时候要以rb方式

读取；其次

，真正的数据只有[value]这一项

，其他

[type]之类的项只是用来

描述的，并不是真正放在

数

据文件里面的信息。

也

就是说，在读取真实数据

之前，要读取4个32位

integer。由[offset]可以

看出，真正的pixel是从

0016开始的

，一个int 32位，所以在读取pixel之前

要读取4个32位integer，也就是magic

number、

number of images、number of rows、

number of columns。

继续

对图片进行分析。在MNIST图片

集中，所有的图片

都是28×28的

，也就是每幅图片都有28×28个

像素。

如图2-28所示，在train-images-idx3-ubyte文件中

偏

移量为0字节处，有一个

4字节的数为0000 0803，表

示魔数。这

里补充一下什么是魔数

，其实它就是一个

校验数

，用来判断这个文件是不

是MNIST里面的train￾images-idx3-ubyte文件。

图2-28 魔数

接

下来是0000 ea60，值为60000，代表容量；从

第8

字节开始有一个4字节

数，值为28，也就是

0000 001c，表示每幅

图片的行数；从第12字节开

始

有一个4字节数，值也为

28，也就是0000

001c，表示

每幅图片的

列数；从第16字节开始才是

我们的像素

值。

这里使用

每784字节代表一幅图片。

2.3.2 MNIST数

据集的特征和标签介绍

前面向读者介绍了两种

不同的MNIST数据集的获取方

式，在这里推荐使用本书

配套源码中的MNIST数据集进

行数据读取，代码如下：

import numpy as np

x_train =

np.load("./dataset/mnist/x_train.npy")

y_train_label = np.load("./dataset/mnist/y_train_label.npy"

)

这

里numpy函数会根据输入的地

址对数据进行处理，并

自

动将其分解成训练集和

验证集。打印训练集的维

度

如下：

(60000, 28, 28)

(60000,)

这是使用数据处

理的第一个步骤，有兴趣

的读者可以

进一步完成

数据的训练集和测试集

的划分。

回到MNIST数据集，每个

MNIST实例数据单元也是由两

部分构成的，包括一幅包

含手写数字的图片和一

个与

其对应的标签。可以

将其中的标签特征设置

成y，而图

片特征矩阵以x来

代替，所有的训练集和测

试集中都包

含x和y。

图2-29用更

为一般化的形式解释了

MNIST数据实例的展

开形式。在

这里，图片数据被展开成

矩阵的形式，矩

阵的大小

为28×28。至于如何处理这个矩

阵，常用的

方法是将其展

开，而展开的方式和顺序

并不重要，只

需要将其按

同样的方式展开即可。

图

2-29

MNIST数据实例的展开形式

下

面回到对数据的读取，前

面已经介绍了，MNIST数据

集实

际上就是一个包含着60000幅

图片的

60000×28×28大小的矩阵张量

[60000,28,28]。

矩阵中行数指的是图片

的索引，用以对图片进行

提

取。而后面的28×28个向量用

以对图片特征进行标

注

。实际上，这些特征向量就

是图片中的像素点，每

幅

手写图片的大小是[28,28]，每个

像素转换为一个0

～1的浮点

数，构成矩阵，如图2-30所示。

图

2-30 手写图片示例

2.3.3

模型的准

备和介绍

对于使用PyTorch进行

深度学习的项目来说，一

个非常

重要的内容是模

型的设计，模型决定了深

度学习在项

目中采用哪

种方式达到目标的主体

设计。在本例中，

我们的目

的是输入一个图像之后

对其进行去噪处理。

对于

模型的选择，一个非常简

单的思路是，图像输出

的

大小应该就是输入的大

小，在这里我们选择使用

Unet作为设计的主要模型。

注

意：对于模型的选择，读者

现在不需要考虑，随着

对

本书学习的深入，见识到

更多处理问题的手段后

，

对模型的选择自然会心

领神会。

我们可以整体看

一下Unet的结构（读者目前只

需要知

道Unet输入和输出大

小是同样的维度即可），如

图2-

31所示。

图2-31 Unet的结构

可以看

到对于整体模型架构来

说，其通过若干个“模

块”（block）与

“直连”（residual）进行数据处

理。这部

分内容我们在后面的章

节中会讲到，目前读

者只

需要知道模型有这种结

构即可。Unet的模型整体

代码

如下：

import torch

import einops.layers.torch

as elt

class Unet(torch.nn.Module):

def __init__(self):

super(Unet, self).__init__()

#模块化结构，这也是

后面常用到的模型结构

self.first_block_down = torch.nn.Sequential(

torch.nn.Conv2d(in_channels=1,out_chann

els=32,kernel_size=3,

padding=1),torch.nn.GELU(),

torch.nn.MaxPool2d(kernel_size=2,stride

=2)

)

self.second_block_down

= torch.nn.Sequential(

torch.nn.Conv2d(in_channels=32,out_chan

nels=64,kernel_size=3,

padding=1),torch.nn.GELU(),

torch.nn.MaxPool2d(kernel_size=2,stride

=2)

)

self.latent_space_block = torch.nn.Sequential

(

torch.nn.Conv2d(in_channels=64,out_chan

nels=128,kernel_size=3,

padding=1),torch.nn.GELU(),

)

self.second_block_up =

torch.nn.Sequential(

torch.nn.Upsample(scale_factor=2),

torch.nn.Conv2d(in_channels=128, out_c

hannels=64, kernel_size=3,

padding=1), torch.nn.GELU(),

)

self.first_block_up = torch.nn.Sequential(

torch.nn.Upsample(scale_factor=2),

torch.nn.Conv2d(in_channels=64, out_ch

annels=32, kernel_size=3,

padding=1),

torch.nn.GELU(),

)

self.convUP_end = torch.nn.Sequential(

torch.nn.Conv2d(in_channels=32,out_chan

nels=1,kernel_size=3,

padding=1), torch.nn.Tanh()

)

def forward(self,img_tensor):

image = img_tensor

image = self.first_block_down(image)

#print(image.shape)

#torch.Size([5, 32, 14, 14])

image

= self.second_block_down(image)

#print(image.shape)

#torch.Size([5, 16, 7,

7])

image = self.latent_space_block(image)

#print(image.shape)

#torch.Size([5,

8, 7, 7])

image = self.second_block_up(image)

#print(image.shape)

#torch.Size([5, 16, 14, 14])

image

= self.first_block_up(image)

#print(image.shape)

#torch.Size([5, 32, 28,

28])

image = self.convUP_end(image)

#print(image.shape)

#torch.Size([5,

32, 28, 28])

return image

if

__name__ == '__main__': #main是Python进行单文

件测试的技

巧，请读者记住这种写法

image =

torch.randn(size=(5,1,28,28))

Unet()(image)

在这里通过一个main架构标

识了可以在单个文件中

对

文件进行测试，请读者

记住这种写法。

2.3.4 模型的损

失函数与优化函数

除了

深度学习模型外，完成一

个深度学习项目设定模

型的损失函数与优化函

数也很重要。这两部分内

容对

于初学者来说可能

并不是很熟悉，在这里读

者只需要

知道有这部分

内容即可。

（1）对损失函数的

选择，在这里选用MSELoss作为损

失函数，MSELoss损失函数的中文

名字为均方损失函数

（Mean Squared

Error Loss）。

MSELoss的

作用是计算预测值和真

实值之间的欧式距

离。预

测值和真实值越接近，两

者的均方差就越小，

均方

差函数常用于线性回归

模型的计算。在PyTorch中

使用MSELoss的

代码如下：

loss = torch.nn.MSELoss(reduction="sum")(pred, y_batch)

（2）优化函数的设

定，在这里我们采用Adam优化

器，

对于Adam优化函数，请读者

自行学习，在这里只提供

使用Adam优化器的代码，如下

所示：

optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

2.3.5 基于深度学习的模

型训练

在介绍了深度学

习的数据准备、模型以及

损失函数和

优化函数后

，下面使用PyTorch训练一个可以

实现去噪

性能的深度学

习整理模型，完整代码如

下（本代码参

看配套资源

的第2章，读者可以直接在

PyCharm中打开

文件运行并查看

结果）：

import

os

os.environ['CUDA_VISIBLE_DEVICES'] = '0' #指定GPU编码

import

torch

import numpy as np

import

unet

import matplotlib.pyplot as plt

from

tqdm import tqdm

batch_size = 320

#设定每次

训练的批次数

epochs = 1024

#设定训练

次数

#device

= "cpu" #PyTorch的特性，需要指定计

算的硬件，如果

没有GPU，就使

用CPU进行计算

device =

"cuda" #在这里默认

使用GPU，如果读者运行出现

问

题，可以将其改成CPU模式

model = unet.Unet()

#导入Unet模型

model = model.to(device) #

将计算模型传

入GPU硬件等待计算

model

= torch.compile(model) #PyTo

rch 2.0的特性

，加速计算速度

optimizer

= torch.optim.Adam(model.parameters(), lr=2e-

5) #设定优化

函数

#载入数据

x_train = np.load("../dataset/mnist/x_train.npy")

y_train_label = np.load("../dataset/mnist/y_train_label.npy

")

x_train_batch = []

for i

in range(len(y_train_label)):

if y_train_label[i] < 2:

#为了加速

演示，这里只运行数据集

中小于2的数字，

也就是0和

1，读者可以自行增加训练

个数

x_train_batch.append(x_train[i])

x_train = np.reshape(x_train_batch,

[-1, 1, 28,

28]) #修正数据输入维度

：

([30596,

28, 28])

x_train /= 512.

train_length

= len(x_train) * 20

#增加数据的单词循环次

数

for

epoch in range(epochs):

train_num = train_length

// batch_size

#计算有多少批次数

train_loss = 0

#用

于损失函数的统计

for i in tqdm(range(train_num)):

#开始

循环训练

x_imgs_batch = []

#创建数据的临

时存储位置

x_step_batch =

[]

y_batch = []

# 对每个批次

内的数据进行处理

for b in range(batch_size):

img =

x_train[np.random.randint(x_tra

in.shape[0])]#提取

单幅图片内容

x = img

y

= img

x_imgs_batch.append(x)

y_batch.append(y)

#将批次数

据转换为PyTorch对应的tensor格式并

将其

传入GPU中

x_imgs_batch = torch.tensor(x_imgs_batch).flo

at().to(device)

y_batch =

torch.tensor(y_batch).float().to(de

vice)

pred = model(x_imgs_batch)

#对模型进行

正向计算

loss = torch.nn.MSELoss(reduction=True)

(pred, y_batch)/batch_size #

使用损失函数

进行计算

#这里读者记住

下面就是固定格式，一般

这样使用即可

optimizer.zero_grad()

#对结果进

行优化计算

loss.backward()

#损失值的反

向传播

optimizer.step()

#对参数进行更新

train_loss += loss.item()

#记录每个批次的损失值

#计算并打印损失值

train_loss

/= train_num

print("train_loss:", train_loss)

#下面

对数据进行打印

image

= x_train[np.random.randint(x_train.shape[0])]#

随机挑

选一条数据计算

image = np.reshape(image,

[1,1,28,28]) #修正数

据维度

image = torch.tensor(image).float().to(device) #挑

选的数据传入

硬件中等待计算

image = model(image)

#使用模

型对数据进行计算

image

= torch.reshape(image, shape=

[28,28]) #修正

模型输出结果

image

= image.detach().cpu().numpy() #将计算结

果导

入CPU中进行后续计算

或者展示

#展示或存储数

据结果

plt.imshow(image)

plt.savefig(f"./img/img_{epoch}.jpg")

在代码中展示了

完整的模型训练过程。首

先传入数

据，然后使用模

型对数据进行计算，计算

结果与真实

值的误差被

回传到模型中，之后PyTorch框架

根据回传

的误差对整体

模型参数进行修正。训练

结果如图2-32

所示。

图2-32 训练结

果

从图2-32中可以很清楚地

看到，随着训练过程的进

行，模型逐渐能够学会对

输入的数据进行整形和

输

出，此时模型的输出结

果表示已经能够很好地

对输入

的图形细节进行

修正，读者可以自行完成

这部分代码

的运行。

2.4 本章

小结

本章是PyTorch实战程序设

计的开始。本章介绍了

PyTorch程

序设计的环境与相关软

件的安装，并演示了

第一

个基于PyTorch的程序的整体设

计过程，以及部分

PyTorch组件的

使用。

实际上可以看到，深

度学习程序设计就是由

一个个小

组件组合来完

成的，本书的后续章节将

会针对每个组

件进行深

入讲解。

第3章

基于PyTorch的MNIST分类

实战

我们在第2章中完成

了第一个PyTorch的示例程序，这

是一个非常简单的MNIST手写

体生成器，其作用是演示

使用一个PyTorch程序的基本构

建与完整的训练过程。

PyTorch作

为一个成熟的深度学习

框架，对于使用者来

说，即

使是初学者也能够很容

易地上手进行深度学习

项目的训练，非常迅捷地

将这些框架作为常用工

具来

使用。初学者只要编

写出简单的代码就可以

构建相应

的模型进行实

验，而其缺点在于框架的

背后内容都被

隐藏起来

了。

本章将首先使用PyTorch完成

MNIST分类的练习，主要

目的是

熟悉PyTorch的基本使用流程；之

后将讲解一下

PyTorch模型结构

输出与可视化工具，以方

便读者对自

己设计的模

型结构有一个直观的认

识。

3.1 实战：基于PyTorch的MNIST手写体分

类

第2章对MNIST数据做了介绍

，描述了其构成方式及其

数据的特征和标签的含

义等。了解这些有助于编

写合

适的程序来对MNIST数据

集进行分析和识别。本节

将使

用同样的数据集完

成对其进行分类的任务

。

3.1.1 数据图像的获取与标签

的说明

MNIST数据集的详细介

绍在第2章中已经完成，读

者可

以使用相同的代码

对数据进行获取，代码如

下：

import numpy as np

x_train =

np.load("./dataset/mnist/x_train.npy")

y_train_label = np.load("./dataset/mnist/y_train_label.npy"

)

基本数据的获取与第

2章类似，这里就不过多阐

述了，

不过需要注意的是

，在第2章介绍数据集时只

使用了图

像数据，没有对

标签进行说明，在这里重

点对数据标

签，也就是y_train_label进

行介绍。

我们可以使用下

面语句打印出数据集的

前10个标签：

print(y_train_label[:10])

结果如下：

[5 0 4 1 9 2

1 3 1 4]

可以

很清楚地看到，这里打印

出了10个数字字符，每

个字

符对应相同序号的数据

图像所对应的数字标签

，

即图像3的标签对应的就

是4这个数字字符。

可以说

训练集中每个实例的标

签对应0～9的任意一个

数字

，用以对图片进行标注。另

外需要注意的是，对

于提

取出来的MNIST的特征值，默认

使用一个0～9的数

值进行标

注，但是这种标注方法并

不能使得损失函数

获得

一个好的结果，因此通常

使用one_hot计算方法，

将数值具

体落在某个标注区间中

。

one_hot的标注方法请读者自行

学习掌握。这里主要介

绍

将单一序列转换成one_hot的方

法。一般情况下，可

以用NumPy实

现one_hot的表示方法，但是转换

生成的

是numpy.array格式的数据，并

不适合直接输入到

PyTorch中使

用。

如果读者能够自行编

写将序列值转换成one_hot的函

数，那么编程功底真是不

错。PyTorch提供了已经编写

好的

转换函数：

torch.nn.functional.one_hot

完整的one_hot使用方

法如下：

import numpy

as np

import torch

x_train =

np.load("./dataset/mnist/x_train.npy")

y_train_label = np.load("./dataset/mnist/y_train_label.npy"

)

x

= torch.tensor(y_train_label[:5],dtype=torch.int64)

# 定义一个张量输

入，因为此时有 5 个数值，且

最大值为9，类别

数为10

# 所以

我们可以得到 y 的输出结

果的形状为 shape=(5,10)，即5

行12列

y = torch.nn.functional.one_hot(x, 10) #

一个

参数张量

x，10为类别数

ptint(y)

结果

如下：

tensor([[0, 0,

0, 0, 0, 1, 0, 0,

0, 0],

[1, 0, 0, 0,

0, 0, 0, 0, 0, 0],

[0, 0, 0, 0, 1, 0,

0, 0, 0, 0],

[0, 1,

0, 0, 0, 0, 0, 0,

0, 0],

[0, 0, 0, 0,

0, 0, 0, 0, 0, 1]])

可以看到，one_hot的作用是

将一个序列转换成以

one_hot形

式表示的数据集。所有的

行或者列都被设置

成0，而

每个特定的位置都对应

一个1来表示，如图3-1

所示。

图

3-1 one_hot形式表示的数据集

对于

MNIST数据集的标签来说，这实

际上就是一个

60000幅图片的

60000×10大小的矩阵张量

[60000,10]。前面的

数指的是数据集中图片

的个数为

60000个，后面的10指的

是10个列向量。

下面使用PyTorch 2.0框

架完成手写体的识别。

3.1.2 模

型的准备（多层感知机）

在

第2章已经讲过了，PyTorch最重要

的一项内容是模

型的准

备与设计，而模型的设计

最关键的一点就是了

解

输出和输入的数据结构

类型。

通过第2章有关图像

去噪的演示，读者已经了

解了我们

的输入数据格

式是一个[28,28]大小的二维图

像。而通

过对数据结构的

分析，我们可以知道，对于

每个图形

都有一个确定

的分类结果，也就是0～10的一

个确定数

字。

下面将按这

个想法来设计模型。从前

面对图像的分析

来看，对

整体图形进行判别的一

个基本想法就是将图

像

作为一个整体直观地进

行判别，因此基于这种解

决

问题的思路，简单的模

型设计就是同时对图像

所有参

数进行计算，即使

用一个多层感知机（Multi￾Layer Perceptron, MLP）对图

像进行分类。整体的

模型

设计结构如图3-2所示。

图3-2 整

体的模型设计结构

从图

3-2可以看到，一个多层感知

机模型就是将数据输

入

后，分散到每个模型的节

点（隐藏层），进行数据

计算

后，再将计算结果输出到

对应的输出层中。多层

感

知机的模型结构如下：

class NeuralNetwork(nn.Module):

def __init__(self):

super(NeuralNetwork, self).__init__()

self.flatten = nn.Flatten()

self.linear_relu_stack = nn.Sequential(

nn.Linear(28*28,312),

nn.ReLU(),

nn.Linear(312, 256),

nn.ReLU(),

nn.Linear(256,

10)

)

def forward(self, input):

x

= self.flatten(input)

logits = self.linear_relu_stack(x)

return

logits

3.1.3 损

失函数的表示与计算

第

2章使用了MSELoss作为目标图形

与预测图形的损失

值，而

在本例中，我们需要预测

的目标是图形的“分

类”

，而

不是图形表示本身，因此

我们需要寻找并使

用一

种新的能够对类别归属

进行“计算”的函数。

本例所

使用的交叉熵损失函数

为

torch.nn.CrossEntropyLoss。PyTorch官方网站对其

介绍如

下：

CLASS

torch.nn.CrossEntropyLoss(weight=None, size_a

verage=None,

ignore_index=- 100,reduce=None, reduction='mean',

label_sm

oothing=0.0)

该损失函数计算输入

值（Input）和目标值（Target）

之间的交叉

熵损失。交叉熵损失函数

CrossEntropyLoss可用于训练单类别或者

多类别的分

类问题。给定

参数weight时，会为传递进来的

每个类

别的计算数值重

新加载一个修正权重。当

数据集分布

不均衡时，这

是很有用的。

同样需要注

意的是，因为torch.nn.CrossEntropyLoss

内置了Softmax运算

，而Softmax的作用是计算分类结

果中最大的那个类。从图

3-3所示的对PyTorch 2.0中

CrossEntropyLoss的实现可以

看到，此时

CrossEntropyLoss已经在计算的

同时实现了Softmax计

算，因此在

使用torch.nn.CrossEntropyLoss作为损失

函数时，不

需要在网络的最后添加

Softmax层。此外，

label应为一个整数，而

不是One-Hot编码形式。

图3-3 使用torch.nn.CrossEntropyLoss()作

为损失

函数

CrossEntropyLoss示例代码如

下：

import torch

y = torch.LongTensor([0])

z = torch.Tensor([[0.2,0.1,-0.1]])

criterion = torch.nn.CrossEntropyLoss()

loss = criterion(z,y)

print(loss)

CrossEntropyLoss的数学公式较为复杂

，建议学有余

力的读者查

阅相关内容进行学习，目

前只需要掌握这

方面内

容即可。

3.1.4 基于PyTorch的手写体识

别的实现

下面介绍基于

PyTorch的手写体识别的实现。通

过前文

的介绍，我们还需

要定义深度学习的优化

器部分，在

这里采用Adam优化

器，相关代码如下：

model = NeuralNetwork()

optimizer = torch.optim.Adam(model.parameters(),

lr=2e-

5) #设定优

化函数

在这个实战案例

中首先需要定义模型，之

后将模型参

数传入优化

器中，lr是对学习率的设定

，根据设定的

学习率进行

模型计算。完整的手写体

识别模型如下：

import os

os.environ['CUDA_VISIBLE_DEVICES'] = '0' #指定GPU编号

import

torch

import numpy as np

from

tqdm import tqdm

batch_size =320#设定每次训练的批次数

epochs=1024 #设定训练次数

#device="cpu" #PyTorch的特性，需

要指定计算的硬件，如

果

没有GPU，就使用CPU进行计算

device="cuda" #在

这里默认使用GPU，如果读者

运行出现问

题，可以将其

改成CPU模式

#设定的多层感

知机网络模型

class NeuralNetwork(torch.nn.Module):

def __init__(self):

super(NeuralNetwork,

self).__init__()

self.flatten = torch.nn.Flatten()

self.linear_relu_stack =

torch.nn.Sequentia

l(

torch.nn.Linear(28*28,312),

torch.nn.ReLU(),

torch.nn.Linear(312, 256),

torch.nn.ReLU(),

torch.nn.Linear(256, 10)

)

def forward(self,

input):

x = self.flatten(input)

logits =

self.linear_relu_stack(x)

return logits

model = NeuralNetwork()

model = model.to(device)

#将计算模

型传入GPU硬件等待计算

model =

torch.compile(model)

#PyTorch 2.0的

特性，加速计算速度

loss_fu = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), l

r=2e-5) #设定

优化函数

#载入数据

x_train = np.load("../../dataset/mnist/x_train.npy"

)

y_train_label

= np.load("../../dataset/mnist/y_trai

n_label.npy")

train_num = len(x_train)//batch_size

#开始

计算

for epoch in range(20):

train_loss

= 0

for i in range(train_num):

start = i * batch_size

end

= (i + 1) * batch_size

train_batch = torch.tensor(x_train[

start:end]).to(device)

label_batch =

torch.tensor(y_train_

label[start:end]).to(device)

pred = model(train_batch)

loss

= loss_fu(pred,label_batch)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss

+= loss.item() # 记

录每个批次的损

失值

#

计算并打印损失值

train_loss /= train_num

accuracy = (pred.argmax(1)

== label_batch

).type(torch.float32).sum().item()

/ batch_size

print("train_loss:",

round(train_loss,2),"ac

curacy:",round(accuracy,2))

此时模型的训练结果如

图3-4所示。

图3-4 模型的训练结

果

可以看到随着模型循

环次数的增加，模型的损

失值在

降低，而准确率在

增高，具体请读者自行验

证测试。

3.2 PyTorch 2.0模型结构输出与

可视化

上一节中完成了

基于PyTorch 2.0的MNIST模型的设

计，并完

成了MNIST手写体的识别。此时

，可能会有读

者对我们自

己设计的模型结构感到

好奇，如果有一种

能够可

视化模型结构的现成方

法，用起来就非常方便

了

。

3.2.1 查看模型结构和参数信

息

为了解决模型结构的

展示问题，PyTorch官方提供了对

应的建议模型打印工具

，即直接调用print函数来完

成

，例如对上一节中我们实

现的MNIST模型：

class NeuralNetwork(nn.Module):

def __init__(self):

super(NeuralNetwork, self).__init__()

self.flatten = nn.Flatten()

self.linear_relu_stack

= nn.Sequential(

nn.Linear(28*28,312),

nn.ReLU(),

nn.Linear(312, 256),

nn.ReLU(),

nn.Linear(256, 10)

)

def forward(self,

input):

x = self.flatten(input)

logits =

self.linear_relu_stack(x)

return logits

读者可以直接

使用如下函数完成模型

参数的打印：

if __name__

== '__main__':

model = NeuralNetwork()

print(model)

打印结果如

图3-5所示。

图3-5 对模型具体使

用的函数及其对应的参

数进行打

印

可以看到此

结果是对模型具体使用

的函数及其对应的

参数

进行打印。

为了更进一步

简化对模型参数的打印

，读者可以使用

作者提供

的对参数和结构进行打

印的函数，代码如下

所示

：

params = list(model.parameters())

k = 0

for i in

params:

l = 1

print("该层的结构：" +

str(list(i.size())))

for j in i.size():

l

*= j

print("该层参数和

：" + str(l))

k

= k + l

print("总参数数量和：" +

str(k))

运行完此

段代码后，可以对每层的

节点输出和参数总

量进

行打印，结果如图3-6所示。

图

3-6 对每层的节点输出和参

数总量进行打印

3.2.2

基于netron库

的PyTorch 2.0模型可视化

上一小节

讲解了模型结构的输出

，但是相对于简单的

文本

化输出方式，PyTorch第三方提供

了许多具有直观

表示的

模型可视化展示方式。netron就

是一个深度学

习模型可

视化库，支持可视化表示

PyTorch

2.0的模

型存档文件。我们可

以将上一小节中PyTorch的模型

结

构进行保存，并通过netron进

行可视化展示。模型保

存

代码如下所示：

import torch

device = "cuda" #在这里默

认使用GPU，如果出现运行问

题可

以将其改成cpu模式

#设

定的多层感知机网络模

型

class NeuralNetwork(torch.nn.Module):

def __init__(self):

super(NeuralNetwork, self).__init__()

self.flatten = torch.nn.Flatten()

self.linear_relu_stack = torch.nn.Sequentia

l(

torch.nn.Linear(28*28,312),

torch.nn.ReLU(),

torch.nn.Linear(312, 256),

torch.nn.ReLU(),

torch.nn.Linear(256, 10)

)

def forward(self, input):

x = self.flatten(input)

logits = self.linear_relu_stack(x)

return logits

#进行模型的保存

model = NeuralNetwork()

torch.save(model, './model.pth')

#将模

型保存为.pth文件

读者可以

自行百度netron的下载地址，建

议从GitHub

上下载netron，其主页上也

提供了不同版本的安装

方

式，如图3-7所示。

图3-7 netron不同版

本的安装方式

读者可以

依照自己的操作系统下

载对应的文件，在这

里安

装的是基于Windows的exe文件，安装

运行后会出

现一个图形

界面，直接在界面上点击

file操作符号，

打开我们刚才

保存的pth文件，显示结果如

图3-8所

示。

图3-8 打开model.pth的显示结

果

可以看到，此时model.pth的模型

结构被可视化展示出

来

，每个模块输入输出维度

在图上都有展示，点击带

颜色的部分可以看到每

个模块更详细的说明，如

图3-9

所示。

图3-9 每个模块更详

细的说明

感兴趣的读者

可以自行安装测试。

3.2.3 更多

的PyTorch 2.0模型可视化工具

除了

上面介绍的netron工具，还有更

多的PyTorch可视

化工具，有兴趣

的读者可以根据需要选

择合适的可视

化工具来

使用。

1. torchsummary

torchsummary会输出网络模型的

过程层结构、层参数

和总

参数等信息。对于大多数

新手和“深度学习炼丹

师

”关注具体的层间参数信

息没有太大意义，但是

torchsummary可

以很方便地用来获取网

络参数量和输

出模型大

小，效果如图3-10所示。

图3-10 torchsummary获取

网络参数量和输出模型

大

小

2. hiddenlayer

hiddenlayer是比较实用的一种

网络可视化方法，功能

也

相对比较多，输出的网络

结构图比较直观，细节也

相对丰富。hiddenlayer输出的网络结

构例子如图3-11

所示。

图3-11 hiddenlayer输出

的网络结构

3. PlotNeuralNet

PlotNeuralNet可以利用Python将

.py文件中定义的网

络结构

转换为.tex文件，并生成相应

图形，实际上这

里的网络

结构与自己训练或者测

试的网络结构没有太

大

关系。PlotNeuralNet上手难度较大，其输

出的网络

结构示例如图

3-12所示。

图3-12 PlotNeuralNet输出的网络结构

4. torchvision

torchvision是PyTorch的一个图形库，它服务

于

PyTorch深度学习框架，主要用

来构建计算机视觉模

型

，其使用效果如图3-13所示。

图

3-13 torchvision输出的网络结构

以上4种

以及上一小节介绍的netron是

较为常用的可视

化工具

。针对PyTorch 2.0的模型可视化工具

，读者

可以选择适合自己

需要的工具来学习。

3.3 本章

小结

本章演示了使用PyTorch框

架进行手写体识别的完

整例

子，在这个例子中我

们完整地对MNIST手写体项目

进行

分类，并讲解了模型

的标签问题以及本书后

面常用的

损失函数计算

方面的内容，可以说CrossEntropy损失

函数是深度学习方面最

重要的一个损失函数，需

要读

者认真掌握。

本章同

时也介绍了PyTorch 2.0的模型可视

化工具，

可以帮助读者在

后续的学习中更加方便

地掌握模型的

基本构建

以及验证模型的架构和

组成。

第4章

深度学习的理

论基础

深度学习是目前

以及可以预见的将来最

为重要、最有

发展前景的

一个学科，而深度学习的

基础是神经网

络，神经网

络本质上是一种无须事

先确定输入输出之

间映

射关系的数学方程，仅通

过自身的训练学习某种

规则，在给定输入值时得

到最接近期望输出值的

结

果。

作为一种智能信息

处理系统，人工神经网络

实现其功

能的核心是反

向传播（Back Propagation,

BP）神

经网络，如图4-1所

示。

图4-1 BP神经网络

BP神经网络

（反向传播神经网络）是一

种按误差反向

传播（简称

误差反传）训练的多层前

馈网络，它的基

本思想是

梯度下降法，利用梯度搜

索技术，以期使网

络的实

际输出值和期望输出值

的误差均方差最小。本

章

将全面介绍BP神经网络的

概念、原理及其背后的数

学原理。

4.1 反向传播神经网

络的历史

在介绍反向传

播神经网络之前，人工神

经网络是必须

提到的内

容。人工神经网络

（Artificial Neural Network, ANN）的发展

经历

了大约半个世纪，从

20世纪40年代初到80年代，神经

网络的研究经历了低潮

和高潮几起几落的发展

过程。

1930年，B.Widrow和M.Hoff提出了自适应

线性元件网

络（ADAptive LINear NEuron, ADALINE），这是

一种

连续取值的线性加权求

和阈值网络。后来，在此

基

础上发展了非线性多层

自适应网络。Widrow-Hoff学

习算法被

称为最小均方误差

（Least Mean Square, LMS）学习

规则。从此，神

经网络的发

展进入了第一个高潮期

。

的确，在有限范围内，感知

机有较好的功能，并且收

敛定理得到证明。单层感

知机能够通过学习把线

性可

分的模式分开，但对

像XOR（异或）这样简单的非线

性

问题却无法求解，这一

点让人们大失所望，甚至

开始

怀疑神经网络的价

值和潜力。

1939年，麻省理工学

院著名的人工智能专家

M.Minsky

和S.Papert出版了颇有影响力的

Perceptron一书，从

数学上剖析了简

单神经网络的功能和局

限性，并且指

出多层感知

机还不能找到有效的计

算方法。由于

M.Minsky在学术界的

地位和影响，其悲观的结

论被大

多数人不做进一

步分析而接受，加之当时

以逻辑推理

为研究基础

的人工智能和数字计算

机的辉煌成就，大

大降低

了人们对神经网络研究

的热情。

其后，人工神经网

络的研究进入了低潮。尽

管如此，

神经网络的研究

并未完全停顿下来，仍有

不少学者在

极其艰难的

条件下致力于这一研究

。

1943年，心理学家W·McCulloch和数理逻辑

学家

W·Pitts在分析、总结神经元

基本特性的基础上提出

了神经元的数学模型（McCulloch-Pitts模

型，简称MP

模型），标志着神经

网络研究正式开始。受当

时研究

条件的限制，很多

工作不能模拟，在一定程

度上影响

了MP模型的发展

。尽管如此，MP模型对后来的

各种神

经元模型及网络

模型都有很大的启发作

用，在此后的

1949年，D.O.Hebb从心理学

的角度提出了至今仍对

神

经网络理论有着重要

影响的Hebb法则。

1945年，冯·诺依曼

领导的设计小组试制成

功存储程

序式电子计算

机，标志着电子计算机时

代的开始。

1948年，他在研究工

作中比较了人脑结构与

存储程序

式计算机的根

本区别，提出了以简单神

经元构成的再

生自动机

网络结构。但是，由于指令

存储式计算机技

术的发

展非常迅速，迫使他放弃

了神经网络研究的新

途

径，继续投身于指令存储

式计算机技术的研究，并

在此领域做出了巨大贡

献。虽然冯·诺依曼的名字

是

与普通计算机联系在

一起的，但他也是人工神

经网络

研究的先驱之一

，其照片如图4-2所示。

图4-2 冯·诺

依曼

1958年，F·Rosenblatt设计制作了“感知

机”

，它是

一种多层的神经

网络。这项工作首次把人

工神经网络

的研究从理

论探讨付诸工程实践。感

知机由简单的阈

值性神

经元组成，初步具备了诸

如学习、并行处理、

分布存

储等神经网络的一些基

本特征，从而确立了从

系

统角度进行人工神经网

络研究的基础。

1972年，T.Kohonen和J.Anderson不约

而同地提出具有

联想记

忆功能的新神经网络。1973年

，S.Grossberg与

G.A.Carpenter提出了自适应共振理

论

（Adaptive Resonance Theory,

ART），并在以后

的若干年内

发展了ART1、ART2、ART3这3个神经网络模

型，从而为神经网络研究

的发展奠定了理论基础

。

进入20世纪80年代，特别是80年

代末期，对神经网络

的研

究从复兴很快转入了新

的热潮。这主要是因为：

·经

过十几年迅速发展，以逻

辑符号处理为主的人工

智能理论和冯·诺依曼计

算机在处理诸如视觉、听

觉、形象思维、联想记忆等

智能信息处理问题上受

到

了挫折。

·并行分布处理

的神经网络本身的研究

成果使人们看

到了新的

希望。

1982年，美国加州工学院

的物理学家J.Hoppfield提出

了Hopfield神经

网络

（Hopfield Neural

Network, HNN）模型，并首次

引入了

网络能量函数的概念，使

网络稳定性研究有了

明

确的判据，其电子电路实

现为神经计算机的研究

奠

定了基础，同时也开拓

了神经网络用于联想记

忆和优

化计算的新途径

。

1983年，K.Fukushima等提出了神经认知机

网络理论。

1985年，D.H.Ackley、G.E.Hinton和T.J.Sejnowski将

模拟退

火概念移植到Boltzmann机模型的

学习中，以保

证网络能收

敛到全局最小值。1983年，D.Rumelhart和

J.McCelland等

提出了PDP

（Parallel

Distributed Processing）理论，致力

于认知

微观结构的探索，同时发

展了多层网络的BP算

法，使

BP神经网络成为目前应用

最广的网络。

反向传播（Back Propagation，见

图4-3）一词的使

用出现在1985年

后，它的广泛使用是在1983年

D.Rumelhart和J.McCelland所著的

Parallel Distributed Processing这本书出版以

后。1987年，T.Kohonen提出了自组织映射

（Self Organizing Map,

SOM）。1987年，美国电

气和电子工程

师学会

（Institute For Electrical And

Electronic

Engineers, IEEE）在圣地亚哥（San Diego）召开

了盛大规模的神经网络

国际学术会议，国际神经

网络

学会

（International

Neural Network Society, INN

S）也随之诞生。

图

4-3

反向传播

1988年，国际神经网

络学会的正式杂志

Neural Networks创刊

。从1988年开始，国际神经网

络

学会和IEEE每年联合召开一

次国际学术年会。1990

年，IEEE神经

网络会刊问世，各种期刊

的神经网络特

刊层出不

穷，神经网络的理论研究

和实际应用进入了

一个

蓬勃发展的时期。

BP神经网

络（见图4-4）的代表者是D.Rumelhart和

J.McCelland。BP神

经网络是一种按误差逆

传播算法训

练的多层前

馈网络，是目前应用最广

泛的神经网络模

型之一

。

图4-4 BP神经网络

BP算法的学习

过程由信息的正向传播

和误差的反向传

播两个

过程组成。而BP神经网络可

分为输入层、输出

层和隐

含层，各层作用说明如下

。

·输入层：各神经元负责接

收来自外界的输入信息

，

并传递给中间层各神经

元。

·中间层：中间层是内部

信息处理层，负责信息变

换，根据信息变化能力的

需求，中间层可以设计为

单

隐含层或者多隐含层

结构。

·隐含层：传递到输出

层各神经元的信息经过

进一步

的处理后，完成一

次学习的正向传播处理

过程，由输

出层向外界输

出信息处理结果。

当实际

输出与期望输出不符时

，进入误差的反向传播

阶

段。误差通过输出层，按误

差梯度下降的方式修正

各层权值，向隐含层、输入

层逐层反传。周而复始的

信息正向传播和误差反

向传播过程是各层权值

不断调

整的过程，也是神

经网络学习训练的过程

，此过程一

直进行到网络

输出的误差减少到可以

接受的程度，或

者预先设

定的学习次数为止。

目前

神经网络的研究方向和

应用很多，反映了多学科

交叉技术领域的特点。主

要的研究工作集中在以

下几

个方面：

·生物原型研

究。从生理学、心理学、解剖

学、脑科

学、病理学等生物

科学方面研究神经细胞

、神经网

络、神经系统的生

物原型结构及其功能机

理。

·建立理论模型。根据生

物原型的研究，建立神经

元、神经网络的理论模型

，其中包括概念模型、知识

模型、物理化学模型、数学

模型等。

·网络模型与算法

研究。在理论模型研究的

基础上构

建具体的神经

网络模型，以实现计算机

模拟或硬件的

仿真，还包

括网络学习算法的研究

。这方面的工作也

称为技

术模型研究。

·人工神经网

络应用系统。在网络模型

与算法研究的

基础上，利

用人工神经网络组成实

际的应用系统。例

如，完成

某种信号处理或模式识

别的功能、构建专家

系统

、制造机器人等。

纵观当代

新兴科学技术的发展历

史，人类在征服宇宙

空间

、基本粒子、生命起源等科

学技术领域的进程中

经

历了崎岖不平的道路。我

们也会看到，探索人脑功

能和神经网络的研究将

伴随着重重困难的克服

而日新

月异。

4.2

反向传播神

经网络两个基础算法详

解

在正式介绍BP神经网络

之前，需要先介绍两个非

常重

要的算法，即最小二

乘法随机梯度和下降算

法。

最小二乘法是统计分

析中常用的逼近计算的

一种算

法，其交替计算结

果使得最终结果尽可能

地逼近真实

结果。而随机

梯度下降算法充分利用

了深度学习的运

算特性

的迭代和高效性，通过不

停地判断和选择当前

目

标下的最优路径，使得能

够在最短路径下达到最

优

的结果，从而提高大数

据的计算效率。

4.2.1 最小二乘

法详解

最小二乘（Least

Square, LS）法是一

种数学优化

技术，也是一

种机器学习常用的算法

。它通过最小化

误差的平

方和寻找数据的最佳函

数匹配。利用最小二

乘法

可以简便地求得未知的

数据，并使得这些求得的

数据与实际数据之间误

差的平方和最小。最小二

乘法

还可用于曲线拟合

。其他一些优化问题也可

通过最小

化能量或最大

化熵用最小二乘法来表

达。

由于最小二乘法不是

本章的重点内容，因此笔

者只通

过一个图示演示

一下最小二乘法LS的原理

。最小二乘

法的原理如图

4-5所示。

图4-5 最小二乘法的原

理

从图4-5可以看到，若干个

点依次分布在向量空间

中，

如果希望找出一条直

线和这些点达到最佳匹

配，那么

最简单的方法就

是希望这些点到直线的

值最小，即下

面的最小二

乘法实现公式的值最小

。

这里直接引用真实值与

计算值之间的差的平方

和，具

体而言，这种差值有

一个专门的名称——残差。基

于

此，表达残差的方式有

以下3种：

·∞范数：残差绝对值

的最大值为，即所有数据

点中

残差距离的最大值

。

·L1范数：绝对残差和，即所有

数据点残差距离之

和。

·L2范

数：残差平方和。

可以看到

，所谓的最小二乘法也就

是L2范数的一个具

体应用

。通俗地说，就是看模型计

算出来的结果与真

实值

之间的相似性。

因此，最小

二乘法的定义如下：

对于

给定的数据(x,y)(i=1,…,m)，在取定的假

设空间

H中，求解f(x)∈H，使得残差

的L2范数最小。

ii

看到这里，可

能有读者会提出疑问，这

里的f(x)该如

何表示？

实际上

，函数f(x)是一条多项式函数

曲线：

由上面的公式可以

知道，所谓的最小二乘法

，就是找

到一组权重w，使得

最小。那么问题又来了，如

何能使

最小二乘法的值

最小？

对于求出的最小二

乘法的结果，可以使用数

学上的微

积分处理方法

，这是一个求极值的问题

，只需要对权

值依次求偏

导数，最后令偏导数为0，即

可求出极值

点。

具体实现

最小二乘法的代码如下

（注意，为了简化起

见，本示

例使用一元一次方程组

来演示拟合）。

【程序4-1】

import numpy as np

from matplotlib import pyplot as plt

A = np.array([[5],[4]])

C = np.array([[4],[6]])

B = A.T.dot(C)

AA = np.linalg.inv(A.T.dot(A))

l=AA.dot(B)

P=A.dot(l)

x=np.linspace(-2,2,10)

x.shape=(1,10)

xx=A.dot(x)

fig

= plt.figure()

ax= fig.add_subplot(111)

ax.plot(xx[0,:],xx[1,:])

ax.plot(A[0],A[1],'ko')

ax.plot([C[0],P[0]],[C[1],P[1]],'r-o')

ax.plot([0,C[0]],[0,C[1]],'m-o')

ax.axvline(x=0,color='black')

ax.axhline(y=0,color='black')

margin=0.1

ax.text(A[0]+margin,

A[1]+margin, r"A",fontsize=20)

ax.text(C[0]+margin, C[1]+margin, r"C",fontsize=20)

ax.text(P[0]+margin,

P[1]+margin, r"P",fontsize=20)

ax.text(0+margin,0+margin,r"O",fontsize=20)

ax.text(0+margin,4+margin, r"y",fontsize=20)

ax.text(4+margin,0+margin,

r"x",fontsize=20)

plt.xticks(np.arange(-2,3))

plt.yticks(np.arange(-2,3))

ax.axis('equal')

plt.show()

最终结

果如图4-6所示。

图4-6 最小二乘

法拟合曲线

4.2.2 道士下山的

故事——梯度下降算法

在介

绍随机梯度下降算法之

前，这里先讲一个道士下

山的故事。请读者参考图

4-7。

图4-7

模拟随机梯度下降算

法的演示图

为了便于理

解，我们将其比喻成道士

想要出去游玩的

一座山

。

设想道士有一天和道友

一起到一座不太熟悉的

山上去

玩，在兴趣盎然中

很快登上了山顶。但是天

有不测，

下起了雨。如果这

时需要道士和其同来的

道友用最快

的速度下山

，那么怎么办呢？

如果想以

最快的速度下山，那么最

快的办法就是顺着

坡度

最陡峭的地方走下去。但

是由于不熟悉路，道士

在

下山的过程中，每走一段

路程就需要停下来观望

，

从而选择最陡峭的下山

路。这样一路走下来的话

，可

以在最短时间内走到

山脚。

根据图4-7可以近似地

表示为：

① → ② → ③

→ ④ → ⑤ → ⑥

→

⑦

每个数字代表每

次停顿的地点，这样只需

要在每个停

顿的地点选

择最陡峭的下山路即可

。

这就是道士下山的故事

，随机梯度下降算法和这

个类

似。如果想要使用最

迅捷的下山方法，那么最

简单的

方法就是在下降

一个梯度的阶层后，寻找

一个当前获

得的最大坡

度继续下降。这就是随机

梯度算法的原

理。

从上面

的例子可以看到，随机梯

度下降算法就是不停

地

寻找某个节点中下降幅

度最大的那个趋势进行

迭代

计算，直到将数据收

缩到符合要求的范围为

止。通过

数学公式表达的

方式计算的话，公式如下

：

在4.2.1节讲最小二乘法的时

候，我们通过最小二乘法

说明了直接求解最优化

变量的方法，也介绍了求

解的

前提条件是计算值

与实际值的偏差的平方

最小。

但是在随机梯度下

降算法中，对于系数需要

不停地求

解出当前位置

下最优化的数据。使用数

学方式表达的

话，就是不

停地对系数θ求偏导数。公

式如下：

在公式中，θ会向着

梯度下降最快的方向减

小，从而

推断出θ的最优解

。

因此，随机梯度下降算法

最终被归结为：通过迭代

计

算特征值，从而求出最

合适的值。求解θ的公式如

下：

在公式中，α是下降系数

。用较为通俗的话表示，就

是用来计算每次下降的

幅度大小。系数越大，每次

计

算中的差值就较大；系

数越小，差值就越小，但是

计

算时间也会相应延长

。

随机梯度下降算法的迭

代过程如图4-8所示。

图4-8 随机

梯度下降算法的迭代过

程

从图4-8中可以看到，实现

随机梯度下降算法的关

键是

拟合算法的实现。而

本例的拟合算法的实现

较为简

单，通过不停地修

正数据值从而达到数据

的最优值。

随机梯度下降

算法在神经网络特别是

机器学习中应用

较广，但

是由于其天生的缺陷，噪

音较大，使得在计

算过程

中并不是都向着整体最

优解的方向优化，往往

只

是得到局部最优解。因此

，为了克服这些困难，最

好

的办法就是增大数据量

，在不停地使用数据进行

迭

代处理的时候，能够确

保整体的方向是全局最

优解，

或者最优结果在全

局最优解附近。

【程序4-2】

x = [(2,

0, 3), (1, 0, 3), (1,

1, 3), (1,4

, 2), (1,

2, 4)]

y = [5, 6,

8, 10, 11]

epsilon = 0.002

alpha = 0.02

diff = [0,

0]

max_itor = 1000

error0 =

0

error1 = 0

cnt =

0

m = len(x)

theta0 =

0

theta1 = 0

theta2 =

0

while True:

cnt += 1

for i in range(m):

diff[0] =

(theta0 * x[i]

[0] + theta1

* x[i][1] + theta2 * x[i][2])

- y[i]

theta0 -= alpha *

diff[0] * x[i][0]

theta1 -= alpha

* diff[0] * x[i][1]

theta2 -=

alpha * diff[0] * x[i][2]

error1

= 0

for lp in range(len(x)):

error1 += (y[lp] - (theta0 +

theta1 * x

[lp][1] + theta2

* x[lp][2])) **

2 / 2

if abs(error1 - error0) < epsilon:

break

else:

error0 = error1

print('theta0

: %f, theta1 : %f, theta2

: %f,

error1 : %f' %

(theta0, theta1,

theta2, error1))

print('Done: theta0

: %f, theta1 : %f, theta2

: %f' % (theta0, theta1, theta2))

print('迭代

次数: %d' % cnt)

最终结果打印如下

：

theta0

: 0.100684, theta1 : 1.564907, theta2

: 1.9206

52, error1 : 0.569459

Done: theta0 : 0.100684, theta1 :

1.564907, theta2 :

1.920652

迭代次数: 24

从结果来看，这

里迭代24次即可获得最优

解。

4.2.3 最小二乘法的梯度下

降算法以及Python实现

从前面

的介绍可以得知，任何一

个需要进行梯度下降

的

函数都可以被比作一座

山，而梯度下降的目标就

是

找到这座山的底部，也

就是函数的最小值。根据

之前

道士下山的场景，最

快的下山方式就是找到

最为陡峭

的山路，然后沿

着这条山路走下去，直到

下一个观望

点。之后在下

一个观望点重复这个过

程，寻找最为陡

峭的山路

，直到山脚。

下面带领读者

实现这个过程，求解最小

二乘法的最小

值，但是在

开始之前，先介绍读者需

要掌握的数学原

理。

1. 微分

高等数学中对函数微分

的解释有很多，主要的有

两

种：

·函数曲线上某点切

线的斜率。

·函数的变化率

。

因此，对于一个二元微分

的计算如下：

2. 梯度

所谓的

梯度，就是微分的一般形

式，多元微分则是各

个变

量的变化率的总和，例子

如下：

可以看到，求解的梯

度值是分别对每个变量

进行微分

计算，之后用逗

号隔开。这里用中括号“[]”将

每个

变量的微分值包裹

在一起形成一个三维向

量，因此可

以认为微分计

算后的梯度是一个向量

。

综上所述，得出梯度的定

义：在多元函数中，梯度是

一个向量，而向量具有方

向性，梯度的方向指出了

函

数在给定点上上升最

快的方向。

这个与前面道

士下山的过程联系在一

起，如果需要到

达山地，则

需要在每一个观察点寻

找梯度最陡峭的地

方。梯

度计算的值是在当前点

上升最快的方向，反方

向

则是给定点下降最快的

方向。梯度计算就是得出

这

个值的具体向量值，如

图4-9所示。

图4-9 梯度计算

3. 梯度

下降的数学计算

前面已

经给出了梯度下降的公

式，此时对其进行变

形：

此

公式中的参数的含义如

下：

J是关于参数θ的函数，假

设当前点为θ，如果需要找

到这个函数的最小值，也

就是山底的话，那么首先

需

要确定行进的方向，也

就是梯度计算的反方向

，之后

走α的步长，走完这个

步长之后，就到了下一个

观察

点。

α的意义在4.2.2节已经

介绍，是学习率或者步长

，使

用α来控制每一步走的

距离。α过小会造成拟合时

间

过长，而α过大会造成下

降幅度太大错过最低点

。如

图4-10所示为学习率太小

（左）与学习率太大（右）

的对

比。

图4-10 学习率太小与学习

率太大的对比

这里还要

注意的是，梯度下降公式

中的∇J(θ)求出的

是斜率最大

值，也就是梯度上升最大

的方向，而这里

所需要的

是梯度下降最大的方向

，因此在∇J(θ)前加

一个负号。下

面用一个例子演示梯度

下降算法的计

算。

假设这

里的公式为：

2

J(θ)=θ

此时的微分

公式为：

∇J(θ)=2θ

0

设第一个值θ=1，α=0.3，则根

据梯度下降公式：

100

θ=θ−α×2θ=1−α×2×1=1−0.6=0.4

211

θ=θ−α×2θ=0.4−α×2×0.4=0.4−0.24=0.16

322

θ=θ−α×2θ=0.16−α×2×0.16=0.16−0.096=0.06

4

这样依

次经过运算，即可到J(θ)的最

小值，也就是

“山底”

，如图4-11所

示。

图4-11 J(θ)的最小值

实现程序

如下：

import

numpy as np

x = 1

def chain(x,gama = 0.1):

x =

x - gama * 2 *

x

return x

for _ in

range(4):

x = chain(x)

print(x)

多变量的梯度下降

算法和前文所述的多元

微分求导类

似。例如，一个

二元函数的形式如下：

此

时对其的梯度微分为：

∇J(θ)=2θ+2θ

12

此

时将设置：

0

J(θ)=(2,5)，α=0.3

则依次计算的

结果如下：

剩下的计算请

读者自行完成。

如果把二

元函数采用图像的方式

展示出来，可以很明

显地

看到梯度下降的每个“观

察点”坐标，如图4-12

所示。

图4-12 梯

度下降的每个“观察点”坐

标

4. 使用梯度下降算法求

解最小二乘法

下面是本

节的实战部分，使用梯度

下降算法计算最小

二乘

法。假设最小二乘法的公

式如下：

其中参数解释如

下：

·m是数据点总数。

· 是一个

常量，这样在求梯度的时

候，二次方微分

后的结果

就与 抵消了，自然就没有

多余的常数系

数，方便了

后续的计算，同时对结果

不会有影响。

·y是数据集中

每个点的真实y坐标的值

。

其中h(x)为预测函数，形式如

下：

θ

h(x)=θ+θx

θ01

根据每个输入x，都有一

个经过参数计算后的预

测值输

出。

h(x)的Python实现如下：

θ

h_pred =

np.dot(x,theta)

其

中x是输入的、维度为[-1,2]的二

维向量，-1的意思

是维度不

定。这里使用了一个技巧

，即将h(x)的公式

转换成矩阵

相乘的形式，而theta是一个[2,1]维

度的

二维向量。

θ

依照最小

二乘法实现的Python代码如下

：

def error_function(theta,x,y):

h_pred = np.dot(x,theta)

j_theta = (1./2*m) * np.dot(np.transpose(h_pred),

h_pred)

return j_theta

这里j_theta的实现同样是将原

始公式转换成矩阵计

算

，即：

2T

(h(x)−y)=(h(x)−y)×(h(x)−y)

θθθ

下面分析一下最小二

乘法公式J(θ)，此时如果求

J(θ)的

梯度，则需要对其中涉及

的两个参数θ和θ进

行微分

：

01

下面分别对两个参数的

求导公式进行求导：

将分

开求导的参数合并，可得

新的公式如下：

公式最右

边的常数1可以被去掉，公

式变为：

使用矩阵相乘表

示的公式为：

T

这里(x)×(h(x)−y)已经转

换为矩阵相乘的表示形

式。

使用Python表示如下：

θ

def gradient_function(theta, X, y):

h_pred

= np.dot(X, theta) - y

return

(1./m) * np.dot(np.transpose(X), h_pred)

如果读

者对np.dot(np.transpose(X), h_pred)理解

有难度，可以将

公式使用逐个x值的形式

列出来，这里

就不罗列了

。

最后是梯度下降的Python实现

，代码如下：

def gradient_descent(X, y,

alpha):

theta = np.array([1, 1]).reshape(2, 1)#[1,1]是

theta的初始化参

数，后面会修改

gradient = gradient_function(theta,X,y)

for i

in range(17):

theta = theta -

alpha * gradient

gradient = gradient_function(theta,

X, y)

return theta

或者使用

如下代码：

def

gradient_descent(X, y, alpha):

theta = np.array([1,

1]).reshape(2, 1) #[1,1]是

theta的初始化参

数，后面会修改

gradient =

gradient_function(theta,X,y)

while not np.all(np.absolute(gradient) <= 1e-

4): #采用abs是因

为gradient

计算的是负梯度

theta = theta

- alpha * gradient

gradient =

gradient_function(theta, X, y)

print(theta)

return theta

这两

个代码段的区别在于：第

一个代码段是固定循环

次数，可能会造成欠下降

或者过下降；而第二个代

码

段使用的是数值判定

，可以设定阈值或者停止

条件。

全部代码如下：

import numpy as

np

m = 20

# 生成

数据集x，此时的数据集x是

一个二维矩阵

x0 = np.ones((m, 1))

x1 =

np.arange(1, m+1).reshape(m, 1)

x = np.hstack((x0,

x1)) #[20,2]

y = np.array([

3,

4, 5, 5, 2, 4, 7,

8, 11, 8, 12,

11, 13,

13, 16, 17, 18, 17, 19,

21

]).reshape(m, 1)

alpha = 0.01

#这里的theta是

一个[2,1]大小的矩阵，用来与

输入x进行计

算，以获得计

算的预测值y_pred，而

y_pred用于与y计

算误差

def error_function(theta,x,y):

h_pred

= np.dot(x,theta)

j_theta = (1./2*m) *

np.dot(np.transpose(h_pred),

h_pred)

return j_theta

def gradient_function(theta,

X, y):

h_pred = np.dot(X, theta)

- y

return (1./m) * np.dot(np.transpose(X),

h_pred)

def gradient_descent(X, y, alpha):

theta

= np.array([1, 1]).reshape(2, 1) #

[2,1]

这里的theta是参数

gradient = gradient_function(theta,X,y)

while not

np.all(np.absolute(gradient) <= 1e-6):

theta = theta

- alpha * gradient

gradient =

gradient_function(theta, X, y)

return theta

theta

= gradient_descent(x, y, alpha)

print('optimal:', theta)

print('error function:', error_function(theta, x, y)

[0,0])

打

印结果和拟合曲线请读

者自行完成。

现在回到前

面的道士下山这个问题

，这个下山的道士

实际上

就代表了反向传播算法

，而要寻找的下山路径

其

实就代表着算法中一直

在寻找的参数θ，山上当前

点的最陡峭的方向实际

上就是代价函数在这一

点的梯

度方向，在场景中

观察最陡峭的方向所用

的工具就是

微分。

4.3 反馈神

经网络反向传播算法介

绍

反向传播算法是神经

网络的核心与精髓，在神

经网络

算法中有着举足

轻重的地位。

用通俗的话

说，所谓的反向传播算法

，就是复合函数

的链式求

导法则的一个强大应用

，而且实际上的应用

比起

理论上的推导强大得多

。本节将主要介绍反向传

播算法的一个简单模型

的推导，虽然模型简单，但

是

这个简单的模型是其

应用广泛的基础。

4.3.1 深度学

习基础

机器学习在理论

上可以看作是统计学在

计算机科学上

的一个应

用。在统计学上，一个非常

重要的内容就是

拟合和

预测，即基于以往的数据

，建立光滑的曲线模

型实

现数据结果与数据变量

的对应关系。

深度学习为

统计学的应用，同样是为

了这个目的，寻

找结果与

影响因素的一一对应关

系，只不过样本点由

狭义

的x和y扩展到向量、矩阵等

广义的对应点。此

时，由于

数据的复杂性，因此对应

关系模型的复杂度

也随

之增加，而不能使用一个

简单的函数表达。

数学上

通过建立复杂的高次多

元函数解决复杂模型拟

合的问题，但是大多数都

失败，因为过于复杂的函

数

式是无法进行求解的

，也就是其公式的获取是

不可能

的。

基于前人的研

究，科研工作人员发现可

以通过神经网

络来表示

一一对应关系，而神经网

络本质就是一个多

元复

合函数，通过增加神经网

络的层次和神经单元可

以更好地表达函数的复

合关系。

图4-13是多层神经网

络的图像表达方式，通过

设置输

入层、隐藏层与输

出层可以形成一个多元

函数，用于

求解相关问题

。

图4-13 多层神经网络

通过数

学表达式将多层神经网

络模型表达出来，公式

如

下：

其中x是输入数值，而w是

相邻神经元之间的权重

，也

就是神经网络在训练

过程中需要学习的参数

。与线性

回归类似的是，神

经网络学习同样需要一

个损失函

数，训练目标通

过调整每个权重值w来使

得损失函数最

小。前面在

讲解梯度下降算法的时

候已经讲过，如果

权重过

大或者指数过大，那么直

接求解系数是一件不

可

能的事情，因此梯度下降

算法是求解权重问题的

比

较好的方法。

4.3.2 链式求导

法则

在前面梯度下降算

法的介绍中，没有对其背

后的原理

做出更为详细

的介绍。实际上，梯度下降

算法就是链

式法则的一

个具体应用，如果把前面

公式中的损失函

数以向

量的形式表示为：

那么其

梯度向量为：

可以看到，其

实所谓的梯度向量就是

求出函数在每个

向量上

的偏导数之和。这也是链

式法则擅长处理的问

题

。

下面以e=(a+b)×(b+1)，其中a = 2，b =

1为

例，计算其

偏导数，如图4-14所示。

图4-14 计算

偏导数

本例中为了求得

最终值e对各个点的梯度

，需要将各个

点与e联系在

一起，例如期望求得e对输

入点a的梯度，

则只需求得

：

这样就把e与a的梯度联系

在一起了，同理可得：

用图

表示如图4-15所示。

图4-15 链式法

则的应用

这样做的好处

是显而易见的，求e对a的偏

导数只要建

立一个e到a的

路径，图中经过c，那么通过

相关的求导

链接就可以

得到所需要的值。对于求

e对b的偏导数，

也只需要建

立所有e到b的路径中的求

导路径，从而获

得需要的

值。

4.3.3 反馈神经网络的原理

与公式推导

在求导过程

中，可能有读者已经注意

到，如果拉长了

求导过程

或者增加了其中的单元

，就会大大增加其中

的计

算过程，即很多偏导数的

求导过程会被反复计

算

，因此在实际应用中对于

权值达到十万或者百万

以

上的神经网络来说，这

样的重复冗余所导致的

计算量

是很大的。

同样是

为了求得对权重的更新

，反馈神经网络算法将

训

练误差E看作以权重向量

每个元素为变量的高维

函

数，通过不断更新权重

寻找训练误差的最低点

，按误

差函数梯度下降的

方向更新权值。

提示：反馈

神经网络算法的具体计

算公式在本小节后

半部

分进行推导。

首先求得最

后的输出层与真实值之

间的差距，如图4-

16所示。

图4-16 反

馈神经网络最终误差的

计算

之后以计算出的测

量值与真实值为起点，反

向传播到

上一个节点，并

计算出节点的误差值，如

图4-17所

示。

图4-17 反馈神经网络

输出层误差的反向传播

以后将计算出的节点误

差重新设置为起点，依次

向后

传播误差，如图4-18所示

。

图4-18 反馈神经网络隐藏层

误差的反向传播

图4-18 反馈

神经网络隐藏层误差的

反向传播（续）

注意：对于隐

藏层，误差并不是像输出

层一样由单个

节点确定

的，而是由多个节点确定

的，因此对它的计

算要求

得所有的误差值之和。

通

俗地解释，一般情况下误

差的产生是由于输入值

与

权重的计算产生了错

误，而输入值往往是固定

不变

的，因此对于误差的

调节，需要对权重进行更

新。而

权重的更新又是以

输入值与真实值的偏差

为基础的，

当最终层的输

出误差被反向一层一层

地传递回来后，

每个节点

都会被相应地分配适合

其所处的神经网络地

位

的误差，即只需要更新其

所需承担的误差量，如图

4-19所示。

图4-19 反馈神经网络权

重的更新

即在每一层，需

要维护输出对当前层的

微分值，该微

分值相当于

被复用于之前每一层中

权值的微分计算。

因此，空

间复杂度没有变化。同时

，也没有重复计

算，每一个

微分值都在之后的迭代

中使用。

下面介绍公式的

推导。公式的推导需要使

用一些高等

数学的知识

，因此读者可以自由选择

学习。

首先进行算法的分

析，前面已经讲过，对于反

馈神经

网络算法，主要需

要知道输出值与真实值

之间的差

值。

·对于输出层

单元，误差项是真实值与

模型计算值之

间的差值

。

·对于隐藏层单元，由于缺

少直接的目标值来计算

隐

藏层单元的误差，因此

需要以间接的方式来计

算隐藏

层的误差项，对受

隐藏层单元影响的每一

个单元的误

差进行加权

求和。

·权值的更新方面，主

要依靠学习速率、该权值

对应

的输入以及单元的

误差项。

1. 前向传播算法

对

于前向传播的值传递，隐

藏层输出值定义如下：

其

中X是当前节点的输入值

，是连接到此节点的权重

，

是输出值。f是当前阶段的

激活函数，为当前节点的

输

入值经过计算后被激

活的值。

i

而对于输出层，定

义如下：

其中hkW为输入的权

重，为将节点输入数据经

过计算后

的激活值作为

输入值。这里将所有输入

值进行权重计

算后求得

的值作为神经网络的最

后输出值a。

k

2. 反向传播算法

与前向传播类似，首先需

要定义两个值：δ与：

k

其中δ为

输出层的误差项，其计算

值为真实值与模型

计算

值之间的差值。Y是计算值

，T是真实值。为输出

层的误

差。

k

提示：对于δ与来说，无论

定义在哪个位置，都可以

看作当前的输出值对于

输入值的梯度计算。

k

通过

前面的分析可以知道，所

谓的神经网络反馈算

法

，就是逐层地将最终误差

进行分解，即每一层只与

下一层打交道，如图4-20所示

。那么，据此可以假设

每一

层均为输出层的前一个

层级，通过计算前一个层

级与输出层的误差得到

权重的更新。

图4-20 权重的逐

层反向传导

因此，反馈神

经网络计算公式定义为

：

即当前层的输出值对误

差的梯度可以通过下一

层的误

差与权重和输入

值的梯度乘积获得。若公

式中的δ为

输出层，则可以

通过求得，若δ为非输出层

，则可使

用逐层反馈的方

式求得。

kk

提示：这里一定要

注意，对于δ与来说，其计算

结果

都是当前的输出值

对于输入值的梯度计算

，是权重更

新过程中一个

非常重要的数据计算内

容。

k

也可以将前面的公式

表示为：

可以看到，通过更

为泛化的公式，可以把当

前层的输

出对输入的梯

度计算转换成求下一个

层级的梯度计算

值。

3.

权重

的更新

反馈神经网络计

算的目的是对权重的更

新，因此与梯

度下降算法

类似，其更新可以仿照梯

度下降对权值的

更新公

式：

即：

其中ji表示反向传播

时对应的节点系数，通过

对

的

计算，就可以更新对

应的权重值。W的计算公式

如上所

示。而对于没有推

导的b，其推导过程与W类似

，但是

在推导过程中输入

值是被消去的，请读者自

行学习。

jijiji

4.3.4

反馈神经网络原

理的激活函数

现在回到

反馈神经网络的函数：

l

对

于此公式中的和以及需

要计算的目标δ已经做了

较

为详尽的解释。但是一

直没有对做出介绍。

回到

前面生物神经元的图示

，传递进来的电信号通过

神经元进行传递，由于神

经元的突触强弱是有一

定的

敏感度的，因此只会

对超过一定范围的信号

进行反

馈，即这个电信号

必须大于某个阈值，神经

元才会被

激活引起后续

的传递。

在训练模型中同

样需要设置神经元的阈

值，即神经元

被激活的频

率用于传递相应的信息

，模型中这种能够

确定是

否为当前神经元节点的

函数被称为激活函数，

如

图4-21所示。

图4-21 激活函数示意

图

激活函数代表了生物

神经元中接收到的信号

的强度，

目前应用范围较

广的是Sigmoid函数。因为其在运

行过

程中只接收一个值

，所以输出也是一个经过

公式计算

的值，且其输出

值的范围为0～1。

其图形如图

4-22所示。

图4-22 Sigmoid激活函数图

而其

倒函数的求法也较为简

单，即：

换一种表示方式为

：

Sigmoid输入一个实值的数，之后

将其压缩到0～1。特

别是对于

较大值的负数被映射成

0，而大的正数被映射

成1。

顺

便讲一下，Sigmoid函数在神经网

络模型中占据了很

长时

间的统治地位，但是目前

已经不常使用，主要原

因

是其非常容易区域饱和

，当输入非常大或者非常

小

的时候，Sigmoid会产生一个平

缓区域，其中的梯度值

几

乎为0，而这又会造成在梯

度传播过程中产生接近

0

的传播梯度。这样在后续

的传播过程中会造成梯

度消

散的现象，因此并不

适合现代的神经网络模

型使用。

除此之外，近年来

涌现出了大量新的激活

函数模型，

例如Maxout、Tanh和ReLU模型，这

些都是为了解决传

统的

Sigmoid模型在更深程度的神经

网络所产生的各种

不良

影响。

提示：Sigmoid函数的具体使

用和影响会在第5章详细

介绍。

4.3.5 反馈神经网络原理

的Python实现

经过前面的介绍

，读者应该对神经网络的

算法和描述

有了一定的

理解，本小节将使用Python代码

实现一个

反馈神经网络

。

为了简化起见，这里的神

经网络被设置成三层，即

只

有一个输入层、一个隐

藏层以及最终的输出层

。

（1）辅助函数的确定：

def

rand(a, b):

return (b - a)

* random.random() + a

def make_matrix(m,n,fill=0.0):

mat = []

for i in

range(m):

mat.append([fill] * n)

return mat

def sigmoid(x):

return 1.0 / (1.0

+ math.exp(-x))

def sigmod_derivate(x):

return x

* (1 - x)

代码首

先定义了随机值，使用random包

中的random函

数生成了一系列

随机数，之后的make_matrix函数生成

了相对应的矩阵。sigmoid和sigmod_derivate分别

是

激活函数和激活函数

的导函数。这也是前文所

定义的

内容。

（2）进入BP神经网

络类的正式定义，类的定

义需要对

数据进行内容

的设定。

def __init__(self):

self.input_n = 0

self.hidden_n = 0

self.output_n = 0

self.input_cells = []

self.hidden_cells = []

self.output_cells = []

self.input_weights = []

self.output_weights = []

init函数是数据内容

的初始化，即在其中设置

了输入

层、隐藏层以及输

出层中节点的个数；各个

cell数据

是各个层中节点的

数值；weights数据代表各个层的

权

重。

（3）使用setup函数对init函数中

设定的数据进行初

始化

。

def setup(self,ni,nh,no):

self.input_n = ni +

1

self.hidden_n = nh

self.output_n =

no

self.input_cells = [1.0] * self.input_n

self.hidden_cells = [1.0] * self.hidden_n

self.output_cells

= [1.0] * self.output_n

self.input_weights =

make_matrix(self.input

_n,self.hidden_n)

self.output_weights = make_matrix(self.hidd

en_n,self.output_n)

# random activate

for i in

range(self.input_n):

for h in range(self.hidden_n):

self.input_weights[i]

[h] = rand(-0.2, 0.2)

for h

in range(self.hidden_n):

for o in range(self.output_n):

self.output_weights[h]

[o] = rand(-2.0, 2.0)

需要注意，输入层节点的

个数被设置成ni+1，这是由

于

其中包含bias偏置数；各个节

点与1.0相乘是初始化

节点

的数值；各个层的权重值

根据输入层、隐藏层以

及

输出层中节点的个数被

初始化并被赋值。

（4）定义完

各个层的数目后，下面进

入正式的神经网

络内容

的定义。首先是对神经网

络前向的计算。

def

predict(self,inputs):

for i in range(self.input_n -

1):

self.input_cells[i] = inputs[i]

for j

in range(self.hidden_n):

total = 0.0

for

i in range(self.input_n):

total += self.input_cells[i]

* se

lf.input_weights[i][j]

self.hidden_cells[j] = sigmoid(total)

for k in range(self.output_n):

total =

0.0

for j in range(self.hidden_n):

total

+= self.hidden_cells[j] * s

elf.output_weights[j][k]

self.output_cells[k]

= sigmoid(total)

return self.output_cells[:]

代码段中

将数据输入函数中，通过

隐藏层和输出层的

计算

，最终以数组的形式输出

。案例的完整代码如

下。

【程

序4-3】

import numpy as np

import math

import random

def rand(a,

b):

return (b - a) *

random.random() + a

def make_matrix(m,n,fill=0.0):

mat

= []

for i in range(m):

mat.append([fill] * n)

return mat

def

sigmoid(x):

return 1.0 / (1.0 +

math.exp(-x))

def sigmod_derivate(x):

return x *

(1 - x)

class BPNeuralNetwork:

def

__init__(self):

self.input_n = 0

self.hidden_n =

0

self.output_n = 0

self.input_cells =

[]

self.hidden_cells = []

self.output_cells =

[]

self.input_weights = []

self.output_weights =

[]

def setup(self,ni,nh,no):

self.input_n = ni

+ 1

self.hidden_n = nh

self.output_n

= no

self.input_cells = [1.0] *

self.input_n

self.hidden_cells = [1.0] * self.hidden_n

self.output_cells = [1.0] * self.output_n

self.input_weights

= make_matrix(self.input

_n,self.hidden_n)

self.output_weights = make_matrix(self.hidd

en_n,self.output_n)

# random activate

for i

in range(self.input_n):

for h in range(self.hidden_n):

self.input_weights[i]

[h] = rand(-0.2, 0.2)

for

h in range(self.hidden_n):

for o in

range(self.output_n):

self.output_weights[h]

[o] = rand(-2.0, 2.0)

def predict(self,inputs):

for i in range(self.input_n

- 1):

self.input_cells[i] = inputs[i]

for

j in range(self.hidden_n):

total = 0.0

for i in range(self.input_n):

total +=

self.input_cells[i

] * self.input_weights[i][j]

self.hidden_cells[j] =

sigmoid(tota

l)

for k in range(self.output_n):

total = 0.0

for j in

range(self.hidden_n):

total += self.hidden_cells[

j] *

self.output_weights[j][k]

self.output_cells[k] = sigmoid(tota

l)

return

self.output_cells[:]

def back_propagate(self,case,label,learn):

self.predict(case)

#计算输出层的误差

output_deltas

= [0.0] * self.output_n

for k

in range(self.output_n):

error = label[k] -

self.output_ce

lls[k]

output_deltas[k] = sigmod_derivate(

self.output_cells[k])

* error

#计

算隐藏层的误差

hidden_deltas = [0.0]

* self.hidden_n

for j in range(self.hidden_n):

error = 0.0

for k in

range(self.output_n):

error += output_deltas[k]

* self.output_weights[j][k]

hidden_deltas[j] = sigmod_derivate(

self.hidden_cells[j]) * error

#更新输

出层的权重

for j in range(self.hidden_n):

for

k in range(self.output_n):

self.output_weights[j]

[k] +=

learn * output_deltas[k] *

self.hidden_cells[j]

#更新隐藏层

的权重

for i in range(self.input_n):

for j

in range(self.hidden_n):

self.input_weights[i]

[j] += learn

* hidden_deltas[j] *

self.input_cells[i]

error =

0

for o in range(len(label)):

error

+= 0.5 * (label[o] - self.o

utput_cells[o]) ** 2

return error

def

train(self,cases,labels,limit = 100,learn = 0.

05):

for i in range(limit):

error =

0

for i in range(len(cases)):

label

= labels[i]

case = cases[i]

error

+= self.back_propagate(

case, label, learn)

pass

def test(self):

cases = [

[0,

0],

[0, 1],

[1, 0],

[1,

1],

]

labels = [[0], [1],

[1], [0]]

self.setup(2, 5, 1)

self.train(cases,

labels, 10000, 0.05)

for case in

cases:

print(self.predict(case))

if __name__ == '__main__':

nn = BPNeuralNetwork()

nn.test()

4.4 本章小结

本章是

深度学习最为基础的内

容，向读者完整地介绍

了

深度学习的基础起始知

识——BP神经网络的原理和

实

现，可以说深度学习所有

的后续发展都是基于对

BP

神经网络的修正而来的

。

在后续章节中，作者会带

领读者了解更多的神经

网

络。

第5章

基于PyTorch卷积层的

MNIST分类实战

第3章使用多层

感知机完成了MNIST分类实战

的演示。

多层感知机是一

种基于对目标数据整体

分类的计算方

法，虽然从

演示效果来看，多层感知

机可以较好地完

成项目

目标，对数据进行完整的

分类。但是使用多层

感知

机需要在模型中使用大

规模的参数，同时由于多

层感知机是对数据进行

总体性的处理，从而无可

避免

地会忽略数据局部

特征的处理和掌握，因此

，我们需

要一种新的能够

对输入数据的局部特征

进行抽取和计

算的工具

，如图5-1所示。

图5-1 对输入数据

的局部特征进行抽取和

计算

卷积神经网络是从

信号处理衍生过来的一

种对数字信

号处理的方

式，发展到图像信号处理

上演变成一种专

门用来

处理具有矩阵特征的网

络结构处理方式。卷积

神

经网络在很多应用上都

有独特的优势，甚至可以

说

是无可比拟的优势，例

如音频的处理和图像的

处理。

本章将首先介绍什

么是卷积神经网络，卷积

实际上是

一种不太复杂

的数学运算，即一种特殊

的线性运算形

式。然后会

介绍“池化”这一概念，这是

卷积神经网

络中必不可

少的操作。另外，为了消除

过拟合，还会

介绍drop-out这一常

用的方法。这些是让卷积

神经网

络运行得更加高

效的常用方法。

5.1

卷积运算

的基本概念

在数字图像

处理中有一种基本的处

理方法——线性滤

波。它将待

处理的二维数字看作一

个大型矩阵，图像

中的每

个像素可以看作矩阵中

的每个元素，像素的大

小

就是矩阵中的元素值。

而

使用的滤波工具是另一

个小型矩阵，这个矩阵被

称

为卷积核。卷积核的大

小远小于图像矩阵，而具

体的

计算方式就是对于

图像大矩阵中的每个像

素，计算其

周围的像素和

卷积核对应位置的乘积

，之后将结果相

加，最终得

到的值就是该像素的值

，这样就完成了一

次卷积

。最简单的图像卷积方式

如图5-2所示。

图5-2

卷积运算

本

节将详细介绍卷积的运

算和定义，以及一些细节

调

整，这些都是卷积使用

中必不可少的内容。

5.1.1 基本

卷积运算示例

前面已经

讲过了，卷积实际上是使

用两个大小不同的

矩阵

进行的一种数学运算。为

了便于读者理解，我们

从

一个例子开始介绍。

对高

速公路上的跑车位置进

行追踪，这是卷积神经网

络图像处理的一个非常

重要的应用。摄像头接收

到的

信号被计算为x(t)，表示

跑车在路上时刻t的位置

。

但是实际的处理没那么

简单，因为在自然界无时

无刻

不面临各种影响和

摄像头传感器的滞后。因

此，为了

得到跑车位置的

实时数据，采用的方法就

是对测量结

果进行均值

化处理。对于运动中的目

标，采样时间越

长，由于滞

后性的原因，定位的准确

率越低，而采样

时间越短

，越接近真实值。因此，可以

对不同的时间

段赋予不

同的权重，即通过一个权

值定义来计算，表

示为：

这

种运算方式被称为卷积

运算。换个符号表示为：

s(t)=(x*ω)(t)

在

卷积公式中，第一个参数

x被称为输入数据，而第二

个参数ω被称为核函数，s(t)是

输出，即特征映射。

对于稀

疏矩阵来说，卷积网络具

有稀疏性，即卷积核

的大

小远远小于输入数据矩

阵的大小。例如，当输入

一

个图片信息时，数据的大

小可能为上万的结构，但

是使用的卷积核只有几

十，这样能够在计算后获

取更

少的参数特征，极大

地减少后续的计算量。稀

疏矩阵

如图5-3所示。

图5-3 稀疏

矩阵

在传统的神经网络

中，每个权重只对其连接

的输入输

出起作用，当其

连接的输入输出元素结

束后就不会再

用到。而参

数共享指的是在卷积神

经网络中，核的每

一个元

素都被用在输入的每一

个位置上，在这个过程

中

只需学习一个参数集合

，就能把这个参数应用到

所

有的图片元素中。卷积

计算示例代码如下：

import struct

import matplotlib.pyplot as plt

import numpy

as np

dateMat = np.ones((7,7))

kernel

= np.array([[2,1,1],[3,0,1],[1,1,0]])

def convolve(dateMat,kernel):

m,n =

dateMat.shape

km,kn = kernel.shape

newMat =

np.ones(((m - km + 1),

(n

- kn + 1)))

tempMat =

np.ones(((km),(kn)))

for row in range(m -

km + 1):

for col in

range(n - kn + 1):

for

m_k in range(km):

for n_k in

range(kn):

tempMat[m_k,n_k] =

dateMat[(row + m_k),(col

+ n_k)] *

kernel[m_k,n_k]

newMat[row,col] =

np.sum(tempMat)

return newMat

上面

代码使用Python基础运算包实

现了卷积操作，这

里卷积

核从左到右、从上到下进

行卷积计算，最后返

回新

的矩阵。

5.1.2 PyTorch 2.0中卷积函数实现

详解

前面通过Python实现了卷

积计算，PyTorch为了框架计

算的

迅捷，同样使用了专门的

高级API函数

Conv2d(Conv)作为卷积计算

函数，如图5-4所示。

图5-4 使用Conv2d(Conv)作

为卷积计算函数

Conv2d(Conv)函数是

搭建卷积神经网络最为

核心的函

数之一，其说明

如下：

class Conv2d(_ConvNd):

…

def __init__(

self, in_channels: int,

out_channels: int

, kernel_size: _size_2_t,

stride:

_size_2_t = 1, padding: Union[st

r,

_size_2_t] = 0,

dilation: _size_2_t =

1,groups: int = 1,

bias: bool

= True,

padding_mode: str = 'zeros',

# TODO:

refine this type

device=None,

dtype=None

) -> None:

Conv2d是PyTorch的卷积层自带的

函数，最重要的5个

参数如

下：

·in_channels：输入的卷积核数目。

·out_channels：输

出的卷积核数目。

·kernel_size：卷积核

大小，它要求是一个输入

向

量，具有[filter_height, filter_width]这样的维

度，具

体含义是[卷积核的高度

，卷积核的宽度]，要

求类型

与参数input相同。

·strides：步进大小，卷

积时在图像每一维的步

长，

这是一个一维的向量

，第一维和第四维默认为

1，而第

三维和第四维分别

是平行和竖直滑行的步

进长度。

·padding：填充方式，int类型的

量，只能是1或0，

这个值决定

了不同的卷积方式。

一个

使用卷积计算的示例如

下：

import torch

image = torch.randn(size=(5,3,128,128))

#下面是定义的卷积层

示例

"""

输入维度：3

输出维度

：10

卷积核大小：基本写法是

[3,3]，这里简略写法3代表卷积

核的长和宽

大小一致

步

长：2

补偿方式：维度不变补

偿

"""

conv2d = torch.nn.Conv2d(3,10,kernel_size=3,stride=1,paddin

g=1)

image_new = conv2d(image)

print(image_new.shape)

上面的代码段展示了

一个使用TensorFlow高级API进行

卷积

计算的例子，在这里随机

生成了5个[3,128,128]

大小的矩阵，之

后使用1个大小为[3,3]的卷积

核对其

进行计算，打印结

果如下：

torch.Size([5, 10, 128,

128])

可以看到，这是计

算后生成的新图形，其大

小根据设

置没有变化，这

是由于我们所使用的padding补

偿方式

将其按原有大小

进行补偿。具体来说，这是

由于卷积

在工作时，边缘

被处理消失，因此生成的

结果小于原

有的图像。

但

是有时需要生成的卷积

结果和原输入矩阵的大

小一

致，因此需要将参数

padding的值设为1，此时表示图

像

边缘将由一圈0补齐，使得

卷积后的图像大小和输

入

大小一致，示意如图5-5所

示。

图5-5 示意图

从图中可以

看到，这里x是图片的矩阵

信息，外面一圈

是补齐的

0，0在卷积处理时对最终结

果没有任何影

响。这里略

微对其进行修改，更多的

参数调整请读者

自行调

试。

下面我们修改一下卷

积核stride，也就是步进的大

小

，代码如下：

import torch

image = torch.randn(size=(5,3,128,128))

conv2d

= torch.nn.Conv2d(3,10,kernel_size=3,stride=2,paddin

g=1)

image_new = conv2d(image)

print(image_new.shape)

我们使用同样

大小的输入数据修正了

卷积层的步进距

离，最终

结果如下：

torch.Size([5, 10, 64,

64])

下面对这个情

况进行总结，经过卷积计

算后，图像的

大小变化可

以由如下公式进行确定

：

N=(W-F+2P)//S+1

·输入图片大小为W×W。

·Filter大小为

F×F。

·步长为S。

·padding的像素数为P，一般

情况下P=1或0（参考

PyTorch）。

把上述数

据代入公式可得（注意取

模计算）：

N=(128-3+2)//2+1

需要注意的是，在

这里是取模计算，因此

127//2 = 63。

5.1.3 池

化运算

在通过卷积获得

了特征（Feature）之后，下一步利用

这些特征进行分类。理论

上讲，人们可以用所有提

取

到的特征来训练分类

器，例如推导Softmax推导分类

器

，但这样做会面临计算量

的挑战。因此，为了降低

计

算量，我们尝试利用神经

网络的“参数共享”这一

特

性。

这也就意味着在一个

图像区域有用的特征极

有可能在

另一个区域同

样适用。因此，为了描述大

的图像，一

个很自然的想

法就是对不同位置的特

征进行聚合统

计，例如特

征提取可以计算图像一

个区域上的某个特

定特

征的平均值（或最大值）。这

些概要统计特征不

仅具

有低得多的维度（相比使

用所有提取得到的特

征

），还会改善结果（不容易过

拟合）。这种聚合的

操作就

叫作池化（Pooling），有时也称为平

均池化或

者最大池化（取

决于计算池化的方法）。

例

如，特征提取可以计算图

像一个区域上的某个特

定

特征的平均值（或最大

值），如图5-6所示。

图5-6 max-pooling后的图片

如果选择图像中的连续

范围作为池化区域，并且

只是

池化相同（重复）的隐

藏单元产生的特征，那么

这些

池化单元就具有平

移不变性

（Translation Invariant）。这就意味着即

使图像

经历了一个小的

平移，依然会产生相同的

（池化的）

特征。在很多任务

（例如物体检测、声音识别

）中，

我们都更希望得到具

有平移不变性的特征，因

为即使

图像经过了平移

，样例（图像）的标记仍然保

持不

变。

在PyTorch 2.0中，池化运算的

函数如下：

class AvgPool2d(_AvgPoolNd):

…

def __init__(self, kernel_size: _size_2_t, stride

: Optional[_size_2_t] =

None, padding: _size_2_t

= 0, ceil_mode: bool = False

, count_include_pad: bool =

True, divisor_override:

Optional[int] = None) -> None:

重要的参数如

下：

·kernel_size：池化窗口的大小，默认

大小一般是

[2, 2]。

·strides：和卷积类似

，窗口在每一个维度上滑

动的

步长，默认大小一般

是[2,2]。

·padding：和卷积类似，可以取1或

0，返回一个

Tensor，类型不变，shape仍然

是

[batch,channel,height, width]这种形式。

池化的一个

非常重要的作用是能够

帮助输入的数据表

示近

似不变性。平移不变性指

的是对输入的数据进行

少量平移时，经过池化后

的输出结果并不会发生

改

变。局部平移不变性是

一个很有用的性质，尤其

是当

关心某个特征是否

出现而不关心它出现的

具体位置

时。

例如，当判定

一幅图像中是否包含人

脸时，并不需要

判定眼睛

的位置，只需要知道有一

只眼睛出现在脸部

的左

侧，另一只眼睛出现在脸

部的右侧就可以了。使

用

池化层的代码如下：

import torch

image = torch.randn(size=(5,3,28,28))

pool

= torch.nn.AvgPool2d(kernel_size=3,stride=2,padding=0)

image_pooled = pool(image)

print(image_pooled.shape)

除此

之外，PyTorch 2.0中还提供了一种新

的池化层

——全局池化层，使

用方法如下：

import torch

image

= torch.randn(size=(5,3,28,28))

image_pooled = torch.nn.AdaptiveAvgPool2d(1)(image)

print(image_pooled.shape)

这个函数的

作用是对输入的图形进

行全局池化，也就

是在每

个channel上对图形整体进行归

一化的池化计

算，结果请

读者自行打印验证。

5.1.4 Softmax激活

函数

Softmax函数在前面已经介

绍过了，并且笔者使用

NumPy自

定义实现了Softmax模型的功能

和函数。

Softmax是一个对概率进

行计算的模型，因为在真

实的

计算模型系统中，对

一个实物的判定并不是

100%的，

而是只有一定的概率

，并且在所有的结果标签

上都可

以求出一个概率

。

其中第一个公式是人为

定义的训练模型，这里采

用输

入数据与权重的乘

积并加上一个偏置b的方

式。偏置b

存在的意义是加

上一定的噪声。

对于求出

的 ，Softmax的作用是将其转换成

概率。换

句话说，这里的Softmax可

以被看作一个激励函数

，将

计算的模型输出转换

为在一定范围内的数值

，并且在

总体中这些数值

的和为1，而每个单独的数

据结果都有

其特定的概

率分布。

用更为正式的语

言表述，就是Softmax是模型函数

定义

的一种形式：把输入

值当成幂指数求值，再正

则化这

些结果值。而这个

幂运算表示更大的概率

计算结果对

应更大的假

设模型里面的乘数权重

值。反之，拥有更

少的概率

计算结果意味着在假设

模型里面拥有更小的

乘

数权重值。

假设模型中的

权值不可以是0或者负值

。softmax会正

则化这些权重值，使

它们的总和等于1，以此构

造一个

有效的概率分布

。

对于最终的公式s来说，可

以将其认为是如图5-7所示

的形式。

图5-7 Softmax的计算形式

图

5-5演示了Softmax的计算公式，这实

际上就是对输

入的数据

与权重的乘积进行Softmax计算

得到的结果。

将其用数学

方法表示如图5-8所示。

图5-8

Softmax矩

阵表示

将这个计算过程

用矩阵的形式表示出来

，即矩阵乘法

和向量加法

，这样有利于使用TensorFlow内置的

数学

公式进行计算，可以

极大地提高程序的效率

。

5.1.5 卷积神经网络的原理

前

面介绍了卷积运算的基

本原理和概念，从本质上

来

说，卷积神经网络就是

将图像处理中的二维离

散卷积

运算和神经网络

相结合。这种卷积运算可

以用于自动

提取特征，而

卷积神经网络主要应用

于二维图像的识

别。下面

采用图示的方法更加直

观地介绍卷积神经网

络

的工作原理。

一个卷积神

经网络一般包含一个输

入层、一个卷积层

和一个

输出层，但是在真正使用

的时候会使用多层卷

积

神经网络不断地提取特

征，特征越抽象，越有利于

识别（分类）。通常卷积神经

网络包含池化层、全连

接

层，最后接输出层。

图5-9展示

了一幅图片进行卷积神

经网络处理的过程。

其中

主要包含4个步骤：

图5-9 卷积

神经网络处理图像的步

骤

（1）图像输入层：获取输入

的数据图像。

（2）卷积层：对图

像特征进行提取。

（3）池化Pooling层

：用于缩小在卷积时获取

的图像

特征。

（4）全连接层：用

于对图像进行分类。

这几

个步骤依次进行，分别具

有不同的作用。经过卷

积

层的图像被卷积核提取

后获得分块的、同样大小

的

图片，如图5-10所示。

图5-10 卷积

处理的分解图像

可以看

到，经过卷积处理后的图

像被分为若干幅大小

相

同的、只具有局部特征的

图片。图5-11表示对分解

后的

图片使用一个小型神经

网络进行进一步的处理

，

即将二维矩阵转换成一

维数组。

图5-11 分解后图像的

处理

需要说明的是，在这

个步骤，也就是对图片进

行卷积

化处理时，卷积算

法对所有分解后的局部

特征进行同

样的计算，这

个步骤称为权值共享。这

样做的依据如

下：

·对图像

等数组数据来说，局部数

组的值经常是高度

相关

的，可以形成容易被探测

到的独特的局部特征。

·图

像和其他信号的局部统

计特征与其位置是不太

相

关的，如果特征图能在

图片的一个部分出现，那

么也

能出现在其他地方

。所以不同位置的单元共

享同样的

权重，并在数组

的不同部分探测相同的

模式。

在数学上，这种由一

个特征图执行的过滤操

作是一个

离散的卷积，卷

积神经网络由此得名。

池

化层的作用是对获取的

图像特征进行缩减，从前

面

的例子中可以看到，使

用[2,2]大小的矩阵来处理特

征

矩阵，使得原有的特征

矩阵可以缩减到1/4大小，特

征

提取的池化效应如图

5-12所示。

图5-12 池化处理后的图

像

经过池化处理后的矩

阵作为下一层神经网络

的输入，

使用一个全连接

层对输入的数据进行分

类计算（见图

5-13），从而计算出

这个图像对应位置最大

的概率类

别。

图5-13 全连接层

判断

采用较为通俗的语

言概括，卷积神经网络是

一个层级

递增的结构，也

可以将其认为是一个人

在读报纸，首

先一字一句

地读取，之后整段地理解

，最后获得全文

的表述。卷

积神经网络也是从边缘

、结构和位置等一

起感知

物体的形状。

5.2

实战：基于卷

积的MNIST手写体分类

前面实

现了基于多层感知机的

MNIST手写体识别，本章

将实现

以卷积神经网络完成的

MNIST手写体识别。

5.2.1 数据准备

在

本例中，依旧使用MNIST数据集

，对这个数据集的数

据和

标签介绍，前面的章节已

详细说明过了，相对于

前

面章节直接对数据进行

“折叠”处理，这里需要显

式

地标注出数据的通道，代

码如下：

import numpy as

np

import einops.layers.torch as elt

#载入数据

x_train = np.load("../dataset/mnist/x_train.npy")

y_train_label = np.load("../dataset/mnist/y_train_label.npy

")

x_train = np.expand_dims(x_train,axis=1)

#在指定

维度上进行扩充

print(x_train.shape)

这里是

对数据的修正，np.expand_dims的作用是

在指

定维度上进行扩充

，这里在第二维（也就是PyTorch的

通道维度）进行扩充，结果

如下：

5.2.2 模型设计

下面使用

PyTorch 2.0框架对模型进行设计，在

本例

中将使用卷积层对

数据进行处理，完整的模

型如下：

import torch

import torch.nn as

nn

import numpy as np

import

einops.layers.torch as elt

class MnistNetword(nn.Module):

def

__init__(self):

super(MnistNetword, self).__init__()

#前置的特征提取

模块

self.convs_stack =

nn.Sequential(

nn.Conv2d(1,12,kernel_size=7), #第

一个卷积层

nn.ReLU(),

nn.Conv2d(12,24,kernel_size=5),

#第

二

个卷积层

nn.ReLU(),

nn.Conv2d(24,6,kernel_size=3)

#第三个卷积层

)

#最终分类器层

self.logits_layer = nn.Linear(in_features=153

6,out_features=10)

def forward(self,inputs):

image = inputs

x = self.convs_stack(image)

#elt.Rearrange的作用是

对输入数据的维度进行

调

整，读者可以使用torch.nn.Flatten

函数

完成此工作

x = elt.Rearrange("b

c h w -

> b

(c h w)")(x)

logits = self.logits_layer(x)

return logits

model = MnistNetword()

torch.save(model,"model.pth")

这里首先设

定了3个卷积层作为前置

的特征提取层，最

后一个

全连接层作为分类器层

，需要注意的是，对于

分类

器的全连接层，输入维度

需要手动计算，当然读

者

可以一步一步尝试打印

特征提取层的结果，依次

将

结果作为下一层的输

入维度。最后对模型进行

保存，

读者可以使用前面

章节中介绍的Netro软件对维

度进行

展示，结果如图5-14所

示。

图5-14 使用Netro软件对维度进

行展示

可以可视化地看

到整体模型的结构与显

示，这里对每

个维度都进

行了展示，感兴趣的读者

可以自行查阅。

5.2.3

基于卷积

的MNIST分类模型

下面进入本

章的最后示例部分，也就

是MNIST手写体的

分类。完整的

训练代码如下：

import torch

import

torch.nn as nn

import numpy as

np

import einops.layers.torch as elt

#载入数据

x_train

= np.load("../dataset/mnist/x_train.npy")

y_train_label = np.load("../dataset/mnist/y_train_label.npy

")

x_train = np.expand_dims(x_train,axis=1)

print(x_train.shape)

class MnistNetword(nn.Module):

def __init__(self):

super(MnistNetword, self).__init__()

self.convs_stack =

nn.Sequential(

nn.Conv2d(1,12,kernel_size=7),

nn.ReLU(),

nn.Conv2d(12,24,kernel_size=5),

nn.ReLU(),

nn.Conv2d(24,6,kernel_size=3)

)

self.logits_layer = nn.Linear(in_features=1

536,out_features=10)

def

forward(self,inputs):

image = inputs

x =

self.convs_stack(image)

x = elt.Rearrange("b c h

w -

> b (c h

w)")(x)

logits = self.logits_layer(x)

return logits

device = "cuda" if torch.cuda.is_available() else

"cpu"

#注意记得将model发送到GPU计算

model = MnistNetword().to(device)

model =

torch.compile(model)

loss_fn = nn.CrossEntropyLoss()

optimizer =

torch.optim.SGD(model.parameters(), lr=1e-4)

batch_size = 128

for

epoch in range(42):

train_num = len(x_train)//128

train_loss = 0.

for i in

range(train_num):

start = i * batch_size

end = (i + 1) *

batch_size

x_batch = torch.tensor(x_train[start:end]).

to(device)

y_batch

= torch.tensor(y_train_label[start:

end]).to(device)

pred = model(x_batch)

loss = loss_fn(pred, y_batch)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item() # 记录每个批

次的损失值

# 计算并打印损失值

train_loss /= train_num

accuracy

= (pred.argmax(1) == y_batch).type(torch

.float32).sum().item() /

batch_size

print("epoch：",epoch,"train_loss:",

round(train_loss,2),"accuracy:",round(accuracy,2))

在这

里，我们使用了本章新定

义的卷积神经网络模块

作为局部特征抽取，而对

于其他的损失函数以及

优化

函数，只使用了与前

期一样的模式进行模型

训练。最

终结果如图5-15所示

，请读者自行验证。

图5-15 模型

训练的最终结果

5.3 PyTorch 2.0的深度

可分离膨胀卷积详解

在

本章开始时就说明了，相

对于多层感知机来说，卷

积神经网络能够对输入

特征局部进行计算，同时

能够

节省大量的待训练

参数，本节将介绍更为深

入的内

容，即本章的进阶

部分——深度可分离膨胀卷

积。

需要说明的是，本例中

的深度可分离膨胀卷积

可以按

功能分为深度、可

分离、膨胀和卷积。

在讲解

之前，首先回到PyTorch 2.0中的卷积

定义

类：

class Conv2d(_ConvNd):

…

def __init__(

self, in_channels: int, out_channels: int,

kernel_size:

_size_2_t, stride:

_size_2_t = 1,

padding:

Union[str, _size_2_t] = 0,dilation

: _size_2_t

= 1,groups: int

= 1, bias:

bool = True,

padding_mode: str =

'zeros', # TODO: re

fine this

type

device=None,

dtype=None

) -> None:

前面讲解了卷积

类中常用的输入输出维

度

（in_channels,out_channels）的定义、卷积核

（kernel_size）以及（stride）大

小的设置，而对于

其他部

分的参数定义却没有详

细说明，本节将通过对

深

度可分离膨胀卷积的讲

解更为细致地说明卷积

类的

定义与使用。

5.3.1 深度可

分离卷积的定义

普通的

卷积其实可以分为两个

步骤来计算：

（1）跨通道计算

。

（2）平面内计算。

这是由于卷

积的局部跨通道计算的

性质所形成的，一

个非常

简单的思想是，能否使用

另一种方法将这部分

计

算过程分开计算，从而获

得参数数据量的减少。

答

案是可以的，可以使用深

度可分离卷积，总体如图

5-16所示。

图5-16 深度可分离卷积

在进行深度卷积的时候

，每个卷积核只关注单个

通道

的信息，而在分离卷

积中，每个卷积核可以联

合多个

通道的信息。这在

PyTorch 2.0中的具体实现如下：

#group=3是依

据通道数设置的分离卷

积数

Conv2d(in_channels=3, out_channels=3, kernel_size=3

, groups=3) #这是第一步完成

的

跨通道计算

Conv2d(in_channels=4, out_channels=4,

kernel_size=1

) #完成平面内

的计算

可以看到，此时我

们在传统的卷积层定义

上额外增加

了groups=3的定义，这

是根据通道数对卷积类

的定义

进行划分的。下面

通过一个具体的例子说

明常规卷积

与深度可分

离卷积的区别。

常规卷积

操作如图5-17所示。

图5-17 常规卷

积操作

假设输入层为一

个大小为28×28像素、三通道的

彩色

图片。经过一个包含

4个卷积核的卷积层，卷积

核尺寸

为3×3×3。最终会输出具

有4个通道数据的特征向

量，而尺寸大小由卷积函

数中的参数padding设置决

定。

在

深度可分离卷积操作中

，深度卷积操作有以下两

个

步骤完成。

（1）分离卷积的

独立计算。

如图5-18所示，深度

卷积使用的是3个尺寸为

3×3的卷

积核，经过该操作之

后，输出的特征图尺寸为

28×28×3（padding=1）。

图5-18 分离卷积的独立计算

（2）堆积多个可分离卷积计

算。

如图5-19所示，输入是第一

步的输出。可以看到图5-

19中

使用了4个独立的通道完

成，经过此步骤后，由第

一

个步骤输入的特征图在

4个独立的通道计算下输

出维

度变为28×28×3。

图5-19 堆积多个

可分离卷积计算

5.3.2 深度的

定义以及不同计算层待

训练参数的比

较

前面向

读者介绍了深度可分离

卷积，并在一开始的时

候

就提到了深度可分离卷

积可以减少待训练参数

，事

实是否如此呢？我们通

过代码打印的方式进行

比较，

代码如下：

import torch

from torch.nn import Conv2d,Linear

linear =

Linear(in_features=3*28*28, out_features=3*28*28)

linear_params = sum(p.numel() for

p in linear.parameter

s() if p.requires_grad)

conv = Conv2d(in_channels=3, out_channels=3, kernel_size=

3)

params = sum(p.numel() for p in

conv.parameters() if

p.requires_grad)

depth_conv = Conv2d(in_channels=3,

out_channels=3, kernel

_size=3, groups=3)

point_conv =

Conv2d(in_channels=3, out_channels=3, kernel

_size=1)

# 需要注意

的是，这里是先实现depth，然后

进行逐点卷积，从而两

者

结合，就得到了深度分离

卷积

depthwise_separable_conv = torch.nn.Sequential(depth_conv,

point_conv)

params_depthwise

= sum(p.numel() for p in depthwise_sep

arable_conv.parameters()

if p.requires_grad)

print(f"多层感知机使用的

参数为 {params} parameters.")

print("----------------")

print(f"普通卷积层使用

的参数为 {params} parameters.")

print("----------------")

print(f"深度可分离卷

积使用的参数

为 {params_depthwise} parameters.")

在上面

的代码段中，依次准备了

多层感知机、普通卷

积层

以及深度可分离卷积，对

其输出待训练参数，结

果

如图5-20所示。

图5-20 深度可分离

卷积可以减少待训练参

数

从图5-20中参数的输出可

以很明显地看到，随着采

用

不同的计算层，待训练

参数也会随之变化，即使

一个

普通的深度可分离

卷积层也能减少一半的

参数使用

量。

5.3.3 膨胀卷积详

解

经过前面对PyTorch 2.0中卷积的

说明，读者应该了

解了group参

数的含义，此时还有一个

不常用的参数

dilation，用于决定

卷积层在计算时的膨胀

系数。

dilation有点类似于stride，实际含

义为：每个点之

间有空隙

的过滤器，即为dilation。如图5-21所示

，

简单地说，膨胀卷积通过

在卷积核中增加空洞，可

以

增加单位面积中计算

的大小，从而扩大模型的

计算视

野。

图5-21

膨胀卷积

卷

积核的膨胀系数（空洞的

大小）在每一层是不同

的

，一般可以取1, 2, 4, 8,

…，即前一层的

两

倍。注意，膨胀卷积的上

下文大小和层数是呈指

数相

关的，可以通过比较

少的卷积层得到更大的

计算面

积。使用膨胀卷积

的方法如下：

#注意这里dilation被

设置为2

depth_conv

= Conv2d(in_channels=3, out_channels=3, kernel

_size=3,

groups=3,dilation=2)

point_conv = Conv2d(in_channels=3, out_channels=3, kernel

_size=1)

# 深度可分离膨胀

卷积的定义

depthwise_separable_conv = torch.nn.Sequential(depth_conv,

point_conv)

需要注意的

是，在卷积层的定义中，只

有dilation被

设置成大于或等于

2的整数才能实现膨胀卷

积。而对于

其参数大小的

计算，读者可以自行完成

。

5.4 实战：基于深度可分离膨

胀卷积的MNIST手写体

识别

下

面进入实战部分，基于前

期介绍的深度可分离膨

胀

卷积完成实战的MNIST手写

体的识别。

首先进行模型

的定义，在这里我们预期

使用自定义的

卷积替代

部分原生卷积完成模型

的设计，代码如下：

import torch

import torch.nn as nn

import numpy

as np

import einops.layers.torch as elt

#下面是

自定义的深度可分离膨

胀卷积的定义

depth_conv = nn.Conv2d(in_channels=12, out_channel

s=12,

kernel_size=3,

groups=6,dilation=2)

point_conv = nn.Conv2d(in_channels=12, out_channel

s=24, kernel_size=1)

depthwise_separable_conv = torch.nn.Sequential(dept

h_conv,

point_conv)

class MnistNetword(nn.Module):

def __init__(self):

super(MnistNetword,

self).__init__()

self.convs_stack = nn.Sequential(

nn.Conv2d(1,12,kernel_size=7),

nn.ReLU(),

depthwise_separable_conv,

#使用自定

义卷积替代了原生卷积

层

nn.ReLU(),

nn.Conv2d(24,6,kernel_size=3)

)

self.logits_layer

= nn.Linear(in_features=153

6,out_features=10)

def forward(self,inputs):

image

= inputs

x = self.convs_stack(image)

x

= elt.Rearrange("b c h w -

> b (c h w)")(x)

logits

= self.logits_layer(x)

return logits

可以看到，我们在中层

部分使用自定义卷积层

替代了

部分原生卷积层

。完整的训练代码如下：

import torch

import torch.nn as nn

import numpy as np

import einops.layers.torch

as elt

#载

入数据

x_train = np.load("../dataset/mnist/x_train.npy")

y_train_label = np.load("../dataset/mnist/y_train_l

abel.npy")

x_train =

np.expand_dims(x_train,axis=1)

print(x_train.shape)

depth_conv = nn.Conv2d(in_channels=12, out_channel

s=12, kernel_size=3,

groups=6,dilation=2)

point_conv = nn.Conv2d(in_channels=12,

out_channel

s=24, kernel_size=1)

# 深度可分离膨胀

卷积的定义

depthwise_separable_conv

= torch.nn.Sequential(dept

h_conv, point_conv)

class MnistNetword(nn.Module):

def __init__(self):

super(MnistNetword, self).__init__()

self.convs_stack =

nn.Sequential(

nn.Conv2d(1,12,kernel_size=7),

nn.ReLU(),

depthwise_separable_conv,

nn.ReLU(),

nn.Conv2d(24,6,kernel_size=3)

)

self.logits_layer = nn.Linear(in_features=153

6,out_features=10)

def

forward(self,inputs):

image = inputs

x =

self.convs_stack(image)

x = elt.Rearrange("b c h

w -

> b (c h

w)")(x)

logits = self.logits_layer(x)

return logits

device = "cuda" if torch.cuda.is_available() els

e "cpu"

#注意记得将

model发送到GPU计算

model = MnistNetword().to(device)

model = torch.compile(model)

loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(model.parameters(), lr

=1e-4)

batch_size

= 128

for epoch in range(63):

train_num = len(x_train)//128

train_loss = 0.

for i in range(train_num):

start =

i * batch_size

end = (i

+ 1) * batch_size

x_batch =

torch.tensor(x_train[start:end]).to

(device)

y_batch = torch.tensor(y_train_label[start:en

d]).to(device)

pred = model(x_batch)

loss = loss_fn(pred,

y_batch)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss +=

loss.item() # 记录每个批

次

的损失值

# 计算并打印

损失值

train_loss /= train_num

accuracy = (pred.argmax(1)

== y_batch).type(torch.f

loat32).sum().item() /

batch_size

print("epoch：",epoch,"train_loss:",

round(train_loss,2),"accuracy:",round(accuracy,2))

最终计算结果请

读者自行学习完成。

5.5 本章

小结

本章内容是PyTorch 2.0中一个

非常重要的部分，详

细介

绍了后期最为常用的API使

用，以及使用卷积对

MNIST数据

集进行识别的实战过程

。这个MNIST手写体

识别是一个

入门案例，但是包含技术

层面的内容非常

多，例如

使用多种不同的层和类

构建一个较为复杂的

卷

积神经网络。最后还介绍

了深度可分离膨胀卷积

。

除此之外，通过演示自定

义层的方法向读者说明

了一

个新的编程范式的

使用，即通过block的形式对模

型进

行组合，这在深度学

习领域有一个专门的名

称叫“残

差卷积”。这是一种

非常优雅的模型设计模

式。

本章的内容非常重要

，希望读者认真学习。

第6章

PyTorch数据处理与模型可视化

前面的章节讲解了PyTorch 2.0模型

与训练方面的内

容，相信

读者已经有能力完成一

定难度的深度学习应

用

项目。读者可能感觉到了

，在前期的学习中，更多

的

是对PyTorch 2.0模型本身的了解，而

对其他部分

介绍较少，特

别是数据处理部分，一直

都是使用NumPy

工具包对数据

进行处理，因此缺乏一个

贴合PyTorch自

身的数据处理器

。

有鉴于此，PyTorch 2.0版本提供了专

门的数据下

载、数据处理

包，即torch.utils.data工具包，使用

这个包

中的数据处理工具可以

极大地提高开发效率和

质量。torch.utils.data包中提供的数据处

理工具箱

如图6-1所示。

图6-1 torch.utils.data包

中提供的数据处理工具

箱

图6-1展示的是基于PyTorch 2.0数据

处理工具箱的总

体框架

，其中3个重要工具介绍如

下。

·DataLoader：定义一个新的迭代器

，实现批量

（Batch）读取，打乱数据

（Shuffle）并提供并行加

速等功能

。

·Dataset：是一个抽象类，其他数据

需要继承这个

类，并且覆

写其中的两个方法：__getitem__和

__len__。

·Sampler：提

供多种采样方法的函数

。

下面基于PyTorch 2.0提供的torch.utils.data数据

处

理工具箱进行讲解。

6.1 用于

自定义数据集的torch.utils.data工具箱

使用详解

本章开头我们

提到torch.utils.data工具箱提供了

Dataset、DataLoader以及

Sampler类，其作用都是对

采集的

数据进行处理，但是Dataset在输

出时每次只能

输出一个

样本，而DataLoader可以弥补这一缺

陷，实

现批量乱序输出样

本，如图6-2所示。

图6-2 DataLoader批量乱序

输出样本

6.1.1 使用torch.utils.data.Dataset封装自定

义

数据集

我们从自定义

数据集开始介绍。在PyTorch 2.0中，

数

据集的自定义使用需要

继承

torch.utils.data.Dataset类，之后实现其中的

__getitem__、__len__方法。最基本的Dataset类架构

如

下：

class Dataset():

def __init__(self,

transform=None): #注意transform

参数会在6.1.2节介绍

super(Dataset, self).__init__()

def __getitem__(self,

index):

pass

def __len__(self):

pass

可以清楚地看到，Dataset除了基

本的init函数外，还

需要填充

两个额外的函数，分别是

__getitem__和

__len__。这是仿照Python中数据list的写

法对其进行

定义的，其使

用方法如下：

data = Customer(Dataset)[index]

#打印出index序号

对应的数

据

length = len(Customer(Dataset)) #打印出数据

集总长度

下面以前面章

节中一直使用的MNIST数据集

为例进行介

绍。

1. init的初始化

方法

在对数据进行输出

之前，首先将数据加载到

Dataset类

中，加载的方法是直接

按数据读取的方案使用

NumPy载

入。当然，读者也可以使

用其他读取数据的技术

来获

取数据。在这里，我们

所使用的数据读取代码

如下：

def __init__(self, transform=None): #注意transform参

数会在6.1.2节介

绍

super(MNIST_Dataset, self).__init__()

# 载入数据

self.x_train

= np.load("../dataset/mnist/x_train.np

y")

self.y_train_label = np.load("../dataset/mnist/y_tr

ain_label.npy")

2. __getitem__与__len__方法

首先

是对数据的获取，__getitem__是Dataset父类

中

内置的数据迭代输出

的方法，在这里只需要显

式地提

供此方法的实现

即可，代码如下：

def __getitem__(self, item):

image = (self.x_train[item])

label = (self.y_train_label[item])

return image,label

而__len__方法用

于获取数据的长度，在这

里直接返回

标签的长度

即可，代码如下：

def __len__(self):

return len(self.y_train_label)

完整的自

定义MNIST_Dataset数据输出的代码如

下：

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self):

super(MNIST_Dataset, self).__init__()

# 载入数据

self.x_train = np.load("../dataset/mnist/x_

train.npy")

self.y_train_label = np.load("../dataset/mn

ist/y_train_label.npy")

def __getitem__(self,

item):

image = self.x_train[item]

label =

self.y_train_label[item]

return image,label

def __len__(self):

return

len(self.y_train_label)

最后建议按

照本小节开始介绍的方

法输出数据结果。

6.1.2 改变数

据类型的Dataset类中transform的使

用

我

们获取的输入数据，PyTorch

2.0不能

直接使用，

因此最少需要

一种转换方法将初始化

载入的数据转换

成我们

所需要的样式。

1. 将自定义

载入的参数转换为PyTorch 2.0专用

的

tensor类

这一步很简单，只需

要额外提供对于输入输

出类的处

理方法即可，代

码如下：

class ToTensor:

def

__call__(self, inputs, targets): #可调用

对象

return

torch.tensor(inputs), torch.tensor(t

argets)

这里

所提供的ToTensor类的作用是对

输入的数据进行

调整，需

要读者注意的是，这个类

的输入输出数据结

构和

类型需要与自定义Dataset类中

的

def __getitem__数据结构和类型相一

致。

2. 新的自定义Dataset类

对于原

本自定义的Dataset类的定义，需

要对其进行修

正，新的数

据读取类的定义如下：

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self,transform = None):

#在

定

义时需要定义transform的参数

super(MNIST_Dataset, self).__init__()

# 载入数据

self.x_train

= np.load("../dataset/mnist/x_

train.npy")

self.y_train_label = np.load("../dataset/mn

ist/y_train_label.npy")

self.transform = transform

#需要显式地提

供transform类

def

__getitem__(self, index):

image = (self.x_train[index])

label

= (self.y_train_label[index])

#通过判定transform类的存在

对其进行调用

if self.transform:

image,label

= self.transform(image,

label)

return image,label

def

__len__(self):

return len(self.y_train_label)

在这里读

者需要显式地提供自定

义Dataset类中

transform的定义与具体使

用位置和操作。因此，要注

意自定义的transform类需要与getitem函

数的输出结

构相一致。

一

个需要显式地提供transform的自

定义Dataset类使

用如下：

import numpy as np

import torch

class ToTensor:

def __call__(self,

inputs, targets): #可调用

对象

return torch.tensor(inputs), torch.tensor(t

argets)

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self,transform =

None): #在定

义时需要定义

transform的参数

super(MNIST_Dataset, self).__init__()

#

载入数据

self.x_train = np.load("../dataset/mnist/x_

train.npy")

self.y_train_label

= np.load("../dataset/mn

ist/y_train_label.npy")

self.transform = transform

#需要显

式地提供transform类

def __getitem__(self, index):

image =

(self.x_train[index])

label = (self.y_train_label[index])

#通过判定transform类

的存在对其进行调用

if

self.transform:

image,label = self.transform(image,

label)

return

image,label

def __len__(self):

return len(self.y_train_label)

mnist_dataset

= MNIST_Dataset()

image,label = (mnist_dataset[1024])

print(type(image),

type(label))

print("----------------------------------")

mnist_dataset = MNIST_Dataset(transform=ToTensor())

image,label

= (mnist_dataset[1024])

print(type(image), type(label))

在

这里作者做了尝试，对同

一个Dataset类分别传入了

None和具

体实现的transform函数，最终结果

如图6-3

所示。

图6-3 比较结果

可

以清楚地看到传入transform后数

据的结构，

transform的存在使其数

据结构有了很大的变化

。

3.

修正数据输出的维度

在

transform类中，还可以进行更为复

杂的操作，例如

对维度进

行转换，代码如下：

class ToTensor:

def

__call__(self, inputs, targets): #可调用

对象

inputs =

np.reshape(inputs,[28*28])

return torch.tensor(inputs), torch.tensor(targets)

可以看到，我们根据

输入大小的维度进行折

叠操作，

从而为后续的模

型输出提供合适的数据

维度格式。此

时读者可以

使用如下方法打印出新

的输出数据维度，

代码如

下：

mnist_dataset = MNIST_Dataset(transform=ToTensor())

image,label

= (mnist_dataset[1024])

print(type(image), type(label))

print(image.shape)

4.

依旧无法使用自定义

的数据对模型进行训练

相信读者学到此部分，一

定信心满满地想将刚学

习到

的内容应用到深度

学习训练中。但是遗憾的

是，到目

前为止，使用自定

义数据集的模型还无法

运行，这是

由于PyTorch 2.0在效能方

面以及损失函数的计算

方

式上对此进行了限制

，读者可以运行以下程序

进行验

证。鼓励有能力的

读者自行查找错误进行

修正，下一

节会对其进行

更正。

#注意下面这段代码

无法正常使用，仅供演示

import numpy as np

import torch

#device = "cpu"#PyTorch的特性，需要指定计算的

硬件，如果没有

GPU，就使用CPU进

行计算

device = "cuda"#在这里默认使用

GPU，如果读者出现运行问题

，可

以将其改成CPU模式

class ToTensor:

def __call__(self, inputs, targets): #可调

用

对象

inputs = np.reshape(inputs,[1,-1])

targets = np.reshape(targets,

[1, -1])

return torch.tensor(inputs), torch.tensor(t

argets)

#注意下面这段代

码无法正常使用，仅供演

示

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self,transform =

None): #在定

义时需要定义transform的

参数

super(MNIST_Dataset, self).__init__()

#

载入数据

self.x_train = np.load("../dataset/mnist/x_

train.npy")

self.y_train_label

= np.load("../dataset/mn

ist/y_train_label.npy")

self.transform = transform

#需要显式

地提供transform类

def __getitem__(self, index):

image =

(self.x_train[index])

label = (self.y_train_label[index])

#通过判定transform类的

存在对其进行调用

if

self.transform:

image,label = self.transform(image,

label)

return

image,label

def __len__(self):

return len(self.y_train_label)

#注意

下面这段代码无法正常

使用，仅供演示

mnist_dataset = MNIST_Dataset(transform=ToTensor())

import os

os.environ['CUDA_VISIBLE_DEVICES']

= '0' #指定GPU编号

import torch

import numpy

as np

batch_size = 320

#设定每次训练的批次数

epochs

= 1024

#设定训练次数

#设定多层

感知机网络模型

class NeuralNetwork(torch.nn.Module):

def __init__(self):

super(NeuralNetwork, self).__init__()

self.flatten =

torch.nn.Flatten()

self.linear_relu_stack = torch.nn.Sequentia

l(

torch.nn.Linear(28*28,312),

torch.nn.ReLU(),

torch.nn.Linear(312, 256),

torch.nn.ReLU(),

torch.nn.Linear(256, 10)

)

def forward(self, input):

x =

self.flatten(input)

logits = self.linear_relu_stack(x)

return logits

model = NeuralNetwork()

model = model.to(device)

#将

计算

模型传入GPU硬件等待计算

torch.save(model, './model.pth')

model = torch.compile(model)

#PyTorc

h 2.0的特性，加速计算速度

loss_fu = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=2e-

5) #设

定优化函数

#注意下面这

段代码无法正常使用，仅

供演示

#开始计算

for epoch in range(20):

train_loss = 0

for sample in

(mnist_dataset):

image = sample[0];label = sample[1]

train_image = image.to(device)

train_label = label.to(device)

pred = model(train_image)

loss = loss_fu(pred,train_label)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item()

# 记录每

个批

次的损失值

# 计算并

打印损失值

train_loss

/= len(mnist_dataset)

print("epoch：",epoch,"train_loss:", round(train_los

s,2))

这段代码看

起来没有问题，但是实际

上在运行时会报

错，这是

由于数据在输出时是逐

个进行输出的，模型

逐个

数据计算损失函数时无

法对其进行计算，同时这

样的计算方法也会极大

地限制PyTorch 2.0的计算性

能，因此

不建议采用这种方法直

接对模型进行计算。

6.1.3 批量

输出数据的DataLoader类详解

下面

讲解torch.utils.data工具箱中最后一个

工具，

针对批量输出数据

的DataLoader类。

首先需要说明的是

，DataLoader就是为了解决使用

Dataset自定

义封装的数据时无法对

数据进行批量化处

理的

问题，使用起来非常简单

，只需要将其包装在使

用

Dataset封装好的数据集外即可

。代码如下：

…

mnist_dataset = MNIST_Dataset(transform=ToTensor())

#通过Dataset获取数

据

集

from torch.utils.data import DataLoader

#导入DataLoader

train_loader

= DataLoader(mnist_dataset, batch_siz

e=batch_size, shuffle=True)

#包装已封装好的

数据集

实际上就这么简

单，对于DataLoader的使用，首先导

入

对应的包，然后使用它包

装已封装好的数据集即

可。DataLoader的定义如下：

class DataLoader(object):

__initialized =

False

def __init__(self, dataset, batch_size=1, shuffle

=False, sampler=None,

def __setattr__(self, attr, val):

def __iter__(self):

def __len__(self):

与前面实

现Dataset的不同之处在于：

·我们

一般不需要自己实现DataLoader的

方法，只需

要在构造函数

中指定相应的参数即可

，比如常见的

batch_size、shuffle等参数。所以

使用DataLoader十

分简洁方便。

·DataLoader实际

上是一个较为高层的封

装类，它的

功能是通过更

底层的_DataLoader来完成的，但是

_DataLoader类

较为低层，这里就不展开

叙述了。

DataLoaderIter就是_DataLoaderIter的一个框架

，用

来传给_DataLoaderIter一堆参数，并把

自己装进

DataLoaderIter里。

对于DataLoader的使用

现在只介绍那么多，下面

是基

于PyTorch 2.0数据处理工具箱

对数据进行识别和训

练

的完整代码。

import numpy as np

import

torch

#device = "cpu" #PyTorch的特性，需要

指定计算的硬件，如果没

有GPU，就使用CPU进行计算

device

= "cuda" #在这

里默认使用GPU，如果出现运

行问题，可以

将其改成CPU模

式

class ToTensor:

def __call__(self, inputs, targets): #可调用

对象

inputs = np.reshape(inputs,[28*28])

return torch.tensor(inputs), torch.tensor(t

argets)

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self,transform =

None): #在定

义时

需要定义transform的参数

super(MNIST_Dataset, self).__init__()

#

载入数

据

self.x_train = np.load("../dataset/mnist/x_

train.npy")

self.y_train_label

= np.load("../dataset/mn

ist/y_train_label.npy")

self.transform = transform

#需要显式地提供transform类

def __getitem__(self, index):

image =

(self.x_train[index])

label = (self.y_train_label[index])

#通

过判定transform类的存在对其进

行调用

if

self.transform:

image,label = self.transform(image,

label)

return

image,label

def __len__(self):

return len(self.y_train_label)

import

torch

import numpy as np

batch_size

= 320

#设定每次训练的

批次数

epochs = 42

#设定训练次数

mnist_dataset = MNIST_Dataset(transform=ToTensor())

from torch.utils.data

import DataLoader

train_loader = DataLoader(mnist_dataset, batch_size=batch_

size)

#设

定多层感知机网络模型

class NeuralNetwork(torch.nn.Module):

def __init__(self):

super(NeuralNetwork,

self).__init__()

self.flatten = torch.nn.Flatten()

self.linear_relu_stack =

torch.nn.Sequentia

l(

torch.nn.Linear(28*28,312),

torch.nn.ReLU(),

torch.nn.Linear(312, 256),

torch.nn.ReLU(),

torch.nn.Linear(256, 10)

)

def forward(self,

input):

x = self.flatten(input)

logits =

self.linear_relu_stack(x)

return logits

model = NeuralNetwork()

model = model.to(device) #将

计算模型传入GPU硬件等

待计算

torch.save(model,

'./model.pth')

model = torch.compile(model) #PyTorc

h

2.0的特性，加速计算

速度

loss_fu = torch.nn.CrossEntropyLoss()

optimizer =

torch.optim.Adam(model.parameters(), lr=2e-

4) #设定优化函数

#开始

计算

for

epoch in range(epochs):

train_loss = 0

for image,label in (train_loader):

train_image =

image.to(device)

train_label = label.to(device)

pred =

model(train_image)

loss = loss_fu(pred,train_label)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item() # 记录每个批

次的损

失值

# 计算并打印损失值

train_loss = train_loss/batch_size

print("epoch：",

epoch, "train_loss:", round(train

_loss, 2))

最终结果请读者自行打

印完成。

6.2 基于tensorboardX的训练可视

化展示

前面带领读者完

成了对PyTorch 2.0中数据处理工具

箱的使用，相信读者已经

可以较好地对PyTorch 2.0

的数据进

行处理。tensorboardX是一种PyTorch

2.0

模型可视

化组件，本节将讲解tensorboardX数据

可视

化的方法。

6.2.1 tensorboardX的安装与

简介

前面介绍了Netron的安装

与使用，这是一种可视化

PyTorch模型的方法，其好处是操

作简单，可视性强。

但是随

之而来的是Netron组件对模型

的展示效果并不

是很准

确，只能大致展示出模型

的组件与结构。

tensorboardX是专门为

PyTorch 2.0进行模型展示与

训练可

视化设计的组件，可以记

录模型训练过程中的

数

字、图像等内容，以方便研

究人员观察神经网络训

练过程。

tensorboardX安装命令如下（注

意一定要在前面安装

的

Anaconda或者Miniconda终端进行）：

pip install tensorboardX

这里作者

在前面已经提醒了，这一

步部分的操作一定

要在

终端中进行，基于pip的安装

和后续操作都是这

样。

6.2.2 tensorboardX可

视化组件的使用

tensorboardX对模型

的展示是最重要的作用

之一，读

者可以遵循以下

步骤获得模型的展示效

果。

1. 存储模型的计算过程

使用tensorboardX首先需要模拟一次

模型的运算过

程，代码如

下：

#创建模型

model =

NeuralNetwork()

# 模拟输入数

据

input_data = (torch.rand(5,

784))

from tensorboardX import SummaryWriter

writer

= SummaryWriter()

with writer:

writer.add_graph(model,(input_data,))

可以看到，首先载入了

已设计好的模型，然后模

拟输

入数据，在载入tensorboardX并建

立读写类之后，将

模型及

运算过程加载到运行图

中。

2. 查看默认位置的run文件

夹

运行上面的代码后，程

序会在当前平行目录下

生成一

个新的runs目录，这是

存储和记录模型展示的

文件

夹，如图6-4所示。可以看

到文件夹是以日期的形

式生

成新的目录的。

图6-4 runs目

录用于存储和记录模型

展示

3. 使用Miniconda终端打开对应

的目录

下面使用Miniconda终端打

开刚才生成的目录，比如

作

者的目录情况如下所

示。

(base)C:\Users\xiaohua>cd C:\Users\xiaohua\Desktop\jupyter_b

ook\src\第六章

需要注意的是

，这里打开的是runs文件夹的

上一级目

录，而不是runs文件

夹本身。之后调用tensorboardX

对模型

进行展示，读者需要在刚

才打开的文件夹中执

行

以下命令：

tensorboard --logdir runs

结果如图6-5所示

。

图6-5 获取HTTP地址

可以看到此

时程序在执行，并提供了

一个HTTP地址。

4. 使用浏览器打

开模型展示页面

下面阅

读模型的展示页面，在这

里使用了Windows自带

的Edge浏览器

，读者也可以尝试不同的

浏览器，在其

中输入图6-6中

的HTTP地址，可以进入本地保

存的存档

页面，如图6-7所示

。

图6-6 阅读模型的展示页面

可以看到这是模型的基

本参数、输入输出以及基

本模

块的展示，之后读者

可以双击模型主题部分

，展开模

型进行进一步的

说明，如图6-7所示。更多操作

建议读

者自行尝试。

图6-7 模

型结构的展示

6.2.3

tensorboardX对模型训

练过程的展示

了解了模

型结构的展示，有的读者

还希望了解模型在

训练

过程中出现的一些问题

和参数变化，

tensorboardX同样提供了

此功能，记录并展示了模

型

在训练过程中损失值

的变化，代码如下：

from

tensorboardX import SummaryWriter

writer = SummaryWriter()

#开始计

算

for epoch in range(epochs):

…

# 计算并打印损失值

train_loss = train_loss/batch_size

writer.add_scalars('evl',

{'train_loss': train_loss

}, epoch)

writer.close()

可

以看到，使用tensorboardX对训练过程

的参数记录

非常简单，直

接记录损失过程即可，而

epoch作为横坐

标标记也会被

记录。完整的代码如下（作

者故意调整

了损失函数

学习率）：

import torch

#device

= "cpu" #PyTorch的特性，需要指定

计算的硬件，如果

没有GPU，就

使用CPU进行计算

device =

"cuda" #在这里默

认使用GPU，如果出现运行问

题，可

以将其改成CPU模式

class ToTensor:

def

__call__(self, inputs, targets): #可

调用

对象

inputs

= np.reshape(inputs,[28*28])

return torch.tensor(inputs), torch.tensor(t

argets)

class MNIST_Dataset(torch.utils.data.Dataset):

def __init__(self,transform = None):

#在定

义时需要

定义transform的参数

super(MNIST_Dataset, self).__init__()

# 载入数据

self.x_train = np.load("../dataset/mnist/x_

train.npy")

self.y_train_label =

np.load("../dataset/mn

ist/y_train_label.npy")

self.transform = transform

#需

要显式地提供transform类

def __getitem__(self, index):

image = (self.x_train[index])

label = (self.y_train_label[index])

#通过判

定transform类的存在对其进行调

用

if self.transform:

image,label = self.transform(image,

label)

return image,label

def __len__(self):

return len(self.y_train_label)

import torch

import numpy as np

batch_size =

320

#设定每次训练的批次

数

epochs = 320

#设定训练次数

mnist_dataset = MNIST_Dataset(transform=ToTensor())

from torch.utils.data import

DataLoader

train_loader = DataLoader(mnist_dataset, batch_size=batch_

size)

#设定的

多层感知机网络模型

class NeuralNetwork(torch.nn.Module):

def __init__(self):

super(NeuralNetwork,

self).__init__()

self.flatten = torch.nn.Flatten()

self.linear_relu_stack =

torch.nn.Sequentia

l(

torch.nn.Linear(28*28,312),

torch.nn.ReLU(),

torch.nn.Linear(312, 256),

torch.nn.ReLU(),

torch.nn.Linear(256, 10)

)

def forward(self,

input):

x = self.flatten(input)

logits =

self.linear_relu_stack(x)

return logits

model = NeuralNetwork()

model = model.to(device) #将

计算模

型传入GPU硬件等待

计算

model

= torch.compile(model) #PyTorch 2.0

的特性，加速计算速

度

loss_fu

= torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=2e-

6) #设定优化函数

from tensorboardX import SummaryWriter

writer = SummaryWriter()

#开始计

算

for epoch

in range(epochs):

train_loss = 0

for

image,label in (train_loader):

train_image = image.to(device)

train_label = label.to(device)

pred = model(train_image)

loss = loss_fu(pred,train_label)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item() # 记录每个批

次的损失

值

# 计算并打印损失值

train_loss = train_loss/batch_size

print("epoch：",

epoch, "train_loss:", round(train

_loss, 2))

writer.add_scalars('evl',

{'train_loss': train_loss

}, epoch)

writer.close()

完

成训练后，在浏览器中打

开图6-5所示的HTTP地址，

在页面

上单击TIME SERIES标签，对存储的模

型变量

进行验证，如图6-8所

示。

图6-8 对存储的模型变量

进行验证

这里记录了模

型在训练过程中保存的

损失值的变化，

更多的模

型训练过程参数值的展

示请读者自行尝试。

6.3 本章

小结

本章主要讲解了PyTorch 2.0数

据处理与模型训练可

视

化方面的内容。同时还介

绍了数据处理的步骤，通

过学习读者可能会有这

样的印象，即PyTorch

2.0中

的数据处

理是依据一个个的“管套

”进行的，如图6-9

所示。事实也

是这样的，PyTorch 2.0通过管套的模

型对数据一步一步地进

行加工最终得到结果，这

是一

种常用的设计模型

，请读者注意。

图6-9

管套

本章

还讲解了基于PyTorch 2.0原生的模

型训练可视

化组件tensorboardX的用

法，除了对模型本身的展

示

外，tensorboardX更侧重对模型训练

过程的展示，记

录了模型

的损失值等信息，读者还

可以进一步尝试加

入对

准确率的记录。

第7章

从冠

军开始

——实战ResNet

随着卷积网

络模型的成功，更深、更宽

、更复杂的网

络已经成为

卷积神经网络搭建的主

流。卷积神经网络

能够用

来提取所侦测对象的低

、中、高特征，网络的

层数越

多，意味着能够提取到不

同层次Level的特征越

丰富，并

且通过还原镜像发现越

深的网络提取的特征

越

抽象，越具有语义信息。

这

就产生了一个非常大的

疑问，是否可以单纯地通

过

增加神经网络模型的

深度和宽度，即增加更多

的隐藏

层和每层中的神

经元来获得更好的结果

？

答案是不可以。因为根据

实验发现，随着卷积神经

网

络层数的加深，出现了

另一个问题，即在训练集

上，

准确率难以达到100%正确

，甚至产生了下降。

这似乎

不能简单地解释为卷积

神经网络的性能下降，

因

为卷积神经网络加深的

基础理论就是越深越好

。如

果强行解释为产生了

“过拟合”

，似乎也不能够完

美

解释准确率下降的原

因，因为如果产生了过拟

合，那

么在训练集上卷积

神经网络应该表现得更

好才对。

这个问题被称为

“神经网络退化”。

神经网络

退化问题的产生说明了

卷积神经网络不能够

被

简单地使用堆积层数的

方法进行优化。

2015年，152层深的

ResNet（Residual Network，残

差神经网络）横空出世

，取得了当年ImageNet竞赛的

冠军

，相关论文在CVPR 2016斩获最佳论

文奖。

ResNet成为视觉乃至整个

AI界的一个经典。ResNet使

得训练

深度达数百层甚至数千

层的网络成为可能，而

且

性能仍然优异。

本章将主

要介绍ResNet及其变种。后面介

绍的

Attention模块是基于ResNet模型的

扩展，因此本章内

容非常

重要。

让我们站在巨人的

肩膀上，从冠军开始！

7.1 ResNet基础

原理与程序设计基础

为

了获取更好的准确率和

辨识度，科研人员不断使

用

更深、更宽、更大的网络

来挖掘对象的数据特征

，但

是随之而来的研究发

现，过多的参数和层数并

不能带

来性能上的提升

，反而随着网络层数的增

加，训练过

程的不稳定性

也会增加。因此，无论是科

学界还是工

业界都在探

索和寻找一种新的神经

网络结构模型。

ResNet的出现彻

底改变了传统靠堆积卷

积层所带来的

固定思维

，破天荒地提出了采用模

块化的集合模式来

替代

整体的卷积层，通过一个

个模块的堆叠来替代不

断增加的卷积层。

对ResNet的研

究和不断改进成为过去

几年中计算机视

觉和深

度学习领域最具突破性

的工作。并且由于其表

征

能力强，ResNet在图像分类任务

以外的许多计算机

视觉

应用上也取得了巨大的

性能提升，例如对象检测

和人脸识别。

7.1.1 ResNet诞生的背景

卷积神经网络的实质就

是无限拟合一个符合对

应目标

的函数。而根据泛

逼近定理

（Universal Approximation Theorem），如果给定

足够

的容量，一个单层的前馈

网络就足以表示任何函

数。但是，这个层可能是非

常大的，而且网络容易过

拟合数据。因此，学术界有

一个共同的认识，就是网

络架构需要更深。

但是，研

究发现只是简单地将层

堆叠在一起，增加网

络的

深度并不会起太大的作

用。这是由于梯度消失

（Vanishing Gradient）问

题的存在，导致深层的网

络很难训练。因为梯度反

向传播到前一层，所以重

复

相乘可能使梯度无穷

小。结果就是，随着网络层

数更

深，其性能趋于饱和

，甚至开始迅速下降，如图

7-1所

示。

图7-1 随着网络的层数

更深，其性能趋于饱和，甚

至

开始迅速下降

在ResNet之前

，已经出现了好几种处理

梯度消失问题

的方法，但

是没有一个方法能够真

正解决这个问题。

何恺明

等人于2015年发表的论文《用

于图像识别的深

度残差

学习》

（Deep Residual Learning for Image

Recogni

tion）中，认为堆叠的层不

应该降低网络的性能，可

以简单地在当前网络上

堆叠映射层（不处理任何

事情

的层），并且所得到的

架构性能不变。

即当f(x)为0时

，f '(x)等于x，而当f(x)不为0时，

所获得

的f

'(x)性能要优于单纯地输

入x。公式表

明，较深的模型

所产生的训练误差不应

比较浅的模型

的误差更

高。让堆叠的层拟合一个

残差映射

（Residual Mapping）要比让它直接

拟合所需的底层

映射更

容易。

从图7-2可以看到，残差

映射与传统的直接相连

的卷积

网络相比，最大的

变化是加入了一个恒等

映射层y=x

层。其主要作用是

使得网络随着深度的增

加而不会产

生权重衰减

、梯度衰减或者消失这些

问题。

图7-2 残差框架模块

图

中F(x)表示的是残差，F(x)+x是最终

的映射输出，

因此可以得

到网络的最终输出为H(x) =F(x) +x。由

于网络框架中有两个卷

积层和两个ReLU函数，因此最

终的输出结果可以表示

为：

其中H是第一层的输出

，而H是第二层的输出。这样

在

输入与输出有相同维

度时，可以使用直接输入

的形式

将数据直接传递

到框架的输出层。

12

ResNet整体结

构图及与VGGNet的比较如图7-3所

示。

图7-3 ResNet模型结构及与VGGNet的比

较

图7-3展示了VGGNet

19、一个34层的普

通结构神经网

络以及一

个34层的ResNet网络的对比。通过

验证可以

知道，在使用了

ResNet的结构后，层数不断加深

导致

的训练集上误差增

大的现象被消除了，ResNet网络

的

训练误差会随着层数

的增大而逐渐减小，并且

在测试

集上的表现也会

变好。

但是，除了用以讲解

的二层残差学习单元外

，实际上

更多的是使用[1,1]结

构的三层残差学习单元

，如图7-

4所示。

图7-4 二层（左）和三

层（右）残差单元的比较

这

是借鉴了NIN模型的思想，在

二层残差单元中包含一

个[3,3]卷积层的基础上，更包

含了两个[1,1]大小的

卷积层

，放在[3,3]卷积层的前后，执行

先降维再升维

的操作。

无

论采用哪种连接方式，ResNet的

核心是引入一个

“身份捷

径连接”

（Identity Shortcut

Connection），直接跳过一

层或

多层将输入层与输出层

进行连接。实际上，

ResNet并不是

第一个利用Shortcut Connection的方

法，早期

相关研究人员就在卷积

神经网络中引入了

“门控

短路电路”

，即参数化的门

控系统允许特定信

息通

过网络通道，如图7-5所示。

图

7-5 门控短路电路

但是并不

是所有加入了Shortcut的卷积神

经网络都会

提升传输效

果。在后续的研究中，有不

少研究人员对

残差块进

行了改进，但是很遗憾并

没有获得性能上的

提升

。

7.1.2 不要重复造轮子——PyTorch 2.0中的模

块

工具

在正式讲解ResNet之前

，我们先熟悉一下ResNet构建

过

程中所使用的PyTorch 2.0模块。

工欲

善其事，必先利其器。在构

建自己的残差网络之

前

，需要准备好相关的程序

设计工具。这里的工具是

指那些已经设计好结构

，可以直接使用的代码。最

重

要的是卷积核的创建

方法。从模型上看，需要更

改的

内容很少，包括卷积

核的大小、输出通道数以

及所定

义的卷积层的名

称，代码如下：

torch.nn.Conv2d

对于PyTorch 2.0中的Conv2d这

个类，在前面的章节

中已

经出现过，后期还会学习

其1D模式。

此外，还有一个非

常重要的方法BatchNorm2d，即获取

数

据的BatchNormalization，它使用批量正则化

对数

据进行处理，代码如

下：

torch.nn.BatchNorm2d

在这里，BatchNorm2d类生成时需要

定义输出的最后一

个维

度，从而在初始化过程中

生成一个特定的数据维

度。

还有最大池化层，代码

如下：

torch.nn.MaxPool2d

平均池化层，代码如

下：

torch.nn.AvgPool2d

这些是在模型单元中

需要使用的基本工具，这

些工具

的用法我们在后

续的模型实现中会进行

讲解。有了这

些工具，就可

以直接构建ResNet模型单元。

7.1.3 ResNet残

差模块的实现

ResNet网络结构

已经在前面介绍过了，它

突破性地使

用模块化思

维对网络进行叠加，从而

使得数据在模块

内部特

征的传递不会丢失。

从图

7-6可以看到，模块的内部实

际上是3个卷积通道

相互

叠加，形成了一种瓶颈设

计。对于每个残差模块

使

用3层卷积。这3层分别是1×1、3×3和

1×1的卷积

层，其中1×1的卷积层

的作用是对输入数据进

行“整

形”

，通过修改通道数

使得3×3的卷积层具有较小

的

输入／输出数据结构。

图

7-6 模块的内部

实现的瓶颈

3层卷积结构的代码如下

：

torch.nn.Conv2d(input_dim,input_dim//4,kernel_size=1,padding=

1)

torch.nn.ReLU(input_dim//4)

//ReLU函数作为神经元的激活

函数

torch.nn.Conv2d(input_dim//4,input_dim//4,kernel_size=3,paddi

ng=1)

torch.nn.ReLU(input_dim//4)

torch.nn.BatchNorm2d(input_dim//4)

torch.nn.Conv2d(input_dim,input_dim,kernel_size=1,padding=1)

torch.nn.ReLU(input_dim)

代码中输入的数据

首先经过Conv2d卷积层计算，这

里

设置了输出维度为1/4的

输入维度，这是为了降低

输入

数据的整个数据量

，为进行下一层的[3,3]的计算

打下

基础。同时，因为PyTorch

2.0的关

系，需要显式地

加入ReLU和BatchNorm2d作

为激活层和批处理层。

在

数据传递的过程中，ResNet模块

使用了名为

shortcut的“信息高速

公路”

，shortcut连接相当于

简单执

行了同等映射，不会产生

额外的参数，也不会

增加

计算复杂度。而且，整个网

络依旧可以通过端到

端

的反向传播训练。

正是因

为有了Shortcut的出现，才使得信

息可以在每

个（Block）中进行传

播，据此构成的

ResNet BasicBlock代码如下

：

import torch

import torch.nn as nn

class BasicBlock(nn.Module):

expansion = 1

def

__init__(self, in_channels, out_channels, strid

e=1):

super().__init__()

#residual function

self.residual_function = nn.Sequential(

nn.Conv2d(in_channels,

out_channels,

kernel_size=3, stride=stride,

padding=1, bias=False),

nn.BatchNorm2d(out_channels),

nn.ReLU(inplace=True),

nn.Conv2d(out_channels, out_channels

* BasicBlock.expansion,

kernel_size=3,

padding=1, bias=False),

nn.BatchNorm2d(out_channels * BasicBl

ock.expansion)

)

#shortcut

self.shortcut = nn.Sequential()

#判定输出的维度是否和

输入一致

if stride != 1 or in_channels

!= BasicBl

ock.expansion * out_channels:

self.shortcut

= nn.Sequential(

nn.Conv2d(in_channels, out_cha

nnels *

BasicBlock.expansion,

kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels *

BasicBlock.expansion)

)

def forward(self, x):

return

nn.ReLU(inplace=True)

(self.residual_function(x) +

self.shortcut(x))

上面代码实现

的是经典的ResNet Block模型，除此之

外，还有更多的ResNet模块化方

式，如图7-7所示。

图7-7 更多的ResNet模

块化方式

有兴趣的读者

可以尝试更多的模块结

构。

7.1.4 ResNet网络的实现

在介绍完

ResNet模块的实现后，下面使用

完成的

ResNet Block实现完整的ResNet。ResNet的结

构如图

7-8所示。

图7-8 ResNet的结构

图

7-8一共提出了5种深度的ResNet，分

别是18、34、

50、101和152，其中所有的网络

都分成5部分，分别是

conv1、conv2_x、conv3_x、conv4_x和conv5_x。

说

明：ResNet完整的实现需要高性

能显卡，因此我们

对其进

行了修改，去掉了Pooling层，并降

低了每次

filter的数目和每层

的层数，这一点请读者注

意。

完整的ResNet模型的结构如

下：

import torch

import torch.nn as nn

class BasicBlock(nn.Module):

expansion = 1

def

__init__(self, in_channels, out_channels, strid

e=1):

super().__init__()

#residual function

self.residual_function = nn.Sequential(

nn.Conv2d(in_channels,

out_channels,

kernel_size=3, stride=stride,

padding=1, bias=False),

nn.BatchNorm2d(out_channels),

nn.ReLU(inplace=True),

nn.Conv2d(out_channels, out_channels

* BasicBlock.expansion,

kernel_size=3,

padding=1, bias=False),

nn.BatchNorm2d(out_channels * BasicBl

ock.expansion)

)

#shortcut

self.shortcut = nn.Sequential()

#判定输出的维度是否

和输入一致

if stride != 1 or in_channels

!= BasicBl

ock.expansion * out_channels:

self.shortcut

= nn.Sequential(

nn.Conv2d(in_channels, out_cha

nnels *

BasicBlock.expansion,

kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels *

BasicBlock.expansion)

)

def forward(self, x):

return

nn.ReLU(inplace=True)

(self.residual_function(x) +

self.shortcut(x))

class ResNet(nn.Module):

def __init__(self, block, num_block, num_classes=10

0):

super().__init__()

self.in_channels = 64

self.conv1 =

nn.Sequential(

nn.Conv2d(3, 64, kernel_size=3, padd

ing=1,

bias=False),

nn.BatchNorm2d(64),

nn.ReLU(inplace=True))

#在这里使用

构造函数的形式，根据传

入的模型结构进行

构建

，读者直接记住这种编写

方法即可

self.conv2_x

= self._make_layer(block, 64,

num_block[0], 1)

self.conv3_x

= self._make_layer(block, 128,

num_block[1], 2)

self.conv4_x

= self._make_layer(block, 256,

num_block[2], 2)

self.conv5_x

= self._make_layer(block, 512,

num_block[3], 2)

self.avg_pool

= nn.AdaptiveAvgPool2d((1, 1))

self.fc = nn.Linear(512

* block.expansion,

num_classes)

def _make_layer(self, block,

out_channels, num_bloc

ks, stride):

strides =

[stride] + [1] * (num_blocks -

1)

layers = []

for stride

in strides:

layers.append(block(self.in_channels,

out_channels, stride))

self.in_channels

= out_channels * b

lock.expansion

return

nn.Sequential(*layers)

def forward(self, x):

output =

self.conv1(x)

output = self.conv2_x(output)

output =

self.conv3_x(output)

output = self.conv4_x(output)

output =

self.conv5_x(output)

output = self.avg_pool(output)

#使用view层作为全

局池化层，fc是最终的分类

函数，

为每层对应的类别

进行分类计算

output = output.view(output.size(0), -1)

output =

self.fc(output)

return output

#18层的ResNet

def resnet18():

return ResNet(BasicBlock, [2, 2, 2, 2])

#34层的

ResNet

def resnet34():

return ResNet(BasicBlock, [3,

4, 6, 3])

if __name__ ==

'__main__':

image = torch.randn(size=(5,3,224,224))

resnet =

ResNet(BasicBlock, [2, 2, 2, 2])

img_out

= resnet(image)

print(img_out.shape)

需要注意的是，根据输入

层数的不同，采用

PyTorch 2.0中特有

的构造方法对传入的Block形

式进

行构建，而使用view层作

为全局池化层，之后的fc层

对结果进行最终分类。这

里为了配合接下来进行

的

CIFAR-10数据集分类，分类结果

被设置成10种。

为了演示，在

这里实现了18层和34层的ResNet模

型的

构建，更多的模型请

读者自行完成。

7.2 实战ResNet：CIFAR-10数据

集分类

本节将使用ResNet实现

CIFAR-10数据集的分类。

7.2.1 CIFAR-10数据集简

介

CIFAR-10数据集共有60000幅彩色图

像，这些图像是

32×32像素的，分

为10类，每类6000幅图，如图7-9所

示

。这里面有50000幅图用于训练

，构成了5个训练

批，每一批

10000幅图；另外，10000幅用于测试，单

独构成一批。测试批的数

据取自100类中的每一类，每

一类随机取1000幅。抽剩下的

就随机排列组成训练

批

。注意，一个训练批中的各

类图像的数量并不一定

相同，总的来看，训练批每

一类都有5000幅图。

图7-9 CIFAR-10数据集

读者自行搜索CIFAR-10数据集下

载地址，进入下载页

面后

，选择下载方式，如图7-10所示

。

图7-10

下载方式

由于PyTorch 2.0采用Python语

言编程，因此选择

Python Version的版本

下载。下载之后解压缩，得

到

如图7-11所示的文件。

图7-11 得

到的文件

data_batch_1～data_batch_5是划分好的训

练数据，

每个文件中包含

10000幅图片，test_batch是测试集数

据，也

包含10000幅图片。

读取数据的

代码如下：

import pickle

def load_file(filename):

with open(filename,

'rb') as fo:

data = pickle.load(fo,

encoding='latin1')

return data

首先定义读取

数据的函数，这几个文件

都是通过

pickle产生的，所以在

读取的时候也要用到这

个包。

返回的data是一个字典

，先来看这个字典里面有

哪些

键。

data = load_file('data_batch_1')

print(data.keys())

输出结果如下：

dict_key3(['batch_label', 'labels', 'data', 'filenames'])

具

体说明如下。

·batch_label：对应的值是

一个字符串，用来表明当

前文件的一些基本信息

。

·labels：对应的值是一个长度为

10000的列表，每个

数字取值范

围为0～9，代表当前图片所属

的类别。

·data：10000×3072的二维数组，每一

行代表一幅图

片的像素

值。

·filenames：长度为10000的列表，里面每

一项是代

表图片文件名

的字符串。

完整的数据读

取函数如下：

【程序7-1】

import pickle

import numpy

as np

import os

def get_cifar10_train_data_and_label(root=""):

def load_file(filename):

with open(filename, 'rb') as

fo:

data = pickle.load(fo, encoding='lat

in1')

return data

data_batch_1 = load_file(os.path.join(root, 'data_ba

tch_1'))

data_batch_2 = load_file(os.path.join(root, 'data_ba

tch_2'))

data_batch_3 = load_file(os.path.join(root, 'data_ba

tch_3'))

data_batch_4

= load_file(os.path.join(root, 'data_ba

tch_4'))

data_batch_5 =

load_file(os.path.join(root, 'data_ba

tch_5'))

dataset = []

labelset = []

for data in

[data_batch_1, data_batch_2, data_batc

h_3, data_batch_4,

data_batch_5]:

img_data = (data["data"])

img_label = (data["labels"])

dataset.append(img_data)

labelset.append(img_label)

dataset = np.concatenate(dataset)

labelset

= np.concatenate(labelset)

return dataset, labelset

def

get_cifar10_test_data_and_label(root=""):

def load_file(filename):

with open(filename, 'rb')

as fo:

data = pickle.load(fo, encoding='lat

in1')

return data

data_batch_1 = load_file(os.path.join(root,

'test_ba

tch'))

dataset = []

labelset

= []

for data in [data_batch_1]:

img_data = (data["data"])

img_label = (data["labels"])

dataset.append(img_data)

labelset.append(img_label)

dataset = np.concatenate(dataset)

labelset

= np.concatenate(labelset)

return dataset, labelset

def

get_CIFAR10_dataset(root=""):

train_dataset, label_dataset = get_cifar10_train_dat

a_and_label(root=root)

test_dataset, test_label_dataset =

get_cifar10_train_data_and_label(root=root)

return train_dataset,

label_dataset, test_dataset,

test_label_dataset

if __name__ ==

"__main__":

train_dataset, label_dataset, test_dataset, test_lab

el_dataset

=

get_CIFAR10_dataset(root="../dataset/cifar-10-batches-py/")

train_dataset = np.reshape(train_dataset,

[len(train_dataset),3,32,32]).

astype(np.float32)/255.

test_dataset = np.reshape(test_dataset,

[len(test_dataset),3,32,32]).

astype(np.float32)/255.

label_dataset = np.array(label_dataset)

test_label_dataset = np.array(test_label_dataset)

其中的

root是下载数据解压后的目

录参数，os.join函

数将其组合成

数据文件的位置。最终返

回训练文件和

测试文件

以及它们对应的label。需要说

明的是，提取

出的文件数

据格式为[-1,3072]，因此需要重新

对数据

维度进行调整，使

之适用于模型的输入。

7.2.2

基

于ResNet的CIFAR-10数据集分类

前面对

ResNet模型以及CIFAR-10数据集进行了

介绍，

本小节开始使用前

面定义的ResNet模型进行分类

任

务。

7.2.1节已经介绍了CIFAR-10数据

集的基本构成，并讲

解了

ResNet的基本模型结构，接下来

直接导入对应的

数据和

模型即可。完整的模型训

练如下：

import torch

import resnet

import

get_data

import numpy as np

train_dataset,

label_dataset, test_dataset, test_l

abel_dataset =

get_data.get_CIFAR10_dataset(root="../dataset/cifar-10-

batches-py/")

train_dataset = np.reshape(train_dataset,

[len(train_dataset),3,32,32]).

astype(np.float32)/255.

test_dataset = np.reshape(test_dataset,

[len(test_dataset),3,32,32]).

astype(np.float32)/255.

label_dataset

= np.array(label_dataset)

test_label_dataset = np.array(test_label_dataset)

device

= "cuda" if torch.cuda.is_available() els

e

"cpu"

model = resnet.resnet18()

#导入Unet模型

model

= model.to(device)

#将计算

模型传入GPU硬件等待计算

model = torch.compile(model) #

PyTorch 2.0的特性，加速计算速度

optimizer = torch.optim.Adam(model.parameters(), l

r=2e-5) #设

定优化函数

loss_fn = torch.nn.CrossEntropyLoss()

batch_size

= 128

train_num = len(label_dataset)//batch_size

for

epoch in range(63):

train_loss = 0.

for i in range(train_num):

start =

i * batch_size

end = (i

+ 1) * batch_size

x_batch =

torch.from_numpy(train_dataset[star

t:end]).to(device)

y_batch = torch.from_numpy(label_dataset[star

t:end]).to(device)

pred = model(x_batch)

loss = loss_fn(pred,

y_batch.long())

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss +=

loss.item() # 记录每个批

次

的损失值

# 计算并打印

损失值

train_loss /= train_num

accuracy = (pred.argmax(1)

== y_batch).type(torch.f

loat32).sum().item() /

batch_size

#2048可根据读者GPU显存

大小调整

test_num = 2048

x_test = torch.from_numpy(test_dataset[:test_num]).to

(device)

y_test = torch.from_numpy(test_label_dataset[:test_nu

m]).to(device)

pred

= model(x_test)

test_accuracy = (pred.argmax(1) ==

y_test).type(tor

ch.float32).sum().item()

/ test_num

print("epoch：",epoch,"train_loss:", round(train_loss,

2),

";accuracy:",round(accuracy,2),";test_accuracy:",round(test_a

ccuracy,2))

在这里使用训

练集数据对模型进行训

练，之后使用测

试集数据

对其输出进行测试，训练

结果如图7-12所

示。

图7-12 训练结

果

可以看到，经过5轮训练

后，模型在训练集的准确

率达

到0.99，而在测试集的准

确率也达到了0.98，这是一

个

较好的成绩，模型的性能

达到较高水平。

其他层次

的模型请读者自行尝试

，根据不同的硬件设

备，模

型的参数和训练集的batch_size都

需要做出调

整，具体数值

读者可以根据需要进行

设置。

7.3 本章小结

本章是一

个起点，让读者站在巨人

的肩膀上，从冠军

开始！

ResNet通

过“直连”和“模块”的方法开

创了一个时

代，开天辟地

地改变了人们仅依靠堆

积神经网络层来

获取更

高性能的做法，在一定程

度上解决了梯度消失

和

梯度爆炸的问题。这是一

项跨时代的发明。

当简单

的堆积神经网络层的做

法失效的时候，人们开

始

采用模块化的思想设计

网络，同时在不断“加宽”

模

块的内部通道。但是当前

这些方法被挖掘穷尽后

，

有没有新的方法能够进

一步提升卷积神经网络

的效果

呢？

答案是有的，对

于深度学习来说，除了对

模型的精巧

设计以外，还

会对损失函数和优化函

数进行修正，甚

至随着对

深度学习的研究，科研人

员对深度学习有了

进一

步的了解，新的模型结构

也被提出，这在后面的

章

节中会讲解。

第8章

梅西-阿

根廷+巴西=？

——有趣的Word Embedding

Word Embedding（词嵌入

）是什么？为什么要

Word Embedding？在深入

了解前，先看几个例子：

·在

购买商品或者入住酒店

后，会邀请顾客填写相关

的评价表明对服务的满

意程度。

·使用几个词在搜

索引擎上搜索一下。

·有些

博客网站会在博客下面

标记一些内容相关的tag

标

签。

那么问题来了，这些是

怎么做到的呢？

实际上这

是文本处理后的应用，目

的是用这些文本进

行情

绪分析、同义词聚类、文章

分类和打标签。

读者在读

文章或者评论的时候，可

以准确地说出这个

文章

大致讲了什么、评论的倾

向如何，但是计算机是

怎

么做到的呢？计算机可以

匹配字符串，然后告诉用

户是否与其所输入的字

符串相匹配，但是怎么能

让计

算机在用户搜索梅

西的时候告诉用户有关

足球或者皮

耶罗的事情

呢？

Word Embedding由此诞生，它就是对文

本的数字表

示。通过其表

示和计算可以使得计算

机很容易得到如

下公式

：

梅西-阿根廷+巴西=内马尔

本章将着重介绍Word Embedding的相关

内容，首先通

过多种计算

Word Embedding的方式，循序渐进地讲解

如何获取对应的Word

Embedding，之后的

实战使用

Word Embedding进行文本分类

。

8.1 文本数据处理

无论是使

用深度学习还是传统的

自然语言处理方式，

一个

非常重要的内容就是将

自然语言转换成计算机

可

以识别的特征向量。文

本的预处理就是如此，通

过文

本分词→词向量训练

→特征词抽取这3个主要步

骤组建

能够代表文本内

容的矩阵向量。

8.1.1 数据集介

绍和数据清洗

新闻分类

数据集AG是由学术社区ComeToMyHead提

供

的，其包含从2000多不同的

新闻来源搜集的超过

1000000篇

新闻文章，用于研究分类

、聚类、信息获取

（排行、分级

、搜索）等非商业活动。在此

基础上，

Xiang Zhang为了研究需要，从

中提取了127600个样

本，其中抽

出了120000个作为训练集，7600个作

为测

试集。分为以下4类：

·World

·Sports

·Business

·Sci/Tec

数

据集一般是用CSV文件存储

的，打开后格式如图8-1

所示

。

图8-1 AG_NEWS数据集

第1列是新闻分

类，第2列是新闻标题，第3列

是新闻的

正文部分，使用

“

,

”和“.”作为断句的符号。

由于

获取的数据集是由社区

自动化存储和收集的，因

此无可避免地存有大量

的数据杂质：

Reuters - Was

absenteeism a little high\on Tue

sday

among the guys at the office?

EA Sports would like\to think it

was because "Madden

NFL 2005" came

out that day,\and

some fans of

the football simulation are rabid enough

to\take a sick day to play

it.

Reuters - A group of

technology companies\inclu

ding Texas Instruments Inc.

(TXN.N), STMicroelectronics\

(STM.PA) and Broadcom Corp.

(BRCM.O), on Thursday said

they\will propose

a new wireless networking standard u

p to 10 times\the speed of

the current generation.

因此，要对数

据进行清洗。

1. 数据的读取

与存储

数据集的存储格

式为CSV，需要按列队数据进

行读取，

代码如下。

【程序8-1】

import csv

agnews_train

= csv.reader(open("./dataset/train.csv","r"))

for line in agnews_train:

print(line)

运

行结果（局部截图）如图8-2所

示。

图8-2 Ag_news中的数据形式

读取

的train中的每行数据内容被

默认以逗号分隔，按

列依

次存储在序列不同的位

置中。为了分类方便，可

以

使用不同的数组将数据

按类别进行存储。当然，也

可以根据需要使用Pandas工具

，但是为了后续操作和

保

证运算速度，这里主要使

用Python原生函数和NumPy

进行计算

。

【程序8-2】

import csv

agnews_label = []

agnews_title = []

agnews_text = []

agnews_train = csv.reader(open("./dataset/train.csv","r"))

for line in agnews_train:

agnews_label.append(line[0])

agnews_title.append(line[1].lower())

agnews_text.append(line[2].lower())

可以看到，不同的内

容被存储在不同的数组

中，并且

为了统一，将所有

的字母统一转换成小写

，以便进行

后续的计算。

2. 文

本的清洗

文本中除了常

用的标点符号外，还包含

大量的特殊字

符，因此需

要对文本进行清洗。

文本

清洗的方法一般是使用

正则表达式，可以匹配小

写'

a

'～'

z

'、大写'A'～'Z'或者数字'0'～'9'的范围

之外的所有字符，并用空

格代替，这个方法无须指

定

所有标点符号，代码如

下：

import re

text =

re.sub(r"[^a-z0-9]"," ",text)

这里re是对应正则表达

式的Python包，字符串“^”的

意义是

求反，即只保留要求的字

符而替换非要求保留

的

字符。进一步分析可以知

道，文本清洗中除了将不

需要的符号使用空格替

换外，还会产生一个问题

，即

空格数目过多或在文

本的首尾有空格残留，会

影响文

本的读取，因此还

需要对替换符号后的文

本进行二次

处理。

【程序8-3】

import re

def

text_clear(text):

text = text.lower()

#将

文本转换成小写

text

= re.sub(r"[^a-z0-9]"," ",text) #替换非

标

准字符，^是求反操作

text

= re.sub(r" +", " ", text)

#

替

换多重空格

text = text.strip()

#取出首尾空

格

text = text.split(" ")

#对句子按空格分隔

return

text

由

于加载了新的数据清洗

工具，因此在读取数据时

可

以使用自定义的函数

，将文本信息处理后存储

，代码

如下。

【程序8-4】

import

csv

import tools

import numpy as

np

agnews_label = []

agnews_title =

[]

agnews_text = []

agnews_train =

csv.reader(open("./dataset/train.csv","r"))

for line in agnews_train:

agnews_label.append(np.float32(line[0]))

agnews_title.append(tools.text_clear(line[1]))

agnews_text.append(tools.text_clear(line[2]))

这里使用

了额外的包和NumPy函数对数

据进行处理，因

此可以获

得处理后较为干净的数

据，如图8-3所示。

图8-3 清理后的

Ag_news数据

8.1.2 停用词的使用

观察

分好词的文本集，每组文

本中除了能够表达含义

的名词和动词外，还有大

量没有意义的副词，例如

is、are、the等。这些词的存在不会给

句子增加太多

含义，反而

由于出现的频率非常高

，影响后续的词向

量分析

。因此，为了减少我们要处

理的词汇量，降低

后续程

序的复杂度，需要清除停

止词。清除停用词一

般使

用的是NLTK工具包。安装代码

如下：

conda install nltk

除了安装NLTK工具包外

，还有一个非常重要的内

容

是，仅安装NLTK工具包并不

能够使用停用词，还需要

额外下载NLTK停用词包，建议

读者通过控制端进入

NLTK，之

后运行如图8-4所示的代码

，打开NLTK的下载

控制端。

图8-4 运

行代码

打开下载控制端

，如图8-5所示。

图8-5

NLTK下载控制端

在Corpora标签下选择stopwords，单击Download按钮

下载数据。下载后验证方

法如下：

stoplist = stopwords.words('english')

print(stoplist)

stoplist将停用词获取到

一个数组列表中，打印结

果

如图8-6所示。

图8-6 停用词数

据

下面将停用词数据加

载到文本清洁器中，除此

之外，

由于英文文本的特

殊性，单词会具有不同的

变化和变

形，例如后缀'ing

'和

'

ed'可以丢弃，

'ies

'可以

用'

y

'替换，等

等。这样可能会变成不是

完整词的词

干，但是只要

将这个词的所有形式都

还原成同一个词

干即可

。NLTK中对这部分词根还原的

处理使用的函数

为：

PorterStemmer().stem(word)

整体

代码如下：

def text_clear(text):

text = text.lower()

#将文本转换成

小写

text = re.sub(r"[^a-z0-

9]"," ",text) #替换非标准字

符，^是

求反操作

text = re.sub(r" +", "

", text)

#替换多重空格

text = text.strip()

#取出首尾空格

text = text.split(" ")

text =

[word for word in text if

word not i

n stoplist] #去除停用

词

text = [PorterStemmer().stem(word) for word in

text] #还原词干部分

text.append("eos")

#添加结

束符

text =

["bos"] + text

#添加开始符

return text

这样生

成的最终结果如图8-7所示

。

图8-7 生成的数据

可以看到

，相对于未处理的文本，获

取的是一个相对

干净的

文本数据。下面对文本的

清洁处理步骤做个总

结

。

·Tokenization：将句子进行拆分，以单个

词或者字

符的形式予以

存储，文本清洁函数中的

text.split函

数执行的就是这个操

作。

·Normalization：将词语正则化，lower函数和

PorterStemmer函数做了此方面的工作

，将数据转为小

写和还原

词干。

·Rare

Word Replacement：对于稀疏性较低的

词进

行替换，一般将词频

小于5的替换成一个特殊

的

Token <UNK>。Rare Word如同噪声。故该方法可

以

降噪并减小字典的大

小。

·Add <BOS> <EOS>：添加每个句子的开始

和结束标

识符。

·Long

Sentence Cut￾Off or Short Sentence Padding：对于过长

的句

子进行截取，对于过

短的句子进行补全。

在处

理的时候由于模型的需

要，并没有完整地使用以

上步骤。读者可以在不同

的项目中自行斟酌使用

。

8.1.3 词向量训练模型Word2Vec使用介

绍

Word2Vec（见图8-8）是Google在2013年推出的一

个

NLP工具，它的特点是将所

有的词向量化，这样就可

以

定量地度量词之间的

关系，挖掘词之间的联系

。

图8-8 Word2Vec模型

用词向量来表示

词并不是Word2Vec首创的，在很久

之

前就出现了。最早的词

向量是很冗长的，它使用

的词

向量维度大小为整

个词汇表的大小，对于每

个具体的

词汇表中的词

，将对应的位置置为1。

例如

5个词组成的词汇表，词"Queen

"的

序号为2，那么

它的词向量

就是(0,1,0,0,0)(0,1,0,0,0)。同样的道

理，词"Woman

"的词

向量就是(0,0,0,1,0)

(0,0,0,1,0)。这种词向量的

编码方式一般叫作1-of￾N Representation或者

One-Hot Representation

（独热表示）。

One-Hot Representation用来表示词向

量非常简单，

但是却有很

多问题。最大的问题是词

汇表一般都非常

大，比如

达到百万级别，每个词都

用百万维的向量来

表示

基本是不可能的。而且这

样的向量其实除了一个

位置是1外，其余的位置全

部是0，表达的效率不高。

将

其使用在卷积神经网络

中会使得网络难以收敛

。

Word2Vec是一种可以解决One-Hot Representation的

方法

，它的思路是通过训练将

每个词都映射到一个较

短的词向量上来。所有的

这些词向量就构成了向

量空

间，进而可以用普通

的统计学的方法来研究

词之间的

关系。

Word2Vec具体的训

练方法主要有两部分，分

别是CBOW

模型和Skip-Gram模型。

（1）CBOW模型：CBOW模

型（Continuous

Bag-Of￾Word Model，连续词袋模型）是一个

三层神经网络，

如图8-9所示

。该模型是输入已知上下

文，输出对当前

单词的预

测。

图8-9 CBOW模型

（2）Skip-Gram模型：Skip-Gram模型与CBOW模

型正好

相反，由当前词预

测上下文，如图8-10所示。

图8-10 Skip-Gram模

型

Word2Vec更为细节的训练模型

和训练方式这里不进行

讨论。接下来主要介绍训

练一个可以使用的Word2Vec

向量

。

对于词向量的模型训练

提出了很多方法，最为简

单的

是使用Python工具包中的

Gensim包对数据进行训练。

1. 训练

Word2Vec模型

对词模型进行训练

的代码非常简单：

from

gensim.models import word2vec #导入

Gensim包

model =

word2vec.Word2Vec(agnews_text,size=64, min

_count = 0,window =

5) #设

置训练参数

model_name = "corpusWord2Vec.bin"

#模型存储

名

model.save(model_name)

#存储训练好的模型

首

先在代码中导入Gensim包，然后

Word2Vec函数根据

设定的参数对

Word2Vce模型进行训练。Word2Vec函数

的主

要参数如下：

Word2Vec(sentences, workers=num_workers, size=num_f

eatures, min_count =

min_word_count, window = context, sample =

downsampling

，iter = 5)

其中，sentences是输入

数据，workers是并行运行的

线程

数，size是词向量的维数，min_count是最

小的词

频，window是上下文窗口

大小，sample是对频繁词汇

进行

下采样设置，iter是循环的次

数。一般没有特殊

要求，按

默认值设置即可。

save函数可

以将生成的模型进行存

储供后续使用。

2. Word2Vec模型的使

用

模型的使用非常简单

，代码如下：

text = "Prediction Unit Helps

Forecast Wildfires"

text = tools.text_clear(text)

print(model[text].shape)

其中text是需要转

换的文本，同样调用text_clear函

数

对文本进行清理。之后使

用训练好的模型对文本

进

行转换。转换后的文本

内容如下：

['bos', 'predict', 'unit',

'help', 'forecast', 'wildfir'

, 'eos']

计算后的Word2Vec文本

向量实际上是一个[7,64]大小

的矩阵，部分数据如图8-11所

示。

图8-11 Word2Vec文本向量

3. 对已有的

模型补充训练

模型训练

完毕后，可以对其存储，但

是随着要训练的

文件的

增加，Gensim同样提供了持续性

训练模型的方

法，代码如

下：

from gensim.models import word2vec

#导入Gensim包

model = word2vec.Word2Vec.load('./corpusWord2Vec.bi

n') #载入存储的模

型

model.train(agnews_title,

epochs=model.epochs,

total_examples=model.corpus_count) #继续模型训练

可以看

到，Word2Vec提供了加载存储模型

的函数。之

后train函数继续对

模型进行训练，可以看到

在最初的

训练集中，agnews_text作为

初始的训练文档，而

agnews_title是后

续训练部分，这样合在一

起可以作

为更多的训练

文件进行训练。完整代码

如下所示。

【程序8-5】

import csv

import

tools

import numpy as np

agnews_label

= []

agnews_title = []

agnews_text

= []

agnews_train = csv.reader(open("./dataset/train.csv

","r"))

for line in agnews_train:

agnews_label.append(np.float32(line[0]))

agnews_title.append(tools.text_clear(line[1]))

agnews_text.append(tools.text_clear(line[2]))

print("开始训练

模型")

from gensim.models import word2vec

model = word2vec.Word2Vec(agnews_text,size=64, min

_count =

0,window =

5,iter=128)

model_name = "corpusWord2Vec.bin"

model.save(model_name)

from gensim.models import word2vec

model

= word2vec.Word2Vec.load('./corpusWord2Vec.bi

n')

model.train(agnews_title, epochs=model.epochs,

total_examples=model.corpus_count)

模型的使用已经介

绍过了，请读者自行完成

代码测

试。

对于需要训练

的数据集和需要测试的

数据集，一般建

议读者在

使用的时候一起予以训

练，这样才能够获得

最好

的语义标注。在现实工程

中，对数据的训练往往

有

着极大的训练样本，文本

容量能够达到几十甚至

上

百吉字节，因此不会产

生词语缺失的问题，在实

际工

程中只需要在训练

集上对文本进行训练即

可。

8.1.4 文本主题的提取：基于

TF-IDF

使用卷积神经网络对文

本进行分类，文本主题的

提取

并不是必需的，所以

本小节可以选学。

一般来

说，文本的提取主要涉及

以下两种：

·基于TF-IDF的文本关

键字提取。

·基于TextRank的文本关

键词提取。

当然，除此之外

，还有很多模型和方法能

够帮助进行

文本抽取，特

别是对于大文本内容。本

书由于篇幅关

系，对这方

面的内容不展开讲解，有

兴趣的读者可以

参考相

关教程。下面先介绍基于

TF-IDF的文本关键字

提取。

1. TF-IDF简介

目标文本经过文本清洗

和停用词的去除后，一般

认为

剩下的均为有着目

标含义的词。如果需要对

其特征进

行进一步的提

取，那么提取的应该是那

些能代表文章

的元素，包

括词、短语、句子、标点以及

描述其他信

息的词。从词

的角度考虑，需要提取对

文章表达贡献

度大的词

。TF-IDF各概念之间的关系如下

。

TF-IDF是一种用于资讯检索与

咨询勘测的常用加权技

术，也是一种统计方法，用

来衡量一个词对一个文

件

集的重要程度。字词的

重要性与其在文件中出

现的次

数成正比，而与其

在文件集中出现的次数

成反比。该

算法在数据挖

掘、文本处理和信息检索

等领域得到广

泛的应用

，其最常见的应用是从一

篇文章中提取文章

的关

键词。

TF-IDF的主要思想是：如果

某个词或短语在一篇文

章

中出现的频率（Term Frequency,

TF，下文简

称词

频）高，并且在其他文

章中很少出现，则认为此

词或

者短语具有很好的

类别区分能力，适合用来

分类。其

中TF表示词条在文

章中出现的频率。

逆文档

频率

（Inverse

Document Frequency, IDF）的主要思

想是：包含

某个词（Word）的文档越少，这个

词的区

分度就越大，也就

是IDF越大。

而TF-IDF的计算实际上

就是TF×IDF。

TF−IDF=词频×逆文档频率=TF×IDF

2. TF-IDF的

实现

首先是IDF的计算，代码

如下：

import math

def idf(corpus): # corpus为输入的全部语料

文本库文件

idfs =

{}

d = 0.0

# 统计词出现

次数

for doc in corpus:

d +=

1

counted = []

for word

in doc:

if not word in

counted:

counted.append(word)

if word in idfs:

idfs[word] += 1

else:

idfs[word] =

1

# 计算每个词的逆文

档值

for word in

idfs:

idfs[word] = math.log(d/float(idfs[word]))

return idfs

然后使用计算好的

idf计算每个文档的TF-IDF值：

idfs = idf(agnews_text)

#获取

计算好的文本中每个词

的idf词频

for

text in agnews_text:

#获取文档集中每

个文档

word_tfidf =

{}

for word in text:

#依次获取每个文

档中的每个词

if word in word_tfidf:

#计算每个

词的词频

word_tfidf[word]

+= 1

else:

word_tfidf[word] = 1

for word in word_tfidf:

word_tfidf[word] *=

idfs[word] #计算每个

词的

TFIDF值

计算TFIDF的完整代码如下

。

【程序8-6】

import

math

def idf(corpus):

idfs = {}

d = 0.0

# 统计词出现的次数

for doc

in corpus:

d += 1

counted

= []

for word in doc:

if not word in counted:

counted.append(word)

if word in idfs:

idfs[word] +=

1

else:

idfs[word] = 1

#

计算每个词的逆文档值

for word in idfs:

idfs[word] =

math.log(d/float(idfs[word]))

return idfs

idfs = idf(agnews_text)

#获取计算好的文本中每

个词的IDF词频，agnews_text是经过处

理

的语料库文档，在8.1.1节详细

介绍过了

for text in agnews_text:

#获取文档集中

的每个文档

word_tfidf = {}

for word

in text: #

依次获取每

个文档中的每个词

if word

in word_idf: #计

算

每个词的词频

word_tfidf[word] +=

1

else:

word_tfidf[word] = 1

for

word in word_tfidf:

word_tfidf[word] *= idfs[word]

# word_t

fidf为计算后

的每个词的TF-IDF值

values_list = sorted(word_tfidf.items(),

key=lambda

item: item[1],

reverse=True) #按value排序

values_list

= [value[0] for value in values_list]

#生

成排序后的单个文档

3. 将

重排的文档根据训练好

的Word2Vec向量建立一

个有限量

的词矩阵

这部分内容请

读者自行完成。

4.

将TF-IDF单独定

义一个类

将TF-IDF的计算函数

单独整合到一个类中，这

样方便

后续使用，代码如

下。

【程序8-7】

class TFIDF_score:

def __init__(self,corpus,model = None):

self.corpus =

corpus

self.model = model

self.idfs =

self.__idf()

def __idf(self):

idfs = {}

d = 0.0

# 统计词出现的次

数

for

doc in self.corpus:

d += 1

counted = []

for word in

doc:

if not word in counted:

counted.append(word)

if word in idfs:

idfs[word]

+

= 1

else:

idfs[word] =

1

# 计算每个词的逆文档

值

for word in

idfs:

idfs[word] = math.log(d / float(idf

s[word]))

return idfs

def __get_TFIDF_score(self, text):

word_tfidf = {}

for word in

text:

# 依次获取每个文档中

的每个词

if word in

word_tfidf:

# 计算每个词的

词频

word_tfidf[word] += 1

else:

word_tfidf[word] = 1

for word

in word_tfidf:

word_tfidf[word] *= self.idfs[word]

#

计算每个词的TF-IDF值

values_list = sorted(word_tfidf.items(), key

=lambda

word_tfidf:

word_tfidf[1], reverse=True)

#将

TF-IDF数据按重要程度从大到

小排序

return values_list

def get_TFIDF_result(self,text):

values_list = self.__get_TFIDF_score(text)

value_list

= []

for value in values_list:

value_list.append(value[0])

return (value_list)

使用方法如下：

tfidf =

TFIDF_score(agnews_text) #agne

ws_text为

获取的数据集

for line in

agnews_text:

value_list = tfidf.get_TFIDF_result(line)

print(value_list)

print(model[value_list])

其中agnews_text为从

文档中获取的正文数据

集，也可

以使用标题或者

文档进行处理。

8.1.5 文本主题

的提取：基于TextRank

本小节内容

可以选学。TextRank算法的核心思

想来源

于著名的网页排

名算法PageRank。PageRank是

Sergey Brin与Larry Page于1998年在WWW7会议

上提

出来的，用来解决链

接分析中网页排名的问

题，如图

8-12所示。在衡量一个

网页的排名时，可以根据

感觉

认为：

图8-12 PageRank算法

·当一个

网页被更多网页链接时

，其排名会更靠前。

·排名高

的网页应具有更大的表

决权，即当一个网页

被排

名高的网页所链接时，其

重要性也会相应提高。

TextRank算

法与PageRank算法类似，其将文本

拆分成

最小组成单元，即

词汇，作为网络节点，组成

词汇网

络图模型，如图8-13所

示。TextRank算法在迭代计算

词汇

权重时与PageRank算法一样，理论

上是需要计算

边权的，但

是为了简化计算，通常会

默认使用相同的

初始权

重，并且在分配相邻词汇

权重时进行均分。

图8-13

TextRank算法

1. TextRank算法前置介绍

TextRank算法用于

对文本关键词进行提取

，步骤如

下：

（1）把给定的文本

T按照完整句子进行分割

。

（2）对于每个句子，进行分词

和词性标注处理，并过

滤

掉停用词，只保留指定词

性的单词，如名词、动

词、形

容词等。

（3）构建候选关键词

图G = (V,E)，其中V为节点

集，由每个

词之间的相似度作为连

接的边值。

（4）根据下面的公

式，迭代传播各节点的权

重，直至

收敛。

对节点权重

进行倒序排序，作为按重

要程度排列的关

键词。

2. TextRank类

的实现

整体TextRank类的实现如

下。

【程序8-8】

class TextRank_score:

def __init__(self,agnews_text):

self.agnews_text = agnews_text

self.filter_list = self.__get_agnews_text()

self.win = self.__get_win()

self.agnews_text_dict = self.__get_TextRank

_score_dict()

def __get_agnews_text(self):

sentence = []

for text in self.agnews_text:

for word

in text:

sentence.append(word)

return sentence

def

__get_win(self):

win = {}

for i

in range(len(self.filter_list)):

if self.filter_list[i] not in

win

.keys():

win[self.filter_list[i]] =

set()

if

i - 5 < 0:

lindex

= 0

else:

lindex = i

- 5

for j in self.filter_list[lindex:i

+ 5]:

win[self.filter_list[i]].add(

j)

return win

def __get_TextRank_score_dict(self):

time = 0

score

= {w: 1.0 for w in

self.filter_l

ist}

while (time < 50):

for k, v in self.win.items():

s

= score[k] / len(v)

score[k] =

0

for i in v:

score[i]

+= s

time += 1

agnews_text_dict

= {}

for key in score:

agnews_text_dict[key] = score[key]

return agnews_text_dict

def

__get_TextRank_score(self, text):

temp_dict = {}

for

word in text:

if word in

self.agnews_text_dict.key

s():

temp_dict[word] = (self.agnew

s_text_dict[word])

values_list = sorted(temp_dict.items(), key=

lambda word_tfidf:

word_tfidf[1],

rev

erse=False) # 将TextRank数据按重要程

度从大到小排序

return

values_list

def get_TextRank_result(self,text):

temp_dict = {}

for word in text:

if word

in self.agnews_text_dict.key

s():

temp_dict[word] = (self.agnew

s_text_dict[word])

values_list = sorted(temp_dict.items(), key=

lambda

word_tfidf:

word_tfidf[1], reverse=False)

value_list = []

for value in values_list:

value_list.append(value[0])

return

(value_list)

TextRank是一种

能够实现关键词抽取的

方法，当然还

有其他可以

实现关键词抽取的算法

，本书不做探究。

对于本书

使用的数据集来说，对于

文本的提取并不是

必需

的。本节为选学内容，有兴

趣的同学可以自行学

习

。

8.2 更多的Word Embedding方法

——FastText和预训练词

向量

在前面我们讲解了

Word2Vec算法，这是自然语言处理

中较为常用的对字符进

行转换的方法，即将“字”转

换成“字嵌入（Word Embedding）”。

对于普通的

文本来说，供人类了解和

掌握的信息传递

方式并

不能简易地被计算机理

解，因此

Word Embedding是目前解决向计

算机传递文字信息的

最

好的方式，如图8-14所示。

图8-14

Word Embedding

随

着研究人员对Word Embedding研究的深

入和计算机

处理能力的

提高，更多、更好的方法被

提出，例如新

的FastText和使用预

训练的词嵌入模型对数

据进行处

理。

本节是对上

一节的延续，从方法上介

绍FastText的训

练和预训练词向

量的使用。

8.2.1 FastText的原理与基础

算法

相对于传统的Word2Vec计算

方法，FastText是一种更

为快速和

新的计算Word Embedding的方法，其优点

主

要有以下几个方面：

·FastText在

保持高精度的情况下加

快了训练速度和

测试速

度。

·FastText对Word

Embedding的训练更加精准。

·FastText采

用两个重要的算法，分别

是N-Gram和

Hierarchical Softmax。

1. N-Gram

相对于Word2Vec中采用的CBOW架

构，FastText采用的

是N-Gram架构，如图8-15所

示。

图8-15 N-Gram架构

其中，x,x,…,x,x表示一个

文本中的N-Gram向量，每

个特征

是词向量的平均值。这里

顺便介绍一下N-Gram

的意义。

12N−1N

N-Gram常

用的有3种：1-Gram、2-Gram和3-Gram，分别

对应一

元、二元、三元。

以“我想去成

都吃火锅”为例，对其进行

分词处理，

得到下面的数

组：["我"

，

“想”

，

“去”

，

“成”

，

“都”

，

“吃”

，

“火”

，

“锅”]。这

就是1-Gram，

分词的时候对应一

个滑动窗口，窗口大小为

1，所以每

次只取一个值。

同

理，假设使用2-Gram，就会得到[“我

想”

，

“想

去”

，

“去成”

，

“成都”

，

“都吃”

，

“吃

火”

，

“火锅”]。N-Gram模型认为词与词

之间有关系的距离

为N，如

果超过N，则认为它们之间

没有联系，所以就

不会出

现“我成”

，

“我去”这些词。

如果

使用3-Gram，就是[“我想去”

，

“想去成

”

，

“去成都”

，…]。N理论上可以设置

为任意值，但是

一般设置

成上面3个类型就够了。

2.

Hierarchical Softmax

Hierarchical Softmax即

当语料类别较多时，使用

Hierarchical Softmax(hs)减轻计算量。FastText中

的Hierarchical

Softmax利用Huffman树

实现，将词

向量作为叶子

节点，之后根据词向量构

建Huffman树，

如图8-16所示。

图8-16 Hierarchical Softmax架构

Hierarchical Softmax的

算法较为复杂，这里就不

过

多阐述了，有兴趣的读

者可以自行研究。

8.2.2 FastText训练以

及与PyTorch 2.0的协同使

用

前面介

绍了架构和理论，本小节

开始使用FastText。

这里主要介绍

中文部分的FastText处理。

1. 数据收

集与分词

为了演示FastText的使

用，构造如图8-17所示的数据

集。

图8-17 演示数据集

text中是一

系列的短句文本，以每个

逗号为一句进行

区分，一

个简单的处理函数如下

：

import jieba

jieba_cut_list = []

for line in

text:

jieba_cut = jieba.lcut(line)

jieba_cut_list.append(jieba_cut)

print(jieba_cut)

打印结果如下：

['卷积', '神经

网络', '在', '图像处理', '领

域', '获

得', '了', '极大', '成功', '，',

'其',

'结合', '特

征提取', '和', '目标', '训练',

'为', '一

体', '的', '模型', '能够', '最好',

'的', '

利

用', '已有', '的', '信息',

'对', '结果', '进

行', '反

馈', '训练',

'。']

['对于', '文本', '识

别', '的', '卷积',

'神经网

络', '来说

', '，', '同样', '也',

'是', '

充分利用', '特征

提取', '时', '提取',

'的', '文本', '特

征

', '来', '计算',

'文本', '特征', '权

值', '大

小', '的',

'，', '归一化', '处理', '需要', '处

理', '的',

'数据', '。']

['这样', '使得', '原来

', '的',

'文本', '信

息', '抽象', '成', '一个

',

'向', '量化', '的

', '样本', '集',

'，', '之后', '将

', '样

本', '集',

'和', '训练', '好', '的', '模板

', '输入

', '卷积', '神经网络', '进行

', '处理', '。']

['本节', '将', '在', '上', '一节', '的

',

'基

础', '上', '使用', '卷积', '神经网

络',

'实

现', '文本', '分类', '的', '问题

',

'，', '这

里', '将', '采用', '两种',

'主要', '基

于', '字符

', '的', '和',

'基于', 'word', ' ', 'embedding', '形

式', '的

', '词', '卷积', '神经网络', '

处理', '方

法', '。']

['实际上', '无论是', '基于',

'字

符', '的', '还

是', '基于', 'word',

' ', 'embedding', '

形式', '的',

'处

理', '方式', '都', '是', '可以', '相

互', '转

换', '的', '，', '这里', '只',

'

介绍', '使用', '基

本', '的', '使用',

'模型', '和', '方

法', '，', '更

',

'多', '的', '应用', '

还', '需要',

'读者', '自

行', '挖掘', '和', '设

计',

'。']

可以看到

，其中每一行都根据jieba的分

词模型进行分

词处理，之

后保存在每一行中的是

已经被分过词的数

据。

2. 使

用Gensim中的FastText进行词嵌入计算

gensim.models中除了包含前文介绍过

的Word2Vec函数

外，还包含FastText的专用

计算类，代码如下：

from gensim.models import FastText

model

=

FastText(min_count=5,vector_size=300,window=7,workers=10,epoc

hs=50,seed=17,sg=1

,hs=1)

其中FastText的

参数定义如下。

·sentences

(iterable of iterables, optiona

l)：供训练的

句子，可以使用简单的列

表，但是对于

大语料库，建

议直接从磁盘／网络流迭

代传输句子。

·vector_size (int, optional) ：词向量的维

度。

·window (int,

optional)：一个句子中当前单词

和被预测单词的最大距

离。

·min_count (int, optional)：忽略词频小于此值

的

单词。

·workers

(int, optional)：训练模型时使用的

线

程数。

·sg ({0, 1},

optional)：模型的训练算法

，1代

表Skip-Gram，0代表CBOW。

·hs ({0, 1}, optional)：1采用

Hierarchical Softmax训练模型

，0采用负采样模

型。

·epochs：模型迭

代的次数。

·seed (int,

optional)：随机数发生器

种子。

在定义的FastText类中，依次

设定了最低词频度、单

词

训练的最大距离、迭代数

以及训练模型等。完整训

练例子如下。

【程序8-9】

text =

[

"卷积神

经网络在图像处理领域

获得了极大成功，其结合

特征提

取和目标训练为

一体的模型能够最

好地

利用已有的信息对结果

进行反馈训练。",

"对于文本

识别的卷积神经网络来

说，同样也是充分利用特

征提

取时提取的文本特

征来计算文本特

征权值

大小的，归一化处理需要

处理的数据。",

"这样使得原

来的文本信息抽象成一

个向量化的样本集，之后

将

样本集和训练好的模

板输入卷积神

经网络进

行处理。",

"本节将在上一节

的基础上使用卷积神经

网络实现文本分类的问

题，这里将采用基于字符

的和基于

Word

Embedding形式的词卷积

神经网络处理方法。",

"实际

上基于字符的和基于Word Embedding形

式的处理方式是

可以相

互转换的，这里只介绍基

本

的使用模型和方法，更

多的应用还需要读者自

行挖掘和设计。"

]

import jieba

jieba_cut_list = []

for

line in text:

jieba_cut = jieba.lcut(line)

jieba_cut_list.append(jieba_cut)

print(jieba_cut)

from gensim.models import FastText

model =

FastText(min_count=5,vector_size=300,window=7,workers=10,epoc

hs=50,seed=17,sg=1

,hs=1)

model.build_vocab(jieba_cut_list)

model.train(jieba_cut_list, total_examples=model.cor

pus_count,

epochs=model.epochs) #这里使用

作者给

出的固定格式即

可

model.save("./xiaohua_fasttext_model_jieba.model")

model中的build_vocab函数是对数据建

立词库，而

train函数是对model模型

训练模式的设定，这里使

用

作者给出的格式即可

。

最后训练好的模型被存

储在models文件夹中。

3.

使用训练

好的FastText模型进行参数读取

使用训练好的FastText进行参数

读取很方便，直接载

入训

练好的模型，之后将带测

试的文本输入即可，代

码

如下：

from gensim.models import

FastText

model = FastText.load("./xiaohua_fasttext_model_jie

ba.model")

embedding

= model.wv["设

计"] #"设计"这个词在

text 中出现，并经过

jieba分词后得

到

模型的训练

print(embedding)

与训练过

程不同的是，这里FastText使用自

带的load

函数将保存的模型

载入，之后使用类似于传

统的list

方式将已训练过的

值打印出来，结果如图8-18所

示。

图8-18

打印结果

注意：FastText的模

型只能打印已训练过的

词向量，

而不能打印未经

过训练的词，在图8-19中，模型

输出

的值，是已经过训练

的“设计”这个词。

打印输出

值的维度如下：

print(embedding.shape)

具体读者

自行决定。

4. 继续对已有的

FastText模型进行词嵌入训练

有

时需要在训练好的模型

上继续进行词嵌入训练

，可

以利用已训练好的模

型或者利用计算机碎片

时间进行

迭代训练。理论

上，数据集内容越多，训练

时间越

长，则训练精度越

高。

from gensim.models import FastText

model

= FastText.load("./xiaohua_fasttext_model_jieba.model

")

#embedding = model.wv["设计"]

#"设计"这个词经过

预训练

model.build_vocab(jieba_cut_list, update=True)

model.train(jieba_cut_list, total_examples=model.corpus_coun

t,

epochs=6)

model.min_count = 10

model.save("./xiaohua_fasttext_model_jieba.model")

在这里需要额外

设置一些model的参数，读者仿

照作者

写的格式即可。

5. 提

取FastText模型的训练结果作为

预训练词的嵌

入数据（读

者一定要注意位置对应

关系）

训练好的FastText模型可以

作为深度学习的预训练

词

嵌入模型中使用，相对

于随机生成的向量，预训

练的

词嵌入数据带有部

分位置和语义信息。

获取

预训练好的词嵌入数据

的代码如下：

def get_embedding_model(Word2VecModel):

vocab_list =

[word for word in Word2VecModel.wv.k

ey_to_index]

# 存储所有的

词语

word_index = {" ":

0} # 初始

化 '[word :

token]' ，后期 tokenize 语料库

使用的

就是该词典

word_vector

= {} # 初始

化'[word : vector]'字典

# 初始化存储所有

向量的大矩阵，留意其中

多了一位（首

行），词向量全

为 0，用于 Padding补

零

# 行数为所有

单词数+1，比如 10000+1；列数为词向

量维

度，比如100

embeddings_matrix =

np.zeros((len(vocab_list) + 1,

Word2VecModel.vector_size))

## 填充上述的

字典和大矩阵

for i in range(len(vocab_list)):

word =

vocab_list[i]

# 每个词语

word_index[word] = i +

1

# 词语：序号

word_vector[word] = Word2VecModel.wv[word]

# 词语：词向量

embeddings_matrix[i + 1] =

Word2VecModel.wv

[word] # 词

向量矩阵

#这里的word_vector 数据量

较大时不好打印

return word_index, word_vector, embeddings_matrix #

word_index和embeddings_

matrix的作

用在下文阐述

在示例代

码中，首先通过迭代方法

获取训练的词库列

表，之

后建立字典，使得词和序

列号一一对应。

返回值是

3个数值，分别是word_index、word_vector

和embeddings_matrix，这里word_index是

词的序列，

embeddings_matrix是生成的与词

向量表对应的

embedding矩阵，在这

里需要注意的是，实际上

embedding可以根据传入的数据的

不同对其位置进行修

正

，但是此修正必须伴随word_index一

起进行位置改

变。

输出的

embeddings_matrix由下列函数完成：

import torch

embedding = torch.nn.Embedding(num_embeddings= embeddings_

matrix.shape[0],

embedding_dim=embeddings_matrix.shape[1])

embedding.weight.data.copy_(torch.tensor(embeddings_matrix))

在这里

训练好的embeddings_matrix被作为参数传

递给

embedding列表，在这里读者只

需要遵循这种写法即

可

。

另外，PyTorch的embedding中进行look_up查询时，传

入的是每个字符的序号

，因此需要一个编码器将

字符

编码为对应的序号

。

# 使用tokenizer对文本进行序列化

处理，并返回每个句子所

对应的

词语索引

# 这个只

能对单个字使用，对词语

进行切词的时候无法处

理

def

tokenizer(texts, word_index):

token_indexs = []

for

sentence in texts:

new_txt = []

for word in sentence:

try:

new_txt.append(word_index[wor

d]) # 把句子中的词语转换

为index

except:

new_txt.append(0)

token_indexs.append(new_txt)

return token_indexs

tokenizer函数用于对单词进行

序列化，这里根据上文

生

成的word_index对每个词语进行编

号。具体应用请

读者参考

前面的内容自行尝试。

8.2.3

使

用其他预训练参数生成

PyTorch 2.0词嵌

入矩阵（中文）

无论是

使用Word2Vec还是FastText作为训练基础

都是

可以的。但是对于个

人用户或者规模不大的

公司机构

来说，做一个庞

大的预训练项目是一个

费时费力的工

程。

他山之

石，可以攻玉。我们可以借

助其他免费的训练

好的

词向量作为使用基础，如

图8-19所示。

图8-19 预训练词向量

在中文部分较为常用并

且免费的词嵌入预训练

数据为

腾讯的词向量，地

址如下：

https://ai.tencent.com/ailab/nlp/embedding.html

下载页面如图8-20所

示。

图8-20 腾讯的词向量下载

页面

可以使用以下代码

载入预训练模型进行词

矩阵的初始

化：

from gensim.models.word2vec import KeyedVectors

wv_from_text =

KeyedVectors.load_word2vec_format(file, bin

ary=False)

接下来的

步骤与8.2.2节相似，读者可以

自行编写完

成。

8.3

针对文本

的卷积神经网络模型简

介——字符卷

积

卷积神经网

络在图像处理领域获得

了极大成功，其结

合特征

提取和目标训练为一体

的模型能够最好地利用

已有的信息对结果进行

反馈训练。

对于文本识别

的卷积神经网络来说，同

样是充分利用

特征提取

时提取的文本特征来计

算文本特征权值大小

的

，归一化处理需要处理的

数据。这样使得原来的文

本信息抽象成一个向量

化的样本集，之后将样本

集和

训练好的模板输入

卷积神经网络进行处理

。

本节将在8.2节的基础上使

用卷积神经网络实现文

本分

类的问题，这里将采

用两种方式，分别是基于

字符和

基于词的卷积神

经网络模型进行处理。实

际上，基于

字符的和基于

Word

Embedding形式的处理方式是可以

相互转换的，这里只介绍

基本的使用模型和方法

，更

多的应用需要读者自

行挖掘和设计。

8.3.1 字符（非单

词）文本的处理

本小节将

介绍基于字符的卷积神

经网络处理方法。基

于单

词的卷积神经网络处理

内容将在8.3.2节介绍，这

样读

者可以循序渐进地学习

。

任何一个英文单词都是

由字母构成的，因此可以

简单

地将英文单词拆分

成字母的表示形式：

hello –> ["h","e","l","l","o"]

这样

可以看到一个单词“hello”被人

为地拆分成

“h”“e”“l”“l”“o”这5个字母。而

对于Hello

的处理有两种方式

，即One-Hot方式和

Word Embedding方式。这样的话

，

“hello”这个单

词就被转换成一

个[5,n]大小的矩阵，本例中采

用One￾Hot的方式进行处理。

使用

卷积神经网络计算字符

矩阵，对于每个单词拆分

成的数据，根据不同的长

度对其进行卷积处理，以

提

取出高层抽象概念。这

样做的好处是不需要使

用预训

练好的词向量和

语法句法结构等信息。除

此之外，字

符级的模型还

有一个好处就是很容易

推广到所有语

言。使用卷

积神经网络处理字符文

本分类的原理如图

8-21所示

。

图8-21 使用卷积神经网络处

理字符文本分类

使用卷

积神经网络处理字符文

本分类的具体步骤如

下

。

1.

标题文本的读取与转换

对于AG_NEWS数据集来说，每个分

类的文本条例既有对

应

的分类，也有标题和文本

内容，对于文本内容的提

取在8.1节的选学内容中也

有介绍，这里直接使用标

题

文本的方法进行处理

，如图8-22所示。

图8-22 AG_NEWS标题文本

读

取标题和label的程序请读者

参考8.1节的内容自行

完成

。由于只是对文本标题进

行处理，因此在进行数

据

清洗的时候不用处理停

用词和进行词干还原，对

于

空格，由于是进行字符

计算，因此不需要保留空

格，

直接将其删除即可。完

整代码如下：

def text_clearTitle(text):

text = text.lower()

#将文本转换

成小写

text =

re.sub(r"[^a-z]"," ",text) #替换非标准字

符

，^是求反操作

text =

re.sub(r" +", " ", text) #替换多

重空

格

text = text.strip()

#取出首尾空格

text

= text + " eos"

#添加结

束符，请注意，eos前面有一个

空格

return text

这样获取的结果如

图8-23所示。

图8-23 AG_NEWS标题文本抽取

结果

可以看到，不同的标

题被整合成一系列可能

对人类来

说没有任何表

示意义的字符。

2. 文本的One-Hot处

理

下面对生成的字符串

进行One-Hot处理，处理方式非常

简单，首先建立一个26个字

母的字符表：

alphabet_title =

"abcdefghijklmnopqrstuvwxyz"

根据每个字

符在字符表中出现的位

置序号进行编号，

建立一

个长度为26，值均为0的序列

，根据字符对应的

序号将

序列中对应位置设置为

1，其他位置依旧为0。

例如字

符“c”在字符表中第3个，那么

获取的字符矩

阵为：

[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]

其他

的类似，代码如下：

def get_one_hot(list):

values =

np.array(list)

n_values = len(alphabet_title) + 1

return np.eye(n_values)[values]

这段代

码的作用是将生成的字

符序列转换成矩阵，如

图

8-24所示。

图8-24 字符序列转换成

矩阵示意图

下一步将字

符串按字符表中的顺序

转换成数字序列，

代码如

下：

def get_char_list(string):

alphabet_title =

"abcdefghijklmnopqrstuvwxyz"

char_list = []

for char

in string:

num = alphabet_title.index(char)

char_list.append(num)

return char_list

这样生成的结果如下

：

hello -> [7,

4, 11, 11, 14]

将代码段整合在一起，最

终结果如下：

def

get_one_hot(list,alphabet_title = None):

if alphabet_title ==

None:

#设置字符集

alphabet_title = "abcdefghijklmnopqrstuvwxy

z"

else:alphabet_title

= alphabet_title

values = np.array(list)

#获取字符数列

n_values = len(alphabet_title) + 1 #获取字

符

表长度

return np.eye(n_values)[values]

def get_char_list(string,alphabet_title =

None):

if alphabet_title == None:

alphabet_title

= "abcdefghijklmnopqrstuvwxy

z"

else:alphabet_title = alphabet_title

char_list = []

for char in

string:

#获取字符串中的

字符

num = alphabet_title.index(char) #获取对

应位置

char_list.append(num)

#组合

位置编码

return char_list

#主代码

def get_string_matrix(string):

char_list = get_char_list(string)

string_matrix

= get_one_hot(char_list)

return string_matrix

这样生

成的结果如图8-25所示。

图8-25

转

换字符串并做one_hot处理

可以

看到，单词“hello”被转换成一个

[5,26]大小的

矩阵，供下一步处

理。但是这里产生了一个

新的问

题，对于不同长度

的字符串，组成的矩阵行

长度是不

同的。虽然卷积

神经网络可以处理具有

不同长度的字

符串，但是

在本例中还是以相同大

小的矩阵作为数据

输入

进行计算。

3. 生成文本的矩

阵的细节处理——矩阵补全

根据文本标题生成One-Hot矩阵

，而第2步中的矩阵生

成One-Hot矩

阵函数，读者可以自行将

其变更成类使

用，这样能

够在使用时更为简易和

便捷。此处将使用

单独的

函数，也就是将第2步写的

函数引入使用。

import csv

import numpy as np

import tools

agnews_title = []

agnews_train

= csv.reader(open("./dataset/train.csv","r"))

for line in agnews_train:

agnews_title.append(tools.text_clearTitle(line[1]))

for title in agnews_title:

string_matrix

= tools.get_string_matrix(title)

print(string_matrix.shape)

打印结果

如图8-26所示。

图8-26 补全后的矩

阵维度

可以看到，生成的

文本矩阵被整形成一个

有一定大小

规则的矩阵

输出。但是这里出现了一

个新的问题，对

于不同长

度的文本，单词和字母的

多少并不是固定

的，虽然

对于全卷积神经网络来

说，输入的数据维度

可以

不用统一和固定，但是这

里还是对其进行处理。

对

于不同长度的矩阵的处

理，一个简单的思路就是

将

其进行规范化处理，即

长的截短，短的补长。这里

的

思路也是如此，代码如

下：

def get_handle_string_matrix(string,n = 64):

#

n为设定的长度，可以根

据需要修正

string_length= len(string)

#获取字符串

长度

if

string_length > 64:

#判断是否大于64

string =

string[:64]

#长度

大于64的字符串予以截短

string_matrix = get_string_matrix(string)

#获取文本矩阵

return

string_matrix

else:

#对于长度

不够的字符串

string_matrix = get_string_matrix(string)

#获取字符

串矩阵

handle_length = n - string_length

#获取需要补全的

长度

pad_matrix = np.zeros([handle_length,28])

#使用全0矩阵进行补

全

string_matrix

= np.concatenate([string_matrix

,pad_matrix],axis=0) #将

字符矩阵和全0矩阵

进行叠加，将全0矩阵叠加

到字符矩阵后面

return

string_matrix

代码分

成两部分，首先对不同长

度的字符进行处理，

对于

长度大于64的字符串只保

留前64个字符，64是人

为设定

的大小，也可以根据需要

自由修改。

而对于长度不

到64的字符串，则需要将其

进行补全，

补全的方法采

用后端补零的方式，即在

序列后端加上

序号0或者

特定的“填充字符”

，补全的

长度为设定的

长度与字

符串本身长度的差值。

这

样经过修饰的代码如下

：

import csv

import numpy as np

import tools

agnews_title = []

agnews_train = csv.reader(open("./dataset/train.csv","r"))

for line in agnews_train:

agnews_title.append(tools.text_clearTitle(line[1]))

for

title in agnews_title:

string_matrix = tools.

get_handle_string_matrix (

title)

print(string_matrix.shape)

打印结果如图8-27所示。

图8-27

标

准化补全后的矩阵维度

4. 标签的One-Hot矩阵构建

对于分

类的表示，这里同样可以

使用矩阵的One-Hot方

法对其进

行分类重构，代码如下：

def get_label_one_hot(list):

values = np.array(list)

n_values = np.max(values)

+ 1

return np.eye(n_values)[values]

仿

照文本的One-Hot函数，根据传进

来的序列化参数对

列表

进行重构，形成一个新的

One-Hot矩阵，从而能够

反映出不

同的类别。

5. 数据集的构建

通过准备文本数据集，对

文本进行清洗，去除不相

干

的词，提取主干，并根据

需要设定矩阵维度和大

小，

全部代码如下（tools工具包

即整合后的上文介绍的

文

字处理工具集）：

import csv

import numpy as np

import tools

agnews_label = []

#空标签

列表

agnews_title = []

#空文本标题文档

agnews_train =

csv.reader(open("./dataset/train.csv

","r")) #读

取数据集

for line in

agnews_train:

#分行迭代文本

数据

agnews_label.append(np.int(line[0]))

#将标签读入标签列

表

agnews_title.append(tools.text_clearTitle(line[1])) #

将文本读入

train_dataset = []

for title

in agnews_title:

string_matrix = tools.get_handle_string_matrix(title)

#构建文本

矩阵

train_dataset.append(string_matrix) #将创建好的文本

One-Hot矩

阵添加到训练数据

集中

train_dataset = np.array(train_dataset)

#将原生的

训练列表转换

成NumPy格式

label_dataset = tools.get_label_one_hot(agnews_labe

l)

#将label列表转换成

One-Hot格

式

这里首先通过CSV库获取

全文本数据，之后逐行将

文本

和标签读入，分别将

其转换成One-Hot矩阵后，利用

NumPy库

将对应的列表转换成NumPy格

式，结果如图8-

28所示。

图8-28 标准

化转换后的AG_NEWS

这里分别生

成了训练集的数量数据

和标签数据的One￾Hot矩阵列表

，训练集的维度为[12000,64,28]，第一

个

数字是总的样本数，第2个

和第3个数字为生成的矩

阵维度。

标签数据是一个

二维矩阵，12000是样本的总数

，5是

类别。这里读者可能会

提出疑问，明明只有4个类

别，

为什么会出现5个？因为

One-Hot是从0开始的，而标签

的分

类是从1开始的，所以会自

动生成一个0的标签，

这一

点请读者自行处理。全部

tools函数实现代码如

下，读者

可以将其改成类的形式

进行处理。

import re

import csv

#rom nltk.corpus import stopwords

from nltk.stem.porter import PorterStemmer

import numpy

as np

#对英文文本进

行数据清洗

#stoplist = stopwords.words('english')

def text_clear(text):

text = text.lower()

#将文本转换

成小写

text = re.sub(r"[^a-z]"," ",text) #替换非标准

字符

，^是求反操作

text = re.sub(r" +", " ",

text) #替换

多重空

格

text = text.strip()

#取出首尾空格

text = text.split(" ")

#text

= [word for word in text

if word not in

stoplist]#去除停

用词

text

= [PorterStemmer().stem(word) for word in te

xt] #还原词干部分

text.append("eos")

#添加

结束符

text =

["bos"] + text

#添加开始符

return text

#对标

题进行处理

def text_clearTitle(text):

text = text.lower()

#将文本转换

成小写

text = re.sub(r"[^a-z]"," ",text) #替换非标准

字符

，^是求反操作

text = re.sub(r" +", "

", text) #替换

多重空

格

#text =

re.sub(" ", "", text) #替换

隔断空格

text = text.strip()

#取出首

尾空格

text =

text + " eos"

#添加结束符

return

text

#生成

标题的One-Hot标签

def get_label_one_hot(list):

values =

np.array(list)

n_values = np.max(values) + 1

return np.eye(n_values)[values]

#生成文本的

One-Hot矩阵

def get_one_hot(list,alphabet_title =

None):

if alphabet_title == None:

#设置字符集

alphabet_title = "abcdefghijklmnopqrstuvwxyz

"

else:alphabet_title =

alphabet_title

values = np.array(list)

#获取字

符数列

n_values

= len(alphabet_title) + 1

#获取字符表长度

return np.eye(n_values)[values]

#获取文本在词典中的位

置列表

def get_char_list(string,alphabet_title = None):

if

alphabet_title == None:

alphabet_title = "abcdefghijklmnopqrstuvwxyz

"

else:alphabet_title = alphabet_title

char_list =

[]

for char in string:

#获取字符串中的

字符

num = alphabet_title.index(char)

#获取对应位置

char_list.append(num)

#组合

位置编码

return char_list

#生成文本矩阵

def get_string_matrix(string):

char_list =

get_char_list(string)

string_matrix = get_one_hot(char_list)

return string_matrix

#获取补全后的文本矩阵

def get_handle_string_matrix(string,n = 64):

string_length= len(string)

if string_length > 64:

string =

string[:64]

string_matrix = get_string_matrix(string)

return string_matrix

else:

string_matrix = get_string_matrix(string)

handle_length =

n - string_length

pad_matrix = np.zeros([handle_length,28])

string_matrix = np.concatenate([string_matrix

,pad_matrix],axis=0)

return string_matrix

#获取数据集

def get_dataset():

agnews_label = []

agnews_title = []

agnews_train = csv.reader(open("../dataset/ag_news数

据集

/dataset/train.csv","r"))

for line in agnews_train:

agnews_label.append(np.int(line[0]))

agnews_title.append(text_clearTitle(line[1]))

train_dataset = []

for

title in agnews_title:

string_matrix = get_handle_string_matrix(titl

e)

train_dataset.append(string_matrix)

train_dataset = np.array(train_dataset)

label_dataset

= get_label_one_hot(agnews_label)

return train_dataset,label_dataset

if __name__

== '__main__':

get_dataset()

8.3.2 卷积

神经网络文本分类模型

的实现——

Conv1d（一维卷积）

对文本

的数据集处理完毕后，下

面进入基于卷积神经

网

络的分类模型设计，如图

8-29所示。

图8-29 使用卷积神经网

络处理字符文本分类

如

同图8-29所示的结构，我们根

据类似的模型设计了

一

个由5层神经网络构成的

文本分类模型，如表8-1所

示

。

表8-1 5层神经网络文本分类

模型结构层次

这里使用

的是5层神经网络，前3层是

基于一维的卷积

神经网

络，后两个全连接层用于

分类任务，代码如

下：

import torch

import einops.layers.torch as elt

def char_CNN(input_dim = 28):

model =

torch.nn.Sequential(

#第一

层卷积

elt.Rearrange("b l c ->

b c l"),

torch.nn.Conv1d(input_dim,32,kernel_size=3,pa

dding=1),

elt.Rearrange("b

c l -> b l c"),

torch.nn.ReLU(),

torch.nn.LayerNorm(32),

#第二层卷积

elt.Rearrange("b l c

-> b c l"),

torch.nn.Conv1d(32, 28,

kernel_size=3, pad

ding=1),

elt.Rearrange("b c l

-> b l c"),

torch.nn.ReLU(),

torch.nn.LayerNorm(28),

#flatten

torch.nn.Flatten(), #

[batch_size,64 * 28]

torch.nn.Linear(64 * 28,64),

torch.nn.ReLU(),

torch.nn.Linear(64,5),

torch.nn.Softmax()

)

return model

if __name__ ==

'__main__':

embedding = torch.rand(size=(5,64,28))

model =

char_CNN()

print(model(embedding).shape)

这里

是完整的训练模型，其训

练代码如下：

首先获取完

整的数据集，然后通过train_test_split

函

数对数据集进行划分，将

数据分为训练集和测试

集，而模型的计算和损失

函数的优化与前面的PyTorch

方

法类似，这里就不赘述了

。

最终结果请读者自行完

成。需要说明的是，这里的

模

型是一个较为简易的

短文本分类模型，8.4节将用

另一

种方式对这个模型

进行修正。

8.4 针对文本的卷

积神经网络模型简介——词

卷积

使用字符卷积对文

本分类是可以的，但是相

对于词来

说，字符包含的

信息并没有“词”的内容多

，即使卷

积神经网络能够

较好地对数据信息进行

学习，但是由

于包含的内

容关系，其最终效果也是

差强人意。

在字符卷积的

基础上，研究人员尝试使

用词为基础数

据对文本

进行处理，图8-30是使用CNN做词

卷积模型。

图8-30

使用CNN做词卷

积模型

在实际读写中，短

文本用于表达较为集中

的思想，由

于文本长度有

限、结构紧凑、能够独立表

达意思，因

此可以使用基

于词卷积的神经网络对

数据进行处理。

8.4.1 单词的文

本处理

首先对文本进行

处理，使用卷积神经网络

对单词进行

处理一个基

本的要求就是将文本转

换成计算机可以识

别的

数据。在8.3节的学习中，使用

卷积神经网络对字

符的

One-Hot矩阵进行了分析处理。一

个简单的想法

是，是否可

以将文本中的单词依旧

处理成One-Hot矩

阵，如图8-31所示。

图

8-31 词的One-Hot处理

使用One-Hot方法对单

词进行表示从理论上可

行，但是

事实上并不是一

种可行的方案，对于基于

字符的One￾Hot方法来说，所有的

字符会在一个相对合适

的字库中

选取，例如从26个

字母中选取，总量并不是

很多（通

常少于128个），因此组

成的矩阵也不会很大。

但

是对于单词来说，常用的

英文单词或者中文词语

一

般在5000左右，因此建立一

个稀疏的庞大的One-Hot矩

阵是

一个不切实际的想法。

目

前一个较好的解决方法

是使用Word2Vec的

Word Embedding方法，这样可以

通过学习将字库中的

词

转换成维度一定的向量

，作为卷积神经网络的计

算

依据。本小节依旧使用

文本标题作为处理的目

标。单

词的词向量的建立

步骤如下。

1. 分词模型的处

理

对读取的数据进行分

词处理，与One-Hot的数据读取类

似，首先对文本进行清理

，去除停用词和标准化文

本。需要注意的是，对于Word2Vec训

练模型来说，需

要输入若

干个词列表，因此要将获

取的文本分词转换

成数

组的形式存储。

def text_clearTitle_word2vec(text):

text =

text.lower()

#将文本转

换成小写

text = re.sub(r"[^a-z]"," ",text)

#替换非标准字

符，^是求反操作

text = re.sub(r" +", "

", text) #替换多

重

空格

text =

text.strip()

#取出首尾空格

text = text +

" eos"

#添加

结束符，注意eos前有空格

text = text.split("

") #

将

文本分词转成列表存储

return text

请读者自行验证。

2.

分词模

型的训练与载入

对分词

模型的训练与载入是基

于已有的分词数组对不

同维度的矩阵分别进行

处理的。这里需要注意的

是，

对于Word2Vec词向量来说，简单

地将待补全的矩阵用

全

0矩阵补全是不合适的，最

好的方法是将0矩阵修改

为一个非常小的常数矩

阵，代码如下：

def get_word2vec_dataset(n

= 12):

agnews_label = []

#创建标签列

表

agnews_title = []

#创建标题列表

agnews_train =

csv.reader(open("../dataset/ag_news数

据集

/dataset/train.csv","r"))

for line in agnews_train:

#将数据读取到对应列表

中

agnews_label.append(np.int(line[0]))

agnews_title.append(text_clearTitle_word2vec(li

ne[1])) #先将数据进行清

洗之

后再读取

from gensim.models import word2vec

# 导入Gensim包

model = word2vec.Word2Vec(agnews_title, vector_size=

64, min_count=0,

window=5) # 设置训

练参数

train_dataset = []

#创建训练集列表

for line in agnews_title:

#对长度进行判定

length

= len(line)

#获取列

表长度

if length >

n:

#对列表长度进行

判断

line = line[:n]

#截取需要的长度列

表

word2vec_matrix = (model.wv[line])

#获取Word2Vec矩阵

train_dataset.append(word2vec_matrix)#

将Word2Vec矩阵添加

到训练集中

else:

#补全长度不

够的操作

word2vec_matrix = (model.wv[line])

#获取Word2Vec矩阵

pad_length = n - length

#获取

需要补全的长度

pad_matrix = np.zeros([pad_length, 6

4]) +

1e-10 #创建补

全矩阵并增加

一个小数

值

word2vec_matrix = np.concatenate([wor

d2vec_matrix, pad_matrix],

axis=0) #矩阵补全

train_dataset.append(word2vec_matrix)

#将Word2Vec矩阵添加

到训练集中

train_dataset = np.expand_dims(train_dataset,3) #

对三维矩阵

进行扩展

label_dataset

= get_label_one_hot(agnews_label) #

转换成One-Hot矩阵

return train_dataset,

label_dataset

最

终结果如图8-32所示。

注意：代

码中使用了np.concatenate函数对三维

矩阵

进行扩展，在不改变

具体数值的前提下扩展

了矩阵的

维度，这样是为

接下来使用二维卷积对

文本进行分类

做数据准

备。

8.4.2 卷积神经网络文本分

类模型的实现——

Conv2d（二维卷积

）

如图8-33所示是使用二维卷

积进行文本分类任务。模

型的思想很简单，根据输

入的已转换成

Word Embedding形式的词

矩阵，通过不同的卷积提

取

不同的长度进行二维

卷积计算，将最终的计算

值进行

链接，之后经过池

化层获取不同的矩阵均

值，最终通

过一个全连接

层对其进行分类。

图8-33 使用

二维卷积进行文本分类

任务

使用模型进行完整

训练的代码如下：

import torch

import einops.layers.torch as elt

def word2vec_CNN(input_dim = 28):

model =

torch.nn.Sequential(

elt.Rearrange("b l d 1 ->

b 1 l d"),

#第一层

卷积

torch.nn.Conv2d(1,3,kernel_size=3),

torch.nn.ReLU(),

torch.nn.BatchNorm2d(num_features=3),

#第二层卷积

torch.nn.Conv2d(3, 5, kernel_size=3),

torch.nn.ReLU(),

torch.nn.BatchNorm2d(num_features=5),

#flatten

torch.nn.Flatten(), #[batch_size,64 *

28]

torch.nn.Linear(2400,64),

torch.nn.ReLU(),

torch.nn.Linear(64,5),

torch.nn.Softmax()

)

return model

"---------------下面是

模型训练部分-------------------

----------"

import get_data_84

as get_data

from sklearn.model_selection import train_test_spl

it

train_dataset,label_dataset = get_data.get_word2vec

_dataset()

X_train,X_test,

y_train, y_test =

train_test_split(train_dataset,label_dataset,test_size=0.1,

random_state=828)

#将数据集

划分为训练集和测试集

#获取device

device = "cuda" if torch.cuda.is_available() els

e "cpu"

model = word2vec_CNN().to(device)

#

定义交叉熵损失函

数

def cross_entropy(pred, label):

res =

-

torch.sum(label * torch.log(pred)) / label.shape[0]

return torch.mean(res)

optimizer = torch.optim.Adam(model.parameters(), l

r=1e-4)

batch_size = 128

train_num =

len(X_test)//128

for epoch in range(99):

train_loss

= 0.

for i in range(train_num):

start = i * batch_size

end

= (i + 1) * batch_size

x_batch =

torch.tensor(X_train[start:end]).type(torch.float32).to(devic

e)

y_batch =

torch.tensor(y_train[start:end]).type(torch.float32).to(devic

e)

pred = model(x_batch)

loss

= cross_entropy(pred, y_batch)

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item() # 记录每个批次

的损失

值

# 计算并打印损失值

train_loss /= train_num

accuracy

= (pred.argmax(1) ==

y_batch.argmax(1)).type(torch.float32).sum().item() / batch

_size

print("epoch：",epoch,"train_loss:",

round(train_loss,2),"accuracy:",round(accuracy,2))

模

型使用不同的卷积核分

别生成了3通道和5通道的

卷

积计算值，池化以后将

数据拉伸并连接为平整

结构，

之后使用两个全连

接层作为分类层进行最

终的计算并

作出预测。

通

过对模型的训练可以看

到，最终测试集的准确率

应

该在80%左右，请读者根据

配置自行完成。

8.5 使用卷积

实现文本分类的补充内

容

在前面的章节中，作者

通过不同的卷积（一维卷

积和

二维卷积）实现了文

本的分类，并且通过使用

Gensim

包掌握了对文本进行词

向量转换的方法。

Word Embedding是目前

最常用的将文本转换成

向量的

方法，适合较为复

杂的词袋中词组较多的

情况。

使用One-Hot方法对字符进

行表示是一种非常简单

的方

法，但是由于其使用

受限较大，产生的矩阵较

为稀

疏，因此实用性并不

是很强，在这里推荐使用

Word Embedding方式对词进行处理。

读者

可能会产生疑问，是否可

以使用Word2Vec的形式

来计算字

符的“字向量”？答案是完全

可以，并且相

对于单纯采

用One-Hot形式的矩阵表示，会有

更好的表

现和准确度。

对

于汉字的文本处理，一个

非常简单的方法是将汉

字

转换成拼音的形式，使

用Python提供的拼音库包：

pip install pypinyin

使用

方法如下：

from pypinyin import pinyin, lazy_pinyin,

Style

value = lazy_pinyin('你好') # 不考虑多

音字的情况

print(value)

打印结果如

下：

['ni', 'hao']

这里是不考虑多音字

的普通模式，除此之外，还

有带

拼音符号的多音字

，有兴趣的读者可以自行

学习。

较为常用的对汉字

进行文本处理的方法是

使用分词器

对文本进行

分词，将分词后的词数列

去除停用词和副

词之后

制作Word Embedding，如图8-34所示。

图8-34 使用分

词器对文本进行分词

这

是本节开始的说明，这里

对其进行分词并转换成

词

向量的形式进行处理

。

1. 读取数据

对于数据的读

取，这里为了演示，直接使

用字符串作

为数据的存

储格式，而对于多行文本

的读取，读者可

以使用Python类

库中的文本读取工具，这

里不再赘

述。

text = "在前面的章

节中，作者通过不同的卷

积（一维卷

积和二维卷积

）实现了文本的分类，并且

通过使

用Gensim掌握了对文本

进行词向量转换的方法

。词向量

Word Embedding是目前最常用的

将文本转成向量的

方法

，比较适合较为复杂词袋

中词组较多的情况。使用

One-Hot方法对

字符进行表示是

一种非常简单的方法，

但

是由于其使用受限较大

，产生的矩阵较为稀疏，因

此在实用性上并

不是很

强，作者在这里统一推荐

使用Word

Embedding的方式对词进行处

理。可能有读者会产生疑

问，如果使用

Word2Vec的形式来计

算字符的“字向量”

是否可

行。那么作者的答案是完

全可以，并且准确度相对

于单纯采用

One-Hot形式的矩阵

表示，都能有更好的

表现

和准确度。"

2. 中文文本的清

理与分词

下面使用分词

工具对中文文本进行分

词计算。对于文

本分词工

具，Python类库中最为常用的是

jieba分词，

导入如下：

import

jieba #分词器

import re #正

则表达式库包

对于正文

的文本，首先需要对其进

行清洗和去除非标

准字

符，这里采用re正则表达式

对文本进行处理，部

分处

理代码如下：

text = re.sub(r"[a-zA-Z0-9-，。“”()]"," ",text)

#替换

非标准

字符，^是求反操作

text = re.sub(r" +",

" ", text)

#替换多

重空格

text =

re.sub(" ", "", text)

#替换隔断空格

处

理好的文本如图8-35所示。

图

8-35 处理好的文本

可以看到

文本中的数字、非汉字字

符以及标点符号已

经被

删除，并且由于删除不标

准字符遗留的空格也被

一一删除了，留下的是完

整的待切分的文本内容

。

jieba库包是用于对中文文本

进行分词的工具，分词函

数如下：

text_list

= jieba.lcut_for_search(text)

使用jieba分词对文本

进行分词后，将结果以数

组的形

式存储，打印结果

如图8-36所示。

图8-36 分词后的中

文文本

3. 使用Gensim构建词向量

使用Gensim构建词向量的方法

相信读者已经比较熟

悉

，这里直接使用即可，代码

如下：

from gensim.models import

word2vec

# 导入Gensim包

# 设置训练参

数，注意方括号中的内容

model =

word2vec.Word2Vec([text_list], size=50, min_count

=1, window=3)

print(model["章节"])

有一个非常重要的

需要注意的细节，因为

word2vec.Word2Vec函

数接收的是一个二维数

组，而本

文通过jieba分词的结

果是一个一维数组，因此

需要加

上一个数组符号

人为地构建一个新的数

据结构，否则

在打印词向

量时会报错。

代码正确执

行后，等待Gensim训练完成后，打

印一个

字符的向量，如图

8-37所示。

图8-37 单个中文词的向

量

完整代码如下。

【程序8-10】

import

jieba

import re

text = re.sub(r"[a-zA-Z0-9-，。""()]","

",text) #

替

换非标准字符，^是求反操

作

text = re.sub(r"

+", " ", text)

#替换多重空格

text

= re.sub(" ", "", text)

#替换隔

断空格

print(text)

text_list = jieba.lcut_for_search(text)

from gensim.models

import word2vec

# 导入Gensim包

model =

word2vec.Word2Vec([text_list], size=50, m

in_count=1, window=3)# 设置

训练

参数

print(model["章节"])

后续工程读者

可以自行参考二维卷积

对文本处理的模

型进行

计算。

通过本实战案例的

演示读者可以看到，对于

普通的本

文完全可以通

过一系列的清洗和向量

化处理将其转换

成矩阵

的形式，之后通过卷积神

经网络对文本进行处

理

。在本实战案例中虽然只

是仅仅做了中文向量的

词

处理，缺乏主题提取、去

除停用词等操作，读者可

以

自行根据需要进行补

全。

8.6 本章小结

卷积神经网

络并不是只能对图像进

行处理，本章演示

了使用

卷积神经网络对文本进

行分类的方法。对于文

本

处理来说，传统的基于贝

叶斯分类和循环神经网

络

实现的文本分类方法

，使用卷积神经网络同样

可以实

现，而且效果并不

比前面两种分类方法差

。

卷积神经网络的应用非

常广泛，通过正确的数据

处理

和建模可以达到程

序设计人员心中所要求

的目标。更

重要的是，相对

于循环神经网络来说，卷

积神经网络

在训练过程

中的训练速度更快（并发

计算），处理范

围更大（图矩

阵），能够建立更大、更深的

特征区域

联系（感受野）。因

此，卷积神经网络在机器

学习中

越来越重要。

预训

练词向量是本章新加入

的内容，可能有读者会问

Word Embedding等价于什么？等价于把Embedding层

的网络用预训练好的参

数矩阵初始化。但是只能

初始

化第一层网络参数

，再高层的参数就无能为

力了。

而下游NLP任务在使用

Word Embedding的时候一般有

两种方法

：一种是Frozen，就是Word

Embedding那层

网络参

数固定不动；另一种是Fine-Tuning，就

是

Word Embedding这层参数使用新的训

练集合训练，也

需要跟着

训练过程更新Word Embedding。

第9章

基于

循环神经网络的中文情

感分类实战

前面带领读

者实现了图像降噪与图

像识别等方面的内

容，并

且在第8章基于卷积神经

网络完成了英文新闻分

类的工作。相信读者已经

对使用PyTorch 2.0完成项

目有了一

定的了解。

但是在前期的

学习过程中，我们主要以

卷积神经网络

为主，较少

讲述神经网络中的另一

个重要内容——循

环神经网

络

（Recurrent Neural Network,

RNN）。本章将讲

解循环神经

网络的基本理论，并以一

个基本实现GRU

（Gate Recurrent Unit）为例向读者

讲解相关的使

用方法。

9.1 实

战：循环神经网络与情感

分类

循环神经网络的目

的是处理序列数据。

在传

统的神经网络模型中，是

从输入层到隐含层再到

输出层，层与层之间是全

连接的，每层之间的节点

是

无连接的。但是这种普

通的神经网络对于很多

问题无

能为力。例如，要预

测句子的下一个单词是

什么，一

般需要用到前面

的单词，因为一个句子中

的前后单词

并不是独立

的，即一个序列当前的输

出与前面的输出

也有关

。

而循环神经网络就不存

在这种问题，因为循环神

经网

络在构建时就天然

地建立了节点之间的相

互联系。

具体的表现形式

为循环神经网络网络会

对前面的信息

进行记忆

并应用于当前输出的计

算中，即隐藏层之间

的节

点不再是无连接的，而是

有连接的，并且隐藏层

的

输入不仅包括输入层的

输出，还包括上一时刻隐

藏

层的输出。

循环神经网

络如图9-1所示。

图9-1

循环神经

网络

9.1.1 基于循环神经网络

的中文情感分类准备

在

讲解循环神经网络的理

论知识之前，先带领读者

完

成循环神经网络的情

感分类实战的准备工作

。

1.

数据的准备

首先是数据

集的准备工作，在这里完

成的是中文数据

集的情

感分类，因此准备了一个

已完成情感分类的数

据

集，读者可以参考本书配

套源码中的

dataset\cn\ChnSentiCorp.txt文件。此时读

者需要掌

握这个数据集

的读取方法，读取的代码

如下：

max_length = 80 #设置获取的文本长

度为80

labels =

[] #用以存放label

context = [] #用以存放

汉字文

本

vocab = set()

with open("../dataset/cn/ChnSentiCorp.txt",

mode="r

", encoding="utf-8") as

emotion_file:

for

line in emotion_file.readlines():

line = line.strip().split(",")

# labels.append(int(line[0]))

if int(line[0]) == 0:

labels.append(0) #由于在后面直

接

采用PyTorch自带的crossentropy函数，

因此

这里直接输入0，否则输入

[1,0]

else:

labels.append(1)

text = "".join(line[1:])

context.append(text)

for char

in text: vocab.add(char) #建

立vocab和vocab编号

voacb_list

= list(sorted(vocab))

# print(len(voacb_list))

token_list =

[]

#下面对context的内

容根据vocab进行token处理

for text in context:

token = [voacb_list.index(char) for char in

text]

token = token[:max_length] + [0]

* (max_length -

len(token))

token_list.append(token)

2.

模型的

建立

根据需求建立需要

的模型，在这里实现一个

带有单向

GRU和双向GRU的循环

神经网络，代码如下：

class RNNModel(torch.nn.Module):

def

__init__(self,vocab_size = 128):

super().__init__()

self.embedding_table =

torch.nn.Embedding(vocab_size,embedding_dim=312)

self.gru = torch.nn.GRU(312,256) # 注

意

这里的输出有两个，分别

是out与

hidden。out是序列在模型运行

后全部隐藏层的状态，而

hidden是最后

一个隐藏层的状

态

self.batch_norm = torch.nn.LayerNorm(256,256)

self.gru2 = torch.nn.GRU(256,128,bidirectiona

l=True) # 注意这里的输出有两

个，

分别是out与hidden。out是序列在模

型运行后全部隐藏层的

状态，而

hidden是最后一个隐藏

层的状态

def forward(self,token):

token_inputs =

token

embedding = self.embedding_table(token_inputs

)

gru_out,_

= self.gru(embedding)

embedding = self.batch_norm(gru_out)

out,hidden

= self.gru2(embedding)

return out

需要注意的是

，对于GRU进行神经网络训练

，无论是单

向GRU还是双向GRU，其

结果输出都是两个隐藏

层状

态，分别是out与hidden。out是序列

在模型运行后序

列全部

隐藏层的状态，而hidden是此序

列最后一个隐

藏层的状

态。

在这里使用的是两层

GRU。有读者会注意到，在对第

二

个GRU进行定义时，有一个

额外的参数

bidirectional，用于定义循

环神经网络是单向计算

还

是双向计算，其具体形

式如图9-2所示。

图9-2 循环神经

网络的具体形式

从图9-2中

可以很明显地看到，左右

两个模块分别是循

环神

经网络单向计算的输出

层，这两个方向同时作

用

，最后生成了最终的隐藏

层。

9.1.2 基于循环神经网络的

中文情感分类实现

9.1.1节完

成了循环神经网络的数

据准备以及模型的定

义

，下面完成对中文数据集

进行情感分类，完整的代

码如下：

import numpy

as np

max_length = 80 #设置获取的文本

长

度为80

labels = [] #用以存放label

context

= [] #用以存

放汉字文

本

vocab =

set()

with open("../dataset/cn/ChnSentiCorp.txt", mode="r

", encoding="utf-8")

as

emotion_file:

for line in emotion_file.readlines():

line = line.strip().split(",")

# labels.append(int(line[0]))

if

int(line[0]) == 0:

labels.append(0) #由于在后面

直接

采用PyTorch自带的crossentropy函数，

因

此这里直接输入0，否则输

入[1,0]

else:

labels.append(1)

text = "".join(line[1:])

context.append(text)

for char in text: vocab.add(char)

#建

立vocab和vocab编号

voacb_list = list(sorted(vocab))

#

print(len(voacb_list))

token_list = []

#下面对context的

内容根据vocab进行token处理

for

text in context:

token = [voacb_list.index(char)

for char in text]

token =

token[:max_length] + [0] * (max_length -

len(token))

token_list.append(token)

seed = 17

np.random.seed(seed);np.random.shuffle(token_list)

np.random.seed(seed);np.random.shuffle(labels)

dev_list = np.array(token_list[:170])

dev_labels =

np.array(labels[:170])

token_list = np.array(token_list[170:])

labels =

np.array(labels[170:])

import torch

class RNNModel(torch.nn.Module):

def

__init__(self,vocab_size = 128):

super().__init__()

self.embedding_table =

torch.nn.Embedding(vocab_size,embedding_dim=312)

self.gru = torch.nn.GRU(312,256) # 注

意

这里的输出有两个，分别

是out与

hidden。out是序列在模型运行

后全部隐藏层的状态，而

hidden是最后

一个隐藏层的状

态

self.batch_norm = torch.nn.LayerNorm(256,256)

self.gru2 = torch.nn.GRU(256,128,bidirect

ional=True) # 注意这里的输出

有两

个，分别是out与hidden。out是序列在模

型运行后全部隐藏层的

状态，而hidden是最后一个隐藏

层

的状态

def forward(self,token):

token_inputs =

token

embedding = self.embedding_table(token_inputs

)

gru_out,_

= self.gru(embedding)

embedding = self.batch_norm(gru_out)

out,hidden

= self.gru2(embedding)

return out

#这里使用顺序

模型的方式建立了训练

模型

def

get_model(vocab_size = len(voacb_list),max_len

gth = max_length):

model = torch.nn.Sequential(

RNNModel(vocab_size),

torch.nn.Flatten(),

torch.nn.Linear(2

* max_length * 128,2)

)

return

model

device = "cuda"

model =

get_model().to(device)

model = torch.compile(model)

optimizer =

torch.optim.Adam(model.parameters(), l

r=2e-4)

loss_func = torch.nn.CrossEntropyLoss()

batch_size = 128

train_length = len(labels)

for epoch in (range(21)):

train_num =

train_length // batch_size

train_loss, train_correct =

0, 0

for i in (range(train_num)):

start = i * batch_size

end

= (i + 1) * batch_size

batch_input_ids = torch.tensor(token_list[sta

rt:end]).to(device)

batch_labels =

torch.tensor(labels[start:end]

).to(device)

pred = model(batch_input_ids)

loss

= loss_func(pred, batch_labels.type(tor

ch.uint8))

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item()

train_correct +=

((torch.argmax(pred, dim=-1

) ==

(batch_labels)).type(torch.float).sum().item() /

len(batch_

labels))

train_loss /= train_num

train_correct

/= train_num

print("train_loss:", train_loss, "train_correct:", t

rain_correct)

test_pred = model(torch.tensor(dev_list).to(device))

correct =

(torch.argmax(test_pred, dim=-1) ==

(torch.tensor(dev_labels).to(device))).type(torch.float).sum(

).item() /

len(test_pred)

print("test_acc:",correct)

print("-------------------")

在这里使用了从左

到右顺序计算的方法来

建立循环神

经网络模型

，在使用GUR对数据进行计算

后，又使用

Flatten对序列embedding进行了

平整化处理。而最终

的Linear是

分类器，作用是对结果进

行分类。具体结

果请读者

自行测试查看。

9.2 循环神经

网络理论讲解

前面完成

了循环神经网络对情感

分类的实战工作，本

节开

始进入循环神经网络的

理论讲解部分，下面还是

以GRU为例向读者讲解相关

内容。

9.2.1 什么是GRU

我们在前面

的实战过程中，使用GRU作为

核心神经网络

层，GRU是循环

神经网络的一种，是为了

解决长期记忆

和反向传

播中的梯度等问题而提

出来的一种神经网络

结

构，是一种用于处理序列

数据的神经网络。

GRU更擅长

处理序列变化的数据，比

如某个单词的意思

会因

为上文提到的内容的不

同而有所不同，GRU就能够

很

好地解决这类问题。

1. GRU的输

入与输出结构

GRU的输入与

输出结构如图9-3所示。

图9-3 GRU的

输入与输出结构

tt-1

通过GRU的

输入与输出结构可以看

到，在GRU中有一个

当前的输

入x，和上一个节点传递下

来的隐状态

（Hidden

State）h，这个隐状态

包含之前节点的相

关信

息。

tt-1tt

结合x和h，GRU会得到当前隐

藏节点的输出y和传递给

下一个节点的隐状态h。

2. 门

-GRU的重要设计

t-1t

一般认为，门

是GRU能够替代传统的循环

神经网络的重

要原因。先

通过上一个传输下来的

状态h和当前节点的

输入

x来获取两个门控状态，如

图9-4所示。

图9-4 两个门控状态

其中r控制重置门控（Reset

Gate），z则控

制更新门

控（Update Gate）。σ为Sigmoid函数，通过

这个函

数可以将数据变

换为0～1范围内的数值，从而

充当门

控信号。

t

得到门控

信号之后，首先使用重置

门控来得到重置之

后的

数据，再将与输入x进行拼

接，再通过一个Tanh激

活函数

来将数据缩放到-1～1的范围

内，即得到如图9-

5所示的h'。

图

9-5 得到h′

t

这里的h'主要是包含

当前输入的x数据。有针对

性地将

h'添加到当前的隐

藏状态，相当于“记忆了当

前时刻

的状态”。

3. GRU的结构

GRU最

关键的是“更新记忆”阶段

。在这个阶段，GRU

同时进行遗

忘和记忆两个步骤，如图

9-6所示。

图9-6 更新记忆

由于使

用了先前得到的更新门

控z，从而能够获得新的

更

新，公式如下：

tt-1

h=z×h +（1-z）×h'

公式说明如

下：

t-1t-1

·z×h：表示对原本隐藏状态

的选择性“遗忘”。这

里的z可

以想象成遗忘门（Forget Gate），忘记h维

度中一些不重要的信息

。

·（1-z）×h'

：表示对包含当前节点信

息的h'进行选

择性“记忆”。与

上面类似，这里的1-z也会忘

记h'维

度中的一些不重要

的信息。或者，这里可以看

作是对

h'维度中的某些信

息进行选择。

t-1

因此，整个公

式的操作就是忘记传递

下来的h中的某些

维度信

息，并加入当前节点输入

的某些维度信息。可

以看

到，这里的遗忘z和选择（1-z）是

联动的。也就

是说，对于传

递进来的维度信息，我们

会进行选择性

遗忘，遗忘

了多少权重（z），就会使用包

含当前输入

的h'中对应的

权重弥补（1-z）的量，从而使得

GRU的

输出保持一种“恒定”状

态。

9.2.2 单向不行，那就双向

前

面简单介绍了GRU中的参数

bidirectional,

bidirectional是双向传输，其目的

是将

相同的信息以不同的方

式呈现给循环网络，以提

高精度并缓解遗忘问题

。双向GRU是一种常见的GRU变

体

，常用于自然语言处理任

务。

GRU特别依赖顺序或时间

，它按顺序处理输入序列

的时

间步，打乱时间步或

反转时间步会完全改变

GRU从序列

中提取的表示。正

因如此，如果顺序对问题

很重要

（比如室温预测等

问题），GRU的表现会很好。

双向

GRU利用了这种顺序敏感性

，每个GRU分别沿一个

方向对

输入序列进行处理（时间

正序和时间逆序）。

然后将

它们的表示合并在一起

，如图9-7所示。

图9-7 双向GRU

一般来

说，按时间正序的模型会

优于按时间逆序的模

型

。但是对于文本分类这些

问题来说，一个单词对理

解句子的重要性通常并

不取决于它在句子中的

位置，

即用正序序列和逆

序序列，或者随机打断“词

语（不

是字）”出现的位置，之

后将修正后的序列发送

给GRU

模型进行训练，性能几

乎相同，这证实了一个假

设：

虽然单词顺序对理解

语言很重要，但使用哪种

顺序并

不重要。

双向GRU还有

一个好处是，在机器学习

中，如果一种数

据表示不

同但有用，那么总是值得

加以利用，这种表

示与其

他表示的差异越大越好

，它提供了查看数据的

全

新角度，抓住了数据中被

其他方法忽略的内容，因

此可以提高模型在某个

任务上的性能。

9.3

本章小结

本章介绍了循环神经网

络的基本用途与理论定

义方

法，可以看到循环神

经网络能够较好地对序

列的离散

数据进行处理

，这是一种较好的处理方

法。但是在实

际应用中读

者应该发现了，这种模型

训练的结果差强

人意。

这

个问题目前读者不用担

心，因为每个深度学习技

术

高手都是从最基本的

内容开始学习的，后续还

要学习

更为高级的PyTorch编程

方法。

第10章

从0起步

——自然语

言处理的编码器

好吧，我

们又要从0开始了。

前面的

章节讲解了使用多种方

式对字符进行表示的方

法，例如最原始的One-Hot方法，以

及现在较常用的

Word2Vec和FastText词嵌

入等。这些都是将字符进

行

向量化处理的方法。

问

题来了，无论是使用旧方

法还是现在常用的方法

，

或者将来出现的新算法

，有没有一个统一的称谓

？答

案是有的，所有的处理

方法可以被简称为Encoder（编

码

器），其原理如图10-1所示。

图10-1 编

码器对文本进行投影

编

码器的作用是构造一种

能够存储字符（词）的若干

特征的表达方式（虽然这

个特征具体是什么我们

不知

道，但这样做就行了

），这就是前文介绍的Embedding

形式

。

本章将从一个简单的编

码器开始，首先介绍其核

心架

构，以及整体框架的

实现，并以此为基础引入

编程实

战，即一个对汉字

和拼音转换的翻译。

但是

编码器并不是简单地使

用，更重要的内容是在此

基础上引入Transformer架构的基础

概念，这是目前最

为流行

和通用的编码器架构，并

在此基础上衍生出了

更

多内容，这些内容将在第

11章详细介绍。而本章着

重

讲解通用的解码器，读者

应将其当成独立的内容

来

学习。

10.1 编码器的核心

——注

意力模型

编码器的作用

是对输入的字符序列进

行编码处理，从

而获得特

定的词向量结果。为了简

便起见，这里直接

使用Transformer的

编码器方案实现本章的

编码器，这

个也是目前最

为常用的编码器架构方

案。编码器的结

构如图10-2所

示。

图10-2 编码器的结构示意

图

从图10-2可见，编码器是由

以下模块构成的：

·初始词

向量（Input Embedding）层。

·位置编码器（Positional Encoding）层。

·多

头自注意力（Multi-Head

Attention）层。

·前馈（Feed Forward）层。

实

际上，编码器的构成模块

并不是固定的，也没有特

定的形式，Transformer的编码器架构

是目前最为常用

的，因此

本章以此为例进行介绍

。首先介绍编码器的

核心

内容：注意力模型和架构

，然后以此为主完成整

个

编码器的介绍和编写。

10.1.1 输

入层——初始词向量层和位

置编码器层

初始词向量

层和位置编码器层是数

据输入最初的层，

作用是

将输入的序列通过计算

组合成向量矩阵，如图

10-3所

示。

图10-3 输入层

下面对每一

部分依次进行讲解。

1. 初始

词向量层

如同大多数的

向量构建方法一样，首先

将每个输入单

词通过词

嵌入算法转换为词向量

。

其中每个词向量被设定

为固定的维度，本书后面

将所

有词向量的维度设

置为312。具体代码如下：

import torch

#将所

有词向量的维度设置为

312

word_embedding_table = torch.nn.Embedding(num_embed

dings=encoder_vocab_size,

embedding_dim=312)

encoder_embedding

= word_embedding_table(inputs)

首先使用torch.nn.Embedding函数创建了一

个随机初

始化的向量矩

阵，encoder_vocab_size是字库的个

数，一般在

编码器中字库是包含所

有可能出现的

“字”的集合

。而embedding_size是定义的Embedding

向量维度，这

里使用通用的312即可。

2. 位置

编码

位置编码是一个非

常重要且有创新性的结

构输入。一

般自然语言处

理使用的都是一个个连

续的长度序列，

因此为了

使用输入的顺序信息，需

要将序列对应的相

对位

置和绝对位置信息注入

模型中。

由于位置编码维

度和词向量维度均被定

义成312，所以

两者可以直接

相加。

具体来说，位置向量

的获取方式有两种：

·通过

模型训练获取。

·根据特定

公式计算获取（用不同频

率的sin和cos函

数直接计算）。

由

于位置向量有两种获取

方式，因此在实际操作中

，

我们既可以使用一个可

训练的参数矩阵，也可以

使用

一个计算好的固定

数值参数矩阵，两者作为

位置向量

的表述可以起

到同样的作用。采用固定

数值的参数矩

阵计算公

式如下：

序列中任意一个

位置都可以用三角函数

表示，pos是输

入序列的最大

长度，d设定与词向量相同

的位置312。

model

i表示由0到d长度构

成的序列中任一位置的

编号。例

如，当d=312时，构成一个

[0,1,2,…,312]的序列，那

么在计算时依

次赋值i

= 0、i = 1……等来对每

个位置

进行计算。完整代码如下

：

modelmodel

class PositionalEncoding(torch.nn.Module):

def __init__(self, d_model =

312, dropout = 0.05

, max_len=80):

"""

:param d_model: pe编码维度，一般与

Word Embedding相同，方

便相加

:param dropout: dorp out

:param max_len:

语料库中最长句

子的长度，即

Word Embedding中的L

"""

super(PositionalEncoding, self).__init__()

# 定义drop out

self.dropout = torch.nn.Dropout(p=dropout)

# 计

算pe编码

pe = torch.zeros(max_len, d_model)

# 建立

空表，每行代

表一个词的位置，每列代

表一个编码位

position = torch.arange(0,

max_len).unsqueez

e(1) # 建个arange表示

词的位

置，以便进行公式

计算，size=(max_len,1)

div_term

= torch.exp(torch.arange(0, d_model

, 2) *

# 计算公式中10000**

（2i/d_model)-(math.log(10000.0)/ d_model))

pe[:, 0::2]

= torch.sin(position * div_ter

m) #

计算偶

数维度的pe值

pe[:, 1::2] = torch.cos(position *

div_ter

m) # 计算奇数维

度的pe值

pe =

pe.unsqueeze(0) # size=

(1, L, d_model)，为了后续与word_embedding

相加

，意为batch维度下的操作相同

self.register_buffer('pe', pe) # pe值是不

参加训练的

def

forward(self, x):

# 输入

的最终编

码 =

word_embedding + positional_embedding

x = x

+ self.pe[:, :x.size(1)].clone().det

ach().requires_grad_(False)

return self.dropout(x)

# size = [batch, L

,

d_model]

这种位置编

码函数的写法过于复杂

，读者不用追究细

节直接

使用即可。最终将词向量

矩阵和位置编码组

合，如

图10-4所示。

图10-4 初始词向量

10.1.2 自

注意力层（重点）

自注意力

层不但是本节的重点，而

且是本书的重点

（然而实

际上非常简单）。

注意力层

是使用注意力机制构建

的，是能够脱离距离

的限

制建立相互关系的一种

计算机制。注意力机制最

早是在视觉图像领域提

出来的，来自2014年“谷歌大

脑

”团队的论文

Recurrent Models of Visual Attention，其在

RNN模型上

使用了注意力机制来进

行图像分类。

随后，Bahdanau等在论

文

Neural Machine Translation by

Jointly Lear

ning to Align and

Translate中使用类似于注意

力

机制在机器翻译任务上

将翻译和对齐同时进行

，这

是第一次将注意力机

制应用到NLP领域中。

接下来

，注意力机制被广泛应用

在基于RNN/CNN等神经

网络模型

的各种NLP任务中。2017年，Google机器翻

译

团队发表的Attention

is all you need中大量使

用了自注意力（Self-Attention）机制来学

习文本表

示。自注意力机

制已成为近期的研究热

点，并在各种

自然语言处

理任务上进行探索。

自然

语言中的自注意力机制

通常指的是不使用其他

额

外的信息，只使用自我

注意力的形式关注本身

，进而

从句子中抽取相关

信息。自注意力又称作内

部注意

力，它在很多任务

上都有十分出色的表现

，比如阅读

理解、文本继承

、自动文本摘要。

下面将介

绍一个简单的自注意力

机制。

本章建议读者先通

读一遍，通读完后，再结合

实战代

码重新阅读两遍

以上以加深理解。

1. 自注意

力中的query、key和value

自注意力机制

是进行自我关注从而抽

取相关信息的机

制。从具

体实现来看，注意力函数

的本质可以被描述

为一

个查询（query）到一系列键-值（key-value）对

的映射，它们被作为一种

抽象的向量，主要用来进

行

计算和辅助自注意力

，如图10-5所示。

图10-5 自注意力机

制

Q

在上图中，一个单词Thinking经

过向量初始化后，经

过3个

不同的全连接层重新计

算后获取特定维度的值

，

即看到的q1、q2的来历也是如

此。对单词Machines经过

Embedding向量初始

化后，经过与上一个单词

相同的全

连接层计算，之

后依次将q1和q2连接起来，组

成一个新

的二维矩阵W，被

定义成Query。

由于是自注意力

机制，Key和Value的值与Query相同

（仅在

自注意架构中，Query、Key、Value的值相

同

），如图10-6所示。

图10-6 自注意层中

的Query、Key与Value

2. 使用Query、Key和Value计算自注意

力的值

使用Query、Key和Value计算自注

意力的值的过程如

下：

（1）将

Query和每个Key进行相似度计算

得到权重，常

用的相似度

函数有点积、拼接、感知机

等，这里使用

点积计算，如

图10-7所示。

图10-7

点积计算

（2）使用

Softmax函数对这些权重进行归

一化。

Softmax函数的作用是计算

不同输入之间的权重分

数，

又称为权重系数。例如

，正在考虑Thinking这个词，

就用它

的q1乘以每个位置的ki，随后

将得分加以处理，

再传递

给Softmax，然后通过Softmax计算，其目的

是

使分数归一化，如图10-8所

示。

图10-8 使用Softmax函数

Softmax计算分数

决定了每个单词在该位

置表达的程

度。相关联的

单词将具有相应位置上

最高的Softmax分

数。用这个得分

乘以每个Value向量，可以增强

需要关

注单词的值，或者

降低对不相关单词的关

注度。

Softmax的分数决定了当前

单词在每个句子中每个

单词

位置的表示程度。很

明显，当前单词对应句子

中此单

词所在位置的Softmax的

分数最高。但是，有时注意

力

机制也能关注到此单

词之外的其他单词。

（3）每个

Value向量乘以Softmax后的得分，如图

10-9

所示。

图10-9 乘以Softmax

累加计算相

关向量，这会在此位置产

生自注意力层的

输出（对

于第一个单词），即将权重

和相应的键值

Value进行加权

求和，得到最后的注意力

值。

综上所述，自注意力的

计算过程（单词级别）就是

得

到一个可以放到前馈

神经网络的向量。然而在

实际的

实现过程中，该计

算会以矩阵的形式完成

，以便更快

地进行处理。计

算自注意力的公式如下

：

换成更为通用的矩阵点

积的形式来实现，其结构

和形

式如图10-10所示。

图10-10 矩阵

点积

3. 自注意力计算的代

码实现

实际上通过前面

的讲解，自注意力模型的

基本架构其

实并不复杂

，基本实现代码如下（仅供

演示）。

【程序10-1】

import torch

import math

import

einops.layers.torch as elt

# word_embedding_table =

torch.nn.Embedding(num_embeddings=encoder_vocab_size,embeddin

g_dim=312)

# encoder_embedding = word_embedding_table(inputs)

vocab_size = 1024 #字符的种类

embedding_dim =

312

hidden_dim = 256

token =

torch.ones(size=(5,80),dtype=int)

input_embedding = torch.nn.Embedding(num_embeddings

=vocab_size,embedding_dim=

embedding_dim)(token)

#对

输入的input_embedding进行修正，这里进

行了简写

query = torch.nn.Linear(embedding_dim,hidden_dim)

(input_embedding)

key

= torch.nn.Linear(embedding_dim,hidden_dim)

(input_embedding)

value = torch.nn.Linear(embedding_dim,hidden_dim)

(input_embedding)

key = elt.Rearrange("b l d

-> b d l")(key)

#计算query与key之间的

权重系数

attention_prob

= torch.matmul(query,key)

#使用softmax对权重系

数进行归一化计算

attention_prob = torch.softmax(attention_prob,dim=-

1)

#计算

权重系数与value的值，从而获

取注意力值

attention_score = torch.matmul(attention_prob,value

)

print(attention_score.shape)

核心代码实

现起来很简单，读者只要

掌握这些核心代

码即可

。

换个角度来看，从概念上

对注意力机制进行解释

，注

意力机制可以理解为

从大量信息中有选择地

筛选出少

量重要信息，并

聚焦到这些重要信息上

，忽略大多不

重要的信息

，这种思路仍然成立。聚焦

的过程体现在

权重系数

的计算上，权重越大，越聚

焦于其对应的

Value值，即权重

代表了信息的重要性，而

权重与

Value的点积是其对应

的最终信息。

完整的注意

力层代码如下，这里需要

注意的是，在计

算自注意

力的完整代码中，相对于

前面的代码段，这

里加入

mask部分，这是为了在计算时

忽略为了将所有

的序列

padding成一样的长度而进行的

掩码计算操作。

【程序10-2】

import torch

import

math

import einops.layers.torch as elt

class

Attention(torch.nn.Module):

def __init__(self,embedding_dim = 312,hidden_dim

=

256):

super().__init__()

self.query_layer = torch.nn.Linear(embeddin

g_dim,

hidden_dim)

self.key_layer = torch.nn.Linear(embedding_

dim, hidden_dim)

self.value_layer = torch.nn.Linear(embeddin

g_dim, hidden_dim)

def

forward(self,embedding,mask):

input_embedding = embedding

query =

self.query_layer(input_embedding)

key = self.key_layer(input_embedding)

value =

self.value_layer(input_embedding)

key = elt.Rearrange("b l d

-> b d l")

(key)

#

计算

query与key之间的权重系数

attention_prob = torch.matmul(query, key)

#

使用

softmax对权重系数进行归一化

计算

attention_prob += mask * -1e5

# 根据

注意力mask的位置

修正注意力权重值

attention_prob = torch.softmax(attention_pr

ob, dim=-1)

# 计算

权重系数与value的值，从而获

取注意力值

attention_score =

torch.matmul(attention_pr

ob, value)

return (attention_score)

具体结果请

读者自行打印查阅。

10.1.3 ticks和LayerNormalization

10.1.2节

的最后通过PyTorch 2.0自定义层编

写了注

意力模型的代码

。可以看到在标准的自注

意力层中还

根据mask的位置

修正了掩码值。掩码层的

作用是获取

输入序列的

“有意义的值”

，而忽视本身

用作填充或

补全序列的

值。一般用0表示有意义的

值，而用1表示

填充值（这一

点并不固定，0和1的意思可

以互换）。

[2,3,4,5,5,4,0,0,0] ->

[0,0,0,0,0,0,1,1,1]

计算掩码的代码

如下所示：

def create_padding_mark(seq):

mask =

torch.not_equal(seq, 0).float()

mask = torch.unsqueeze(mask, dim=-1)

return mask

此外，计算出来

的query与key（参见上一小节程序

10-

2中加黑的代码）的点积还

需要除以一个常数，其作

用

是缩小点积的值，以便

进行Softmax计算。

这种做法常被

称为ticks，即采用一点小技巧

使得模型

训练更加准确

和便捷。LayerNormalization函数作用

就是如

此，下面详细介绍。

LayerNormalization函数是

专门用于对序列进行整

形

的函数，其目的是防止

字符序列在计算过程中

发散，

从而对神经网络的

拟合过程造成影响。PyTorch 2.0

中对

LayerNormalization也定义了高级API，调用方法

如下：

layer_norm = torch.nn.LayerNorm(normalized_shape,

eps=1e-05,

elementwise_affine=True,

device=None, dtype=None)函数

embedding = layer_norm(embedding)

#使用layer_norm对输入数

据进行处理

图10-11展示了LayerNormalization函

数与

BatchNormalization函数的不同，可以看

到，

BatchNormalization是对一个batch中不同序列

处于同

一位置的数据进

行归一化计算，而

LayerNormalization是对同

一序列不同位置的数据

进

行归一化处理。

图10-11 LayerNormalization函数

与

BatchNormalization函数的不同

有兴趣的

读者可以进一步学习，这

里不再赘述。具体

归一化

方法如下（注意一定要显

式声明归一化的维

度）：

embedding = torch.rand(size=(5,80,312))

print(torch.nn.LayerNorm(normalized_shape=[80,312])

(embedding).shape) #显

式

声明归一化的维度

10.1.4 多

头自注意力

10.1.2节的最后使

用PyTorch 2.0编写了完整的自注

意

力层的代码。从中可以看

到，除了使用自注意力核

心模型外，还额外加入了

掩码层和点积的除法运

算，

以及为了整形所使用

的LayerNormalization函数。实

际上，这些都是

为了使得整体模型在训

练时更加简易

和便捷而

做出的优化。

聪明的读者

应该发现了，前面无论是

掩码计算、点积

计算还是

使用LayerNormalization，都是在一些细枝

末

节上进行修补，有没有可

能对注意力模型做一个

较

大的结构调整，使其更

加适应模型的训练？

下面

将在此基础上介绍一种

较为大型的ticks，即多头

自注

意力架构，这种架构在原

始的自注意力模型的基

础上做出较大的优化。

多

头注意力（Multi-Head Attention）结构如图10-

12所示

，query、key、value首先经过一个线性变换

，

之后计算相互之间的注

意力值。相对于原始自注

意力

的计算方法，这里的

计算要做h次（h为头的数目

），

其实就是所谓的多头，每

一次计算一个头。而每次

query、key、value进行线性变换的参数W是

不一样

的。

图10-12 多头注意力

结构

将h次的缩放点积注

意力值的结果进行拼接

，再进行一

次线性变换，得

到的值作为多头注意力

的结果，如图

10-13所示。

图10-13 多头

注意力的结果

可以看到

，这样计算得到的多头注

意力值的不同之处

在于

进行了h次计算，而不只是

计算一次，这样的好处

是

允许模型在不同的表示

子空间学习到相关的信

息，

并且相对于单独的注

意力模型，多头注意力模

型的计

算复杂度大大降

低了。拆分多头注意力模

型的代码如

下：

def

splite_tensor(tensor,h_head):

embedding = elt.Rearrange("b l (h

d) -

> b l h

d",h = h_head)(tensor)

embedding = elt.Rearrange("b

l h d -

> b

h l d", h=h_head)(embedding)

return embedding

在此基础

上，可以对注意力模型进

行修正，新的多头

注意力

层代码如下：

【程序10-3】

class Attention(torch.nn.Module):

def

__init__(self,embedding_dim = 312,hidden_dim

= 312,n_head =

6):

super().__init__()

self.n_head = n_head

self.query_layer

= torch.nn.Linear(embeddin

g_dim, hidden_dim)

self.key_layer =

torch.nn.Linear(embedding_

dim, hidden_dim)

self.value_layer = torch.nn.Linear(embeddin

g_dim, hidden_dim)

def forward(self,embedding,mask):

input_embedding =

embedding

query = self.query_layer(input_embedding)

key =

self.key_layer(input_embedding)

value = self.value_layer(input_embedding)

query_splited =

self.splite_tensor(query,se

lf.n_head)

key_splited = self.splite_tensor(key,self.n

_head)

value_splited = self.splite_tensor(value,se

lf.n_head)

key_splited =

elt.Rearrange("b h l d -

>

b h d l")(key_splited)

# 计算query与

key之间的权重系数

attention_prob = torch.matmul(query_splited

, key_splited)

#

使用softmax对

权重系数进行归一化计

算

attention_prob += mask * -1e5

# 在自

注意力权重的基

础上加上掩码值

attention_prob = torch.softmax(attention_pr

ob, dim=-1)

# 计算权

重系数与value的值，从而获取

注意力值

attention_score =

torch.matmul(attention_pr

ob, value_splited)

attention_score = elt.Rearrange("b

h l d

-> b l

(h d)")(attention_score)

return (attention_score)

def splite_tensor(self,tensor,h_head):

embedding = elt.Rearrange("b l (h d)

-

> b l h d",h

= h_head)(tensor)

embedding = elt.Rearrange("b l

h d -

> b h

l d", h=h_head)(embedding)

return embedding

if

__name__ == '__main__':

embedding = torch.rand(size=(5,16,312))

mask = torch.ones((5,1,16,1)) #注

意设计mask的位

置，长度是16

Attention()(embedding,mask)

比较单一的注

意力模型，多头注意力模

型能够简化计

算，并且在

更多维的空间上对数据

进行整合。最新的

研究表

明，实际上使用多头注意

力模型，每个头所关

注的

内容并不一致，有的头关

注相邻之间的序列，而

有

的头会关注更远处的单

词。

图10-14展示了一个8头注意

力模型架构。

图10-14 8头注意力

模型架构

10.2 编码器的实现

本节开始介绍编码器的

实现。

前面介绍了编码器

的核心部件——注意力模型

，并且

介绍了输入端的词

嵌入初始化方法和位置

编码，本节

将使用Transformer编码器

方案来构建，这是目前最

常

用的架构方案。

从图10-15可

以看到，一个编码器的构

造分成3部分：

初始向量层

、注意力层和前馈层。

图10-15 编

码器的构造

初始向量层

和注意力层在上一节已

经讲解完毕，本节

将介绍

最后一部分：前馈层。之后

将使用这3部分构造

出本

书的编码器架构。

10.2.1 前馈层

的实现

从编码器输入的

序列在经过一个自注意

力层后，会传

递到前馈（Feed Forward）神

经网络中，这个神经网

络

被称为前馈层。前馈层的

作用是进一步整形通过

注

意力层获取的整体序

列向量。

本书的解码器遵

循的是Transformer架构，因此参考

Transformer中

解码器的构建，如图10-16所示

。相信

读者看到图10-16一定会

很诧异，是不是放错了？并

没

有。

图10-16 Transformer中解码器的构建

所谓前馈神经网络，实际

上就是加载了激活函数

的全

连接层神经网络（或

者使用一维卷积实现的

神经网

络，这里不详细介

绍）。

既然了解了前馈神经

网络，其实现也很简单，代

码如

下：

【程序10-4】

import torch

class

FeedForWard(torch.nn.Module):

def __init__(self,embedding_dim = 312,scale =

4)

:

super().__init__()

self.linear1 = torch.nn.Linear(embedding_dim,embedd

ing_dim*scale)

self.relu_1 = torch.nn.ReLU()

self.linear2 =

torch.nn.Linear(embedding_dim*scale,

embedding_dim)

self.relu_2 = torch.nn.ReLU()

self.layer_norm

= torch.nn.LayerNorm(normalized_sha

pe=embedding_dim)

def forward(self,tensor):

embedding

= self.linear1(tensor)

embedding = self.relu_1(embedding)

embedding

= self.linear2(embedding)

embedding = self.relu_2(embedding)

embedding

= self.layer_norm(embedding)

return embedding

代码很简单

，需要注意的是，前面使用

了两个带激活

函数的全

连接层实现了“前馈”

，然而

实际上为了减

少参数，减

轻运行负担，可以使用一

维卷积或者空洞

卷积替

代全连接层实现前馈神

经网络，具体读者可以

自

行完成。

10.2.2 编码器的实现

经

过本章前面的分析，实现

一个Transformer架构的编

码器并不

困难，只需要按照架构依

次组合在一起即

可。实现

代码如下：

import torch

import

math

import einops.layers.torch as elt

class

FeedForWard(torch.nn.Module):

def __init__(self,embedding_dim = 312,scale =

4):

super().__init__()

self.linear1 = torch.nn.Linear(embedding_dim,

embedding_dim*scale)

self.relu_1 = torch.nn.ReLU()

self.linear2 = torch.nn.Linear(embedding_dim*

scale,embedding_dim)

self.relu_2 = torch.nn.ReLU()

self.layer_norm =

torch.nn.LayerNorm(normaliz

ed_shape=embedding_dim)

def forward(self,tensor):

embedding =

self.linear1(tensor)

embedding = self.relu_1(embedding)

embedding =

self.linear2(embedding)

embedding = self.relu_2(embedding)

embedding =

self.layer_norm(embedding)

return embedding

class Attention(torch.nn.Module):

def

__init__(self,embedding_dim = 312,hidden_dim =

312,n_head =

6):

super().__init__()

self.n_head = n_head

self.query_layer

= torch.nn.Linear(embedding_

dim, hidden_dim)

self.key_layer =

torch.nn.Linear(embedding_di

m, hidden_dim)

self.value_layer = torch.nn.Linear(embedding_

dim, hidden_dim)

def forward(self,embedding,mask):

input_embedding =

embedding

query = self.query_layer(input_embedding)

key =

self.key_layer(input_embedding)

value = self.value_layer(input_embedding)

query_splited =

self.splite_tensor(query,self

.n_head)

key_splited = self.splite_tensor(key,self.n_h

ead)

value_splited = self.splite_tensor(value,self

.n_head)

key_splited =

elt.Rearrange("b h l d -

>

b h d l")(key_splited)

# 计算query与key之间的

权重系数

attention_prob = torch.matmul(query_splited,

key_splited)

# 使用softmax对权重系

数进行归一化计算

attention_prob += mask * -1e5 #

在自

注

意力权重的基础上加

上掩码值

attention_prob = torch.softmax(attention_prob

,

dim=-1)

# 计算权重系数

与value的值，从而获取注意力

值

attention_score = torch.matmul(attention_prob

, value_splited)

attention_score = elt.Rearrange("b h

l d

-> b l (h

d)")(attention_score)

return (attention_score)

def splite_tensor(self,tensor,h_head):

embedding

= elt.Rearrange("b l (h d) -

> b l h d",h =

h_head)(tensor)

embedding = elt.Rearrange("b l h

d -

> b h l

d", h=h_head)(embedding)

return embedding

class PositionalEncoding(torch.nn.Module):

def __init__(self, d_model = 312, dropout

= 0.05

, max_len=80):

"""

:param

d_model: pe编码维度，一般与

Word Embedding相同

，方便相加

:param dropout:

dorp out

:param max_len: 语料库中最长

句子的长度，即

Word

Embedding中的L

"""

super(PositionalEncoding, self).__init__()

# 定义

drop

out

self.dropout = torch.nn.Dropout(p=dropout)

# 计算pe编码

pe = torch.zeros(max_len, d_model) # 建立

空表，每行

代表一个词的位置，每列

代

表一个编码位

position = torch.arange(0, max_len).unsqueez

e(1) # 建个arange表

示词的位

置，以便进行公

式计算，size=(max_len,1)

div_term =

torch.exp(torch.arange(0, d_model

, 2) * #

计算公式中10000**

（2i/d_model)-(math.log(10000.0)/ d_model))

pe[:, 0::2] =

torch.sin(position * div_ter

m) # 计算

偶数维度的pe值

pe[:, 1::2] = torch.cos(position * div_ter

m) # 计算奇数

维度的pe值

pe = pe.unsqueeze(0)

# size=

(1, L, d_model)，为了后续与word_embedding

相

加，意为batch维度下的操作相

同

self.register_buffer('pe', pe) # pe值是不

参加训练的

def

forward(self, x):

# 输

入的最终编

码 =

word_embedding + positional_embedding

x = x

+ self.pe[:, :x.size(1)].clone().det

ach().requires_grad_(False)

return self.dropout(x)

# size = [batch, L

,

d_model]

class Encoder(torch.nn.Module):

def __init__(self,vocab_size =

1024,max_length = 8

0,embedding_size =

312,n_head

= 6,scale = 4,n_layer = 3):

super().__init__()

self.n_layer = n_layer

self.embedding_table =

torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim=em

bedding_size)

self.position_embedding = PositionalEncoding(

max_len=max_length)

self.attention = Attention(embedding_size,emb

edding_size,n_head)

self.feedward =

FeedForWard()

def forward(self,token_inputs):

token = token_inputs

mask = self.create_mask(token)

embedding = self.embedding_table(token)

embedding = self.position_embedding(embedding

)

for _

in range(self.n_layer):

embedding = self.attention(embedding,

mask)

embedding = torch.nn.Dropout(0.1)

(embedding)

embedding =

self.feedward(embedding)

return embedding

def create_mask(self,seq):

mask

= torch.not_equal(seq, 0).float()

mask = torch.unsqueeze(mask,

dim=-1)

mask = torch.unsqueeze(mask, dim=1)

return

mask

if __name__ == '__main__':

seq

= torch.ones(size=(3,80),dtype=int)

Encoder()(seq)

可以看到

，真正实现一个编码器，从

理论和架构上来

说并不

困难，只需要读者细心即

可。

10.3

实战编码器：汉字拼音

转换模型

本节将结合前

面两节的内容实战编码

器，即使用编码

器完成一

个训练——拼音与汉字的转

换，效果参考图

10-17。

图10-17 拼音和

汉字

10.3.1 汉字拼音数据集处

理

首先是数据集的准备

和处理，本例准备了150000条汉

字和拼音的对应数据。

1. 数

据集展示

汉字拼音数据

集如下：

A11_0 lv4 shi4 yang2 chun1 yan1

jing3

da4 kuai4 wen2 zhang1 de

di3 se4 si4 yue4

de lin2

luan2 geng4 shi4 lv4 de2 xian1

huo2 xiu4 me

i4 shi1 yi4

ang4 ran2 绿

是

阳 春

烟 景 大

块 文 章 的

底 色 四 月 的 林

峦

更 是 绿 得 鲜 活

秀 媚 诗

意 盎 然

A11_1

ta1 jin3 ping2 yao1 bu4 de

li4

liang4 zai4 yong3 dao4 shang4

xia4 fan1

teng2 yong3 dong4 she2

xing2 zhuang4 ru2 hai3 tun2 y

i1 zhi2 yi3 yi1 tou2 de

you1

shi4 ling3 xian1 他 仅

凭 腰 部 的 力

量 在

泳 道 上 下 翻 腾

蛹 动

蛇 行 状 如 海

豚 一

直 以 一

头 的

优 势 领 先

A11_10 pao4

yan3 da3 hao3 le zha4 yao4

zen3

me zhuang1 yue4 zheng4 cai2

yao3

le yao3 ya2 shu1 de

tuo1 qu4 yi1 fu2 guang1 bang3

zi chong1 jin4 le shui3 cuan4

dong4

炮 眼 打 好

了 炸

药 怎 么 装 岳 正

才

咬

了 咬 牙 倏 地

脱 去 衣 服 光

膀 子

冲 进

了 水 窜 洞

A11_100 ke3 shei2 zhi1 wen2 wan2

hou4 ta1 yi1

zhao4 jing4 zi

zhi3 jian4 zuo3 xia4

yan3 jian3

de xian4 you4 cu1 you4 hei1

yu3 you4 ce4

ming2 xian3 bu4

dui4 cheng1

可 谁

知 纹 完

后 她 一 照 镜 子

只

见

左 下 眼 睑 的

线 又 粗 又

黑 与 右

侧 明 显

不 对 称

简

单介绍一下。数据集中的

数据分成3部分，每部分使

用特定空格键隔开：

A11_10 … … … ke3

shei2 … … …可 谁

… …

…

·第一部分的A11_i为序号，表示

序列的条数和行号。

·第二

部分是拼音编号，这里使

用的是汉语拼音，与

真实

的拼音标注不同的是去

除了拼音原始标注，而使

用数字1、2、3、4替代，分别代表当

前读音的第一声

到第四

声，这一点请读者注意。

·最

后一部分是汉字的序列

，这里与第二部分的拼音

部分一一对应。

2. 获取字库

和训练数据

获取数据集

中字库的个数是一个非

常重要的问题，一

个非常

好的办法是：使用set格式的

数据读取全部字库

中的

不同字符。

创建字库和训

练数据的完整代码如下

：

max_length = 64

with open("zh.tsv", errors="ignore",

encoding="utf

-8") as f:

context =

f.readlines()

#读取内容

for line in context:

line = line.strip().split(" ")

#切分每行中的

不同部

分

pinyin = ["GO"] + line[1].split(" ")

+ [

"END"] #处理拼音部分

，在头尾加上

起止符号

hanzi

= ["GO"] + line[2].split(" ") +

["

END"] #处

理汉字部分，在头尾加上

起止符号

for _pinyin, _hanzi

in zip(pinyin, hanzi):

#创建字库

pinyin_vocab.add(_pinyin);hanzi_vocab.a

dd(_hanzi)

pinyin = pinyin + ["PAD"] *

(max_length

- len(pinyin))

hanzi = hanzi

+ ["PAD"] * (max_length -

len(hanzi))

pinyin_list.append(pinyin);hanzi_list.append(ha

nzi) #创建

汉字列表

这里说明一下

，首先context读取了全部数据集

中的内

容，之后根据空格

将其分成3部分。对拼音和

汉字部

分，将其转换成一

个序列，并在前后分别加

上起止符

GO和END。实际上也可

以不加，为了明确地描述

起止关

系，才加上了起止

标注。

实际上，还需要加上

一个特定的符号PAD，这是为

了对

单行序列进行补全

，最终的数据如下：

['GO', 'liu2',

'yong3' , … … … ,

'gan1', ' END',

'PAD', 'PAD' ,

… … …]

['GO', '柳', '永'

, … … … , '感

',

'END', 'PAD',

'PAD' , … …

…]

pinyin_list和hanzi_list是两个列表，分别用来

存放

对应的拼音和汉字

训练数据。最后不要忘记

在字库中

加上PAD符号：

pinyin_vocab =

["PAD"] + list(sorted(pinyin_vocab))

hanzi_vocab = ["PAD"]

+ list(sorted(hanzi_vocab))

3. 根据

字库生成Token数据

获取的拼

音标注和汉字标注的训

练数据并不能直接用

于

模型训练，模型需要转换

成Token的一系列数字列

表，代

码如下：

def get_dataset():

pinyin_tokens_ids = []

#新的拼

音Token列表

hanzi_tokens_ids = [] #新

的

汉字Token列表

for pinyin,hanzi in zip(tqdm(pinyin_list),hanzi_list

):

#获取新的拼

音Token

pinyin_tokens_ids.append([pinyin_vocab.index(

char) for char in

pinyin])

#获取新的汉字Token

hanzi_tokens_ids.append([hanzi_vocab.index(ch

ar) for char

in hanzi])

return pinyin_vocab,hanzi_vocab,pinyin_tokens_ids,hanz

i_tokens_ids

代码中

创建了两个新的列表，分

别用于对拼音和汉字

的

Token进行存储，从而获取根据

字库序号编号后形成

的

新序列Token。

10.3.2 汉字拼音转换模

型的确定

下面进行模型

的编写。实际上，单纯使用

10.2节提供

的模型也是可以

的，但是一般需要对其进

行修正。单

纯使用一层编

码器对数据进行编码，在

效果上可能并

没有多层

编码器准确率高，一个简

单的解决方法就是

增加

多层编码器对数据进行

编码，如图10-18所示。

图10-18 多层编

码器

实现代码如下。

【程序

10-5】

import torch

import math

import

einops.layers.torch as elt

class FeedForWard(torch.nn.Module):

def

__init__(self,embedding_dim = 312,scale = 4):

super().__init__()

self.linear1 = torch.nn.Linear(embedding_dim,

embedding_dim*scale)

self.relu_1 =

torch.nn.ReLU()

self.linear2 = torch.nn.Linear(embedding_dim*

scale,embedding_dim)

self.relu_2

= torch.nn.ReLU()

self.layer_norm = torch.nn.LayerNorm(normaliz

ed_shape=embedding_dim)

def forward(self,tensor):

embedding = self.linear1(tensor)

embedding

= self.relu_1(embedding)

embedding = self.linear2(embedding)

embedding

= self.relu_2(embedding)

embedding = self.layer_norm(embedding)

return

embedding

class Attention(torch.nn.Module):

def __init__(self,embedding_dim =

312,hidden_dim =

312,n_head = 6):

super().__init__()

self.n_head = n_head

self.query_layer = torch.nn.Linear(embedding_

dim, hidden_dim)

self.key_layer = torch.nn.Linear(embedding_di

m,

hidden_dim)

self.value_layer = torch.nn.Linear(embedding_

dim, hidden_dim)

def forward(self,embedding,mask):

input_embedding = embedding

query

= self.query_layer(input_embedding)

key = self.key_layer(input_embedding)

value

= self.value_layer(input_embedding)

query_splited = self.splite_tensor(query,self

.n_head)

key_splited = self.splite_tensor(key,self.n_h

ead)

value_splited =

self.splite_tensor(value,self

.n_head)

key_splited = elt.Rearrange("b h

l d -

> b h

d l")(key_splited)

# 计算query与key之间的权重系数

attention_prob = torch.matmul(query_splited,

key_splited)

# 使用softmax对权重系数进行归

一化计算

attention_prob += mask

* -1e5 # 在自注

意力权

重的基础上加上掩码值

attention_prob =

torch.softmax(attention_prob

, dim=-1)

# 计算权重系数与value的值，从

而获取注意力值

attention_score

= torch.matmul(attention_prob

, value_splited)

attention_score =

elt.Rearrange("b h l d

-> b

l (h d)")(attention_score)

return (attention_score)

def

splite_tensor(self,tensor,h_head):

embedding = elt.Rearrange("b l (h

d) -

> b l h

d",h = h_head)(tensor)

embedding = elt.Rearrange("b

l h d -

> b

h l d", h=h_head)(embedding)

return embedding

class PositionalEncoding(torch.nn.Module):

def __init__(self, d_model =

312, dropout = 0.05

, max_len=80):

"""

:param d_model: pe编码维

度，一般与

Word Embedding相同，方便相加

:param

dropout: dorp out

:param max_len: 语料库中最长句子的长

度，即

Word Embedding中的L

"""

super(PositionalEncoding, self).__init__()

#

定义drop out

self.dropout = torch.nn.Dropout(p=dropout)

#

计算pe编码

pe = torch.zeros(max_len, d_model) # 建立

空表，每行代表一个

词的位置，每列代

表一个

编码位

position = torch.arange(0, max_len).unsqueez

e(1) # 建个arange表示词的位

置，以便进行公式计算，size=(max_len,1)

div_term = torch.exp(torch.arange(0,

d_model

, 2) * # 计

算公式中10000**

（2i/d_model)- (math.log(10000.0) / d_model))

pe[:, 0::2]

= torch.sin(position * div_ter

m) #

计算偶数维度

的pe值

pe[:, 1::2] = torch.cos(position *

div_ter

m) # 计算奇数维度的pe值

pe = pe.unsqueeze(0)

# size=

(1, L, d_model)，为了后续与word_embedding

相加，意为batch维

度下的操作相同

self.register_buffer('pe', pe) # pe值是不

参加训练的

def forward(self,

x):

# 输入最终的

编

码 = word_embedding

+ positional_embedding

x = x +

self.pe[:, :x.size(1)].clone().det

ach().requires_grad_(False)

return self.dropout(x) #

size = [batch, L

, d_model]

class Encoder(torch.nn.Module):

def __init__(self,vocab_size = 1024,max_length

= 8

0,embedding_size =

312,n_head =

6,scale = 4,n_layer = 3):

super().__init__()

self.n_layer = n_layer

self.embedding_table = torch.nn.Embedding(num

_embeddings=vocab_size,

embedding_dim=embedding_size)

self.position_embedding = PositionalEncoding(

max_len=max_length)

self.attention = Attention(embedding_size,emb

edding_size,n_head)

self.feedward =

FeedForWard()

def forward(self,token_inputs):

token = token_inputs

mask = self.create_mask(token)

embedding = self.embedding_table(token)

embedding = self.position_embedding(embedding

)

for _

in range(self.n_layer):

embedding = self.attention(embedding,

mask)

embedding = torch.nn.Dropout(0.1)

(embedding)

embedding =

self.feedward(embedding)

return embedding

def create_mask(self,seq):

mask

= torch.not_equal(seq, 0).float()

mask = torch.unsqueeze(mask,

dim=-1)

mask = torch.unsqueeze(mask, dim=1)

return

mask

相对于10.2.2节的编码器

构建示例，这里使用了多

层

的自注意力层和前馈

层。需要注意的是，这里只

是在

编码器层中加入了

更多层的多头注意力层

和前馈层，

而不是直接加

载更多的编码器。

10.3.3

模型训

练部分的编写

本小节进

行模型训练部分的编写

。在这里采用简单的

模型

训练程序的编写方式来

完成代码的编写。

1. 导入数

据集和创建数据的生成

函数

对于数据的获取，由

于模型在训练过程中不

可能一次

性将所有的数

据导入，因此需要创建一

个生成器，将

获取的数据

按批次发送到训练模型

，在这里我们使用

一个for循

环来完成这个数据输入

的任务。

【程序10-6】

pinyin_vocab,hanzi_vocab,pinyin_tokens_ids,hanzi_toke

ns_ids

=

get_data.get_dataset()

batch_size = 32

train_length

= len(pinyin_tokens_ids)

for epoch in range(21):

train_num = train_length // batch_size

train_loss,

train_correct = [], []

for i

in tqdm(range((train_num))):

…

这段代码用

于完成数据的生成工作

，按既定的

batch_size大小生成数据

batch，之后在epoch的循环

中将数据

输入进行迭代。

下面是训

练模型的完整实战代码

。

【程序10-7】

import numpy as np

import torch

import attention_model

import get_data

max_length = 64

from tqdm import

tqdm

char_vocab_size = 4462

pinyin_vocab_size =

1154

def get_model(embedding_dim = 312):

model

= torch.nn.Sequential(

attention_model.Encoder(pinyin_vocab_size,max_l

ength=max_length),

torch.nn.Dropout(0.1),

torch.nn.Linear(embedding_dim,char_vocab_size)

)

return model

device = "cuda"

model = get_model().to(device)

model = torch.compile(model)

optimizer = torch.optim.Adam(model.parameters(), l

r=3e-5)

loss_func

= torch.nn.CrossEntropyLoss()

pinyin_vocab,hanzi_vocab,pinyin_tokens_ids,hanzi_toke

ns_ids =

get_data.get_dataset()

batch_size = 32

train_length = len(pinyin_tokens_ids)

for epoch in range(21):

train_num =

train_length // batch_size

train_loss, train_correct =

[], []

for i in tqdm(range((train_num))):

model.zero_grad()

start = i * batch_size

end = (i + 1) *

batch_size

batch_input_ids = torch.tensor(pinyin_tokens_

ids[start:end]).

int().to(device)

batch_labels = torch.tensor(hanzi_tokens_ids[

start:end]).to(device)

pred =

model(batch_input_ids)

batch_labels = batch_labels.to(torch.uint8)

active_loss =

batch_labels.gt(0).view(-1) ==

1

loss = loss_func(pred.view(-1,

char_vocab_si

ze)[active_loss],

batch_labels.view(-1)[active_loss])

optimizer.zero_grad()

loss.backward()

optimizer.step()

if (epoch +1) %10 == 0:

state = {"net":model.state_dict(), "optimize

r":optimizer.state_dict(),

"epoch":epoch}

torch.save(state, "./saver/modelpara.pt")

通过将训练代码和

模型组合在一起，即可完

成模型的

训练。最后的预

测部分，即使用模型进行

拼音和汉字

的转换部分

，请读者自行完成。

10.4

本章小

结

本小结对模型的设计

做一些补充。

首先，需要向

读者说明的是，本章的模

型设计并没有

完全遵守

Transformer中编码器的设计，而是仅

仅建立

了多层注意力层

和前馈层，这是与真实的

Transformer

中的解码器不一致的地

方。

其次，对于数据的设计

，这里直接将不同字符或

者拼

音作为独立的字符

进行存储，这样做的好处

在于可以

使得数据的最

终生成更加简单，但是增

加字符个数会

增大搜索

空间，因此对训练的要求

更高。还有一种划

分方法

，即将拼音拆开，使用字母

和音标分离的方式

进行

处理，有兴趣的读者可以

尝试一下。

作者在编写本

章内容时发现，对于输入

的数据来说，

这里输入的

值是词嵌入的Embedding和位置编

码的和，

如果读者尝试了

只使用单一的词嵌入Embedding的

话，

可以发现，相对于使用

叠加的Embedding值，单一的词

嵌入

Embedding对于同义字的分辨会产

生问题，即：

qu4

na3去哪 去拿

qu4 na3的发

音相同，无法分辨出到底

是“去哪”还

是“去拿”。有兴趣

的读者可以做一个测试

，也可以

深入研究一下这

个问题。

本章就是这些内

容，但是相对于Transformer架构来

说

，仅有编码器是不完整的

，在编码器的基础上，还

存

在一个对应的解码器，这

将在第12章介绍，并且会

解

决一个非常重要的问题

——文本对齐。

到这里，读者一

定急不可耐地想继续学

习下去，但

是，请记住本章

开始的提示，建议阅读本

章内容3遍以

上。

现在，请你

带着编码器和汉字拼音

转换模型回过头来

重新

阅读本章内容。

第11章

站在

巨人肩膀上的预训练模

型BERT

经过前面的学习，读者

应该对使用深度学习框

架

PyTorch 2.0进行自然语言处理有

了一个基础性的认

识，如

果读者按部就班地学习

，那么也不会觉得很

难。

第

10章介绍了一种新的基于

注意力模型的编码器，如

果读者在学习第10章内容

时注意到，作为编码器的

encoder_layer（编码器层）与作为分类使

用的

dense_layer（全连接层）可以分开

独立使用，那么一

个自然

而然的想法就是能否将

编码器层和全连接层分

开，利用训练好的模型作

为编码器独立使用，并且

可

以根据具体项目接上

不同的“尾端”

，以便在预训

练

好的编码器上通过“微

调”的方式进行训练。

好了

，有了想法就要行动起来

。

11.1

预训练模型BERT

BERT

（Bidirectional Encoder Representation from

Transformer）是2018年10月由Google AI研

究院提

出的一种预训练

模型，如图11-1所示。它使用了

在第

10章中介绍的编码器

结构的层级和构造方法

，最大的

特点是抛弃了传

统的循环神经网络和卷

积神经网络，

通过注意力

模型将任意位置的两个

单词的距离转换成

1，有效

地解决了自然语言处理

中棘手的文本长期依赖

问题。

图11-1 BERT

BERT实际上是一种替

代了Word Embedding的新型文字

编码方

案，是一种目前计算文字

在不同文本中的语境

而

“动态编码”的最优方法。BERT被

用来学习文本句

子的语

义信息，比如经典的词向

量表示。BERT包括句

子级别的

任务（如句子推断、句子间

的关系）和字符

级别的任

务（如实体识别）。

11.1.1 BERT的基本架

构与应用

BERT的模型架构是

一个多层的双向注意力

结构的

Encoder部分。本节先来看

BERT的输入，再复习前面介

绍

的BERT模型架构。

1. BERT的输入

BERT的输

入的编码向量（长度是512）是

3个嵌入特征

的单位，如图

11-2所示。

图11-2 BERT的输入

·词嵌入（Token Embedding）：根

据每个字符在

“字表”中的

位置赋予一个特定的Embedding值

。

·位置嵌入（Position Embedding）：将单词的位置

信息编码成特征向量，是

向模型中引入单词位置

关系

至关重要的一环。

·分

割嵌入（Segment Embedding）：用于区分两个

句

子，例如B是不是A的下文（如

对话场景、问答场景

等）。对

于句子对，第一个句子的

特征值是0，第二个

句子的

特征值是1。

2. BERT的模型架构

与

第10章介绍的编码器结构

相似，BERT实际上是由多

个Encoder

Block叠

加而成的，通过使用注意

力模型

的多个层次来获

得文本的特征提取，如图

11-3所示。

图11-3 BERT的模型架构

11.1.2 BERT预训

练任务与Fine-Tuning

在介绍BERT的预训

练任务的方案前，先介绍

一下BERT

在使用时的思路，即

BERT在训练过程中将自己的

训练

任务和可替换的微

调（Fine-Tuning）系统分离。

1. 开创性的预

训练任务方案

Fine-Tuning的目的是

根据具体任务的需求替

换不同的

后端接口，即在

已经训练好的语言模型

的基础上加入

少量的任

务的专门属性。例如，对于

分类问题，在语

言模型的

基础上加一层Softmax网络，然后

在新的语料

上重新训练

来进行Fine-Tuning。除了最后一层外

，所

有的参数都没有变化

，如图11-4所示。

图11-4

Fine-Tuning

BERT在设计时作

为预训练模型训练任务

，为了最好地

让BERT掌握预言

的含义和方法，BERT采用了多

任务的

方式，包括遮蔽语

言模型

（Masked Language

Model, MLM）和下一个句子预

测（Next Sentence Prediction, NSP）。

（1）任务1：MLM

MLM是指在训练的时

候随机从输入语料上Mask（遮

挡、

替换）掉一些单词，然后

通过上下文预测该单词

，该

任务非常像读者在中

学时期经常做的完形填

空题。正

如传统的语言模

型算法和RNN匹配那样，MLM的这

个性

质和Transformer的结构是非常

匹配的。在BERT的实验

中，15%的Embedding

Token会

被随机Mask掉。在训练

模型时

，一个句子会被多次“喂”到

模型中用于参数

学习，但

是Google并没有每次都Mask掉这些

单词，而

是在确定要Mask掉的

单词之后按一定比例进

行处理：

80%直接替换为[Mask]，10%替换

为其他任意单词，10%

保留原

始Token。

·80%：

my dog is hairy→my dog

is [mask]。

·10%：my dog is hairy→my

dog is apple。

·10%：my dog is

hairy→my dog is hairy。

这么做的原因是，如果

句子中的某个Token 100%被

Mask，那么在

Fine-Tuning的时候模型就会有一些

没有

见过的单词，如图11-5所

示。

图11-5 MLM

加入随机Token的原因是

，Transformer要保持对每个

输入Token的分

布式表征，否则模型会记

住这个[mask]

是Token

'hairy

'。至于单词带来

的负面影响，因为

一个单

词被随机替换掉的概率

只有15%×10% =1.5%，

所以这个负面影响

其实是可以忽略不计的

。

（2）任务2：NSP

NSP的任务是判断句子

B是不是句子A的下文。如果

是的

话，就输出'IsNext'

，否则输出

'NotNext'。训练数据

的生成方式是

从平行语料中随机抽取

连续的两句话，

其中50%保留

抽取的两句话，符合IsNext关系

；剩下的

50%随机从语料中提

取，它们的关系是NotNext。这个

关

系保存在如图11-6所示的[CLS]符

号中。

图11-6 NSP

2. BERT用于具体NLP任务（Fine-Tuning）

在

海量单语料上训练完BERT之

后，便可以将其应用到

NLP的

各个任务中了。对于其他

任务来说，我们也可以

根

据BERT的输出信息做出对应

的预测。图11-7展示了

BERT在11个不

同任务中的模型，它们只

需要在BERT的

基础上再添加

一个输出层，便可以完成

对特定任务的

微调。这些

任务类似于我们做过的

文科试卷，其中有

选择题

、简答题等。

图11-7 模型训练任

务

预训练得到的BERT模型可

以在后续执行NLP任务时进

行

微调，主要涉及以下内

容：

·一对句子的分类任务

：自然语言推断（MNLI）、句

子语义

等价判断（QQP）等，如图11-7（a）所示，需

要将两个句子传入BERT，然后

使用[CLS]的输出值C对

句子对

进行分类。

·单个句子的分

类任务：句子情感分析（SST-2）、判

断句子语法是否可以接

受（CoLA）等，如图11-7（b）

所示。只需要输

入一个句子，无须使用[SEP]标

志，然

后使用[CLS]的输出值C进

行分类。

·问答任务：SQuAD

v1.1数据集

，样本是语句对

（Question, Paragraph）。其中，Question表示

问

题；Paragraph是一段来自Wikipedia的文本

，包含问

题的答案。训练的

目标是在Paragraph中找出答案的

起

始位置（Start,

End）。如图11-7（c）所示，将

Question和

Paragraph传入BERT，然后BERT根据

Paragraph所有单词

的输出预测Start和End的位置。

·单

个句子的标注任务：命名

实体识别（NER）等。输

入单个句

子，然后根据BERT对每个单词

的输出T预测这

个单词属

于Person、Organization、Location、

Miscellaneous还是Other（非命名实体）。

11.2 实战

BERT：中文文本分类

前面介绍

了BERT的结构与应用，本节将

实战BERT的文

本分类。

11.2.1

使用Hugging Face获

取BERT预训练模型

BERT是一个预

训练模型，其基本架构和

存档都有相应

的服务公

司提供下载服务，而Hugging Face是目

前专

门免费提供自然语

言处理预训练模型的公

司。

Hugging Face是一家总部位于纽约

的聊天机器人初创

服务

商，开发的应用在青少年

中颇受欢迎，相比于其

他

公司，Hugging Face更加注重产品带来

的情感以及

环境因素。在

GitHub上开源的自然语言处理

、预训练

模型库Transformer提供了NLP领

域大量优秀的预训练

语

言模型和调用框架。

使用

Hugging Face获取BERT预训练模型的步骤

如下：

步骤01 安装依赖。

安装

Hugging Face依赖的方法很简单，命令

如下：

pip install transformers

安装完成后，即可使

用Hugging

Face提供的预训练模

型BERT。

步

骤02 使用Hugging Face提供的代码格式

进行BERT

的引入与使用，代码

如下：

from transformers import BertTokenizer

from transformers

import BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base￾chinese')

pretrain_model

= BertModel.from_pretrained("bert-base￾chinese")

这里需要从网上下

载模型，下载完毕后即可

进行使

用，如图11-8所示。

图11-8 下

载模型

下面的代码演示

了使用BERT编码器获取对应

文本的

Token。

【程序11-1】

from transformers import

BertTokenizer

from transformers import BertModel

tokenizer

= BertTokenizer.from_pretrained('bert￾base-chinese')

pretrain_model = BertModel.from_pretrained("bert￾base-chinese")

tokens

= tokenizer.encode("春眠不觉晓

",max_length=12,padding="max_length",truncation=True)

print(tokenizer("春眠不觉晓

",max_length=12,padding="max_length",truncation=True))

这里使用了

两种方法打印，结果如下

：

[101,

3217, 4697, 679, 6230, 3236, 102,

0, 0,

0, 0, 0]

{'input_ids':

[101, 3217, 4697, 679, 6230, 3236

, 102, 0, 0, 0, 0,

0],

'token_type_ids': [0, 0, 0, 0,

0, 0, 0, 0, 0, 0,

0

, 0], 'attention_mask': [1, 1,

1, 1, 1, 1, 1, 0,

0, 0, 0, 0]}

第一行是使用encode函数获取

的Token，第二行是直接

对其加

码获取到的3个不同的Token表

示，对应前面的

BERT输入，请读

者自行验证学习。

需要注

意的是，我们输入的是5个

字符“春眠不觉

晓”

，而在加

码后变成了7个字符，这是

因为BERT默认

会在单独的文

本中加入[CLS]和[SEP]作为特定的

分隔

符。

如果想打印使用

BERT计算的对应文本的Embedding值，

那

么可以使用如下代码。

【程

序11-2】

import torch

from

transformers import BertTokenizer

from transformers import

BertModel

tokenizer = BertTokenizer.from_pretrained('bert￾base-chinese')

pretrain_model =

BertModel.from_pretrained("bert￾base-chinese")

tokens = tokenizer.encode("春眠不觉晓

",max_length=12,padding="max_length",truncation=True)

print(tokens)

print("----------------------")

print(tokenizer("春眠不觉

晓

",max_length=12,padding="max_length",truncation=True))

print("----------------------")

tokens =

torch.tensor([tokens]).int()

print(pretrain_model(tokens))

打印结果如图11-9所示。最

终获得一个维度为

[1,12,768]的矩

阵，用以表示输入的文本

。

图11-9 打印结果

11.2.2 BERT实战文本分

类

我们先回到第9章中的

一个实战演示，在第9章带

领读

者完成了基于循环

神经网络的中文情感分

类实战，但

是当时的问题

是结果可能并不能令人

满意，此时通过

使用预训

练模型查看预测结果。

步

骤01 数据的准备。

对于所需

要使用的情感分类数据

集，这里使用本书自

带的

dataset数据集中的ChnSentiCorp.txt文件。

步骤02 数

据的处理。

在这里使用BERT自

带的tokenizer函数将文本转换成

需

要的Token。完整代码如下：

import numpy as np

from transformers import BertTokenizer

tokenizer =

BertTokenizer.from_pretrained('bert￾base-chinese')

max_length = 80 #设

置获取的文本长

度为80

labels = [] #用

以存放label

context =

[] #用以存放汉字文

本

token_list = []

with

open("../dataset/cn/ChnSentiCorp.txt", mode="r

", encoding="utf-8") as

emotion_file:

for line in emotion_file.readlines():

line =

line.strip().split(",")

# labels.append(int(line[0]))

if int(line[0]) ==

0:

labels.append(0) #由于后面直接采

用PyTorch自

带的crossentropy函数，因

此这里直接

输入0，否则输入[1,0]

else:

labels.append(1)

text = "".join(line[1:])

token =

tokenizer.encode(text,max_length=max_

length,padding=

"max_length",truncation=True)

token_list.append(token)

context.append(text)

seed

= 828

np.random.seed(seed);np.random.shuffle(token_list)

np.random.seed(seed);np.random.shuffle(labels)

dev_list =

np.array(token_list[:170]).astype(int)

dev_labels = np.array(labels[:170]).astype(int)

token_list =

np.array(token_list[170:]).astype(int)

labels = np.array(labels[170:]).astype(int)

这里首先

通过BERT自带的tokenizer对输入的文

本进行

编码处理，之后将

其拆分成训练集与验证

集。

步骤03 模型的设计。

与第

10章的示例不同之处在于

，这里使用BERT作为文

本的特

征提取器，而在后面仅使

用了一个二分类层作

为

分类函数，需要说明的是

，由于BERT的输入不同，

这里将

其拆分成两种模型，分别

是simple版与标准

版。simple版的代码

如下：

import torch

import torch.utils.data as

Data

from transformers import BertModel

from

transformers import BertTokenizer

from transformers import

AdamW

# 定义下游任务模型

class ModelSimple(torch.nn.Module):

def __init__(self,

pretrain_model_name = "bert￾base-chinese"):

super().__init__()

self.pretrain_model =

BertModel.from_pretrain

ed(pretrain_model_name)

self.fc = torch.nn.Linear(768, 2)

def forward(self, input_ids):

with torch.no_grad(): #

上游的模型不进行梯

度

更新

output = self.pretrain_model(input_id

s=input_ids)

# input_ids: 编

码之后的数字(token) )

output

= self.fc(output[0][:, 0]) # 取

出

每个

batch 的第一列作为 CLS，即

(16,

786)

output =

output.softmax(dim=1) # 通

过 softmax 函数，使其在

1 的维度

上进行缩

放，使元素位于

[0,1] 范围内，总和为 1

return

output

标准版预

训练模型代码如下：

class Model(torch.nn.Module):

def __init__(self,

pretrain_model_name = "bert￾base-chinese"):

super().__init__()

self.pretrain_model =

BertModel.from_pretrain

ed(pretrain_model_name)

self.fc = torch.nn.Linear(768, 2)

def forward(self, input_ids,attention_mask,token_type

_ids):

with torch.no_grad():

# 上游

的模型不进行梯

度更新

output = self.pretrain_model(input_id

s=input_ids,

# input_ids: 编

码之后的数字(token)

attention_mask=attention_mask, #

at

tention_mask: 其中 pad 的

位置是0，

其他位置是

1

# token_type_ids: 第一

个句子和特殊符

号的位

置是0，第二个句子的位置

是1

token_type_ids=token_type_ids)

output = self.fc(output[0][:, 0]) # 取出

每个 batch 的第一列作

为CLS，即

(16,786)

output =

output.softmax(dim=1) # 通

过 softmax 函数，使其在

1 的

维度上进行缩

放，使元素

位于[0,1] 范围内，总和为 1

return

output

标准

版和simple版的区别主要在于

输入格式不同。而

对于不

同的输入格式，有兴趣的

读者可以在本章内容

完

成后自行尝试。

步骤04 模型

的训练。

完整代码如下。

【程

序11-3】

import torch

import model

device = "cuda"

model = model.ModelSimple().to(device)

model = torch.compile(model)

optimizer = torch.optim.Adam(model.parameters(),

l

r=2e-4)

loss_func = torch.nn.CrossEntropyLoss()

import

get_data

token_list = get_data.token_list

labels =

get_data.labels

dev_list = get_data.dev_list

dev_labels =

get_data.dev_labels

batch_size = 128

train_length =

len(labels)

for epoch in (range(21)):

train_num

= train_length // batch_size

train_loss, train_correct

= 0, 0

for i in

(range(train_num)):

start = i * batch_size

end = (i + 1) *

batch_size

batch_input_ids = torch.tensor(token_list[sta

rt:end]).to(device)

batch_labels

= torch.tensor(labels[start:end]

).to(device)

pred = model(batch_input_ids)

loss = loss_func(pred, batch_labels.type(tor

ch.uint8))

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_loss += loss.item()

train_correct

+= ((torch.argmax(pred, dim=-1

) ==

(batch_labels)).type(torch.float).sum().item()

/ len(batch_

labels))

train_loss /= train_num

train_correct /= train_num

print("train_loss:", train_loss, "train_correct:",

t

rain_correct)

test_pred = model(torch.tensor(dev_list).to(device))

correct

= (torch.argmax(test_pred, dim=-1) ==

(torch.tensor(dev_labels).to(device))).type(torch.float).sum(

).item()

/

len(test_pred)

print("test_acc:",correct)

print("-------------------")

上面的代码比较简单

，就不再过多解析了。需要

注意

的是，使用BERT会增大显

存的消耗，因此在具体使

用

场景中，读者可以根据

自己硬件的具体情况设

置不同

的batch_size值。代码运行最

终结果如下所示。

-------------------

train_loss: 0.46196953270394925 train_correct:

0.859507415

2542372

test_acc: 0.9

-------------------

train_loss:

0.4603629142551099 train_correct: 0.8603019067

79661

test_acc: 0.9

-------------------

train_loss: 0.4588900986364332 train_correct: 0.8609639830

508474

test_acc: 0.9058823529411765

-------------------

train_loss: 0.4575323578664812 train_correct:

0.8626853813

559322

test_acc: 0.9058823529411765

-------------------

train_loss:

0.4562745584269701 train_correct: 0.8636122881

355932

test_acc: 0.9117647058823529

-------------------

train_loss: 0.4551042293087911 train_correct: 0.8641419491

525424

test_acc: 0.9117647058823529

-------------------

train_loss: 0.45401099576788434 train_correct:

0.865201271

1864406

test_acc: 0.9117647058823529

-------------------

读者可

以运行一下代码，结果将

展示全部10个Epoch的

过程，最终

准确率达到0.9176。另外，由于作

者设置

的训练时间与学

习率的关系，此结果并不

是最优的结

果，读者可以

自行尝试完成。

11.3 更多的预

训练模型

Hugging

Face除了提供BERT预训

练模型的下载之外，

还提

供了其他预训练模型的

下载，打开Hugging Face

主页，如图11-10所示

。

图11-10 Hugging

Face主页

单击页面顶端的

Models菜单之后，可以出现预训

练模

型的选择界面，如图

11-11所示。

图11-11 预训练模型的选

择

在左侧依次是“任务选

择”“使用框架”“训练数据

集

”以及“模型语言”选项，这里

选择我们使用的

PyTorch与zh标签

，即使用PyTorch构建的中文数据

集，右边会呈现对应的模

型，如图11-12所示。

图11-12 选择我们

需要的模型

图11-13右边为Hugging Face所

提供的基于PyTorch框

架的中文

预训练模型，刚才我们所

使用的BERT模型也

在其中。我

们可以选择另一个模型

进行模型训练，比

如基于

“全词遮掩”的GPT2模型进行训

练，如图11-13

所示。

图11-13 选择中文

的PyTorch的BERT模型

这里首先复制

Hugging Face所提供的预训练模型的

全

名：

model_name = "uer/gpt2-chinese-ancient"

注意，需要保留“/”和前

后的名称。替换不同的预

训

练模型只需要替换说

明字符，代码如下：

from transformers import BertTokenizer,GPT2Model

model_name = "uer/gpt2-chinese-ancient"

tokenizer = BertTokenizer.from_pretrained(model_nam

e)

pretrain_model = GPT2Model.from_pretrained(model_na

me)

tokens

= tokenizer.encode("春眠不

觉

晓",max_length=12,padding="max_length",

truncation=True)

print(tokens)

print("----------------------")

print(tokenizer("春眠不觉

晓",max_length=12,padding="max_length",

truncation=True))

print("----------------------")

tokens =

torch.tensor([tokens]).int()

print(pretrain_model(tokens))

剩下的

内容与11.2节的方法一致，有

兴趣的读者可以

自行完

成验证。

最终结果与普通

的BERT预训练模型相比可能

会有出

入，原因可能是多

种多样的，这不在本书的

评判范围

内，有兴趣的读

者可以自行验证更多模

型的使用方

法。

11.4 本章小结

本章介绍了预训练模型

的使用，以经典的预训练

模型

BERT为例，演示了使用BERT进

行文本分类的方法。

除此

之外，在使用预训练模型

时，使用每个序列中的

第

一个Token可以较好地表示完

整序列的功能，这在某

些

任务中能起到较好的作

用。

Hugging Face提供了很多预训练模

型下载，本章也介

绍了很

多使用预训练模型的方

法，有兴趣的读者可以

自

行学习和比较不同的预

训练模型。

第12章

从1起步

——自

然语言处理的解码器

本

章从1开始。

第10章介绍了编

码器的架构和实现代码

。如果读者按

要求把第10章

阅读了3遍或者3遍以上，那

么相信你对

编码器的编

写已经很熟悉了。

本章的

解码器是在编码器的基

础上对模型进行少量修

正，在不改变整体架构的

情况下进行模型设计。可

以

说，如果读者掌握了编

码器的原理，那么掌握解

码器

的概念、设计和原理

一定易如反掌。

本章首先

介绍解码器的原理和程

序编写，然后着重解

决一

个非常大的问题——文本对

齐。

文本对齐是自然语言

处理中一个不可轻易逾

越的障

碍，本章将以翻译

模型的实战为例，系统地

讲解文本

对齐的方法，并

实现一个基于汉字和拼

音的“翻译系

统”。

本章是对

上一章的继承，如果读者

想先完整地体验编

码器

-解码器系统，可以先查看

12.1.4节，这是对解码

器的完整

实现，然后详细学习12.2节的

实战部分。待

程序运行顺

畅之后，再参考12.1节重新学

习解码器相

关内容，加深

印象。如果读者想了解更

多细节，建议

按讲解的顺

序循序渐进地学习。

12.1 解码

器的核心

——注意力模型

顾

名思义，解码器就是对传

送过来的数据进行解码

，

如编码后的数据或者通

过词嵌入输入的Embedding数

据。解

码器的结构如图12-1所示。

图

12-1 解码器结构示意图

解码

器的架构与编码器相似

，但是还有相当一部分的

区别，说明如下：

·相对于编

码器的单一输入（无论是

叠加还是单独的

词向量

Embedding），解码器的输入有两部分

，分别是

编码器的输入和

目标的Embedding输入。

·相对于编码

器中的多头注意力模型

，解码器中的多

头注意力

模型分成两种，分别是多

头自注意力层和多

头交

互注意力层。

总而言之，相

对于编码器中的“单一模

块”

，解码器

中更多的是“双

模块”

，即“编码器”的输入和

“解

码器本身”的输入协同

处理。下面就这些内容详

细介

绍。

12.1.1 解码器的输入和

交互注意力层的掩码

如

果换一种编码器和解码

器的表示方法，如图12-2所

示

，可以清楚地看到，经过多

层编码器的输出被输入

多层的解码器中。但是需

要注意的是，编码器的输

出

对于解码器来说，并不

是直接使用，而是解码器

本身

先进行一次自注意

力编码。

图12-2 编码器和解码

器的表示方法

1. 解码器的

词嵌入Embedding输入

与解码器的

词嵌入输入方式一样，编

码器本身的词嵌

入Embedding的处

理也是由初始化的Embedding向量

和

位置编码构成的，结构

如图12-3所示。

图12-3 词嵌入Embedding的处

理

2. 解码器的自注意力层

（重点学习掩码的构建）

解

码器的自注意力层是对

输入的词嵌入Embedding进行

编码

的部分，这里的构造与编

码器中的构造相同，不

再

过多阐述。

但是相对于编

码器的掩码部分，解码器

的掩码操作有

其特殊的

要求。

事实上，解码器的输

入和编码器在处理上不

太一样，

一般可以认为编

码器的输入都是一个完

整的序列，而

解码器在训

练以及在数据的生成过

程中是逐个进行

Token生成的

，为了防止“偷看”

，解码器的

自注意力

层只能够关注

输入序列当前位置以及

之前的字，不能

够关注之

后的字。因此，需要将当前

输入的字符Token

之后的Token都添

加上Mask，使之在经过Softmax计算

之

后的权重变为0，拟态输入

的是PAD字符，代码如

下：

def create_look_ahead_mask(size):

mask

= 1 - tf.linalg.band_part(tf.ones((size, si

ze)),

-1, 0)

return mask

如果

单独打印代码：

mask

= create_look_ahead_mask(4)

print(mask)

这里的参

数size设置成4，以此打印的结

果如图12-4所

示。

图12-4

打印结果

可以看到，函数的实际作

用是生成一个三角掩码

，对

输入的值做出依次增

加的梯度，这样可以保持

在输入

模型的过程中，数

据的接收也是依次增加

的，当前的

Token只与其本身和

其前面的Token进行注意力计

算，

而不会与后续的进行

计算。这段内容的图形化

效果如

图12-5所示。

图12-5 三角掩

码器

此外，对于解码器自

注意力层的输入，即query、

key、value的定

义和设定，在解码器的自

注意力层的

输入都是由

叠加后的词嵌入Embedding输入，因

此可以

与编码器类似，直

接将其设置成同一个。

3. 解

码器和编码器的交互注

意力层（重点学习

query、key、value的定义

）

编码器和解码器处理后

的数据需要“交融”

，从而进

行新的数据整合和生成

，而进行数据整合和生成

的架

构和模块在本例中

所处的位置是交互注意

力层。

编码器中的交互注

意力层的架构和同处于

编码器中的

自注意力层

没有太大的差别，其差别

主要是输入的不

同，以及

使用掩码对数据的处理

上。下面分别进行阐

述。

1）交

互注意力层

交互注意力

层的作用是将编码器输

送的“全部”词嵌

入Embedding，与解码

器获取的“当前”的词嵌入

Embedding进行“融合”计算，使得当前

的词嵌入“对

齐”编码器中

对应的信息，从而获取解

码后的信息。

下面从解码

器的图示角度进行讲解

。

从图12-6可以看到，对于交互

注意力的输入，从编码

器

中输入的是两个，而解码

器自注意力层中输入的

是

一个。读者可能会有疑

问，对于注意力层的query、

key、value到底

是如何安排和处理的？

图

12-6 解码器

问题的解答还是

要回归注意力层的定义

：

实际上，就是首先使用query计

算与key的权重，之后

使用权

重与value携带的信息进行比

较，从而将value

中的信息“融合

”到query中。

非常简单地可以得

到，在交互注意力层中，解

码器的

自注意力词嵌入

Embedding首先与编码器的输入词

嵌入

Embedding计算权重，之后使用

计算后的权重来计算编

码器中的信息。即：

query =

解码器

词嵌入Embedding

key = 编码器词嵌入Embedding

value =

编

码器词嵌入Embedding

2）交互注意力

中的掩码层（对谁进行掩

码处理）

下面处理的是解

码器中多头注意力的掩

码层，相对于

单一的自注

意力层来说，一个非常显

著的问题是对谁

进行掩

码处理。

对这个问题的解

答，需要重新回到注意力

模型的定

义：

z =Softmax(scores)×v

i

从权重的计

算来看，解码器的词嵌入

Embedding

（query）与编码器输入词嵌入Embedding（key和

value）进行权重计算，从而将query的

值与key和

value进行“融合”。基于这

点考虑，选择对编码器输

入的词嵌入Embedding进行掩码处

理。

如果读者对此不理解

，现在请记住：

mask the encoder input

embedding

有兴趣的读

者可以自行查阅更多资

料进行学习。

下面两个函

数分别展示了普通掩码

处理和在解码器中

自注

意力层掩码的程序写法

。

#创建解码器中的交互注

意力掩码

def

creat_self_mask(from_tensor, to_tensor):

"""

这里需要注意

，from_tensor是输入的文本序列，

即 input_word_ids

，应

该是2D的，

即[1,2,3,4,5,6,0,0,0,0]

to_tensor 是输入的input_word_ids，应该

是2D的，即

[1,2,3,4,5,6,0,0,0,0]

最终的结果是输

出两个3D的相乘

注意：后面

如果需要4D的，则使用expand添加

一个维度即可

"""

batch_size, from_seq_length = from_tensor.shape

to_mask = torch.not_equal(from_tensor, 0).int()

to_mask =

elt.Rearrange("b l -> b 1 l")

(to_mask) # 这里扩充

了数据类型

broadcast_ones = torch.ones_like(to_tensor)

broadcast_ones = torch.unsqueeze(broadcast_ones, d

im=-1)

mask

= broadcast_ones * to_mask

mask.to("cuda")

return

mask

打印结果和

演示请读者自行完成。

如

果需要进一步提高准确

率，那么需要对掩码进行

进

一步处理：

def create_look_ahead_mask(from_tensor,

to_tensor):

cross_mask = creat_self_mask(from_tensor, to_tensor)

look_ahead_mask

= torch.tril(torch.ones(to_tensor.sha

pe[1],

from_tensor.shape[1]))

look_ahead_mask =

look_ahead_mask.to("cuda")

cross_mask = look_ahead_mask * cross_mask

return cross_mask

下面的代码

段合成了pad_mask和look_ahead_mask，并

通过maximum函数

建立与或门，将其合成为

一体。

tf.Tensor(

[[[[1.

0. 0. 0.]]]

[[[1. 1. 0.

0.]]]

[[[1. 1. 1. 0.]]]

[[[1.

1. 1. 1.]]]], shape=

(4, 1,

1, 4), dtype=float32)

+

tf.Tensor(

[[0.

1. 1. 1.]

[0. 0. 1.

1.]

[0. 0. 0. 1.]

[0.

0. 0. 0.]], shape=(4, 4), dtype=float32)

=

tf.Tensor(

[[[[1. 1. 1. 1.]

[1. 0. 1. 1.]

[1. 0.

0. 1.]

[1. 0. 0. 0.]]]

[[[1. 1. 1. 1.]

[1. 1.

1. 1.]

[1. 1. 0. 1.]

[1. 1. 0. 0.]]]

[[[1. 1.

1. 1.]

[1. 1. 1. 1.]

[1. 1. 1. 1.]

[1. 1.

1. 0.]]]

[[[1. 1. 1. 1.]

[1. 1. 1. 1.]

[1. 1.

1. 1.]

[1. 1. 1. 1.]]]],

shape=(4, 1, 4, 4), dtype=float32)

这样的处理可以最

大限度地对无用部分进

行“掩码操

作”

，从而使得解

码器的输入（query）与编码器的

输

入（key, value）能够最大限度地融

合在一起，以减

少干扰。

12.1.2

为

什么通过掩码操作能够

减少干扰

为什么在注意

力层中，通过掩码操作能

够减少干扰？

这是由于query和

value在进行点积计算时会产

生大量

负值，而负值在进

行Softmax计算时，由于Softmax的

计算特

性，因此会对平衡产生影

响，代码如下。

【程序12-1】

class ScaledDotProductAttention(nn.Module):

def __init__(self):

super(ScaledDotProductAttention, self).__init_

_()

def forward(self, Q, K, V,

attn_mask):

'''

Q: [batch_size, n_heads, len_q,

d_k]

K: [batch_size, n_heads, len_k, d_k]

V: [batch_size, n_heads, len_v(=len_k), d_v

]

attn_mask: [batch_size, n_heads, seq_len, s

eq_len]

'''

scores = torch.matmul(Q, K.transpose(-1, -2

)) / np.sqrt(d_k)

# scores :

[batch_size, n_heads, len_q, l

en_k]

scores.masked_fill_(attn_mask

== 0, -1e9)

# attn_mask将矩阵

中所有计算为True的部分（序

列

中被填充为0的部分），scores填

充

为负无穷，表示这个位

置的值对于softmax没有影响

attn = nn.Softmax(dim=-1)(scores)

# attn：

[batch_size, n_heads, len_q, len_

k]

#

对

每一行进行softmax

context = torch.matmul(attn, V)

#

[batch_size, n_heads, len_q, d_v]

return context,

attn

结果如图12-7所

示。

图12-7 打印结果

实际上其

中负值是不需要的，因此

需要在计算本身的

基础

上加上一个“负无穷”

，降低

负值对Softmax计算

的影响（一般

使用-1e5即可）。

12.1.3 解码器的输出

（移位训练方法）

前面两个

小节介绍了解码器的一

些基本操作，本小节

主要

介绍解码器在最终阶段

解码的变化和一些相关

的

细节，如图12-8所示。

图12-8 解码

器的输出

解码器通过交

互注意力的计算选择将

当前的解码器词

嵌入Embedding关

注到编码器词嵌入的Embedding中

，

选择生成一个新的词嵌

入Embedding。

这是整体的步骤，当程

序开始启动时，首先将编

码器

中的词嵌入全部输

入，解码器首先接收一个

起始符号

的词嵌入，从而

生成第一个解码的结果

。

这种输入和输出错位的

训练方法是“移位训练”方

法。

接下来重复这个过程

，每个步骤的输出在下一

个时间

步被提供给底端

解码器，就像编码器之前

做的那样，

这些解码器会

输出它们的解码结果。直

到到达一个特

殊的终止

符号，表示编码器-解码器

架构已经完成了输

出。

还

有一点需要补充的是，解

码器输出一个计算后的

向

量，之后需要将向量重

新转换成一个特定的词

或者字

序列。转换的方法

是使用Softmax层对这个向量进

行分

类，根据分类后的概

率对其进行映射。

全连接

层是一个简单的全连接

神经网络，它将解码器

栈

产生的向量投影到一个

更高维的向量（Output）。

之后的Softmax层

将这些分数转换为概率

。选择概率最

大的维度，并

对应地生成与之关联的

字或者词作为此

时间步

的输出。

12.1.4 解码器的实现

本

小节进行解码器的实现

。

首先，多注意力层实际上

是通用的，代码如下。

【程序

12-2】

class

MultiHeadAttention(tf.keras.layers.Layer):

def __init__(self):

super(MultiHeadAttention, self).__init__()

def

build(self, input_shape):

self.dense_query =

tf.keras.layers.Dense(units=embedding_size,activation=tf.nn.r

elu)

self.dense_key =

tf.keras.layers.Dense(units=embedding_size,activation=tf.nn.r

elu)

self.dense_value =

tf.keras.layers.Dense(units=embedding_size,activation=tf.nn.r

elu)

self.dense =

tf.keras.layers.Dense(units=embedding_size,activation=tf.nn.r

elu)

super(MultiHeadAttention, self).build(input_sh

ape) # 一定要在最后调用它

def

call(self, inputs):

query,key,value,mask = inputs

shape

= tf.shape(query)

query_dense = self.dense_query(query)

key_dense

= self.dense_query(key)

value_dense = self.dense_query(value)

query_dense

= splite_tensor(query_dense)

key_dense = splite_tensor(key_dense)

value_dense

= splite_tensor(value_dense)

attention = tf.matmul(query_dense,key_dense,t

ranspose_b=True)

/tf.math.sqrt(tf.cast(embedding_size,tf.float32))

attention += (mask*-1e9)

attention =

tf.nn.softmax(attention)

attention = tf.matmul(attention,value_dense)

attention =

tf.transpose(attention,[0,2,1,3])

attention = tf.reshape(attention,

[shape[0],-1,embedding_size])

attention

= self.dense(attention)

return attention

其

次，前馈层也可以通用，代

码如下。

【程序12-3】

class FeedForWard(tf.keras.layers.Layer):

def __init__(self):

super(FeedForWard, self).__init__()

def build(self, input_shape):

self.conv_1 =

tf.keras.layers.Conv1D(embedding_size*4,1,activation=tf.nn.re

lu)

self.conv_2 =

tf.keras.layers.Conv1D(embedding_size,1,activation=tf.nn.relu

)

super(FeedForWard,

self).build(input_shape)

# 一定要在最

后调用它

def call(self, inputs):

output = self.conv_1(inputs)

output = self.conv_2(output)

return output

综合利用多层

注意力层和前馈层，实现

了专用的解码

器的程序

设计，代码如下。

【程序12-4】

class

DecoderLayer(nn.Module):

def __init__(self):

super(DecoderLayer, self).__init__()

self.dec_self_attn

= MultiHeadAttention()

self.dec_enc_attn = MultiHeadAttention()

self.pos_ffn

= PoswiseFeedForwardNet()

def forward(self, dec_inputs, enc_outputs,

dec_self

_attn_mask,

dec_enc_attn_mask):

'''

dec_inputs: [batch_size,

tgt_len, d_model]

enc_outputs: [batch_size, src_len, d_model]

dec_self_attn_mask: [batch_size, tgt_len, tg

t_len]

dec_enc_attn_mask:

[batch_size, tgt_len, src

_len]

'''

#

dec_outputs: [batch_size, tgt_len, d_mode

l], dec_self_attn:

[batch_size, n_heads, tgt_len, tgt_len]

dec_outputs, dec_self_attn

= self.dec_self_a

ttn(dec_inputs, dec_inputs,

dec_inputs, dec_self_attn_mask)

# dec_outputs: [batch_size, tgt_len, d_mode

l],

dec_enc_attn: [batch_size,

h_heads, tgt_len, src_len]

dec_outputs,

dec_enc_attn = self.dec_enc_att

n(dec_outputs, enc_outputs,

enc_outputs,

dec_enc_attn_mask)

# 再是

encoder-decoder attention部分

dec_outputs =

self.pos_ffn(dec_outputs) #

[batch_size, tgt_len,

d_model]

#

特征提取

return dec_outputs, dec_self_attn, dec_enc_a

ttn

12.2 实战解码

器：汉字拼音翻译模型

本

节进入汉字拼音翻译模

型实战。

前面的章节带领

读者学习了注意力模型

相关知识、前

馈层相关知

识以及掩码。这3方面的内

容共同构成了编

码器-解

码器架构的主要内容，共

同组成的就是

Transformer这一基本

架构和内容，如图12-9所示。

图

12-9 解码器

相信读者已经急

不可耐地想利用前面学

习的知识完成

一个翻译

系统，不过在开始之前，还

有两个问题留给

读者：

（1）与

编码器的转换模型相比

，编码器-解码器的翻

译模

型有什么区别？

（2）如果想做

汉字→拼音翻译系统，编码

器和解码器

的输入端分

别输入什么内容？

接下来

详细介绍。

12.2.1

数据集的获取

与处理

在本例中准备了

150000条汉字和拼音对应数据

。

1. 数据集展示

本节实战的

汉字拼音数据集如下：

A11_0

lv4 shi4 yang2 chun1 yan1 jing3

da4 kuai4 wen2 zhang1 de di3

se4 si4 yue4

de lin2 luan2

geng4 shi4 lv4 de2 xian1 huo2

xiu4 me

i4 shi1 yi4 ang4

ran2 绿

是

阳 春 烟 景

大 块 文 章 的

底 色

四 月 的 林

峦 更

是 绿

得 鲜 活 秀 媚

诗 意 盎 然

A11_1 ta1

jin3 ping2 yao1 bu4 de li4

liang4 zai4 yong3 dao4 shang4 xia4

fan1

teng2 yong3 dong4 she2 xing2

zhuang4 ru2 hai3 tun2 y

i1

zhi2 yi3 yi1 tou2 de you1

shi4 ling3 xian1 他

仅 凭 腰

部 的 力

量 在 泳

道

上 下 翻 腾 蛹 动

蛇 行 状 如

海

豚 一

直 以 一 头 的 优

势

领 先

A11_10 pao4 yan3 da3

hao3 le zha4 yao4 zen3

me

zhuang1 yue4 zheng4 cai2 yao3

le

yao3 ya2 shu1 de tuo1 qu4

yi1 fu2 guang1 bang3

zi chong1

jin4 le shui3 cuan4 dong4

炮

眼 打 好 了 炸 药

怎

么 装 岳 正 才

咬

了 咬 牙 倏

地 脱 去

衣 服 光 膀 子 冲

进

了 水

窜 洞

A11_100 ke3

shei2 zhi1 wen2 wan2 hou4 ta1

yi1

zhao4 jing4 zi zhi3 jian4

zuo3 xia4

yan3 jian3 de xian4

you4 cu1 you4 hei1 yu3 you4

ce4

ming2 xian3 bu4 dui4 cheng1

可 谁 知 纹 完 后

她

一 照 镜 子 只 见

左 下 眼

睑 的 线 又

粗 又 黑 与 右 侧

明

显

不 对

称

下面简单介

绍一下。数据集中的数据

分成3部分，每部

分使用特

定空格键隔开。

A11_10 … … … ke3 shei2

… … …可 谁 … …

…

·第一

部分A11_i为序号，表示序列的

条数和行号。

·第二部分是

拼音编号，这里使用的是

汉语拼音，而

与真实的拼

音标注不同的是去除了

拼音原始标注，而

使用数

字1、2、3、4进行替代，分别代表当

前读音的

第一声到第四

声，这点请读者注意。

·最后

一分部是汉字的序列，这

里是与第二部分的拼

音

部分一一对应。

2. 获取字库

和训练数据

获取数据集

中字库的个数是一个非

常重要的问题，一

个非常

好的方法就是使用set格式

的数据对全部字库中

的

不同字符进行读取。

创建

字库和训练数据的完整

代码如下：

import numpy as np

sentences = []

src_vocab = {'⊙':

0, '＞': 1, '＜': 2} #这个是

汉字vocab

tgt_vocab = {'⊙': 0, '＞':

1, '＜': 2} #这

个是

拼音vocab

with

open("../dataset/zh.tsv", errors="ignore", encoding="

utf-8") as f:

context = f.readlines()

for line in

context:

line = line.strip().split(" ")

pinyin

= line[1]

hanzi = line[2]

(hanzi_s)

= hanzi.split(" ")

(pinyin_s) = pinyin.split("

")

#[＞＜]

pinyin_inp = ["＞"] +

pinyin_s

pinyin_trg = pinyin_s + ["＜"]

line = [hanzi_s,pinyin_inp,pinyin_trg]

for char in

hanzi_s:

if char not in src_vocab:

src_vocab[char] = len(src_v

ocab)

for char

in pinyin_s:

if char not in

tgt_vocab:

tgt_vocab[char] = len(tgt_v

ocab)

sentences.append(line)

这里做一个说

明，首先context读取了全部数据

集中的

内容，之后根据空

格将其分成3部分。对于拼

音和汉字

部分，将其转换

成一个序列，并在前后分

别加上起止

符“GO”和“END”。这实际

上也可以不加，为了明确

地描述起止关系，才加上

了起止的标注。

实际上，还

需要加上的一个特定符

号是“PAD”

，这是

为了对单行序

列进行补全的操作，那么

最终的数据如

下所示如

下：

['GO', 'liu2', 'yong3' ,

… … … , 'gan1', '

END',

'PAD', 'PAD' , … …

…]

['GO', '柳', '永' , …

… … , '感', ' END',

'PAD'

, 'PAD' , … …

…]

pinyin_list和hanzi_list分别是两个

列表，分别用来

存放对应

的拼音和汉字训练数据

。最后不要忘记在字

库中

加上“PAD”符号。

pinyin_vocab =

["PAD"] + list(sorted(pinyin_vocab))

hanzi_vocab = ["PAD"]

+ list(sorted(hanzi_vocab))

3. 根据字库生成

Token数据

获取的拼音标注和

汉字标注的训练数据并

不能直接用

于模型训练

，模型需要转换成Token的一系

列数字列

表，代码如下：

enc_inputs, dec_inputs, dec_outputs = [],

[], [

]

for line in

sentences:

enc = line[0];dec_in = line[1];dec_tgt

= line[2]

if len(enc) <= src_len

and len(dec_in) <= tgt_le

n and

len(dec_tgt) <= tgt_len:

enc_token = [src_vocab[char]

for char in

enc];enc_token = enc_token

+ [0]

* (src_len - len(enc_token))

dec_in_token = [tgt_vocab[char] for char i

n dec_in];dec_in_token =

dec_in_token + [0]

* (tgt_len - len(dec_in_token))

dec_tgt_token =

[tgt_vocab[char] for char

in dec_tgt];dec_tgt_token =

dec_tgt_token + [0] * (tgt_len -

len(dec_tgt_token))

enc_inputs.append(enc_token);dec_inputs.append(

dec_in_token);

dec_outputs.append(dec_tgt_token)

代

码中创建了两个新的列

表，分别对拼音和汉字的

token进行存储，而获取根据字

库序号编号后新的序列

token。

12.2.2

翻译模型

翻译模型就是

经典的编码器-解码器模

型，整体代码如

下。

【程序12-5】

# 导

入库

import math

import torch

import numpy

as np

import torch.nn as nn

import torch.optim as optim

import torch.utils.data

as Data

import einops.layers.torch as elt

import get_dataset_v2

from tqdm import tqdm

sentences = get_dataset_v2.sentences

src_vocab = get_dataset_v2.src_vocab

tgt_vocab = get_dataset_v2.tgt_vocab

src_vocab_size = len(src_vocab)

#4462

tgt_vocab_size = len(tgt_vocab) #1154

src_len

= 48

tgt_len = 47 #由于输出比输入多

一个符号，因此

如此使用

# ***********************************************#

# transformer的参数

# Transformer

Parameters

d_model = 512

# 每一个词的

Word Embedding 用多

少位表示

# （包括positional encoding应该用多

少位表示，因为这

两个要

维度相加，应该是一样的

维度）

d_ff = 2048 # FeedForward

dimension

# forward线性层变成多少位

(d_model->d_ff->d_model)

d_k = d_v

= 64 # dimension of K(=Q),

V

# K、Q、V矩阵的维度（K和Q是一样的

，因为要用K乘以Q的

转置），V不

一定

'''

换一种说法，就是在

进行self-attention的时候，

从input（当然，是加

了位置编码之后的input）线性

变换之

后的3个向量 K、Q、V的维

度

'''

n_layers =

6

# encoder和decoder各有多少层

n_heads = 8

# multi-head attention有几个

头

# ***********************************************#

#

数据预处理

# 将encoder_input、decoder_input和decoder_output进行

id化

enc_inputs, dec_inputs, dec_outputs

= [], [], [

]

for

line in sentences:

enc = line[0];dec_in

= line[1];dec_tgt = line[2]

if len(enc)

<= src_len and len(dec_in) <= tgt_le

n and len(dec_tgt) <= tgt_len:

enc_token

= [src_vocab[char] for char in

enc];enc_token

= enc_token + [0]

* (src_len

- len(enc_token))

dec_in_token = [tgt_vocab[char] for

char i

n dec_in];dec_in_token =

dec_in_token

+ [0] * (tgt_len - len(dec_in_token))

dec_tgt_token = [tgt_vocab[char] for char

in

dec_tgt];dec_tgt_token =

dec_tgt_token + [0] *

(tgt_len - len(dec_tgt_token))

enc_inputs.append(enc_token);dec_inputs.append(

dec_in_token);

dec_outputs.append(dec_tgt_token)

enc_inputs = torch.LongTensor(enc_inputs)

dec_inputs = torch.LongTensor(dec_inputs)

dec_outputs = torch.LongTensor(dec_outputs)

# print(enc_inputs[0])

#

print(dec_inputs[0])

# print(dec_outputs[0])

# ***********************************************#

print(enc_inputs.shape,dec_inputs.shape,dec_outputs.s

hape)

class MyDataSet(Data.Dataset):

def __init__(self, enc_inputs,

dec_inputs, dec_outp

uts):

super(MyDataSet, self).__init__()

self.enc_inputs

= enc_inputs

self.dec_inputs = dec_inputs

self.dec_outputs

= dec_outputs

def __len__(self):

return self.enc_inputs.shape[0]

# 有几个sentence

def __getitem__(self, idx):

return

self.enc_inputs[idx], self.dec_inputs[

idx],

self.dec_outputs[idx]

# 根据索引找encoder_input、decoder_input、

decoder_output

loader = Data.DataLoader(

MyDataSet(enc_inputs, dec_inputs,

dec_outputs),

batch_size=512,

shuffle=True)

# ***********************************************#

class

PositionalEncoding(nn.Module):

def __init__(self, d_model, dropout=0.1, max_len=50

00):

super(PositionalEncoding, self).__init__()

self.dropout = nn.Dropout(p=dropout)

# max_length_（一

个sequence的最大长度）

pe = torch.zeros(max_len, d_model)

# pe [max_len,d_model]

position = torch.arange(0,

max_len, dtype=t

orch.float).unsqueeze(1)

# position [max_len，1]

div_term = torch.exp(

torch.arange(0, d_model, 2).float()

* (-math.log(10000.0) / d_model))

# div_term:[d_model/2]

# e^(-i*log10000/d_model)=10000^(-i/d_model)

# d_model为embedding_dimension

# 两个相

乘的维度为[max_len,d_model/2]

pe[:, 0::2] = torch.sin(position * div_ter

m)

pe[:, 1::2] = torch.cos(position *

div_ter

m)

# 计算position encoding

#

pe的维度

为[max_len,d_model]，每一行的奇数和

偶数

分别取sin和cos(position *

div_term)里面的值

pe =

pe.unsqueeze(0).transpose(0, 1)

# 维度

变成(max_len,1,d_model)

# 所以直接用pe=pe.unsqueeze(1)也可以

self.register_buffer('pe',

pe)

# 放入buffer中，参数不会训练

def forward(self, x):

'''

x: [seq_len, batch_size, d_model]

'''

x = x + self.pe[:x.size(0), :,

:]

# 选

取和x一样维度的seq_length，将pe加到

x上

return self.dropout(x)

#

***********************************************#

# 由于在 Encoder 和 Decoder

中都需要进

行 mask 操作

# 因此无法确定这

个函数的参数中 seq_len

的值

# 如

果在 Encoder中调用，seq_len 就等于 src_len

# 如果

在 Decoder中调用，seq_len 就有可能等

于

src_len

#

也有可能等于 tgt_len（因为 Decoder 有两

次 mask）

#

src_len 是在encoder-decoder中的mask

# tgt_len是decoder mask

def

creat_self_mask(from_tensor, to_tensor):

"""

这里需要

注意，from_tensor是输入的文本序列

，

即 input_word_ids，应该是2D的，即

[1,2,3,4,5,6,0,0,0,0]

to_tensor 是输入的

input_word_ids，应该是2D的，即

[1,2,3,4,5,6,0,0,0,0]

最终的结果

是输出2个3D的相乘

注意：后

面如果需要4D的，则使用expand添

加一个维度即可

"""

batch_size, from_seq_length = from_tensor.shape

#

这里只

能进行self attention，不能进行交互

# assert from_tensor ==

to_tensor,print("输

入

from_tensor与to_tensor不一致，

检查mask创建部

分，需要自己完成")

to_mask = torch.not_equal(from_tensor,

0).int()

to_mask = elt.Rearrange("b l ->

b 1 l")

(to_mask) # 这里扩

充了数据类型

broadcast_ones = torch.ones_like(to_tensor)

broadcast_ones = torch.unsqueeze(broadcast_ones,

dim

=-1)

mask = broadcast_ones *

to_mask

mask.to("cuda")

return mask

def create_look_ahead_mask(from_tensor,

to_tensor):

cross_mask = creat_self_mask(from_tensor, to_tensor)

look_ahead_mask

= torch.tril(torch.ones(to_tensor.sha

pe[1],

from_tensor.shape[1]))

look_ahead_mask =

look_ahead_mask.to("cuda")

cross_mask = look_ahead_mask * cross_mask

return cross_mask

# ***********************************************#

class ScaledDotProductAttention(nn.Module):

def __init__(self):

super(ScaledDotProductAttention, self).__init_

_()

def

forward(self, Q, K, V, attn_mask):

'''

Q: [batch_size, n_heads, len_q, d_k]

K:

[batch_size, n_heads, len_k, d_k]

V: [batch_size,

n_heads, len_v(=len_k), d_v

]

attn_mask: [batch_size,

n_heads, seq_len, s

eq_len]

'''

scores

= torch.matmul(Q, K.transpose(-1, -2

)) /

np.sqrt(d_k)

# scores : [batch_size, n_heads,

len_q, l

en_k]

scores.masked_fill_(attn_mask == 0,

-1e9)

# attn_mask将矩阵中

所有计算为True的部分（序列

中被填充为0的部分），scores填充

为负无穷，表示这个位置

的值对于softmax没有影响

attn = nn.Softmax(dim=-1)(scores)

# attn： [batch_size, n_heads, len_q, len_

k]

# 对每

一行进行softmax

context = torch.matmul(attn,

V)

# [batch_size, n_heads, len_q, d_v]

return context, attn

'''

这里要做的是

，通过Q和K计算出scores，然后将scores和

V相

乘，得到每个单词的

context

vector。

首

先是将Q和K的转置相乘，相

乘之后得到的 scores 还不

能立

刻进行

Softmax，需要和

attn_mask 相加，把一

些需要屏蔽的信息屏蔽

掉，attn_mask是一个

仅由 True 和

False 组成的

tensor，并且一定会保证attn_mask和scores的维

度的4个值相同（不

然无法

进行对应位置相加）。

mask完了

之后，就可以对 scores 进行

Softmax 了。然

后再与V相乘，得到 context。

'''

# ***********************************************#

class MultiHeadAttention(nn.Module):

def __init__(self):

super(MultiHeadAttention, self).__init__()

self.W_Q = nn.Linear(d_model, d_k * n_head

s, bias=False)

self.W_K = nn.Linear(d_model, d_k

* n_head

s, bias=False)

self.W_V =

nn.Linear(d_model, d_v * n_head

s, bias=False)

# 3个矩阵

，分别对输入进行3次线性

变化

self.fc = nn.Linear(n_heads *

d_v, d_model

, bias=False)

# 变换维度

def forward(self, input_Q, input_K, input_V, attn_

mask):

'''

input_Q: [batch_size, len_q, d_model]

input_K: [batch_size, len_k, d_model]

input_V: [batch_size,

len_v(=len_k), d_model

]

attn_mask: [batch_size, seq_len,

seq_len]

'''

residual, batch_size = input_Q,

input_Q.siz

e(0)

# [batch_size, len_q, d_model]

# (W)-> [batch_size, len_q,d_k * n_heads]

# (view)->[batch_size, len_q,n_heads,d_k]

# (transpose)-

>

[batch_size,n_heads, len_q,d_k ]

Q = self.W_Q(input_Q).view(batch_size,

-1,

n_heads, d_k).transpose(1, 2)

K =

self.W_K(input_K).view(batch_size, -1,

n_heads, d_k).transpose(1, 2)

V

= self.W_V(input_V).view(batch_size, -1,

n_heads, d_v).transpose(1, 2)

# 生成Q、K、V矩阵

attn_mask = attn_mask.unsqueeze(1)

# attn_mask

: [batch_size, n_heads, seq_le

n, seq_len]

context, attn = ScaledDotProductAttention()

(Q, K,

V, attn_mask)

# context: [batch_size, n_heads,

len_q, d_

v],

# attn: [batch_size,

n_heads, len_q, len_k

]

context =

context.transpose(1, 2).reshape(ba

tch_size, -1, n_heads *

d_v)

# context: [batch_size, len_q, n_heads

*

d_v]

output = self.fc(context)

#

[batch_size, len_q, d_model]

return nn.LayerNorm(d_model).cuda()

(output

+ residual), attn

完整代码中一定会有3处

地方调用

MultiHeadAttention()，Encoder Layer调用一次，

传入

的input_Q、input_K、input_V全部都是

enc_inputs，代码如下：

Decoder Layer中

的两次调用，第一次是

decoder_inputs，第

二次是两个encoder_outputs和一

个decoder_input。

# ***********************************************#

class PoswiseFeedForwardNet(nn.Module):

def __init__(self):

super(PoswiseFeedForwardNet, self).__init__()

self.fc = nn.Sequential(

nn.Linear(d_model,

d_ff, bias=False),

nn.ReLU(),

nn.Linear(d_ff, d_model, bias=False)

)

def forward(self, inputs):

'''

inputs:

[batch_size, seq_len, d_model]

'''

residual =

inputs

output = self.fc(inputs)

return nn.LayerNorm(d_model).cuda()

(output + residual) # [batch_size,

seq_len,

d_model]

# 也有

残差连接和layer normalization

# 这段代码非

常简单，就是进行两次线

性变换，残差连接后再

跟

一个 Layer Norm

# ***********************************************#

class

EncoderLayer(nn.Module):

def __init__(self):

super(EncoderLayer, self).__init__()

self.enc_self_attn

= MultiHeadAttention()

# 多头注意力机制

self.pos_ffn =

PoswiseFeedForwardNet()

# 提

取特征

def forward(self, enc_inputs,

enc_self_attn_mask):

'''

enc_inputs: [batch_size, src_len, d_model]

enc_self_attn_mask: [batch_size, src_len, sr

c_len]

'''

# enc_outputs: [batch_size, src_len, d_mode

l],

# attn: [batch_size, n_heads, src_len, src

_len]每一个头建立一

个注意力矩阵

enc_outputs, attn = self.enc_self_attn(enc_i

nputs,

enc_inputs,

enc_inputs, enc_self_attn_mask)

# enc_inputs to

same Q,K,V

#输入的enc inputs分

别乘以WQ、WK、WV生成3个独立

的Query、Key、Value矩

阵

enc_outputs

= self.pos_ffn(enc_outputs)

# enc_outputs: [batch_size, src_len,

d_mode

l]

# 输入和输出的维度是

一样的

return enc_outputs,

attn

# 将上述组件拼起

来，就是一个完整的 Encoder Layer

#

***********************************************#

class DecoderLayer(nn.Module):

def __init__(self):

super(DecoderLayer,

self).__init__()

self.dec_self_attn = MultiHeadAttention()

self.dec_enc_attn =

MultiHeadAttention()

self.pos_ffn = PoswiseFeedForwardNet()

def forward(self,

dec_inputs, enc_outputs, dec_self

_attn_mask,

dec_enc_attn_mask):

'''

dec_inputs: [batch_size, tgt_len, d_model]

enc_outputs: [batch_size,

src_len, d_model]

dec_self_attn_mask: [batch_size, tgt_len, tg

t_len]

dec_enc_attn_mask: [batch_size, tgt_len, src

_len]

'''

# dec_outputs: [batch_size, tgt_len, d_mode

l], dec_self_attn:

[batch_size, n_heads, tgt_len, tgt_len]

dec_outputs, dec_self_attn = self.dec_self_a

ttn(dec_inputs, dec_inputs,

dec_inputs, dec_self_attn_mask)

# dec_outputs: [batch_size, tgt_len,

d_mode

l], dec_enc_attn: [batch_size,

h_heads, tgt_len,

src_len]

# 先是

decoder的self-attention

# print(dec_outputs.shape)

#

print(enc_outputs.shape)

#

# print(dec_enc_attn_mask.shape)

dec_outputs, dec_enc_attn

= self.dec_enc_att

n(dec_outputs, enc_outputs,

enc_outputs, dec_enc_attn_mask)

# 再是encoder-decoder attention部分

dec_outputs = self.pos_ffn(dec_outputs)

# [

batch_size, tgt_len, d_model]

#

特征提取

return dec_outputs, dec_self_attn, dec_enc_a

ttn

# 在

Decoder Layer 中会调用两

次 MultiHeadAttention，第一次是

计算

Decoder Input 的

self-attention，得到输出 dec_outputs

#

然后将

dec_outputs 作为生成 Q 的元素，

enc_outputs 作为生

成

K 和 V 的元素，再调用一

次

MultiHeadAttention，得到的

是

Encoder 和 Decoder Layer 之间的 context

vector。最

后

将

dec_outputs 做一次维度变换，然后

返回

# ***********************************************#

class Encoder(nn.Module):

def __init__(self):

super(Encoder, self).__init__()

self.src_emb = nn.Embedding(src_vocab_size,

d_model)

# 对encoder输入的每个单词

进行词向量计算词向量

/

字向量（src_vocab_size个词，每

个词的维

度为d_model)

self.pos_emb = PositionalEncoding(d_model)

#

计算位置向量

self.layers = nn.ModuleList([EncoderLayer()

for _

in range(n_layers)])

# 将6个

Encoder Layer组成一个module

def

forward(self, enc_inputs):

'''

enc_inputs: [batch_size, src_len]

'''

enc_outputs = self.src_emb(enc_inputs)

# 对每个单词进

行词向量计算

# enc_outputs [batch_size, src_len, d_model

]

enc_outputs = self.pos_emb(enc_outputs.transp

ose(0, 1)).transpose(0, 1)

# 添加位置

编码

# enc_outputs [batch_size, src_len,

d_mod

el]

enc_self_attn_mask = creat_self_mask(enc_inpu

ts,

enc_inputs)

# enc_self_attn: [batch_size, src_len, src_

len]

# 计算得到encoder-attention的pad martix

enc_self_attns =

[]

# 创建一

个列表，保存接下来返回

的自注意力值

for layer in

self.layers:

# enc_outputs: [batch_size, src_len,

d_model]

# enc_self_attn: [batch_size, n_head

s, src_len,

src_len]

enc_outputs, enc_self_attn = layer(e

nc_outputs,

enc_self_attn_mask)

enc_self_attns.append(enc_self_attn)

# 再传进来

就不用positional decoding

#

记录下每一次的

attention

return enc_outputs, enc_self_attns

# nn.ModuleList()里面的参数是列表，列表

里面保存

了 n_layers 个 Encoder Layer

#

由于我们

控制好了 Encoder Layer的输入和输出

维度相

同，因此可以直接

用一个 for 循环以嵌套

的方

式，将上一次 Encoder Layer 的输出作为

下一

次 Encoder

Layer 的输入

# ***********************************************#

class Decoder(nn.Module):

def __init__(self):

super(Decoder, self).__init__()

self.tgt_emb =

nn.Embedding(tgt_vocab_size,

d_model)

self.pos_emb = PositionalEncoding(d_model)

self.layers

= nn.ModuleList([DecoderLayer()

for _ in range(n_layers)])

def forward(self, dec_inputs, enc_inputs, enc_outpu

ts):

'''

dec_inputs: [batch_size, tgt_len]

enc_intputs: [batch_size,

src_len]

enc_outputs: [batsh_size, src_len, d_model]

经过6次encoder之

后得到的东西

'''

dec_outputs = self.tgt_emb(dec_inputs)

# [batch_size,

tgt_len, d_model]

# 同样地，对

decoder_layer进行词向量的生成

dec_outputs =

self.pos_emb(dec_outputs.transp

ose(0, 1)).transpose(0,

1).cuda()

# 计算

它的位置向量

# [batch_size, tgt_len, d_model]

dec_self_attn_mask =

creat_self_mask(dec_inpu

ts, dec_inputs)

# [batch_size, tgt_len,

tgt_len]

#dec_self_attn_subsequence_mask =

create_look_ahead_mask(dec_inputs).cuda()

# [batch_size,

tgt_len, tgt_len]

# 当前时刻

看不到未来时刻的东西

dec_enc_attn_mask = create_look_ahead_mask(en

c_inputs,dec_inputs)

# [batch_size, tgt_len, tgt_len]

#

布尔+int false 0 true 1，gt 大

于 True

# 这样把dec_self_attn_pad_mask和

dec_self_attn_subsequence_mask里面

为

True的部分都剔除掉了

# 也

就是说，既屏蔽掉了pad，也屏

蔽掉了mask

# 在decoder的第二个attention里面

使用

dec_self_attns, dec_enc_attns

= [], []

for layer in

self.layers:

# dec_outputs: [batch_size, tgt_len,

d_model],

# dec_self_attn: [batch_size, n_head

s, tgt_len,

tgt_len],

# dec_enc_attn: [batch_size, h_heads

,

tgt_len, src_len]

dec_outputs, dec_self_attn, dec_enc_a

ttn

= layer(dec_outputs,

enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)

dec_self_attns.append(dec_self_attn)

dec_enc_attns.append(dec_enc_attn)

return dec_outputs, dec_self_attns, dec_enc_

attns

# ***********************************************#

class Transformer(nn.Module):

def __init__(self):

super(Transformer, self).__init__()

self.encoder = Encoder().cuda()

self.decoder

= Decoder().cuda()

self.projection = nn.Linear(d_model, tgt_voc

ab_size, bias=False).cuda()

# 对decoder的输出转换维度

# 从隐藏层维数->英语单词

词典大小（选取概率最大

的那一个，作为预测结果

）

def

forward(self, enc_inputs, dec_inputs):

'''

enc_inputs维度：[batch_size, src_len]

对于encoder-input，一个batch中有几个

sequence，

一个sequence有几个字

dec_inputs: [batch_size, tgt_len]

对于decoder-input，一个

batch中有几个sequence，

一个sequence有几个字

'''

# enc_outputs: [batch_size, src_len, d_mode

l]，

# d_model是每一个字的Word Embedding长度

"""

enc_self_attns:

[n_layers, batch_size, n_hea

ds, src_len, src_len]

注意

力矩阵，对于encoder和decoder，每一层、每

一

句话、每一个头、每两个

字之间都有

一个权重系

数，

这些权重系数组成了

注意力矩阵(之后的

dec_self_attns同理

，当然decoder还有一个

decoder-encoder的矩阵)

"""

enc_outputs, enc_self_attns = self.encoder(e

nc_inputs)

# dec_outputs: [batch_size, tgt_len, d_mode

l],

# dec_self_attns: [n_layers, batch_size, n_

heads,

tgt_len, tgt_len],

# dec_enc_attn: [n_layers, batch_size,

tgt_

len, src_len]

dec_outputs, dec_self_attns, dec_enc_attns

=

self.decoder(dec_inputs,

enc_inputs, enc_outputs)

dec_logits =

self.projection(dec_outputs)

# 将

输出的维度

从 [batch_size, tgt_len,

d_model]变成

[batch_size, tgt_len,

tgt_vocab_size]

# dec_logits:

[batch_size, tgt_len, tgt_voc

ab_size]

return dec_logits.view(-1,

dec_logits.size(-1

)), enc_self_attns,

dec_self_attns, dec_enc_attns

#

dec_logits 的维

度

是 [batch_size * tgt_len,

tgt_vocab_size]，可以理解为一个

句

子

# 这个句子有batch_size*tgt_len个单词，每

个单词有

tgt_vocab_size种情况，取概率

最大者

#

Transformer主要是调用 Encoder 和 Decoder

# 最

后返回

***********************************************#

save_path = "./saver/transformer.pt"

device =

"cuda"

model = Transformer()

model.to(device)

#model.load_state_dict(torch.load(save_path))

criterion = nn.CrossEntropyLoss(ignore_index=0)

optimizer = optim.AdamW(model.parameters(),

lr=2e-

5)

# ***********************************************#

for epoch

in range(1024):

pbar = tqdm((loader), total=len(loader))

# 显示

进度条

for enc_inputs, dec_inputs,

dec_outputs in pbar:

enc_inputs, dec_inputs, dec_outputs

= enc_i

nputs.to(device),

dec_inputs.to(device), dec_outputs.to(device)

#

outputs: [batch_size * tgt_len, tgt_voca

b_size]

outputs, enc_self_attns, dec_self_attns, dec

_enc_attns =

model(enc_inputs, dec_inputs)

loss = criterion(outputs, dec_outputs.view(-

1))

optimizer.zero_grad()

loss.backward()

optimizer.step()

pbar.set_description(f"epoch {epoch

+ 1} :

train loss {loss.item():.6f}

") # : learn_rate {lr_scheduler.get_last_lr()[0]:.6f}

torch.save(model.state_dict(),

save_path)

idx2word = {i: w for

i, w in enumerate(tgt_vo

cab)}

enc_inputs,

dec_inputs, dec_outputs = next(iter(l

oader))

predict,

e_attn, d1_attn, d2_attn = model(enc_in

puts[0].view(1,

-1).cuda(),

dec_inputs[0].view(1, -1).cuda())

predict = predict.data.max(1,

keepdim=True)[1]

print(enc_inputs[0], '-

>', [idx2word[n.item()] for

n in predict.squeeze()])

以上

就是Transformer的结构代码，实际上

就是综合了

前面所学的

全部知识，结合编码器和

解码器。读者可

以对以上

代码进行测试，测试代码

如下：

if __name__ == "__main__":

encoder_input =

tf.keras.Input(shape=(None,))

decoder_input = tf.keras.Input(shape=(None,))

output =

Transformer(1024,1024)

([encoder_input,decoder_input])

model = tf.keras.Model((encoder_input,decoder_input),

output)

print(model.summary())

打印结果请读者自

行验证。

12.2.3 汉字拼音模型的

训练

下面是Transformer的训练。需要

注意的是，相对于第

11章的

学习，Transformer的训练过程要注意

编码器的

输出和解码器

的输入的“错位计算”

，说明

如下。

·第1次输入：编码器输

入完整的序列

[GO]ni hao ma[END]。与此同时

，解码器的输入端

是输入

的解码开始符“GO”

，经过交互

计算后，解码

器的输出为

“你”。

·第2次输入：编码器输入

完整的序列

[GO]ni hao

ma[END]。与此同时，解

码器的输入端

是输入的

解码开始符“GO”和字符“你”

，经

过交互

计算后，解码器的

输出为“你好”。

这样依次进

行多次的输入和输出。

·最

后一次输入：编码器的输

入还是完整序列，而此

时

在解码器的输出端会输

出带有结束符的序列，表

明

解码结束。

第1次输入：

编

码器输入：[GO]ni hao ma[END]

解码器输入：[GO]

解

码器输出：你

第2次输入：

编

码器输入：[GO]ni hao ma[END]

解码器输入：[GO]你

解码器输出：你 好

第3次输

入：

编码器输入：[GO]ni hao ma[END]

解码器输

入：[GO]你 好

解码器输出：你 好

吗

最终次输入：

编码器输

入：[GO]ni

hao ma[END]

解码器输入：[GO]你 好 吗

解

码器输出：你

好 吗 [END]

计算步

骤如图12-10所示。

图12-10 计算步骤

在训练过程中，由于硬件

计算显存的大小限制，一

般

需要用数据生成器（PyTorch中

的DataLoader）循环生

成数据，并且在

生成器中进行错位输入

，具体请读者

自行完成。

12.2.4 汉

字拼音模型的使用

相信

读者已经发现，相对于汉

字拼音转换模型，汉字

拼

音翻译模型并不是整体

一次性输出，而是根据在

编

码器中输入的内容生

成特定的输出内容。

根据

这个特性，如果想获取完

整的解码器生成的数据

内容，就需要采用循环输

入的方式完成模型的使

用，

代码如下。

【程序12-6】

idx2pinyin

= {i: w for i, w

in enumerate(tgt_vocab)}

idx2hanzi = {i: w

for i, w in enumerate(src_vocab)}

context

= "你好吗

"

token = [src_vocab[char] for

char in context]

token = torch.tensor(token)

sentence_tensor = torch.LongTensor(token).unsqueeze(0).to(d

evice)

outputs =

[1]

for i in range(tgt_len):

trg_tensor

= torch.LongTensor(outputs).unsqueeze(0)

.to(device)

with torch.no_grad():

output=

model(sentence_tensor, trg_tensor)

best_guess = torch.argmax(output,dim=-1).detach()

.cpu()

outputs.append(best_guess[-1])

# if best_guess[-1] == 2:

# break

print([idx2pinyin[id.item()] for id in

outputs[1:]])

代码演示了循环输出预

测结果的示例，这里使用

了一

个for循环对预测进行

输入，具体请读者自行验

证。

12.3 本章小结

首先回答10.2节

提出的两个问题。

（1）问题1：与

编码器的转换模型相比

，编码器-解码

器的翻译模

型有什么区别？

答：对于转

换模型来说，模型在工作

时不需要对其进

行处理

，默认所有的信息都包含

在编码器编码的词嵌

入

Embedding中，最后直接进行Softmax计算即

可。而

编码器-解码器的翻

译模型需要综合编码器

的编码内容

和解码器的

原始输入共同完成后续

的交互计算。

（2）问题2：如果想

做汉字→拼音的翻译系统

，编码

器和解码器的输入

端分别输入什么内容？

答

：编码器的输入端输入的

是汉字，解码器的输入端

输入的是错位的拼音。

本

章和第10章是相互衔接的

，主要针对最新的

Transformer模型，从

其架构入手，详细介绍其

主要架

构部分、编码器和

解码器，以及各种ticks和编程

小细

节，有针对性地对模

型优化做了说明。

读者在

学习这两章的时候一定

要仔细阅读，掌握全部

内

容，相对于目前和后续的

自然语言处理的问题，

Transformer架

构是最重要和最基础的

内容。

第13章

我也可以成为

马斯克

——无痛的基于PyTorch的强

化学习实战

强化学习（Reinforcement Learning, RL）又

称再

励学习、评价学习或

增强学习，是机器学习的

范式和

方法论之一，用于

描述和解决智能体（Agent）在与

环

境交互的过程中，通过

学习策略达成回报最大

化或实

现特定目标的问

题。

换句话说，强化学习是

一种学习如何从状态映

射到行

为以使获取的奖

励最大的机制。这样的一

个Agent需要

不断地在环境中

进行实验，通过环境给予

的反馈（奖

励）来不断优化

状态-行为的对应关系，如

图13-1所

示。因此，反复实验（Trial and Error）和

延迟奖

励（Delayed Reward）是强化学习最

重要的两个特

征。

图13-1 强化

学习

凭借ChatGPT的成功，强化学

习从原本不太受重视到

一

跃而起，成为协助ChatGPT登顶

的一个重要辅助工具。

本

章将讲解强化学习方面

的内容，尽量少地使用公

式

来说明，而采用图示或

者文字的方式介绍其理

论。

13.1 实战：基于强化学习的

火箭回收

我们也可以成

为马斯克，这并不是天方

夜谭。对于马

斯克来说，他

创立的SpaceX公司制造的猎鹰

火箭回收

技术处于世界

顶端地位。难道说这个火

箭回收技术对

于深度学

习者来说，就是一个遥不

可及的梦吗？答案

是否定

的。中国的老子说过“九层

之台，起于累土；

千里之行

，始于足下”。我们将从头开

始，进行火箭

回收实战。火

箭回收技术的示意图如

图13-2所示，读

者可以先运行

一下配套代码中的火箭

回收程序，以便

对火箭回

收有个感性认识。

图13-2 火箭

回收

13.1.1 火箭回收技术基本

运行环境介绍

前面介绍

了强化学习的基本内容

，本小节需要完成基

于强

化学习的火箭回收实战

，也就是通过强化学习的

方案完成对火箭的控制

系统，从而将其正常地降

落。

首先是项目环境的搭

建，在这里读者要有一定

的深度

学习基础，并准备

相应的环境，即Python的运行环

境

Miniconda以及PyTorch 2.0框架。除此之外，还

需要

一个专用的深度学

习框架Gym，这是一个用于测

试强化

学习算法的工具

包，以游戏过程的形式调

试不同的算

法。它是一个

不依赖强化学习实现的

具体编程框架，

PyTorch 2.0也可以直

接对Gym进行调用，读者只需

要

关注强化学习部分即

可，所以我们只需要考虑

“状

态”→“神经网络”→“动作”就行

了。

对Gym的安装如下：

pip install gym

pip install box2d

box2d-kengz --user

这里请

注意，如果安装报错，请自

行查询相关的技术

网页

进行解决。为了验证具体

的安装情况，执行以下

代

码：

import

gym

import time

# 环境初始化

env

= gym.make('LunarLander-v2', render_mode='human')

if True:

state

= env.reset()

while True:

# 渲染画面

#

env.render()

# 从动作空间随机获取一

个动作

action = env.action_space.sample()

# Agent与环境进行一次

交互

observation, reward, done, _

, _= env.s

tep(action)

print('state =

{0}; reward = {1}'.format

(state, reward))

# 判断当前episode 是否完成

if done:

print('游戏结束')

break

time.sleep(0.01)

# 环境结束

env.close()

代码

首先导入Gym库的运行环境

，即完成了火箭回收的

环

境配置，读者通过运行此

代码段可以看到如图13-3

所

示的界面。

图13-3 火箭回收运

行界面

这是火箭回收的

主要展示界面，而在PyCharm下方

的输

出框中会输出如图

13-4所示的内容。

图13-4

PyCharm运行代码

输出的数据界面

13.1.2 火箭回

收参数介绍

在13.1.1节最后的

代码输出结果中打印了

火箭回收的

state参数，这是火

箭回收过程中的环境参

数值，也就

是可以通过观

测器获取到的火箭状态

数值，这些参数

包括：

·水平

坐标x。

·垂直坐标y。

·水平速度

。

·垂直速度。

·角度。

·角速度。

·腿

1触地。

·腿2触地。

而对于操作

者来说，可以通过4种离散

的行动对火箭进

行操作

，分别如下：

·0：代表不采取任

何行动。

·2：代表主引擎向下

喷射。

·1、3：分别代表向左、向右

喷射。

除此之外，对于火箭

还有一个最终的奖励，即

对于每

一步的操作都要

额外计算分值，如下所示

：

·小艇坠毁得到-100分。

·小艇在

旗帜之间成功着地则得

100～140分。

·喷射主引擎（向下喷火

）每次-0.3分。

·小艇最终完全静

止再得100分。

·腿1或是腿2都落

地能获得10分。

13.1.3 基于强化学

习的火箭回收实战

本小

节完成基于强化学习的

火箭回收实战，完整的代

码如下（请读者运行本章

配套源码中的火箭回收

代

码，第一次学会运行即

可，对于部分代码段的理

解可

参考13.2节的算法部分

）：

import matplotlib.pyplot as plt

import torch

from torch.distributions import Categorical

import gym

import time

import numpy

as np

import random

from IPython

import display

class Memory:

def __init__(self):

"""初始化"""

self.actions = [] # 行动(共4

种)

self.states = [] # 状态，由

8个数字组成

self.logprobs = [] # 概率

self.rewards

= [] # 奖励

self.is_dones =

[] # 游

戏是否结

束 is_terminals

def

clear_memory(self):

del self.actions[:]

del self.states[:]

del

self.logprobs[:]

del self.rewards[:]

del self.is_dones[:]

class

Action(torch.nn.Module):

def __init__(self, state_dim=8, action_dim=4):

super().__init__()

# actor

self.action_layer = torch.nn.Sequential(

torch.nn.Linear(state_dim,

128),

torch.nn.ReLU(),

torch.nn.Linear(128, 64),

torch.nn.ReLU(),

torch.nn.Linear(64,

action_dim),

torch.nn.Softmax(dim=-1)

)

def forward(self, state):

action_logits = self.action_layer(state) #

计算4个方向

概率

return

action_logits

class Value(torch.nn.Module):

def __init__(self, state_dim=8):

super().__init__()

# value

self.value_layer = torch.nn.Sequential(

torch.nn.Linear(state_dim, 128),

torch.nn.ReLU(),

torch.nn.Linear(128, 64),

torch.nn.ReLU(),

torch.nn.Linear(64, 1)

)

def forward(self, state):

state_value = self.value_layer(state)

return state_value

class

PPOAgent:

def __init__(self,state_dim,action_dim,n_latent_var,lr

,betas,gamma,

K_epochs, eps_clip):

self.lr = lr

# 学习率

self.betas

= betas

# betas

self.gamma =

gamma

# gamma

self.eps_clip = eps_clip

# 裁剪限制值

范围

self.K_epochs = K_epochs

#

获取的每批次的数

据作为训练使用的次数

# action

self.action_layer = Action()

#

critic

self.value_layer = Value()

self.optimizer =

torch.optim.Adam([{"params":

self.action_layer.

parameters()},

{"params":self.value_layer.parameters()}], lr=lr, betas=bet

as)

#损失函数

self.MseLoss = torch.nn.MSELoss()

def

evaluate(self,state,action):

action_probs = self.action_layer(state)#这里

输出的结

果是4类别的[-1,4]

dist

= Categorical(action_probs) # 转

换成类别

分布

#

计算概率密度，log(概率

)

action_logprobs = dist.log_prob(action)

# 计算信息熵

dist_entropy = dist.entropy()

# 评判，对当前

的状态进行评判

state_value

= self.value_layer(state)

# 返回行

动概率密度、评判值、行动

概率熵

return action_logprobs,

torch.squeeze(state_v

alue), dist_entropy

def update(self,memory):

#

预测状态回报

rewards = []

discounted_reward =

0 #discounted = 不

重要

#这里可以这样理解

，当前步骤决定未来的步

骤，而模

型需要根据当前

步骤对未来的最终结果

进行修正，如果遵循了现

在的步骤，就可以看到未

来的结果如何

#而未来的

j结果会很差，所以模型需

要远离会造成坏

的结果

的步骤，所以就反过来计

算

for reward, is_done in

zip(reversed(memory.

rewards),

reversed(memory.is_dones)):

# 回合结束

if

is_done:

discounted_reward = 0

# 更新削减奖

励(当前状态奖励

+ 0.99*

上一状

态奖励

discounted_reward = reward

+ (self.

gamma * discounted_reward)

#

首插入

rewards.insert(0, discounted_reward)

#print(len(rewards)) #这里的长

度

是根据batch_size的长度设置的

#

标准化奖励

rewards = torch.tensor(rewards, dtype=torch.

float32)

rewards = (rewards - rewards.mean()) /

(r

ewards.std() + 1e-5)

#print(len(self.memory.states),len(self.memory.

actions),

len(self.memory.logprobs)) # 这里的长度

是根

据batch_size的长度设置的

# 张

量转换

# convert list to tensor

old_states

= torch.tensor(memory.states)

old_actions = torch.tensor(memory.actions)

old_logprobs

= torch.tensor(memory.logprobs)

#迭代优化 K 次

for

_ in range(5):

# Evaluating old

actions and value

s : 新策

略，重用旧样本进行训练

logprobs,

state_values, dist_entropy

= self.evaluate(old_states,

old_actions)

ratios

= torch.exp(logprobs - old

_logprobs.detach())

advantages

= rewards - state_values

.detach()

surr1

= ratios * advantages

surr2 =

torch.clamp(ratios, 1 - se

lf.eps_clip,1 +

self.eps_clip) *

advantages

loss = -

torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_v

alues,

rewards) - 0.01 * dist_entropy

# take gradient step

self.optimizer.zero_grad()

loss.mean().backward()

self.optimizer.step()

def act(self,state):

state = torch.from_numpy(state).float()

# 计算4个方向的概率

action_probs = self.action_layer(state)

#

通过

最大概率计算最终行动

方向

dist = Categorical(action_probs)

#这个是根据action_probs做出符

合分布action_probs

的抽样结果

action = dist.sample()

return action.item(),dist.log_prob(action)

state_dim

= 8 ###

游戏

的状态是一个8维向量

action_dim =

4 ### 游

戏的输出有4个取值

n_latent_var = 128

# 神经

元个

数

update_timestep = 1200

# 每1200步

policy更新一次

lr = 0.002

#

learning rate

betas = (0.9,

0.999)

gamma = 0.99 # di

scount factor

K_epochs = 5 #

po

licy迭

代更新次数

eps_clip = 0.2 #

clip

parameter for PPO 论文中表明

0.2效果不错

random_seed

= 929

agent = PPOAgent(state_dim ,action_dim,n_latent_va

r,lr,betas,gamma,

K_epochs,eps_clip)

memory = Memory()

#

agent.network.train()

EPISODE_PER_BATCH = 5 # update

the agent

every 5 episode

NUM_BATCH

= 200 # totally update

the

agent for 400 time

avg_total_rewards, avg_final_rewards

= [], []

env = gym.make('LunarLander￾v2',

render_mode='rgb_array')

rewards_list = []

for i

in range(200):

rewards = []

#

collect trajectory

for episode in range(EPISODE_PER_BATCH):

### 重开一把游戏

state = env.reset()[0]

while True:

#这里agent做出act动作后，数据已

经被存储

了。注意这里记

录动作时并没有对模型

进

行即时更新，而是使用

上一轮的模型进行此轮

的“数据收集”工作

with torch.no_grad():

action,action_prob

= agent.ac

t(state) ### 按照策

略网络输出的概率随

机

采样一个动作

memory.states.append(state)

memory.actions.append(action)

memory.logprobs.append(action_p

rob)

next_state, reward,

done, _, _ =

env.step(action) ###

与环境state

进

行交互，输出reward和环境next_state

state = next_state

rewards.append(reward)

### 记录

每一

个动作的reward

memory.rewards.append(reward)

memory.is_dones.append(done)

if

len(memory.rewards) >= 1200:

agent.update(memory)

memory.clear_memory()

if

done or len(rewards) > 1024:

rewards_list.append(np.sum(rewa

rds))

#print('游戏结束

')

break

print(f"epoch: {i} ,rewards

looks like ", rewards

_list[-1])

plt.plot(range(len(rewards_list)),rewards_list)

plt.show()

plt.close()

env = gym.make('LunarLander￾v2', render_mode='human')

for episode in range(EPISODE_PER_BATCH):

### 重开一把游戏

state = env.reset()[0]

step = 0

while True:

step += 1

#这里agent做出

act动作后，数据已经被存储

了。注意

这里记录动作时

并没有对模型进行即

时

更新，而是使用上一轮的

模型进行此轮的“数据收

集”工作

action,action_prob = agent.act(state) ###

按照策略网络输

出的概率随机采样一

个

动作

# agent与环境进行一次交

互

state, reward,

terminated, truncated, info

= env.step(action)

#print('state

= {0}; reward = {1}'.format(

state,

reward))

# 判断当前episode 是否完成

if terminated

or step >= 600:

print('游

戏结束')

break

time.sleep(0.01)

print(np.mean(rewards_list))

此时火箭回收的

最终得分如图13-5所示。

图13-5 火

箭回收的得分图

13.1.4

强化学

习的基本内容

在进入实

战之前，还需要了解一些

强化学习的基本思

想、相

关思路和方法。

1. 强化学习

的总体思想

强化学习背

后的思想是，代理（Agent）将通过

与环境

（Environment）的动作（Action）交互，进而

获得奖

励（Reward）。从与环境的交

互中进行学习，这一思

想

来自我们的自然经验，想

象一下当你是个孩子的

时

候，看到一团火，并尝试

接触它，如图13-6所示。

图13-6 看到

一团火

你觉得火很温暖

，感觉很开心（奖励+1），就会觉

得

火是个好东西，如图13-7所

示。

图13-7 感觉很开心

一旦你

尝试去触摸它，就会狠狠

地被教育，即火把你

的手

烧伤了（惩罚-1）。你才明白只

有与火保持一定

距离，才

会产生温暖，火才是个好

东西，但不能靠太

近，如果

太过靠近，就会烧伤自己

，如图13-8所示。

图13-8 手被烧伤

这

是人类通过交互进行学

习的过程。强化学习是一

种

可以根据行为进行计

算的学习方法，如图13-9所示

。

图13-9 强化学习的过程

举个

例子，思考如何训练Agent学会

玩超级玛丽游戏。

这个强

化学习过程可以被建模

为如下示例的一组循环

过程（0、1……即为上图中的t下标

）：

·Agent从环境中接收到状态S0。

·基

于状态S0，Agent执行A0操作。

·环境转

移至新状态S1。

·环境给予R1奖

励。

·……

强化学习的整体过程

中会循环输出状态、行为

、奖励

的序列，而整体的目

标是最大化全局reward期望。

2.

强

化学习的奖励与衰减

奖

励与衰减是强化学习的

核心思想，在强化学习中

，

为了得到最好的行为序

列，我们需要最大化累积

reward期望，也就是奖励的最大

化是强化学习的核心

目

标。

对于奖励的获取，每个

时间步的累积reward可以写

作

：

等价于：

相对于长期的奖

励来说，更简单的是对近

期的奖励的

获取。因为相

对于长期的奖励，短期的

奖励来得快，

且发生的概

率非常大，因此比长期的

奖励更容易预

测。

用一个

猫捉老鼠的例子来介绍

，如图13-10所示。图中

Agent是老鼠，对

手是猫，目标是老鼠在被

猫吃掉之

前，先吃掉最多

的奶酪。从图中可以看到

，吃掉身边

的奶酪要比吃

掉猫旁边的奶酪容易许

多。

图13-10 长期与短期激励

老

鼠一旦被猫抓住，游戏就

会结束，猫身边的奶酪奖

励就会有衰减，因此也要

把这个考虑进去，对折扣

的

处理如下（定义Gamma为衰减

比例，取值范围为0～

1）：

·Gamma越大，衰

减越小。这意味着Agent的学习

过程

更关注长期的回报

。

·Gamma越小，衰减越大。这意味着

Agent更关注短期

的回报。

衰减

后的累计奖励期望为：

每

个时间步间的奖励将与

Gamma参数相乘，以获得衰减

后

的奖励值。随着时间步骤

的增加，猫距离我们更

近

，因此未来的奖励概率将

变得越来越小。

3. 强化学习

的任务分类

任务是强化

学习问题中的基础单元

，可以有两类任

务：事件型

与持续型。

事件型任务指

的是在一个任务中，有一

个起始点和终

止点（终止

状态）。这会创建一个事件

：一组状态、

行为、奖励以及

新奖励。对于超级玛丽游

戏来说，一

个事件从游戏

开始进行记录，直到角色

被杀结束，如

图13-11所示。

图13-11 事

件型任务

持续型任务意

味着任务不存在终止状

态。在这种任务

中，Agent将学习

如何选择最好的动作，并

与环境同步

交互。例如通

过Agent进行自动股票交易，如

图13-12

所示。在这种任务中，并

不存在起始点和终止状

态，

直到我们主动终止，Agent将

一直运行下去。

图13-12 持续型

任务

4. 强化学习的基本处

理方法

对于一般的强化

学习来说，其主要的学习

与训练方法

有两种，分别

是基于值函数的学习方

法与基于策略梯

度的学

习方法。分别说明如下：

·基

于值函数的学习方法：学

习价值函数，计算每个

动

作在当前环境下的价值

，目标就是获取最大的动

作

价值，即每一步采取回

报最大的动作和环境进

行互

动。

·基于策略梯度的

学习方法：学习策略函数

，计算当

前环境下每个动

作的概率，目标是获取最

大的状态价

值，即该动作

发生后期望回报越高越

好。

·AC（Actor-Critic）算法：融合了上述两种

方法，

将价值函数和策略

函数一起进行优化。价值

函数负责

在环境学习并

提升自己的价值判断能

力，而策略函数

则接受价

值函数的评价，尽量采取

在价值函数中可以

得到

高分的策略。

读者可以参

考走迷宫的例子进行理

解。

其中基于值的方法是

对模型迷宫中每个步骤

的环境进

行评分，如图13-13所

示。

图13-13 对每个环境本身进

行打分

在迷宫问题中，每

一步都对周围环境进行

打分，并选

择得分最大的

前进，即-7、-6、-5等。

而在基于策略

梯度的方案中，由模型对

行走者的每个

动作进行

打分，如图13-14所示。

图13-14 对行走

者的每个动作进行打分

在这个过程中，每个动作

就是其行进方向是由模

型决

定的。

在这两个方法

中，一个施加在环境中，另

一个施加在

行走的人上

，其中各有各的利弊，为了

取长补短，一

种新的处理

方法——AC算法被提出，如图13-15所

示。

图13-15 AC算法（Action & Critic Method）

在这里的AC算法

将基于值和基于策略梯

度的算法做了

结合，同时

对环境和环境的使用者

进行建模，从而可

以获得

更好的环境适配性。

AC算法

分为两部分，Actor用的是policy gradient，

它可

以在连续动作空间内选

择合适的动作；Critic用

的是Q-Learning，它

可以解决离散动作空间

的问题。

除此之外，因为Actor只

能在一个回合之后进行

更新，

导致学习效率较慢

，Critic的加入使得可以使用TD方

法实现单步更新。这样两

种算法相辅相成，就形成

了

AC算法。

Actor输出每个Action的概率

，有多少个Action就有多

少个输

出。Critic基于Actor输出的行为评判

得分，

Actor再根据Critic的评分修改

该行为的概率。

13.2 强化学习

的基本算法

——PPO算法

在一般

的强化学习过程中，一份

数据只能进行一次更

新

，更新完就只能丢掉，等待

下一份数据。但是这种

方

式对深度学习来说是一

种极大的浪费，尤其在强

化

学习中，获取到的数据

更是弥足珍贵。

因此，我们

需要一种新的算法，通过

获得的完整数据

进行模

型的多次更新，即将每次

获取到的数据进行多

次

利用，而对数据进行多次

利用的算法中，具有代表

性的就是PPO（Proximal Policy Optimization，近

端策略优化

）算法。

13.2.1 PPO算法简介

PPO算法属于

AC算法框架下的一种强化

学习代表算法，

在采样策

略梯度算法训练的同时

，还可以重复利用历

史的

采样数据进行网络参数

更新，提升了策略梯度方

法的效率。

PPO算法的突破在

于对新旧策略函数的约

束，希望新的

策略网络与

旧的策略网络越接近越

好，即实现近端策

略优化

的本质目的是：新的策略

网络可以利用旧的策

略

网络学习到的数据进行

学习，不希望这两个策略

相

差很大。PPO算法的损失函

数如下：

其参数说明如下

。

clip

·L：价值网络的评分，即Critic网络

的评分结果，采

用Clip的方式

使得新旧网络的差距不

要过大。

vf

·L：价值网络预测的

结果和真实环境的回报

值越接近

越好。

·S：策略网络

的输出结果，这个值越大

越好，目的是

希望策略网

络的输出分布不要太过

集中，以提高不同

动作在

环境中发生的可能性。

13.2.2 函

数使用说明

在讲解PPO算法

时，需要用到一些特定的

函数，这些函

数是我们以

前没有用过的，在这里详

细说明一下。

1. Categorical类

Categorical类的作用

是根据传递的数据概率

建立相应

的数据抽样分

布，其使用如下：

import

torch

from torch.distributions import Categorical

action_probs

= torch.tensor([0.3,0.7]) #人工建立

一个概

率值

dist =

Categorical(action_probs) #根据概率

建

立分布

c0 = 0

c1 = 1

for _ in

range(10240):

action = dist.sample()

#根据概率分布进

行抽样

if

action == 0:

#对抽样结果进行

存储

c0 +=

1

else:

c1 += 1

print("c0的概率为：",c0/(c0

+ c1)) #打印输出

的

结果

print("c1的概率为：",c1/(c0 +

c1))

首先人

工建立一个概率值，然后

Categorical类帮助我

们建立依照这

个概率构成的分布函数

，sample的作用

是依据存储的概

率进行抽样。从最终的打

印可以看

到，输出的结果

可以反映人工概率的分

布。

c0的概率为：

0.294795430133776

c1的概率为： 0.705204569866224

2. log_prob函

数

log_prob(x)函数用来计算输入数

据x在分布中对于概

率密

度的对数，读者可以通过

如下代码进行验证：

import torch

from torch.distributions import

Categorical

action_probs = torch.tensor([0.3,0.7])

#输出

不同分布的log值

print(torch.log(action_probs))

#根据概率

建立一个分布并抽样

dist = Categorical(action_probs)

action =

dist.sample()

#获

取抽样结果对应的分布

log值

action_logprobs = dist.log_prob(action)

print(action_logprobs)

通过打印可以看到，首

先输出了不同分布的log值

，然

后可以反查出不同取

值所对应的分布log值。

tensor([-1.2040, -0.3567])

tensor(-1.2040)

3.

entropy函数

前面在讲解过程中用到

了交叉熵（crossEntropy）相

关的内容，而

entropy用于计算数据中蕴含的

信息量，

在这里熵的计算

如下：

import torch

from

torch.distributions import Categorical

action_probs = torch.tensor([0.3,0.7])

#自己定义的entropy实现

def entropy(data):

min_real = torch.min(data)

logits = torch.clamp(data,min=min_real)

p_log_p = logits

* torch.log(data)

return -p_log_p.sum(-1)

print(entropy(action_probs))

此

时读者可以对自定义的

entropy与PyTorch

2.0自带

的entropy计算方式进行

比较，代码如下：

import torch

from torch.distributions

import Categorical

action_probs = torch.tensor([0.3,0.7])

#根据概率

建立一个分布并抽样

dist = Categorical(action_probs)

dist_entropy = dist.entropy()

print("dist_entropy:",dist_entropy)

#自

己定义的entropy实现

def entropy(data):

min_real =

torch.min(data)

logits = torch.clamp(data,min=min_real)

p_log_p =

logits * torch.log(data)

return -p_log_p.sum(-1)

print("self_entropy:",entropy(action_probs))

从最终结

果可以看到，两者的计算

结果是一致的。

dist_entropy: tensor(0.6109)

self_entropy: tensor(0.6109)

13.2.3

一学就会

的TD-Error理论介绍

本节讲解TD-Error理

论，这个理论主要让读者

明确一

个分段思维的方

法，而不能以主观的评价

经验对事物

进行估量。

这

个TD-Error是ChatGPT中的一个非常重要

的理论算

法，TD-Error用于动态地

解决后续数据量的估算

问

题。TD-Error示例如图13-16所示。

图13-16 TD-Error示

例

1. 项目描述与模型预估

在图13-16右侧，一名司机驾车

从NYC到Atlanta，中途

有个中转站DC。按

照现有的先验知识，可以

获得如下

内容：

·NYC到Atlanta的距离

为90千米，而DC是距离出发点

NYC300千米的中转站。

·训练好的

模型预估整体路途需要

耗时1000分钟。

·训练好的模型

预估从NYC到DC耗时400分钟。

·训练

好的模型预估从DC到Atlanta耗时

600分钟。

这是对项目的描述

，完整用到预估的知识。需

要注意

的是，整体1000分钟的

耗时是由离线模型在出

发前预

先训练好的，不能

根据具体情况随时调整

。

2. 到达DC后模型重新估算整

体耗时

当司机实际到达

中转站DC时，发现耗时只有

300分钟，

此时如果模型进一

步估算余下的路程所需

要的时间，

按照出发前的

估算算法，剩余时间应该

为1000-

400 = 600分钟。因此，若模型此时

重新估算整体路

程耗费

的时间，可以用如下公式

得到：

900=300+600

这是模型在DC估算的

整体用时，其中300为起始到

DC的

耗时，而600为模型按原算

法估算的DC到Atalanta的耗

时，900为已

训练模型在DC估算的总体

耗时。

此时如果在DC中转站

重新进行模型训练，整体

耗时的

target就会变为900。

3.

问题

可

能有读者会问，为什么不

用按比例缩短的剩余时

间

进行估算，即剩下的时

间变为：

V =600×(300)/400

future

这样做的问题在

于，我们需要相信前期模

型做出的预

测是基于良

好训练的一个可信度很

高的值，不能人为

地随意

对整体的路途进行修正

，即前一段路途可能由

于

种种原因（顺风、逆风等）造

成了时间变更，但是

并不

能保证在后续的路途同

样会遇到这样的情况。

因

此，在剩余的模型拟合过

程中，依旧需要假定模型

对未来的原始拟合是正

确的，而不能加入自己的

假

设。

可能有读者会继续

问，如果下面再遇到一些

事，修正

了原计划的路途

，怎么办？一个非常好的解

决办法就

是以那个时间

段为中转站重新训练整

个模型。把前面

路过的作

为前面部分，后面没有路

过的作为后面部分

来处

理。

进一步说明，在这个问

题中，我们把整体的路段

分成

了若干份，每隔一段

就重新估算时间，这样使

得最终

的时间与真实时

间的差值不会太大。

4. TD-Error

此时

，模型整体估算的差值100

= 1000-900，这

一

点相信读者很容易理

解，即TD-Error代表现阶段（也

就是

在DC位置）的估算时间与真

实时间的差值为100。

对于这

个用法，可以看到这里的

TD-Error实际上就是

根据现有的

误差修正整体模型的预

估结果，这样可以

使得模

型在拟合过程中更好地

反映真实的数据。

13.2.4 基于TD-Error的

结果修正

下面的内容会

涉及PPO算法的一些细节。

1. 修

正后的模型生成的结果

不应该和未修正的模型

有太大差别

继续前面的

例子，如果按原始的假设

，对总路程进行

拟合分析

，在DC中转站估算的耗费时

间为：

·错误的模型估算时

间：

300 + 600×(300)/400

= 750

·模型应该输入的时间

：300 + 600 =

900

分析结果错误的原因，除

了主观地将前期时间损

耗同

样按比例施加在后

期未发生的路程之外，还

有一个较

为重要的因素

是相对原始的估算值1000，模

型对于每

次修正的幅度

太大（错误的差距为250，而正

确的差距

为100），这样并不适

合模型尽快使用已有的

数据重新

拟合剩下路程

的耗费时间。换算到模型

输出，其决策

器的输出跳

跃比较大，很有可能造成

模型失真的问

题。

下面回

到PPO算法的说明，这样对于

每次做出动作的决

定，决

策器Policy会根据更新做出一

个新的分布，我

们可以将

其记作pθ′(at|st)

，而对于旧的

pθ(at|st) ，这两

个分布差距太大的话，也

就是修

正值过大，会使得

模型接受不了。读者可以

参考下面

两个分布的修

正过程：

·[0.1,0.2]→[0.15,0.20]→[0.25,0.30]→[0.45,0.

40]：一个好的分布修

正过程。

·[0.1,0.2]→[1.5,0.48]→[1.2,4.3]→[-0.1,0.7]

：一个坏的分布修

正过程。

这部分的实现可

以参考源码的这条代码

进行解读：

ratios =

torch.exp(logprobs - old_logprobs.detach())

其具体公式如

下：·

使用数据存储器π去收

集数据，数据来源是θ，这是

初始模型。

θ

·使用π中收集的

数据去重新训练模型θ，之

后重复

上述步骤。

θ

·从θ'中继

续收集数据。

·根据新的数

据重新训练模型θ。

2.

模型每

次输出概率的权重问题

从前面的公式可以得知

（新引入的Adventure参数会在

后文

介绍）：

模型应该输入的时

间：300 + 600 =

900

TD-Error=1000-900=100

这样，如果继续对下面

的路径进行划分，对于不

同的

路径，可以得到如下

的TD-Error序列：

TD-Error1=80

TD-Error2=50

TD-Error3=20

…

接下来对后续路

径多次进行模型拟合，输

出新的动作

概率时，需要

一种连续的概率修正方

法，即将当前具

体的动作

概率输出与不同的整体

结果误差的修正联系

在

一起，具体实现如下：

ratios = torch.exp(logprobs - old_logprobs.detach())

advantages

= rewards - state_values.detach()

#多个

advantage组成的序列

surr1

= ratios * advantages

代码中的advantages表

示新的输出对原有输出

的改变

和修正，具体公式

如下：

公式中横线标注的

是离散后的advantages，其作用是

对

输出的概率进行修正。

13.2.5 对

于奖励的倒序构成的说

明

关于奖励的构成方法

，实现代码如下：

for

reward, is_done in zip(reversed(memory.reward

s),

reversed(memory.is_dones)):

# 回合结束

if is_done:

discounted_reward = 0

# 更新削减奖励(当前状态

奖励+0.99×上一状态奖励)

discounted_reward = reward +

(self.gamma * discou

nted_reward)

# 首插

入

rewards.insert(0, discounted_reward)

# 标准化奖励

rewards =

torch.tensor(rewards, dtype=torch.float32)

rewards = (rewards -

rewards.mean()) / (rewards.s

td() + 1e-5)

可以看到

，在这里对获取的奖励进

行倒转，之后将奖

励得分

叠加，对于这部分的处理

，读者可以这样理

解：当前

步骤决定未来的步骤，而

模型需要根据当前

步骤

对未来的最终结果进行

修正，如果遵循现在的步

骤，就可以看到未来的结

果如何；如果未来的结果

很

差，模型就需要尽可能

远离造成此结果的步骤

，即对

输出进行修正。

13.3 本章

小结

本章讲解了强化学

习的实战，因为涉及较多

的理论讲

解，因此学习起

来难度较大。但是通过本

章的学习，

读者可以了解

和掌握强化学习的原理

及其实现方法，

并可以独

立完成及成功训练一个

强化学习模型。

本章选用

的是一个较为简单的火

箭回收的强化学习例

子

，其作用是抛砖引玉向读

者介绍基本的算法和训

练

形式。读者可以根据自

身的需要查找相关资料

，对这

部分内容进行强化

。

第14章

创建你自己的小精

灵

——基于MFCC的语音唤醒实战

基于语音唤醒技术的个

人助手是目前非常火爆

的深度

学习应用方向，其

基本原理是基于语音识

别技术与语

音转换，最重

要的是所借助的语音识

别技术。

本章开始进入本

书非常重要的实战内容

——语音识

别。本章将介绍语

音转换中常用的MFCC的来龙

去脉，

包括音频转换的基

本方法，以及使用MFCC进行一

项基

本的语音识别内容

——基于特征词的语音识别

实战。

14.1 语音识别的理论基

础

——MFCC

在语音识别研究领域

，音频特征的选择至关重

要。本

书大部分内容中都

在使用一种非常成功的

音频特征

——梅尔频率倒谱

系数（Mel￾Frequency Cepstrum Coefficient, MFCC）。

MFCC特征的成功很大程

度上得益于心理声学的

研究成

果，它对人的听觉

机理进行了建模。研究发

现，音频

信号从时域信号

转换为频域信号之后，可

以得到各种

频率分量的

能量分布。心理声学的研

究结果表明，人

耳对于低

频信号更加敏感，对于高

频信号比较不敏

感，具体

是什么关系？

心理声学研

究结果表明，在低频部分

是一种线性关

系，但是随

着频率的升高，人耳对于

频率的敏感程度

呈现对

数增长的态势。这意味着

只从各个频率能量的

分

布来设计符合人的听觉

习惯的音频特征是不太

合理

的。

MFCC是基于人耳听觉

特性提出来的，它与Hz频率

呈非

线性对应关系。MFCC利用

这种关系，计算得到Hz频谱

特征，已经广泛地应用于

语音识别领域。MFCC特征提

取

包含两个关键步骤：

（1）转换

到梅尔频率。

（2）进行倒谱分

析。

下面依次进行讲解。

1. 梅

尔频率

梅尔刻度是一种

基于人耳对等距的音高

（Pitch）变化

的感官判断而定的

非线性频率刻度。作为一

种频率域

的音频特征，离

散傅里叶变换是这些特

征计算的基

础。一般选择

快速傅里叶变换

（Fast Fourier Transform,

FFT）算法，其

粗略

的流程如图14-1所示。

图

14-1 快速傅里叶变换

而梅尔

刻度和频率的赫兹关系

如下：

所以，如果在梅尔刻

度上是均匀分度的话，赫

兹之间

的距离就会越来

越大。梅尔刻度的滤波器

组的尺度变

化如图14-2所示

。

图14-2 梅尔刻度的滤波器组

的尺度变化

梅尔刻度的

滤波器组在低频部分的

分辨率高，跟人耳

的听觉

特性是相符的，这也是梅

尔刻度的物理意义所

在

。这一步的含义是：首先对

时域信号进行傅里叶变

换，转换到频域，然后利用

梅尔频率刻度的滤波器

组

对对应频域信号进行

切分，最后每个频率段对

应一个

数值。

2. 倒谱分析

倒

谱的含义是：对时域信号

进行傅里叶变换，然后取

log，再进行反傅里叶变换，如

图14-3所示。倒谱可以

分为复

倒谱、实倒谱和功率倒谱

，这里使用的是功率

倒谱

。倒谱分析可用于将信号

分解，将两个信号的卷

积

转换为两个信号的相加

，从而简化计算。

图14-3 倒谱分

析演示

具体公式这里就

不阐述了，有兴趣的读者

在学习之余

可以自行钻

研相关内容。接下来向读

者演示使用

Python音频处理库

librosa计算MFCC的过程，代码如

下：

# 使

用librosa音频处理库获取音频

的梅尔频谱

wav,

sr = librosa.load(data_path, sr=32000) #s

r为取样频率

#

计算音频信号的MFCC

spec_image = librosa.feature.mfcc(y=wav, sr=sr)

这里需

要注意的是，sr的意思是取

样频率，其作用是

对输入

的音频根据特定的取样

频率生成对应的音频特

征。

读者可以使用.wav后缀的

音频进行尝试。

14.2 语音识别

的数据获取与准备

在开

始进入语音识别之前，需

要获得语音识别基本的

数据集，在这里使用Speech Commands作为

我们进行

语音识别的基

础数据集。

14.2.1 Speech Commands简介与数据说

明

深度学习的第一步（也

是重要的步骤）是数据的

准

备。数据的来源多种多

样，既有不同类型的数据

集，

也有根据项目需求由

项目团队自行准备的数

据集。由

于本章实战的目

的是识别特定词语而进

行语音唤醒，

因此采用一

整套专门的语音识别数

据集

Speech Commands。

首先是对于数据集

的获取，读者可以使用如

下的

PyTorch代码直接下载数据

集：

from torchaudio import datasets

datasets.SPEECHCOMMANDS(

root="../dataset/", # 保存

数据的路径

url='speech_commands_v0.02', #

下载

数据版本

URL

folder_in_archive='SpeechCommands',

download=True) #

这个记得选True

在

这里root是存储的路径，url是选

择下载的版本，读

者可以

直接运行这段下载代码

对数据进行下载。

打开数

据集可以看到，根据不同

的文件夹名称，其中

内部

被分成了35个类别，每个类

别以名称命名，包含

符合

该文件名的语音发音，内

容如图14-4所示。

图14-4

数据集

可

以看到，根据文件名对每

个发音进行归类，其中包

含：

·训练集：包含51088个WAV音频文

件。

·验证集：包含6798个WAV音频文

件。

·测试集：包含6835个WAV音频文

件。

读者可以使用计算机

自带的音频播放程序试

听部分音

频。

下面进入数

据的处理部分，在这里使

用上文的librosa

库直接提取音

频信号，代码如下：

import librosa

#

使用librosa音

频处理库获得音频的梅

尔频谱

wav, sr = librosa.load("../dataset/SpeechCommands/

speech_commands_v0.02/

bird/0a7c2a8d_nohash_1.wav", sr=32000)

#sr为取样频率

# 计算

音频信号的MFCC

spec_image

= librosa.feature.mfcc(y=wav, sr=sr)

print(wav.shape)

print(spec_image.shape)

在这里读取

了音频数据集bird中的0a7c2a8d_nohash_1

命名

的音频部分，之后将其转

换为梅尔频率。前面已

经

介绍过梅尔频率是基于

人耳听觉特性提出来的

，它

与Hz频率呈非线性对应

关系，主要用于语音数据

特征

提取和降低运算维

度。打印结果如下：

(32000,)

(20,

63)

这里可

以看到，首先32000是设置的采

样率，之后提取

出的(20, 63)是采

样频谱。为了简易起见，这

里采用

5条具有较多数据

集的音频数据制作数据

集，代码如

下：

import librosa

import os

import torch

import numpy as np

labels =

["bed","bird","dog","cat","yes"]

image_list = []

label_list =

[]

for label in labels:

path

= f"../dataset/SpeechCommands/speech_commands_v0

.02/{label}"

for file_name in

os.listdir(path):

file_path = path + "/"

+ file_name

wav, sr = librosa.load(path,

sr=32000)

spec_image = librosa.feature.mfcc(y=wav, sr=

sr)

#注意这里补

0

pec_image = np.pad(spec_image, ((0, 0),

(0

, 63 - spec_image.shape[1])),

'constant')

image_list.append(spec_image)

label_list.append(labels.index(label))

#image_list =np.array(image_list) #

(12281,

20, 63)

#label_list = np.array(label_list) #(12281,)

#下面是自制的dataload方面的内

容

class MyDataset(torch.utils.data.Dataset):

def __init__(self, input_data,label_list):

self.input_data = input_data

self.label_list = label_list

def __len__(self):

return len(self.input_data)

def __getitem__(self,

idx):

# grab a chunk of

(block_size + 1) char

acters from

the data

input_token = self.input_data[idx]

output_token

= self.label_list[idx]

input_token = torch.tensor(input_token)

output_token

= torch.tensor(output_token)

return input_token, output_token

14.2.2

语音识别编码器模块

与代码实现

下面开始介

绍深度学习模型，通过对

数据集的分析可

以看到

，生成的训练集数据维度

如下：

(12281, 20, 63)

(12281,)

即这里的音频内容

被转换为[20, 63]大小的维度，那

么可以简单地将其当成

一个序列内容进行处理

，而对

于序列内容的处理

，前面已经有了非常多的

讲解，其

中最为前沿和优

选的是基于注意力模型

的编码器。下

面采用Attention架构

建立对应的编码器模块

。

需要注意的是，在本例中

获取的是完整的音频特

征图

谱，相对于一般的Attention模

型，这里的注意力模型

不

需要掩码操作。完整的模

型代码如下：

import torch

import

math

import einops.layers.torch as elt

class

FeedForWard(torch.nn.Module):

def __init__(self,embedding_dim = 312,scale =

4)

:

super().__init__()

self.linear1 = torch.nn.Linear(embedding_di

m,embedding_dim*scale)

self.relu_1 = torch.nn.ReLU()

self.linear2 =

torch.nn.Linear(embedding_di

m*scale,embedding_dim)

self.relu_2 = torch.nn.ReLU()

self.layer_norm

= torch.nn.LayerNorm(normal

ized_shape=embedding_dim)

def forward(self,tensor):

embedding

= self.linear1(tensor)

embedding = self.relu_1(embedding)

embedding

= self.linear2(embedding)

embedding = self.relu_2(embedding)

embedding

= self.layer_norm(embedding)

return embedding

class Attention(torch.nn.Module):

def __init__(self,embedding_dim = 312,hidden_dim

= 312,n_head

= 6):

super().__init__()

self.n_head = n_head

self.query_layer = torch.nn.Linear(embeddin

g_dim, hidden_dim)

self.key_layer

= torch.nn.Linear(embedding_

dim, hidden_dim)

self.value_layer =

torch.nn.Linear(embeddin

g_dim, hidden_dim)

def forward(self,embedding):

input_embedding

= embedding

query = self.query_layer(input_embedding)

key

= self.key_layer(input_embedding)

value = self.value_layer(input_embedding)

query_splited

= self.splite_tensor(query,se

lf.n_head)

key_splited = self.splite_tensor(key,self.n

_head)

value_splited = self.splite_tensor(value,se

lf.n_head)

key_splited

= elt.Rearrange("b h l d -

> b h d l")(key_splited)

#

计算query与key之间

的权重系数

attention_prob = torch.matmul(query_splited

, key_splited)

# 使用softmax对权重

系数进行归一化计算

attention_prob = torch.softmax(attention_pr

ob,

dim=-1)

# 计

算权重系数与value的值，从而

获取注意力值

attention_score = torch.matmul(attention_pr

ob, value_splited)

attention_score = elt.Rearrange("b h

l d

-> b l (h

d)")(attention_score)

return (attention_score)

def splite_tensor(self,tensor,h_head):

embedding

= elt.Rearrange("b l (h d) -

> b l h d",h =

h_head)(tensor)

embedding = elt.Rearrange("b l h

d -

> b h l

d", h=h_head)(embedding)

return embedding

class PositionalEncoding(torch.nn.Module):

def __init__(self, d_model = 312, dropout

= 0.

05, max_len=80):

"""

:param

d_model: pe编码维度

，一般与

Word Embedding相同，方便相加

:param dropout:

dorp out

:param max_len: 语

料库中最长句子的长度

，即

Word

Embedding中的L

"""

super(PositionalEncoding, self).__init__()

# 定义drop

out

self.dropout = torch.nn.Dropout(p=dropout)

# 计算pe编码

pe = torch.zeros(max_len, d_model) # 建

立空表，每行代表一个词

的位置，每列代

表一个编

码位

position = torch.arange(0, max_len).unsque

eze(1)

# arange表示词的位置，

以便

进行公式计算，size=(max_len,1)

div_term = torch.exp(torch.arange(0,

d_mod

el, 2) * # 计算公式

中10000**

（2i/d_model)-(math.log(10000.0) / d_model))

pe[:, 0::2] =

torch.sin(position * div_t

erm) # 计算偶数维度的pe值

pe[:, 1::2] = torch.cos(position * div_t

erm) # 计

算奇数维度的pe值

pe = pe.unsqueeze(0)

# size=

(1, L, d_model)，为了后

续与word_embedding

相加，意为batch维度下的

操作相同

self.register_buffer('pe', pe) # pe值是

不参加训

练的

def

forward(self, x):

# 输入的最终编

码 =

word_embedding + positional_embedding

x = x

+ self.pe[:, :x.size(1)].clone().d

etach().requires_grad_(False)

return self.dropout(x)

# size = [batch,

L, d_model]

#编

码器类

class Encoder(torch.nn.Module):

def __init__(self,max_length =

20,embedding_size

= 312,n_head = 6,scale =

4,n_layer = 3):

super().__init__()

self.n_layer =

n_layer

self.input_layer = torch.nn.Linear(63,312)

self.position_embedding =

PositionalEncodin

g(max_len=max_length)

self.attention = Attention(embedding_size,e

mbedding_size,n_head)

self.feedward = FeedForWard()

def forward(self,token_inputs):

embedding

= token_inputs

embedding = self.input_layer(embedding)

embedding

= self.position_embedding(embeddi

ng)

for _ in

range(self.n_layer):

embedding = self.attention(embeddin

g)

embedding

= torch.nn.Dropout(0.1)

(embedding)

embedding = self.feedward(embedding

)

return embedding

if __name__ ==

'__main__':

image = torch.randn(size=(3,20,63))

Encoder()(image)

14.3

实战：PyTorch 2.0语音识别

14.3.1 基

于PyTorch 2.0的语音识别模型

下面

建立基于PyTorch

2.0的语音识别模

型，经过前

面的分析，可以

看到不同的语音经过特

征抽取后被转

换为一维

的序列结构，而对于序列

结构的分析，可以

注意力

为核心建立对应的识别

模型。

完整代码如下：

import

torch

import attention_moudle

import einops.layers.torch as

elt

class VideoRec(torch.nn.Module):

def __init__(self):

super().__init__()

self.encoder = attention_moudle.Encoder()

self.trans_layer = torch.nn.Sequential(

elt.Rearrange("b l d -

> b

d l"),

torch.nn.AdaptiveAvgPool1d((1)),

elt.Rearrange("b d 1

-> b d")

)

self.last_linear =

torch.nn.Linear(312,5)

def forward(self,image):

img = self.encoder(image)

img = self.trans_layer(img)

img = torch.nn.Dropout(0.1)(img)

logits = self.last_linear(img)

return logits

if

__name__ == '__main__':

image = torch.randn(size=(3,20,63))

VideoRec()(image)

14.3.2 基于

PyTorch 2.0的语音识别实现

前面介

绍了数据集与模型的基

本组成，下面进入本章

的

模型实现部分，即实现语

音识别功能。相信读者经

过以上分析，能够很顺利

地完成该部分内容。

import torch

import model

from torch.utils.data

import DataLoader

from tqdm import tqdm

device = "cuda"

video_model = model.VideoRec()

video_model.to(device)

optimizer = torch.optim.Adam(video_model.parameters

(), lr=2e-5)

criterion = torch.nn.CrossEntropyLoss()

batch_size = 64

import get_data

train_dataset = get_data.MyDataset(get_data.image_l

ist,get_data.label_list)

loader = DataLoader(train_dataset,batch_size=batch_

size,pin_memory =

True,shuffle=True,num_workers=0)

for epoch in (range(1024)):

pbar =

tqdm(loader,total=len(loader))

for token_inp,token_tgt in pbar:

token_inp

= token_inp.to(device)

token_tgt = token_tgt.to(device)

logits

= video_model(token_inp)

loss = criterion(logits.view(-1,logits.size(-

1)),token_tgt.view(-1))

optimizer.zero_grad()

loss.backward()

optimizer.step()

train_correct = (torch.argmax(logits,

dim=-1

) ==

(token_tgt)).type(torch.float).sum().item() / len(token_inp

)

pbar.set_description(f"epoch:

{epoch + 1}, train_loss:{loss.item():.5f},

train_correct:{train_correct:.5f}")

在本

例中设置batch_size为64，同时使用tqdm模

块对

数据的结果进行显

示，这段代码的输出结果

如图14-5

所示。

图14-5 输出结果

可

以看到随着训练的进行

，模型的准确率随之加深

，

能够取得0.9左右的准确率

。请读者自行验证此部分

内

容。

14.4 本章小结

相信读者

学完本章内容，已经对语

音识别基础有了初

步的

了解，创建自己的小助手

吧。本章基于

PyTorch 2.0完成语音识

别的实战部分，重要的是

向

读者介绍了将一段语

音转换成梅尔矩阵之后

对其进行

处理的方法，这

是语音识别的基础内容

。语音识别是

深度学习的

一个研究方向，也是热点

之一。希望本章

可以帮助

读者熟悉语音识别技术

，从而为后续的学习

和研

究打下良好的基础。

第15章

基于PyTorch的人脸识别实战

随

着电子商务等应用的发

展，人脸识别成为最有潜

力

的生物身份验证手段

，这种应用背景要求自动

人脸识

别系统能够对一

般图像具有一定的识别

能力，如图15-

1所示。

图15-1 人脸识

别

受限于技术手段，人脸

识别一直并未有大规模

的应

用，直到深度学习的

出现才解决了人脸识别

应用的落

地。可以看到，今

天的人脸检测应用背景

已经远远超

出了人脸识

别系统的范畴，在基于内

容的检索、数字

视频处理

、视频检测等方面有着重

要的应用价值。

本章将主

要介绍人脸识别方面的

实战内容，从创建人

脸数

据库开始，到基于相似度

计算，完成人脸识别的

深

度学习模型，完完整整地

实现人脸识别应用并为

读

者做详细的说明。

15.1 人脸

识别数据集的建立

在使

用深度学习进行人脸识

别之前，首先需要创建一

个可用的人脸识别数据

集，基于人脸涉及的一些

隐私

性问题，本章将使用

公开的数据集并进行相

应的调

整。本节将介绍基

于传统的Python库建立所需要

的数

据集的方法，并实现

使用Dlib进行人脸检测的应

用。

15.1.1 LFW数据集简介

LFW（Labeled Faces in the

Wild）数据集是

目

前人脸识别的常用测

试集，其中提供的人脸图

片均来

源于生活中的自

然场景，因此识别难度比

较大，尤其

由于多姿态、光

照、表情、年龄、遮挡等因素

影响，

导致即使同一人的

照片差别也很大，并且有

些照片中

可能不止出现

一张人脸，对这些多人脸

图像仅选择中

心坐标的

人脸作为目标，其他区域

的视为背景干扰。

LFW数据集

共有13233幅人脸图像，每幅图

像均给出对

应的人名，共

有5749人，且绝大部分人仅有

一幅图

片。每幅图片的尺

寸为250×250，绝大部分为彩色图

像，但也存在少许黑白人

脸图片，如图15-2所示。

图15-2 LFW数据

集

LFW数据集主要测试人脸

识别的准确率，从该数据

集中

随机选择6000对人脸组

成了人脸辨识图片对，其

中

3000对属于同一个人的两

幅人脸照片，3000对属于不

同

的人，每人一幅人脸照片

。测试过程LFW数据集给出

一

对照片，询问测试中的两

幅照片是不是同一个人

，

系统给出“是”或“否”的答案

。通过6000对人脸测

试结果的

系统答案与真实答案的

比值，可以得到人脸

识别

的准确率。

15.1.2 Dlib库简介

在介绍

完LFW数据集后，本小节介绍

Dlib这个常用的

Python库。Dlib是一个机

器学习的开源库，包含机

器

学习的很多算法，使用

起来很方便，直接包含其

头文

件即可，并且不依赖

于其他库（自带图像编解

码库源

码）。Dlib可以帮助用户

编写很多复杂的机器学

习方

面的软件来解决实

际问题。目前Dlib已经被广泛

地应

用于工业和学术领

域，包括机器人、嵌入式设

备、移

动电话和大型高性

能计算环境。

Dlib是一个使用

现代C++技术编写的跨平台

的通用库，

遵守Boost Software Licence，主要特点

如下。

·完善的文档：每个类

、每个函数都有详细的文

档，

并且提供了大量的示

例代码。

·可移植代码：代码

符合ISO C++标准，不需要第三

方

库支持，支持Win32、Linux、Mac OS X、

Solaris、HPUX、BSDs和POSIX系统。

·线程

支持：提供简单的可移植

的线程API。

·网络支持：提供简

单的可移植的Socket API和一个

简

单的HTTP服务器。

·图形用户界

面：提供线程安全的GUI API。

·数值

算法：矩阵、大整数、随机数

运算等。

除了人脸检测之

外，Dlib库还包含其他多种工

具，例

如，用于检测数据压

缩和完整性算法，比如CRC32、

MD5以

及不同形式的PPM算法；用于

测试的线程安全的

日志

类和模块化的单元测试

框架，以及各种测试

Assert支持

的工具；还有一般工具类

，如XML解析、内

存管理、类型安

全的Big/Little Endian转换、序列

化支持和

容器类等。

15.1.3 OpenCV简介

本小节介

绍一个重要的Python常用库——OpenCV。对

于Python用户来说，OpenCV可能是最常

用的图像处理

工具。OpenCV是一

个基于BSD许可（开源）发行的

跨平

台计算机视觉和机

器学习软件库。OpenCV用C++语言编

写，轻量级且高效——由一系

列C函数和少量C++类构

成，同

时提供了C++、Java、Python、Ruby、MATLAB等

语言的接口

，实现了图像处理和计算

机视觉方面的很

多通用

算法，支持Windows、Linux、Android和macOS

系统。OpenCV主要倾

向于实时视觉应用，并在

可用时

利用MMX和SSE指令，如今

也提供对于C#、Ch、Ruby、

GO的支持。

由于

OpenCV不是本书的重点，因此对

于OpenCV的介绍

就到这里，本书

涉及使用OpenCV对图像进行处

理的部

分函数会有提示

性说明，详细的技术细节

有兴趣的读

者可以查找

资料自行学习。

15.1.4 使用Dlib检测

人脸位置

下面使用Dlib进行

图像中的人脸检测，在前

面下载的

LFW数据集中随机

选择一幅图片，如图15-3所示

。

图15-3 LFW数据集中的一幅人脸

图

图15-3中是一个成年男性

的图片，对于计算机视觉

来

说，无论是背景还是衣

饰，实际上都不是需要关

心的

目标，最重要的是图

片中人脸的表示，而所在

的背景

和衣饰可能会成

为一种干扰的噪声。

1. 使用

OpenCV读取图片

使用OpenCV读取图片

，在这里使用LFW文件夹的第

一个

文件夹Aaron_Eckhart中的第一幅

图片，代码如下：

import cv2

image =

cv2.imread("./dataset/lfw￾deepfunneled/Aaron_Eckhart/

Aaron_Eckhart_0001.jpg") #使用OpenCV读取

图片

cv2.imshow("image",image) #展示图片结

果

cv2.waitKey(0) #暂停

进程，按空格恢复

上面的

代码展示了使用OpenCV读取图

片并展示的过

程，imread函数根

据图片地址读取图片内

容到内存

中，imshow函数展示图

片结果，而waitKey通过设置参

数

决定进程暂停的时间。

2. 加

载Dlib的检测器

加载Dlib的检测

器，Dlib的检测器的作用是对

图像中

的人脸目标进行

检测，代码如下：

import cv2

import dlib

image = cv2.imread("./dataset/lfw￾deepfunneled/Aaron_Eckhart/

Aaron_Eckhart_0001.jpg")

detector = dlib.get_frontal_face_detector() #Dlib

创建的检

测器

boundarys

= detector(image, 2) #对人脸图片进行检

测，找到人脸的位置框

print(list(boundarys)) #打

印位置框内

容

其中的dlib.get_frontal_face_detector函

数创建了

用于对人脸检

测的检测器，之后使用detector对

人脸

的位置进行检测，并

将找到的位置以列表的

形式存

储，若未找到，则返

回一个空列表。打印结果

如下：

[rectangle(78,

89, 171, 182)]

可以看到，这里的列

表中是一个rectangle格式的数据

元组，其中框体的位置表

示如下。

·框体上方：rectangle[1]，使用rectangle.top()函

数获取。

·框体下方：rectangle[3]，使用

rectangle. bottom()函

数获取。

·框体左方：rectangle[0]，使用

rectangle. left()函

数获取。

·框体右方：rectangle[2]，使用

rectangle. right()函

数获取。

获取并打印框体

位置的代码如下：

import cv2

import

dlib

import numpy as np

image

= cv2.imread("./dataset/lfw￾deepfunneled/Aaron_Eckhart/

Aaron_Eckhart_0001.jpg")

detector = dlib.get_frontal_face_detector()

#Dlib

创建的

切割器

boundarys = detector(image, 2)

#找到人脸框的坐

标，若没有，则返回空集

print(list(boundarys)) #打

印结果

draw = image.copy()

rectangles = list(boundarys)

for rectangle in

rectangles:

top = np.int(rectangle.top()) # idx

= 1

bottom = np.int(rectangle.bottom()) #idx

= 3

left = np.int(rectangle.left()) #idx

= 0

right = np.int(rectangle.right()) #idx

= 2

print([left,top,right,bottom])

打印结果如下：

[rectangle(78, 89,

171, 182)]

[78, 89, 171, 182]

3. 使

用Dlib进行人脸检测

输入检

测到的人脸框图，对于给

定的人脸框图的位置

坐

标来说，OpenCV提供了专门用于

画框图的函数

rectangle()，这样将OpenCV与

Dlib结合在一起，可以

很好地

达到人脸检测的需求，代

码如下：

import cv2

import dlib

import numpy

as np

image = cv2.imread("./dataset/lfw￾deepfunneled/Aaron_Eckhart/

Aaron_Eckhart_0001.jpg")

detector = dlib.get_frontal_face_detector() #切割

器

boundarys

= detector(image, 2)

rectangles = list(boundarys)

draw = image.copy()

for rectangle in

rectangles:

top = np.int(rectangle.top())

# idx

= 1

bottom = np.int(rectangle.bottom())

#idx

= 3

left = np.int(rectangle.left())

#idx

= 0

right = np.int(rectangle.right())

#idx

= 2

W = -

int(left)

+ int(right)

#获取人脸

框体的宽度

H = -

int(top) + int(bottom)

#获取人脸框

体的高度

paddingH =

0.01 * W

paddingW = 0.02

* H

#将人脸的图片

单独“切割出来”

crop_img = image[int(top

+ paddingH):int(bottom -

paddingH), int(left -

paddingW):int(right + paddingW)]

#进行人脸

框体描绘

cv2.rectangle(draw, (int(left),

int(top)), (int(righ

t), int(bottom)), (255,

0,

0), 1)

cv2.imshow("test", draw)

c =

cv2.waitKey(0)

这里使用了图

像截取，crop_img的作用是将图片

矩阵

按大小进行截取，而

cv2.rectangle使用OpenCV在图片

上画出框体

线。最终结果如图15-4所示。

图

15-4 画出人脸框的图片

从图

15-4可以清楚地看到，使用Dlib和

OpenCV可以很

好地解决人脸定

位问题，而对于切割图片

的显示如图

15-5所示。

图15-5 对于

切割图片的显示

可以看

到，图片的右侧边缘有一

个明显的竖线，这是

因为

图片的尺寸过小，从而影

响了OpenCV的画图，此

时将切割

图片的大小重新进行缩

放即可，代码如下：

import cv2

import dlib

import numpy as np

image =

cv2.imread("./dataset/lfw￾deepfunneled/Aaron_Eckhart/ Aaron_Eckhart_

0001.jpg")

detector = dlib.get_frontal_face_detector()

#切割

器

boundarys = detector(image, 2)

print(list(boundarys))

draw = image.copy()

rectangles = list(boundarys)

for rectangle in rectangles:

top =

np.int(rectangle.top()) # idx = 1

bottom

= np.int(rectangle.bottom()) #idx = 3

left

= np.int(rectangle.left()) #idx = 0

right

= np.int(rectangle.right()) #idx = 2

W

= -int(left) + int(right)

H =

-int(top) + int(bottom)

paddingH = 0.01

* W

paddingW = 0.02 *

H

crop_img = image[int(top + paddingH):int(bottom

-

paddingH), int(left -

paddingW):int(right +

paddingW)]

#进行切割放大

crop_img = cv2.resize(crop_img,dsize=(128,128))

cv2.imshow("test",

crop_img)

c = cv2.waitKey(0)

结果请读

者自行验证。

15.1.5

使用Dlib和OpenCV建立

自己的人脸检测数据

集

由于LFW数据集在创建时并

没有专门整理人脸框图

的位

置数据，因此借助Dlib和

OpenCV，读者可以建立自己

的人

脸检测数据集。

1. 找到LFW数据

集中的所有图片位置

下

面找到LFW数据集中所有图

片的位置，这里使用

pathlib库对

数据库地址进行查找，代

码如下：

path = "./dataset/lfw-deepfunneled/"

path

= Path(path)

file_dirs = [x for

x in path.iterdir() if x.is_dir()]

for

file_dir in tqdm(file_dirs):

image_path_list = list(Path(file_dir).glob('*.jpg')

)

这里file_dirs查找当前路

径中所有的文件夹，在一

个

for循环后，使用glob函数将符

合对应后缀名的所有文

件都找到。最终生成一个

image_path_list列表，用于

存储所有找到

的对应后缀名的文件。这

里顺便讲一下

tqdm的作用，tqdm是

一个可视化进程运行函

数，将路

径的进程予以可

视化显示。

2. 在图片中查找

人脸框体

查找的方法是

结合Dlib进行人脸框的查找

并存储结

果，完整的代码

如下：

from pathlib

import Path

import dlib

import cv2

import numpy as np

from tqdm

import tqdm

detector = dlib.get_frontal_face_detector() #人脸检测器

path = "./dataset/lfw-deepfunneled/"

path = Path(path)

file_dirs = [x for x in

path.iterdir() if x.is_dir()]

rec_box_list = []

counter = 0

for file_dir in

tqdm(file_dirs):

image_path_list = list(Path(file_dir).glob('*.jpg')

)

for

image_path in image_path_list:

image_path = "./"

+ str(image_path)

image = (cv2.imread(image_path))

draw

= image.copy()

boundarys = detector(image, 2)

rectangle = list(boundarys)

#为了简

便起见，限定每幅图片中

只有一个人的图

if len(rectangle)

== 1:

rectangle = rectangle[0]

top

= np.int(rectangle.top())

# idx = 1

bottom = np.int(rectangle.bottom())

# idx =

3

left = np.int(rectangle.left())

# idx

= 0

right = np.int(rectangle.right())

#

idx = 2

if rectangle is

not None:

W = -int(left) +

int(right)

H = -int(top) + int(bottom)

paddingH = 0.01 * W

paddingW

= 0.02 * H

crop_img =

image[int(top +

paddingH):int(bottom - paddingH),

int(left

- paddingW):int(right + paddingW)]

cv2.rectangle(draw, (int(left)

, int(top)), (int(right),

int(bottom)), (255, 0,

0), 1)

rec_box = [top,bottom,left,right]

rec_box_list.append(rec_box)

new_path = "./dataset/lfw/" + str(c

ounter)

+ ".jpg"

cv2.imwrite(new_path, image)

counter +=

1

np.save("./dataset/lfw/rec_box_list.npy",rec_box_list

)

这段代

码的作用是读取LFW数据集

中不同文件夹中的图

片

，获取其面部坐标框之后

，存储在特定的列表中。

为

了简单起见，这里限定了

每幅图中只有一张人脸

进

行检测。

3. 对结果进行验

证

这里随机获取一个图

片ID，使用Dlib即时获取对应的

人脸框，对打印存储的人

脸列表内容进行验证，代

码

如下：

import

dlib

import cv2

import numpy as

np

detector = dlib.get_frontal_face_detector() #切割器

img_path

= "./dataset/lfw/10240.jpg"

image = (cv2.imread(img_path))

boundarys

= detector(image, 2)

print(list(boundarys))

rec_box_list =

np.load("./dataset/lfw/rec_box_list.npy")

print(rec_box_list[10240])

打印结果

请读者自行验证。

15.1.6 基于人

脸定位制作适配深度学

习的人脸识别

数据集

对

于普通用户而言，为了使

用深度学习模型而直接

将

LFW数据集的全部图片数

据载入内存或者显存中

，这是

一件较为困难的事

。因为LFW数据集的数据量比

较大，

一次性载入全部完

整的图片数据会造成很

多问题。

在上一小节中，我

们讲解了使用Dlib制作通用

人脸识

别模型，目的是在

含有人脸的基础数据集

中找到（检

测）人脸的位置

。接下来的工作需要基于

此完成适配

本章人脸识

别模型的数据集。

1. 第一步

：使用Dlib定位人脸位置并制

作新的人脸

图片

首先使

用Dlib将人脸位置固定，并制

作新的人脸图

片，目的是

加强模型的训练，从而使

得模型在识别时

能够更

加注重人脸细节的分辨

。

在此过程中，我们直接将

裁剪后的新人脸图片放

到当

前目录下的dataset/lfw-deepfunneled子目录

中，注

意与上一节的人脸

图片数据集保存的目录

不一样。代

码如下：

import numpy as np

import dlib

import matplotlib.image as mpimg

import cv2

import imageio

from pathlib import Path

import os

from tqdm import tqdm

shape = 144

def clip_image(image, boundary):

top = np.clip(boundary.top(), 0, np.Inf).astype(n

p.int16)

bottom = np.clip(boundary.bottom(), 0, np.Inf).as

type(np.int16)

left = np.clip(boundary.left(), 0, np.Inf).astype

(np.int16)

right = np.clip(boundary.right(), 0, np.Inf).asty

pe(np.int16)

image = cv2.resize(image[top:bottom, left:right],

(128,128))

return

image

def fun(file_dirs):

for file_dir in

tqdm(file_dirs):

image_path_list = list(file_dir.glob('*.jpg

'))

for

image_path in image_path_list:

image = np.array(mpimg.imread(image

_path))

boundarys = detector(image, 2)

if

len(boundarys) == 1:

image_new = clip_image(imag

e, boundarys[0])

os.remove(image_path)

image_path_new = image_path

#这里可

以对保存的地点调整路

径

imageio.imsave(image_path_new

, image_new)

else:

os.remove(image_path)

detector = dlib.get_frontal_face_detector() #切割器

path="./dataset/lfw-deepfunneled"

path

= Path(path)

file_dirs = [x for

x in path.iterdir() if x.is_dir()]

print(len(file_dirs))

fun(file_dirs)

2. 第二步：创建新

图片结构的位置地址

在

本小节一开始的部分我

们已经讲了，对于部分读

者

来说，一次性将数据集

中所有的人脸图片读取

到内存

或者显存中是较

为困难的，因此最好的办

法是对图片

的地址进行

保存，通过地址名称来读

取图片，并且采

用地址名

称上的人名来判断是否

归属于同一个人，形

式如

图15-6所示。

图15-6 相似人脸的判

定

而对于不同的人脸图

片，同样可以通过地址上

的人名

对其进行判断，形

式如图15-7所示。

图15-7 不相似人

脸的判定

另外，需要注意

的是，为了加速我们的训

练，这里人

为设置了一个

规则：只有每个人的图片

数量大于10，

才将其加入数

据集。代码如下：

cutoff = 10

for folder in

folders:

files = list_files(folder)

if len(files)

>= cutoff:

path_file_collect += (files)

完整代码

如下（注意lfw-deepfunneled文件的存放位

置）：

import numpy as np

import dlib

import matplotlib.image as mpimg

import cv2

import imageio

from pathlib import Path

import os

from tqdm import tqdm

shape = 144

def clip_image(image, boundary):

top = np.clip(boundary.top(), 0, np.Inf).astype(n

p.int16)

bottom = np.clip(boundary.bottom(), 0, np.Inf).as

type(np.int16)

left = np.clip(boundary.left(), 0, np.Inf).astype

(np.int16)

right = np.clip(boundary.right(), 0, np.Inf).asty

pe(np.int16)

image = cv2.resize(image[top:bottom, left:right],

(128,128))

return

image

def fun(file_dirs):

for file_dir in

tqdm(file_dirs):

image_path_list = list(file_dir.glob('*.jpg

'))

for

image_path in image_path_list:

image = np.array(mpimg.imread(image

_path))

boundarys = detector(image, 2)

if

len(boundarys) == 1:

image_new = clip_image(imag

e, boundarys[0])

os.remove(image_path)

image_path_new = image_path

#这里可以对保存的地

点调整路径

imageio.imsave(image_path_new

, image_new)

else:

os.remove(image_path)

import os

# 列出所有目

录下文件夹的函数

def list_folders(path):

"""

列出

指定路径下的所有文件

夹名

"""

folders = []

for root, dirs, files in os.walk(path):

for dir in dirs:

folders.append(os.path.join(root, di

r))

return folders

def list_files(path):

files

= []

for item in os.listdir(path):

file = os.path.join(path, item)

if os.path.isfile(file):

files.append(file)

return files

if __name__ ==

'__main__':

detector = dlib.get_frontal_face_detector() #切割

器

path="./dataset/lfw-deepfunneled"

path = Path(path)

file_dirs =

[x for x in path.iterdir() if

x.i

s_dir()]

print(len(file_dirs))

fun(file_dirs)

folders =

list_folders(path)

path_file_collect = []

cutoff =

10

for folder in folders:

files

= list_files(folder)

if len(files) >= cutoff:

path_file_collect += (files)

path_file = "./dataset/lfw-path_file.txt"

file2 = open(path_file, 'w+')

for line

in path_file_collect:

file2.write(line)

file2.write("\n")

file2.close()

读者可以等

程序结束之后查询定义

的地址存放目录，

可以看

到其中的内容如图15-8所示

。

图15-8 不相似人脸的判定

读

者可以自行尝试。

15.2 实战：基

于深度学习的人脸识别

模型

使用深度学习完成

人脸识别，一个最简单的

思路就是

利用卷积神经

网络抽取人脸图像的特

征，之后使用分

类器对人

脸进行二分类，这样就完

成了前面所定义的

任务

。

15.2.1 人脸识别的基本模型Siamese

Model

首

先介绍一下人脸识别模

型Siamese Model（孪生模

型）。在介绍这个

模型之前，先对人脸识别

的输入进

行一下分类。在

本书前面的模型设计中

，输入端无论

输入的是一

组数据还是多组数据，都

是被传送到模型

中进行

计算，无非就是前后的区

别。

而对于人脸识别模型

来说，一般情况下会输入

两项并

行的内容，一项是

需要验证的数据，另一项

是数据库

中的人脸数据

。

这样并行处理两个数据

集模型称为Siamese Model。

Siamese在英语中指

“孪生”“连体”

，这是一个外来

词，来源于19世纪泰国出生

的一对连体婴儿（他们长

大后的照片如图15-9所示），具

体的故事这里就不介

绍

了，读者可以自己去了解

。

图15-9 连体婴儿长大后的样

子

简单来说，Siamese

Model就是“连体的

神经网络模

型”

，神经网络

的“连体”是通过共享权重

来实现

的，如图15-10所示。

图15-10 Siamese

Model

这

里详细说明一下，所谓共

享权重，可以认为是同一

个网络，实际上就是同一

个网络。因为其网络的架

构

和模块完全相同，而且

权值是同一份权值，也就

是对

同一个深度学习网

络进行了重复使用。这里

顺便讲一

下，如果网络架

构和模块完全相同，但是

权值却不

同，那么这种网

络叫作伪孪生（Pseudo￾Siamese

Model）神经网络

。

孪生网络的作用是衡量

两个输入的相似程度。孪

生神

经网络有两个输入

（Input 1和Input 2），将两个

输入分别输送

到两个神经网络（Network

1和

Network 2）中，这

两个神经网络分别将输

入映射到

新的空间，形成

输入在新的空间中的表

示。

读者可能会问，目前一

直讲的是Siamese的整体架构，

其

中的Model部分到底是什么？实

际上答案很简单，对

于Siamese Model来

说，其中的Model的作用是进行

特

征提取，只需要保证在

这个架构中，Model所使用的是

同一个网络即可，而具体

的网络是什么，最简单的

如

卷积神经网络模型VGG16，或

者最新的卷积神经网络

模

型SENET都是可以的。

Siamese

Model的架构

如图15-11所示。

图15-11 Siamese Model的架构

最后

的损失函数就是前面介

绍过的普通交叉熵函数

，

使用L2正则对其进行权重

修正，从而使得网络能够

学

习更为平滑的权重，进

而提高泛化能力。

其中是

两个输入样本经过孪生

神经网络

（Siamese Network）输出的计算合

并值（这里使用

了点乘，实

际上使用差值也可以），而

t则是标签值。

15.2.2

基于PyTorch 2.0的Siamese Model的实

现

下面介绍Siamese Model的实现部分

。在15.2.1节已

经介绍过了，Siamese

Model实际

上是并行使用一个

“主干

”神经网络同时计算两个

输入端内容的模型。

在这

里采用建立一个卷积神

经网络的方式构建相应

的

模型，代码如下：

class SiameseNetwork(nn.Module):

def __init__(self):

super(SiameseNetwork,self).__init__()

self.cnn1=nn.Sequential(

nn.Conv2d(1,4,kernel_size=5),

nn.BatchNorm2d(4),

nn.ReLU(inplace=True),

nn.Conv2d(4, 8, kernel_size=5),

nn.BatchNorm2d(8),

nn.ReLU(inplace=True),

nn.Conv2d(8, 8, kernel_size=3),

nn.BatchNorm2d(8),

nn.ReLU(inplace=True),

)

self.fc1=nn.Sequential(

nn.Linear(8 * 90 * 90,500),

nn.ReLU(inplace=True),

nn.Linear(500,500),

nn.ReLU(inplace=True),

nn.Linear(500,40)

)

def

forward(self, input1,input2):

output1=self.forward_once(input1)

output2=self.forward_once(input2)

return output1,output2

15.2.3 人脸识

别的Contrastive Loss详解与实现

一般在

孪生神经网络（Siamese Network）中，采用

的

损失函数是Contrastive

Loss（对比损失），这

是

一种常用于训练深度

神经网络中人脸识别模

型的损失

函数。其本质是

通过比较两幅图像之间

的相似度来使

得同一张

人脸的特征向量距离更

小，不同人脸的特征

向量

距离更大，从而实现人脸

识别的效果。

Contrastive

Loss损失函数可

以有效地处理孪生神经

网络中的配对数据的关

系，其表达式如下：

其中d代

表两个样本特征的欧氏

距离，y为两个样本是

否匹

配的标签，y=1代表两个样本

相似或者匹配，y=0

则代表不

匹配，margin为设定的阈值。

当两

幅图像是同一张人脸时

，我们希望它们的距离越

小越好，因此损失函数的

第一项为0，只考虑第二项

。

当两幅图像是不同的人

脸时，我们希望它们的距

离越

大越好，因此损失函

数的第二项为0，只考虑第

一项。

通过这种方式，我们

可以让相同人脸的特征

向量更加

相似，不同人脸

的特征向量更加不同。最

终，模型将

会学到一组特

征向量，每个特征向量都

代表该人脸的

独特属性

，可以用于人脸识别的任

务。

Contrastive

Loss示意如图15-12所示。

图15-12 Siamese Model的Contrastive Loss

基

于PyTorch

2.0实现的损失函数如下

：

class ContrastiveLoss(torch.nn.Module):

def __init__(self,margin=2.0):

super(ContrastiveLoss,self).__init__()

self.margin=margin

def forward(self,output1,output2,label):

label=label.view(label.size()[0],)

euclidean_distance=F.pairwise_distance(output1,

output2)

loss_contrastive = torch.mean((1 - label)

*

torch.pow(euclidean_distance,

2) + (label) * torch.pow(torch.clamp(self.margin

- eucl

idean_distance, min=0.0),

2))

return

loss_contrastive

15.2.4 基于PyTorch 2.0的人脸识别模型

经

过前文的分析，读者对人

脸识别模型的基本架构

和

训练方法有了一定的

了解，实际上人脸识别模

型在具

体训练和使用中

复用上一节的模型训练

即可。从训练

方法到结果

的预测没有太大的差异

。最大不同就是训

练时间

的长度。由于人脸的特殊

性，训练过程需要耗

费非

常长的时间和机器性能

，这一点请读者注意。

1. 第一

步：人脸识别数据集的输

入

在上一节中我们准备

了人脸识别的数据集，并

通过

Dlib工具对数据进行了

人脸切割，只留下需要提

取特

征的人脸部分。对于

模型的输入是通过batch的方

式进

行，而每个batch中不同个

体的数据和每个个体能

够提

供的图片数量都是

有要求的。生成数据的代

码如下所

示：

from torch.utils.data import DataLoader, Dataset

import

random

import linecache

import torch

import

numpy as np

from PIL import

Image

class MyDataset(Dataset):

def __init__(self,path_file,transform=None,should_inve

rt=False):

#

path_file是所有人脸

图片的地址，每行地址是

一幅图片

self.transform=transform

self.should_invert=should_invert

self.path_file =

path_file

def __getitem__(self, index):

line=linecache.getline(self.path_file,

random.randint(1,self.__len__()))

img0_list=line.split("\\")

#若为0，取得不同

人的图片

shouled_get_same_class=random.randint(0,1)

if shouled_get_same_class:

while

True:

img1_list=linecache.getline(sel

f.path_file,

random.randint(1,self.__len__())).split('\\')

if img0_list[-1]==img1_list[-1

]:

break

else:

while True:

img1_list=linecache.getline(sel

f.path_file,

random.randint(1,self.__len__())).split('\\')

if img0_list[-1]!=img1_list[-1

]:

break

img0_path = "/".join(img0_list).replace("\n",

"")

img1_path =

"/".join(img1_list).replace("\n",

"")

im0=Image.open(img0_path).convert('L')

im1=Image.open(img1_path).convert('L')

im0 =

torch.tensor(np.array(im0))

im1 = torch.tensor(np.array(im1))

return im0,im1,torch.tensor(shouled_get_same_c

lass,

dtype=torch.float32)

def __len__(self):

fh=open(self.path_file,'r')

num=len(fh.readlines())

fh.close()

return num

if __name__ ==

'__main__':

path_file = "./dataset/lfw-path_file.txt"

ds =

MyDataset(path_file)

for _ in range(1024):

a,b,l

= ds.__getitem__(0)

print(a.shape)

print(b.shape)

print(l)

print("----------------")

2. 第二步：人脸识

别模型实战

在前文的介

绍中，我们已经讲解了本

实战中使用的方

法和相

关代码，接下来就是使用

这些内容进行人脸识

别

实战，这部分的完整代码

如下所示：

import

torch

from torch.utils.data import DataLoader

from

_15_2_2 import *

from _15_2_4 import

*

device = "cuda"

net=SiameseNetwork().to(device)

criterion=ContrastiveLoss()

optimizer=torch.optim.Adam(net.parameters(),lr=0.001)

counter=[]

loss_history=[]

iteration_number=0

batch_size =

2

path_file = "./dataset/lfw-path_file.txt"

train_dataset =

MyDataset(path_file=path_file)

train_loader = DataLoader(train_dataset,batch_size=

batch_size,

shuffle=True,num_workers=0,pin_memory=True)

for epoch in range(0,20):

for i,data

in enumerate(train_loader,0):

img0,img1,label=data

img0,img1,label=img0.float().to(device),

img1.float().to(device),label.to(device)

optimizer.zero_grad()

output1,output2=net(img0,img1)

loss_contrastive=criterion(output1,output2,labe

l)

loss_contrastive.backward()

optimizer.step()

if

i % 2 ==0:

print('epoch:{},loss:

{}\n'.format(epoch,loss_contrastive.item()))

counter.append(iteration_number)

loss_history.append(loss_contrastive.it

em())

最终结果请读

者自行完成。

15.3 本章小结

本

章讲解了利用深度学习

进行人脸识别的应用，实

现

了人脸识别模型的基

本架构，并通过实战做了

一个详

尽的演示，希望能

够帮助大家较好地掌握

人脸识别模

型的基本训

练和预测方法。本章采用

了一种较简易的

解决方

案来讲解使用深度学习

和PyTorch 2.0完成人

脸识别的基本

过程和步骤。除了本章实

现的人脸检测

和人脸识

别模型之外，随着人们对

深度学习模型研究

的深

入，更多更好的模型和框

架被创建和部署，人脸

识

别的准确率也有了进一

步的提高。本章只起到一

个

抛砖引玉的作用，如果

读者想要深入掌握人脸

识别技

术，后续还需要关

注和收集其最新研究成

果继续学习

和实践。

Copyright Copyright 2010, 2012 Adobe

Systems

Incorporated (http://www.adobe.com/), with

Reserved Font

Name ‘Source’. License This

Font Software

is licensed under the SIL Open

Font License, Version 1.1. This license

is

copied below, and is also

available with a FAQ

at: http://scripts.sil.org/OFL

SIL OPEN FONT

LICENSE Version 1.1

- 26 February 2007 PREAMBLE

The

goals of the Open Font License

(OFL) are to

stimulate worldwide development

of

collaborative font projects, to support

the

font creation efforts of academic

and

linguistic communities, and to provide

a free

and open framework in

which fonts may be shared

and

improved in partnership with others. The

OFL allows the licensed fonts to

be used,

studied, modified and redistributed

freely as

long as they are

not sold by themselves. The

fonts,

including any derivative works, can be

bundled, embedded, redistributed and/or sold

with

any software provided that any reserved

names are not used by derivative

works. The

fonts and derivatives, however,

cannot be

released under any other

type of license. The

requirement for

fonts to remain under this

license

does not apply to any document

created

using the fonts or their

derivatives.

DEFINITIONS "Font Software

"

refers

to the set

of files released

by the Copyright Holder(s)

under this

license and clearly marked as such.

This may include source files, build

scripts

and documentation. "Reserved Font Name

"

refers

to any names specified

as such after the

copyright statement(s).

"Original Version

"

refers to the

collection of Font Software

components as

distributed by the Copyright

Holder(s). "Modified

Version

"

refers to any

derivative

made by adding to, deleting, or

substituting â€” in part or in

whole â€”

any of the components

of the Original Version,

by changing

formats or by porting the Font

Software to a new environment. "Author

"

refers

to any designer, engineer,

programmer,

technical writer or other person

who

contributed to the Font Software.

PERMISSION &

CONDITIONS Permission is hereby

granted, free

of charge, to any

person obtaining a copy of

the

Font Software, to use, study, copy,

merge,

embed, modify, redistribute, and sell

modified

and unmodified copies of the

Font Software,

subject to the following

conditions: 1) Neither

the Font Software

nor any of its individual

components,

in Original or Modified Versions,

may

be sold by itself. 2) Original

or Modified

Versions of the Font

Software may be bundled,

redistributed and/or

sold with any software,

provided that

each copy contains the above

copyright

notice and this license. These can

be

included either as stand-alone text

files,

human-readable headers or in the

appropriate

machine-readable metadata fields within text

or

binary files as long as

those fields can be

easily viewed

by the user. 3) No Modified

Version of the Font Software may

use the

Reserved Font Name(s) unless

explicit written

permission is granted by

the corresponding

Copyright Holder. This restriction

only applies

to the primary font

name as presented to the

users.

4) The name(s) of the Copyright

Holder(s) or the Author(s) of the

Font Software

shall not be used

to promote, endorse or

advertise any

Modified Version, except to

acknowledge the

contribution(s) of the

Copyright Holder(s) and

the Author(s) or with

their explicit

written permission. 5) The Font

Software,

modified or unmodified, in part or

in

whole, must be distributed entirely

under this

license, and must not

be distributed under any

other license.

The requirement for fonts to

remain

under this license does not apply

to any

document created using the

Font Software.

TERMINATION This license becomes

null and void

if any of

the above conditions are not met.

DISCLAIMER THE FONT SOFTWARE IS PROVIDED

"AS

IS"

, WITHOUT WARRANTY OF

ANY KIND, EXPRESS OR

IMPLIED, INCLUDING

BUT NOT LIMITED TO ANY

WARRANTIES

OF MERCHANTABILITY, FITNESS FOR A

PARTICULAR

PURPOSE AND NONINFRINGEMENT OF

COPYRIGHT, PATENT,

TRADEMARK, OR OTHER RIGHT.

IN NO

EVENT SHALL THE COPYRIGHT HOLDER BE

LIABLE FOR ANY CLAIM, DAMAGES OR

OTHER

LIABILITY, INCLUDING ANY GENERAL, SPECIAL,

INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES,

WHETHER

IN AN ACTION OF CONTRACT, TORT

OR

OTHERWISE, ARISING FROM, OUT OF

THE USE OR

INABILITY TO USE

THE FONT SOFTWARE OR FROM

OTHER

DEALINGS IN THE FONT SOFTWARE.

*文中

代码字体版权说明
