








表示程序库

中存在的 bug 或经常会发生

的问题等警告信息，引起

读

者对该处内容的注意

。

读者意见与咨询

虽然笔

者已经尽最大努力对本

书的内容进行了检查与

确认，但是仍不免

在某些

地方出现错误或者容易

引起误解的表达及排版

问题等。如果读者遇到

这

些问题，请及时告知，我们

在本书重印时会将其改

正。与此同时，也欢迎

读者

为本书将来的修订提出

建议。本书编辑部的联系

方式如下。

株式会社 O’Reilly

Japan

电子

邮件 japan@oreilly.co.jp

本书的主页地址如

下。

https://www.ituring.com.cn/book/2678

https://www.oreilly.co.jp/books/9784873118369（日语）

https://github.com/oreilly-japan/deep-learning-from-scratch-2

  前言 xx

关于

O’Reilly 的其他

信息，可以访问下面的 O’Reilly 主

页查看。

http://www.oreilly.com/（英语）

http://www.oreilly.co.jp/（日语）

电子书

扫描如下二维码，即可购

买本书电子版。

第1章

神经

网络的复习

用一种以上

的方法认识一个事物，才

能真正理解它。

——马文·明斯

基（计算机科学家、认知科

学家）

本书是《深度学习入

门：基于

Python 的理论与实现》的

续作，将进一步

深入探索

深度学习的可能性。本书

和前作一样，不使用既有

的库和框架，重

视“从零开

始创建”，通过亲自动手，来

探寻深度学习相关技术

的乐趣和深度。

本章我们

将复习一下神经网络。也

就是说，本章相当于前作

的摘要。此

外，本书更加重

视效率，对前作中的部分

代码规范进行了修改（比

如，方法

名和参数的命名

方法等）。关于这一点，我们

也会在本章进行确认。

1.1 数

学和Python的复习

我们先来复

习一下数学。具体来说，就

是以神经网络的计算所

需的向

量、矩阵等为主题

展开讨论。为了顺利地切

入神经网络的实现，这里

将一并

展示相应的

Python 代码

，特别是基于 NumPy 的代码。

1.1.1

向量

和矩阵

在神经网络中，向

量和矩阵（或者张量）随处

可见。本节将对这些术语

进行简单的整理，为读者

阅读本书做准备。

  第 1章

神

经网络的复习 2

我们从向

量开始。向量是同时拥有

大小和方向的量。向量可

以表示为

排成一排的数

字集合，在 Python 实现中可以处

理为一维数组。与此相对

，

矩阵是排成二维形状（长

方阵）的数字集合。向量和

矩阵的例子如图 1-1

所示。

图

1-1　向量和矩阵的例子

行

列

如图 1-1 所示，向量和矩阵可

以分别用一维数组和二

维数组表示。另

外，在矩阵

中，将水平方向上的排列

称为行（row），将垂直方向上的

排列

称为列（column）。因此，图 1-1

中的

矩阵可以称为“3 行 2 列的矩

阵”，记

为“3 ×

2 的矩阵”。

将向量和

矩阵扩展到 N 维的数据集

合，就是张量。

向量是一个

简单的概念，请注意有两

种方式表示向量。如图

1-2 所

示，

一种是在垂直方向上

排列（列向量）的方法，另一

种是在水平方向上排列

（行向量）的方法。

1.1 数学和 Python的

复习

3

图1-2　向量的表示方法

列向量 行向量

在数学和

深度学习等许多领域，向

量一般作为列向量处理

。不过，考

虑到实现层面的

一致性，本书将向量作为

行向量处理（每次都会注

明是行向

量）。此外，在数学

式中写向量或矩阵时，会

用 x 或 W 等粗体表示，以将

它

们与单个元素（标量）区分

开。在源代码中，会用

x 或 W 这

样的字体表示。

在 Python

的实现

中，在将向量作为行向量

处理的情况下，会将

向量

明确变形为水平方向上

的矩阵。比如，当向量的元

素个数是

N 时，将其处理为

形状为 1 ×

N 的矩阵。我们后面

会看一个具体

的例子。

下

面，我们使用 Python 的对话模式

来生成向量和矩阵。当然

，这里将

使用处理矩阵的

标准库 NumPy。

>>> import numpy as

np

>>> x = np.array([1, 2,

3])

>>> x.__class__ # 输出类名

<class

'numpy.ndarray'>

>>> x.shape

(3,)

>>> x.ndim

1

>>> W = np.array([[1, 2,

3], [4, 5, 6]])

>>> W.shape

(2, 3)

>>> W.ndim

2

第 1章　神

经网络的复习 4

如上所示

，可以使用 np.array()

方法生成向量

或矩阵。该方法会生成

NumPy 的

多维数组类 np.ndarray。np.ndarray 类有许多便

捷的方法和实例

变量。上

面的例子中使用了实例

变量

shape 和 ndim。shape 表示多维数组的

形状，ndim 表示维数。从上面的

结果可知，x 是一维数组，是

一个元素个数

为 3 的向量

；而 W 是一个二维数组，是一

个 2

× 3（2 行 3 列）的矩阵。

1.1.2

矩阵的对

应元素的运算

前面我们

把数字的集合组织为了

向量或矩阵，现在利用它

们进行一些简

单的运算

。首先，我们看一下“对应元

素的运算”。顺便说一下，“对

应元素

的”的英文是“element-wise”。

>>> W

= np.array([[1, 2, 3], [4, 5,

6]])

>>> X = np.array([[0, 1,

2], [3, 4, 5]])

>>> W

+ X

array([[ 1, 3, 5],

[ 7, 9, 11]])

>>> W

* X

array([[ 0, 2, 6],

[12, 20, 30]])

这里

对 NumPy 多维数组执行了

+、* 等运

算。此时，运算是对应多维

数组中的元素（独立）进行

的，这就是 NumPy 数组中的对应

元素的运算。

1.1.3

广播

在 NumPy 多维

数组中，形状不同的数组

之间也可以进行运算，比

如下

面这个计算。

>>>

A = np.array([[1, 2], [3, 4]])

>>> A * 10

array([[10, 20],

[30, 40]])

1.1 数学和

Python的复习  5

这个计算是一个

2 × 2 的矩阵 A 乘以标量

10。此时，如

图 1-3 所示，标

量 10 先被扩展为

2

× 2 的矩阵，之后进行对应元

素的运算。这个灵巧的功

能称为广播（broadcast）。

图1-3　广播的例

子1：标量10被处理为2 ×

2的矩阵

下面再看另一个广播的

例子。

>>> A = np.array([[1, 2],

[3, 4]])

>>> b = np.array([10,

20])

>>> A * b

array([[10,

40],

 [30, 80]])

在这个计算中，如图

1-4 所示，一维数组

b 被“灵巧地

”扩展成了与二

维数组 A 相

同的形状。

图1-4

广播的例子

2

像这样，因为 NumPy 有广播功能

，所以可以智能地执行不

同形状的数

组之间的运

算。

为了使

NumPy 的广播功能生

效，多维数组的形状需要

满足几个

规则。关于广播

的详细规则，请参考文献

[1]。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重

版权

  第 1章　神经网络的复

习

6

1.1.4  向量内积和矩阵乘积

接着，我们来看一下向量

内积和矩阵乘积的相关

内容。首先，向量内积

可以

表示为

x

· y = x1y1 + x2y2

+ ··· + xnyn (1.1)

这里假设有x

= (x1, ··· , xn)和y =

(y1, ··· , yn)两

个向量。此时，如式(1.1)

所示，向

量内积是两个向量对应

元素的乘积之和。

向量内

积直观地表示了“两个向

量在多大程度上指向同

一方向”。如

果限定向量的

大小为 1，当两个向量完全

指向同一方向时，它们的

向量内积是 1。反之，如果两

个向量方向相反，则内积

为 −1。

下面再来看一下矩阵

乘积。矩阵乘积可以按照

图 1-5

所示的步骤计算。

图1-5　矩

阵乘积的计算方法

A B

如图

1-5

所示，矩阵乘积通过“左侧

矩阵的行向量（水平方向

）”和

“右侧矩阵的列向量（垂

直方向）”的内积（对应元素

的乘积之和）计算得出。

此

时，计算结果保存在新矩

阵的对应元素中。比如，A 的

第 1 行和

B 的第

1 列的结果保

存在第 1 行第

1 列的元素中

，A 的第 2 行和 B

的第 1 列的结

1.1 数

学和 Python的复习

7

果保存在第

2 行第 1 列的元素中。

现在，我

们用

Python 实现一下向量内积

和矩阵乘积。为此，可以利

用

np.dot()。

# 向量内积

>>>

a = np.array([1, 2, 3])

>>>

b = np.array([4, 5, 6])

>>>

np.dot(a, b)

32

# 矩阵乘积

>>>

A = np.array([[1, 2], [3, 4]])

>>> B = np.array([[5, 6], [7,

8]])

>>> np.dot(A, B)

array([[19, 22],

[43, 50]])

如

上所示，向量内积和矩阵

乘积中都使用了 np.dot()。当 np.dot(x, y)

的参

数都是一维数组时，计算

向量内积。当参数都是二

维数组时，计算矩阵

乘积

。

除了这里看到的 np.dot() 方法外

，NumPy 还有很多其他的进行矩

阵计

算的便捷方法。如果

能熟练掌握这些方法，神

经网络的实现就会更顺

利。

熟能生巧

要掌握 NumPy，实际

动手练习是最有效的。比

如，“100 numpy

exercises”[2] 中准备了 100 道 NumPy 的

练 习 题

。如 果 你 想

积 累

NumPy 经验，请挑

战一下。

1.1.5

矩阵的形状检查

在使用矩阵和向量的计

算中，很重要的一点就是

要注意它们的形状。这

里

，我们留意着矩阵的形状

，再来看一下矩阵乘积。前

面已经介绍过了矩阵

乘

积的计算步骤，所以这里

我们将重点放在图 1-6 所示

的“形状检查”上。

第 1章　神经

网络的复习 8

图1-6　形状检查

：在矩阵乘积中，要使对应

维度的元素个数一致

A B C

形

状：

一致

图

1-6 展示了基于 3 × 2 的

矩阵

A 和 2 × 4 的矩阵

B 生成 3 × 4 的矩

阵

C 的示例。此时，如该图所

示，需要对齐矩阵 A 和矩阵

B 的对应维度的元

素个数

。作为计算结果的矩阵

C 的

形状由 A 的行数和 B 的列数

组成。这就

是矩阵的形状

检查。

在矩阵乘积等计算

中，注意矩阵的形状并观

察其变化的形状检查非

常重要。据此，神经网络的

实现可以更顺利地进行

。

1.2 神经网络的推理

现在我

们开始复习神经网络。神

经网络中进行的处理可

以分为学习和推

理两部

分。本节将围绕神经网络

的推理展开说明，而神经

网络的学习会在下

一节

进行讨论。

1.2.1  神经网络的推

理的全貌图

简单地说，神

经网络就是一个函数。函

数是将某些输入变换为

某些输出

的变换器，与此

相同，神经网络也将输入

变换为输出。

举个例子，我

们来考虑输入二维数据

、输出三维数据的函数。为

了使用

神经网络进行实

现，需要在输入层准备 2 个

神经元，在输出层准备 3 个

神经

元。然后，在隐藏层（中

间层）放置若干神经元，这

里我们放置 4 个神经元。

1.2 神

经网络的推理

9

这样一来

，我们的神经网络就可以

画成图 1-7。

图1-7　神经网络的例

子

输入层

隐藏层 输出层

在图 1-7 中，用〇表示神经元，用

箭头表示它们的连接。此

时，在箭

头上有权重，这个

权重和对应的神经元的

值分别相乘，其和（严格地

讲，是

经过激活函数变换

后的值）作为下一个神经

元的输入。另外，此时还要

加上

一个不受前一层的

神经元影响的常数，这个

常数称为偏置。因为所有

相邻

的神经元之间都存

在由箭头表示的连接，所

以图 1-7 的神经网络称为全

连接

网络。

图

1-7 的网络一共

包含 3 层，但有权重的层实

际上是 2 层，本书中将

这样

的神经网络称为 2 层神经

网络。因为图 1-7 的网络由 3

层

组成，

所以有的文献也称

之为 3 层神经网络。

下面用

数学式来表示图 1-7

的神经

网络进行的计算。这里用

(x1, x2) 表

示输入层的数据，用 w11 和

w12

表示权重，用 b1 表示偏置。这

样一来，图

1-7 中的隐藏层的

第 1

个神经元就可以如下

进行计算：

h1 = x1w11 + x2w21

+ b1 (1.2)

如式 (1.2) 所示，隐藏

层的神经元是基于加权

和计算出来的。之后，改

变

权重和偏置的值，根据神

经元的个数，重复进行相

应次数的式 (1.2) 的计

算，这样

就可以求出所有隐藏层

神经元的值。

图灵社区会

员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 1章

神

经网络的复习 10

权重和偏

置都有下标，这个下标的

规则（为何将下标设为 11 或

12 等）

并不是很重要，重要的

是神经元是通过加权和

计算的，并且可以通过矩

阵

乘积整体计算。实际上

，基于全连接层的变换可

以通过矩阵乘积如下进

行

整理：

(h1, h2, h3,

h4)=(x1, x2)

 w11 w12

w13 w14

w21 w22 w23 w24

+ (b1, b2, b3, b4)

(1.3)

这里，隐藏层的神

经元被整理为 (h1, h2, h3, h4)，它可以看

作

1 × 4 的矩阵

（或者行向量）。另

外，输入是 (x1,

x2)，这是一个 1 × 2 的矩

阵。再者，权

重是

2 × 4 的矩阵，偏

置是 1 ×

4 的矩阵。这样一来，式

(1.3) 可以如下进行

简化：

h =

xW + b (1.4)

这里

，输入是 x，隐藏层的神经元

是

h，权重是 W，偏置是 b，这些都

是矩

阵。此时，留意式 (1.4) 的矩

阵形状，可知进行了如图

1-8

所示的变换。

图1-8　形状检查

：确认对应维度的元素个

数一致（省略偏置）

形状：

一

致

如图

1-8 所示，在矩阵乘积

中，要使对应维度的元素

个数一致。通过像

这样观

察矩阵的形状，可以确认

变换是否正确。

在矩阵乘

积的计算中，形状检查非

常重要。据此，可以判断计

算是

否正确（至少可以判

断计算是否成立）。

1.2

神经网

络的推理  11

这样一来，我们

就可以利用矩阵来整体

计算全连接层的变换。不

过，这

里进行的变换只针

对单笔样本数据（输入数

据）。在神经网络领域，我们

会

同时对多笔样本数据

（称为

mini-batch，小批量）进行推理和

学习。因此，

我们将单独的

样本数据保存在矩阵 x 的

各行中。假设要将 N 笔样本

数据作

为 mini-batch 整体处理，关注

矩阵的形状，其变换如图

1-9 所示。

图1-9　形状检查：mini-batch版的矩

阵乘积（省略偏置）

形状：

一

致

如图1-9所示，根据形状检

查，可知各mini-batch被正确地进行

了变换。

此时，N 笔样本数据

整体由全连接层进行变

换，隐藏层的 N

个神经元被

整

体计算出来。现在，我们

用 Python 写出 mini-batch 版的全连接层变

换。

>>> import numpy as np

>>>

W1 = np.random.randn(2, 4) # 权重

>>> b1 = np.random.randn(4) # 偏置

>>> x = np.random.randn(10, 2) #

输入

>>> h = np.dot(x, W1)

+ b1

在这个

例子中，10 笔样本数据分别

由全连接层进行变换。此

时，x 的第

1

个维度对应于各

笔样本数据。比如，x[0] 是第 0 笔

输入数据，x[1] 是第 1

笔输入数

据……类似地，h[0] 是第 0 笔数据的

隐藏层的神经元，h[1] 是第

1

笔

数据的隐藏层的神经元

，以此类推。

在上面的代码

中，偏置 b1的加法运算会触

发广播功能。b1的形状

是 (4,)，它

会被自动复制，变成 (10,

4)的形

状。

  第 1章　神经网络的复习

12

全连接层的变换是线性

变换。激活函数赋予它“非

线性”的效果。严格

地讲，使

用非线性的激活函数，可

以增强神经网络的表现

力。激活函数有很

多种，这

里我们使用式 (1.5) 的 sigmoid

函数（sigmoid function）：

σ(x) = 1

1

+ exp(−x) (1.5)

如

图 1-10 所示，sigmoid

函数呈 S 形曲线。

图

1-10 sigmoid函数的图像

sigmoid

函数接收任

意大小的实数，输出 0 ～ 1 的实

数。现在我们用

Python

来实现这

个 sigmoid 函数。

def sigmoid(x):

return 1 / (1 + np.exp(-x))

这是式 (1.5) 的直接实

现，应该没有特别难的地

方。现在，我们使用这

个 sigmoid 函

数，来变换刚才的隐藏层

的神经元。

1.2 神经网络的推

理  13

>>> a

= sigmoid(h)

基于 sigmoid 函数，可以进行非

线性变换。然后，再用另一

个全连接层

来变换这个

激活函数的输出

a（也称为

activation）。这里，因为隐藏层有 4

个神

经元，输出层有 3 个神经元

，所以全连接层使用的权

重矩阵的形状必须

设置

为

4 × 3，这样就可以获得输出

层的神经元。以上就是神

经网络的推理。

现在我们

用 Python 将这一段内容总结如

下。

import numpy as np

def sigmoid(x):

return 1 / (1 + np.exp(-x))

x = np.random.randn(10, 2)

W1 =

np.random.randn(2, 4)

b1 = np.random.randn(4)

W2

= np.random.randn(4, 3)

b2 = np.random.randn(3)

h = np.dot(x, W1) + b1

a = sigmoid(h)

s = np.dot(a,

W2) + b2

这里，x 的形状是 (10,

2)，表示 10 笔

二维数据组织为了 1 个 mini-batch。

最

终输出的 s 的形状是 (10, 3)。同样

，这意味着 10

笔数据一起被

处理了，

每笔数据都被变

换为了三维数据。

上面的

神经网络输出了三维数

据。因此，使用各个维度的

值，可以分为

3 个类别。在这

种情况下，输出的三维向

量的各个维度对应于各

个类的“得

分”（第

1 个神经元

是第 1 个类别，第 2 个神经元

是第

2 个类别……）。在

实际进行

分类时，寻找输出层神经

元的最大值，将与该神经

元对应的类别作

为结果

。

得分是计算概率之前的

值。得分越高，这个神经元

对应的类别的概

率也越

高。后面我们会看到，通过

把得分输入

Softmax 函数，可以

获

得概率。

  第

1章　神经网络的

复习 14

以上就是神经网络

的推理部分的实现。接下

来，我们使用Python的类，

将这些

处理实现为层。

1.2.2

层的类化

及正向传播的实现

现在

，我们将神经网络进行的

处理实现为层。这里将全

连接层的变换实

现为 Affine 层

，将 sigmoid

函数的变换实现为 Sigmoid 层

。因为全连接层

的变换相

当于几何学领域的仿射

变换，所以称为 Affine 层。另外，将

各个层

实现为 Python 的类，将主

要的变换实现为类的 forward() 方

法。

神经网络的推理所进

行的处理相当于神经网

络的正向传播。顾名思义

，

正向传播是从输入层到

输出层的传播。此时，构成

神经网络的各层

从输入

向输出方向按顺序传播

处理结果。之后我们会进

行神经网络

的学习，那时

会按与正向传播相反的

顺序传播数据（梯度），所以

称

为反向传播。

神经网络

中有各种各样的层，我们

将其实现为 Python

的类。通过这

种

模块化，可以像搭建乐

高积木一样构建网络。本

书在实现这些层时，制定

以

下“代码规范”。

• 所有的层

都有 forward()

方法和 backward() 方法

• 所有的

层都有 params

和 grads 实例变量

简单

说明一下这个代码规范

。首先，forward() 方法和 backward()

方法

分别对

应正向传播和反向传播

。其次，params 使用列表保存权重

和偏置等参

数（参数可能

有多个，所以用列表保存

）。grads 以与 params

中的参数对应

的形

式，使用列表保存各个参

数的梯度（后述）。这就是本

书的代码规范。

遵循上述

代码规范，代码看上去会

更清晰。我们后面会说明

为什么

要遵循这样的规

范，以及它的有效性。

图灵

社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

1.2 神经网络的推理  15

因为这

里只考虑正向传播，所以

我们仅关注代码规范中

的以下两点：一

是在层中

实现 forward() 方法；二是将参数整

理到实例变量 params 中。我们

基

于这样的代码规范来实

现层，首先实现

Sigmoid 层，如下所

示（ ch01/

forward_net.py）。

import numpy

as np

class Sigmoid:

 def

__init__(self):

 self.params = []

def forward(self, x):

 return 1

/ (1 + np.exp(-x))

如 上

所 示，sigmoid 函 数 被 实

现

为 一 个 类， 主 变

换 处 理 被

实 现 为

forward(x) 方法。这里，因为 Sigmoid 层

没有需要学习的参数，所

以使用空

列表来初始化

实例变量 params。下面，我们接着

来看一下全连接层

Affine 层

的

实现，如下所示（ ch01/forward_net.py）。

class Affine:

def __init__(self, W, b):

 self.params

= [W, b]

 def forward(self,

x):

 W, b = self.params

out = np.dot(x, W) + b

return out

Affine 层在初始

化时接收权重和偏置。此

时，Affine 层的参数是权重和

偏

置（在神经网络的学习时

，这两个参数随时被更新

）。因此，我们使用列

表将这

两个参数保存在实例变

量 params 中。然后，实现基于 forward(x) 的

正

向传播的处理。

根据本书

的代码规范，所有的层都

需要在实例变量 params中保存

要

学习的参数。因此，可以

很方便地将神经网络的

全部参数整理在一起，

参

数的更新操作、在文件中

保存参数的操作都会变

得更容易。

第 1章　神经网络

的复习 16

现在，我们使用上

面实现的层来实现神经

网络的推理处理。这里实

现如

图

1-11 所示的层结构的

神经网络。

Affine Sigmoid Affine

图1-11

要实现的神

经网络的层结构

如图 1-11 所

示，输入 X 经由

Affine 层、Sigmoid 层和 Affine 层后

输出

得分

S。我们将这个神

经网络实现为名为 TwoLayerNet 的类

，将主推理处理

实现为 predict(x) 方

法。

之前，我们在用图表示

神经网络时，使用的是像

图 1-7 那样的“神经

元视角”的

图。与此相对，图 1-11 是“层视角

”的图。

TwoLayerNet 的代码如下所示（ ch01/forward_net.py）。

class TwoLayerNet:

def __init__(self, input_size, hidden_size, output_size):

I, H, O = input_size, hidden_size,

output_size

 # 初

始化权重和偏置

 W1

= np.random.randn(I, H)

 b1 =

np.random.randn(H)

 W2 = np.random.randn(H, O)

b2 = np.random.randn(O)

1.2 神经网

络的推理

17

 # 生成层

 self.layers

= [

 Affine(W1, b1),

Sigmoid(),

 Affine(W2, b2)

 ]

# 将所有

的权重整理到列表中

 self.params = []

for layer in self.layers:

 self.params

+= layer.params

 def predict(self, x):

for layer in self.layers:

 x

= layer.forward(x)

 return x

在

这个类的初始化方法中

，首先对权重进行初始化

，生成

3 个层。然

后，将要学习

的权重参数一起保存在

params 列表中。这里，因为各个层

的

实例变量 params

中都保存了

学习参数，所以只需要将

它们拼接起来即可。

这样

一来，TwoLayerNet 的 params 变量中就保存了

所有的学习参数。像这样

，

通过将参数整理到一个

列表中，可以很轻松地进

行参数的更新和保存。

此

外，Python 中可以使用 + 运算符进

行列表之间的拼接。下面

是一个

简单的例子。

>>>

a = ['A', 'B']

>>> a

+= ['C', 'D']

>>> a

['A',

'B', 'C', 'D']

如上

所示，通过列表之间的加

法将列表拼接了起来。在

上面的 TwoLayerNet

的实现中，通过将

各个层的

params 列表加起来，从

而将全部学习参数整理

到了一个列表中。现在，我

们使用 TwoLayerNet 类进行神经网络

的推理。

x =

np.random.randn(10, 2)

model = TwoLayerNet(2, 4,

3)

s = model.predict(x)

第 1章　神经网络的

复习 18

这样就可以求出输

入数据 x

的得分 s 了。像这样

，通过将层实现为类，

可以

轻松实现神经网络。另外

，因为要学习的参数被汇

总在 model.params

中，所以之后进行神

经网络的学习会更加容

易。

1.3 神经网络的学习

不进

行神经网络的学习，就做

不到“好的推理”。因此，常规

的流程是，

首先进行学习

，然后再利用学习好的参

数进行推理。所谓推理，就

是对上一

节介绍的多类

别分类等问题给出回答

的任务。而神经网络的学

习的任务是寻

找最优参

数。本节我们就来研究神

经网络的学习。

1.3.1  损失函数

在神经网络的学习中，为

了知道学习进行得如何

，需要一个指标。这个

指标

通常称为损失（loss）。损失指示

学习阶段中某个时间点

的神经网络的

性能。基于

监督数据（学习阶段获得

的正确解数据）和神经网

络的预测结果，

将模型的

恶劣程度作为标量（单一

数值）计算出来，得到的就

是损失。

计算神经网络的

损失要使用损失函数（loss function）。进

行多类别分

类的神经网

络通常使用交叉熵误差

（cross entropy error）作为损失函数。

此时，交叉

熵误差由神经网络输出

的各类别的概率和监督

标签求得。

现在，我们来求

一下之前一直在研究的

那个神经网络的损失。这

里，我

们将 Softmax 层和 Cross Entropy

Error 层新添加

到网络中。用 Softmax 层

求 Softmax

函数的

值，用 Cross Entropy Error 层求交叉熵误差。如

果基于

“层视角”来绘制此

时的网络结构，则如图

1-12 所

示。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

1.3 神经网络的学习

19

图1-12　使用了损失函数的神

经网络的层结构

Affine

Sigmoid Affine Softmax

Cross 

Entropy

Error

⯾Ⲑᴴカ

在图 1-12 中

，X 是输入数据，t

是监督标签

，L 是损失。此时，Softmax

层的输出是

概率，该概率和监督标签

被输入 Cross Entropy Error

层。

下面，我们来介

绍一下 Softmax 函数和交叉熵误

差。首先，Softmax

函数可由下式表

示：

yk

= exp(sk)

 n

i=1

exp(si)

(1.6)

式 (1.6) 是当输出总共有 n

个

时，计算第 k 个输出 yk 时的算

式。这个

yk

是对应于第 k 个类

别的 Softmax 函数的输出。如式 (1.6)

所

示，Softmax

函数的分子是得分 sk 的

指数函数，分母是所有输

入信号的指数函数的和

。

Softmax 函数输出的各个元素是

0.0

～ 1.0 的实数。另外，如果将这些

元素全部加起来，则和为

1。因此，Softmax 的输出可以解释为

概率。之后，

这个概率会被

输入交叉熵误差。此时，交

叉熵误差可由下式表示

：

L

= −



k

tk

log yk (1.7)

  第

1章　神经网络的复习 20

这

里，tk 是对应于第 k

个类别的

监督标签。log 是以纳皮尔数

e 为底的对数

（严格地说，应

该记为 log e）。监督标签以

one-hot 向量

的形式表示，比如

t = (0, 0,

1)。

one-hot 向量是

一个元素为 1，其他元素为

0 的向量。因为元素 1

对

应正

确解的类，所以式 (1.7) 实际上

只是在计算正确解标签

为 1 的元

素所对应的输出

的自然对数 (log)。

另外，在考虑

了 mini-batch 处理的情况下，交叉熵

误差可以由下式

表示：

L = −N

1

n

 k

tnk log

ynk (1.8)

这

里假设数据有 N 笔，tnk 表示第

n

笔数据的第 k 维元素的值

，ynk 表示神

经网络的输出，tnk 表

示监督标签。

式 (1.8) 看上去有

些复杂，其实只是将表示

单笔数据的损失函数的

式

(1.7) 扩展到了 N

笔数据的情

况。用式 (1.8) 除以 N，可以求单笔

数据的平

均损失。通过这

样的平均化，无论 mini-batch

的大小

如何，都始终可以获

得一

致的指标。

本书将计算 Softmax 函

数和交叉熵误差的层实

现为 Softmax

with Loss

层（通过整合这两个

层，反向传播的计算会变

简单）。因此，学习阶段的神

经网络具有如图 1-13 所示的

层结构。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

1.3 神经网络的

学习

21

图1-13 使用Softmax with Loss层输出损失

Affine Sigmoid

Affine

Softmax 

with 

Loss

如图 1-13 所示，本书使用 Softmax with Loss

层。这

里我们省略对其

实现的

说明，代码在 common/layers.py 中，感兴趣的

读者可以参考。此外，

前作

《深度学习入门：基于 Python

的理

论与实现》的 4.2 节中也详细

介绍

了 Softmax with

Loss 层。

1.3.2  导数和梯度

神

经网络的学习的目标是

找到损失尽可能小的参

数。此时，导数和梯度

非常

重要。这里我们来简单说

明一下导数和梯度。

现在

，假设有一个函数 y = f(x)。此时，y 关

于

x 的导数记为 d

d

x

y

。这

个 d

d

x

y

的意

思是变化程度，具体来说

，就是 x 的微小（严格地讲，“微

小”为

无限小）变化会导致

y 发生多大程度的变化。

比

如函数

y = x2

，其导数可以解析

性地求出，即 d

d

x

y = 2x。这个导数

结

果表示 x

在各处的变化程

度。实际上，如图 1-14 所示，它相

当于函数的

斜率。

第 1章　神

经网络的复习 22

x

y

图1-14 y = x2的导数

表示x在各处的斜率

在图

1-14 中，我们求了关于

x 这一个

变量的导数，其实同样可

以求关

于多个变量（多变

量）的导数 A。假设有函数 L =

f(x)，其

中 L 是标量，x

是向量。此时，L 关

于 xi（x

的第 i 个元素）的导数可

以写成 x

L

i。另外，

也可以求关

于向量的其他元素的导

数，我们将其整理如下：

L

x =

x

L

1

, x

L

2

,..., x

L

n

(1.9)

像

这样，将关于向量各个元

素的导数罗列到一起，就

得到了梯度（gradient）。

另外，矩阵也

可以像向量一样求梯度

。假设 W 是一个 m

× n 的矩阵，

则函

数 L =

g(W) 的梯度如下所示：

L

W =





W

L

11 ··· L

W1n

.

.

. ...

L

Wm1

L

Wmn





(1.10)

A 从严

格意义上讲，对多变量函

数的某个变量求得的导

数称为偏导数。本书考虑

到易读性，在不影响

理解

的情况下，统一使用“导数

”一词。——译者注

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

1.3

神经网络的

学习  23

如式 (1.10) 所示，L

关于 W 的梯

度可以写成矩阵（准确地

说，矩阵的

梯度的定义如

上所示）。这里的重点是，W 和

W

L

具有相同的形状。利用

“矩

阵和其梯度具有相同形

状”这一性质，可以轻松地

进行参数的更新和链

式

法则（后述）的实现。

严格地

说，本书使用的“梯度”一词

与数学中的“梯度”是不同

的。

数学中的梯度仅限于

关于向量的导数。而在深

度学习领域，一般

也会定

义关于矩阵和张量的导

数，称为“梯度”。

1.3.3  链式法则

学

习阶段的神经网络在给

定学习数据后会输出损

失。这里我们想得到的

是

损失关于各个参数的梯

度。只要得到了它们的梯

度，就可以使用这些梯度

进行参数更新。那么，神经

网络的梯度怎么求呢？这

就轮到误差反向传播法

出场了。

理解误差反向传

播法的关键是链式法则

。链式法则是复合函数的

求导法

则，其中复合函数

是由多个函数构成的函

数。

现在，我们来学习链式

法则。这里考虑 y = f(x) 和

z = g(y) 这两个

函

数。如 z

= g(f(x)) 所示，最终的输出

z 由两个函数计算而来。此

时，z 关

于

x 的导数可以按下

式求得：

z

x = z

y

y

x (1.11)

如式 (1.11)

所示，z 关于 x 的

导数由 y =

f(x) 的导数和 z = g(y) 的导

数

之积求得，这就是链式法

则。链式法则的重要之处

在于，无论我们要处理

的

函数有多复杂（无论复合

了多少个函数），都可以根

据它们各自的导数来

求

复合函数的导数。也就是

说，只要能够计算各个函

数的局部的导数，就能

基

于它们的积计算最终的

整体的导数。

∂

∂

∂

∂

∂

∂

∂

∂

第 1章　神经网

络的复习 24

可以认为神经

网络是由多个函数复合

而成的。误差反向传播法

会充

分利用链式法则来

求关于多个函数（神经网

络）的梯度。

1.3.4  计算图

下面，我

们将研究误差反向传播

法。不过在此之前，作为准

备工作，我

们先来介绍一

下计算图的相关内容。计

算图是计算过程的图形

表示。图

1-15

所示为计算图的

一个例子。

x

y

z

+

图1-15 z = x + y的计算图

如

图 1-15 所示，计算图通过节点

和箭头来表示。这里，“+”表示

加法，

变量 x 和

y 写在各自的

箭头上。像这样，在计算图

中，用节点表示计算，处

理

结果有序（本例中是从左

到右）流动。这就是计算图

的正向传播。

使用计算图

，可以直观地把握计算过

程。另外，这样也可以直观

地求梯

度。这里重要的是

，梯度沿与正向传播相反

的方向传播，这个反方向

的传播

称为反向传播。

这

里我想先说明一下反向

传播的全貌。虽然我们处

理的是 z = x + y

这

一计算，但是在

该计算的前后，还存在其

他的“某种计算”（图1-16）。另外，

假

设最终输出的是标量 L（在

神经网络的学习阶段，计

算图的最终输出是损

失

，它是一个标量）。

1.3

神经网络

的学习  25

x

y

z

+ L

᳽⻺䃎ツ

᳽⻺䃎ツ ᳽⻺䃎ツ

图1-16

加法节点构成

“复杂计算”的一部分

我们

的目标是求 L 关于各个变

量的导数（梯度）。这样一来

，计算图的

反向传播就可

以绘制成图 1-17。

x

y

z

+

L

᳽⻺䃎ツ

᳽⻺䃎ツ

᳽⻺䃎ツ L

x

L

y

L

z

L

L

图1-17　计算图的

反向传播

如图 1-17 所示，反向

传播用蓝色的粗箭头表

示，在箭头的下方标注传

播的值。此时，传播的值是

指最终的输出 L 关于各个

变量的导数。在这个

例子

中，关于

z 的导数是 L

z，关于 x 和

y

的导数分别是 L

x 和 L

y。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

  第 1章

神

经网络的复习 26

接着，该链

式法则出场了。根据刚才

复习的链式法则，反向传

播中流动

的导数的值是

根据从上游（输出侧）传来

的导数和各个运算节点

的局部导数

之积求得的

。因此，在上面的例子中， L

x = L

z

z

x

， L

y = L

z

z

y。

这

里，我们来处理 z = x

+ y 这个基于

加法节点的运算。此时，分

别解

析性地求得 x

z

= 1， y

z = 1。因此，如

图

1-18 所示，加法节点将上游

传来

的值乘以 1，再将该梯

度向下游传播。也就是说

，只是原样地将从上游传

来

的梯度传播出去。

x

y

z

L

z · 1

L

z · 1

L

z

+ +

图1-18　加

法节点的正向传播（左图

）和反向传播（右图）

像这样

，计算图直观地表示了计

算过程。另外，通过观察反

向传播的梯

度的流动，可

以帮助我们理解反向传

播的推导过程。

在构成计

算图的运算节点中，除了

这里见到的加法节点之

外，还有很多

其他的运算

节点。下面，我们将介绍几

个典型的运算节点。

1.3.4.1　乘法

节点

乘法节点是 z

= x × y 这样的

计算。此时，导数可以分别

求出，即

z

x = y 和 y

z

= x。因此，如图 1-19 所示

，乘法节点的反向传播会

将“上游

传来的梯度”乘以

“将正向传播时的输入替

换后的值”。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

1.3 神经网络的学

习  27

x

y

z

L

z x

L

z

L

z y

图1-19　乘法节点的正向传

播（左图）和反向传播（右图

）

另外，在目前为止的加法

节点和乘法节点的介绍

中，流过节点的数据都

是

“单变量”。但是，不仅限于单

变量，也可以是多变量（向

量、矩阵或张

量）。当张量流

过加法节点（或者乘法节

点）时，只需独立计算张量

中的各

个元素。也就是说

，在这种情况下，张量的各

个元素独立于其他元素

进行对

应元素的运算。

1.3.4.2

分

支节点

如图 1-20 所示，分支节

点是有分支的节点。

x

x

x L

x +

L

x

L

x

L

x

图1-20　分

支节点的正向传播（左图

）和反向传播（右图）

严格来

说，分支节点并没有节点

，只有两根分开的线。此时

，相同的值

被复制并分叉

。因此，分支节点也称为复

制节点。如图 1-20 所示，它的反

向传播是上游传来的梯

度之和。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

第 1章　神经网络的

复习 28

1.3.4.3

Repeat 节点

分支节点有两

个分支，但也可以扩展为

N 个分支（副本），这里称为

Repeat 节

点。现在，我们尝试用计算

图绘制一个

Repeat 节点（图 1-21）。

D

D

D

D

Repeat

Repeat

N

N

图1-21

Repeat节

点的正向传播（上图）和反

向传播（下图）

如图 1-21 所示，这

个例子中将长度为 D 的数

组复制了

N 份。因为这个

Repeat 节

点可以视为 N 个分支节点

，所以它的反向传播可以

通过

N 个梯度

的总和求出

，如下所示。

>>> import numpy

as np

>>> D, N =

8, 7

>>> x = np.random.randn(1,

D) # 输入

>>> y =

np.repeat(x, N, axis=0) # 正向传播

>>> dy

= np.random.randn(N, D) # 假设的梯度

>>>

dx = np.sum(dy, axis=0, keepdims=True) #

反向传播

1.3 神

经网络的学习  29

这里通过

np.repeat()

方法进行元素的复制。上

面的例子中将复制 N 次

数

组 x。通过指定 axis，可以指定沿

哪个轴复制。因为反向传

播时要计算

总和，所以使

用 NumPy 的 sum() 方法。此时，通过指定

axis 来指定对哪

个轴求和。另

外，通过指定 keepdims=True，可以维持二

维数组的维数。在

上面的

例子中，当 keepdims=True 时，np.sum() 的结果的形

状是

(1, D)；当

keepdims=False 时，形状是 (D,)。

NumPy

的广播

会复制数组的元素。这可

以通过 Repeat 节点来表示。

1.3.4.4 Sum 节点

Sum

节点是通用的加法节点

。这里考虑对一个 N × D 的数组

沿第 0

个

轴求和。此时，Sum 节点

的正向传播和反向传播

如图 1-22 所示。

D

Sum

Sum

N

N

D

D

D

图1-22 Sum节点的正向

传播（上图）和反向传播（下

图）

  第

1章　神经网络的复习

30

如图 1-22 所示，Sum 节点的反向传

播将上游传来的梯度分

配到所有箭

头上。这是加

法节点的反向传播的自

然扩展。下面，和 Repeat 节点一样

，

我们也来展示一下 Sum 节点

的实现示例，如下所示。

>>> import numpy as np

>>>

D, N = 8, 7

>>>

x = np.random.randn(N, D) # 输

入

>>> y = np.sum(x, axis=0, keepdims=True)

# 正向传播

>>> dy = np.random.randn(1,

D) # 假设的梯度

>>> dx = np.repeat(dy,

N, axis=0) # 反向传播

如上所示，Sum 节点

的正向传播通过

np.sum() 方法实

现，反向传播通

过 np.repeat() 方法实

现。有趣的是，Sum 节点和

Repeat 节点

存在逆向关

系。所谓逆向

关系，是指 Sum 节点的正向传

播相当于 Repeat

节点的反向传

播，Sum 节点的反向传播相当

于 Repeat 节点的正向传播。

1.3.4.5 MatMul

节点

本书将矩阵乘积称为 MatMul 节

点。MatMul 是 Matrix Multiply

的缩

写。因为 MatMul 节点的

反向传播稍微有些复杂

，所以这里我们先进行一

般

性的介绍，再进行直观

的解释。

为了解释

MatMul 节点，我

们来考虑 y = xW 这个计算。这里

，x、

W、y 的形状分别是 1 × D、D ×

H、1 × H（图 1-23）。

x

W

y

(1×D)

(D×H)

(1×H)

MatMul

图1-23

MatMul节点

的正向传播：矩阵的形状

显示在各个变量的上方

1.3 神经网络的学习  31

此时，可

以按如下方式求得关于

x 的第

i 个元素的导数 x

L

i。

L

xi

= 

j y

L

j

∂y

∂x

j

i

(1.12)

式 (1.12) 的

x

L

i

表示变化程度，即当 xi 发生

微小的变化时，L 会有多

大

程度的变化。如果此时改

变 xi，则向量

y 的所有元素都

会发生变化。另

外，因为 y 的

各个元素会发生变化，所

以最终 L

也会发生变化。因

此，从

xi 到 L 的链式法则的路

径有多个，它们的和是 x

L

i。

式

(1.12) 仍可进一步简化。利用 y

x

j

i = Wij，将

其代入式 (1.12)：

L

xi

= 

j y

L

j

y

x

j

i

= 

j

L

yj

Wij (1.13)

由式 (1.13) 可知， x

L

i 由向

量 L

y 和

W 的第 i 行向量的内积

求得。从这

个关系可以导

出下式：

L

x = L

y

WT (1.14)

如式 (1.14) 所示， L

x 可由矩

阵乘积一次求得。这里，WT

的

T 表示转

置矩阵。对式 (1.14) 进行

形状检查，结果如图 1-24

所示

。

L

x

L

y

1×D

1×H H×D

WT

形状：

图1-24　矩阵乘积的形状

检查

形状：

如图 1-24 所示，矩阵

形状的演变是正确的。由

此，可以确认式 (1.14)

的计算是

正确的。然后，我们可以反

过来利用它（为了保持形

状合规）来推

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

第 1章　神经网

络的复习 32

导出反向传播

的数学式（及其实现）。为了

说明这个方法，我们再次

考虑矩

阵乘积的计算

y = xW。不

过，这次考虑 mini-batch 处理，假设 x

中

保存

了 N 笔数据。此时，x、W、y 的形

状分别是 N

× D、D × H、N × H，反

向传播的计

算图如图 1-25 所示。

x

W

y

(N×D)

(N×D)

(N×H)

(N×H)

(D×H)

(D×H)

MatMul

L

x

L

W

L

y

图1-25 MatMul节点的

反向传播

那么， L

x

将如何计

算呢？此时，和 L

x 相关的变量

（矩阵）是上游传来

的 L

y 和 W。为

什么说和 W 有关系呢？考虑

到乘法的反向传播的话

，就容

易理解了。因为乘法

的反向传播中使用了“将

正向传播时的输入替换

后的

值”。同理，矩阵乘积的

反向传播也使用“将正向

传播时的输入替换后的

矩

阵”。之后，留意各个矩阵

的形状求矩阵乘积，使它

们的形状保持合规。如

此

，就可以导出矩阵乘积的

反向传播，如图 1-26 所示。

如图

1-26

所示，通过确认矩阵的形

状，可以推导矩阵乘积的

反向传播

的数学式。这样

一来，我们就推导出了 MatMul 节

点的反向传播。现在我们

将 MatMul 节点实现为层，如下所

示（

common/layers.py）。

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

1.3 神经网络的学习  33

L

x

L

y

WT

L

y

xT L

W

H×D N×H

N×H

D×H D×N

形状

： N×D

形状：

图1-26

通过确认矩阵形

状，推导反向传播的数学

式

形状：

形状：

class MatMul:

def __init__(self, W):

 self.params =

[W]

 self.grads = [np.zeros_like(W)]

self.x = None

 def forward(self,

x):

 W, = self.params

out = np.dot(x, W)

 self.x

= x

 return out

def backward(self, dout):

 W, =

self.params

 dx = np.dot(dout, W.T)

dW = np.dot(self.x.T, dout)

 self.grads[0][...]

= dW

 return dx

MatMul

层在 params 中保存

要学习的参数。另外，以与

其对应的形式，

将梯度保

存在 grads 中。在反向传播时求

dx

和 dw，并在实例变量 grads 中设

置

权重的梯度。

另外，在设置

梯度的值时，像

grads[0][...] = dW 这样，使用

了省略号。

∂

∂

∂

∂

∂

∂

∂

∂

第 1章　神经网络

的复习 34

由此，可以固定

NumPy 数

组的内存地址，覆盖 NumPy 数组

的元素。

和省略号一样，这

里也可以进行基于grads[0] =

dW 的赋

值。不

同的是，在使用省略

号的情况下会覆盖掉NumPy数

组。这是浅复

制（shallow copy）和深复制

（deep copy）的差异。grads[0]

= dW

的赋值相当于浅

复制，grads[0][...] = dW 的覆盖相当于深

复

制。

省略号的话题稍微有

些复杂，我们举个例子来

说明。假设有 a 和 b 两个

NumPy 数组

。

>>> a = np.array([1,

2, 3])

>>> b = np.array([4,

5, 6])

这里，不管是 a = b，还是

a[...] = b，a 都被赋

值 [4,5,6]。但是，此

时

a 指向的内存

地址不同。我们将内存（简

化版）可视化，如图 1-27 所示。

··· ···

··· ··· 1 2 3 4

5 6

··· ··· ··· ···

1 2 3 4 5 6

··· ··· ··· ··· 4 5

6 4 5 6

ႅڲ

a[...]

= b

a = b

a

b

a b

a b

图

1-27

a=b和a[...]=b的区别：使用省略号时

数据被覆盖，变量指向的

内存地址不变

如图 1-27 所示

，在 a =

b 的情况下，a 指向的内存

地址和 b 一样。由于

实际的

数据（4,5,6）没有被复制，所以这

可以说是浅复制。而在

a[...] = b

时

，a 的内存地址保持不变，b 的

元素被复制到

a 指向的内

存上。这时，因

为实际的数

据被复制了，所以称为深

复制。

由此可知，使用省略

号可以固定变量的内存

地址（在上面的例子中，a

1.3 神

经网络的学习

35

的地址是

固定的）。通过固定这个内

存地址，实例变量 grads 的处理

会变

简单。

在

grads 列表中保存

各个参数的梯度。此时，grads 列

表中的各个

元素是 NumPy 数组

，仅在生成层时生成一次

。然后，使用省略号，

在不改

变 NumPy 数组的内存地址的情

况下覆盖数据。这样一来

，

将梯度汇总在一起的工

作就只需要在开始时进

行一次即可。

以上就是 MatMul

层

的实现，代码在 common/layers.py 中。

1.3.5  梯度的

推导和反向传播的实现

计算图的介绍结束了，下

面我们来实现一些实用

的层。这里，我们将实

现 Sigmoid 层

、全连接层 Affine 层和 Softmax

with Loss 层。

1.3.5.1 Sigmoid 层

sigmoid 函数

由 y = 1+exp(

1

−x) 表示，sigmoid 函数的导数由下

式表示。

y

x =

y(1 − y) (1.15)

根据式 (1.15)，Sigmoid

层的计算

图可以绘制成图 1-28。这里，将

输出

侧的层传来的梯度

（ L

y）乘以 sigmoid

函数的导数（ L

x），然后将

这个值传

递给输入侧的

层。

x y

Sigmoid

∂L

∂y

∂L

∂y y(1−y)

图1-28 Sigmoid层的计算图

∂

∂

∂

∂

∂

∂

  第 1章

神

经网络的复习 36

这里，我们

省略 sigmoid 函数的偏导数的推

导过程。相关内容会在

附

录

A 中介绍，感兴趣的读者

可以参考一下。

接下来，我

们使用 Python 来实现 Sigmoid

层。参考图

1-28，可以像下

面这样进行实

现（ common/layers.py）。

class Sigmoid:

def __init__(self):

 self.params, self.grads =

[], []

 self.out = None

def forward(self, x):

 out =

1 / (1 + np.exp(-x))

self.out = out

 return out

def backward(self, dout):

 dx =

dout * (1.0 - self.out) *

self.out

 return dx

这里将正向传播的输

出保存在实例变量 out

中。然

后，在反向传播中，

使用这

个 out 变量进行计算。

1.3.5.2 Affine

层

如前

所示，我们通过 y = np.dot(x, W)

+ b 实现了 Affine 层

的正向传播。

此时，在偏置

的加法中，使用了

NumPy 的广播

功能。如果明示这一点，则

Affine 层的计算图如图 1-29 所示。

如

图

1-29 所示，通过 MatMul 节点进行矩

阵乘积的计算。偏置被 Repeat

节

点复制，然后进行加法运

算（可以认为

NumPy 的广播功能

在内部进行了

Repeat 节点的计

算）。下面是 Affine 层的实现（

common/layers.py）。

1.3 神经

网络的学习  37

图1-29

Affine层的计算

图

W

x

y z

b

b

(D×H)

(N×D)

(N×H) (N×H)

(N×H)

(H)

MatMul

Repeat

+

class Affine:

def __init__(self, W, b):

 self.params

= [W, b]

 self.grads =

[np.zeros_like(W), np.zeros_like(b)]

 self.x = None

def forward(self, x):

 W, b

= self.params

 out = np.dot(x,

W) + b

 self.x =

x

 return out

 def

backward(self, dout):

 W, b =

self.params

 dx = np.dot(dout, W.T)

dW = np.dot(self.x.T, dout)

 db

= np.sum(dout, axis=0)

 self.grads[0][...] =

dW

 self.grads[1][...] = db

return dx

  第 1章

神经网络的复习

38

根据本书的代码规范，Affine 层

将参数保存在实例变量

params 中，将

梯度保存在实例变

量 grads

中。它的反向传播可以

通过执行 MatMul 节点和

Repeat 节点的

反向传播来实现。Repeat 节点的

反向传播可以通过

np.sum()

计算

出来，此时注意矩阵的形

状，就可以清楚地知道应

该对哪个轴（axis）

求和。最后，将

权重参数的梯度设置给

实例变量 grads。以上就是 Affine 层

的

实现。

使用已经实现的 MatMul 层

，可以更轻松地实现 Affine 层。这

里

出于复习的目的，没有

使用 MatMul 层，而是使用 NumPy 的方

法

进行了实现。

1.3.5.3 Softmax with Loss 层

我们将

Softmax 函

数和交叉熵误差一起实

现为 Softmax with Loss 层。

此时，计算图如图

1-30 所示。

a1

a2

a3

t1

t2

t3

y1

y2

y3

y1

− t1

y2 − t2

y3

− t3

Softmax

1

L Cross

Entropy 

Error

图1-30 Softmax with

Loss层的计算图

图 1-30 的

计算图将 Softmax 函数记为

Softmax 层，将

交叉熵误差记为

1.3 神经网

络的学习  39

Cross Entropy Error层。这里假设要

执行3类别分类的任务，从

前一层（靠

近输入的层）接

收 3 个输入。

如图 1-30 所示，Softmax 层对

输入 a1, a2,

a3 进行正规化，输出 y1, 

y2, y3。Cross

Entropy Error 层

接收 Softmax 的输出 y1,

y2, y3 和监督标签

t1, t2, t3，并基于这些数据输出损

失 L。

在图 1-30 中，需要注意的是

反向传播的结果。从 Softmax 层传

来的

反向传播有y1

− t1, y2 − t2, y3

− t3这样一

个很“漂亮”的结果。因为

y1, y2, y3 是

Softmax

层的输出，t1, t2, t3 是监督标签，所

以 y1 −

t1,

y2 − t2, y3 −

t3 是 Softmax 层的输出和监督标

签的差分。神经网络

的反

向传播将这个差分（误差

）传给前面的层。这是神经

网络的学习

中的一个重

要性质。

这里我们省略对

Softmax with Loss 层的实现的说明，具体代

码在

common/layers.py 中。另外，Softmax

with Loss 层的反向传

播的推导过程在

前作《深

度学习入门：基于 Python 的理论

与实现》的附录

A 中有详细

说明，

感兴趣的读者可以

参考一下。

1.3.6  权重的更新

通

过误差反向传播法求出

梯度后，就可以使用该梯

度更新神经网络的参

数

。此时，神经网络的学习按

如下步骤进行。

•　步骤 1：mini-batch

从训

练数据中随机选出多笔

数据。

•　步骤 2：计算梯度

基于

误差反向传播法，计算损

失函数关于各个权重参

数的梯度。

•　步骤

3：更新参数

使用梯度更新权重参数

。

•　步骤 4：重复

根据需要重复

多次步骤 1、步骤

2 和步骤 3。

  第

1章

神经网络的复习 40

我们

按照上面的步骤进行神

经网络的学习。首先，选择

mini-batch 数

据，根据误差反向传播

法获得权重的梯度。这个

梯度指向当前的权重参

数所

处位置中损失增加

最多的方向。因此，通过将

参数向该梯度的反方向

更新，

可以降低损失。这就

是梯度下降法（gradient descent）。之后，根据

需要将

这一操作重复多

次即可。

我们在上面的步

骤 3 中更新权重。权重更新

方法有很多，这里我们来

实现其中最简单的随机

梯度下降法（Stochastic

Gradient Descent，SGD）。

其中，“随机”是

指使用随机选择的数据

（mini-batch）的梯度。

SGD 是一个很简单的

方法。它将（当前的）权重朝

梯度的（反）方向

更新一定

距离。如果用数学式表示

，则有：

W ← W − η

L

W (1.16)

这里将要更新的权

重参数记为 W，损失函数关

于 W 的梯度记为

W

L 。η 表

示学习

率，实际上使用 0.01、0.001

等预先定

好的值。

现在，我们来进行

SGD 的实现。这里考虑到模块

化，将进行参数更

新的类

实现在 common/optimizer.py 中。除了

SGD 之外，这个

文件中还有

AdaGrad 和 Adam 等的实现

。

进行参数更新的类的实

现拥有通用方法 update(params, grads)。这里，

在

参数 params 和

grads 中分别以列表形

式保存了神经网络的权

重和梯度。此

外，假定 params 和 grads

在

相同索引处分别保存了

对应的参数和梯度。这样

一来，SGD 就可以像下面这样

实现（ common/optimizer.py）。

class SGD:

def __init__(self, lr=0.01):

 self.lr =

lr

 def update(self, params, grads):

for i in range(len(params)):

 params[i]

-= self.lr * grads[i]

∂

∂

∂

∂

1.4 使用神经网络解决

问题  41

初始化参数 lr 表示学

习率（learning rate）。这里将学习率保存

为实例

变量。然后，在 update(params,

grads) 方法

中实现参数的更新处理

。

使用这个 SGD 类，神经网络的

参数更新可按如下方式

进行（下面的代码

是不能

实际运行的伪代码）。

model = TwoLayerNet(...)

optimizer = SGD()

for i in range(10000):

 ...

x_batch, t_batch = get_mini_batch(...) # 获取

mini-batch

loss = model.forward(x_batch, t_batch)

 model.backward()

optimizer.update(model.params, model.grads)

 ...

像这样，通过独立实现进

行最优化的类，系统的模

块化会变得更加容易。除

了 SGD

外，本书还实现了 AdaGrad 和 Adam 等

方法，它们的实现都在

common/optimizer.py

中

。这里省略对这些最优化

方法的介绍，详细内容请

参

考前作《深度学习入门

：基于 Python 的理论与实现》的 6.1 节

。

1.4 使用神经网络解决问题

到这里为止，我们的准备

工作就做好了。现在，我们

对一个简单的数据

集进

行神经网络的学习。

1.4.1  螺旋

状数据集

本书在 dataset 目录中

提供了几个便于处理数

据集的类，本节将使用其

中的 dataset/spiral.py 文件。这个文件中实

现了读取螺旋（旋涡）状数

据

的类，其用法如下所示

（

ch01/show_spiral_dataset.py）。

import sys

sys.path.append('..') # 为了引入父目录的文件

而进行的设定

第 1章　神经

网络的复习 42

from

dataset import spiral

import matplotlib.pyplot as

plt

x, t = spiral.load_data()

print('x',

x.shape) # (300, 2)

print('t', t.shape)

# (300, 3)

在上面的例

子中，要从 ch01 目录的

dataset 目录引

入 spiral.py。因此，

上面的代码通过

sys.path.append('..') 将父目录添加到了 import

的检

索路

径中。

然后，使用 spiral.load_data() 进行

数据的读入。此时，x 是输入

数据，

t 是监督标签。观察 x 和

t 的形状，可知它们各自有

300 笔样本数据，其中

x 是二维

数据，t 是三维数据。另外，t 是

one-hot 向量，对应的正确解标签

的类标记为 1，其余的标记

为

0。下面，我们把这些数据

绘制在图上，结果

如图 1-31 所

示。

1.00

0.75

0.50

0.25

0.00

−0.25

−0.50

−0.75

−1.00 −0.75 −0.50 −0.25 0.00 0.25

0.50 0.75 1.00

图1-31　学习用的螺旋状数

据集（用×▲●分别表示3个类）

图

灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

1.4 使用神经网络解决问

题

43

如图 1-31 所示，输入是二维

数据，类别数是 3。观察这个

数据集可知，

它不能被直

线分割。因此，我们需要学

习非线性的分割线。那么

，我们的神

经网络（具有使

用非线性的 sigmoid 激活函数的

隐藏层的神经网络）能否

正

确学习这种非线性模

式呢？让我们实验一下。

因

为这个实验相对简单，所

以我们不把数据集分成

训练数据、验

证数据和测

试数据。不过，实际任务中

会将数据集分为训练数

据

和测试数据（以及验证

数据）来进行学习和评估

。

1.4.2  神经网络的实现

现在，我

们来实现一个具有一个

隐藏层的神经网络。首先

，import 语句

和初始化程序的 __init__() 如

下所示（ ch01/two_layer_net.py）。

import sys

sys.path.append('..')

import numpy as np

from

common.layers import Affine, Sigmoid, SoftmaxWithLoss

class

TwoLayerNet:

 def __init__(self, input_size, hidden_size,

output_size):

 I, H, O =

input_size, hidden_size, output_size

 # 初始化权重和偏

置

W1 = 0.01 * np.random.randn(I, H)

b1 = np.zeros(H)

 W2 =

0.01 * np.random.randn(H, O)

 b2

= np.zeros(O)

 # 生成层

self.layers = [

 Affine(W1, b1),

Sigmoid(),

 Affine(W2, b2)

 ]

self.loss_layer = SoftmaxWithLoss()

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 1章　神经

网络的复习 44

# 将所有的权

重和梯度整理到列表中

self.params, self.grads = [],

[]

 for layer in self.layers:

self.params += layer.params

 self.grads +=

layer.grads

初始化程序接收 3 个参数

。input_size 是输入层的神经元数，hidden_

size

是

隐藏层的神经元数，output_size 是输

出层的神经元数。在内部

实

现中，首先用零向量（np.zeros()）初

始化偏置，再用小的随机

数（0.01 * 

np.random.randn()）初始化权重。通过将权

重设成小的随机数，学习

可以更

容易地进行。接着

，生成必要的层，并将它们

整理到实例变量 layers 列表

中

。最后，将这个模型使用到

的参数和梯度归纳在一

起。

因为 Softmax

with Loss 层和其他层的处

理方式不同，所以不将

它

放入 layers列表中，而是单独存

储在实例变量 loss_layer中。

接着，我

们为 TwoLayerNet 实现 3 个方法，即进行

推理的 predict()

方

法、正向传播的

forward() 方法和反向传播的 backward() 方法

（ ch01/two_

layer_net.py）。

def predict(self, x):

 for

layer in self.layers:

 x =

layer.forward(x)

 return x

def forward(self,

x, t):

 score = self.predict(x)

loss = self.loss_layer.forward(score, t)

 return

loss

def backward(self, dout=1):

 dout

= self.loss_layer.backward(dout)

 for layer in

reversed(self.layers):

 dout = layer.backward(dout)

return dout

1.4 使用神经网络解决问题

45

如上所示，这个实现非常

清楚。因为我们已经将神

经网络中要用的处理

模

块实现为了层，所以这里

只需要以合理的顺序调

用这些层的 forward() 方

法和 backward() 方法

即可。

1.4.3  学习用的代码

下面

，我们来看一下学习用的

代码。首先，读入学习数据

，生成神经网

络（模型）和优

化器。然后，按照之前介绍

的学习的 4

个步骤进行学

习。另

外，在机器学习领域

，通常将针对具体问题设

计的方法（神经网络、SVM

等）称

为模型。学习用的代码如

下所示（ ch01/train_custom_loop.py）。

import sys

sys.path.append('..')

import numpy as np

from

common.optimizer import SGD

from dataset import

spiral

import matplotlib.pyplot as plt

from

two_layer_net import TwoLayerNet

# ❶ 设定超参数

max_epoch = 300

batch_size = 30

hidden_size = 10

learning_rate = 1.0

# ❷ 读入

数据，生成模型和优化器

x, t = spiral.load_data()

model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)

optimizer

= SGD(lr=learning_rate)

# 学习用的变量

data_size =

len(x)

max_iters = data_size // batch_size

total_loss = 0

loss_count = 0

loss_list = []

  第

1章　神经

网络的复习 46

for epoch in

range(max_epoch):

 # ❸ 打乱数据

idx = np.random.permutation(data_size)

 x =

x[idx]

 t = t[idx]

for iters in range(max_iters):

 batch_x

= x[iters*batch_size:(iters+1)*batch_size]

 batch_t = t[iters*batch_size:(iters+1)*batch_size]

# ❹ 计

算梯度，更新参数

 loss =

model.forward(batch_x, batch_t)

 model.backward()

 optimizer.update(model.params,

model.grads)

 total_loss += loss

loss_count += 1

 # ❺

定期输

出学习过程

 if (iters+1) % 10

== 0:

 avg_loss = total_loss

/ loss_count

 print('| epoch %d

| iter %d / %d |

loss %.2f'

 % (epoch +

1, iters + 1, max_iters, avg_loss))

loss_list.append(avg_loss)

 total_loss, loss_count = 0,

0

首先，在代码

❶的地方设定超参数。具体

而言，就是设定学习的 epoch

数

、mini-batch 的大小、隐藏层的神经元

数和学习率。接着，在代码

❷的地

方进行数据的读入

，生成神经网络（模型）和优

化器。我们已经将

2 层神经

网络实现为了 TwoLayerNet 类，将优化

器实现为了 SGD 类，这里直接

使用它

们就可以。

epoch 表示学

习的单位。1 个 epoch 相当于模型

“看过”一遍所有的

学习数

据（遍历数据集）。这里我们

进行 300 个 epoch 的学习。

在进行学

习时，需要随机选择数据

作为

mini-batch。这里，我们以

1.4 使用神

经网络解决问题  47

epoch

为单位

打乱数据，对于打乱后的

数据，按顺序从头开始抽

取数据。数

据的打乱（准确

地说，是数据索引的打乱

）使用 np.random.permutation() 方

法。给定参数 N，该方

法可以返回

0 到 N − 1 的随机序

列，其实际的使用

示例如

下所示。

>>> import numpy as np

>>> np.random.permutation(10)

array([7, 6, 8, 3,

5, 0, 4, 1, 9, 2])

>>> np.random.permutation(10)

array([1, 5, 7, 3,

9, 2, 8, 6, 0, 4])

像这样，调用 np.random.permutation() 可以

随机打乱数据的索引。

接

着，在代码❹的地方计算梯

度，更新参数。最后，在代码

❺的地方定

期地输出学习

结果。这里，每 10

次迭代计算

1 次平均损失，并将其添加

到

变量 loss_list 中。以上就是学习

用的代码。

这里实现的神

经网络的学习用的代码

在本书其他地方也可以

使用。

因此，本书将这部分

代码作为 Trainer类提供出来。使

用 Trainer

类，可以将神经网络的

学习细节嵌入 Trainer类。详细的

用法将

在

1.4.4 节说明。

运行一

下上面的代码（ch01/train_custom_loop.py）就会发现

，向终端

输出的损失的值

在平稳下降。我们将结果

画出来，如图 1-32 所示。

由图 1-32 可

知，随着学习的进行，损失

在减小。我们的神经网络

正在

朝着正确的方向学

习！接下来，我们将学习后

的神经网络的区域划分

（也称

为决策边界）可视化

，结果如图 1-33

所示。

由图 1-33 可知

，学习后的神经网络可以

正确地捕获“旋涡”这个模

式。

也就说，模型正确地学

习了非线性的区域划分

。像这样，神经网络通过隐

藏

层可以实现复杂的表

现力。深度学习的特征之

一就是叠加的层越多，表

现力

越丰富。

  第 1章　神经网

络的复习

48

图1-33　学习后的神

经网络的决策边界（用不

同颜色描绘神经网络识

别的各个类别的区域）

图

1-32 损失的图形：横轴是学习

的迭代次数（刻度值的10倍

），竖轴是每10次迭代的平

均

损失

1.2

1.0

0.6

0.4

0.2

0

50 100 150 200 250 300

0.8

䔚А⁎᪝

 (×10)

ᢌ๞

1.4

使用神经网络解决

问题  49

1.4.4  Trainer类

如前所述，本书中

有很多机会执行神经网

络的学习。为此，就需要编

写

前面那样的学习用的

代码。然而，每次都写相同

的代码太无聊了，因此我

们

将进行学习的类作为

Trainer 类提供出来。Trainer 类的内部实

现和刚才的源

代码几乎

相同，只是添加了一些新

的功能而已，我们在需要

的时候再详细说

明其用

法。

Trainer 类的代码在 common/trainer.py 中。这个类

的初始化程序接收神

经

网络（模型）和优化器，具体

如下所示。

model = TwoLayerNet(...)

optimizer = SGD(lr=1.0)

trainer = Trainer(model, optimizer)

然后，调用 fit()

方法

开始学习。fit() 方法的参数如

表 1-1 所示。

表1-1 Trainer类的fit()方法的参

数。表中的"(=XX)"表示参数的默

认值

参数 说明

x 输入数据

t 监督标签

max_epoch（=

10） 进行学习的 epoch 数

batch_size（= 32） mini-batch

的大小

eval_interval（= 20） 输出结果（平均损

失等）的间隔。

例如设置 eval_interval=20，则

每

20 次迭代计算 1 次平均

损

失，并将结果输出到界面

上

max_grad（=

None） 梯度的最大范数。

当梯

度的范数超过这个值时

，缩小梯度（梯度裁剪，具体

请

参考第 5 章）

另外，Trainer 类有 plot() 方

法，它将 fit() 方法记录的损失

（准确地

说，是按照 eval_interval 评价的

平均损失）在图上画出来

。使用 Trainer 类

进行学习的代码

如下所示（

ch01/train.py）。

  第 1章　神经网络

的复习

50

import sys

sys.path.append('..')

from common.optimizer

import SGD

from common.trainer import Trainer

from dataset import spiral

from two_layer_net

import TwoLayerNet

max_epoch = 300

batch_size

= 30

hidden_size = 10

learning_rate

= 1.0

x, t = spiral.load_data()

model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)

optimizer

= SGD(lr=learning_rate)

trainer = Trainer(model, optimizer)

trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)

trainer.plot()

执行这段代码，会

进行和之前一样的神经

网络的学习。通过将之前

展

示的学习用的代码交

给 Trainer 类负责，代码变简洁了

。本书今后都将使用

Trainer 类进

行学习。

1.5 计算的高速化

神

经网络的学习和推理需

要大量的计算。因此，如何

高速地计算神经网

络是

一个重要课题。本节将简

单介绍一下可以有效加

速神经网络的计算的位

精度和 GPU 的相关内容。

相比

计算的高速化，本书更加

重视实现的易理解性。但

是，从计算

的高速化的角

度出发，之后进行的实现

将考虑数据的位精度。另

外，

在需要花费大量时间

进行计算的地方，会将代

码设计为可在 GPU

上执行。

1.5

计

算的高速化  51

1.5.1  位精度

NumPy 的浮

点数默认使用 64 位的数据

类型。不过，是否为 64 位还依

赖于具体的环境，包括操

作系统、Python

和 NumPy 的版本等。我们

可以

使用下面的代码来

验证是否使用了 64 位浮点

数。

>>> import numpy as np

>>>

a = np.random.randn(3)

>>> a.dtype

dtype('float64')

通过 NumPy 数组的实例变量

dtype，可以查看数据类型。上面

的结果

是 float64，表示 64

位的浮点

数。

NumPy 中默认使用 64 位浮点数

。但是，我们已经知道使用

32 位浮点

数也可以无损地

（识别精度几乎不下降）进

行神经网络的推理和学

习。从

内存的角度来看，因

为 32 位只有 64 位的一半，所以

通常首选

32 位。另

外，在神经

网络的计算中，数据传输

的总线带宽有时会成为

瓶颈。在这种

情况下，毫无

疑问数据类型也是越小

越好。再者，就计算速度而

言，32 位

浮点数也能更高速

地进行计算（浮点数的计

算速度依赖于

CPU 或 GPU 的

架构

）。

因此，本书优先使用

32 位浮

点数。要在 NumPy 中使用 32 位浮点

数，

可以像下面这样将数

据类型指定为 np.float32 或者 'f'。

>>> b

= np.random.randn(3).astype(np.float32)

>>> b.dtype

dtype('float32')

>>>

c = np.random.randn(3).astype('f')

>>> c.dtype

dtype('float32')

另外

，我们已经知道，如果只是

神经网络的推理，则即使

使用 16 位浮

点数进行计算

，精度也基本上不会下降

[6]。不过，虽然 NumPy 中准备有

第 1章

神经网络的复习 52

16 位浮点

数，但是普通

CPU 或 GPU 中的运算

是用 32 位执行的。因此，

即便

变换为 16 位浮点数，因为计

算本身还是用 32 位浮点数

执行的，所以处

理速度方

面并不能获得什么好处

。

但是，如果是要（在外部文

件中）保存学习好的权重

，则 16 位浮点数

是有用的。具

体地说，将权重数据用 16 位

精度保存时，只需要

32 位时

的一

半容量。因此，本书仅

在保存学习好的权重时

，将其变换为 16 位浮点数。

随

着深度学习备受瞩目，最

近的

GPU 已经开始支持 16 位半

精度浮

点数的存储与计

算。另外，谷歌公司设计了

一款名为 TPU

的专用芯

片，可

以支持 8 位计算 [7]。

1.5.2

GPU（CuPy）

深度学习

的计算由大量的乘法累

加运算组成。这些乘法累

加运算的绝大

部分可以

并行计算，这是 GPU 比 CPU

擅长的

地方。因此，一般的深度学

习框架都被设计为既可

以在 CPU 上运行，也可以在 GPU 上

运行。

本书中可以选用

Python 库

CuPy[3]。CuPy 是基于 GPU 进行并行计

算的

库。要使用

CuPy，需要使用安装

有 NVIDIA 的 GPU 的机器，并且需

要安

装

CUDA 这个面向 GPU 的通用并行

计算平台。详细的安装方

法请参

考 CuPy

的官方安装文

档 [4]。

使用 Cupy，可以轻松地使用

NVIDIA 的 GPU

进行并行计算。更重要

的是，CuPy 和 NumPy 拥有共同的 API。下面

我们来看一个简单的使

用

示例。

>>> import cupy as cp

>>>

x = cp.arange(6).reshape(2, 3).astype('f')

>>> x

array([[ 0., 1., 2.],

 [

3., 4., 5.]], dtype=float32)

>>> x.sum(axis=1)

array([ 3., 12.], dtype=float32)

1.5 计算的高速化

53

如

上所示，CuPy 的使用方法与 NumPy 基

本相同。另外，它的内部使

用 GPU

进行计算。这意味着使

用 NumPy 写的代码可以轻松地

改成“GPU

版”，因为我们要做的

（基本上）只是把 numpy 替换为

cupy 而

已。

截至 2018 年 6

月，CuPy 并没有完全

覆盖 NumPy 的方法。虽然

CuPy 和

NumPy 并不

完全兼容，但是它们有许

多共同的 API。

重申一下，本书

为了使代码实现易于理

解，基本上都基于 CPU 进行实

现。

对于计算上要耗费大

量时间的代码，则提供使

用了 CuPy 的实现（可选）。不

过，即

便在使用 CuPy 的情况下，也会

尽量做到不让读者感到

是在使用

CuPy。

本书中可以在

GPU 上运行的代码最先出现

在第 4 章（ch04/train.py）。

这个

ch04/train.py 从以下的 import 语

句开始。

import sys

sys.path.append('..')

import numpy as np

from

common import config

# 在用GPU运行时，请打

开下面的注释（需要cupy）

#

===============================================

# config.GPU = True

#

===============================================

...

用 CPU 执

行上述代码需要花费几

个小时，但是如果使用 GPU，则

只

需要几十分钟。并且，只

要修改上述源代码中的

一行，本书提供的代码就

可

以在 GPU 模式下运行。具体

而言，只需打开注释 # config.GPU

= True，并

用

CuPy 代替 NumPy 即可。如此，代码就可

以在

GPU 上运行，并高速地

进

行学习。请有 GPU 的读者多多

尝试一下。

将

NumPy 切换为 CuPy 的机

制非常简单，感兴趣的读

者请参考

common/config.py、common/np.py 和

common/layers.py 的 import

语句。

图灵

社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

第 1章　神经网络的复习

54

1.6 小

结

本章我们复习了神经

网络的基础。首先回顾了

向量和矩阵等数学知识

，

确认了 Python（特别是

NumPy）的基本用

法。然后，我们观察了神经

网

络的结构。另外，我们还

讨论了几个计算图的基

本部件（加法节点、乘法节

点等），并介绍了它们的正

向传播和反向传播。

接着

，我们进行了神经网络的

实现。考虑到模块化，我们

将神经网络的

基本部件

实现为了层。在层的实现

中，我们制定了本书的代

码规范，即具有

类方法 forward()

和

backward()，以及实例变量 params 和 grads。这使得

神经

网络的实现更加清

晰。

最后，我们对人造的螺

旋状数据集使用具有一

个隐藏层的神经网络进

行了学习，并确认了模型

学习的正确性。到这里为

止，神经网络的复习就结

束了。现在，让我们手握神

经网络这一可靠的武器

，迈入自然语言处理的世

界。出发吧！

1.6 小结  55

本章所学

的内容

•

神经网络具有输

入层、隐藏层和输出层

•　通

过全连接层进行线性变

换，通过激活函数进行非

线性变换

•　全连接层和 mini-batch

处

理都可以写成矩阵计算

•　使用误差反向传播法可

以高效地求解神经网络

的损失的梯度

• 使用计算

图能够将神经网络中发

生的处理可视化，这有助

于理解正

向传播和反向

传播

•

在神经网络的实现

中，通过将组件模块化为

层，可以简化实现

•　数据的

位精度和 GPU 并行计算对神

经网络的高速化非常重

要

第2章

自然语言和单词

的分布式表示

Marty: “This is heavy（棘手）.”

Dr.

Brown: “In the future, things are

so heavy（重）?”

—电

影《回到未来》

接下来，我们

将踏入自然语言处理的

世界。自然语言处理涉及

多个子

领域，但是它们的

根本任务都是让计算机

理解我们的语言。何谓让

计算机理

解我们的语言

？存在哪些方法？本章我们

将以这些问题为中心展

开讨论。为

此，我们将先详

细考察古典方法，即深度

学习出现以前的方法。从

下一章开

始，再介绍基于

深度学习（确切地说，是神

经网络）的方法。

本章我们

还会练习使用 Python 处理文本

，实现分词（将文本分割成

单

词）和单词

ID 化（将单词转

换为单词 ID）等任务。本章实

现的函数在后

面的章节

中也会用到。因此，本章也

可以说是后续文本处理

的准备工作。那

么，让我们

一起进入自然语言处理

的世界吧！

2.1

什么是自然语

言处理

我们平常使用的

语言，如日语或英语，称为

自然语言（natural language）。

所谓自然语言

处理（Natural Language Processing，NLP），顾名思义，就

是处理

自然语言的科学。简单地

说，它是一种能够让计算

机理解人类语言的

技术

。换言之，自然语言处理的

目标就是让计算机理解

人说的话，进而完成

对我

们有帮助的事情。

  第

2章　自

然语言和单词的分布式

表示 58

另外，说到计算机可

以理解的语言，我们可能

会想到编程语言或者标

记

语言等。这些语言的语

法定义可以唯一性地解

释代码含义，计算机也能

根据

确定的规则分析代

码。

众所周知，编程语言是

一种机械的、缺乏活力的

语言。换句话说，它是

一种

“硬语言”。而英语或日语等

自然语言是“软语言”。这里

的“软”是指

意思和形式会

灵活变化，比如含义相同

的文章可以有不同的表

述，或者文章

存在歧义，等

等。另外，自然语言的“软”还

体现在，新的词语或新的

含义

会随着时代的发展

不断出现。

自然语言是活

着的语言，具有“柔软性”。因

此，要让“头脑僵硬”的

计算

机去理解自然语言，使用

常规方法是无法办到的

。但是，如果我们能办

到，就

能让计算机去完成一些

对人们有用的事情。事实

上，我们可以看到很

多这

样的应用。搜索引擎和机

器翻译就是两个比较好

理解的例子，除此之

外，还

有问答系统、假名汉字转

化（IME）、自动文本摘要和情感

分析等，

在我们身边已经

使用了很多自然语言处

理技术。

问答系统是自然

语言处理技术的一个应

用。作为代表，国际商业机

器公司（IBM）的

Watson 系统非常有名

。2011 年，美国的一档答题

闯关

节目《危险边缘》（Jeopardy！）让Watson 名声大

噪。在这个节

目中，Watson

的答题

比任何人都准确，战胜了

往届的冠军（不过，

给 Watson 的问

题是用文本给出的）。这一

“事件”引起了世人的广

泛

关注，大概也是从这个时

候开始，人们对人工智能

的期待和不安

都开始增

加。此外，IBM

将 Watson 称为决策支援

系统。最近，有

媒体报道，Watson 利

用过往的大量医疗数据

，针对疑难症提供了

正确

的治疗建议，挽救了患者

生命。

单词含义

我们的语

言是由文字构成的，而语

言的含义是由单词构成

的。换句话

说，单词是含义

的最小单位。因此，为了让

计算机理解自然语言，让

它理解

单词含义可以说

是最重要的事情了。

2.2 同义

词词典

59

本章的主题是让

计算机理解单词含义。确

切地说，我们将探讨一些

巧

妙地蕴含了单词含义

的表示方法。具体来说，本

章和下一章将讨论以下

3 种

方法。

•

基于同义词词典

的方法 本章

•　基于计数的

方法 本章

•

基于推理的方

法（word2vec） 下一章

首先，我们将简

单介绍一下使用人工整

理好的同义词词典的方

法。然

后，对利用统计信息

表示单词的方法（这里称

为“基于计数的方法”）进行

说

明。这些都是本章学习

的内容。在下一章，我们将

讨论利用神经网络的基

于

推理的方法（具体来说

，就是

word2vec 方法）。本章的结构参

考了斯坦福大

学课程“CS224d: Deep Learning for

Natural Language Processing”[10]。

2.2 同

义词词典

要表示单词含

义，首先可以考虑通过人

工方式来定义单词含义

。一种方

法是像《新华字典

》那样，一个词一个词地说

明单词含义。比如，当你用

字

典查“汽车”这个单词时

，就会看到“装有车轮并依

靠它们前行的交通工具

或运输工具……”这样的说明

。通过像这样定义单词，计

算机或许也能够理

解单

词含义。

回顾自然语言处

理的历史，人们已经尝试

过很多次类似这样的人

工定

义单词含义的活动

。但是，目前被广泛使用的

并不是《新华字典》那样的

常规词典，而是一种被称

为同义词词典（thesaurus）的词典。在

同义词词

典中，具有相同

含义的单词（同义词）或含

义类似的单词（近义词）被

归

类到同一个组中。比如

，使用同义词词典，我们可

以知道 car 的同义词有

automobile、motorcar 等（图

2-1）。

第 2章　自然语言和单词的

分布式表示 60

motorcar car

auto automobile machine

图2-1　同义词的

例子：car、auto和automobile等都是表示“汽车

”的同义词

另外，在自然语

言处理中用到的同义词

词典有时会定义单词之

间的粒度

更细的关系，比

如“上位 - 下位”关系、“整体 - 部

分”关系。举个例子，

如图

2-2 所

示，我们利用图结构定义

了各个单词之间的关系

。

hatch-back

object

motor vehicle

car go-kart truck

SUV compact

图2-2

根据各单词的含义，基

于上位-下位关系形成的

图（参考文献[14]）

在图 2-2 中，单词

motor vehicle（机动车）是单词 car

的上位概

念。

car 的下位概念有 SUV、compact 和 hatch-back

等更

加具体的车种。

像这样，通

过对所有单词创建近义

词集合，并用图表示各个

单词的关

系，可以定义单

词之间的联系。利用这个

“单词网络”，可以教会计算

机单

词之间的相关性。也

就是说，我们可以将单词

含义（间接地）教给计算机

，

然后利用这一知识，就能

让计算机做一些对我们

有用的事情。

如何使用同

义词词典根据自然语言

处理的具体应用的不同

而不同。

比如，在信息检索

场景中，如果事先知道 automobile 和

car 是近义词，

就可以将 automobile

的检

索结果添加到 car 的检索结

果中。

2.2 同义词词典

61

2.2.1  WordNet

在自然

语言处理领域，最著名的

同义词词典是 WordNet[17]。WordNet

是普林斯

顿大学于 1985 年开始开发的

同义词词典，迄今已用于

许多研究，

并活跃于各种

自然语言处理应用中。

使

用 WordNet，可以获得单词的近义

词，或者利用单词网络。使

用单

词网络，可以计算单

词之间的相似度。这里，我

们不对 WordNet 进行详细

说明，对

WordNet 的 Python

实现感兴趣的读者，可

以参考附录 B。在附

录 B 中，我

们会安装 WordNet（准确地说，是安

装

NLTK 模块），并进行

一些简单

的实验。

在附录 B 中，我们将

实际使用

WordNet 来计算单词之

间的相似度。

具体来说，就

是基于一个人工定义的

单词网络，来计算单词之

间的

相似度。如果能（在一

定程度上正确）计算单词

之间的相似度，那么

我们

就踏出了理解单词含义

的第一步。

2.2.2

同义词词典的

问题

WordNet 等同义词词典中对

大量单词定义了同义词

和层级结构关系等。

利用

这些知识，可以（间接地）让

计算机理解单词含义。不

过，人工标记也

存在一些

较大的缺陷。下面，我们就

来看一下同义词词典的

主要问题，并分

别对其进

行简要说明。

难以顺应时

代变化

我们使用的语言

是活的。随着时间的推移

，新词不断出现，而那些

落

满尘埃的旧词不知哪天

就会被遗忘。比如，“众筹”（crowdfunding）

就

是一个最近才开始使用

的新词。

另外，语言的含义

也会随着时间的推移而

变化。比如，英语中的

heavy

一词

，现在有“事态严重”的含义

（主要用作俚语），但以前是

第 2章　自然语言和单词的

分布式表示 62

没有这种用

法的。在电影《回到未来》中

，有这样一个场景：从 1985

年穿

越回来的马蒂和生活在

1955 年的博士的对话中，对 heavy 的

含义

有不同的理解。如果

要处理这样的单词变化

，就需要人工不停地更新

同

义词词典。

人力成本高

制作词典需要巨大的人

力成本。以英文为例，据说

现有的英文单

词总数超

过 1000 万个。在极端情况下，还

需要对如此大规模的单

词进

行单词之间的关联

。顺便提一下，WordNet 中收录了超

过 20 万个的

单词。

无法表示

单词的微妙差异

同义词

词典中将含义相近的单

词作为近义词分到一组

。但实际上，

即使是含义相

近的单词，也有细微的差

别。比如，vintage 和 retro 虽

然表示相同

的含义，但是用法不同，而

这种细微的差别在同义

词词典中

是无法表示出

来的（让人来解释是相当

困难的）。

因此，使用同义词

词典，即人工定义单词含

义的方法存在很多问题

。为

了避免这些问题，接下

来我们将介绍基于计数

的方法和利用神经网络

的基于

推理的方法。这两

种方法可以从海量的文

本数据中自动提取单词

含义，将我

们从人工关联

单词的辛苦劳动中解放

出来。

不仅限于自然语言

处理，在图像识别领域，多

年来也一直是人工设

计

特征量。但是，随着深度学

习的出现，现在从原始图

像直接获得

最终结果已

成为可能，人为介入的必

要性大幅降低。在自然语

言处

理领域也有类似现

象。也就是说，我们正在从

人工制作词典或设计

特

征量的旧范式，向尽量减

少人为干预的、仅从文本

数据中获取最

终结果的

新范式转移。

2.3

基于计数的

方法  63

2.3 基于计数的方法

从

介绍基于计数的方法开

始，我们将使用语料库（corpus）。简

而言之，

语料库就是大量

的文本数据。不过，语料库

并不是胡乱收集数据，一

般收集

的都是用于自然

语言处理研究和应用的

文本数据。

说到底，语料库

只是一些文本数据而已

。不过，其中的文章都是由

人写

出来的。换句话说，语

料库中包含了大量的关

于自然语言的实践知识

，即文

章的写作方法、单词

的选择方法和单词含义

等。基于计数的方法的目

标就是

从这些富有实践

知识的语料库中，自动且

高效地提取本质。

自然语

言处理领域中使用的语

料库有时会给文本数据

添加额外的

信息。比如，可

以给文本数据的各个单

词标记词性。在这种情况

下，为了方便计算机处理

，语料库通常会被结构化

（比如，采用

树结构等数据

形式）。这里，假定我们使用

的语料库没有添加标签

，

而是作为一个大的文本

文件，只包含简单的文本

数据。

2.3.1

基于 Python的语料库的预

处理

自然语言处理领域

存在各种各样的语料库

。说到有名的语料库，有

Wikipedia 和

Google News

等。另外，莎士比亚、夏目漱

石等伟大作家的作

品集

也会被用作语料库。本章

我们先使用仅包含一个

句子的简单文本作为语

料库，然后再处理更实用

的语料库。

现在，我们使用

Python 的交互模式，对一个非常

小的文本数据（语料

库）进

行预处理。这里所说的预

处理是指，将文本分割为

单词（分词），并

将分割后的

单词列表转化为单词

ID 列

表。

下面，我们一边确认一

边实现。首先来看一下作

为语料库的样本文章。

>>> text =

'You say goodbye and I say

hello.'

这

里我们使用由单个句子

构成的文本作为语料库

。本来文本（text）应

  第 2章

自然语

言和单词的分布式表示

64

该包含成千上万个（连续

的）句子，但是，考虑到简洁

性，这里先对这个小

的文

本数据进行预处理。下面

，我们对上面的 text 进行分词

。

>>>

text = text.lower()

>>> text =

text.replace('.', ' .')

>>> text

'you

say goodbye and i say hello

.'

>>> words = text.split(' ')

>>> words

['you', 'say', 'goodbye', 'and',

'i', 'say', 'hello', '.']

首先，使用 lower()

方法将所有字

母转化为小写，这样可以

将句子开头

的单词也作

为常规单词处理。然后，将

空格作为分隔符，通过 split(' ') 切

分句子。考虑到句子结尾

处的句号（.），我们先在句号

前插入一个空格（即

用“

.”替

换“.”），再进行分词。

这里，在进

行分词时，我们采用了一

种在句号前插入空格的

“临 时 对 策”，其 实

还 有 更 加

聪 明、更 加

通 用 的 实 现 方

式，比

如 使 用 正 则 表

达 式

。通 过 导 入 正

则 表 达 式 的

re 模

块，使 用

re.split('(\W+)?', text)也可以进行分

词。关于正则表达式的

详

细信息，可以参考文献 [15]。

现

在，我们已经可以将原始

文章作为单词列表使用

了。虽然分词后文

本更容

易处理了，但是直接以文

本的形式操作单词，总感

觉有些不方便。因

此，我们

进一步给单词标上 ID，以便

使用单词 ID 列表。为此，我们

使用

Python 的字典来创建单词

ID 和单词的对应表。

>>> word_to_id =

{}

>>> id_to_word = {}

>>>

>>> for word in words:

...

if word not in word_to_id:

...

new_id = len(word_to_id)

... word_to_id[word] =

new_id

... id_to_word[new_id] = word

2.3

基于计

数的方法  65

变量 id_to_word 负责将单

词

ID 转化为单词（键是单词

ID，值是单词），

word_to_id 负责将单词转

化为单词 ID。这里，我们从头

开始逐一观察分词后

的

words

的各个元素，如果单词不

在 word_to_id 中，则分别向 word_to_id 和

id_to_word

添加新

ID 和单词。另外，我们将字典

的长度设为新的单词 ID，

单

词 ID 按

0, 1, 2, ··· 逐渐增加。

这样一来

，我们就创建好了单词

ID 和

单词的对应表。下面，我们

来实

际看一下它们的内

容。

>>> id_to_word

{0:

'you', 1: 'say', 2: 'goodbye', 3:

'and', 4: 'i', 5: 'hello', 6:'.'}

>>> word_to_id

{'you': 0, 'say': 1,

'goodbye': 2, 'and': 3, 'i': 4,

'hello': 5, '.': 6}

使用这些词典，可以根

据单词检索单词 ID，或者反

过来根据单词

ID 检

索单词

。我们实际尝试一下，如下

所示。

>>> id_to_word[1]

'say'

>>> word_to_id['hello']

5

最后，我们将单词列

表转化为单词 ID 列表。这里

，我们使用

Python

的列表解析式

将单词列表转化为单词

ID 列表，然后再将其转化为

NumPy

数组。

>>> import

numpy as np

>>> corpus =

[word_to_id[w] for w in words]

>>>

corpus = np.array(corpus)

>>> corpus

array([0,

1, 2, 3, 4, 1, 5,

6])

列表解析式（list comprehension）或字典

解析式（dict comprehension）

是一种便于对列

表或字典进行循环处理

的写法。比如，要创建元素

为列表 xs

= [1,2,3,4]中各个元素的平

方的新列表，可以写成 [x**2 

for x

in xs]。

  第

2章　自然语言和单词的分

布式表示

66

至此，我们就完

成了利用语料库的准备

工作。现在，我们将上述一

系列

处理实现为 preprocess() 函数（ common/util.py）。

def preprocess(text):

 text = text.lower()

text = text.replace('.', ' .')

words = text.split(' ')

 word_to_id

= {}

 id_to_word = {}

for word in words:

 if

word not in word_to_id:

 new_id

= len(word_to_id)

 word_to_id[word] = new_id

id_to_word[new_id] = word

 corpus =

np.array([word_to_id[w] for w in words])

return corpus, word_to_id, id_to_word

使

用这个函数，可以按如下

方式对语料库进行预处

理。

>>>

text = 'You say goodbye and

I say hello.'

>>> corpus, word_to_id,

id_to_word = preprocess(text)

到这里，语料库的预处

理就结束了。这里准备的

corpus、word_to_id 和

id_to_word

这 3 个变量名在本书接

下来的很多地方都会用

到。corpus 是单词

ID 列表，word_to_id

是单词到

单词 ID 的字典，id_to_word 是单词 ID 到单

词

的字典。

现在，我们已经

做好了操作语料库的准

备，接下来的目标就是使

用语料

库提取单词含义

。为此，本节我们将考察基

于计数的方法。采用这种

方法，

我们能够将单词表

示为向量。

2.3.2

单词的分布式

表示

世界上存在各种各

样的颜色，有的颜色被赋

予了固定的名字，比如钴

蓝

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

2.3 基于计数的方法

67

（cobalt blue）或者锌红（zinc red）；颜色也可以通

过

RGB（Red/Green/

Blue）三原色分别存在多少

来表示。前者为不同的颜

色赋予不同的名字，有

多

少种颜色，就需要有多少

个不同的名字；后者则将

颜色表示为三维向量。

需

要注意的是，使用 RGB 这样的

向量表示可以更准确地

指定颜色，并

且这种基于

三原色的表示方式很紧

凑，也更容易让人想象到

具体是什么颜

色。比如，即

便不知道“深绯”是什么样

的颜色，但如果知道它的

(R, G, 

B) =

(201, 23, 30)，就至少可以知道它是红

色系的颜色。此外，颜色之

间的

关联性（是否是相似

的颜色）也更容易通过向

量表示来判断和量化。

那

么，能不能将类似于颜色

的向量表示方法运用到

单词上呢？更准确地

说，可

否在单词领域构建紧凑

合理的向量表示呢？接下

来，我们将关注能准

确把

握单词含义的向量表示

。在自然语言处理领域，这

称为分布式表示。

单词的

分布式表示将单词表示

为固定长度的向量。这种

向量的特

征在于它是用

密集向量表示的。密集向

量的意思是，向量的各个

元 素（大 多 数）是

由 非 0 实数

表示的。例如，三维分布式

表示是

[0.21,-0.45,0.83]。如何构建这样的

单词的分布式表示是我

们接下

来的一个重要课

题。

2.3.3  分布式假设

在自然语

言处理的历史中，用向量

表示单词的研究有很多

。如果仔

细看一下这些研

究，就会发现几乎所有的

重要方法都基于一个简

单的想

法，这个想法就是

“某个单词的含义由它周

围的单词形成”，称为分布

式假

设（distributional hypothesis）。许多用向量表示

单词的近期研究也基于

该

假设。

分布式假设所表

达的理念非常简单。单词

本身没有含义，单词含义

由它

所在的上下文（语境

）形成。的确，含义相同的单

词经常出现在相同的语

境

中。比如“I

drink beer.”“We drink wine.”，drink 的附近常有饮

料出现。

另外，从“I

guzzle beer.”“We guzzle wine.”可知，guzzle 和 drink

所

在

的语境相似。进而我们

可以推测出，guzzle 和 drink 是近义词

（顺便说一下，

图灵社区会

员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

  第

2章　自

然语言和单词的分布式

表示 68

guzzle 是“大口喝”的意思）。

从

现在开始，我们会经常使

用“上下文”一词。本章说的

上下文是指某

个单词（关

注词）周围的单词。在图 2-3 的

例子中，左侧和右侧的 2 个

单词

就是上下文。

you say and goodbye i say

hello.

图2-3　窗口

大小为2的上下文例子。在

关注goodbye时，将其左右各2个单

词用作上下文

如图 2-3 所示

，上下文是指某个居中单

词的周围词汇。这里，我们

将上

下文的大小（即周围

的单词有多少个）称为窗

口大小（window size）。窗口

大小为 1，上下

文包含左右各 1 个单词；窗

口大小为

2，上下文包含左

右各

2 个单词，以此类推。

这

里，我们将左右两边相同

数量的单词作为上下文

。但是，根据

具体情况，也可

以仅将左边的单词或者

右边的单词作为上下文

。

此外，也可以使用考虑了

句子分隔符的上下文。简

单起见，本书

仅处理不考

虑句子分隔符、左右单词

数量相同的上下文。

2.3.4  共现

矩阵

下面，我们来考虑如

何基于分布式假设使用

向量表示单词，最直截了

当

的实现方法是对周围

单词的数量进行计数。具

体来说，在关注某个单词

的情况

下，对它的周围出

现了多少次什么单词进

行计数，然后再汇总。这里

，我们将

这种做法称为“基

于计数的方法”，在有的文

献中也称为“基于统计的

方法”。

现在，我们就来看一

下基于计数的方法。这里

我们使用 2.3.1 节的语料

库和

preprocess()

函数，再次进行预处理。

import sys

sys.path.append('..')

import numpy

as np

2.3 基

于计数的方法  69

from common.util import preprocess

text =

'You say goodbye and I say

hello.'

corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)

# [0 1 2 3

4 1 5 6]

print(id_to_word)

#

{0: 'you', 1: 'say', 2: 'goodbye',

3: 'and', 4: 'i', 5: 'hello',

6:

 '.'}

从上面的

结果可以看出，词汇总数

为 7 个。下面，我们计算每个

单词的

上下文所包含的

单词的频数。在这个例子

中，我们将窗口大小设为

1，从单

词 ID 为 0 的

you 开始。

从图 2-4 可

以清楚地看到，单词 you

的上

下文仅有 say 这个单词。用表

格表示的话，如图 2-5 所示。

you

goodbye say and i say hello

.

图

2-4　单词you的上下文

hello you

you

10 00000

goodbye say and i

.

图2-5　用表格

表示单词you的上下文中包

含的单词的频数

图 2-5 表示

的是作为单词

you 的上下文

共现的单词的频数。同时

，这也

意味着可以用向量

[0, 1, 0, 0,

0, 0, 0] 表示单词 you。

接着对单词

ID 为

1 的 say 进行同样的处理，结果

如图 2-6

所示。

  第 2章　自然语言

和单词的分布式表示

70

hello you goodbye say

say

and i .

111 1 0

0 0

you goodbye say and

i say . hello

图

2-6　用表格表示单词say的上下

文中包含的单词的频数

从上面的结果可知，单词

say

可以表示为向量 [1, 0, 1, 0, 1,

1, 0]。

对所有

的 7 个单词进行上述操作

，会得到如图 2-7

所示的结果

。

hello

hello

you

you

goodbye

goodbye

say

say

and

and

i

i

.

.

0 0 1

0 000

0 0 1 0

0 0 1

0 0 0

0 0 1 0

0 1

1 0 0 0 0

0

0 0 1 1 0 0

0 1 1 0 0 0

0

1 1 0 0 1

1 0

图2-7　用表格汇总各个单词

的上下文中包含的单词

的频数

图 2-7

是汇总了所有

单词的共现单词的表格

。这个表格的各行对应相

应

单词的向量。因为图 2-7 的

表格呈矩阵状，所以称为

共现矩阵（co-occurence 

matrix）。

接下来，我们来

实际创建一下上面的共

现矩阵。这里，将图 2-7 的结果

按原样手动输入。

C = np.array([

2.3 基于计

数的方法  71

 [0,

1, 0, 0, 0, 0, 0],

[1, 0, 1, 0, 1, 1,

0],

 [0, 1, 0, 1,

0, 0, 0],

 [0, 0,

1, 0, 1, 0, 0],

[0, 1, 0, 1, 0, 0,

0],

 [0, 1, 0, 0,

0, 0, 1],

 [0, 0,

0, 0, 0, 1, 0],

],

dtype=np.int32)

这就是共现矩

阵。使用这个共现矩阵，可

以获得各个单词的向量

，如下所示。

print(C[0]) # 单词ID为0的向量

# [0

1 0 0 0 0 0]

print(C[4]) # 单词ID为4的向量

# [0 1

0 1 0 0 0]

print(C[word_to_id['goodbye']])

# goodbye的向量

# [0 1 0

1 0 0 0]

至

此，我们通过共现矩阵成

功地用向量表示了单词

。上面我们是手动输

入共

现矩阵的，但这一操作显

然可以自动化。下面，我们

来实现一个能直接从

语

料库生成共现矩阵的函

数。我们把这个函数称为

create_co_matrix(corpus, 

vocab_size, window_size=1)，其中参数 corpus 是单词

ID 列表，参

数 vocab_

size 是词汇个数，window_size 是窗口大

小（

common/util.py）。

def create_co_matrix(corpus, vocab_size, window_size=1):

corpus_size = len(corpus)

 co_matrix =

np.zeros((vocab_size, vocab_size), dtype=np.int32)

 for idx,

word_id in enumerate(corpus):

 for i

in range(1, window_size + 1):

left_idx = idx - i

right_idx = idx + i

if left_idx >= 0:

 left_word_id

= corpus[left_idx]

 co_matrix[word_id, left_word_id] +=

1

 if right_idx < corpus_size:

第 2章　自然语言和单词

的分布式表示 72

right_word_id = corpus[right_idx]

 co_matrix[word_id, right_word_id]

+= 1

 return co_matrix

首先，用元

素为

0 的二维数组对 co_matrix 进行

初始化。然后，针对语

料库

中的每一个单词，计算它

的窗口中包含的单词。同

时，检查窗口内的单

词是

否超出了语料库的左端

和右端。

这样一来，无论语

料库多大，都可以自动生

成共现矩阵。之后，我们都

将使用这个函数生成共

现矩阵。

2.3.5  向量间的相似度

前面我们通过共现矩阵

将单词表示为了向量。下

面，我们看一下如何测

量

向量间的相似度。

测量向

量间的相似度有很多方

法，其中具有代表性的方

法有向量内

积或欧式距

离等。虽然除此之外还有

很多方法，但是在测量单

词的向量

表示的相似度

方面，余弦相似度（cosine similarity）是很常

用的。设有

x = (x1,

x2, x3, ··· , xn) 和

y = (y1, y2, y3, ···

, yn) 两个向量，它

们之间的余

弦相似度的

定义如下式所示。

similarity(x, y)

= x · y

x  y

= x1y1 + ··· + xnyn

x2

1 + ··· +

x2

n

 y1

2

+ ··· + yn

2 (2.1)

在式 (2.1) 中

，分子是向量内积，分母是

各个向量的范数。范数表

示向

量的大小，这里计算

的是 L2 范数（即向量各个元

素的平方和的平方根）。

式

(2.1) 的要点是先对向量进行

正规化，再求它们的内积

。

余弦相似度直观地表示

了“两个向量在多大程度

上指向同一方向”。

两个向

量完全指向相同的方向

时，余弦相似度为 1；完全指

向相反

的方向时，余弦相

似度为

−1。

2.3 基于计数的方法

73

现在，我们来实现余弦相

似度。基于式 (2.1)，代码如下所

示。

def cos_similarity(x, y):

 nx =

x / np.sqrt(np.sum(x**2)) # x的正规化

ny = y / np.sqrt(np.sum(y**2)) #

y的正规化

 return np.dot(nx, ny)

这

里，我们假定参数

x 和 y 是 NumPy 数

组。首先对向量进行正规

化，

然后求两个向量的内

积。这里余弦相似度的实

现虽然完成了，但是还有

一个

问题。那就是当零向

量（元素全部为 0 的向量）被

赋值给参数时，会出现

“除

数为 0”（zero

division）的错误。

解决此类问

题的一个常用方法是，在

执行除法时加上一个微

小值。这

里，通过参数指定

一个微小值 eps（eps 是 epsilon

的缩写），并

默认 eps=1e-8

（= 0.000 000 01）。这样修改后的余弦

相似度的实现如下所示

（

common/

util.py）。

def cos_similarity(x, y, eps=1e-8):

nx = x / (np.sqrt(np.sum(x **

2)) + eps)

 ny =

y / (np.sqrt(np.sum(y ** 2)) +

eps)

 return np.dot(nx, ny)

这里我们用了

1e-8作为微小

值，在这么小的值的情况

下，根据浮点

数的舍入误

差，这个微小值会被其他

值“吸收”掉。在上面的实现

中，

因为这个微小值会被

向量的范数“吸收”掉，所以

在绝大多数情况下，

加上

eps不会对最终的计算结果

造成影响。而当向量的范

数为 0 时，

这个微小值可以

防止“除数为 0”的错误。

利用

这个函数，可以如下求得

单词向量间的相似度。这

里，我们尝试求

you 和 i（=

I）的相似

度（ ch02/similarity.py）。

import sys

sys.path.append('..')

from

common.util import preprocess, create_co_matrix, cos_similarity

第 2章　自然语言和单词

的分布式表示 74

text =

'You say goodbye and I say

hello.'

corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)

C = create_co_matrix(corpus,

vocab_size)

c0 = C[word_to_id['you']] # you的单词向

量

c1 = C[word_to_id['i']] # i的单词向量

print(cos_similarity(c0,

c1))

# 0.7071067691154799

从上面的

结果可知，you 和 i

的余弦相似

度是 0.70 ...。由于余弦相似度

的

取值范围是 −1 到

1，所以可以

说这个值是相对比较高

的（存在相似性）。

2.3.6  相似单词

的排序

余弦相似度已经

实现好了，使用这个函数

，我们可以实现另一个便

利的

函数：当某个单词被

作为查询词时，将与这个

查询词相似的单词按降

序显示出

来。这里将这个

函数称为 most_similar()，通过下列参数

进行实现（表 2-1）。

most_similar(query, word_to_id, id_to_word,

word_matrix, top=5)

表2-1 most_similar()函数的参

数

参数名 说明

query 查询词

word_to_id 单

词到单词 ID 的字典

id_to_word 单词 ID 到

单词的字典

word_matrix 汇总了单词

向量的矩阵，假定保存了

与各行对应的单词向量

top

显示到前几位

这里我们

直接给出 most_similar() 函数的实现，如

下所示（ common/

util.py）。

def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):

# ❶ 取出查询词

 if query

not in word_to_id:

 print('%s is

not found' % query)

 return

2.3 基于

计数的方法  75

 print('\n[query]

' + query)

 query_id =

word_to_id[query]

 query_vec = word_matrix[query_id]

# ❷ 计算余弦相

似度

 vocab_size =

len(id_to_word)

 similarity = np.zeros(vocab_size)

for i in range(vocab_size):

 similarity[i]

= cos_similarity(word_matrix[i], query_vec)

 # ❸

基于余弦相似度，按

降序输出值

 count = 0

for i in (-1 * similarity).argsort():

if id_to_word[i] == query:

 continue

print(' %s: %s' % (id_to_word[i], similarity[i]))

count += 1

 if count

>= top:

 return

上述实现按

如下顺序执行。

❶

取出查询

词的单词向量。

❷ 分别求得

查询词的单词向量和其

他所有单词向量的余弦

相似度。

❸ 基于余弦相似度

的结果，按降序显示它们

的值。

我们仅对步骤❸进行

补充说明。在步骤❸中，将

similarity 数

组中的元

素索引按降序

重新排列，并输出顶部的

单词。这里使用 argsort() 方法对数

组的索引进行了重排。这

个 argsort()

方法可以按升序对 NumPy 数

组的元

素进行排序（不过

，返回值是数组的索引）。下

面是一个例子。

>>> x

= np.array([100, -20, 2])

>>> x.argsort()

array([1, 2, 0])

上述代码

对 NumPy 数组

[100, −20, 2] 的各个元素按升

序进行了排列。

图灵社区

会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 2章

自然语言和单词的分布

式表示

76

此时，返回的数组

的各个元素对应原数组

的索引。上述结果的顺序

是“第 1

个元素（−20）”“第 2 个元素（2）”“第

0

个元素（100）”。现在我们想做的

是将单词的相似度按降

序排列，因此，将 NumPy 数组的各

个元素乘以 −1

后，再使用 argsort()

方

法。接着上面的例子，有如

下代码。

>>> (-x).argsort()

array([0, 2, 1])

使用这个 argsort()，可以按

降序输出单词相似度。以

上就是 most_

similar() 函数的实现，下面

我们来试着使用一下。这

里将 you

作为查询词，

显示与

其相似的单词，代码如下

所示（ ch02/most_similar.py）。

import sys

sys.path.append('..')

from common.util import preprocess, create_co_matrix, most_similar

text = 'You say goodbye and

I say hello.'

corpus, word_to_id, id_to_word

= preprocess(text)

vocab_size = len(word_to_id)

C

= create_co_matrix(corpus, vocab_size)

most_similar('you', word_to_id, id_to_word,

C, top=5)

执行代码后，会得到

如下结果。

[query] you

goodbye: 0.7071067691154799

 i: 0.7071067691154799

hello: 0.7071067691154799

 say: 0.0

and: 0.0

这个结果只按

降序显示了 you 这个查询词

的前 5

个相似单词，各个单

词旁边的值是余弦相似

度。观察上面的结果可知

，和 you 最接近的单词有 3

个，分

别是 goodbye、i（=

I）和 hello。因为 i 和 you 都是人称

代词，所以

二者相似可以

理解。但是，goodbye 和 hello 的余弦相似

度也很高，这和我

图灵社

区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

2.4 基

于计数的方法的改进  77

们

的感觉存在很大的差异

。一个可能的原因是，这里

的语料库太小了。后面

我

们会用更大的语料库进

行相同的实验。

如上所述

，我们通过共现矩阵成功

地将单词表示为了向量

。至此，基

于计数的方法的

基本内容就介绍完了。之

所以说“基本”，是因为还有

许多

事情需要讨论。下一

节，我们将说明当前方法

的改进思路，并实现这个

改进

思路。

2.4 基于计数的方

法的改进

上一节我们创

建了单词的共现矩阵，并

使用它成功地将单词表

示为了向

量。但是，这个共

现矩阵还有许多可以改

进的地方。本节我们将对

其进行改

进，并使用更实

用的语料库，获得单词的

“真实的”分布式表示。

2.4.1

点互

信息

上一节的共现矩阵

的元素表示两个单词同

时出现的次数。但是，这种

“原始”的次数并不具备好

的性质。如果我们看一下

高频词汇（出现次数很

多

的单词），就能明白其原因

了。

比如，我们来考虑某个

语料库中 the 和

car 共现的情况

。在这种情况下，

我们会看

到很多“...the car...”这样的短语。因此

，它们的共现次数将会很

大。另外，car 和 drive

也明显有很强

的相关性。但是，如果只看

单词的出

现次数，那么与

drive 相比，the 和 car 的相关性更强。这

意味着，仅仅因

为 the 是个常

用词，它就被认为与 car 有很

强的相关性。

为了解决这

一问题，可以使用点互信

息（Pointwise

Mutual Information，

PMI）这一指标。对于随机变

量 x 和 y，它们的

PMI 定义如下（关

于概率，

将在 3.5.1 节详细说明

）：

PMI(x,

y) = log2

P(x, y)

P(x)P(y)

(2.2)

  第 2章　自然语言和单词的

分布式表示

78

其中，P(x) 表示 x 发

生的概率，P(y) 表示

y 发生的概

率，P(x, y) 表示 x

和

y 同时发生的概

率。PMI 的值越高，表明相关性

越强。

在自然语言的例子

中，P(x) 就是指单词 x

在语料库

中出现的概率。假设

某个

语料库中有 10 000 个单词，其中

单词 the

出现了 100 次，则 P("the") = 10

000

100 = 0.01

P("the") =

10 000

100 = 0.01。另外，P(x, y)

表

示单词 x 和 y 同时出现的概

率。假设 the

和

car 一起出现了 10 次

，则P("the", "car")

= 10 000

10 = 0.001。

现在，我们使用共现矩

阵（其元素表示单词共现

的次数）来重写式(2.2)。

这里，将

共现矩阵表示为 C，将单词

x 和 y 的共现次数表示为

C(x, y)，将

单词 x 和 y 的出现次数分别

表示为

C(x)、C(y)，将语料库的单词

数量记为

N，则式 (2.2) 可以重写

为：

PMI(x, y)

= log2

P(x, y)

P(x)P(y) =

log2

C(x,y)

C(x)

N

N

C(y)

N

= log2

C

C

(

(

x, y

x)C

)

(

·

y

N

) (2.3)

根据式

(2.3)，可以由共现矩

阵求 PMI。下面我们来具体地

算一下。这

里假设语料库

的单词数量（N）为 10 000，the 出现

100 次，car 出

现 20 次，

drive

出现 10 次，the 和 car 共现

10 次，car 和

drive 共现 5 次。这时，如

果从共现

次数的角度来看，则与 drive 相

比，the 和 car 的相关性更强。而

如

果从 PMI 的角度来看，结果是

怎样的呢？我们来计算一

下。

PMI("the", "car") =

log2

10 · 10 000

1000

· 20 ≈ 2.32 (2.4)

PMI("car",

"drive") = log2

5 · 10

000

20 · 10 ≈ 7.97

(2.5)

结果表明，在使用 PMI 的情

况下，与 the 相比，drive

和 car 具有更强

的相关性。这是我们想要

的结果。之所以出现这个

结果，是因为我们考虑了

单词单独出现的次数。在

这个例子中，因为 the 本身出

现得多，所以 PMI

的得分被拉

低了。式中的“≈”（near equal）表示近似相

等的意思。

虽然我们已经

获得了PMI这样一个好的指

标，但是PMI也有一个问题。

2.4 基

于计数的方法的改进

79

那

就是当两个单词的共现

次数为 0 时，log20 = −∞。为了解决这个

问题，

实践上我们会使用

下述正的点互信息（Positive PMI，PPMI）。

PPMI(x, y) = max

(0, PMI(x, y)) (2.6)

根据

式 (2.6)，当

PMI 是负数时，将其视为

0，这样就可以将单词间

的

相关性表示为大于等于

0 的实数。下面，我们来实现

将共现矩阵转化为

PPMI 矩阵

的函数。我们把这个函数

称为

ppmi(C, verbose=False, eps=1e-8)

（ common/util.py）。

def

ppmi(C, verbose=False, eps=1e-8):

 M =

np.zeros_like(C, dtype=np.float32)

 N = np.sum(C)

S = np.sum(C, axis=0)

 total

= C.shape[0] * C.shape[1]

 cnt

= 0

 for i in

range(C.shape[0]):

 for j in range(C.shape[1]):

pmi = np.log2(C[i, j] * N

/ (S[j]*S[i]) + eps)

 M[i,

j] = max(0, pmi)

 if

verbose:

 cnt += 1

if cnt % (total//100+1) == 0:

print('%.1f%% done' % (100*cnt/total))

 return

M

这里，参数 C 表示共现

矩阵，verbose 是决定是否输出运

行情况的标志。

当处理大

语料库时，设置

verbose=True，可以用于

确认运行情况。在这段代

码

中，为了仅从共现矩阵

求 PPMI 矩阵而进行了简单的

实现。具体来说，当单

词 x

和

y 的共现次数为 C(x, y) 时，C(x) =

iC(i, x)，C(y) = iC(i, y)，

N

= i

jC(i, j)，进行这

样近似并实现。另外，在上

述代码中，为了防

止 np.log2(0)=-inf

而使

用了微小值 eps。

  第 2章

自然语

言和单词的分布式表示

80

在 2.3.5 节中，为了防止“除数为

0”的错误，我们给分母添加

了一

个微小值。这里也一

样，通过将 np.log(x)改为

np.log(x + eps)，

可以防止

对数运算发散到负无穷

大。

现在将共现矩阵转化

为 PPMI

矩阵，可以像下面这样

进行实现（ ch02/

ppmi.py）。

import sys

sys.path.append('..')

import numpy as np

from common.util

import preprocess, create_co_matrix, cos_similarity,

 ppmi

text = 'You say goodbye and

I say hello.'

corpus, word_to_id, id_to_word

= preprocess(text)

vocab_size = len(word_to_id)

C

= create_co_matrix(corpus, vocab_size)

W = ppmi(C)

np.set_printoptions(precision=3) # 有效位数为3位

print('covariance matrix')

print(C)

print('-'*50)

print('PPMI')

print(W)

运行该文件，可以得到下

述结果。

covariance matrix

[[0

1 0 0 0 0 0]

[1 0 1 0 1 1

0]

 [0 1 0 1

0 0 0]

 [0 0

1 0 1 0 0]

[0 1 0 1 0 0

0]

 [0 1 0 0

0 0 1]

 [0 0

0 0 0 1 0]]

--------------------------------------------------

2.4 基于计数的方法

的改进  81

PPMI

[[

0. 1.807 0. 0. 0. 0.

0. ]

 [ 1.807 0.

0.807 0. 0.807 0.807 0. ]

[ 0. 0.807 0. 1.807 0.

0. 0. ]

 [ 0.

0. 1.807 0. 1.807 0. 0.

]

 [ 0. 0.807 0.

1.807 0. 0. 0. ]

[ 0. 0.807 0. 0. 0.

0. 2.807]

 [ 0. 0.

0. 0. 0. 2.807 0. ]]

这样一来，我们就

将共现矩阵转化为了 PPMI 矩

阵。此时，PPMI 矩

阵的各个元素

均为大于等于 0

的实数。我

们得到了一个由更好的

指标形成的

矩阵，这相当

于获取了一个更好的单

词向量。

但是，这个 PPMI 矩阵还

是存在一个很大的问题

，那就是随着语料库

的词

汇量增加，各个单词向量

的维数也会增加。如果语

料库的词汇量达到

10 万，则

单词向量的维数也同样

会达到 10 万。实际上，处理 10 万

维向量

是不现实的。

另外

，如果我们看一下这个矩

阵，就会发现其中很多元

素都是 0。这表

明向量中的

绝大多数元素并不重要

，也就是说，每个元素拥有

的“重要性”

很低。另外，这样

的向量也容易受到噪声

影响，稳健性差。对于这些

问题，

一个常见的方法是

向量降维。

2.4.2  降维

所谓降维

（dimensionality reduction），顾名思义，就是减少向量

维度。

但是，并不是简单地

减少，而是在尽量保留“重

要信息”的基础上减少。如

图

2-8 所示，我们要观察数据

的分布，并发现重要的“轴

”。

  第 2章

自然语言和单词的

分布式表示 82

图2-8　降维示意

图：发现重要的轴（数据分

布广的轴），将二维数据表

示为一维数据

在图 2-8

中，考

虑到数据的广度，导入了

一根新轴，以将原来用二

维坐

标表示的点表示在

一个坐标轴上。此时，用新

轴上的投影值来表示各

个数据

点的值。这里非常

重要的一点是，选择新轴

时要考虑数据的广度。如

此，仅

使用一维的值也能

捕获数据的本质差异。在

多维数据中，也可以进行

同样的

处理。

向量中的大

多数元素为

0 的矩阵（或向

量）称为稀疏矩阵（或稀疏

向

量）。这里的重点是，从稀

疏向量中找出重要的轴

，用更少的维度对

其进行

重新表示。结果，稀疏矩阵

就会被转化为大多数元

素均不为

0 的密集矩阵。这

个密集矩阵就是我们想

要的单词的分布式表示

。

降 维 的 方 法 有

很 多， 这 里

我 们 使

用 奇异值分解（Singular Value 

Decomposition，SVD）。SVD 将

任意矩阵分解为

3 个矩阵

的乘积，如下式

所示：

X = USV

T (2.7)

如式

(2.7) 所示，SVD 将任意的矩阵 X

分解

为 U、S、V 这 3 个矩阵的

乘积，其中

U

和 V 是列向量彼此正交的

正交矩阵，S 是除了对角线

元素以

外其余元素均为

0 的对角矩阵。图

2-9 中直观地

表示出了这些矩阵。

2.4 基于

计数的方法的改进  83

图2-9　基

于SVD的矩阵变换（白色部分

表示元素为0）

X U S V

T =

在式 (2.7) 中，U 是正

交矩阵。这个正交矩阵构

成了一些空间中的基轴

（基向量），我们可以将矩阵

U

作为“单词空间”。S 是对角矩

阵，奇异值在

对角线上降

序排列。简单地说，我们可

以将奇异值视为“对应的

基轴”的重

要性。这样一来

，如图 2-10 所示，减少非重要元

素就成为可能。

图2-10　基于SVD的

降维示意图

X U

S

V

T

䃺ࢂ

ID ࢂ䃺 ID

䃺ा䛼ࢂ

䭺㐡ऻ⮱

䃺ा䛼ࢂ

如图 2-10 所示，矩

阵 S

的奇异值小，对应的基

轴的重要性低，因此，

可以

通过去除矩阵 U 中的多余

的列向量来近似原始矩

阵。用我们正在处理

的“单

词的 PPMI

矩阵”来说明的话，矩

阵 X 的各行包含对应的单

词 ID

的单词向量，这些单词

向量使用降维后的矩阵

U 

表示。

单词的共现矩阵是

正方形矩阵，但在图 2-10 中，为

了和之前的

图一致，画的

是长方形。另外，这里对 SVD

的

介绍仅限于最直

观的概

要性的说明。想从数学角

度仔细理解的读者，请参

考文献

[20] 等。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第 2章　自然

语言和单词的分布式表

示

84

2.4.3  基于 SVD的降维

接下来，我

们使用

Python 来实现 SVD，这里可以

使用 NumPy 的

linalg

模块中的 svd 方法。linalg 是

linear algebra（线性代数）的简称。下

面，我

们创建一个共现矩阵，将

其转化为

PPMI 矩阵，然后对其

进行 SVD

（ ch02/count_method_small.py）。

import

sys

sys.path.append('..')

import numpy as np

import matplotlib.pyplot as plt

from common.util

import preprocess, create_co_matrix, ppmi

text =

'You say goodbye and I say

hello.'

corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(id_to_word)

C = create_co_matrix(corpus,

vocab_size, window_size=1)

W = ppmi(C)

#

SVD

U, S, V = np.linalg.svd(W)

SVD 执行完毕。上面的变

量 U 包含经过 SVD 转化的密集

向量表示。现

在，我们来看

一下它的内容。单词 ID 为 0 的

单词向量如下。

print(C[0])

# 共现矩阵

# [0 1 0 0

0 0 0]

print(W[0]) # PPMI矩阵

# [ 0. 1.807 0. 0.

0. 0. 0. ]

print(U[0]) #

SVD

# [ 3.409e-01 -1.110e-16 -1.205e-01

-4.441e-16 0.000e+00 -9.323e-01

# 2.226e-16]

如上所示，原先的稀

疏向量

W[0] 经过 SVD 被转化成了

密集向量 U[0]。

2.4

基于计数的方

法的改进  85

如果要对这个

密集向量降维，比如把它

降维到二维向量，取出前

两个元素

即可。

print(U[0,

:2])

# [ 3.409e-01 -1.110e-16]

这样我们

就完成了降维。现在，我们

用二维向量表示各个单

词，并把它

们画在图上，代

码如下。

for word, word_id in word_to_id.items():

plt.annotate(word, (U[word_id, 0], U[word_id, 1]))

plt.scatter(U[:,0],

U[:,1], alpha=0.5)

plt.show()

plt.annotate(word, x, y)

函数在 2D 图形中坐

标为 (x, y) 的地方绘制单

词的

文本。执行上述代码，结果

如图 2-11 所示 A 。

0.0

−0.1

−0.2

−0.3

−0.4

−0.5

−0.6

0.6 0.0

and

you hello igoodbye

say

.

0.1 0.2 0.3 0.4

0.5 0.7

图2-11　对共现矩阵

执行SVD，并在图上绘制各个

单词的二维向量（i和goodbye重叠

）

A 根据操作系统的种类或

Matplotlib版本的不同，输出的图可

能和图2-11所有不同。

第 2章　自

然语言和单词的分布式

表示 86

观察该图可以发现

，goodbye

和 hello、you 和 i 位置接近，这是比较

符

合我们的直觉的。但是

，因为我们使用的语料库

很小，有些结果就比较微

妙。下面，我们将使用更大

的

PTB 数据集进行相同的实

验。首先，我们简

单介绍一

下 PTB 数据集。

如果矩阵大小

是

N，SVD 的计算的复杂度将达

到 O(N3)。这意味

着SVD需要与N的立

方成比例的计算量。因为

现实中这样的计算

量是

做不到的，所以往往会使

用Truncated SVD[21]等更快的方法。

Truncated SVD 通过截

去（truncated）奇异值较小的部分，从

而实现高速化。下一节，作

为另一个选择，我们将使

用 sklearn

库的 Truncated

SVD。

2.4.4  PTB数据集

到目前为

止，我们使用了非常小的

文本数据作为语料库。这

里，我们将

使用一个大小

合适的“真正的”语料库——Penn

Treebank 语

料库（以下

简称为 PTB）。

PTB 语料库

经常被用作评价提案方

法的基准。本书中我们将

使用

PTB 语料库进行各种实

验。

我们使用的 PTB 语料库在

word2vec 的发明者托马斯·米科洛

夫

（Tomas Mikolov）的网页上有提供。这个

PTB 语料库是以文本文件的

形式

提供的，与原始的 PTB 的

文章相比，多了若干预处

理，包括将稀有单词替

换

成特殊字符 <unk>（unk 是 unknown 的简称），将

具体的数字替换成“N”

等。下

面，我们将经过这些预处

理之后的文本数据作为

PTB

语料库使用。

作为参考，图

2-12 给出了 PTB 语料库的部分内

容。

如图

2-12 所示，在 PTB 语料库中

，一行保存一个句子。在本

书中，我

们将所有句子连

接起来，并将其视为一个

大的时序数据。此时，在每

个句子

的结尾处插入一

个特殊字符

<eos>（eos 是 end of sentence 的简称）。

图

灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

2.4 基于计数的方法的改

进

87

图2-12 PTB语料库（文本文件）的

例子

本书不考虑句子的

分割，将多个句子连接起

来得到的内容视为一

个

大的时序数据。当然，也可

以以句子为单位进行处

理，比如，

以句子为单位计

算词频。不过，考虑到简单

性，本书不进行以句

子为

单位的处理。

在本书中，为

了方便使用 Penn Treebank 数据集，我们

准备了专门

的

Python 代码。这个

文件在 dataset/ptb.py 中，并假定从章节

目录（ch01、

ch02、...）使用。比如，我们将当

前目录移到 ch02

目录，并在这

个目录中调用

python show_ptb.py。使用 ptb.py 的例

子如下所示（ ch02/show_ptb.py）。

import sys

sys.path.append('..')

from dataset import

ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')

print('corpus size:', len(corpus))

print('corpus[:30]:', corpus[:30])

print()

print('id_to_word[0]:', id_to_word[0])

print('id_to_word[1]:', id_to_word[1])

print('id_to_word[2]:', id_to_word[2])

print()

print("word_to_id['car']:", word_to_id['car'])

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第 2章　自

然语言和单词的分布式

表示

88

print("word_to_id['happy']:", word_to_id['happy'])

print("word_to_id['lexus']:", word_to_id['lexus'])

后面再具体解释这

段代码，我们先来看一下

它的执行结果。

corpus size: 929589

corpus[:30]: [ 0

1 2 3 4 5 6

7 8 9 10 11 12

13 14 15 16 17 18

19 20 21 22 23

24 25 26 27 28 29]

id_to_word[0]: aer

id_to_word[1]: banknote

id_to_word[2]: berlitz

word_to_id['car']: 3856

word_to_id['happy']: 4428

word_to_id['lexus']: 7426

语料库的

用法和之前一样。corpus 中保存

了单词 ID 列表，id_to_word 是

将单词

ID 转

化为单词的字典，word_to_id 是将单

词转化为单词 ID 的字典。

如

上面的代码所示，使用

ptb.load_data() 加

载数据。此时，指定参

数 'train'、'test' 和

'valid' 中的一个，它们分别对应

训练用数据、测试用

数据

和验证用数据中的一个

。以上就是 ptb.py 文件的使用方

法。

2.4.5  基于

PTB数据集的评价

下

面，我们将基于计数的方

法应用于 PTB 数据集。这里建

议使用更快

速的 SVD

对大矩

阵执行 SVD，为此我们需要安

装 sklearn 模块。当然，虽

然仍可以

使用基本版的 SVD（np.linalg.svd()），但是这需

要更多的时间和

内存。我

们把源代码一并给出，如

下所示（ ch02/count_method_big.py）。

import sys

sys.path.append('..')

import

numpy as np

from common.util import

most_similar, create_co_matrix, ppmi

from dataset import

ptb

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

2.4

基于计数的

方法的改进  89

window_size = 2

wordvec_size = 100

corpus, word_to_id, id_to_word

= ptb.load_data('train')

vocab_size = len(word_to_id)

print('counting

co-occurrence ...')

C = create_co_matrix(corpus, vocab_size,

window_size)

print('calculating PPMI ...')

W =

ppmi(C, verbose=True)

print('calculating SVD ...')

try:

# truncated SVD (fast!)

 from

sklearn.utils.extmath import randomized_svd

 U, S,

V = randomized_svd(W, n_components=wordvec_size, n_iter=5,

random_state=None)

except ImportError:

 # SVD

(slow)

 U, S, V =

np.linalg.svd(W)

word_vecs = U[:, :wordvec_size]

querys

= ['you', 'year', 'car', 'toyota']

for

query in querys:

 most_similar(query, word_to_id,

id_to_word, word_vecs, top=5)

这里，为了执

行 SVD，我们使用了 sklearn

的 randomized_svd() 方法。

该

方法通过使用了随机数

的 Truncated SVD，仅对奇异值较大的部

分进行

计算，计算速度比

常规的 SVD 快。剩余的代码和

之前使用小语料库时的

代

码差不太多。执行代码

，可以得以下结果（因为使

用了随机数，所以在使用

Truncated SVD 的情况下，每次的结果都

不一样）。

[query] you

 i: 0.702039909619

we: 0.699448543998

 've: 0.554828709147

do: 0.534370693098

 else: 0.512044146526

第 2章　自然语言和

单词的分布式表示 90

[query] year

month: 0.731561990308

 quarter: 0.658233992457

last: 0.622425716735

 earlier: 0.607752074689

next: 0.601592506413

[query] car

 luxury:

0.620933665528

 auto: 0.615559874277

 cars:

0.569818364381

 vehicle: 0.498166879744

 corsica:

0.472616831915

[query] toyota

 motor: 0.738666107068

nissan: 0.677577542584

 motors: 0.647163210589

honda: 0.628862370943

 lexus: 0.604740429865

观察

结果可知，首先，对于查询

词

you，可以看到 i、we 等人称代词

排在前面，这些都是在语

法上具有相同用法的词

。再者，查询词 year 有

month、quarter

等近义词

，查询词 car 有 auto、vehicle 等近义词。此外

，

将

toyota 作为查询词时，出现了

nissan、honda 和 lexus 等汽车制造商名或

者

品牌名。像这样，在含义或

语法上相似的单词表示

为相近的向量，这符合

我

们的直觉。

我们终于成功

地将单词含义编码成了

向量，真是可喜可贺！使用

语料

库，计算上下文中的

单词数量，将它们转化 PPMI 矩

阵，再基于 SVD

降维

获得好的

单词向量。这就是单词的

分布式表示，每个单词表

示为固定长度的

密集向

量。

在本章的实验中，我们

只看了一部分单词的近

义词，但是可以确认许多

其他的单词也有这样的

性质。期待使用更大的语

料库可以获得更好的单

词的

分布式表示！

2.5

小结  91

2.5 小

结

本章，我们以自然语言

为对象，特别是以让计算

机理解单词含义为主题

展开了讨论。为了达到这

一目标，我们介绍了基于

同义词词典的方法，也考

察了基于计数的方法。

使

用基于同义词词典的方

法，需要人工逐个定义单

词之间的相关性。这

样的

工作非常费力，在表现力

上也存在限制（比如，不能

表示细微的差别）。

而基于

计数的方法从语料库中

自动提取单词含义，并将

其表示为向量。具体

来说

，首先创建单词的共现矩

阵，将其转化为 PPMI 矩阵，再基

于

SVD 降

维以提高稳健性，最

后获得每个单词的分布

式表示。另外，我们已经确

认

过，这样的分布式表示

具有在含义或语法上相

似的单词在向量空间上

位置相

近的性质。

为了方

便处理语料库的文本数

据，我们实现了几个预处

理函数。具体来

说，包括测

量向量间相似度的函数

（cos_similarity()）、用于显示相似单词

的排

名的函数（most_similar()）。这些函数在后

面的章节中还会用到。

  第

2章　自然语言和单词的分

布式表示

92

本章所学的内

容

• 使用 WordNet 等同义词词典，可

以获取近义词或测量单

词间的相似

度等

• 使用同

义词词典的方法存在创

建词库需要大量人力、新

词难更新等

问题

•　目前，使

用语料库对单词进行向

量化是主流方法

• 近年来

的单词向量化方法大多

基于“单词含义由其周围

的单词构成”

这一分布式

假设

• 在基于计数的方法

中，对语料库中的每个单

词周围的单词的出现频

数进行计数并汇总（= 共现

矩阵）

• 通过将共现矩阵转

化为 PPMI 矩阵并降维，可以将

大的稀疏向量转

变为小

的密集向量

•

在单词的向

量空间中，含义上接近的

单词距离上理应也更近

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重

版权

第3章

word2vec

“没有判断依据

，就不要去推理。”

——阿瑟·柯南

·道尔 《波希米亚丑闻》（收录

于《冒险史》）

接着上一章，本

章的主题仍是单词的分

布式表示。在上一章中，我

们使

用基于计数的方法

得到了单词的分布式表

示。本章我们将讨论该方

法的替代

方法，即基于推

理的方法。

顾名思义，基于

推理的方法使用了推理

机制。当然，这里的推理机

制用

的是神经网络。本章

，著名的 word2vec 将会登场。我们将

花很多时间考察

word2vec 的结构

，并通过代码实现来加深

对它的理解。

本章的目标

是实现一个简单的 word2vec。这个

简单的 word2vec 会优

先考虑易理

解性，从而牺牲一定的处

理效率。因此，我们不会用

它来处理大

规模数据集

，但用它处理小数据集毫

无问题。下一章我们会对

这个简单的

word2vec 进行改进，从

而完成一个“真正的”word2vec。现在

，让我们一

起进入基于推

理的方法和 word2vec 的世界吧！

3.1

基

于推理的方法和神经网

络

用向量表示单词的研

究最近正在如火如荼地

展开，其中比较成功的方

法大致可以分为两种：一

种是基于计数的方法；另

一种是基于推理的方法

。

  第 3章

word2vec 94

虽然两者在获得单

词含义的方法上差别很

大，但是两者的背景都是

分布式

假设。

本节我们将

指出基于计数的方法的

问题，并从宏观角度说明

它的替代方

法——基于推理

的方法的优点。另外，为了

做好

word2vec 的准备工作，我

们会

看一个用神经网络处理

单词的例子。

3.1.1  基于计数的

方法的问题

如上一章所

说，基于计数的方法根据

一个单词周围的单词的

出现频数

来表示该单词

。具体来说，先生成所有单

词的共现矩阵，再对这个

矩阵进行

SVD，以获得密集向

量（单词的分布式表示）。但

是，基于计数的方法在处

理大规模语料库时会出

现问题。

在现实世界中，语

料库处理的单词数量非

常大。比如，据说英文的词

汇

量超过 100

万个。如果词汇

量超过 100 万个，那么使用基

于计数的方法就需

要生

成一个 100 万

× 100 万的庞大矩阵

，但对如此庞大的矩阵执

行 SVD 显

然是不现实的。

对于

一个 n × n 的矩阵，SVD 的复杂度是

O(n3)，这表示计算量与

n 的立方

成比例增长。如此大的计

算成本，即便是超级计算

机也无

法胜任。实际上，利

用近似方法和稀疏矩阵

的性质，可以在一定程

度

上提高处理速度，但还是

需要大量的计算资源和

时间。

基于计数的方法使

用整个语料库的统计数

据（共现矩阵和 PPMI

等），

通过一

次处理（SVD 等）获得单词的分

布式表示。而基于推理的

方法使用

神经网络，通常

在 mini-batch 数据上进行学习。这意

味着神经网络一次只

需

要看一部分学习数据（mini-batch），并

反复更新权重。这种学习

机制上

的差异如图 3-1 所示

。

3.1 基于推理的方法和神经

网络

95

SVD

ധλ䃎᪝⮱᫦∂喍CBUDI႓΍喎

ധλᣕ⤳⮱᫦∂喍NJOJCBUDI႓΍喎 ⺋㏼㑾㐉

႓΍᪝ᢛ

႓΍᪝ᢛ

图3-1　基于计

数的方法和基于推理的

方法的比较

如图 3-1 所示，基

于计数的方法一次性处

理全部学习数据；反之，基

于

推理的方法使用部分

学习数据逐步学习。这意

味着，在词汇量很大的语

料库

中，即使 SVD 等的计算量

太大导致计算机难以处

理，神经网络也可以在部

分数据上学习。并且，神经

网络的学习可以使用多

台机器、多个 GPU 并行

执行，从

而加速整个学习过程。在

这方面，基于推理的方法

更有优势。

基于推理的方

法和基于计数的方法相

比，还有一些其他的优点

。关于这

一点，在详细说明

基于推理的方法（特别是

word2vec）之后，我们会在3.5.3

节再次讨

论。

3.1.2

基于推理的方法的概

要

基于推理的方法的主

要操作是“推理”。如图 3-2 所示

，当给出周围的

单词（上下

文）时，预测“？”处会出现什么

单词，这就是推理。

you

? goodbye and i say hello.

图3-2　基于

两边的单词（上下文），预测

“？”处出现什么单词

解开图

3-2 中的推理问题并学习规

律，就是基于推理的方法

的主要任

务。通过反复求

解这些推理问题，可以学

习到单词的出现模式。从

“模型视

角”出发，这个推理

问题如图

3-3 所示。

  第 3章

word2vec 96

you

goodbye

ắ⢴ܳጰ

̷̸᪴

὎ಸ

图3-3　基

于推理的方法：输入上下

文，模型输出各个单词的

出现概率

如图 3-3 所示，基于

推理的方法引入了某种

模型，我们将神经网络用

于

此模型。这个模型接收

上下文信息作为输入，并

输出（可能出现的）各个单

词的出现概率。在这样的

框架中，使用语料库来学

习模型，使之能做出正确

的预测。另外，作为模型学

习的产物，我们得到了单

词的分布式表示。这就

是

基于推理的方法的全貌

。

基于推理的方法和基于

计数的方法一样，也基于

分布式假设。分布

式假设

假设“单词含义由其周围

的单词构成”。基于推理的

方法将这

一假设归结为

了上面的预测问题。由此

可见，不管是哪种方法，如

何对基于分布式假设的

“单词共现”建模都是最重

要的研究主题。

3.1.3

神经网络

中单词的处理方法

从现

在开始，我们将使用神经

网络来处理单词。但是，神

经网络无法直

接处理 you 或

say 这样的单词，要用神经网

络处理单词，需要先将单

词转化

为固定长度的向

量。对此，一种方式是将单

词转换为 one-hot 表示（one-hot

向量）。在 one-hot 表

示中，只有一个元素是

1，其

他元素都是 0。

我们来看一

个 one-hot 表示的例子。和上一章

一样，我们用“You say

goodbye and I say hello.”这个一句话

的语料库来说明。在这个

语料库中，

一共有

7 个单词

（“you”“say”“goodbye”“and”“i”“hello”“.”）。此时，

各个单词可以转化

为图 3-4 所示的 one-hot

表示。

图灵社

区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权 you

say .goodbye and

i

hello

3.1

基

于推理的方法和神经网

络  97

图3-4　单词、单词ID以及它们

的one-hot表示

you

0 (1, 0, 0, 0, 0,

0, 0)

(0, 0, 1, 0,

0, 0, 0) 2 goodbye

POFIPU㶕⹧

䃺ࢂ

%*䃺ࢂ

如图 3-4 所示，单词可

以表示为文本、单词 ID

和 one-hot 表

示。此时，

要将单词转化为

one-hot 表示，就需要准备元素个

数与词汇个数相等的向

量，并将单词 ID

对应的元素

设为 1，其他元素设为 0。像这

样，只要将单

词转化为固

定长度的向量，神经网络

的输入层的神经元个数

就可以固定下来

（图 3-5）。

you

you

(1, 0, 0, 0,

0, 0, 0) (0, 1, 0,

0, 0, 0, 0) (0, 0,

1, 0, 0, 0, 0)

hello

goodbye ...

...

...

goodbye

say

say

and

i

.

䓀ڒᅯ

图3-5

输

入层的神经元：各个神经

元对应于各个单词。图中

神经元为1的地方用黑色

绘制，

为0的地方用白色绘

制

如图 3-5 所示，输入层由 7

个

神经元表示，分别对应于

7 个单词（第 1

个神经元对应

于 you，第 2

个神经元对应于 say）。

现

在事情变得很简单了。因

为只要将单词表示为向

量，这些向量就可以

  第

3章

word2vec 98

由构成神经网络的各种

“层”来处理。比如，对于one-hot表示

的某个单词，

使用全连接

层对其进行变换的情况

如图 3-6 所示。

图3-6 基于神经网

络的全连接层的变换：输

入层的各个神经元分别

对应于7个单词（中间

层的

神经元暂为3个）

you

hello

goodbye

say

and

i

.

䓀ڒᅯ

͚䬡ᅯ

如图 3-6 所示

，全连接层通过箭头连接

所有节点。这些箭头拥有

权重

（参数），它们和输入层

神经元的加权和成为中

间层的神经元。另外，本章

使

用的全连接层将省略

偏置（这是为了配合后文

对 word2vec

的说明）。

没有偏置的全

连接层相当于在计算矩

阵乘积。在很多深度学习

的

框架中，在生成全连接

层时，都可以选择不使用

偏置。在本书中，

不使用偏

置的全连接层相当于 MatMul 层

（该层已经在第

1 章中

实现

）。

在图 3-6 中，神经元之间的连

接是用箭头表示的。之后

，为了明确地显

示权重，我

们将使用图 3-7 所示的方法

。

3.1 基于推理的方法和神经

网络

99

you

hello

goodbye

(7×3)

say

and

i

.

䓀ڒᅯ ͚䬡ᅯ

W

图3-7 基于全连接层的

变换的简化图示：将全连

接层的权重表示为一个

7 × 3形状的W

矩阵

现在，我们看

一下代码。这里的全连接

层变换可以写成如下的

Python

代码。

import numpy as np

c

= np.array([[1, 0, 0, 0, 0,

0, 0]]) # 输入

W =

np.random.randn(7, 3) # 权重

h =

np.dot(c, W) # 中间节点

print(h)

# [[-0.70012195

0.25204755 -0.79774592]]

这段代码将单词 ID 为 0

的单

词表示为了 one-hot 表示，并用全

连接层

对其进行了变换

。作为复习，全连接层的计

算通过矩阵乘积进行。这

可以用

NumPy 的

np.dot() 来实现（省略偏

置）。

这里，输入数据（变量c）的

维数（ndim）是2。这是考虑了mini-batch

处理

，将各个数据保存在了第

1 维（0 维度）中。

希望读者注意

一下 c 和 W 进行矩阵乘积计

算的地方。此处，c 是

one￾hot 表示，单

词 ID 对应的元素是 1，其他地

方都是 0。因此，如图

3-8 所示，

  第

3章　word2vec

100

上述代码中的 c 和 W 的矩

阵乘积相当于“提取”权重

的对应行向量。

图3-8 在上下

文c和权重W的矩阵乘积中

，对应位置的行向量被提

取（权重的各个元素

的大

小用灰度表示）

c h W

1000000

这里，仅为

了提取权重的行向量而

进行矩阵乘积计算好像

不是很有效

率。关于这一

点，我们将在 4.1 节进行改进

。另外，上述代码的功能也

可以

使用第

1 章中实现的

MatMul 层完成，如下所示。

import sys

sys.path.append('..')

import numpy as np

from common.layers

import MatMul

c = np.array([[1, 0,

0, 0, 0, 0, 0]])

W

= np.random.randn(7, 3)

layer = MatMul(W)

h = layer.forward(c)

print(h)

# [[-0.70012195

0.25204755 -0.79774592]]

这里，我

们先导入了 common 目录下的 MatMul

层

。之后，将 MatMul

层的权重设为了

W，并使用 forward() 方法执行正向传

播。

3.2

简单的word2vec  101

3.2 简单的 word2vec

上一节

我们学习了基于推理的

方法，并基于代码讨论了

神经网络中单词

的处理

方法，至此准备工作就完

成了，现在是时候实现 word2vec 了

。

我们要做的事情是将神

经网络嵌入到图 3-3

所示的

模型中。这里，我们

使用由

原版 word2vec 提出的名为 continuous bag-of-words（CBOW）的

模型

作为神经网络。

word2vec 一词最初

用来指程序或者工具，但

是随着该词的流行，

在某

些语境下，也指神经网络

的模型。正确地说，CBOW 模型

和

skip-gram

模型是 word2vec 中使用的两个神

经网络。本节我

们将主要

讨论 CBOW 模型。关于这两个模

型的差异，我们将在

3.5.2 节详

细介绍。

3.2.1  CBOW模型的推理

CBOW

模型

是根据上下文预测目标

词的神经网络（“目标词”是

指中间

的单词，它周围的

单词是“上下文”）。通过训练

这个 CBOW 模型，使其能

尽可能

地进行正确的预测，我们

可以获得单词的分布式

表示。

CBOW

模型的输入是上下

文。这个上下文用 ['you', 'goodbye'] 这样

的

单词列表表示。我们将其

转换为 one-hot

表示，以便 CBOW 模型可

以进

行处理。在此基础上

，CBOW 模型的网络可以画成图

3-9 这样。

图 3-9 是 CBOW 模型的网络。它

有两个输入层，经过中间

层到达输出

层。这里，从输

入层到中间层的变换由

相同的全连接层（权重为

Win）完成，

从中间层到输出层

神经元的变换由另一个

全连接层（权重为 Wout）完成。

这

里，因为我们对上下文仅

考虑两个单词，所以输入

层有两个。

如果对上下文

考虑 N 个单词，则输入层会

有

N 个。

  第 3章

word2vec 102

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

(7×3)

Win

(7×3)

Win

(3×7)

Wout

图3-9 CBOW模

型的网络结构

现在，我们

注意一下图 3-9 的中间层。此

时，中间层的神经元是各

个输

入层经全连接层变

换后得到的值的“平均”。就

上面的例子而言，经全连

接

层变换后，第 1

个输入层

转化为 h1，第 2 个输入层转化

为 h2，那么中间层

的神经元

是

1

2 (h1 + h2)。

最后是图

3-9 中的输出层

，这个输出层有 7 个神经元

。这里重要的是，

这些神经

元对应于各个单词。输出

层的神经元是各个单词

的得分，它的值越

大，说明

对应单词的出现概率就

越高。得分是指在被解释

为概率之前的值，

对这些

得分应用 Softmax 函数，就可以得

到概率。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专

享

尊重版权

3.2 简单的word2vec  103

有时

将得分经过

Softmax 层之后的神

经元称为输出层。这里，我

们将输出得分的节点称

为输出层。

如图 3-9 所示，从输

入层到中间层的变换由

全连接层（权重是 Win）完

成。此

时，全连接层的权重 Win 是一

个 7 × 3

的矩阵。提前剧透一下

，这个

权重就是我们要的

单词的分布式表示，如图

3-10 所示。

图3-10　权重的各行对应

各个单词的分布式表示

you

hello

goodbye

(7×3)

say

and

i

.

W

如图 3-10 所示，权重 Win 的各行保

存着各个单词的分布式

表示。通过

反复学习，不断

更新各个单词的分布式

表示，以正确地从上下文

预测出应当

出现的单词

。令人惊讶的是，如此获得

的向量很好地对单词含

义进行了编

码。这就是 word2vec 的

全貌。

中间层的神经元数

量比输入层少这一点很

重要。中间层需要将预测

单词所需的信息压缩保

存，从而产生密集的向量

表示。这时，中间

层被写入

了我们人类无法解读的

代码，这相当于“编码”工作

。而

从中间层的信息获得

期望结果的过程则称为

“解码”。这一过程将被

编码

的信息复原为我们可以

理解的形式。

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第 3章　word2vec

104

到

目前为止，我们从神经元

视角图示了 CBOW 模型。下面，我

们从层

视角图示 CBOW

模型。这

样一来，这个神经网络的

结构如图 3-11 所示。

×

you

(1,

0, 0, 0, 0, 0, 0)

(0, 0, 1, 0, 0, 0,

0)

goodbye

Win

Win

Wout

MatMul

MatMul

MatMul +

0.5

ᓄܳ

图3-11

层视角

下的CBOW模型的网络结构：MatMul层

中使用的权重（Win、Wout）

画在各自

的层中

如图 3-11 所示，CBOW 模型一

开始有两个

MatMul 层，这两个层

的输

出被加在一起。然后

，对这个相加后得到的值

乘以 0.5 求平均，可以得到中

间层的神经元。最后，将另

一个 MatMul

层应用于中间层的

神经元，输出得

分。

不使用

偏置的全连接层的处理

由 MatMul 层的正向传播代理。这

个层在内部计算矩阵乘

积。

3.2

简单的word2vec  105

参考图 3-11，我们来

实现 CBOW

模型的推理（即求得

分的过程），具

体实现如下

所示（ ch03/cbow_predict.py）。

import sys

sys.path.append('..')

import numpy as np

from common.layers

import MatMul

# 样本的上下文数据

c0 = np.array([[1,

0, 0, 0, 0, 0, 0]])

c1 = np.array([[0, 0, 1, 0,

0, 0, 0]])

# 权重的初始值

W_in

= np.random.randn(7, 3)

W_out = np.random.randn(3,

7)

# 生成层

in_layer0 = MatMul(W_in)

in_layer1 = MatMul(W_in)

out_layer = MatMul(W_out)

# 正

向传播

h0 = in_layer0.forward(c0)

h1

= in_layer1.forward(c1)

h = 0.5 *

(h0 + h1)

s = out_layer.forward(h)

print(s)

# [[ 0.30916255 0.45060817 -0.77308656

0.22054131 0.15037278

# -0.93659277 -0.59612048]]

这里，我们首先将

必要的权重（W_in

和 W_out）初始化。然

后，生成与

上下文单词数

量等量（这里是两个）的处

理输入层的 MatMul 层，输出侧仅

生成一个 MatMul

层。需要注意的

是，输入侧的 MatMul 层共享权重

W_in。

之后，输入侧的 MatMul 层（in_layer0

和 in_layer1）调用

forward()

方法，计算中间数据，并通

过输出侧的 MatMul 层（out_layer）计算各个

单词的得分。

第 3章　word2vec 106

以上就

是 CBOW

模型的推理过程。这里

我们见到的 CBOW 模型是没

有

使用激活函数的简单的

网络结构。除了多个输入

层共享权重外，并没有什

么难点。接下来，我们继续

看一下 CBOW 模型的学习。

3.2.2  CBOW模型

的学习

到目前为止，我们

介绍的CBOW模型在输出层输

出了各个单词的得分。

通

过对这些得分应用 Softmax

函数

，可以获得概率（图 3-12）。这个概

率表

示哪个单词会出现

在给定的上下文（周围单

词）中间。

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

you

hello

goodbye Softmax

say

and

i

.

Win

Wout

(7×3)

Win

(7×3)

(3×7)

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

ắ⢴

ₐ⶛㼐ᴴカ

1

0

0

0

0

0

0

0

0

1

0

0

0

0

0

1

0

0

0

0

0

图3-12

CBOW模型

的示例（节点值的大小用

灰度表示）

在图 3-12 所示的例

子中，上下文是 you 和

goodbye，正确解

标签（神

经网络应该预测

出的单词）是 say。这时，如果网

络具有“良好的权重”，

那么

在表示概率的神经元中

，对应正确解的神经元的

得分应该更高。

CBOW 模型的学

习就是调整权重，以使预

测准确。其结果是，权重

Win（确

切地说是 Win 和 Wout 两者）学习到

蕴含单词出现模式的向

量。根

3.2

简单的word2vec  107

据过去的实

验，CBOW 模型（和 skip-gram

模型）得到的单

词的分布式表

示，特别是

使用维基百科等大规模

语料库学习到的单词的

分布式表示，在单

词的含

义和语法上符合我们直

觉的案例有很多。

CBOW模型只

是学习语料库中单词的

出现模式。如果语料库不

一样，

学习到的单词的分

布式表示也不一样。比如

，只使用“体育”相关

的文章

得到的单词的分布式表

示，和只使用“音乐”相关的

文章得

到的单词的分布

式表示将有很大不同。

现

在，我们来考虑一下上述

神经网络的学习。其实很

简单，这里我们处

理的模

型是一个进行多类别分

类的神经网络。因此，对其

进行学习只是使用

一下

Softmax 函数和交叉熵误差。首先

，使用 Softmax

函数将得分转化为

概率，再求这些概率和监

督标签之间的交叉熵误

差，并将其作为损失进行

学

习，这一过程可以用图

3-13 表示。

you

(1, 0,

0, 0, 0, 0, 0)

say

(0, 1, 0, 0, 0, 0,

0) MatMul

(0, 0, 1, 0,

0, 0, 0)

goodbye

MatMul

MatMul

Softmax

Cross 

Entropy

Error

Win

Win

Wout

× +

0.5

ᢌ๞

图3-13　学习时的CBOW模型的

网络结构

如图 3-13 所示，只需

向上一节介绍的进行推

理的 CBOW

模型加上

Softmax 层和 Cross Entropy Error

层，就

可以得到损失。这就是 CBOW

图

灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

第 3章　word2vec 108

模型计算损失的

流程，对应于神经网络的

正向传播。

虽然图 3-13 中使用

了 Softmax 层和 Cross

Entropy Error 层，但是我

们将这

两个层实现为了一个 Softmax with

Loss 层

。因此，我们接下来要实

现

的网络实际上如图 3-14 所示

。

you

(1, 0, 0, 0, 0, 0,

0) MatMul

MatMul

Softmax 

with

Loss

MatMul (0, 0, 1, 0,

0, 0, 0)

goodbye

say

(0,

1, 0, 0, 0, 0, 0)

× +

0.5

Win

Win

Wout

ᢌ๞

图3-14　将Softmax层和Cross Entropy Error层统一为Softmax with

Loss层

3.2.3  word2vec的

权重和分布式表示

如前

所述，word2vec 中使用的网络有两

个权重，分别是输入侧的

全连

接层的权重（Win）和输出

侧的全连接层的权重（Wout）。一

般而言，输入

侧的权重 Win 的

每一行对应于各个单词

的分布式表示。另外，输出

侧的权

重 Wout

也同样保存了

对单词含义进行了编码

的向量。只是，如图 3-15 所

示，输

出侧的权重在列方向上

保存了各个单词的分布

式表示。

3.2 简单的word2vec

109

图3-15　输入侧

和输出侧的权重都可以

被视为单词的分布式表

示

you

hello

goodbye

say

and

i

.

(7×3)

Win

(3×7)

Wout

那么，我们最终应该使

用哪个权重作为单词的

分布式表示呢？这里有三

个选项。

A. 只使用输入侧的

权重

B.

只使用输出侧的权

重

C. 同时使用两个权重

方

案 A 和方案

B 只使用其中一

个权重。而在采用方案 C 的

情况下，根

据如何组合这

两个权重，存在多种方式

，其中一个方式就是简单

地将这两个

权重相加。

就

word2vec（特别是 skip-gram 模型）而言，最受欢

迎的是方案 A。

许多研究中

也都仅使用输入侧的权

重 Win

作为最终的单词的分

布式表示。

遵循这一思路

，我们也使用 Win 作为单词的

分布式表示。

文献 [38]

通过实

验证明了 word2vec 的 skip-gram 模型中 Win

的有

效性。另外，在与 word2vec 相似的 GloVe[27] 方

法中，通过将两个

权重相

加，也获得了良好的结果

。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重

版权 youhello goodbye

sayand i

.

  第

3章　word2vec 110

3.3 学习数据的准

备

在开始

word2vec 的学习之前，我

们先来准备学习用的数

据。这里我们

仍以“You say goodbye and

I say hello.”这个只

有一句话的语料库为例

进

行说明。

3.3.1

上下文和目标

词

word2vec 中使用的神经网络的

输入是上下文，它的正确

解标签是被这

些上下文

包围在中间的单词，即目

标词。也就是说，我们要做

的事情是，当

向神经网络

输入上下文时，使目标词

出现的概率高（为了达成

这一目标而进

行学习）。

下

面我们就从语料库生成

上下文和目标词，如图 3-16 所

示。

you you, goodbye

say, and

say

say

and goodbye,

i 

goodbye 

and, say

i, hello

i

hello say, .

corpus contexts target

say s goodbye

d i ayan hello .

you

say s goodbye d i ayan

hello .

you say s goodbye

d i ayan hello .

you

say s goodbye d i ayan

hello .

you say s goodbye

d i ayan hello .

you

say s goodbye d i ayan

hello .

图3-16　从语料库生成上下

文和目标词

在图 3-16

中，将语

料库中的目标单词作为

目标词，将其周围的单词

作

为上下文提取出来。我

们对语料库中的所有单

词都执行该操作（两端的

单词

除外），可以得到图 3-16 右

侧的 contexts（上下文）和

target（目标词）。

contexts 的

各行成为神经网络的输

入，target 的各行成为正确解标

签（要预

测出的单词）。另外

，在各笔样本数据中，上下

文有多个单词（这个例子

中有

两个），而目标词则只

有一个，因此只有上下文

写成了复数形式

contexts。

3.3 学习数

据的准备  111

现在，我们来实

现从语料库生成上下文

和目标词的函数。在此之

前，我

们先复习一下上一

章的内容。首先，将语料库

的文本转化成单词 ID。这需

要使用第 2 章实现的 preprocess() 函数

。

import sys

sys.path.append('..')

from common.util import

preprocess

text = 'You say goodbye

and I say hello.'

corpus, word_to_id,

id_to_word = preprocess(text)

print(corpus)

# [0

1 2 3 4 1 5

6]

print(id_to_word)

# {0: 'you', 1:

'say', 2: 'goodbye', 3: 'and', 4:

'i', 5: 'hello', 6:

 '.'}

然后，从单词 ID 列表 corpus 生成 contexts

和

target。具体来说，如图

3-17 所示，实现

一个当给定 corpus 时返回 contexts

和 target 的

函数。

corpus contexts target

[0 1 2 3 4 1

5 6]

[1 3]

[2 4]

[3 1]

[4 5]

[[0 2]

[1 6]]

[1

5]

2

3

4

1

ᒏ⟣喟

(8,) ᒏ⟣喟 (6,)

ᒏ⟣喟 (6, 2)

图3-17　从单词ID列

表corpus生成contexts和target的例子（上下文

的窗口大小为1）

如图

3-17 所示

，contexts 是二维数组。此时，contexts 的第 0 维

保

存的是各个上下文数

据。具体来说，contexts[0] 保存的是第

0 个上下文，

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 3章　word2vec 112

context[1]

保存

的是第 1 个上下文……同样地

，就目标词而言，target[0]

保存的是

第 0 个目标词，target[1]

保存的是第

1 个目标词……

现在，我们来实

现这个生成上下文和目

标词的函数，这里将其称

为

create_contexts_target(corpus, window_size)（ common/util.py）。

def create_contexts_target(corpus, window_size=1):

 target =

corpus[window_size:-window_size]

 contexts = []

for idx in range(window_size, len(corpus)-window_size):

cs = []

 for t

in range(-window_size, window_size + 1):

if t == 0:

 continue

cs.append(corpus[idx + t])

 contexts.append(cs)

return np.array(contexts), np.array(target)

这个函数有两个参数

：一个是单词 ID 列表（corpus）；另一个

是上下文

的窗口大小（window_size）。另

外，函数返回的是 NumPy 多维数

组格式的

上下文和目标

词。现在，我们来实际使用

一下这个函数，接着刚才

的实现，

代码如下所示。

contexts,

target = create_contexts_target(corpus, window_size=1)

print(contexts)

#

[[0 2]

# [1 3]

#

[2 4]

# [3 1]

#

[4 5]

# [1 6]]

print(target)

# [1 2 3 4 1

5]

3.3 学

习数据的准备  113

这样就从

语料库生成了上下文和

目标词，后面只需将它们

赋给

CBOW

模型即可。不过，因为

这些上下文和目标词的

元素还是单词 ID，所以还需

要将它们转化为 one-hot 表示。

3.3.2

转

化为one-hot表示

下面，我们将上

下文和目标词转化为 one-hot 表

示，如图 3-18 所示。

contexts target contexts target contexts target

you, goodbye

say, and

say

say

and goodbye, i 

goodbye

and, say

i, hello

i

hello

say, .

[1 3]

[2 4]

[3 1]

[4 5]

[[0 2]

[1 6]]

[1

5]

2

3

4

1

[[[100000 0]

 [0010000]]

[[010000 0]

 [000100 0]]

[[001000

0]

 [000010 0]]

[[000100 0]

[010000 0]]

[[000010 0]

 [000001

0]]

[[010000 0]

 [000000 1]]]

[[010000 0]

 [0010000]

 [000100

0]

 [000010 0]

 [010000

0]

 [000001 0]]

ᒏ⟣喟

(6,)

ᒏ⟣喟

(6, 2, 7)

ᒏ⟣喟

(6,

7) ᒏ⟣喟 (6, 2)

䃺ࢂ

ID

one-hot 㶕⹧

图

3-18　将上下文和目标词转化

为one-hot表示的例子

如图 3-18

所示

，上下文和目标词从单词

ID 转化为了 one-hot 表示。

这里需要

注意各个多维数组的形

状。在上面的例子中，使用

单词 ID

时的

contexts 的形状是 (6,2)，将其

转化为 one-hot 表示后，形状变为

(6,2,7)。

本书提供了 convert_one_hot() 函数以将单

词 ID 转化为 one-hot

表示。

这个函数

的实现不再说明，内容很

简单，代码在 common/util.py 中。该函数

的

参数是单词 ID

列表和词汇

个数。我们再把到目前为

止的数据预处理总结

一

下，如下所示。

import sys

sys.path.append('..')

from

common.util import preprocess, create_contexts_target,

 convert_one_hot

text = 'You say goodbye and

I say hello.'

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第 3章　word2vec

114

corpus, word_to_id, id_to_word = preprocess(text)

contexts, target = create_contexts_target(corpus, window_size=1)

vocab_size

= len(word_to_id)

target = convert_one_hot(target, vocab_size)

contexts = convert_one_hot(contexts, vocab_size)

至

此，学习数据的准备就完

成了，下面我们来讨论最

重要的 CBOW

模

型的实现。

3.4 CBOW模型

的实现

现在，我们来实现

CBOW 模型。这里要实现的神经

网络如图

3-19 所示。

you

(1, 0, 0,

0, 0, 0, 0) MatMul

MatMul

MatMul (0, 0, 1, 0, 0,

0, 0)

goodbye

say

(0, 1,

0, 0, 0, 0, 0)

Win

Win

Wout

Softmax 

with

Loss

× +

0.5

ᢌ๞

图3-19

CBOW模型的

网络结构

3.4 CBOW模型的实现  115

我

们将图

3-19 中的神经网络实

现为 SimpleCBOW 类（下一章将实现对

其

进行了改进的 CBOW

类）。首先

，让我们看一下 SimpleCBOW 类的初始

化方法

（ ch03/simple_cbow.py）。

import

sys

sys.path.append('..')

import numpy as np

from common.layers import MatMul, SoftmaxWithLoss

class

SimpleCBOW:

 def __init__(self, vocab_size, hidden_size):

V, H = vocab_size, hidden_size

# 初始化权重

 W_in = 0.01

* np.random.randn(V, H).astype('f')

 W_out =

0.01 * np.random.randn(H, V).astype('f')

 #

生成

层

 self.in_layer0 = MatMul(W_in)

self.in_layer1 = MatMul(W_in)

 self.out_layer =

MatMul(W_out)

 self.loss_layer = SoftmaxWithLoss()

# 将所有的权重和梯度

整理到列表中

 layers = [self.in_layer0,

self.in_layer1, self.out_layer]

 self.params, self.grads =

[], []

 for layer in

layers:

 self.params += layer.params

self.grads += layer.grads

 # 将单词的

分布式表示设置为成员

变量

self.word_vecs = W_in

这里，初始化方法的

参数包括词汇个数 vocab_size 和中

间层的神经元

个数 hidden_size。关于

权重的初始化，首先我们

生成两个权重（W_in 和 W_

out），并用一

些小的随机值初始化这

两个权重。此外，我们指定

NumPy 数

组的数据类型为 astype('f')，这样

一来，初始化将使用 32 位的

浮点数。

图灵社区会员 Kensuke(cpy4ever@gmail.com)

专

享 尊重版权

  第 3章

word2vec 116

接着，我

们创建必要的层。首先，生

成两个输入侧的 MatMul 层、一个

输出侧的 MatMul

层，以及一个 Softmax with Loss 层

。这里，用来处理输

入侧上

下文的

MatMul 层的数量与上下

文的单词数量相同（本例

中是两个）。

另外，我们使用

相同的权重来初始化 MatMul 层

。

最后，将该神经网络中使

用的权重参数和梯度分

别保存在列表类型的成

员变量

params 和 grads 中。

这里，多个层

共享相同的权重。因此，params列

表中存在多个相

同的权

重。但是，在

params列表中存在多

个相同的权重的情况

下

，Adam、Momentum 等优化器的运行会变得

不符合预期（至

少就我们

的代码而言）。为此，在 Trainer类的

内部，在更新参数

时会进

行简单的去重操作。关于

这一点，这里省略说明，感

兴趣

的读者可以参考 common/trainer.py的

remove_duplicate(params, 

grads)。

接下来，我们来实现神经

网络的正向传播 forward()

函数。这

个函数接

收参数 contexts 和 target，并返

回损失（loss）。

def

forward(self, contexts, target):

 h0 =

self.in_layer0.forward(contexts[:, 0])

 h1 = self.in_layer1.forward(contexts[:,

1])

 h = (h0 +

h1) * 0.5

 score =

self.out_layer.forward(h)

 loss = self.loss_layer.forward(score, target)

return loss

这里，我们假定参

数 contexts 是一个三维 NumPy

数组，即上

一节图

3-18 的例子中 (6,2,7)的形状

，其中第 0 维的元素个数是

mini-batch

的数量，

第 1 维的元素个数

是上下文的窗口大小，第

2 维表示 one-hot

向量。此外，

target 是 (6,7) 这样

的二维形状。

最后，我们实

现反向传播

backward()。这个反向传

播的计算图如图 3-20

所示。

3.4 CBOW模

型的实现

117

图3-20 CBOW模型的反向

传播：蓝色的粗线表示反

向传播的路线

you

(1, 0,

0, 0, 0, 0, 0)

(0,

0, 1, 0, 0, 0, 0)

goodbye

MatMul

MatMul

say

(0, 1,

0, 0, 0, 0, 0)

Win

Win

MatMul

Wout

Softmax 

with

Loss

× +

0.5

0.5·da da

ds 1

ᢌ๞

神经网络

的反向传播在与正向传

播相反的方向上传播梯

度。这个反向传

播从 1

出发

，并将其传向 Softmax with Loss 层。然后，将 Softmax

with 

Loss 层

的反向传播的输出 ds 传到

输出侧的

MatMul 层。

之后就是“+”和

“×”运算的反向传播。“×”的反向

传播将正向传播

时的输

入值“交换”后乘以梯度。“+”的

反向传播则将梯度“原样

”传播。

我们按照图 3-20

来实现

反向传播。

def backward(self, dout=1):

 ds

= self.loss_layer.backward(dout)

 da = self.out_layer.backward(ds)

da *= 0.5

 self.in_layer1.backward(da)

self.in_layer0.backward(da)

 return None

至此，反向传播

的实现就结束了。我们已

经将各个权重参数的梯

度

0.5·da

0.5·da

  第 3章　word2vec

118

保存在了成员变

量 grads 中。因此，通过先调用 forward() 函

数，再调

用 backward() 函数，grads 列表中的

梯度被更新。下面，我们继

续看一下

SimpleCBOW 类的学习。

学习

的实现

CBOW 模型的学习和一

般的神经网络的学习完

全相同。首先，给神

经网络

准备好学习数据。然后，求

梯度，并逐步更新权重参

数。这里，我

们使用第 1

章介

绍的 Trainer 类来执行学习过程

，学习的源代码如下所示

（ ch03/train.py）。

import sys

sys.path.append('..')

from common.trainer import Trainer

from

common.optimizer import Adam

from simple_cbow import

SimpleCBOW

from common.util import preprocess, create_contexts_target,

convert_one_hot

window_size = 1

hidden_size =

5

batch_size = 3

max_epoch =

1000

text = 'You say goodbye

and I say hello.'

corpus, word_to_id,

id_to_word = preprocess(text)

vocab_size = len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)

target

= convert_one_hot(target, vocab_size)

contexts = convert_one_hot(contexts,

vocab_size)

model = SimpleCBOW(vocab_size, hidden_size)

optimizer

= Adam()

trainer = Trainer(model, optimizer)

3.4 CBOW模型的实现  119

trainer.fit(contexts, target,

max_epoch, batch_size)

trainer.plot()

common/optimizer.py 中实现了 SGD、AdaGrad

等

多个著名的参数更新

方

法。这里，我们选择 Adam 算法。如

第 1 章所述，Trainer

类会执行神经

网络的学习过程，包括从

学习数据中选出 mini-batch 给神经

网络以算出梯

度，并将这

个梯度给优化器以更新

权重参数等一系列操作

。

之后，我们也使用 Trainer类进行

神经网络的学习。使用

Trainer类

，

可以理清容易变复杂的

学习代码。

运行上面的代

码，结果如图 3-21 所示。

图3-21

用图

形表示学习过程（横轴表

示学习的迭代次数，纵轴

表示损失）

2.00

1.75

1.50

1.25

1.00

䔚А⁎᪝

 (×20)

0.75

0.50

0.25

0 200 400 600 800 1000

如图 3-21 所示，通过

不断学习，损失在减小，看

起来学习进行得一

切正

常。我们来看一下学习结

束后的权重参数。这里，我

们取出输入侧的 ᢌ๞

第 3章　word2vec 120

MatMul 层

的权重，实际确认一下它

的内容。因为输入侧的

MatMul 层

的权

重已经赋值给了成

员变量 word_vecs，所以接着上面的

代码，我们追加下面

的代

码。

word_vecs

= model.word_vecs

for word_id, word in

id_to_word.items():

 print(word, word_vecs[word_id])

这里，使用 word_vecs

这个变量保

存权重。word_vecs 的各行保存了对

应

的单词 ID 的分布式表示

。实际运行一下，可以得到

下述结果。

you

[-0.9031807 -1.0374491 -1.4682057 -1.3216232 0.93127245]

say

[ 1.2172916 1.2620505 -0.07845993 0.07709391 -1.2389531

]

goodbye [-1.0834033 -0.8826921 -0.33428606 -0.5720131

1.0488235 ]

and [ 1.0244362 1.0160093

-1.6284224 -1.6400533 -1.0564581]

i [-1.0642933 -0.9162385

-0.31357735 -0.5730831 1.041875 ]

hello [-0.9018145

-1.035476 -1.4629668 -1.3058501 0.9280102]

. [

1.0985303 1.1642815 1.4365371 1.3974973 -1.0714306]

我们终于将单

词表示为了密集向量！这

就是单词的分布式表示

。我们有

理由相信，这样的

分布式表示能够很好地

捕获单词含义。

不过，遗憾

的是，这里使用的小型语

料库并没有给出很好的

结果。当

然，主要原因是语

料库太小了。如果换成更

大、更实用的语料库，相信

会获

得更好的结果。但是

，这样在处理速度方面又

会出现新的问题，这是因

为当

前这个 CBOW

模型的实现

在处理效率方面存在几

个问题。下一章我们将改

进这个简单的 CBOW 模型，实现

一个“真正的”CBOW 模型。

3.5 word2vec的补充

说明

至此，我们详细探讨

了 word2vec 的 CBOW 模型。接下来，我们将

对

word2vec

补充说明几个非常重

要的话题。首先，我们从概

率的角度，再来

看一下 CBOW 模

型。

3.5 word2vec的补充说明

121

3.5.1  CBOW模型和概

率

首先简单说明一下概

率的表示方法。本书中将

概率记为 P(·)，比如事

件 A 发生

的概率记为 P(A)。联合概率记

为 P(A, B)，表示事件

A 和事件

B 同时

发生的概率。

后验概率记

为 P(A|B)，字面意思是“事件发生

后的概率”。从另一个

角度

来看，也可以解释为“在给

定事件 B（的信息）时事件 A 发

生的概率”。

下面，我们用概

率的表示方法来描述 CBOW

模

型。CBOW 模型进行

的处理是，当

给定某个上下文时，输出

目标词的概率。这里，我们

使用包含

单词 w1, w2,

···, wT 的语料库

。如图 3-22 所示，对第 t

个单词，考

虑窗口大

小为 1 的上下文

。

图3-22 word2vec的CBOW模型：从上下文的单

词预测目标词

... . . . w1w2 wt−1

wt wt+1 wT−1wT

下面，我们

用数学式来表示当给定

上下文 wt−1 和

wt+1 时目标词为 wt

的

概率。使用后验概率，有式

(3.1)：

 P(wt|wt−1,

wt+1) (3.1)

式 (3.1) 表示“在 wt−1

和 wt+1 发生后，wt 发生

的概率”，也可以解释

为“当

给定 wt−1

和 wt+1 时，wt 发生的概率”。也

就是说，CBOW 模型可

以建模为

式

(3.1)。

这里，使用式 (3.1) 可以简洁

地表示 CBOW 模型的损失函数

。我

们把第 1 章介绍的交叉

熵误差函数（式 (1.7)）套用在这

里。式 (1.7) 是

L = − k

tk

log yk，其中，yk 表示第 k 个事

件发生的概率。tk 是监督标

签，

它是 one-hot 向量的元素。这里

需要注意的是，“wt 发生”这一

事件是正确

解，它对应的

one-hot 向量的元素是

1，其他元素

都是 0（也就是说，当 wt

图灵社

区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第

3章　word2vec 122

之外的事件发生时，对

应的 one-hot 向量的元素均为 0）。考

虑到这一点，可

以推导出

下式：

L = −log P(wt|wt−1, wt+1) (3.2)

CBOW 模型的损失函数只

是对式 (3.1) 的概率取 log，并加上

负号。顺

便提一下，这也称

为负对数似然（negative

log likelihood）。式 (3.2) 是一

笔

样本数据的损失函数。如

果将其扩展到整个语料

库，则损失函数可以

写为

：

L = − 1

T

T

t=1

log P(wt|wt−1, wt+1) (3.3)

CBOW

模型学习的任务就是让

式 (3.3) 表示的损失函数尽可

能地小。

那时的权重参数

就是我们想要的单词的

分布式表示。这里，我们只

考虑了窗

口大小为 1

的情

况，不过其他的窗口大小

（或者窗口大小为 m 的一般

情况）

也很容易用数学式

表示。

3.5.2

skip-gram模型

如前所述，word2vec 有两

个模型：一个是我们已经

讨论过的 CBOW

模型；另一个是

被称为 skip-gram

的模型。skip-gram 是反转了

CBOW 模

型处理的上下文和目

标词的模型。举例来说，两

者要解决的问题如图 3-23

所

示。

图3-23 CBOW模型和skip-gram模型处理的

问题

you goodbye and i

say hello. and i say hello.

say ? ? ?

CBOW ὎ಸ

skip-gram ὎ಸ

如图 3-23 所示，CBOW 模型从上

下文的多个单词预测中

间的单词（目

3.5 word2vec的补充说明

123

标词），而 skip-gram 模型则从中间的

单词（目标词）预测周围的

多个单词

（上下文）。此时，skip-gram 模

型的网络结构如图 3-24 所示

。

图3-24 skip-gram模型的例子

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

Win

0

1

0

0

0

0

0

1

0

0

0

0

0

0

0

0

1

0

0

0

0

Wout

Wout

䓀ڒᅯ

ₐ⶛㼐ᴴカ

䓀ܧᅯ

喍ᓄܳ喎 ͚䬡ᅯ

由图

3-24 可知，skip-gram 模型的输入层只有

一个，输出层的数量则

与

上下文的单词个数相等

。因此，首先要分别求出各

个输出层的损失（通过

Softmax with Loss 层

等），然后将它们加起来作

为最后的损失。

现在，我们

使用概率的表示方法来

表示 skip-gram

模型。我们来考虑

根

据中间单词（目标词）wt 预测

上下文 wt−1 和 wt+1

的情况。此时，skip￾gram 可

以建模为式 (3.4)：

 P(wt−1, wt+1|wt)

(3.4)

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 3章　word2vec 124

式

(3.4) 表示“当给定

wt 时，wt−1 和 wt+1 同时发

生的概率”。这里，

在

skip-gram 模型中

，假定上下文的单词之间

没有相关性（正确地说是

假定

“条件独立”），将式 (3.4) 如下

进行分解：

P(wt−1, wt+1|wt) = P(wt−1|wt)P(wt+1|wt) (3.5)

通过将式

(3.5) 代入

交叉熵误差函数，可以推

导出 skip-gram 模型的损

失函数：

L

= − log P(wt−1, wt+1|wt)

=

− log P(wt−1|wt)P(wt+1|wt)

= −(log P(wt−1|wt)

+ log P(wt+1|wt))

（3.6）

这

里利用了对数的性质 log

xy = log x + log

y。如

式 (3.6) 所示，skip￾gram 模型的损失函数

先分别求出各个上下文

对应的损失，然后将它们

加在

一起。式 (3.6)

是一笔样本

数据的损失函数。如果扩

展到整个语料库，则

skip-gram 模型

的损失函数可以表示为

式 (3.7)：

L =

−T

1 T

t=1

(log

P(wt−1|wt) + log P(wt+1|wt)) (3.7)

比较式

(3.7) 和 CBOW 模型的式 (3.3)，差

异是非常明显的。因为 skip￾gram

模

型的预测次数和上下文

单词数量一样多，所以它

的损失函数需要求

各个

上下文单词对应的损失

的总和。而CBOW模型只需要求

目标词的损失。

以上就是

对 skip-gram 模型的介绍。

那么，我们

应该使用

CBOW 模型和 skip-gram 模型中

的哪一个呢？答

案应该是

skip-gram 模型。这是因为，从单词的

分布式表示的准确度来

看，

在大多数情况下，skip-grm 模型

的结果更好。特别是随着

语料库规模的增

大，在低

频词和类推问题的性能

方面，skip-gram 模型往往会有更好

的表现

（单词的分布式表

示的评价方法会在 4.4.2

节说

明）。此外，就学习速度而言

，

CBOW 模型比 skip-gram 模型要快。这是因

为 skip-gram

模型需要根据上

3.5 word2vec的补

充说明  125

下文数量计算相

应个数的损失，计算成本

变大。

skip-gram 模型根据一个单词

预测其周围的单词，这是

一个非常难

的问题。假如

我们来解决图 3-23 中的问题

，此时，对于 CBOW

模

型的问题，我

们很容易回答“say”。但是，对于

skip-gram 模型的问题，

则存在许多

候选。因此，可以说 skip-gram 模型要

解决的是更难的

问题。经

过这个更难的问题的锻

炼，skip-gram 模型能提供更好的

单

词的分布式表示。

理解了

CBOW 模型的实现，在实现 skip-gram

模型

时应该就不存在什

么难

点了。因此，这里就不再介

绍 skip-gram 模型的实现。感兴趣的

读者可

以参考 ch03/simple_skip_gram.py。

3.5.3  基于计数

与基于推理

到目前为止

，我们已经了解了基于计

数的方法和基于推理的

方法（特别

是 word2vec）。两种方法在

学习机制上存在显著差

异：基于计数的方法通过

对整个语料库的统计数

据进行一次学习来获得

单词的分布式表示，而基

于推

理的方法则反复观

察语料库的一部分数据

进行学习（mini-batch 学习）。这

里，我们

就其他方面来对比一下

这两种方法。

首先，我们考

虑需要向词汇表添加新

词并更新单词的分布式

表示的场

景。此时，基于计

数的方法需要从头开始

计算。即便是想稍微修改

一下单词

的分布式表示

，也需要重新完成生成共

现矩阵、进行

SVD 等一系列操

作。

相反，基于推理的方法

（word2vec）允许参数的增量学习。具

体来说，可

以将之前学习

到的权重作为下一次学

习的初始值，在不损失之

前学习到的经

验的情况

下，高效地更新单词的分

布式表示。在这方面，基于

推理的方法

（word2vec）具有优势。

其

次，两种方法得到的单词

的分布式表示的性质和

准确度有什么差

异呢？就

分布式表示的性质而言

，基于计数的方法主要是

编码单词的相似

性，而 word2vec（特

别是 skip-gram 模型）除了单词的相

似性以外，还能

图灵社区

会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 3章

word2vec 126

理解更复杂的单词之间

的模式。关于这一点，word2vec 因能

解开“king −

man + woman = queen”这样的类推问题而

知名（关于类推问题，我们

将

在

4.4.2 节说明）。

这里有一个

常见的误解，那就是基于

推理的方法在准确度方

面优于基于

计数的方法

。实际上，有研究表明，就单

词相似性的定量评价而

言，基于推

理的方法和基

于计数的方法难分上下

[25]。

2014

年发表的题为“Don’t count, predict!”（不要计数

，要预测！）

的论文 [24] 系统地比

较了基于计数的方法和

基于推理的方法，并给

出

了基于推理的方法在准

确度上始终更好的结论

。但是，之后又有

其他的论

文 [25]提出，就单词的相似性

而言，结论高度依赖于超

参数，

基于计数的方法和

基于推理的方法难分胜

负。

另外一个重要的事实

是，基于推理的方法和基

于计数的方法存在关联

性。具体地说，使用了 skip-gram

和下

一章介绍的 Negative Sampling 的模

型被证

明与对整个语料库的共

现矩阵（实际上会对矩阵

进行一定的修改）进

行特

殊矩阵分解的方法具有

相同的作用

[26]。换句话说，这

两个方法论（在

某些条件

下）是“相通”的。

此外，在 word2vec 之后

，有研究人员提出了 GloVe

方法

[27]。GloVe

方法融合了基于推理的

方法和基于计数的方法

。该方法的思想是，将整个

语

料库的统计数据的信

息纳入损失函数，进行 mini-batch 学

习（具体请参考

论文

[27]）。据此

，这两个方法论成功地被

融合在了一起。

图灵社区

会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

3.6

小结

127

3.6 小结

托马斯·米科洛夫（Tomas Mikolov）在

一系列论文

[22] [23] 中提出了

word2vec。自

论文发表以来，word2vec 受到了许

多关注，它的作用也在许

多自然语言处理任务中

得到了证明。下一章，我们

将结合具体的例子来说

明

word2vec

的重要性，特别是 word2vec 的迁

移学习的作用。

本章我们

详细解释了 word2vec 的

CBOW 模型，并对

其进行了实

现。CBOW 模型基本

上是一个 2 层的神经网络

，结构非常简单。我们使用

MatMul

层和 Softmax with Loss 层构建了 CBOW

模型，并用

一个小规模

语料库确认

了它的学习过程。遗憾的

是，现阶段的 CBOW 模型在处理

效率

上还存在一些问题

。不过，在理解了本章的 CBOW

模

型之后，离真正的

word2vec 也就一

步之遥了。下一章，我们将

改进 CBOW 模型。

第 3章　word2vec 128

本章所学

的内容

•

基于推理的方法

以预测为目标，同时获得

了作为副产物的单词的

分

布式表示

• word2vec 是基于推理

的方法，由简单的 2

层神经

网络构成

• word2vec 有 skip-gram 模型和

CBOW 模型

• CBOW 模型从多个单词（上下文

）预测 1 个单词（目标词）

• skip-gram 模型

反过来从 1 个单词（目标词

）预测多个单词（上下文）

•

由

于 word2vec 可以进行权重的增量

学习，所以能够高效地更

新或添

加单词的分布式

表示

第4章

word2vec的高速化

不要

企图无所不知，否则你将

一无所知。

——德谟克利特（古

希腊哲学家）

上一章我们

学习了 word2vec 的机制，并实现了

CBOW 模型。因为

CBOW 模型是一个简

单的 2 层神经网络，所以实

现起来比较简单。但是，

目

前的实现存在几个问题

，其中最大的问题是，随着

语料库中处理的词汇量

的增加，计算量也随之增

加。实际上，当词汇量达到

一定程度之后，上一章

的

CBOW

模型的计算就会花费过

多的时间。

因此，本章将重

点放在 word2vec 的加速上，来改善

word2vec。具

体而言，我们将对上一

章中简单的 word2vec

进行两点改

进：引入名为

Embedding 层的新层，以

及引入名为 Negative Sampling 的新损失函

数。

这样一来，我们就能够

完成一个“真正的”word2vec。完成这

个真正的

word2vec 后，我们将在 PTB 数

据集（一个大小比较实用

的语料库）上进行

学习，并

实际评估所获得的单词

的分布式表示的优劣。

4.1 word2vec的

改进①

我们先复习一下上

一章的内容。在上一章中

，我们实现了图 4-1 中的

CBOW

模型

。

  第 4章　word2vec的高速化

130

you

hello

goodbye

say

and

i

.

you

hello

goodbye

say

and

i

.

you

hello

goodbye

Softmax

say

and

i

.

Win

Wout

(7×3)

Win

(7×3)

(3×7)

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

ắ⢴

ₐ⶛㼐ᴴカ

1

0

0

0

0

0

0

0

0

1

0

0

0

0

0

1

0

0

0

0

0

图4-1　上一章中实现的CBOW模型

如图 4-1 所示，上一章的

CBOW 模型

接收拥有 2 个单词的上下

文，并

基于它们预测 1

个单

词（目标词）。此时，通过输入

层和输入侧权重（Win）

之间的

矩阵乘积计算中间层，通

过中间层和输出侧权重

（Wout）之间的矩

阵乘积计算每

个单词的得分。这些得分

经过 Softmax 函数转化后，得到每

个单词的出现概率。通过

将这些概率与正确解标

签进行比较（更确切地说

，

使用交叉熵误差函数），从

而计算出损失。

在上一章

中，我们限定了上下文的

窗口大小为 1。这相当于只

将

目标词的前一个和后

一个单词作为上下文。本

章我们将给模型新

增一

个功能，使之能够处理任

意窗口大小的上下文。

图

4-1 中的

CBOW 模型在处理小型语

料库时问题不大。实际上

，图 4-1

中处理的词汇量一共

只有 7 个，这个规模自然毫

无问题。不过在处理大规

模

语料库时，这个模型就

存在多个问题了。为了指

出这些问题，这里我们考

虑

4.1 word2vec的改进①  131

一个例子。假设

词汇量有

100 万个，CBOW 模型的中

间层神经元有 100 个，

此时

word2vec 进

行的处理如图 4-2 所示。

图4-2　假

设词汇量为100万个时的CBOW模

型

you

goodbye

say

and

.

you

goodbye

say

and

.

you

goodbye

say

and

.

Win

(1 000

000×100)

Win

(1 000 000×100)

Wout

(100×1 000 000)

Softmax

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

ắ⢴

1

1

0

0

0

0

0

0

0

0

如图 4-2 所示，输

入层和输出层存在 100

万个

神经元。在如此多的神经

元的情况下，中间的计算

过程需要很长时间。具体

来说，以下两个地方的计

算会出现瓶颈。

•　输入层的

one-hot 表示和权重矩阵 Win 的乘积

（4.1

节解决）

•　中间层和权重矩

阵 Wout 的乘积以及 Softmax

层的计算

（4.2 节解决）

第 1 个问题与输入

层的 one-hot

表示有关。这是因为

我们用 one-hot 表

示来处理单词

，随着词汇量的增加，one-hot 表示

的向量大小也会增加。比

如，在词汇量有 100

万个的情

况下，仅 one-hot 表示本身就需要

占用 100 万

第 4章　word2vec的高速化 132

个

元素的内存大小。此外，还

需要计算 one-hot

表示和权重矩

阵 Win 的乘

积，这也要花费大

量的计算资源。关于这个

问题，我们会在 4.1 节中通过

引

入新的 Embedding 层来解决。

第 2 个

问题是中间层之后的计

算。首先，中间层和权重矩

阵

Wout 的乘

积需要大量的计

算。其次，随着词汇量的增

加，Softmax 层的计算量也会增

加

。关于这些问题，我们将在

4.2 节通过引入

Negative Sampling 这一新的

损

失函数来解决。下面就让

我们通过改进来消除这

两个瓶颈。

改进前的版本

（上一章中的 word2vec

实现）在 ch03目录

下的 simple_

cbow.py（或 者 simple_skip_gram.py）中。改

进 后 的 word2vec 版

本

在

ch04目录下的 cbow.py（或者 skip_gram.py）中。

4.1.1  Embedding层

在上一章的

word2vec 实现中，我们

将单词转化为了 one-hot 表示，并

将其输入了 MatMul 层，在

MatMul 层中计

算了该 one-hot 表示和权重矩阵

的乘积。这里，我们来考虑

词汇量是 100 万个的情况。假

设中间层的神经元

个数

是 100，则 MatMul 层中的矩阵乘积可

画成图 4-3。

图4-3

one-hot表示的上下文

和MatMul层的权重的乘积

010 00

c h Win

ᒏ⟣喟

(1 × 1 000 000)

(1 000 000 × 100) (1

× 100)

4.1 word2vec的

改进①  133

如图 4-3 所示，如果语料

库的词汇量有 100 万个，则单

词的 one-hot

表

示的维数也会是

100 万，我们需要计算这个巨

大向量和权重矩阵的乘

积。但

是，图 4-3 中所做的无非

是将矩阵的某个特定的

行取出来。因此，直觉上将

单词转化为

one-hot 向量的处理

和 MatMul 层中的矩阵乘法似乎

没有必要。

现在，我们创建

一个从权重参数中抽取

“单词 ID

对应行（向量）”的

层，这

里我们称之为 Embedding 层。顺便说

一句，Embedding 来自“词嵌

入”（word

embedding）这一术

语。也就是说，在这个 Embedding 层存

放

词嵌入（分布式表示）。

在

自然语言处理领域，单词

的密集向量表示称为 词

嵌入（word

embedding）或者单词的分布式

表示（distributed representation）。

过 去，将 基 于

计 数 的

方 法 获 得

的 单 词 向 量 称

为

distributional 

representation，将使用神经网络的基

于推理的方法获得的单

词向

量称为 distributed representation。不过，中文里

二者都译为“分

布式表示

”。

4.1.2  Embedding层的实现

从矩阵中取出

某一行的处理是很容易

实现的。这里，假设权重 W

是

NumPy 的二维数组。如果要从这

个权重中取出某个特定

的行，只需写 W[2]

或者 W[5]。用 Python

代码

来实现，如下所示。

>>> import numpy as np

>>> W = np.arange(21).reshape(7, 3)

>>>

W

array([[ 0, 1, 2],

[ 3, 4, 5],

 [

6, 7, 8],

 [ 9,

10, 11],

 [12, 13, 14],

[15, 16, 17],

 [18, 19,

20]])

>>> W[2]

array([6, 7, 8])

第 4章　word2vec的

高速化 134

>>>

W[5]

array([15, 16, 17])

另外，从权重 W

中一

次性提取多行的处理也

很简单。只需通过数组指

定

行号即可，实际的代码

如下所示。

>>> idx = np.array([1,

0, 3, 0])

>>> W[idx]

array([[

3, 4, 5],

 [ 0,

1, 2],

 [ 9, 10,

11],

 [ 0, 1, 2]])

在这个例子中

，我们一次性提取了 4 个索

引（1、0、3、0）。通过将数

组作为参数

，可以一次性提取多行。顺

便说一下，这里的实现假

定用于

mini-batch 处理。

下面，我们来

实现 Embedding 层的 forward() 方法。参照之前

的例子，

实现如下所示（

common/layers.py）。

class Embedding:

 def __init__(self,

W):

 self.params = [W]

self.grads = [np.zeros_like(W)]

 self.idx =

None

 def forward(self, idx):

W, = self.params

 self.idx =

idx

 out = W[idx]

return out

根

据本书的代码规范，使用

params 和 grads 作为成员变量，并在成

员变

量 idx 中以数组的形式

保存需要提取的行的索

引（单词 ID）。

接下来，我们考虑

反向传播。Embedding 层的正向传播

只是从权重矩

阵 W 中提取

特定的行，并将该特定行

的神经元原样传给下一

层。因此，在反

向传播时，从

上一层（输出侧的层）传过

来的梯度将原样传给下

一层（输

4.1 word2vec的改进①

135

入侧的层

）。不过，从上一层传来的梯

度会被应用到权重梯度

dW 的特定行

（idx），如图 4-4 所示。

idx Embed

h idx

dW dh

dh

dW

h

h

W

W

W

Embed

ₐाэ᧚

ाэ᧚ࣺ

图4-4 Embedding层

的正向传播和反向传播

处理的概要（Embedding层记为Embed）

基于

以上内容，我们来实现 backward()，代

码如下所示。

def backward(self, dout):

dW, = self.grads

 dW[...] =

0

 dW[self.idx] = dout #

不太好的方

式

 return None

这里，取出权重梯度 dW，通

过

dW[...] = 0 将 dW 的元素设为

0（并不是

将 dW 设为 0，而是保持 dW 的形状

不变，将它的元素设为

0）。然

后，将上一

层传来的梯度

dout 写入 idx 指定的行。

第 4章　word2vec的高

速化 136

这里创建了和权重

W相同大小的矩阵 dW，并将梯

度写入了

dW对应

的行。但是

，我们最终想做的事情是

更新权重 W，所以没有必要

特意创建 dW（大小与 W相同）。相

反，只需把需要更新的行

号（idx）

及其对应的梯度（dout）保存

下来，就可以更新权重（W）的

特定行。

但是，这里为了兼

容已经实现的优化器类

（Optimizer），所以写

成了现在的样子

。

实际上，在刚才的 backward() 的实现

中，存在一个问题，这一问

题发生

在

idx 的元素出现重

复时。比如，当 idx 为 [0, 2,

0, 4] 时，就会发

生图 4-5

中的问题。

图4-5

当idx数组

的元素中出现相同的行

号时，简单地将dh的行写入

对应位置就会有问题

idx

0

2

0

4

dh

dW

如

图 4-5 所示，我们将 dh

各行的值

写入 dW 中 idx 指定的位置。在这

种

情况下，dW

的第 0 行会被写

入两次。这样一来，其中某

个值就会被覆盖掉。

为了

解决这个重复问题，需要

进行“加法”，而不是“写入”（请

读者考

虑一下为什么是

加法）。也就是说，应该把 dh

各

行的值累加到 dW 的对应行

中。下面，我们来实现正确

的反向传播。

def backward(self, dout):

dW, = self.grads

4.2 word2vec的改进②

137

 dW[...] = 0

for i, word_id in enumerate(self.idx):

dW[word_id] += dout[i]

 # 或者



# np.add.at(dW, self.idx, dout)

 return

None

这里，我们使用 for 循环语句

将梯度累加到对应索引

上。这样一来，即

便 idx

中出现

了重复的索引，也能被正

确处理。此外，这里使用 for 循

环语

句的实现也可以通

过 NumPy 的

np.add.at() 进行。np.add.at(A, idx, B) 将

B

加到 A 上，此时

可以通过 idx 指定 A

中需要进

行加法的行。

通常情况下

，NumPy 的内置方法比 Python 的 for循环处

理更快。

这是因为 NumPy 的内置

方法在底层做了高速化

和提高处理效率的

优化

。因此，上面的代码如果使

用 np.add.at()来实现，效率会比

使用

for循环处理高得多。

关于 Embedding 层

的实现就介绍到这里。现

在，我们可以将 word2vec

（CBOW 模型）的实

现中的输入侧的

MatMul 层换成

Embedding 层。这样

一来，既能减少内

存使用量，又能避免不必

要的计算。

4.2 word2vec的改进②

下面，我

们来进行 word2vec 的第 2 个改进。如

前所述，word2vec 的另

一个瓶颈在

于中间层之后的处理，即

矩阵乘积和 Softmax 层的计算。本

节的

目标就是解决这个

瓶颈。这里，我们将采用名

为负采样（negative sampling）

的方法作为解

决方案。使用

Negative Sampling 替代 Softmax，无论词

汇量

有多大，都可以使计

算量保持较低或恒定。

本

节的内容有些复杂，特别

是实现方面会有点“纠结

”。因此，我们会

一个知识点

一个知识点地确认，一步

一步地前进。

  第 4章　word2vec的高速

化

138

4.2.1  中间层之后的计算问

题

为了指出中间层之后

的计算问题，和上一节一

样，我们来考虑词汇量为

100 万个、中间层的神经元个

数为

100 个的 wod2vec（CBOW 模型）。此时，

word2vec 进行

的处理如图

4-6 所示。

图4-6　词汇

量为100万个时的word2vec：上下文是

you和goodbye，目标词是say

you

goodbye

say

and

.

you

goodbye

say

and

.

you

goodbye

say

and

.

Win

(1 000 000×100)

Win

(1 000 000×100)

Wout

(100×1 000

000)

Softmax

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

ắ⢴

1

1

0

0

0

0

0

0

0

0

如

图 4-6 所示，输入层和输出层

有 100 万个神经元。在上一节

中，通过

引入

Embedding 层，节省了输

入层中不必要的计算。剩

下的问题就是中间

层之

后的处理。此时，在以下两

个地方需要很多计算时

间。

•　中间层的神经元和权

重矩阵（Wout）的乘积

•

Softmax 层的计算

4.2 word2vec的改进②  139

第

1 个问题在于巨

大的矩阵乘积计算。在上

面的例子中，中间层向量

的

大小是 100，权重矩阵的大

小是 100 ×

1 000 000 万，如此巨大的矩阵

乘积

计算需要大量时间

（也需要大量内存）。此外，因

为反向传播时也要进行

同

样的计算，所以很有必

要将矩阵乘积计算“轻量

化”。

其次，Softmax 也会发生同样的

问题。换句话说，随着词汇

量的增加，

Softmax 的计算量也会

增加。观察 Softmax 的公式，就可以

清楚地看出这

一点。

yk = exp(sk)

1 000

000



i=1

exp(si)

(4.1)

式 (4.1) 是

第 k 个元素（单词）的 Softmax

的计算

式（各个元素的得分

为 s1, s2, ···）。因

为假定词汇量是 100

万个，所

以式 (4.1) 的分母需要进行

100 万

次的 exp

计算。这个计算也与

词汇量成正比，因此，需要

一个可以替

代 Softmax 的“轻量”的

计算。

4.2.2

从多分类到二分类

下面，我们来解释一下负

采样。这个方法的关键思

想在于二分类

（binary classification），更准确地

说，是用二分类拟合多分

类（multiclass 

classification），这是理解负采样的重

点。

到目前为止，我们处理

的都是多分类问题。拿刚

才的例子来说，我们把

它

看作了从 100 万个单词中选

择 1 个正确单词的任务。那

么，可不可以将这

个问题

处理成二分类问题呢？更

确切地说，我们是否可以

用二分类问题来拟

合这

个多分类问题呢？

二 分 类

处 理 的

是 答 案 为“Yes/No”的 问 题

。诸

如，“这 个 数 字 是

7

吗？”“这是

猫吗？”“目标词是 say 吗？”等，这些

问题都可以用

“Yes/No”来回答。

第

4章　word2vec的高速化 140

到目前为止

，我们已经做到了当给定

上下文时，以较高的概率

预测出作

为正确解的单

词。比如，当给定 you

和 goodbye 时，使神

经网络预测出单

词 say 的概

率最高。如果学习进展顺

利，神经网络就可以进行

正确的预测。

换句话说，对

于“当上下文是 you 和 goodbye 时，目标

词是什么？”这个问

题，神经

网络可以给出正确答案

。

现在，我们来考虑如何将

多分类问题转化为二分

类问题。为此，我们先

考察

一个可以用“Yes/No”来回答的问

题。比如，让神经网络来回

答“当

上下文是 you 和 goodbye

时，目标

词是 say 吗？”这个问题，这时输

出层只

需要一个神经元

即可。可以认为输出层的

神经元输出的是 say 的得分

。

那么，此时 CBOW 模型进行什么

样的处理呢？用图来表示

的话，可以

画出图 4-7。

you

goodbye

say

and

.

you

goodbye

say

and

.

1

0

0

0

1

0

0

0

0

0

Win

(1 000 000×100)

Win

(1 000 000×100)

(100×1)

Wout[:, 1]

Sigmoid

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍

ᓄܳ喎

͚䬡ᅯ

say ᄦᏁ⮱݄ा䛼

ắ⢴

图4-7　仅计算目标词的得

分的神经网络

4.2

word2vec的改进②  141

如

图 4-7 所示，输出层的神经元

仅有一个。因此，要计算中

间层和输出

侧的权重矩

阵的乘积，只需要提取 say 对

应的列（单词向量），并用它

与中

间层的神经元计算

内积即可。这个计算的详

细过程如图 4-8 所示。

dot

Wout

(100×1 000 000)

h

(1×100)

s

(1×1) ᒏ⟣喟

图4-8　计

算say对应的列向量和中间

层的内积（图中的“dot”指内积

运算）

如图 4-8 所示，输出侧的

权重 Wout 中保存了各个单词

ID 对应的单词

向量。此处，我

们提取 say 这个单词向量，再

求这个向量和中间层神

经元的

内积，这就是最终

的得分。

到目前为止，输出

层是以全部单词为对象

进行计算的。这里，我们

仅

关注单词

say，计算它的得分

。然后，使用 sigmoid 函数将其转化

为概率。

4.2.3  sigmoid函数和交叉熵误

差

要使用神经网络解决

二分类问题，需要使用 sigmoid 函

数将得分转化为

概率。为

了求损失，我们使用交叉

熵误差作为损失函数。这

些都是二分类神

经网络

的老套路。

图灵社区会员

Kensuke(cpy4ever@gmail.com)

专享 尊重版权 you goodbye say

.

第 4章　word2vec的高

速化 142

在多分类的情况下

，输出层使用Softmax函数将得分

转化为概率，损

失函数使

用交叉熵误差。在二分类

的情况下，输出层使用sigmoid 函

数，

损失函数也使用交叉

熵误差。

这里我们先回顾

一下 sigmoid 函数，如下式所示：

y = 1

1 + exp(−x)

(4.2)

式

(4.2) 的图像如图 4-9 中的右图所

示。从图中可以看出，sigmoid 函

数

呈 S 形，输入值 x 被转化为 0

到

1 之间的实数。这里的要点

是，sigmoid

函数的输出 y 可以解释

为概率。

另外，与

sigmoid 函数相关

的 Sigmoid 层已经实现好了，如图

4-9 中的

左图所示。

x y y

x

L

y

y(1−y) L

y

Sigmoid

1.0

0.8

0.6

0.4

0.2

0.0

10 −10

0 5 −5

图4-9 Sigmoid层（左图

）和sigmoid函数（右图）的图像

通过

sigmoid

函数得到概率 y 后，可以由

概率 y 计算损失。与多分类

一

样，用于

sigmoid 函数的损失函

数也是交叉熵误差，其数

学式如下所示：

 L = −(t

log y + (1−t) log (1−y))

(4.3)

其中，y 是 sigmoid 函

数的输出，t 是正确解标签

，取值为

0 或 1：取值为 1

时表示

正确解是“Yes”；取值为0时表示

正确解是“No”。因此，当t为1时，

∂

∂

∂

∂

4.2 word2vec的

改进②

143

输出 −log y；当 t 为

0 时，输出 −log (1 − y)。

二

分类和多分类的损失函

数均为交叉熵误差，其数

学式分别为式

(4.3) 和式 (1.7)。不过

，它们只是写法不同而已

，实际上表示的

内容是一

样的。确切地说，在多分类

的情况下，如果输出层只

有

两个神经元，则式

(1.7) 和二

分类的式 (4.3) 是完全一致的

。因此，

Sigmoid with

Loss 层的实现只要在 Softmax with Loss 层

的基

础上稍加改动即可

。

下面，我们用图来表示 Sigmoid 层

和 Cross Entropy

Error 层，如图

4-10 所示。

x y

x

t t

y − t

L

y

L

x

L

x

1 1

Sigmoid

Cross 

Entropy

Error

Sigmoid 

with 

Loss

ᢌ๞

ᢌ๞

图4-10 Sigmoid 层和 Cross

Entropy Error 层

的计算图。右图整合为了

Sigmoid 

with Loss层

图 4-10 中值得注意的是反

向传播的 y − t

这个值。y 是神经

网络输出的

概率，t 是正确

解标签，y − t

正好是这两个值

的差。这意味着，当正确解

标

签是 1 时，如果 y 尽可能地

接近

1（100%），误差将很小。反过来

，如果 y

远离 1，误差将增大。随

后，这个误差向前面的层

传播，当误差大时，模型

学

习得多；当误差小时，模型

学习得少 A。

A 也就是说，误差

越大，模型参数的更新力

度就越大。——译者注

∂

∂

∂

∂

∂

∂

  第 4章

word2vec的

高速化 144

sigmoid 函数和交叉熵误

差的组合产生了 y −

t 这样一

个漂亮的结果。

同样地，Softmax 函

数和交叉熵误差的组合

，或者恒等函数和均方

误

差的组合也会在反向传

播时传播 y

− t。

4.2.4  多分类到二分

类的实现

下面，我们从实

现的角度把之前讲的内

容整理一下。前面我们处

理了

多分类问题，在输出

层使用了与词汇量同等

数量的神经元，并将它们

传给了

Softmax 层。如果把重点放

在“层”和“计算”上，则此时的

神经网络可以

画成图 4-11。

you

0 say

1

2

goodbye

Win

Embed

MatMul Softmax

Cross

Entropy

Error

Embed

× +

0.5

Wout

h

Win

ᢌ๞

图

4-11　进行多分类的CBOW模型的全

貌图（Embedding层记为Embed）

图 4-11

中展示了

上下文是 you 和 goodbye、作为正确解

的目标词是

say 的例子（假定

you

的单词 ID 是 0，say 的单词 ID

是 1，goodbye 的单

词 ID 是 2）。在输入层中，为了提

取单词

ID 对应的分布式表

示，使用了

Embedding 层。

4.2 word2vec的改进②

145

上一

节，我们实现了 Embedding 层，该层提

取单词 ID 对应的分

布式表

示（单词向量）。以前我们用

的是 MatMul 层。

现在，我们将图 4-11 中

的神经网络转化成进行

二分类的神经网络，网

络

结构如图 4-12 所示。

you

0

say

1

1

2

goodbye

Win

Win

Embed

dot

Sigmoid

with

Loss

Embed

Embed

0.5

Wout

h

ₐ⶛㼐ᴴカ

ᢌ๞

图4-12 进行二

分类的CBOW模型的全貌图

这

里，将中间层的神经元记

为 h，并计算它与输出侧权

重 Wout 中的单

词 say 对应的单词

向量的内积。然后，将其输

出输入 Sigmoid with Loss

层，

得到最终的损

失。

在图 4-12 中，向 Sigmoid

with Loss 层输入正确

解标签 1，这意

味着现在正

在处理的问题的答案是

“Yes”。当答案是“No”时，

向

Sigmoid with Loss 层输入 0。

为

了便于理解后面的内容

，我们把图

4-12 的后半部分进

一步简化。为

此，我们引入

Embedding Dot 层，该层将图 4-12

中的 Embedding 层和 dot

第

4章　word2vec的高速化 146

运算（内积）合

并起来处理。使用这个层

，图 4-12 的后半部分可以画成

图

4-13。

图4-13 只关注图4-12的中间层

之后的处理。使用Embedding Dot层合并

Embedding

层和内积运算

Embedding

Dot

Sigmoid

with

Loss

say

1

1

Wout

h

ₐ⶛㼐ᴴカ ᢌ๞

中间层的

神经元

h 流经 Embedding Dot 层，传给 Sigmoid

with Loss

层。从

图中可以看出，使用 Embedding Dot 层之

后，中间层之后的处理被

简化了。

下面，我们简单地

看一下 Embedding Dot 层的实现，这里我

们将这个

层实现为 EmbeddingDot

类（ ch04/negative_sampling_layer.py）。

class EmbeddingDot:

 def

__init__(self, W):

 self.embed = Embedding(W)

self.params = self.embed.params

 self.grads =

self.embed.grads

 self.cache = None

def forward(self, h, idx):

 target_W

= self.embed.forward(idx)

 out = np.sum(target_W

* h, axis=1)

 self.cache =

(h, target_W)

 return out

4.2

word2vec的

改进②  147

 def backward(self,

dout):

 h, target_W = self.cache

dout = dout.reshape(dout.shape[0], 1)

 dtarget_W

= dout * h

 self.embed.backward(dtarget_W)

dh = dout * target_W

return dh

EmbeddingDot 类共有 4 个成员变量

：embed、params、grads

和 cache。根

据本书的代码规范

，params 保存参数，grads 保存梯度。另外

，作为缓存，

embed

保存 Embedding 层，cache 保存正

向传播时的计算结果。

正

向传播的 forward(h,

idx) 方法的参数接

收中间层的神经元（h）和单

词 ID 的 NumPy 数组（idx）。这里，idx

是单词 ID 列

表，这是因为我们假

定了

对数据进行 mini-batch 处理。

在上面

的代码中，forward() 方法首先调用

Embedding 层的 forward(idx)

方法，然后通过 np.sum(self.

target_W * h, axis=1) 计算

内积。通过观察具

体值来

理解这个实现会比较快

，如图

4-14 所示。

embed = Embedding(W)

W

idx target_W h target_W * h

out = np.sum(target_W * h, axis=1)

target_W = embed.forward(idx)

[[ 0 1

2]

 [ 3 4 5]

[ 6 7 8]

 [

9 10 11]

 [12 13

14]

 [15 16 17]

[18 19 20]]

[[ 0 1

2]

 [ 9 10 11]

[ 3 4 5]]

[[0 1

2]

 [3 4 5]

[6 7 8]]

[[ 0 1

4]

 [27 40 55]

[18 28 40]]

[ 5 122

86] [0 3 1]

out

图4-14

Embedding Dot层中各个变

量的值

如图 4-14 所示，准备适

当的 W、h

和 idx。这里，idx 是 [0, 3, 1]，这

个例子

表示 mini-batch 一并处理 3 笔数据。因

为 idx

是 [0, 3, 1]，所以

target_W 将提取出

W 的第

0 行、第 3 行和第 1

行。另外，target_W * h 计算

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重

版权

  第 4章　word2vec的高速化

148

对应

元素的乘积（NumPy 的“*”计算对应

元素的乘积）。然后，对结果

逐

行（axis=1）进行求和，得到最终

的结果 out。

以上就是对

Embedding Dot 层的

正向传播的介绍。反向传

播以相反的

顺序传播梯

度，这里我们省略对其实

现的说明（并不是特别难

，请大家自己

思考）。

4.2.5

负采样

至此，我们成功地把要解

决的问题从多分类问题

转化成了二分类问题。

但

是，这样问题就被解决了

吗？很遗憾，事实并非如此

。因为我们目前仅学

习了

正例（正确答案），还不确定

负例（错误答案）会有怎样

的结果。

现在，我们再来思

考一下之前的例子。在之

前的例子中，上下文是 you

和

goodbye，目标词是

say。我们到目前为

止只是对正例 say 进行了二

分类，

如果此时模型有“好

的权重”，则 Sigmoid 层的输出（概率

）将接近

1。用

计算图来表示

此时的处理，如图 4-15 所示。

图

4-15 CBOW模型的中间层之后的处

理：上下文是you和goodbye，此时目标

词是

say的概率为0.993（99.3%）

say

1

h

Wout

Embedding

Dot Sigmoid 0.993

当前的神

经网络只是学习了正例

say，但是对 say 之外的负例一无

所知。

而我们真正要做的

事情是，对于正例（say），使 Sigmoid 层的

输出接近 1；

对于负例（say 以外

的单词），使

Sigmoid 层的输出接近

0。用图来表示，如

图 4-16 所示。

图

灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

4.2 word2vec的改进②

149

图4-16 如果say是正例

（正确答案），则当输入say时使

Sigmoid层的输出接近1，当输

入say以

外的单词时使输出接近

0，我们要求的是这样的权

重

say

1

you

0

hello

6

0.993

0.003

0.021

h

h h

Embedding

Dot

Sigmoid

Embedding

Dot Sigmoid Embedding

Dot

Sigmoid

Wout

Wout Wout

ₐҸ

䉌Ҹ

比如，当上下文是 you 和 goodbye 时

，我们希望目标词是 hello（错误

答案）的概率较低。在图

4-16 中

，目标词是 hello 的概率为 0.021（2.1%），

我们

要求的就是这种能使输

出接近

0 的权重。

为了把多

分类问题处理为二分类

问题，对于“正确答案”（正例

）和“错

误答案”（负例），都需要

能够正确地进行分类（二

分类）。因此，需要

同时考虑

正例和负例。

那么，我们需

要以所有的负例为对象

进行学习吗？答案显然是

“No”。

如果以所有的负例为对

象，词汇量将暴增至无法

处理（更何况本章的目的

本

来就是解决词汇量增

加的问题）。为此，作为一种

近似方法，我们将选择若

干个（5 个或者 10 个）负例（如何

选择将在下文介绍）。也就

是说，只使用

少数负例。这

就是负采样方法的含义

。

总而言之，负采样方法既

可以求将正例作为目标

词时的损失，同时也可

以

采样（选出）若干个负例，对

这些负例求损失。然后，将

这些数据（正例

和采样出

来的负例）的损失加起来

，将其结果作为最终的损

失。

下面，让我们结合具体

的例子来说明。这里使用

与之前相同的例子（正

例

目标词是 say）。假设选取

2 个负

例目标词 hello 和 i，此时，如果我

们只

第 4章　word2vec的高速化 150

关注

CBOW 模型的中间层之后的部

分，则负采样的计算图如

图

4-17 所示。

say

1

hello

5

0

1

h Embedding

Dot

Sigmoid

with

Loss

Embedding

Dot

Sigmoid

with

Loss

Embedding

Dot

Sigmoid

with

Loss

Wout

Wout

Wout

i

4 0

ₐ⶛㼐ᴴカ

ₐ⶛㼐ᴴカ

ₐ⶛㼐ᴴカ

ᢌ๞

图4-17　负采样的例子

（只关注中间层之后的处

理，画出基于层的计算图

）

图 4-17 中需要注意的是对正

例和负例的处理。正例（say）和

之前一样，

向 Sigmoid with

Loss 层输入正确

解标签 1；而因为负例（hello 和 i）是

错

误答案，所以要向

Sigmoid with Loss 层输

入正确解标签 0。此后，将各

个

数据的损失相加，作为

最终损失输出。

图灵社区

会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

4.2 word2vec的改

进②

151

4.2.6  负采样的采样方法

下

面我们来看一下如何抽

取负例。关于这一点，基于

语料库的统计数据

进行

采样的方法比随机抽样

要好。具体来说，就是让语

料库中经常出现的单

词

容易被抽到，让语料库中

不经常出现的单词难以

被抽到。

基于语料库中单

词使用频率的采样方法

会先计算语料库中各个

单词的出

现次数，并将其

表示为“概率分布”，然后使

用这个概率分布对单词

进行采

样（图 4-18）。

“i”

“you”

“you”

“.”

“goodbye”

ắ⢴ܳጰ

䛴ᵤ

图4-18　根据概率

分布多次进行采样的例

子

基于语料库中各个单

词的出现次数求出概率

分布后，只需根据这个概

率

分布进行采样就可以

了。通过根据概率分布进

行采样，语料库中经常出

现的

单词将容易被抽到

，而“稀有单词”将难以被抽

到。

负采样应当尽可能多

地覆盖负例单词，但是考

虑到计算的复杂度，

有必

要将负例限定在较小范

围内（5 个或者 10 个）。这里，如果

只选

择稀有单词作为负

例会怎样呢？结果会很糟

糕。因为在现实问题中，

稀

有单词基本上不会出现

。也就是说，处理稀有单词

的重要性较低。

相反，处理

好高频单词才能获得更

好的结果。

下面，我们使用

Python 来说明基于概率分布的

采样。为此，可以使用

NumPy 的 np.random.choice()

方

法。这里我们结合几个具

体的例子来看一

下这个

方法的用法。

hello

.

i

you

say

and

goodbye

  第

4章　word2vec的高速

化 152

>>> import numpy

as np

# 从0到9的数字中随机选

择一个数字

>>> np.random.choice(10)

7

>>> np.random.choice(10)

2

# 从words列表中随

机选择一个元素

>>> words = ['you', 'say', 'goodbye',

'I', 'hello', '.']

>>> np.random.choice(words)

'goodbye'

# 有放回

采样5次

>>> np.random.choice(words, size=5)

array(['goodbye',

'.', 'hello', 'goodbye', 'say'],

 dtype='<U7')

# 无放回采样5次

>>> np.random.choice(words, size=5, replace=False)

array(['hello', '.', 'goodbye', 'I', 'you'],

dtype='<U7')

# 基

于概率分布进行采样

>>> p =

[0.5, 0.1, 0.05, 0.2, 0.05, 0.1]

>>> np.random.choice(words, p=p)

'you'

如

上所示，np.random.choice() 可以用于随机抽

样。如果指定

size 参数，

将执行

多次采样。如果指定 replace=False，将进

行无放回采样。通过给参

数 p 指定表示概率分布的

列表，将进行基于概率分

布的采样。剩下的就是使

用这个函数抽取负例。

word2vec 中

提出的负采样对刚才的

概率分布增加了一个步

骤。如式 (4.4)

所示，对原来的概

率分布取 0.75 次方。

4.2 word2vec的改进②  153

P

(wi) = n

P(wi)

0.75

j

P(wj )0.75

(4.4)

这

里，P(wi) 表示第

i 个单词的概率

。式 (4.4) 只是对原来的概率分

布的各

个元素取 0.75

次方。不

过，为了使变换后的概率

总和仍为 1，分母需要变成

“变换后的概率分布的总

和”。

那么，为什么我们要进

行式 (4.4) 的变换呢？这是为了

防止低频单词被

忽略。更

准确地说，通过取

0.75 次方，低

频单词的概率将稍微变

高。我们

来看一个具体例

子，如下所示。

>>> p =

[0.7, 0.29, 0.01]

>>> new_p =

np.power(p, 0.75)

>>> new_p /= np.sum(new_p)

>>> print(new_p)

[ 0.64196878 0.33150408 0.02652714]

根据这个例

子，变换前概率为 0.01（1%）的元素

，变换后为 0.026 ...

（2.6 ...%）。通过这种方式

，取

0.75 次方作为一种补救措

施，使得低频单词

稍微更

容易被抽到。此外，0.75 这个值

并没有什么理论依据，也

可以设置成

0.75 以外的值。

如

上所示，负采样从语料库

生成单词的概率分布，在

取其 0.75 次方之

后，再使用之

前的 np.random.choice() 对负例进行采样。本

书中将这些处理

实现为

了 UnigramSampler 类。这里仅简单说明 UnigramSampler 类

的使用方法，

其具体实现

在

ch04/negative_sampling_layer.py 中，对细节感兴趣的读

者可

以参考一下。

unigram 是“1 个（连

续）单

词”的 意 思。同 样 地，bigram 是

“2

个 连 续 单 词”的 意

思，trigram 是“3 个

连 续 单 词”的

意 思。这 里

使

用 UnigramSampler这个名字，是因为我们

以 1

个单词为对象创

建 概

率 分 布。如 果

是 bigram，则 以 ‘( you’, ‘say’)、‘(

you’, 

‘goodbye’)……这样

的 2 个单词的组合为对象

创建概率分布。

第 4章　word2vec的高

速化 154

在进行初始化时，UnigramSampler 类

取

3 个参数，分别是单词 ID 列

表

格式的 corpus、对概率分布取

的次方值

power（默认值是 0.75）和负

例的采

样个数 sample_size。UnigramSampler 类有 get_negative_sample(target)

方法

，

该方法以参数 target 指定的单

词 ID 为正例，对其他的单词

ID

进行采样。

下面，我们来看

一个简单的 UnigramSampler 类的使用示

例。

corpus =

np.array([0, 1, 2, 3, 4, 1,

2, 3])

power = 0.75

sample_size

= 2

sampler = UnigramSampler(corpus, power,

sample_size)

target = np.array([1, 3, 0])

negative_sample = sampler.get_negative_sample(target)

print(negative_sample)

# [[0

3]

# [1 2]

# [2

3]]

这里，考虑将 [1, 3, 0] 这

3 个数据

的 mini-batch 作为正例。此时，对

各个

数据采样 2

个负例。在上面

的例子中，可知第 1 个数据

的负例是 [0, 3]，

第

2 个是 [1, 2]，第 3 个是

[2,

3]。这样一来，我们就完成了

负采样。

4.2.7  负采样的实现

最

后，我们来实现负采样。我

们把它实现为 NegativeSamplingLoss

类，

首先从

初始化开始（ ch04/negative_sampling_layer.py）。

class NegativeSamplingLoss:

def __init__(self, W, corpus, power=0.75, sample_size=5):

self.sample_size = sample_size

 self.sampler =

UnigramSampler(corpus, power, sample_size)

 self.loss_layers =

[SigmoidWithLoss() for _ in range(sample_size

+

1)]

 self.embed_dot_layers = [EmbeddingDot(W) for

_ in

range(sample_size + 1)]

图灵社区会

员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

4.2 word2vec的改进

②

155

 self.params, self.grads = [],

[]

 for layer in self.embed_dot_layers:

self.params += layer.params

 self.grads +=

layer.grads

初始化的参数有表示输

出侧权重的 W、语料库（单词

ID 列表）corpus、

概率分布的次方值

power 和负例的采样数

sample_size。这里，生

成上一节所

说的 UnigramSampler 类，并使

用成员变量 sampler 保存。另外，将

负例的采

样数设置为成

员变量 sample_size。

成员变量 loss_layers 和 embed_dot_layers

中以

列表格式保存了必要的

层。在这两个列表中生成

sample_size + 1 个层，这是因为需要生成

一个正

例用的层和 sample_size

个负

例用的层。这里，我们假设

列表的第一个层处

理正

例。也就是说，loss_layers[0] 和 embed_dot_layers[0] 是处理正

例的层。

然后，将

embed_dot_layers 层使用的

权重和梯度分别保存在

数组中。下面，

我们给出正

向传播的实现（ ch04/negative_sampling_layer.py）。

def forward(self,

h, target):

 batch_size = target.shape[0]

negative_sample = self.sampler.get_negative_sample(target)

 # 正例的正

向传播

score = self.embed_dot_layers[0].forward(h, target)

 correct_label

= np.ones(batch_size, dtype=np.int32)

 loss =

self.loss_layers[0].forward(score, correct_label)

 # 负例的正向传播

negative_label

= np.zeros(batch_size, dtype=np.int32)

 for i

in range(self.sample_size):

 negative_target = negative_sample[:,

i]

 score = self.embed_dot_layers[1 +

i].forward(h, negative_target)

 loss += self.loss_layers[1

+ i].forward(score, negative_label)

 return loss

forward(h, target) 方法接收的参数是中间

层的神经元 h 和正例目标

词 target。这里进行的处理是，首

先使用

self.sampler 采样负例，并设为

第 4章　word2vec的高速化

156

negative_sample。然后，分别

对正例和负例的数据进

行正向传播，求损失的

和

。具体而言，通过 Embedding Dot 层的

forward 输出

得分，再将这个得

分和标

签一起输入 Sigmoid with Loss

层来计算损

失。这里需要注意的是，

正

例的正确解标签为 1，负例

的正确解标签为 0。

最后，我

们来看反向传播的实现

。

def

backward(self, dout=1):

 dh = 0

for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):

dscore = l0.backward(dout)

 dh +=

l1.backward(dscore)

 return dh

反向传播的实现非常简

单，只需要以与正向传播

相反的顺序调用各层的

backward() 函数即可。在正向传播时

，中间层的神经元被复制

了多份，这相

当于 1.3.4.3 节中介

绍的 Repeat 节点。因此，在反向传

播时，需要将多份梯

度累

加起来。以上就是负采样

的实现的说明。

4.3 改进版 word2vec的

学习

到目前为止，我们进

行了 word2vec 的改进。首先说明了

Embedding

层，又介绍了负采样的方

法，然后对这两者进行了

实现。现在我们进一步来

实现进行了这些改进的

神经网络，并在 PTB 数据集上

进行学习，以获得更

加实

用的单词的分布式表示

。

4.3.1

CBOW模型的实现

这里，我们将

改进上一章的简单的 SimpleCBOW类

，来实现 CBOW 模型。

改进之处在

于使用

Embedding 层和 Negative Sampling Loss 层。此外，我

们

将上下文部分扩展为可

以处理任意的窗口大小

。

改进版的 CBOW 类的实现如下

所示。首先，我们来看一下

初始化方法

（ ch04/cbow.py）。

4.3 改进版word2vec的学

习  157

import sys

sys.path.append('..')

import numpy as np

from

common.layers import Embedding

from ch04.negative_sampling_layer import

NegativeSamplingLoss

class CBOW:

 def __init__(self,

vocab_size, hidden_size, window_size, corpus):

 V,

H = vocab_size, hidden_size

 #

初始化权重

 W_in = 0.01 *

np.random.randn(V, H).astype('f')

 W_out = 0.01

* np.random.randn(V, H).astype('f')

 # 生成层

self.in_layers = []

 for i

in range(2 * window_size):

 layer

= Embedding(W_in) # 使

用Embedding层

 self.in_layers.append(layer)

self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75,

sample_size=5)

 # 将所有的权重和梯

度整理到列表中

 layers

= self.in_layers + [self.ns_loss]

 self.params,

self.grads = [], []

 for

layer in layers:

 self.params +=

layer.params

 self.grads += layer.grads

# 将单词

的分布式表示设置为成

员变量

 self.word_vecs = W_in

这个初始化方法

有 4 个参数。vocab_size 是词汇量，hidden_size 是中

间

层的神经元个数，corpus

是单

词 ID 列表。另外，通过 window_size 指定上

下

文的大小，即上下文包

含多少个周围单词。如果

window_size

是 2，则目标

词的左右 2 个单

词（共 4

个单词）将成为上下

文。

  第 4章　word2vec的高速化

158

在 SimpleCBOW类（改

进前的实现）中，输入侧的

权重和输出侧的

权重的

形状不同，输出侧的权重

在列方向上排列单词向

量。而

CBOW类的输出侧的权重

和输入侧的权重形状相

同，都在行方向

上排列单

词向量。这是因为

NegativeSamplingLoss类中使

用了

Embedding 层。

在权重的初始化

结束后，继续创建层。这里

，创建 2 *

window_size 个

Embedding 层，并将其保存在

成员变量 in_layers 中。然后，创建

Negative 

Sampling Loss 层

。

在创建好层之后，将神经

网络中使用的参数和梯

度放入成员变量

params

和 grads 中。另

外，为了之后可以访问单

词的分布式表示，将权重

W_in 设置

为成员变量

word_vecs。下面，我

们来看一下正向传播的

forward() 方法和反

向传播的 backward() 方法

（ ch04/cbow.py）。

def forward(self, contexts, target):

 h

= 0

 for i, layer

in enumerate(self.in_layers):

 h += layer.forward(contexts[:,

i])

 h *= 1 /

len(self.in_layers)

 loss = self.ns_loss.forward(h, target)

return loss

def backward(self, dout=1):

dout = self.ns_loss.backward(dout)

 dout *=

1 / len(self.in_layers)

 for layer

in self.in_layers:

 layer.backward(dout)

 return

None

这里的实现只是按适当

的顺序调用各个层的正

向传播（或反向传播），

这是

对上一章的 SimpleCBOW 类的自然扩

展。不过，虽然 forward

(contexts, 

target) 方法取的参

数仍是上下文和目标词

，但是它们是单词 ID 形式的

（上

一章中使用的是 one-hot 向量

，不是单词 ID），具体示例如图

4-19 所示。

4.3

改进版word2vec的学习  159

you, goodbye

say,

and

goodbye, i 

and, say

i, hello

say, .

say

say

and

goodbye 

i

hello

contexts

target contexts target

[1 3]

[2

4]

[3 1]

[4 5]

[[0

2]

[1 6]]

[1

5]

2

3

4

1

䃺ࢂ

ID

图4-19

用

单词ID表示上下文和目标

词的例子：这里显示的是

窗口大小为1的上下文

图

4-19 的右侧显示的单词 ID 列表

是 contexts

和 target 的例子。可以

看出，contexts 是

一个二维数组，target 是一个一

维数组，这样的数据被输

入

forward(contexts, target) 中。以上就是 CBOW 类的说明

。

4.3.2

CBOW模型的学习代码

最后，我

们来实现 CBOW 模型的学习部

分。其实只是复用一下神

经网

络的学习，如下所示

（ ch04/train.py）。

import sys

sys.path.append('..')

import numpy as

np

from common import config

#

在用GPU运行时，请打开下面

的注释（需要cupy）

# ===============================================

# config.GPU =

True

# ===============================================

import pickle

from

common.trainer import Trainer

from common.optimizer import

Adam

from cbow import CBOW

from

common.util import create_contexts_target, to_cpu, to_gpu

from

dataset import ptb

# 设定超参数



第 4章　word2vec的高速化 160

window_size =

5

hidden_size = 100

batch_size =

100

max_epoch = 10

# 读入数据

corpus,

word_to_id, id_to_word = ptb.load_data('train')

vocab_size =

len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)

if config.GPU:

 contexts, target =

to_gpu(contexts), to_gpu(target)

# 生成模型等

model =

CBOW(vocab_size, hidden_size, window_size, corpus)

optimizer =

Adam()

trainer = Trainer(model, optimizer)

#

开始学习

trainer.fit(contexts, target, max_epoch, batch_size)

trainer.plot()

# 保

存必要数据，以便后续使

用

word_vecs = model.word_vecs

if

config.GPU:

 word_vecs = to_cpu(word_vecs)

params

= {}

params['word_vecs'] = word_vecs.astype(np.float16)

params['word_to_id']

= word_to_id

params['id_to_word'] = id_to_word

pkl_file

= 'cbow_params.pkl'

with open(pkl_file, 'wb') as

f:

 pickle.dump(params, f, -1)

本次的

CBOW 模型的窗口大

小为 5，隐藏层的神经元个

数为 100。虽

然具体取决于语

料库的情况，但是一般而

言，当窗口大小为 2

～ 10、中间

层

的神经元个数（单词的分

布式表示的维数）为50～500时，结

果会比较好。

稍后我们会

对这些超参数进行讨论

。

4.3 改进版word2vec的学习

161

这次我们

利用的 PTB 语料库比之前要

大得多，因此学习需要很

长时间

（半天左右）。作为一

种选择，我们提供了使用

GPU 运行的模式。如果要使

用

GPU 运行，需要打开顶部的“# config.GPU = True”。不

过，使用 GPU

运行需要有一台

安装了 NVIDIA GPU 和 CuPy 的机器。

在学习

结束后，取出权重（输入侧

的权重），并保存在文件中

以备后用

（用于单词和单

词 ID 之间的转化的字典也

一起保存）。这里，使用 Python

的

pickle 功

能进行文件保存。pickle 可以将

Python 代码中的对象保存到文

件中（或者从文件中读取

对象）。

ch04/cbow_params.pkl中提供了学习好的

参数。如果不想等学习

结

束，可以使用本书提供的

学习好的参数。根据学习

环境的不同，

学习到的权

重数据也不一样。这是由

权重初始化时用到的随

机初

始值、mini-bath 的随机选取，以

及负采样的随机抽样造

成的。

因为这些随机性，最

后得到的权重在各自的

环境中会不一样。不

过宏

观来看，得到的结果（趋势

）是类似的。

4.3.3

CBOW模型的评价

现

在，我们来评价一下上一

节学习到的单词的分布

式表示。这里我们

使用第

2 章中实现的 most_similar() 函数，显示几

个单词的最接近的单词

（

ch04/eval.py）。

import sys

sys.path.append('..')

from common.util

import most_similar

import pickle

pkl_file =

'cbow_params.pkl'

with open(pkl_file, 'rb') as f:

params = pickle.load(f)

 word_vecs =

params['word_vecs']

 word_to_id = params['word_to_id']

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重

版权

  第

4章　word2vec的高速化 162

 id_to_word =

params['id_to_word']

querys = ['you', 'year', 'car',

'toyota']

for query in querys:

most_similar(query, word_to_id, id_to_word, word_vecs, top=5)

运行

上面的代码，可以得以下

结果（具体结果会根据各

自的学习环境而

有所差

异）。

[query] you

 we: 0.610597074032

someone: 0.591710150242

 i: 0.554366409779

something: 0.490028560162

 anyone: 0.473472118378

[query]

year

 month: 0.718261063099

 week:

0.652263045311

 spring: 0.62699586153

 summer:

0.625829637051

 decade: 0.603022158146

[query] car

luxury: 0.497202396393

 arabia: 0.478033810854

auto: 0.471043765545

 disk-drive: 0.450782179832

travel: 0.40902107954

[query] toyota

 ford:

0.550541639328

 instrumentation: 0.510020911694

 mazda:

0.49361255765

 bethlehem: 0.474817842245

 nissan:

0.474622786045

我们看一下结果。首先

，在查询 you 的情况下，近似单

词中出现了人

称代词 i（=

I）和

we 等。接着，查询 year，可以看到 month、week 等

表

4.3

改进版word2vec的学习  163

示时间

区间的具有相同性质的

单词。然后，查询 toyota，可以得到

ford、

mazda

和 nissan 等表示汽车制造商的

词汇。从这些结果可以看

出，由

CBOW 模型获得的单词的

分布式表示具有良好的

性质。

此外，由

word2vec 获得的单词

的分布式表示不仅可以

将近似单词聚

拢在一起

，还可以捕获更复杂的模

式，其中一个具有代表性

的例子是因

“king − man

+ woman = queen”而出名的类

推问题（类比问题）。更准确

地说，使用 word2vec 的单词的分布

式表示，可以通过向量的

加减法来解决

类推问题

。

如图 4-20 所示，要解决类推问

题，需要在单词向量空间

上寻找尽可能

使“man →

woman”向量和

“king → ?”向量接近的单词。

woman

?

man

king

图4-20 借助

“man : woman =

king : ?”这个类推问题，展示各个

单词在单词向量空

间上

的相关性

这里用 vec(‘man’)

表示单

词 man 的分布式表示（单词向

量）。如此一来，

图 4-20 中要求的

关联性可以用数学式表

示为

vec(‘woman’) − vec(‘man’) = 

vec(?)

− vec(‘king’)。将其变形，有 vec(‘king’) + vec(‘woman’) −

vec(‘man’) = 

vec(?)。也就是

说，我们的任务是找到离

向量 vec(‘king’) +

vec(‘woman’) −

vec(‘man’) 最近的单词向量。本

书在 common/util.py 中提供了实现此逻

辑

的函数 analogy()。使用这个函数

，可以用 analogy('man', 'king', 'woman',

word_to_id, id_to_word, word_vecs, top=5) 这样 1

行代码来回

答刚才的类推

问题。此时

，输出的结果如下所示。

  第

4章　word2vec的高速化

164

[analogy] man:king = woman:?

word1: 5.003233

 word2: 4.400302

word3: 4.22342

 word4: 4.003234

word5: 3.934550

第 1 行显示的

是问题，之后按照得分从

高到低的顺序输出 5

个单

词，得

分显示在各个单词

的旁边。现在，我们来实际

解决几个类推问题。下面

是 4

个问题（ ch04/eval.py）。

analogy('king',

'man', 'queen', word_to_id, id_to_word, word_vecs)

analogy('take',

'took', 'go', word_to_id, id_to_word, word_vecs)

analogy('car',

'cars', 'child', word_to_id, id_to_word, word_vecs)

analogy('good',

'better', 'bad', word_to_id, id_to_word, word_vecs)

执行这些代码

，可以得到如下结果。

[analogy] king:man = queen:?

 woman:

5.161407947540283

 veto: 4.928170680999756

 ounce:

4.689689636230469

 earthquake: 4.633471488952637

 successor:

4.6089653968811035

[analogy] take:took = go:?

went: 4.548568248748779

 points: 4.248863220214844

began: 4.090967178344727

 comes: 3.9805688858032227

oct.: 3.9044761657714844

[analogy] car:cars = child:?

children: 5.217921257019043

 average: 4.725458145141602

yield: 4.208011627197266

 cattle: 4.18687629699707

priced: 4.178797245025635

4.4 wor2vec相关

的其他话题  165

[analogy] good:better = bad:?

 more:

6.647829532623291

 less: 6.063825607299805

 rather:

5.220577716827393

 slower: 4.733833312988281

 greater:

4.672840118408203

结果符合我

们的预期。第 1 个问题是“king : man

= queen : ?”，这

里

正确地回答了“woman”。第 2

个问

题是“take : took = go :

?”，也按预期

回答了“went”。这

是捕获了现在时和过去

时之间的模式的证据，可

以解释

为单词的分布式

表示编码了时态相关的

信息。从第 3 题可知，单词的

单数形

式和复数形式之

间的模式也被正确地捕

获。可惜的是，对于第

4 题“good : 

better =

bad : ?”，并

没能回答出“worse”。不过，看到 more、less 等

比较

级的单词出现在回

答中，说明这些性质也被

编码在了单词的分布式

表示中。

像这样，使用 word2vec 获得

的单词的分布式表示，可

以通过向量的加

减法求

解类推问题。不仅限于单

词的含义，它也捕获了语

法中的模式。另

外，我们还

在 word2vec

的单词的分布式表示

中发现了一些有趣的结

果，比

如 good 和 best 之间存在

better 这样

的关系。

这里的类推问题

的结果看上去非常好。不

过遗憾的是，这是笔者

特

意选出来的能够被顺利

解决的问题。实际上，很多

问题都无法

获得预期的

结果。这是因为 PTB

数据集的

规模还是比较小。如

果使

用更大规模的语料库，可

以获得更准确、更可靠的

单词的分

布式表示，从而

大大提高类推问题的准

确率。

4.4 wor2vec相关的其他话题

关

于

word2vec 的机制和实现，我们差

不多都介绍完了。本节我

们来讨

论一些有关 word2cev 的其

他话题。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

  第

4章　word2vec的高速

化 166

4.4.1  word2vec的应用例

使用 word2vec 获得的

单词的分布式表示可以

用来查找近似单词，但是

单词的分布式表示的好

处不仅仅在于此。在自然

语言处理领域，单词的分

布

式表示之所以重要，原

因就在于迁移学习（transfer learning）。迁移

学习是

指在某个领域学

到的知识可以被应用于

其他领域。

在解决自然语

言处理任务时，一般不会

使用 word2vec 从零开始学习单

词

的分布式表示，而是先在

大规模语料库（Wikipedia、Google News 等文

本数

据）上学习，然后将学习好

的分布式表示应用于某

个单独的任务。比如，

在文

本分类、文本聚类、词性标

注和情感分析等自然语

言处理任务中，第一

步的

单词向量化工作就可以

使用学习好的单词的分

布式表示。在几乎所有类

型的自然语言处理任务

中，单词的分布式表示都

有很好的效果！

单词的分

布式表示的优点是可以

将单词转化为固定长度

的向量。另外，

使用单词的

分布式表示，也可以将文

档（单词序列）转化为固定

长度的向

量。目前，关于如

何将文档转化为固定长

度的向量，相关研究已经

进行了很

多，最简单的方

法是，把文档的各个单词

转化为分布式表示，然后

求它们的

总和。这是一种

被称为 bag-of-words 的不考虑单词顺

序的模型（思想）。此

外，使用

即将在第 5

章中说明的循

环神经网络，可以以更加

优美的方式利用

word2vec 的单词

的分布式表示来将文档

转化为固定长度的向量

。

将单词和文档转化为固

定长度的向量是非常重

要的。因为如果可以将

自

然语言转化为向量，就可

以使用常规的机器学习

方法（神经网络、SVM

等），如图

4-21 所

示。

めᵵ

䬛䷅

喍㜗♣䄚㼭喎

ࡃ䃺ा䛼ࢂ

喍

word2vec喎

ᱧக႓΍㈨㐌 喍⺋㏼㑾㐉Ƞ SVMぶ喎

图4-21

使用

了单词的分布式表示的

系统的处理流程

由图 4-21 可

知，如果可以将自然语言

书写的问题转化为固定

长度的向

4.4 wor2vec相关的其他话

题

167

量，就可以将这个向量

作为其他机器学习系统

的输入。通过将自然语言

转化

为向量，可以利用常

规的机器学习框架输出

目标答案（包括它的学习

）。

在图 4-21 的流程中，单词的分

布式表示的学习和机器

学习系统的学

习通常使

用不同的数据集独立进

行。比如，单词的分布式表

示使用

Wikipedia 等通用语料库预

先学习好，然后机器学习

系统（SVM 等）

再使用针对当前

问题收集到的数据进行

学习。但是，如果当前我们

面对的问题存在大量的

学习数据，则也可以考虑

从零开始同时进行

单词

的分布式表示和机器学

习系统的学习。

下面让我

们结合具体的例子来说

明一下单词的分布式表

示的使用方法。

假设你现

在开发并维护着一个拥

有超过 1 亿用户的智能手

机应用，你的公司

每天都

要收到堆积如山的用户

邮件（在 Twitter

等上也有很多吐

槽）。虽然

有一部意见是积

极的，但是也存在很多表

达不满的用户意见。

为此

，你考虑开发一个可以对

用户发来的邮件（吐槽等

）自动进行分类

的系统。如

图 4-22 所示，你想根据邮件的

内容将用户情感分为

3 类

。如果

可以正确地对用户

情感进行分类，就可以按

序浏览表达不满的用户

邮件。如

此一来，或许可以

发现应用的致命问题，并

尽早采取应对措施，从而

提高用

户的满意度。

ࡃ䃺ा䛼ࢂ

喍

word2vec喎

⺋㏼㑾㐉

图4-22　邮件的自动分类系统

（情感分析）的例子

要开发

邮件自动分类系统，首先

需要从收集数据（邮件）开

始。在这

个例子中，我们收

集用户发送的邮件，并人

工对邮件进行标注，打上

表示

3

类情感的标签（positive/neutral/negative）。标注

工作结束后，用学习好的

word2vec 将邮件转化为向量。然后

，将向量化的邮件及其情

感标签输入某

个情感分

类系统（SVM 或神经网络等）进

行学习。

第 4章　word2vec的高速化 168

如

本例所示，可以基于单词

的分布式表示将自然语

言处理问题转化为

向量

，这样就可以利用常规的

机器学习方法来解决问

题。另外，这样也能从

word2vec 的迁

移学习中受益。换句话说

，利用 word2vec 的单词的分布式表

示，可以期待大多数自然

语言处理任务获得精度

上的提高。

4.4.2

单词向量的评

价方法

使用 word2vec，我们得到了

单词的分布式表示。那么

，我们应该如何

评价分布

式表示的优劣呢？本节我

们将简单说明分布式表

示的评价方法。

正如上述

情感分析的例子那样，在

现实世界中，单词的分布

式表示

往往被用在具体

的应用中。我们最终想要

的是一个高精度的系统

。这里我

们必须考虑到的

是，这个系统（比如情感分

析系统）是由多个子系统

组成

的。所谓多个子系统

，拿刚才的例子来说，包括

生成单词的分布式表示

的系统（word2vec）、对特定问题进行

分类的系统（比如进行情

感分类的

SVM 等）。

单词的分布

式表示的学习和分类系

统的学习有时可能会分

开进行。在这

种情况下，如

果要调查单词的分布式

表示的维数如何影响最

终的精度，首先需

要进行

单词的分布式表示的学

习，然后再利用这个分布

式表示进行另一个机器

学习系统的学习。换句话

说，在进行两个阶段的学

习之后，才能进行评价。在

这种情况下，由于需要调

试出对两个系统都最优

的超参数，所以非常费时

。

因此，单词的分布式表示

的评价往往与实际应用

分开进行。此时，经常

使用

的评价指标有“相似度”和

“类推问题”。

单词相似度的

评价通常使用人工创建

的单词相似度评价集来

评估。比

如，cat 和

animal 的相似度是

8，cat 和 car 的相似度是 2……类似这样

，

用 0 ～ 10 的分数人工地对单词

之间的相似度打分。然后

，比较人给出的分

数和

word2vec 给

出的余弦相似度，考察它

们之间的相关性。

类推问

题的评价是指，基于诸如

“king : queen =

man : ?”这样的类

推问题，根据正

确率测量单词的分布式

表示的优劣。比如，论文 [27] 中

给出

了一个类推问题的

评价结果，其部分内容如

图 4-23 所示。

4.4 wor2vec相关的其他话题

169

CBOW

skip-gram

Semantics Syntax Total

300

300

16.1

61

63.6

73.0

57.3

66.1

52.6

61

67.4

66.0

68.9

65.1

36.1

61

65.7

69.1

63.7

65.6

16 Ϭ

10 Ϭ

䄚᫆Ꮐ⮱๔ᄼ

60 Ϭ

60 Ϭ

60 Ϭ

60 Ϭ

300

300

1000

1000

CBOW

skip-gram

CBOW

skip-gram

὎ಸ 㐡᪝

图4-23　基于类推问题的单词

向量的评价结果（参考论

文[27]）

在图 4-23 中，以 word2vec

的模型、单词

的分布式表示的维数和

语料库

的大小为参数进

行了比较实验，结果在右

侧的 3 列中。图 4-23 的

Semantics

列显示的

是推断单词含义的类推

问题（像“king : queen = actor

: actress”

这样询问单词含

义的问题）的正确率，Syntax 列是

询问单词形态信息的问

题，比如“bad : worst

= good : best”。

由图 4-23

可知：

•　模型不

同，精度不同（根据语料库

选择最佳的模型）

•　语料库

越大，结果越好（始终需要

大数据）

•

单词向量的维数

必须适中（太大会导致精

度变差）

基于类推问题可

以在一定程度上衡量“是

否正确理解了单词含义

或语法

问题”。因此，在自然

语言处理的应用中，能够

高精度地解决类推问题

的单

词的分布式表示应

该可以获得好的结果。但

是，单词的分布式表示的

优劣对

目标应用贡献多

少（或者有无贡献），取决于

待处理问题的具体情况

，比如

应用的类型或语料

库的内容等。也就是说，不

能保证类推问题的评价

高，目

标应用的结果就一

定好。这一点请一定注意

。

  第 4章　word2vec的高速化

170

4.5 小结

本章

我们以 word2vec 的高速化为主题

，对上一章的

CBOW 模型进行

了

改进。具体来说，我们实现

了 Embedding 层，并引入了一种名为

负采样

的新方法。这一改

进的背景是，原先的方法

随着语料库词汇量的增

加，计算

量也按比例增加

。

利用“部分”数据而不是“全

部”数据，这是本章的一个

重要话题。正

如人不能全

知全能一样，以当前的计

算机性能，要处理所有的

数据也是不现

实的。相反

，仅处理对我们有用的那

一小部分数据会有更好

的效果。本章我

们仔细研

究了基于这一思想的负

采样技术。负采样通过仅

关注部分单词实现

了计

算的高速化。

通过上一章

和本章的介绍，关于 word2vec 的一

系列话题就要进入尾声

了。word2vec 对自然语言处理领域

产生了很大的影响，基于

它获得的单词

的分布式

表示被应用在了各种自

然语言处理任务中。另外

，不仅限于自然语

言处理

，word2vec

的思想还被应用在了语

音、图像和视频等领域中

。希望

读者能切实理解本

章所讲的 word2vec 的相关内容，这

些知识在许多领域都

能

派上用场。

4.5

小结  171

本章所学

的内容

• Embedding

层保存单词的分

布式表示，在正向传播时

，提取单词 ID

对应的向量

• 因

为 word2vec

的计算量会随着词汇

量的增加而成比例地增

加，所以

最好使用近似计

算来加速

• 负采样技术采

样若干负例，使用这一方

法可以将多分类问题转

化为

二分类问题进行处

理

•

基于 word2vec 获得的单词的分

布式表示内嵌了单词含

义，在相似的

上下文中使

用的单词在单词向量空

间上处于相近的位置

• word2vec

的

单词的分布式表示的一

个特性是可以基于向量

的加减法

运算来求解类

推问题

• word2vec 的迁移学习能力

非常重要，它的单词的分

布式表示可以应

用于各

种各样的自然语言处理

任务

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

第5章

RNN

只记得我

在一个昏暗潮湿的地方

喵喵地哭泣着。

——夏目漱石

《我是猫》

到 目 前 为 止，

我 们

看 到 的 神 经

网 络 都 是 前

馈 型

神 经 网 络。 前 馈

（feedforward）是指

网络的传播方向是单向

的。具体地说，先将输入信

号传

给下一层（隐藏层），接

收到信号的层也同样传

给下一层，然后再传给下

一

层……像这样，信号仅在一

个方向上传播。

虽然前馈

网络结构简单、易于理解

，但是可以应用于许多任

务中。不过，

这种网络存在

一个大问题，就是不能很

好地处理时间序列数据

（以下简称为

“时序数据”）。更

确切地说，单纯的前馈网

络无法充分学习时序数

据的性质（模

式）。于是，RNN（Recurrent Neural Network，循环

神经网络）便应运而生。

本

章我们将指出前馈网络

的问题，并介绍 RNN 如何很好

地解决这些问

题。然后，我

们会详细解释 RNN 的结构，并

用 Python 对其进行实现。

5.1

概率和

语言模型

作为介绍 RNN 的准

备，我们将首先复习上一

章的 word2vec，然后使

用概率描述

自然语言相关的现象，最

后介绍从概率视角研究

语言的“语言

模型”。

  第 5章　RNN

174

5.1.1  概

率视角下的word2vec

我们先复习

一下 word2vec

的 CBOW 模型。这里，我们来

考虑由单词

序列 w1, w2,

··· , wT 表示的

语料库，将第 t 个单词作为

目标词，将它左右的

（第 t − 1 个

和第 t

+ 1 个）单词作为上下文

。

在本书中，目标词是指中

间的单词，上下文是指目

标词周围的单词。

如图 5-1

所

示，CBOW 模型所做的事情就是

从上下文（wt−1 和 wt+1）

预测目标词

（wt）。

...

. . . w1w2 wt−1 wt

wt+1 wT−1wT

图5-1 word2vec的CBOW模型：从上下文预测

目标词

下面，我们用数学

式来表示“当给定 wt−1

和 wt+1 时目

标词是 wt 的概

率”，如式

(5.1) 所示

：

 P(wt|wt−1, wt+1) (5.1)

CBOW 模型对式 (5.1) 这一后验概率

进行建模。这个后验概率

表示“当

给定 wt−1

和 wt+1 时 wt 发生的

概率”。这是窗口大小为 1

时

的 CBOW 模型。

顺便提一下，我们

之前考虑的窗口都是左

右对称的。这里我们将上

下文

限定为左侧窗口，比

如图 5-2

所示的情况。

5.1 概率和

语言模型  175

...

. . . w1w2 wt−1 wt

wt+1 wT−1wT wt−2

图5-2　仅有左侧窗

口的上下文

在仅将左侧

2

个单词作为上下文的情

况下，CBOW 模型输出的概率如

式 (5.2) 所示：

 P(wt|wt−2,

wt−1) (5.2)

word2vec 的上下文窗口大

小是超参数，可以设置为

任何值。这里将

窗口大小

设置为左右非对称的形

式，即左侧 2

个单词，右侧 0 个

单词。

这样设定的理由是

提前考虑了后面要说的

语言模型。

使用式 (5.2)

的写法

，CBOW 模型的损失函数可以写

成式 (5.3)。式

(5.3) 是从交叉熵误差

推导出来的结果（具体请

参考 1.3.1

节）。

 L = −log P(wt|wt−2,

wt−1) (5.3)

CBOW 模型的学习旨在

找到使式 (5.3) 表示的损失函

数（确切地说，

是整个语料

库的损失函数之和）最小

的权重参数。只要找到了

这样的权重参

数，CBOW 模型就

可以更准确地从上下文

预测目标词。

像这样，CBOW 模型

的学习目的是从上下文

预测出目标词。为了达成

这一目标，随着学习的推

进，（作为副产品）获得了编

码了单词含义信息的

单

词的分布式表示。

那么，CBOW 模

型本来的目的“从上下文

预测目标词”是否可以用

来

做些什么呢？式 (5.2) 表示的

概率 P(wt|wt−2,

wt−1) 是否可以在一些实

际场

景中发挥作用呢？说

到这里，就要提一下语言

模型了。

  第

5章　RNN 176

5.1.2  语言模型

语

言模型（language model）给出了单词序列

发生的概率。具体来说，

就

是使用概率来评估一个

单词序列发生的可能性

，即在多大程度上是自然

的

单词序列。比如，对于“you say goodbye”这

一单词序列，语言模型给

出

高概率（比如 0.092）；对于“you say good die”这一

单词序列，模型则给

出低

概率（比如

0.000 000 000 003 2）。

语言模型可以

应用于多种应用，典型的

例子有机器翻译和语音

识别。比

如，语音识别系统

会根据人的发言生成多

个句子作为候选。此时，使

用语言

模型，可以按照“作

为句子是否自然”这一基

准对候选句子进行排序

。

语言模型也可以用于生

成新的句子。因为语言模

型可以使用概率来评价

单词序列的自然程度，所

以它可以根据这一概率

分布造出（采样）单词。另

外

，第 7 章中我们会讨论如何

使用语言模型生成文章

。

现在，我们使用数学式来

表示语言模型。这里考虑

由 m 个单词 w1, ··· ,

wm 构成的句子，将

单词按 w1, ··· , wm

的顺序出现的概

率记为 P(w1, ··· ,

wm)。因为这个概率是

多个事件一起发生的概

率，所以称为联合概率。

使

用后验概率可以将这个

联合概率

P(w1, ··· , wm) 分解成如下形

式：

A

P(w1, ··· , wm) = P(wm

|w1, ··· , wm−1)P(wm−1 |w1, ···

, wm−2)

··· P(w3 |w1, w2)P(w2

|w1)P(w1)

=

m



t=1

P(wt |w1, ··· , wt−1)

(5.4)

与表示总和的 （sigma）相对，式

(5.4) 中的 （pi）表示所有元素

相乘

的乘积。如式 (5.4)

所示，联合概

率可以由后验概率的乘

积表示。

式 (5.4) 的结果可以从

概率的乘法定理推导出

来。这里我们花一点时间

来说明一下乘法定理，并

看一下式 (5.4) 的推导过程。

A 为

了简化数学式，这里将P(w1 | w0)作

为P(w1)处理。

5.1 概率和语言模型



177

首先，概率的乘法定理可

由下式表示：

 P(A,B) = P(A|B)P(B)

(5.5)

式 (5.5) 表示的乘

法定理是概率论中最重

要的定理，意思是“A 和 B

两

个

事件共同发生的概率 P(A, B)”是

“B 发生的概率 P(B)”和“B

发生后

A 发

生的概率 P(A|B)”的乘积（这个解

释感觉上非常自然）。

概 率

P

(A, B) 也可以分解为 P (A, B)

= P (B|A)P (A)。也 就

是说

，根据将

A 和 B 中的哪一个作

为后验概率的条件，存在

P (A, B)

= P (B|A)P (A) 和 P

(A, B) = P (A|B)P (B)

两种表示

方法。

使用这

个乘法定理，m 个单词的联

合概率 P(w1, ···

, wm) 就可以用后

验概

率来表示。为了便于理解

，我们先将式子如下变形

：

P(w1, ···

, wm−1

A

, wm) =

P(A, wm) = P(wm |A)P(A) (5.6)

这里，将 w1, ··· , wm−1 整体表示为

A。这样

一来，按照乘法定理，可以

推

导出式 (5.6)。接着，再对 A(w1, ··· ,

wm−1) 进行

同样的变形：

P(A) = P(w1, ···

, wm−2 

A

, wm−1)

= P(A , wm−1) =

P(wm−1 |A )P(A )

(5.7)

像这样，单词

序列每次减少一个，分解

为后验概率。然后，重复这

一过程，

就可以推导出式

(5.4)。

如式 (5.4) 所示，联合概率

P(w1, ··· , wm) 可以

表示为后验概率的乘积

的全部单词为上下文（条

件）时的概率，如图

P(wt|w1,

··· , wt−1)。这里需

要注意的是，这个后验概

率是以目标词左侧

5-3 所示

。

第 5章　RNN 178

w1 ...

wt ... wm w2 wt−1

图5-3

语言模型中的后

验概率：若以第t个单词为

目标词，则第t个单词左侧

的全部单词

构成上下文

（条件）

这里我们来总结一

下，我们的目标是求 P(wt|w1, ··· ,

wt−1) 这个

概率。

如果能计算出这个

概率，就能求得语言模型

的联合概率 P(w1, ··· ,

wm)。

由 P (wt|w1, ··· ,

wt−1) 表示的模

型称为条件语言模型（conditional 

language model），有

时也将其称为语言模型

。

5.1.3

将CBOW模型用作语言模型？

那

么，如果要把 word2vec 的 CBOW 模型（强行

）用作语言模型，该

怎么办

呢？可以通过将上下文的

大小限制在某个值来近

似实现，用数学式可

以如

下表示：

P(w1, ··· , wm)

=

m



t=1

P(wt

|w1, ··· , wt−1) ≈

m

t=1

P(wt |wt−2, wt−1) (5.8)

这里，我们将上下

文限定为左侧的 2 个单词

。如此一来，就可以用 CBOW

模型

（CBOW 模型的后验概率）近似表

示。

在机器学习和统计学

领域，经常会听到“马尔可

夫性”（或者“马尔

可夫模型

”“马尔可夫链”）这个词。马尔

可夫性是指未来的状态

仅

依存于当前状态。此外

，当某个事件的概率仅取

决于其前面的 N 个

事件时

，称为“N

阶马尔可夫链”。这里

展示的是下一个单词仅

取决

于前面 2 个单词的模

型，因此可以称为“2 阶马尔

可夫链”。

5.1

概率和语言模型

179

式 (5.8) 是使用 2

个单词作为上

下文的例子，但是这个上

下文的大小可

以设定为

任意长度（比如 5 或 10）。不过，虽

说可以设定为任意长度

，但

必须是某个“固定”长度

。比如，即便是使用左侧

10 个

单词作为上下文的

CBOW 模型

，其上下文更左侧的单词

的信息也会被忽略，而这

会导致问题，

如图 5-4

中的例

子所示。

Tom was watching TV in

his room. Mary came into the

room. Mary said hi to ?

图5-4　需要较长的上

下文的问题示例：“？”中应填

入什么单词？

在图 5-4 的问题

中，“Tom 在房间看电视，Mary

进了房

间”。根据该语

境（上下文），正

确答案应该是 Mary 向 Tom（或者“him”）打

招呼。这

里要获得正确答

案，就必须将“?”前面第

18 个单

词处的 Tom 记住。如果

CBOW 模型的

上下文大小是

10，则这个问

题将无法被正确回答。

那

么，是否可以通过增大 CBOW 模

型的上下文大小（比如变

为 20 或

30）来解决此问题呢？的

确，CBOW 模型的上下文大小可

以任意设定，但

是 CBOW 模型还

存在忽视了上下文中单

词顺序的问题。

CBOW

是 Continuous Bag-Of-Words 的简称

。Bag-Of-Words 是

“一袋子单词”的意思，这

意味着袋子中单词的顺

序被忽视了。

关于上下文

的单词顺序被忽视这个

问题，我们举个例子来具

体说明。比

如，在上下文是

2 个单词的情况下，CBOW 模型的

中间层是那 2 个单词向

量

的和，如图 5-5 所示。

如图 5-5 的左

图所示，在

CBOW 模型的中间层

求单词向量的和，因此

上

下文的单词顺序会被忽

视。比如，(you, say) 和 (say,

you) 会被作为相同

的内容进行处理。

  第 5章

RNN 180

Win

Win

Wout

Win

Win

Wout

䓀ڒᅯ

喍

̷̸᪴喎

䓀ڒᅯ

喍̷̸᪴喎

䓀ܧᅯ

喍ᓄܳ喎

䓀ܧᅯ

喍ᓄܳ喎

͚䬡ᅯ

͚䬡ᅯ

图5-5 左图

是常规的CBOW模型。右图模型

的中间层由上下文的各

个单词向量“拼接”

而成（图

中的输入层是one-hot向量）

我们

想要的是考虑了上下文

中单词顺序的模型。为此

，可以像图 5-5 中

的右图那样

，在中间层“拼接”（concatenate）上下文的

单词向量。实际上，

“Neural Probabilistic Language Model”[28] 中提出

的模型就采用了这个方

法

（关于模型的详细信息

，请参考论文 [28]）。但是，如果采

用拼接的方法，权重

参数

的数量将与上下文大小

成比例地增加。显然，这是

我们不愿意看到的。

那么

，如何解决这里提出的问

题呢？这就轮到 RNN 出场了。RNN

具

有一个机制，那就是无论

上下文有多长，都能将上

下文信息记住。因此，使

用

RNN 可以处理任意长度的时

序数据。下面，我们就来感

受一下 RNN 的

魅力。

word2vec 是以获取

单词的分布式表示为目

的的方法，因此一般

不会

用于语言模型。这里，为了

引出 RNN 的魅力，我们拓展了

话题，强行将 word2vec

的 CBOW 模型应用

在了语言模型上。

word2vec 和基于

RNN 的语言模型是由托马斯

·米科洛夫团队分

别在 2013 年

和 2010 年提出的。基于 RNN

的语言

模型虽然也能

获得单词

的分布式表示，但是为了

应对词汇量的增加、提高

分布

式表示的质量，word2vec 被提

了出来。

5.2 RNN

181

5.2 RNN

RNN（Recurrent Neural Network）中的

Recurrent 源自拉丁语

，意

思是“反复发生”，可以翻

译为“重复发生”“周期性地

发生”“循环”，因此

RNN 可以直译

为“复发神经网络”或者“循

环神经网络”。下面，我们将

探

讨“循环”一词。

Recurrent Neural Network 通常译为

“循环神经网络”。另外，

还有

一种被称为 Recursive

Neural Network（递归神经网

络）的

网络。这个网络主要

用于处理树结构的数据

，和循环神经网络不

是一

个东西。

5.2.1

循环的神经网络

“循环”是什么意思呢？是“反

复并持续”的意思。从某个

地点出发，

经过一定时间

又回到这个地点，然后重

复进行，这就是“循环”一词

的含

义。这里要注意的是

，循环需要一个“环路”。

只有

存在了“环路”或者“回路”这

样的路径，媒介（或者数据

）才能

在相同的地点之间

来回移动。随着数据的循

环，信息不断被更新。

血液

在我们体内循环。今天流

动的血液是接着昨天的

血液继续流动的。

另外，它

也是接着一周前的、一个

月前的、一年前的，甚至刚

出生

时的血液继续流动

的。血液通过在体内循环

，从过去一直被“更新”

到现

在。

RNN 的特征就在于拥有这

样一个环路（或回路）。这个

环路可以使数据

不断循

环。通过数据的循环，RNN

一边

记住过去的数据，一边更

新到最新

的数据。

下面，我

们来具体地看一下 RNN。这里

，我们将 RNN 中使用的层称

为

“RNN 层”，如图 5-6 所示。

图灵社区会

员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 5章

RNN 182

ht xt RNN

图

5-6

拥有环路的RNN层

如图 5-6 所示

，RNN 层有环路。通过该环路，数

据可以在层内循环。

在图

5-6

中，时刻 t 的输入是 xt，这暗示

着时序数据 (x0, x1,

··· , xt, ···) 会

被输入到

层中。然后，以与输入对应

的形式，输出

(h0, h1, ··· , ht, ···)。

这里假定在

各时刻向 RNN 层输入的 xt 是向

量。比如，在处理句子（单

词

序列）的情况下，将各个单

词的分布式表示（单词向

量）作为

xt 输入

RNN 层。

仔细看一

下图 5-6，可以发现输出有两

个分叉，这意味着同一个

东西被复制了。输出中的

一个分叉将成为其自身

的输入。

接着，我们来详细

介绍一下图 5-6 的循环结构

。在此之前，我们先将

RNN 层的

绘制方法更改如下。

如图

5-7

所示，到目前为止，我们在

绘制层时都是假设数据

从左向右流

动的。不过，从

现在开始，为了节省纸面

空间，我们将假设数据是

从下向上

流动的（这是为

了在之后需要展开循环

时，能够在左右方向上将

层铺开）。

5.2 RNN

183

ht

xt

xt

ht

RNN

RNN

图5-7　把层旋转90度

5.2.2  展

开循环

现在，准备工作已

经完成了，我们来仔细看

一下 RNN 层的循环结构。

RNN 的循

环结构在之前的神经网

络中从未出现过，但是通

过展开循环，可

以将其转

化为我们熟悉的神经网

络。百闻不如一见，现在我

们就实际地进行

展开（图

5-8）。

ht

xt xt x0 x1

x0

ht

ht

h0 h1 h2

RNN RNN RNN RNN RNN

图5-8

RNN层的循环的展开

如图

5-8 所示，通过展开 RNN 层的循环

，我们将其转化为了从左

向右

延伸的长神经网络

。这和我们之前看到的前

馈神经网络的结构相同

（前馈网

络的数据只向一

个方向传播）。不过，图 5-8 中的

多个 RNN 层都是“同一

个层”，这

一点与之前的神经网络

是不一样的。

第 5章　RNN 184

时序数

据按时间顺序排列。因此

，我们用“时刻”这个词指代

时

序数据的索引（比如，时

刻 t 的输入数据为 xt）。在自然

语言处理

的情况下，既使

用“第 t

个单词”“第 t 个 RNN 层”这样

的表述，

也使用“时刻

t 的单

词”或者“时刻 t 的 RNN 层”这样的

表述。

由图 5-8 可以看出，各个

时刻的 RNN 层接收传给该层

的输入和前一个

RNN

层的输

出，然后据此计算当前时

刻的输出，此时进行的计

算可以用下

式表示：

	 ht =

tanh(ht−1Wh + xtWx + b) (5.9)

首先

说明一下式 (5.9) 中的符号。RNN 有

两个权重，分别是将输入

x

转化为输出 h

的权重 Wx 和将

前一个 RNN 层的输出转化为

当前时刻的输出

的权重

Wh。此外，还有偏置

b。这里，ht−1 和 xt 都

是行向量。

在式 (5.9)

中，首先执

行矩阵的乘积计算，然后

使用 tanh 函数（双曲

正切函数

）变换它们的和，其结果就

是时刻 t 的输出

ht。这个 ht 一方

面向

上输出到另一个层

，另一方面向右输出到下

一个 RNN 层（自身）。

观察式 (5.9) 可以

看出，现在的输出 ht 是由前

一个输出 ht−1

计算出来

的。从

另一个角度看，这可以解

释为，RNN 具有“状态”h，并以式 (5.9)

的

形式被更新。这就是说RNN层

是“具有状态的层”或“具有

存储（记忆）

的层”的原因。

RNN 的

h 存储“状态”，时间每前进一

步（一个单位），它就以式 (5.9)

的

形式被更新。许多文献中

将 RNN

的输出 ht称为隐藏状态

（hidden 

state）或隐藏状态向量（hidden state vector），本书中

也是如此。

另外，许多文献

中将展开后的 RNN 层绘制成

图 5-9 的左图。

在图

5-9 的左图中

，从 RNN 层输出了两个箭头，但

是请注意这两个箭

头代

表的是同一份数据（准确

地说，是同一份数据被复

制了）。在本书中，

和之前一

样，我们明确地在图中显

示了输出处存在分叉，如

图

5-9 的右图

所示。

5.2 RNN

185

图5-9　展开后

的RNN层的画法比较

RNN RNN RNN

RNN

᱙Γ͚⮱⩨∂

̭㝙⮱⩨∂

5.2.3  Backpropagation

Through Time

将 RNN 层展

开后，就可以视为在水平

方向上延伸的神经网络

，因此

RNN

的学习可以用与普

通神经网络的学习相同

的方式进行，如图 5-10 所示。

RNN RNN RNN

RNN

图

5-10　将循环展开后的RNN层的误

差反向传播法

如图 5-10 所示

，将循环展开后的

RNN 可以使

用（常规的）误差反向

传播

法。换句话说，可以通过先

进行正向传播，再进行反

向传播的方式求目

标梯

度。因为这里的误差反向

传播法是“按时间顺序展

开的神经网络的误差

反

向传播法”，所以称为 Backpropagation

Through Time（基于

时间的反

向传播），简称 BPTT。

通

过该 BPTT，RNN

的学习似乎可以进

行了，但是在这之前还有

一个

必须解决的问题，那

就是学习长时序数据的

问题。因为随着时序数据

的时间

跨度的增大，BPTT 消耗

的计算机资源也会成比

例地增大。另外，反向传播

的梯度也会变得不稳定

。

第 5章　RNN 186

要基于 BPTT

求梯度，必须

在内存中保存各个时刻

的 RNN 层的中

间数据（RNN 层的反

向传播将在后文中说明

）。因此，随着时序数据

变长

，计算机的内存使用量（不

仅仅是计算量）也会增加

。

5.2.4  Truncated BPTT

在处理长时序数据时，通

常的做法是将网络连接

截成适当的长度。具

体来

说，就是将时间轴方向上

过长的网络在合适的位

置进行截断，从而创建

多

个小型网络，然后对截出

来的小型网络执行误差

反向传播法，这个方法称

为 Truncated BPTT（截断的 BPTT）。

Truncated 是“被截断”的意

思。Truncated

BPTT 是指按适当长

度截断

的误差反向传播法。

在 Truncated BPTT

中

，网络连接被截断，但严格

地讲，只是网络的

反向传

播的连接被截断，正向传

播的连接依然被维持，这

一点很重要。也就

是说，正

向传播的信息没有中断

地传播。与此相对，反向传

播则被截断为适

当的长

度，以被截出的网络为单

位进行学习。

现在，我们结

合具体的例子来介绍 Truncated

BPTT。假

设有一个长

度为 1000 的时序

数据。在自然语言处理的

情况下，这相当于一个有

1000 个

单词的语料库。顺便说

一下，我们之前处理的

PTB 数

据集将多个串联起来

的

句子当作一个大的时序

数据。这里也一样，将多个

串联起来的句子当作一

个时序数据。

在处理长度

为 1000 的时序数据时，如果展

开

RNN 层，它将成为在水

平方

向上排列有 1000 个层的网络

。当然，无论排列多少层，都

可以根据误

差反向传播

法计算梯度。但是，如果序

列太长，就会出现计算量

或者内存使

用量方面的

问题。此外，随着层变长，梯

度逐渐变小，梯度将无法

向前一层

传递。因此，如图

5-11 所示，我们来考虑在水平

方向上以适当的长度截

断

网络的反向传播的连

接。

5.2 RNN

187

图5-11 在适当位置截断反

向传播的连接。这里，将反

向传播的连接中的某一

段RNN层

称为“块”（块的背景为

灰色）

RNN RNN

RNN RNN RNN RNN RNN

h0

h1 h9 h10 h11 h19 h20

x0 x1 x9 x10 x11 x19

x20

在图 5-11 中，我们截断了

反向传播的连接，以使学

习可以以 10 个

RNN 层为单位进

行。像这样，只要将反向传

播的连接截断，就不需要

再考

虑块范围以外的数

据了，因此可以以各个块

为单位（和其他块没有关

联）完

成误差反向传播法

。

这里需要注意的是，虽然

反向传播的连接会被截

断，但是正向传播的

连接

不会。因此，在进行

RNN 的学习

时，必须考虑到正向传播

之间是有关

联的，这意味

着必须按顺序输入数据

。下面，我们来说明什么是

按顺序输入

数据。

我们之

前看到的神经网络在进

行 mini-batch

学习时，数据都是

随机

选择的。但是，在 RNN 执行 Truncated BPTT

时，数

据需

要按顺序输入。

现在

，我们考虑使用 Truncated BPTT 来学习

RNN。我

们首先要做的

是，将第 1 个

块的输入数据 (x0, ...

, x9) 输入 RNN 层。这

里要进行的处理

如图

5-12 所

示。

  第 5章

RNN 188

h0

dh0

dx0

dh1

dx1

dh9

dx9

h1 h9

x0

x1 x9

h9 RNN RNN RNN

RNN RNN RNN

ₐाэ᧚

ाэ᧚ࣺ

图5-12

第1个块的正向

传播和反向传播：因为从

后面的时刻传来的梯度

被截断，所以误

差反向传

播法仅在本块内进行

如

图 5-12 所示，先进行正向传播

，再进行反向传播，这样可

以得到所

需的梯度。接着

，对下一个块的输入数据

(x10,

x11, ··· , x19) 执行误差反向

传播法，如

图

5-13 所示。

5.2 RNN  189

h10 h11 h19

x10 x11 x19

h19 h9

dh10

dx10

dh11

dx11

dh19

dx19

RNN RNN RNN

RNN

RNN RNN

ₐाэ᧚

ाэ᧚ࣺ

图5-13　第2个块的正向

传播和反向传播

这里，和

第 1 个块一样，先执行正向

传播，再执行反向传播。这

里的重

点是，这个正向传

播的计算需要前一个块

最后的隐藏状态 h9，这样可

以维

持正向传播的连接

。

用同样的方法，继续学习

第 3 个块，此时要使用第 2 个

块最后的隐藏状

态

h19。像这

样，在 RNN 的学习中，通过将数

据按顺序输入，从而继承

隐

藏状态进行学习。根据

到目前为止的讨论，可知

RNN 的学习流程如图 5-14

所示。

图

灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

第 5章　RNN 190

h0 h1

h9

h10 h11 h19

h20 h21

h29

x0 x1 x9

x10 x11

x19

x20 x21 x29

RNN RNN

RNN

RNN RNN RNN

RNN RNN

RNN

h10

h0 h1 h9 h20

h21 h29 h30

h0 h1 h9

h10 h11 h19

h11 h19 h20

h21 h29 h30

x10 x11 x19

x20 x21 x29 x30

RNN

RNN

RNN RNN RNN RNN RNN RNN

RNN RNN RNN RNN

RNN

RNN

RNN

RNN RNN RNN RNN RNN

RNN

x0 x1 x9 x20 x21

x29 x30

x0 x1 x9 x10

x11 x19

h9

h19

h30

x30

图5-14 Truncated BPTT的数据处理顺

序（参见彩图）

如图 5-14 所示，Truncated

BPTT 按

顺序输入数据，进行学习

。这

样一来，能够在维持正

向传播的连接的同时，以

块为单位应用误差反向

传

播法。

5.2.5

Truncated BPTT的mini-batch学习

到目前为

止，我们在探讨 Truncated BPTT 时，并没有

考虑

mini￾batch 学习。换句话说，我们

之前的探讨对应于批大

小为 1 的情况。为了执

行 mini-batch

学

习，需要考虑批数据，让它

也能像图 5-14 一样按顺序输

入

数据。因此，在输入数据

的开始位置，需要在各个

批次中进行“偏移”。

为了说

明“偏移”，我们仍用上一节

的通过 Truncated

BPTT 进行学习

的例子

，对长度为1000的时序数据，以

时间长度10为单位进行截

断。此时，

如何将批大小设

为 2 进行学习呢？在这种情

况下，作为

RNN 层的输入数据

，

第 1 笔样本数据从头开始

按顺序输入，第 2

笔数据从

第 500 个数据开始按顺

序输

入。也就是说，将开始位置

平移 500，如图 5-15

所示。 䔈㵹႓΍⮱䶧Ꮌ

5.2 RNN  191

图5-15　在进行

mini-batch学习时，在各批次（各样本

）中平移输入数据的开始

位置

RNN RNN RNN

RNN

RNN RNN

RNN RNN RNN

RNN

RNN RNN

h0 h1 h9

h500

h501 h509

h10 h11 h19

h510

h511 h519

h519

h19

h509

h509

h9

h9

x0

x0

x1

x1

x9

x9

x500 x501 x509

x10

x11 x19

x510 x511 x519

x500

x501 x509

x10 x11 x19

x510

x511 x519

ឦ⁎⮱す

1 ٰ͗㉍

ឦ⁎⮱す

2 ٰ͗㉍

如图 5-15 所示，批次的第

1 个元素是

x0, ··· , x9，批次的第 2 个元

素

是 x500, ··· , x509，将这个 mini-batch

作为 RNN 的输入

数据进行学习。因

为要输

入的数据是按顺序的，所

以接下来是时序数据的

第 10 ~

19 个数据和

第 510 ~ 519

个数据。像

这样，在进行 mini-batch 学习时，平移

各批次输入

数据的开始

位置，按顺序输入。此外，如

果在按顺序输入数据的

过程中遇到

了结尾，则需

要设法返回头部。

如上所

述，虽然

Truncated BPTT 的原理非常简单

，但是关于数据的

输入方

法有几个需要注意的地

方。具体而言，一是要按顺

序输入数据，二是 䔈㵹႓΍⮱䶧Ꮌ

第 5章　RNN 192

要

平移各批次（各样本）输入

数据的开始位置。这里的

探讨有些复杂，大家

一时

间可能还不能理解，之后

通过实际查看和运行代

码，相信大家就能够理

解

了。

5.3 RNN的实现

通过之前的探

讨，我们已经看到了 RNN 的全

貌。实际上，我们要实

现的

是一个在水平方向上延

伸的神经网络。另外，考虑

到基于 Truncated 

BPTT 的学习，只需要创

建一个在水平方向上长

度固定的网络序列即可

，如

图

5-16 所示。

h0 h1 hT−1

x0

x1 xT−1

RNN RNN RNN

图5-16

基于RNN的神经

网络：在水平方向上长度

固定

如图 5-16 所示，目标神经

网络接收长度为 T 的时序

数据（T

为任意值），

输出各个

时刻的隐藏状态 T 个。这里

，考虑到模块化，将图 5-16 中在

水平

方向上延伸的神经

网络实现为“一个层”，如图

5-17 所示。

5.3 RNN的实现  193

hs = (h0, h1, · ·

· , hT −1)

hs

xs

= (x0, x1, · · ·

, xT −1)

xs

RNN RNN

RNN Time RNN

图5-17 Time RNN层：将展开

循环后的层视为一个层

如图

5-17 所示，将垂直方向上

的输入和输出分别捆绑

在一起，就可以

将水平排

列的层视为一个层。换句

话说，可以将 (x0, x1, ···

, xT−1) 捆绑为

xs 作为

输入，将 (h0,

h1, ··· , hT−1) 捆绑为 hs

作为输出

。这里，我们将进

行 Time RNN 层中的

单步处理的层称为“RNN 层”，将

一次处理

T 步的层

称为“Time RNN 层

”。

像

Time RNN 这样，将整体处理时序

数据的层以单词“Time”开

头命

名，这是本书中规定的命

名规范。之后，我们还会实

现 Time

Affine 层、Time Embedding 层等。

我们接下来进

行的实现的流程是：首先

，实现进行 RNN

单步处理的 RNN

类

；然后，利用这个 RNN 类，完成一

次进行 T

步处理的 TimeRNN 类。

5.3.1  RNN层的

实现

现在，我们来实现进

行 RNN 单步处理的 RNN 类。首先复

习一下 RNN

正向传播的数学

式，如式 (5.10) 所示：

	 ht

= tanh(ht−1Wh + xtWx + b)

(5.10)

这里，我们将

数据整理为 mini-batch 进行处理。因

此，xt（和 ht）在

行方向上保存各

样本数据。在矩阵计算中

，矩阵的形状检查非常重

要。这

里，假设批大小是 N，输

入向量的维数是 D，隐藏状

态向量的维数是 H，

第 5章　RNN 194

则

矩阵的形状检查可以像

下面这样进行（图 5-18）。

xt Wx ht ht−1 Wh

N

×H N H×H D N ×D

×H ×H

̭㜡

̭㜡

图5-18　形状

检查：在矩阵乘积中，对应

维度的元素数量要一致

（这里省略偏置）

如图 5-18 所示

，通过矩阵的形状检查，可

以确认它的实现是否正

确，

至少可以确认它的计

算是否成立。基于以上内

容，现在我们给出 RNN 类的初

始化方法和正向传播的

forward()

方法（ common/time_layers.py）。

class RNN:

 def

__init__(self, Wx, Wh, b):

 self.params

= [Wx, Wh, b]

 self.grads

= [np.zeros_like(Wx), np.zeros_like(Wh),

np.zeros_like(b)]

 self.cache

= None

 def forward(self, x,

h_prev):

 Wx, Wh, b =

self.params

 t = np.dot(h_prev, Wh)

+ np.dot(x, Wx) + b

h_next = np.tanh(t)

 self.cache =

(x, h_prev, h_next)

 return h_next

RNN 的初始化方法接收

两个权重参数和一个偏

置参数。这里，将通过函

数

参数传进来的模型参数

设置为列表类型的成员

变量 params。然后，以各个

参数对

应的形状初始化梯度，并

保存在 grads

中。最后，使用 None 对反

向传

播时要用到的中间

数据 cache 进行初始化。

正向传

播的 forward(x, h_prev) 方法接收两个参数

：从下方输入的 x 和

5.3 RNN的实现

195

从左边输入的 h_prev。剩下的就

是按式 (5.10)

进行实现。顺便说

一下，这里

从前一个 RNN 层接

收的输入是 h_prev，当前时刻的

RNN 层的输出（=

下

一时刻的 RNN 层

的输入）是 h_next。

接下来，我们继

续实现

RNN 的反向传播。在此

之前，让我们通过图 5-19

的计

算图再次确认一下 RNN 的正

向传播。

Wh

Wx

hprev

hnext

x

b

hnext MatMul

MatMul

tanh

图5-19 RNN层的计算图（MatMul节

点表示矩阵乘积）

RNN 层的正

向传播可由图 5-19 的计算图

表示。这里进行的计算由

矩阵

乘积“MatMul”、加法“+”和“tanh”这 3

种运

算构成。此外，因为偏置

b 的

加法运算会触发广播操

作，所以严格地讲，这里还

应该加上 Repeat 节

点。不过简单

起见，这里省略了它（具体

请参考

1.3.4.3 节）。

那么，图 5-19 的计算

图的反向传播是什么样

的呢？答案很简单。因为

这

3

种运算的反向传播我们

都已经掌握了（关于反向

传播，请参考 1.3 节），

剩下就是

基于图 5-20，按正向传播的反

方向实现各个运算的反

向传播。

第 5章　RNN 196

图5-20　基于RNN层的

计算图的反向传播

Wh

dWh

dWx

hprev

dhprev

dhnext

hnext

x dx

db

hnext

Wx

MatMul

MatMul

tanh

b

下面

实现 RNN

层的 backward()。参考图 5-20，可以如

下实现。

def backward(self, dh_next):

Wx, Wh, b = self.params

x, h_prev, h_next = self.cache

dt = dh_next * (1 -

h_next ** 2)

 db =

np.sum(dt, axis=0)

 dWh = np.dot(h_prev.T,

dt)

 dh_prev = np.dot(dt, Wh.T)

dWx = np.dot(x.T, dt)

 dx

= np.dot(dt, Wx.T)

 self.grads[0][...] =

dWx

 self.grads[1][...] = dWh

self.grads[2][...] = db

 return dx,

dh_prev

5.3 RNN的实现  197

以上就是

RNN

层的反向传播的实现。接

下来，我们将实现 Time RNN 层。

5.3.2

Time RNN层的

实现

Time RNN 层由 T

个 RNN 层构成（T 可以

设置为任意值），如图 5-21

所示

。

图5-21 Time RNN层和RNN层

hs = (h0,

h1, · · · , hT

−1)

RNN RNN RNN

xs =

(x0, x1, · · · ,

xT −1)

hs

xs

Time RNN

由图 5-21 可知，Time RNN 层是

T 个

RNN 层连接起来的网络。我

们

将这个网络实现为 Time RNN 层

。这里，RNN

层的隐藏状态 h 保存

在成

员变量中。如图 5-22 所示

，在进行隐藏状态的“继承

”时会用到它。

...

hs1 hs0

h

xs0 xs1

RNN RNN RNN RNN RNN RNN

Ԋႅౕ

TimeRNNㆨ⮱

᜽অअ䛼͚

图5-22 Time RNN层将隐藏

状态h保存在成员变量中

，以在块之间继承隐藏状

态

如图 5-22 所示，我们使用 Time RNN 层

管理

RNN 层的隐藏状态。这

样

一来，使用 Time RNN 的人就不必考

虑

RNN 层的隐藏状态的“继承

工

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

第 5章　RNN 198

作”了。另外，我

们可以用

stateful 这个参数来控

制是否继承隐藏状态。

下

面，我们来看一下 Time RNN 层的实

现。首先实现初始化方法

和两

个方法（ common/time_layers.py）。

class TimeRNN:

 def

__init__(self, Wx, Wh, b, stateful=False):

self.params = [Wx, Wh, b]

self.grads = [np.zeros_like(Wx), np.zeros_like(Wh),

np.zeros_like(b)]

self.layers = None

 self.h, self.dh

= None, None

 self.stateful =

stateful

 def set_state(self, h):

self.h = h

 def reset_state(self):

self.h = None

初始化方法

的参数有权重、偏置和布

尔型（True/False）的 stateful。

一个成员变量

layers 在

列表中保存多个 RNN 层，另一

个成员变量，h 保

存调用

forward() 方

法时的最后一个 RNN 层的隐

藏状态。另外，在调用

backward() 时，dh

保

存传给前一个块的隐藏

状态的梯度（关于 dh，我们会

在

反向传播的实现中说

明）。

考虑到 TimeRNN类的扩展性，将

设定 Time

RNN 层的隐藏状态的

方

法实现为 set_state(h)。另外，将重设隐

藏状态的方法实现为

reset_state()。

上

述参数中的

stateful 是“有状态”的

意思。在本书的实现中，当

stateful 为 True 时，Time RNN

层“有状态”。这里说的

“有状态”是指维

持 Time RNN 层的隐

藏状态。也就是说，无论时

序数据多长，Time RNN

层的正向传

播都可以不中断地进行

。而当 stateful 为 False 时，每次调用

5.3

RNN的实

现  199

Time RNN 层的

forward() 时，第一个 RNN 层的隐

藏状态都会被初始化为

零矩阵（所有元素均为 0 的

矩阵）。这是没有状态的模

式，称为“无状态”。

在处理长

时序数据时，需要维持 RNN 的

隐藏状态，这一功能通

常

用“stateful”一词表示。在许多深度

学习框架中，RNN 层都有

stateful参数

，该参数用于指定是否保

存上一时刻的隐藏状态

。

接着，我们来看一下正向

传播的实现。

def forward(self, xs):

 Wx,

Wh, b = self.params

 N,

T, D = xs.shape

 D,

H = Wx.shape

 self.layers =

[]

 hs = np.empty((N, T,

H), dtype='f')

 if not self.stateful

or self.h is None:

 self.h

= np.zeros((N, H), dtype='f')

 for

t in range(T):

 layer =

RNN(*self.params)

 self.h = layer.forward(xs[:, t,

:], self.h)

 hs[:, t, :]

= self.h

 self.layers.append(layer)

 return

hs

正向传播的

forward(xs) 方法从下方获取输入 xs，xs 囊

括了 T

个时序数

据。因此，如

果批大小是 N，输入向量的

维数是 D，则 xs 的形状为

(N,T,D)。

在首

次调用时（self.h 为 None 时），RNN 层的隐藏

状态

h 由所有元素

均为 0 的

矩阵初始化。另外，在成员

变量 stateful

为 False 的情况下，h 将

总是

被重置为零矩阵。

在主体

实现中，首先通过

hs=np.empty((N, T, H), dtype='f') 为输出

准

备一个“容器”。接着，在

T 次

for 循环中，生成 RNN 层，并将其添

加到成

第 5章　RNN 200

员变量 layers

中。然

后，计算 RNN 层各个时刻的隐

藏状态，并存放在 hs

的对应

索引（时刻）中。

如果调用

Time RNN 层

的 forward()方法，则成员变量 h中将

存放

最后一个

RNN 层的隐藏

状态。在 stateful为 True的情况下，在下

一次调用 forward()方法时，刚才的

成员变量 h将被继续使用

。而在

stateful为 False的情况下，成员变

量 h将被重置为零向量。

接

下来是 Time RNN

层的反向传播的

实现。用计算图绘制这个

反向传

播，如图 5-23 所示。

Ԋႅౕ

TimeRNNㆨ⮱

᜽অअ䛼͚

hs

xs

dh

dxs =

(dx0, dx1, · · · ,

dxT −1)

dhs = (dh0, dh1,

· · · , dhT −1)

RNN RNN RNN

图5-23 Time RNN层

的反向传播

在图 5-23 中，将从

上游（输出侧的层）传来的

梯度记为 dhs，将流向

下游的

梯度记为 dxs。因为这里我们

进行的是

Truncated BPTT，所以不需

要流

向这个块上一时刻的反

向传播。不过，我们将流向

上一时刻的隐藏状态

的

梯度存放在成员变量 dh 中

。这是因为在第

7 章探讨 seq2seq（sequence￾to-sequence，序

列到序列）时会用到它（具

体请参考第 7 章）。

以上就是

Time

RNN层的反向传播的全貌图

。如果关注第t个RNN层，

则它的

反向传播如图 5-24 所示。

图灵

社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

5.3 RNN的实现  201

图5-24

第t个RNN层的反向

传播

dhprev

dht

dhnext

dxt

RNN

RNN RNN

从上方传来的梯度

dht 和从将来的层传来的梯

度 dhnext 会传到第

t 个

RNN 层。这里需

要注意的是，RNN 层的正向传

播的输出有两个分叉。在

正

向传播存在分叉的情

况下，在反向传播时各梯

度将被求和。因此，在反向

传

播时，流向 RNN 层的是求和

后的梯度。考虑到以上这

些，反向传播的实现

如下

所示。

def backward(self,

dhs):

 Wx, Wh, b =

self.params

 N, T, H =

dhs.shape

 D, H = Wx.shape

dxs = np.empty((N, T, D), dtype='f')

dh = 0

 grads =

[0, 0, 0]

 for t

in reversed(range(T)):

 layer = self.layers[t]

dx, dh = layer.backward(dhs[:, t, :]

+ dh) # 求和后的梯度

 dxs[:,

t, :] = dx

 for

i, grad in enumerate(layer.grads):

 grads[i]

+= grad

 for i, grad

in enumerate(grads):

 self.grads[i][...] = grad

self.dh = dh

 return dxs

第 5章

RNN 202

这里，首先创建传给下游

的梯度的“容器”（dxs）。接着，按与

正向传

播相反的方向，调

用

RNN 层的 backward() 方法，求得各个时

刻的梯度 dx，

并存放在

dxs 的对

应索引处。另外，关于权重

参数，需要求各个 RNN 层的

权

重梯度的和，并通过“...”用最

终结果覆盖成员变量 self.grads。

在

Time RNN 层中有多个 RNN 层。另外，这些

RNN 层使用相

同的权重。因此

，Time RNN 层的（最终）权重梯度是各

个 RNN

层的权重梯度之和。

以

上就是对

Time RNN 层的实现的说

明。

5.4 处理时序数据的层的

实现

本章我们的目标是

使用

RNN 实现语言模型。目前

我们已经实现了

RNN 层和整

体处理时序数据的 Time RNN

层，本

节将创建几个可以处

理

时序数据的新层。我们将

基于 RNN 的语言模型称为 RNNLM（RNN

Language Model，RNN 语

言模型）。现在，我们来完成

RNNLM。

5.4.1  RNNLM的全貌图

首先，我们看一

下 RNNLM 使用的网络。图 5-25 所示为

最简单的

RNNLM

的网络，其中左

图显示了 RNNLM 的层结构，右图

显示了在时间

轴上展开

后的网络。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享

尊重版权

5.4 处理时序

数据的层的实现  203

wt

wt w0 w1

yt yt y0

y1

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

...

...

. . . . .

.

. . . . .

.

. . . . .

.

. . . . .

.

... ...

图5-25 RNNLM的网

络图（左图是展开前，右图

是展开后）

图

5-25 中的第 1 层是

Embedding 层，该层将单词 ID

转化为单

词的分

布式表示（单词向

量）。然后，这个单词向量被

输入到 RNN 层。RNN 层向

下一层（上

方）输出隐藏状态，同时也

向下一时刻的

RNN 层（右侧）输

出

隐藏状态。RNN 层向上方输

出的隐藏状态经过 Affine 层，传

给

Softmax 层。

现在，我们仅考虑正

向传播，向图 5-25 的神经网络

传入具体的数据，

并观察

输出结果。这里使用的句

子还是我们熟悉的“you

say goodbye and i 

say

hello.”，此时

RNNLM 进行的处理如图 5-26 所示。

第

5章　RNN 204

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

䃺ࢂ

ID

ࢂ䃺 0

you

say goodbye hello

1 2 5

...

...

...

...

...

...

...

图5-26　处理样本语料库“you say goodbye and

i say hello.”的

RNNLM的例子

如图 5-26 所示，被输入

的数据是单词

ID 列表。首先

，我们关注第 1 个

时刻。作为

第 1

个单词，单词 ID 为 0 的 you

被输

入。此时，查看 Softmax

层输出的概

率分布，可知 say 的概率最高

，这表明正确预测出了 you

后

面出

现的单词为 say。当然，这

样的正确预测只在有“好

的”（学习顺利的）权

重时才

会发生。

接着，我们关注第

2 个单词

say。此时，Softmax 层的输出在

goodbye

处和 hello 处概率较高。确实，“you say

goodby”和

“you say hello”都

是很自然的句子（顺便

说一下，正确答案是 goodbye）。这里

需要注意的是，

RNN

层“记忆”了

“you say”这一上下文。更准确地说

，RNN 将“you 

say”这一过去的信息保存

为了简短的隐藏状态向

量。RNN 层的工作是将这

个信

息传送到上方的 Affine 层和下

一时刻的 RNN 层。

像这样，RNNLM

可以

“记忆”目前为止输入的单

词，并以此为基础预 you say goodbye and ihello

.you say goodbye and ihello .you

say goodbye and ihello .you say

goodbye and ihello .

5.4 处理

时序数据的层的实现

205

测

接下来会出现的单词。RNN 层

通过从过去到现在继承

并传递数据，使得

编码和

存储过去的信息成为可

能。

5.4.2

Time层的实现

之前我们将

整体处理时序数据的层

实现为了 Time RNN 层，这里也同

样

使用

Time Embedding 层、Time Affine 层等来实现整体

处理时序数据的

层。一旦

创建了这些

Time 层，我们的目

标神经网络就可以像图

5-27 这

样实现。

yT−1 ys

y0 y1

wT−1 ws w0 w1

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Softmax

Affine

Embedding

RNN

Time Softmax

Time Affine

Time Embedding

Time RNN

. . .

.

. .

. . .

.

. .

. . .

.

. .

图5-27　将整体处理

时序数据的层实现为Time 层

我们将整体处理含有 T

个

时序数据的层称为“Time 层”。如

果

可以实现这些层，通过

像组装乐高积木一样组

装它们，就可以完成

处理

时序数据的网络。

Time 层的实

现很简单。比如，在

Time Affine 层的情

况下，只需要像

图 5-28 那样，准

备

T 个 Affine 层分别处理各个时

刻的数据即可。

图灵社区

会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 5章

RNN

206

图5-28　将Time Affine层实现为T个Affine层的集

合

yT−1 ys

y0 y1

xT−1 xs x0 x1

Affine Affine Affine Time Affine

.

. .

. . .

Time

Embedding 层也一样，在正向传播

时准备 T 个 Embedding 层，

由各个 Embedding 层处

理各个时刻的数据。

关于

Time Affine 层和

Time Embedding 层没有什么特别难

的内容，

我们就不再赘述

了。需要注意的是，Time Affine 层并不

是单纯地使用

T 个

Affine 层，而是

使用矩阵运算实现了高

效的整体处理。感兴趣的

读者可以参

考源代码（common/time_layers.py 的

TimeAffine

类）。接下来我们看一下时

序版本的 Softmax。

我们在 Softmax 中一并

实现损失误差 Cross

Entropy Error 层。这里，

按

照图 5-29 所示的网络结构实

现

Time Softmax with Loss 层。

×

LT−1 L1

L0

L

L

xs

ts

xT−1 t T−1 x0 x1

t1 t0

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Time

Softmax with Loss

. . .

. . .

. . .

. . .

图5-29 Time Softmax

with Loss层的全貌图

图 5-29 中

的 x0、x1

等数据表示从下方的

层传来的得分（得分是正

规

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

5.5

RNNLM的学习和评价  207

化

为概率之前的值），t0、t1 等数据

表示正确解标签。如该图

所示，T 个

Softmax with Loss 层各自算出损失

，然后将它们加在一起取

平均，将得到

的值作为最

终的损失。此处进行的计

算可用下式表示：

L

= 1

T (L0 + L1

+ ··· + LT −1) (5.11)

顺便说

一下，本书的Softmax with Loss层计算mini-batch的平

均损失。

具体而言，假设 mini-batch 有

N

笔数据，通过先求 N 笔数据

的损失之和，

再除以 N，可以

得到单笔数据的平均损

失。这里也一样，通过取时

序数据的

平均，可以求得

单笔数据的平均损失作

为最终的输出。

以上就是

对 Time 层的说明。这里只是做

了一个简短的说明，实际

的实

现可以在 common/time_layers.py 中找到，感

兴趣的读者可以参考一

下。

5.5 RNNLM的学习和评价

实现 RNNLM 所

需要的层都已经准备好

了，现在我们来实现 RNNLM，

并对

其进行训练，然后再评价

一下它的结果。

5.5.1  RNNLM的实现

这

里我们将 RNNLM

使用的网络实

现为 SimpleRnnlm 类，其层结构如图

5-30 所

示。

第 5章　RNN 208

ᢌ๞

Time

Embedding

Time RNN

Time Affine

Time

Softmax with Loss

ts

ws

图5-30

SimpleRnnlm的层结构：RNN层的

状态在类内部进行管理

如图 5-30 所示，SimpleRnnlm 类是一个堆叠

了 4 个

Time 层的神经网络。

我们

先来看一下初始化的代

码（ ch05/simple_rnnlm.py）。

import sys

sys.path.append('..')

import numpy as np

from

common.time_layers import *

class SimpleRnnlm:

def __init__(self, vocab_size, wordvec_size, hidden_size):

V, D, H = vocab_size, wordvec_size,

hidden_size

 rn = np.random.randn

# 初始化权重

 embed_W = (rn(V,

D) / 100).astype('f')

5.5 RNNLM的学习和

评价

209

 rnn_Wx = (rn(D, H)

/ np.sqrt(D)).astype('f')

 rnn_Wh = (rn(H,

H) / np.sqrt(H)).astype('f')

 rnn_b =

np.zeros(H).astype('f')

 affine_W = (rn(H, V)

/ np.sqrt(H)).astype('f')

 affine_b = np.zeros(V).astype('f')

# 生成层

 self.layers = [

TimeEmbedding(embed_W),

 TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),

TimeAffine(affine_W, affine_b)

 ]

 self.loss_layer

= TimeSoftmaxWithLoss()

 self.rnn_layer = self.layers[1]

# 将所有的权

重和梯度整理到列表中

self.params, self.grads = [],

[]

 for layer in self.layers:

self.params += layer.params

 self.grads +=

layer.grads

这里，对各个层使用的参

数（权重和偏置）进行初始

化，生成必要的层。

假设使

用 Truncated BPTT 进行学习，将

Time RNN 层的 stateful 设置

为

True，如此

Time RNN 层就可以继承上

一时刻的隐藏状态。

另外

，在上面的初始化代码中

，RNN 层和 Affine

层使用了“Xavier 初

始值”。如

图 5-31 所示，在上一层的节点

数是 n

的情况下，使用标准

差为

1

√n的分布作为 Xavier 初始值

A。顺便说一下，标准差可以

直观地解释为表

示数据

分散程度的指标。

A 这是一

个简略版的实现，原始论

文中提出的权重初始值

还考虑了下一层的节点

数。

  第 5章

RNN 210

Ҭ⩕ᴴ۳ጛͧ



√1

n

⮱ܳጰ䔈㵹݊໸ࡃ

n ͗㞯◦

图5-31 Xavier初始值：在上一

层有n个节点的情况下，使

用标准差为 √1

n 的分布作为

初始值

在深度学习中，权

重的初始值非常重要。关

于这一点，我们在前

作《深

度学习入门：基于 Python 的理论

与实现》中已经进行了详

细的探讨。同样，对

RNN 而言，权

重的初始值也很重要。通

过

设置好的初始值，学习

的进展和最终的精度都

会有很大变化。本

书此后

都将使用 Xavier 初始值作为权

重的初始值。另外，在语

言

模型的相关研究中，经常

使用 0.01 * np.random.uniform(...)

这样的经过缩放的

均匀分布。

接着，我们来实

现

forward() 方法、backward() 方法和 reset_state() 方法。

def

forward(self, xs, ts):

 for layer

in self.layers:

 xs = layer.forward(xs)

loss = self.loss_layer.forward(xs, ts)

 return

loss

def backward(self, dout=1):

 dout

= self.loss_layer.backward(dout)

 for layer in

reversed(self.layers):

 dout = layer.backward(dout)

return dout

5.5 RNNLM的学

习和评价  211

def reset_state(self):

 self.rnn_layer.reset_state()

可以看出实现

非常简单。在各个层中，正

向传播和反向传播都正

确地进

行了实现。因此，我

们只要以正确的顺序调

用

forward()（或者 backward()）

即可。方便起见，这

里将重设网络状态的方

法实现为 reset_state()。以上

就是对 SimpleRnnlm

类

的说明。

5.5.2  语言模型的评价

SimpleRnnlm 的实现结束了，接下来要

做的就是向这个网络输

入数据进行

学习。在实现

用于学习的代码之前，我

们先来讨论一下语言模

型的评价方法。

语言模型

基于给定的已经出现的

单词（信息）输出将要出现

的单词的概

率分布。困惑

度（perplexity）常被用作评价语言模

型的预测性能的指标。

简

单地说，困惑度表示“概率

的倒数”（这个解释在数据

量为 1 时严格一

致）。为了说

明概率的倒数，我们仍旧

考虑“you

say goodbye and i say hello.”

这一语料库。假设在

向语言模型“模型 1”传入单

词 you 时会输出图 5-32 的

左图所

示的概率分布。此时，下一

个出现的单词是 say 的概率

为 0.8，这是一

个相当不错的

预测。取这个概率的倒数

，可以计算出困惑度为 0

1

.8 = 1.25。

὎ಸ

1

὎ಸ 2

0.2

0.8

you you

图

5-32　当输入单词you时，模型输出

下一个出现的单词的概

率分布

而图 5-32 右侧的模型

（“模型 2”）预测出的正确单词

的概率为

0.2，这

显然是一个

很差的预测，此时的困惑

度为 0

1

.2 =

5。

总结一下，“模型 1”能准

确地预测，困惑度是 1.25；“模型

2”的预测

未能命中，困惑度

是 5.0。此例表明，困惑度越小

越好。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

you

say

goodbye

and

i

hello

.

you

say

goodbye

and

i

hello

.

第 5章　RNN 212

那么，如何

直观地解释值 1.25

和 5.0 呢？它们

可以解释为“分叉度”。

所谓

分叉度，是指下一个可以

选择的选项的数量（下一

个可能出现的单词的

候

选个数）。在刚才的例子中

，好的预测模型的分叉度

是 1.25，这意味着下

一个要出

现的单词的候选个数可

以控制在 1 个左右。而在差

的模型中，下一

个单词的

候选个数有 5 个。

如上面的

例子所示，基于困惑度可

以评价模型的预测性能

。好的模

型可以高概率地

预测出正确单词，所以困

惑度较小（困惑度的最小

值是 1.0）；而差的模型只能低

概率地预测出正确单词

，困惑度较大。

以上都是输

入数据为 1 个时的困惑度

。那么，在输入数据为多个

的情况

下，结果会怎样呢

？我们可以根据下面的式

子进行计算。

L = −N

1

n

 k

tnk log

ynk (5.12)

困惑度 = eL (5.13)

这里

，假设数据量为 N 个。tn 是 one-hot 向量

形式的正确解标签，tnk

表

示

第 n 个数据的第 k 个值，ynk

表示

概率分布（神经网络中的

Softmax 的

输出）。顺便说一下，L 是神

经网络的损失，和式 (1.8) 完全

相同，使用这个

L 计算出的

eL 就是困惑度。

式子 (5.12) 看上去

有些复杂，但是前面我们

介绍的数据量为

1 时的“概

率的倒数”“分叉度”“候选个

数”等在这里也通用。也就

是说，困惑度越小，

分叉度

越小，表明模型越好。

在信

息论领域，困惑度也称为

“平均分叉度”。这可以解释

为，数据

量为 1

时的分叉度

是数据量为 N 时的分叉度

的平均值。

5.5 RNNLM的学习和评价

213

5.5.3  RNNLM的学习代码

下面，我们使

用 PTB 数据集进行学习，不过

这里仅使用

PTB 数据集

（训练

数据）的前 1000 个单词。这是因

为在本节实现的 RNNLM

中，即便

使用所有的训练数据，也

得不出好的结果。下一章

我们将对它进行改进。下

面我们先来看一下学习

用的代码（ ch05/train_custom_loop.py）。

import sys

sys.path.append('..')

import

matplotlib.pyplot as plt

import numpy as

np

from common.optimizer import SGD

from

dataset import ptb

from simple_rnnlm import

SimpleRnnlm

# 设定超参数

batch_size = 10

wordvec_size = 100

hidden_size = 100

# RNN的

隐藏状态向量的元素个

数

time_size = 5 #

Truncated BPTT的时间跨度大小

lr = 0.1

max_epoch

= 100

# 读入

训练数据（缩小了数据集

）

corpus, word_to_id,

id_to_word = ptb.load_data('train')

corpus_size = 1000

corpus = corpus[:corpus_size]

vocab_size = int(max(corpus)

+ 1)

xs = corpus[:-1] #

输入

ts = corpus[1:] # 输出（监督标签）

data_size = len(xs)

print('corpus size: %d,

vocabulary size: %d' % (corpus_size, vocab_size))

# 学习

用的参数

max_iters = data_size //

(batch_size * time_size)

time_idx = 0

total_loss = 0

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 5章　RNN 214

loss_count

= 0

ppl_list = []

#

生成

模型

model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)

optimizer = SGD(lr)

# ❶ 计算读入mini-batch的各笔样

本数据的开始位置

jump = (corpus_size - 1) //

batch_size

offsets = [i * jump

for i in range(batch_size)]

for epoch

in range(max_epoch):

 for iter in

range(max_iters):

 # ❷ 获取

mini-batch

batch_x = np.empty((batch_size, time_size), dtype='i')

batch_t = np.empty((batch_size, time_size), dtype='i')

for t in range(time_size):

 for

i, offset in enumerate(offsets):

 batch_x[i,

t] = xs[(offset + time_idx) %

data_size]

 batch_t[i, t] = ts[(offset

+ time_idx) % data_size]

 time_idx

+= 1

 # 计算梯度，更新参数

loss = model.forward(batch_x, batch_t)

 model.backward()

optimizer.update(model.params, model.grads)

 total_loss += loss

loss_count += 1

 # ❸

各个

epoch的困惑度评价

 ppl = np.exp(total_loss /

loss_count)

 print('| epoch %d |

perplexity %.2f'

 % (epoch+1, ppl))

ppl_list.append(float(ppl))

 total_loss, loss_count = 0,

0

以上就是

学习用的代码，这和我们

之前看到的神经网络的

学习基本上是

一样的。不

过，从宏观上看，仍有两点

和之前的学习代码不同

，即“数据的

输入方式”和“困

惑度的计算”。这里，我们将

重点关注这两点，并对代

码

进行说明。

5.5

RNNLM的学习和评

价  215

首先是数据的输入方

式。这里我们使用 Truncated BPTT

进行学

习，

因此数据需要按顺序

输入，并且 mini-batch 的各批次要平

移读入数据的开

始位置

。在源代码❶处，计算各批次

读入数据的开始位置 offsets。offsets

的

各个元素中存放了读入

数据的开始位置（偏移量

）。

接着，在源代码 ❷ 处，按顺序

读入数据。首先准备容器

batch_x 和

batch_t，然后依次增加

time_idx 变量，将

time_idx 处的数据从语料库中取

出。这里利用❶中计算好的

offsets，各批次增加偏移量。另外

，当读入语

料库的位置超

过语料库大小时，为了回

到语料库的开头处，将当

前位置除以

语料库大小

后的余数作为索引使用

。

最后，基于式

(5.12) 计算困惑度

，这在代码 ❸ 处完成。为了求

每个

epoch 的困惑度，需要计算

每个

epoch 的平均损失，然后再

据此求困惑度。

以上就是

对代码的说明，现在我们

看一下学习结果。在上面

的代码中，

各个 epoch 的困惑度

的结果都保存在了

perplexity_list 中，我

们可以将它

绘制出来，如

图 5-33 所示。

350

300

250

200

150

100

50

0

0 20 40 60

epochs

80 100

图5-33　困惑度的演变

ఝᗾᏓ

第 5章　RNN 216

由图 5-33

可知，随着学习

的进行，困惑度稳步下降

。一开始超过 300

的困惑度到

最后接近 1（最小值）了。不过

这里使用的是很小的语

料库，在

实际情况下，当语

料库增大时，现在的模型

根本无法招架。下一章我

们将指

出当前

RNNLM 存在的问

题，并进行改进。

5.5.4  RNNLM的Trainer类

本书

提供了用于学习

RNNLM 的 RnnlmTrainer 类，其

内部封装了刚才

的 RNNLM

的学

习。将刚才的学习代码重

构为 RnnlmTrainer 类，结果如下。

这里只

摘录源代码的一部分（全

部代码在 ch05/train.py 中）。

...

from common.trainer import RnnlmTrainer

...

model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)

optimizer

= SGD(lr)

trainer = RnnlmTrainer(model, optimizer)

trainer.fit(xs, ts, max_epoch, batch_size, time_size)

如上所示，首

先使用

model 和 optimizer 初始化 RnnlmTrainer 类，然后

调用

fit()，完成学习。此时，RnnlmTrainer 类的

内部将执行上一节进行

的

一系列操作，具体如下

所示。

•　按顺序生成 mini-batch

•　调用模

型的正向传播和反向传

播

•　使用优化器更新权重

•　评价困惑度

RnnlmTrainer类

与 1.4.4 节中介

绍的 Trainer类有相同的 API。神

经网

络的常规学习使用

Trainer 类，而

RNNLM 的学习则使用

RnnlmTrainer类。

5.6 小结

217

使

用 RnnlmTrainer 类，可以避免每次写重

复的代码。本书的剩余部

分都

将使用 RnnlmTrainer

类学习 RNNLM。

5.6 小结

本章的主题是 RNN。RNN 通过数据

的循环，从过去继承数据

并传递

到现在和未来。如

此，RNN 层的内部获得了记忆

隐藏状态的能力。本书

中

我们花了很多时间说明

RNN 层的结构，并实现了 RNN 层（和

Time

RNN 层）。

本章还利用 RNN 创建了语

言模型。语言模型给单词

序列赋概率值。特

别地，条

件语言模型从已经出现

的单词序列计算下一个

将要出现的单词的概

率

。通过构成利用了 RNN 的神经

网络，理论上无论多么长

的时序数据，都

可以将它

的重要信息记录在 RNN 的隐

藏状态中。但是，在实际问

题中，这

样一来，许多情况

下学习将无法顺利进行

。下一章我们将指出 RNN 存在

的

问题，并研究替代 RNN 的

LSTM 层

或 GRU 层。这些层在处理时序

数据方

面非常重要，被广

泛用于前沿研究。

第 5章　RNN 218

本

章所学的内容

•

RNN 具有环路

，因此可以在内部记忆隐

藏状态

• 通过展开 RNN 的循环

，可以将其解释为多个

RNN 层

连接起来的神

经网络，可

以通过常规的误差反向

传播法进行学习（= BPTT）

• 在学习

长时序数据时，要生成长

度适中的数据块，进行以

块为单位

的 BPTT 学习（= Truncated BPTT）

•

Truncated BPTT 只截断

反向传播的连接

• 在 Truncated

BPTT 中，为

了维持正向传播的连接

，需要按顺序输

入数据

• 语

言模型将单词序列解释

为概率

•

理论上，使用 RNN 层的

条件语言模型可以记忆

所有已出现单词的

信息

第6章

Gated RNN

卸下包袱，轻装上阵

。

——尼采

上一章的 RNN 存在环路

，可以记忆过去的信息，其

结构非常简单，易

于实现

。不过，遗憾的是，这个

RNN 的效

果并不好。原因在于，许多

情况

下它都无法很好地

学习到时序数据的长期

依赖关系。

现在，上一章的

简单 RNN 经常被名为

LSTM 或 GRU 的层

所代替。实

际上，当我们说

RNN 时，更多的是指

LSTM 层，而不是

上一章的 RNN。

顺便说一句，当

需要明确指上一章的 RNN 时

，我们会说“简单

RNN”或

“Elman”。

LSTM 和 GRU 中增

加了一种名为“门”的结构

。基于这个门，可以学

习到

时序数据的长期依赖关

系。本章我们将指出上一

章的 RNN 的问题，介

绍代替它

的 LSTM 和

GRU 等“Gated RNN”。特别是我们将花

很多时间研

究 LSTM 的结构，并

揭示它实现“长期记忆”的

机制。此外，我们将使用

LSTM 创

建语言模型，并展示它可

以在实际数据上很好地

学习。

  第 6章

Gated RNN 220

6.1 RNN的问题

上一章

介绍的

RNN 之所以不擅长学

习时序数据的长期依赖

关系，是因

为 BPTT 会发生梯度

消失和梯度爆炸的问题

。本节我们将首先回顾一

下上

一章介绍的

RNN 层，并通

过一个实际的例子来说

明为什么 RNN 层不擅长

长期

记忆。

6.1.1

RNN的复习

RNN 层存在环路

。如果展开它的循环，它将

变成一个在水平方向上

延

伸的网络，如图 6-1 所示。

ht h0

x0 x1 x2

h1

h2 ht

xt xt

ht RNN

RNN RNN RNN RNN

图

6-1 RNN层：循环展开前和展开后

在图

6-1 中，当输入时序数据

xt 时，RNN 层输出 ht。这个 ht

也称为

RNN 层

的隐藏状态，它记录过去

的信息。

RNN 的特点在于使用

了上一时刻的隐藏状态

，由此，RNN 可以继承过

去的信

息。顺便说一下，如果用计

算图来表示此时 RNN 层进行

的处理，则

有图 6-2。

6.1

RNN的问题  221

ht

Wh

Wx

ht−1 ht

b

xt

MatMul

MatMul

tanh

+ +

图

6-2 RNN层的计算图（MatMul节点表示矩

阵乘积）

如图

6-2 所示，RNN 层的正

向传播进行的计算由矩

阵乘积、矩阵加法

和基于

激活函数 tanh 的变换构成，这

就是我们上一章看到的

RNN

层。下

面，我们看一下这个

RNN 层存在的问题（关于长期

记忆的问题）。

6.1.2  梯度消失和

梯度爆炸

语言模型的任

务是根据已经出现的单

词预测下一个将要出现

的单词。上

一章我们实现

了基于 RNN 的语言模型 RNNLM，这里

借着探讨 RNNLM

问题的机会，我

们再来考虑一下图 6-3 所示

的任务。

Tom was watching

TV in his room. Mary came

into the room. Mary said hi

to ?

图6-3　某种程度上需

要“长期记忆”的问题示例

：“?”中应填入什么单词?

如前

所述，填入“?”中的单词应该

是 Tom。要正确回答这个问题

，

RNNLM 需要记住“Tom 在房间看电视

，Mary 进了房间”这些信息。这些

信息必须被编码并保存

在 RNN 层的隐藏状态中。

第 6章

Gated RNN 222

现在让我们站在

RNNLM 进行学

习的角度来考虑上述问

题。在正确解

标签为 Tom 时，RNNLM 中

的梯度是如何传播的呢

？这里我们使用

BPTT

进行学习

，因此梯度将从正确解标

签 Tom 出现的地方向过去的

方向传播，

如图 6-4

所示。

Softmax

with Loss

Softmax

with

Loss

Softmax

with Loss

Softmax

with

Loss

Affine Affine Affine Affine

Embedding

Embedding Embedding Embedding

RNN RNN RNN

RNN

Tom

ₐ⶛㼐ᴴカ

to watching was

Tom

ᢌ๞ . . .

...

...

...

图6-4　正

确解标签为Tom时梯度的流

动

在学习正确解标签 Tom

时

，重要的是 RNN 层的存在。RNN 层通

过

向过去传递“有意义的

梯度”，能够学习时间方向

上的依赖关系。此时梯度

（理论上）包含了那些应该

学到的有意义的信息，通

过将这些信息向过去传

递，RNN 层学习长期的依赖关

系。但是，如果这个梯度在

中途变弱（甚至

没有包含

任何信息），则权重参数将

不会被更新。也就是说，RNN 层

无法学

习长期的依赖关

系。不幸的是，随着时间的

回溯，这个简单 RNN 未能避免

梯度变小（梯度消失）或者

梯度变大（梯度爆炸）的命

运。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

6.1 RNN的问题

223

6.1.3  梯度消失

和梯度爆炸的原因

现在

，我们深挖一下 RNN

层中梯度

消失（或者梯度爆炸）的起

因。如

图 6-5 所示，这里仅关注

RNN 层在时间方向上的梯度

传播。

h0

h0 h1

x0 x1

b b

b

hinit

dhinit dh0

h1

dh1

dhT−1

Wh

Wx

Wh

Wx

Wh

Wx

hT−1

xT−1

Mat

Mul

tanh

Mat

Mul

+ +

Mat

Mul

tanh

Mat

Mul

+ +

Mat

Mul

tanh

Mat

Mul

+ +

图6-5 RNN层在时间方向上

的梯度传播

如图 6-5 所示， 这

里考虑长度为

T 的时序数

据，关注从第 T 个正确解

标

签传递出的梯度如何变

化。就上面的问题来说，这

相当于第 T

个正确解标

签

是 Tom 的情形。此时，关注时间

方向上的梯度，可知反向

传播的梯度流

经 tanh、“+”和

MatMul（矩阵

乘积）运算。

“+”的反向传播将

上游传来的梯度原样传

给下游，因此梯度的值不

变。

那么，剩下的 tanh 和 MatMul

运算会

怎样变化呢？我们先来看

一下 tanh。

附录 A 中会详细说明

。当 y

= tanh(x) 时，它的导数是 d

d

x

y = 1 − y2

。

此时，将

y = tanh(x) 的值及其导数的值分别

画在图上，如图 6-6 所示。

第 6章

Gated RNN 224

1.0

−1.0

0.5

−4 −2 0

tanh(x)

dy/dx

2 4

0.0

x

−0.5

图6-6 y = tanh(x)的图（虚线是导数）

图 6-6

中

的虚线是 y = tanh(x) 的导数。从图中

可以看出，它的值小于

1.0，并

且随着

x 远离 0，它的值在变

小。这意味着，当反向传播

的梯度经过

tanh 节点时，它的

值会越来越小。因此，如果

经过 tanh

函数 T 次，则梯度

也会

减小 T 次。

RNN 层的激活函数一

般使用 tanh 函数，但是如果改

为 ReLU 函数，

则有希望抑制梯

度消失的问题（当 ReLU 的输入

为 x 时，它的输出是

max(0,

x)）。这是因

为，在 ReLU 的情况下，当 x 大于 0

时

，反向

传播将上游的梯度

原样传递到下游，梯度不

会“退化”。实际上，题

为“ Improving performance of

recurrent neural network with 

relu

nonlinearity”的论

文 [29]就使用 ReLU 实现了性能改

善。

接下来，我们关注图 6-5

中

的 MatMul（矩阵乘积）节点。简单起

见，

这里我们忽略图 6-5 中的

tanh 节点。如此一来，如图

6-7 所示

，RNN 层的

反向传播的梯度就

仅取决于 MatMul 运算。

y

6.1 RNN的问题  225

dh

Wh

T···Wh

T dh Wh

T

dh

Wh Wh Wh

MatMul MatMul

MatMul

图

6-7　仅关注RNN层的矩阵乘积时

的反向传播的梯度

在图

6-7 中，假定从上游传来梯度

dh，此时 MatMul

节点的反向传播

通

过矩阵乘积 dhWh

T 计算梯度。之

后，根据时序数据的时间

步长，将这个

计算重复相

应次数。这里需要注意的

是，每一次矩阵乘积计算

都使用相同的

权重 Wh。

那么

，反向传播时梯度的值通

过 MatMul 节点时会如何变化呢

？一旦

有了疑问，最好的方

法就是做实验！让我们通

过下面的代码，来观察梯

度大

小的变化（ ch06/rnn_gradient_graph.py）。

import numpy as np

import matplotlib.pyplot as plt

N =

2 # mini-batch的大小

H = 3

# 隐

藏状态向量的维数

T = 20 #

时序

数据的长度

dh = np.ones((N, H))

np.random.seed(3)

# 为了复现，固

定随机数种子

Wh = np.random.randn(H, H)

norm_list = []

for t in

range(T):

 dh = np.dot(dh, Wh.T)

norm = np.sqrt(np.sum(dh**2)) / N

norm_list.append(norm)

这里用 np.ones() 初

始化 dh（np.ones() 是所有元素均为

1 的

矩阵）。然

后，根据反向传播

的 MatMul 节点的数量更新 dh

相应

次数，并将各步的 dh

  第 6章

Gated RNN 226

的

大小（范数）添加到 norm_list 中。这里

，dh

的大小是 mini-batch（N 笔）

中的平均“L2 范

数”。L2 范数对所有元素的平

方和求平方根。

下面，我们

将上述代码的执行结果

（norm_list）画在图上，如图 6-8

所示。

250

0

200

1 5 10 15 20

150

50

100

ᬣ䬡ₒ䪬

图6-8　梯

度dh的大小随时间步长呈

指数级增加

如图

6-8 所示，可

知梯度的大小随时间步

长呈指数级增加，这就是

梯度

爆炸（exploding gradients）。如果发生梯度

爆炸，最终就会导致溢出

，出

现 NaN（Not

a Number，非数值）之类的值。如

此一来，神经网络的学习

将

无法正确运行。

现在做

第 2 个实验，将

Wh 的初始值改

为下面的值。

# Wh = np.random.randn(H,

H) # before

Wh = np.random.randn(H,

H) * 0.5 # after

使用这个初

始值，进行与上面相同的

实验，结果如图

6-9 所示。

图灵

社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

ᷜᏓ㠰᪝

6.1 RNN的问题  227

1.2

0.2

0.0

1.0

1 5 10 15

20

0.8

0.4

0.6

ᬣ䬡ₒ䪬

图6-9

梯度dh的大小随

时间步长呈指数级减小

从图 6-9 中可以看出，这次梯

度呈指数级减小，这就是

梯度消失

（vanishing gradients）。如果发生梯度

消失，梯度将迅速变小。一

旦梯度变

小，权重梯度不

能被更新，模型就会无法

学习长期的依赖关系。

在

这里进行的实验中，梯度

的大小或者呈指数级增

加，或者呈指数级减

小。为

什么会出现这样的指数

级变化呢？因为矩阵 Wh 被反

复乘了 T 次。如

果 Wh 是标量，则

问题将很简单：当 Wh 大于 1

时

，梯度呈指数级增加；当 Wh

小

于 1 时，梯度呈指数级减小

。

那么，如果

Wh 不是标量，而是

矩阵呢？此时，矩阵的奇异

值将成为指

标。简单而言

，矩阵的奇异值表示数据

的离散程度。根据这个奇

异值（更准

确地说是多个

奇异值中的最大值）是否

大于 1，可以预测梯度大小

的变化。 ᷜᏓ㠰᪝

第 6章　Gated RNN 228

如果奇异值

的最大值大于 1，则可以预

测梯度很有可能会呈指

数

级增加；而如果奇异值

的最大值小于 1，则可以判

断梯度会呈指

数级减小

。但是，并不是说奇异值比

1 大就一定会出现梯度爆

炸。

也就是说，这是必要条

件，并非充分条件。文献 [30] 中

详细探

讨了 RNN 的梯度消失

和梯度爆炸问题，感兴趣

的读者可以参考

一下。

6.1.4  梯

度爆炸的对策

至此，我们

探讨了 RNN

的梯度爆炸和梯

度消失问题，现在我们继

续讨

论解决方案。首先来

看一下梯度爆炸。

解决梯

度爆炸有既定的方法，称

为梯度裁剪（gradients clipping）。这

是一个非

常简单的方法，它的伪代

码如下所示：

if

gˆ  threshold:

gˆ = threshold



gˆ

gˆ

这里假设可

以将神经网络用到的所

有参数的梯度整合成一

个，并用符号gˆ 表

示。另外，将

阈值设置为 threshold。此时，如果梯

度的 L2

范数  gˆ 大于或

等于阈

值，就按上述方法修正梯

度，这就是梯度裁剪。如你

所见，虽然这个

方法很简

单，但是在许多情况下效

果都不错。

gˆ 整合了神经网

络中用到的所有参数的

梯度。比如，当某个模型

有

W1和 W2两个参数时，gˆ

就是这两

个参数对应的梯度 dW1和 dW2

的

组合。

现在，我们用 Python

来实现

梯度裁剪，将其实现为 clip_grads(grads, 

max_norm) 函

数。参数 grads

是梯度的列表，max_norm 是

阈值，此时梯度裁剪

可以

如下实现（ ch06/clip_grads.py）。

6.2 梯度消失和

LSTM 229

import numpy as np

dW1 = np.random.rand(3, 3) * 10

dW2 = np.random.rand(3, 3) * 10

grads = [dW1, dW2]

max_norm =

5.0

def clip_grads(grads, max_norm):

 total_norm

= 0

 for grad in

grads:

 total_norm += np.sum(grad **

2)

 total_norm = np.sqrt(total_norm)

rate = max_norm / (total_norm +

1e-6)

 if rate < 1:

for grad in grads:

 grad

*= rate

clip_grads(grads, max_norm)

这

就是梯度裁剪的实现，并

没有什么特别难的地方

。因为将来还会用到

clip_grads(grads,

max_norm)，所以

我们在 common/util.py 中也放了一份相

同的代码。

本书提供了用

于RNNLM学习的RnnlmTrainer类（common/trainer.

py），它的内部利

用了上述梯度裁剪以防

止梯度爆炸。我们会在 6.4

节

再次说明 RnnlmTrainer类中的梯度裁

剪。

以上就是对梯度裁剪

的说明。下面，我们看一下

防止梯度消失的对策。

6.2 梯

度消失和LSTM

在 RNN 的学习中，梯

度消失也是一个大问题

。为了解决这个问题，需

要

从根本上改变 RNN 层的结构

，这里本章的主题

Gated RNN 就要登

场了。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

第 6章　Gated RNN 230

人们已经

提出了诸多 Gated RNN 框架（网络结

构），其中具有代表性的有

LSTM 和 GRU。本节我们将关注

LSTM，仔细

研究它的结构，并阐明为

何

它不会（难以）引起梯度

消失。另外，附录 C 中会对 GRU 进

行说明。

6.2.1  LSTM的接口

接下来，我

们仔细看一下 LSTM 层。在此之

前，为了将来方便，我们

在

计算图中引入“简略图示

法”。如图 6-10 所示，这种图示法

将矩阵计算等

整理为一

个长方形节点。

Wh

Wx

xt xt

b

ht−1 ht−1

ht

ht ht

ht

Mat

Mul

Mat

Mul

+ + tanh tanh

图6-10

应用了

简略图示法的RNN层：本节使

用简略图示法以方便观

察

如图 6-10 所示，这里将 tanh(ht−1Wh +

xtWx + b) 这个

计算表示为

一个长方形

节点 tanh（ht−1

和 xt 是行向量），这个长

方形节点中包含了矩

阵

乘积、偏置的和以及基于

tanh 函数的变换。

现在我们已

经做好了准备。首先，我们

来比较一下

LSTM 与 RNN 的

接口（输

入和输出）（图 6-11）。

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

6.2 梯度消

失和

LSTM 231

ht−1

c t−1 c

t

ht−1

ht ht

ht

ht

xt xt

RNN LSTM

图6-11 RNN层与LSTM层的比较

如

图 6-11 所示，LSTM 与 RNN 的接口的不同

之处在于，LSTM

还有

路径 c。这个

c 称为记忆单元（或者简称

为“单元”），相当于 LSTM 专用

的记

忆部门。

记忆单元的特点

是，仅在 LSTM 层内部接收和传

递数据。也就是说，

记忆单

元在 LSTM

层内部结束工作，不

向其他层输出。而 LSTM 的隐藏

状

态 h 和

RNN 层相同，会被（向上

）输出到其他层。

从接收LSTM的

输出的一侧来看，LSTM的输出

仅有隐藏状态向量h。

记忆

单元 c 对外部不可见，我们

甚至不用考虑它的存在

。

6.2.2  LSTM层的结构

现在，我们来看

一下 LSTM 层的内部结构。这里

，我想一个一个地组

装 LSTM 的

部件，并仔细研究它们的

结构。以下内容参考了“colah’s blog: 

Understanding

LSTM Networks”[31] 这

篇优秀的文章。

如前所述

，LSTM 有记忆单元 ct。这个

ct 存储了

时刻 t 时 LSTM 的记

忆，可以认为

其中保存了从过去到时

刻 t 的所有必要信息（或者

以此为目的

进行了学习

）。然后，基于这个充满必要

信息的记忆，向外部的层

（和下一

时刻的 LSTM）输出隐藏

状态

ht。如图 6-12 所示，LSTM 输出经 tanh 函

数变换后的记忆单元。

第

6章　Gated RNN 232

图6-12

LSTM层基于记忆单元ct计

算隐藏状态ht

ht−1

c t−1

ht

c

t

ht

xt

tanh ᳽⻺䃎ツ

如图

6-12 所示，当

前的记忆单元 ct 是基于 3 个

输入

ct−1、ht−1 和 xt，

经过“某种计算”（后

述）算出来的。这里的重点

是隐藏状态 ht 要使用更新

后的

ct 来计算。另外，这个计

算是 ht = tanh(ct)，表示对 ct

的各个元素

应

用 tanh 函数。

到目前为止，记

忆单元 ct和隐藏状态

ht的关

系只是按元素应用

tanh 函数

。这意味着，记忆单元 ct和隐

藏状态 ht的元素个数相同

。

如果记忆单元

ct的元素个

数是 100，则隐藏状态 ht的元素

个数也

是 100。

在进入下一项

之前，我们先简单说明一

下

Gate 的功能。Gate 是“门”

的意思，就

像将门打开或合上一样

，控制数据的流动。直观上

，如图 6-13

所示，门的作用就是

阻止或者释放水流。

图灵

社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

6.2 梯度消失和 LSTM

233

图6-13　门的比喻

：控制水流

LSTM 中使用的门并

非只能“开或合”，还可以根

据将门打开多少来控

制

水的流量。如图

6-14 所示，可以

将“开合程度”控制在 0.7（70%）或者

0.2（20%）。

图6-14　将水的流量控制在0.0 ~

1.0的

范围内

0.7 (70%) 0.2 (20%)

如图

6-14 所示，门的开

合程度由 0.0 ~1.0 的实数表示（1.0 为

全开），

通过这个数值控制

流出的水量。这里的重点

是，门的开合程度也是（自

动）

从数据中学习到的。

有

专门的权重参数用于控

制门的开合程度，这些权

重参数通过学习

被更新

。另外，sigmoid 函数用于求门的开

合程度（sigmoid 函数的

输出范围

在 0.0 ~ 1.0）。

第 6章　Gated RNN 234

6.2.3

输出门

现在，我们

将话题转回到 LSTM。在刚才的

说明中，隐藏状态 ht 对记

忆

单元

ct 仅仅应用了 tanh 函数。这

里考虑对 tanh(ct) 施加门。换句话

说，

针对 tanh(ct) 的各个元素，调整

它们作为下一时刻的隐

藏状态的重要程

度。由于

这个门管理下一个隐藏

状态 ht 的输出，所以称为输

出门（output

gate）。

输出门的开合程度

（流出比例）根据输入 xt 和上

一个状态 ht−1 求

出。此时进行

的计算如式（6.1）所示。这里在

使用的权重参数和偏置

的上

标上添加了 output 的首字

母 o。之后，我们也将使用上

标表示门。另外，

sigmoid

函数用 σ( ) 表

示。

o =

σ(xtWx

(o) + ht−1Wh

(o) +

b(o)) (6.1)

如式 (6.1) 所示，输入 xt

有权重

Wx

(o)，上一时刻的状态 ht−1 有权

重

Wh

(o)（xt

和 ht−1 是行向量）。将它们的矩

阵乘积和偏置 b(o) 之和传给

sigmoid 函数，结果就是输出门的

输出

o。最后，将这个 o 和 tanh(ct) 的

对

应元素的乘积作为

ht 输出

。将这些计算绘制成计算

图，结果如图 6-15

所示。

6.2 梯度消

失和

LSTM 235

图6-15　添加输出门

ht−1

c

t−1

ht

c t

o

σ

ht

xt

᳽⻺䃎ツ

tanh

在图

6-15 中，将输出门进行的式

(6.1) 的

计算表示为 σ。然后，将它的

输出表示为 o，则 ht 可由

o 和 tanh(ct) 的

乘积计算出来。这里说的

“乘积”

是对应元素的乘积

，也称为阿达玛乘积。如果

用 

表示阿达玛乘积，则此

处的计算如下所示：

	 ht = o

 tanh(ct) (6.2)

以上

就是 LSTM 的输出门。这样一来

，LSTM

的输出部分就完成了，

接

着我们再来看一下记忆

单元的更新部分。

tanh的输出

是−1.0 ~ 1.0的实数。我们可以认为

这个−1.0 ~

1.0的

数值表示某种被

编码的“信息”的强弱（程度

）。而sigmoid 函数的

输出是0.0~1.0的实数

，表示数据流出的比例。因

此，在大多数情

况下，门使

用sigmoid函数作为激活函数，而

包含实质信息的数据

则

使用tanh函数作为激活函数

。

第 6章　Gated RNN 236

6.2.4  遗忘门

只有放下包

袱，才能轻装上路。接下来

，我们要做的就是明确告

诉记忆

单元需要“忘记什

么”。这里，我们使用门来实

现这一目标。

现在，我们在

记忆单元

ct−1 上添加一个忘

记不必要记忆的门，这里

称

为遗忘门（forget gate）。将遗忘门添

加到LSTM层，计算图如图6-16所示

。

ht−1

c

t−1

f o

ht

c t

σ σ

ht

xt

tanh

图6-16

添加遗忘门

在图 6-16 中，将

遗忘门进行的一系列计

算表示为 σ，其中有遗忘门

专

用的权重参数，此时的

计算如下：

f = σ(xtWx

(f) + ht−1Wh

(f) + b(f)) (6.3)

遗忘门的输出

f 可以由式

(6.3) 求得。然后，ct 由这

个 f 和上一个记忆

单元

ct−1 的

对应元素的乘积求得（ct = f  ct−1）。

6.2 梯

度消失和 LSTM 237

6.2.5

新的记忆单元

遗忘门从上一时刻的记

忆单元中删除了应该忘

记的东西，但是这样一

来

，记忆单元只会忘记信息

。现在我们还想向这个记

忆单元添加一些应当记

住的新信息，为此我们添

加新的 tanh 节点（图 6-17）。

图6-17

向新的

记忆单元添加必要信息

ht−1

c t−1

ht

c t

ht

xt

tanh

tanh

o gf

σ σ

如图 6-17 所示，基于 tanh

节点计算

出的结果被加到上一时

刻的记忆单

元 ct−1 上。这样一

来，新的信息就被添加到

了记忆单元中。这个 tanh 节点

的作用不是门，而是将新

的信息添加到记忆单元

中。因此，它不用

sigmoid

函数作为

激活函数，而是使用 tanh 函数

。tanh 节点进行的计算如下所

示：

g

= tanh(xtWx

(g) + ht−1Wh

(g)

+ b(g)) (6.4)

这里用 g 表示向记忆单

元添加的新信息。通过将

这个

g 加到上一时刻的 ct−1

上

，从而形成新的记忆。

图灵

社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

第 6章　Gated

RNN 238

6.2.6  输入门

最后，我们给

图

6-17 的 g 添加门，这里将这个

新添加的门称为输入门

（input gate）。添加输入门后，计算图如

图 6-18

所示。

ht−1

c t−1

ht

c

t

ht

xt

tanh

tanh

o

g i f

σσ σ

图6-18

添加输入门

输

入门判断新增信息 g 的各

个元素的价值有多大。输

入门不会不经考虑

就添

加新信息，而是会对要添

加的信息进行取舍。换句

话说，输入门会添加

加权

后的新信息。

在图 6-18 中，用 σ 表

示输入门，用 i

表示输出，此

时进行的计算如下

所示

：

i = σ(xtWx

(i)

+ ht−1Wh

(i) + b(i)) (6.5)

然后，将 i 和 g 的对应元素的

乘积添加到记忆单元中

。以上就是对

LSTM

内部处理的

说明。

6.2 梯度消失和 LSTM 239

LSTM

有 多 个

“变 体”。这 里 说

明 的 LSTM 是最有

代表性的

LSTM，也有许多在门

的连接方式上稍微不同

的其他 LSTM。

6.2.7  LSTM的梯度的流动

上

面我们介绍了 LSTM 的结构，那

么，为什么它不会引起梯

度消失呢？

其原因可以通

过观察记忆单元 c 的反向

传播来了解（图 6-19）。

图6-19　记忆单

元的反向传播

ht−1

ht−1 ht

xt−1 xt

c

t−1

ht

c t

ht−2

f

g i o f g i

o f g i o

c

t−2

tanh tanh tanh

tanh σσ

σ tanh σ σ σ tanh

σ σ σ

xt+1

ht+1

ht+1

c t+1

在图 6-19 中，我

们仅关注记忆单元，绘制

了它的反向传播。此时，记

忆单元的反向传播仅流

过“+”和“×”节点。“+”节点将上游传

来的梯度

原样流出，所以

梯度没有变化（退化）。

而“×”节

点的计算并不是矩阵乘

积，而是对应元素的乘积

（阿达玛

积）。顺便说一下，在

之前的 RNN 的反向传播中，我

们使用相同的权重矩

阵

重复了多次矩阵乘积计

算，由此导致了梯度消失

（或梯度爆炸）。而这里

的

LSTM 的

反向传播进行的不是矩

阵乘积计算，而是对应元

素的乘积计算，

而且每次

都会基于不同的门值进

行对应元素的乘积计算

。这就是它不会发生

梯度

消失（或梯度爆炸）的原因

。

图 6-19

的“×”节点的计算由遗忘

门控制（每次输出不同的

门值）。遗忘

门认为“应该忘

记”的记忆单元的元素，其

梯度会变小；而遗忘门认

为“不能

忘记”的元素，其梯

度在向过去的方向流动

时不会退化。因此，可以期

待记忆

单元的梯度（应该

长期记住的信息）能在不

发生梯度消失的情况下

传播。

第 6章　Gated RNN 240

从以上讨论可

知，LSTM的记忆单元不会（难以

）发生梯度消失。因此，

可以

期待记忆单元能够保存

（学习）长期的依赖关系。

LSTM是

Long Short-Term Memory（长短期记忆）的缩写，意思

是

可以长（Long）时间维持短期

记忆（Short-Term Memory）。

6.3 LSTM的实现 

下面，我们来

实现 LSTM。这里将进行单步处

理的类实现为 LSTM

类，

将整体

处理 T 步的类实现为 TimeLSTM 类。现

在我们先来整理一下

LSTM 中

进行的计算，如下所示：

f = σ(xtWx

(f)

+ ht−1Wh

(f) + b(f))

g

= tanh(xtWx

(g) + ht−1Wh

(g)

+ b(g))

i = σ(xtWx

(i)

+ ht−1Wh

(i) + b(i))

o

= σ(xtWx

(o) + ht−1Wh

(o)

+ b(o))

(6.6)

	 ct

= f  ct−1 + g

 i (6.7)

	 ht

= o  tanh(ct) (6.8)

以

上就是

LSTM 进行的计算。这里

需要注意式 (6.6) 中的 4 个仿射

变换。

这里的仿射变换是

指 xWx + hWh + b

这样的式子。式 (6.6) 中通过

4 个式

子分别进行仿射变

换，但其实可以整合为通

过 1

个式子进行，如图 6-20 所示

。

6.3 LSTM的实现 241

图6-20　整合4个权重，通

过1次仿射变换进行4个计

算

xtWx

(f) + ht−1Wh

(f) + b(f)

xtWx

(g) +

ht−1Wh

(g) + b(g)

xtWx

(i)

+ ht−1Wh

(i) + b(i)

xtWx

(o)

xtWx

+ ht−1Wh

(o)

ht−1Wh

+ b

b

(o)

xt Wx

(f) Wx

(g) Wx

(i) Wx

(o)

Wx

(f)

Wx

(g)

Wx

(i)

Wx

(o)

+ Wh

(f)

Wh

(g) Wh

(i) Wh

(o)

W(f)

h W(g)

h W(i)

h

W(o)

h

ht−1 +

+ +

b(f) b(g) b(i) b(o)

b(f) b(g)

b(i) b(o)

在图 6-20 中，4 个权重（或偏置

）被整合为了

1 个。如此，原本

单独

执行 4 次的仿射变换

通过 1

次计算即可完成，可

以加快计算速度。这是因

为

矩阵库计算“大矩阵”时

通常会更快，而且通过将

权重整合到一起管理，源

代码也会更简洁。

假设 Wx、Wh 和

b 分别包含

4 个权重（或偏置

），此时 LSTM 的计算

图如图 6-21

所示

。

  第 6章　Gated

RNN 242

图6-21　整合4个权重进行

仿射变换的LSTM的计算图

ht−1

b

f

g i o

xt

ht

ht

c t c t−1

MatMul

MatMul

tanh

tanh

slice

Wh

Wx

σ σ σ

如

图 6-21 所示，先一起执行

4 个仿

射变换。然后，基于 slice 节点，取

出 4 个结果。这个

slice 节点很简

单，它将仿射变换的结果

（矩阵）均等地

分成 4 份，然后

取出内容。在 slice

节点之后，数

据流过激活函数（sigmoid

函数或

tanh 函数），进行上一节介绍的

计算。

现在，参考图 6-21，我们来

实现 LSTM

类。首先来看一下 LSTM 类

的初始

化代码（ common/time_layers.py）。

class

LSTM:

 def __init__(self, Wx, Wh,

b):

 self.params = [Wx, Wh,

b]

 self.grads = [np.zeros_like(Wx), np.zeros_like(Wh),

np.zeros_like(b)]

 self.cache = None

初始化的

参数有权重参数

Wx、Wh 和偏置

b。如前所述，这些权重（或偏

置）整合了 4 个权重。把这些

参数获得的权重参数设

定给成员变量 params，

并初始化

形状与之对应的梯度。另

外，成员变量

cache 保存正向传

播的中间

结果，它们将在

反向传播的计算中使用

。

接下来实现正向传播的

forward(x, h_prev, c_prev)

方法。它的参数接

收当前

时刻的输入 x、上一时刻的

隐藏状态 h_prev，以及上一时刻

的记忆单

6.3 LSTM的实现

243

元 c_prev（ common/time_layers.py）。

def forward(self,

x, h_prev, c_prev):

 Wx, Wh,

b = self.params

 N, H

= h_prev.shape

 A = np.dot(x,

Wx) + np.dot(h_prev, Wh) + b

# slice

 f = A[:,

:H]

 g = A[:, H:2*H]

i = A[:, 2*H:3*H]

 o

= A[:, 3*H:]

 f =

sigmoid(f)

 g = np.tanh(g)

i = sigmoid(i)

 o =

sigmoid(o)

 c_next = f *

c_prev + g * i

h_next = o * np.tanh(c_next)

self.cache = (x, h_prev, c_prev, i,

f, g, o, c_next)

 return

h_next, c_next

首先

进行仿射变换。重复一下

，此时的成员变量 Wx、Wh 和 b

保存

的是 4

个权重，矩阵的形状

将变为如图 6-22 所示的样子

。

第 6章　Gated RNN 244

图6-22

仿射变换的形状

的改变（省略偏置）

Wx

(f)

Wx

(g) Wx

(i) Wx

(o)

W(f)

h W(g)

h W(i)

h W(o)

h

xt

Wx ht−1 Wh A

N ×D

N ×4H N ×D N ×4H

N ×4H

̭㜡

̭㜡

在图 6-22

中

，批大小是 N，输入数据的维

数是 D，记忆单元和隐藏状

态

的维数都是 H。另外，计算

结果 A

中保存了 4 个仿射变

换的结果。因此，通

过 A[:, :H]、A[:,

H:2*H] 这样

的切片取出数据，并分配

给之后的运算节点。

参考

LSTM 的数学式和计算图，剩余

的实现应该不难。

LSTM 层中保

存了

4 个权重。这样一来，LSTM 层

只需管理 Wx、

Wh和 b这

3 个参数。顺

便说一下，RNN 层中也保存着

Wx、Wh和 b

这 3

个参数。LSTM 层和 RNN 层的参

数数量虽然相同，但是它

们的形状不一样。

LSTM 的反向

传播可以通过将图

6-21 的计

算图反方向传播而求得

。基

于前面介绍的知识，这

并不困难。不过，因为 slide 节点

是第一次见到，所

以我们

简要说明一下它的反向

传播。

slice 节点将矩阵分成了

4 份，因此它的反向传播需

要整合 4 个梯度，

如图

6-23 所示

。

6.3 LSTM的实现 245

图6-23

slice节点的正向传

播（上）和反向传播（下）

f

(N×H)

(N×H) (N×H) (N×H)

(N×H)

(N×H) (N×H) (N×H)

g i

o

df dg di do

df

dg di do

f g i

o

A

(N×4H)

dA

(N×4H)

slice

slice

由图

6-23 可知，在 slice 节点的反向传播

中，拼接 4

个矩阵。图中有 4

个

梯度 df、dg、di 和 do，将它们拼接成

dA。如

果通过 NumPy 进行，则可

以使用

np.hstack()。np.hstack() 在水平方向上将参数中

给定的数组拼接起

来（垂

直方向上的拼接使用

np.vstack()）。因

此，上述处理可以用下面

1 行

代码完成。

dA = np.hstack((df,

dg, di, do))

以上就是对

slice 节点的反向传播的说明

。

Time

LSTM层的实现

现在我们继续

TimeLSTM 的实现。Time LSTM 层是整体处理 T

个

时序数

据的层，由 T 个 LSTM 层构

成，如图

6-24 所示。

  第 6章

Gated RNN 246

图6-24 Time LSTM的输

入和输出

hs = (h0,h1, · · ·

,hT −1)

hs

xs

=

xs

= (x0, x1, · · ·

, xT −1)

Time LSTM LSTM

LSTM LSTM

如 前 所 述，RNN

中使

用 Truncated BPTT 进 行 学

习。Truncated 

BPTT 以适当的长

度截断反向传播的连接

，但是需要维持正向传播

的数据

流。为此，如图 6-25

所示

，将隐藏状态和记忆单元

保存在成员变量中。这

样

一来，在调用下一个 forward() 函数

时，就可以继承上一时刻

的隐藏状态

（和记忆单元

）。

...

LSTM LSTM LSTM LSTM LSTM LSTM

hs0 hs1

xs0 xs1

h c

Ԋႅౕ

TimeLSTM

ㆨ⮱᜽অअ䛼͚

图6-25 Time LSTM的反向传播的输入和

输出

我们已经实现了 Time RNN 层

，这里也以同样的方式实

现 Time

LSTM 层。TimeLSTM 可以像下面这样实

现（ common/time_layers.py）。

class TimeLSTM:

def __init__(self, Wx, Wh, b, stateful=False):

self.params = [Wx, Wh, b]

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

6.3 LSTM的实现 247

self.grads = [np.zeros_like(Wx), np.zeros_like(Wh),

np.zeros_like(b)]

self.layers = None

 self.h, self.c

= None, None

 self.dh =

None

 self.stateful = stateful

def forward(self, xs):

 Wx, Wh,

b = self.params

 N, T,

D = xs.shape

 H =

Wh.shape[0]

 self.layers = []

hs = np.empty((N, T, H), dtype='f')

if not self.stateful or self.h is

None:

 self.h = np.zeros((N, H),

dtype='f')

 if not self.stateful or

self.c is None:

 self.c =

np.zeros((N, H), dtype='f')

 for t

in range(T):

 layer = LSTM(*self.params)

self.h, self.c = layer.forward(xs[:, t, :],

self.h, self.c)

 hs[:, t, :]

= self.h

 self.layers.append(layer)

 return

hs

 def backward(self, dhs):

Wx, Wh, b = self.params

N, T, H = dhs.shape

D = Wx.shape[0]

 dxs =

np.empty((N, T, D), dtype='f')

 dh,

dc = 0, 0

 grads

= [0, 0, 0]

 for

t in reversed(range(T)):

  第

6章　Gated RNN 248

 layer

= self.layers[t]

 dx, dh, dc

= layer.backward(dhs[:, t, :] + dh,

dc)

 dxs[:, t, :] =

dx

 for i, grad in

enumerate(layer.grads):

 grads[i] += grad

for i, grad in enumerate(grads):

self.grads[i][...] = grad

 self.dh =

dh

 return dxs

 def

set_state(self, h, c=None):

 self.h, self.c

= h, c

 def reset_state(self):

self.h, self.c = None, None

在

LSTM 中

，除了隐藏状态 h 外，还使用

记忆单元 c。TimeLSTM 类的实

现和 TimeRNN 类

几乎一样。这里仍通过参

数 stateful 指定是否维持状态。接

下来，我们使用这个 TimeLSTM

创建

语言模型。

6.4 使用LSTM的语言模

型 

Time LSTM

层的实现完成了，现在

我们来实现正题——语言模

型。

这里实现的语言模型

和上一章几乎是一样的

，唯一的区别是，上一章使

用

Time RNN 层的地方这次使用 Time

LSTM 层

，如图 6-26 所示。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享

尊重版权

6.4 使用 LSTM的语

言模型 249

Time

Softmax

with Loss

Time Softmax

with

Loss

Time Affine Time Affine

Time

Embedding Time Embedding

Time RNN Time

LSTM

ts

ws ws

ts

ᢌ๞

ᢌ๞

图6-26 语言模型的网

络结构。左图是上一章创

建的使用Time RNN的模型，右图是

本

章创建的使用Time LSTM的模型

由图

6-26 可知，这里和上一章

实现的语言模型的差别

在于使用了

LSTM。我们将图 6-26 右

图中的神经网络实现为

Rnnlm 类。Rnnlm

类和上一

章介绍的 SimpleRnnlm 类

几乎相同，但是增加了一

些新方法。下面给出使用

LSTM 层实现的 Rnnlm

类的代码 A。

import sys

sys.path.append('..')

from

common.time_layers import *

A 这里

给出的代码对应于ch06/rnnlm.py。ch06/rnnlm.py通过

继承BaseModel类，实现更为简略。

图

灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

  第

6章　Gated RNN 250

import pickle

class Rnnlm:

 def __init__(self, vocab_size=10000,

wordvec_size=100,

hidden_size=100):

 V, D, H

= vocab_size, wordvec_size, hidden_size

 rn

= np.random.randn

 # 初始化权重

embed_W = (rn(V, D) / 100).astype('f')

lstm_Wx = (rn(D, 4 * H)

/ np.sqrt(D)).astype('f')

 lstm_Wh = (rn(H,

4 * H) / np.sqrt(H)).astype('f')

lstm_b = np.zeros(4 * H).astype('f')

affine_W = (rn(H, V) / np.sqrt(H)).astype('f')

affine_b = np.zeros(V).astype('f')

 # 生成

层

self.layers = [

 TimeEmbedding(embed_W),

TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),

 TimeAffine(affine_W,

affine_b)

 ]

 self.loss_layer =

TimeSoftmaxWithLoss()

 self.lstm_layer = self.layers[1]

# 将所有的权重和梯度

整理到列表中

 self.params, self.grads =

[], []

 for layer in

self.layers:

 self.params += layer.params

self.grads += layer.grads

 def predict(self,

xs):

 for layer in self.layers:

xs = layer.forward(xs)

 return xs

def forward(self, xs, ts):

 score

= self.predict(xs)

 loss = self.loss_layer.forward(score,

ts)

 return loss

6.4 使用

LSTM的语

言模型 251

 def backward(self, dout=1):

dout = self.loss_layer.backward(dout)

 for layer

in reversed(self.layers):

 dout = layer.backward(dout)

return dout

 def reset_state(self):

self.lstm_layer.reset_state()

 def save_params(self, file_name='Rnnlm.pkl'):

with open(file_name, 'wb') as f:

pickle.dump(self.params, f)

 def load_params(self, file_name='Rnnlm.pkl'):

with open(file_name, 'rb') as f:

self.params = pickle.load(f)

Rnnlm 类将到 Softmax

层为止的

处理实现为 predict() 方法，这个方

法

在第 7 章进行文本生成

时还会用到。此外，该类还

添加了用于读写参数的

save_params()

和 load_params() 方法。剩下的实现与上

一章的 SimpleRnnlm 类

相同。

common/base_model.py 中有一个

BaseModel 类，该 类 实 现

了 save_

params()和 load_params()方法。因

此，通过继承 BaseModel类，也

能获得

数据读写的功能。另外，BaseModel类

的实现还进行了优化，

以

支持 GPU 和进行缩位（使用 16 位

浮点数存储）。

下面，我们在

PTB

数据集上学习这个网络

。这次我们使用 PTB 数据

集的

所有训练数据进行学习

（上一章中只使用了 PTB 数据

集的一部分），代

码如下所

示（ ch06/train_rnnlm.py）。

import sys

sys.path.append('..')

from

common.optimizer import SGD

from common.trainer import

RnnlmTrainer

from common.util import eval_perplexity

第 6章　Gated RNN 252

from

dataset import ptb

from rnnlm import

Rnnlm

# 设定超参数

batch_size = 20

wordvec_size = 100

hidden_size = 100

# RNN的隐

藏状态向量的元素个数

time_size = 35 # RNN的展开大小

lr = 20.0

max_epoch = 4

max_grad = 0.25

# 读入训练数

据

corpus,

word_to_id, id_to_word = ptb.load_data('train')

corpus_test, _,

_ = ptb.load_data('test')

vocab_size = len(word_to_id)

xs = corpus[:-1]

ts = corpus[1:]

# 生成模型

model = Rnnlm(vocab_size, wordvec_size,

hidden_size)

optimizer = SGD(lr)

trainer =

RnnlmTrainer(model, optimizer)

# ❶ 应用梯度裁

剪进行学习

trainer.fit(xs,

ts, max_epoch, batch_size, time_size, max_grad,

eval_interval=20)

trainer.plot(ylim=(0, 500))

# ❷ 基于测试数

据进行评价

model.reset_state()

ppl_test = eval_perplexity(model, corpus_test)

print('test

perplexity: ', ppl_test)

# ❸ 保存参数

model.save_params()

这

里给出的代码和上一章

的代码（ ch05/train.py）有很多相同的地

方，

因此这里重点介绍不

同的地方。首先，代码❶处使

用 RnnlmTrainer 类进行模

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

6.4 使用

LSTM的

语言模型 253

型的学习。RnnlmTrainer 类的

fit() 方法求模型的梯度，更新

模型的参数。

另外，在方法

内部，通过指定

max_grad 参数，从而

应用梯度裁剪。顺便说一

下，fit() 方法内部进行的实现

如下所示（这里给出的是

伪代码）。

# 求梯度

model.forward(...)

model.backward(...)

params, grads = model.params, model.grads

# 梯度裁剪

if max_grad is not None:

clip_grads(grads, max_grad)

# 更新参数

optimizer.update(params, grads)

我们在 6.1.4 节将梯

度裁剪实现为了 clip_grads(grads, max_grad)，这

里使

用该方法进行梯度裁剪

。

另外，通过❶处的 fit() 方法的参

数 eval_interval=20，每 20 次迭代对

困惑度进

行 1 次评价。因为这次的数

据量很大，所以没有对每

个 epoch 进行

评价，而是每

20 次迭

代评价 1 次。后面我们会将

评价结果用 plot() 方法绘

制成

图。

在学习结束后，在代码

❷处使用测试数据对困惑

度进行评价。这里需要

注

意的是，此时需要先重置

模型的状态（LSTM 的隐藏状态

和记忆单元）。

此外，因为评

价困惑度的函数 eval_perplexity()

在 common/util.py 中已

经实

现，所以直接使用即

可。

最后，在代码❸处将学习

好的参数保存到外部文

件。在下一章生成句子

时

，将会使用这些学习好的

权重参数。

以上就是 RNNLM 的学

习代码。执行代码后，在终

端上会输出图 6-27

的结果。

第

6章　Gated RNN 254

图6-27　终端的输出结果

在

图 6-27 中，每 20 次迭代输出 1

次困

惑度的值。我们来看一下

结果，

刚开始的困惑度为

10 000.84，这意味着下一个单词的

候选个数能减少到

10 000 个左

右。因为这次数据集的词

汇量是

10 000 个，所以这是什么

也没

学习的状态，相当于

猜测。但是随着学习的进

行，困惑度开始变好。实际

上，当迭代超过 300 次时，困惑

度已经降到了

400 以下。现在

，我们看一下

困惑度的演

变图，如图 6-28 所示。

6.5

进一步改

进RNNLM 255

图6-28　困惑度的演变（每20次

迭代对训练数据进行1次

评价）

500

400

300

200

100

0

0 50

100

䔚А⁎᪝

 (×20)

150 200

250

在这次的实验中，一

共进行了 4 个 epoch 的学习（按迭

代来算，相当于

1327 * 4 次）。如图 6-28 所

示，困惑度顺利下降，最终

达到

100 左右。基于

最终的测

试数据的评价（源代码❷处

）结果为 136.07 ...。该结果在每次执

行时都不相同，但是都在

135 前后。换句话说，我们的模

型成长到了能将下

一个

单词的候选个数（从 10 000 个）缩

小到 136 个左右的水平。

那么

，136 这样的困惑度在实践中

是什么水平呢？说实话，这

并不是一

个很好的结果

。在 2017 年的一个研究中，PTB 数据

集上的困惑度已经降到

了

60 以下 [34]。我们的模型还有

很大的改进空间，下面我

们就来进一步改进

现有

的 RNNLM。

6.5

进一步改进RNNLM 

本节我们

先针对当前的 RNNLM 说明 3

点需

要改进的地方，然后实施

这些改进，并评价最后精

度提高了多少。 ఝᗾᏓ

  第 6章

Gated RNN 256

6.5.1  LSTM层的

多层化

在使用 RNNLM 创建高精

度模型时，加深 LSTM 层（叠加多

个 LSTM

层）的方法往往很有效

。之前我们只用了一个 LSTM 层

，通过叠加多个

层，可以提

高语言模型的精度。例如

，在图 6-29 中，RNNLM

使用了两个

LSTM 层。

图

6-29　使用两个LSTM层的RNNLM

Softmax

Softmax Softmax

Affine Affine Affine

LSTM

LSTM LSTM

LSTM LSTM LSTM

Embedding

Embedding Embedding

yT −1 y1 y0

wT −1 w0 w1

...

...

...

...

...

...

...

...

...

图 6-29 显示了

叠加两个 LSTM 层的例子。此时

，第一个

LSTM 层的

隐藏状态是

第二个 LSTM 层的输入。按照同

样的方式，我们可以叠加

多个

6.5

进一步改进RNNLM 257

LSTM 层，从而

学习更加复杂的模式，这

和前馈神经网络时的层

加深是一样

的。在前作《深

度学习入门：基于 Python

的理论

与实现》中，我们通过叠

加

多个 Affine 层和 Convolution 层，创建了表现

力更好的模型。

那么，应该

叠加几个层呢？这其实是

一个关于超参数的问题

。因为层

数是超参数，所以

需要根据要解决的问题

的复杂程度、能给到的训

练数据

的规模来确定。顺

便说一句，在 PTB 数据集上学

习语言模型的情况下，当

LSTM 的层数为

2 ～ 4 时，可以获得比

较好的结果。

据报道，谷歌

翻译中使用的 GNMT

模型 [50] 是叠

加了 8 层 LSTM

的

网络。如该例所

示，如果待解决的问题很

难，又能准备大量的训练

数据，

就可以通过加深 LSTM 层

来提高精度。

6.5.2

基于Dropout抑制过

拟合

通过叠加 LSTM 层，可以期

待能够学习到时序数据

的复杂依赖关系。

换句话

说，通过加深层，可以创建

表现力更强的模型，但是

这样的模型往往

会发生

过拟合（overfitting）。更糟糕的是，RNN

比常

规的前馈神经网络更

容

易发生过拟合，因此 RNN 的过

拟合对策非常重要。

过拟

合是指过度学习了训练

数据的状态，也就是说，过

拟合是一种

缺乏泛化能

力的状态。我们想要的是

一个泛化能力强的模型

，因此

必须基于训练数据

和验证数据的评价差异

，判断是否发生了过拟合

，

并据此来进行模型的设

计。

抑制过拟合已有既定

的方法：一是增加训练数

据；二是降低模型的复杂

度。我们会优先考虑这两

个方法。除此之外，对模型

复杂度给予惩罚的正则

化也很有效。比如，L2 正则化

会对过大的权重进行惩

罚。

此外，像 Dropout[9]

这样，在训练时

随机忽略层的一部分（比

如

50%）神经元，也可以被视为

一种正则化（图 6-30）。本节我们

将仔细研究

Dropout，并将其应用

于 RNN。

第 6章　Gated RNN 258

图6-30

Dropout 的概念图（参考

文献 [9]）: 左边是常规的神经

网络，右边是使用了

Dropout的网

络

（a）常规的神经网络

（b）使用

Dropout的神经网络

如图 6-30 所示，Dropout 随

机选择一部分神经元，然

后忽略它们，停

止向前传

递信号。这种“随机忽视”是

一种制约，可以提高神经

网络的泛

化能力。我们在

前作《深度学习入门：基于

Python 的理论与实现》中已

经实

现了 Dropout。如图 6-31 所示，当时我们

给出了在激活函数后插

入

Dropout 层的示例，并展示了它

有助于抑制过拟合。

6.5 进一

步改进RNNLM 259

Dropout

Dropout

ReLU ReLU

Affine Affine

Softmax

with Loss

t

x

ᢌ๞

图6-31

将Dropout层应用于前

馈神经网络的例子

那么

，在使用 RNN 的模型中，应该将

Dropout 层插入哪里呢？首先可

以

想到的是插入在

LSTM 层的时

序方向上，如图 6-32 所示。不过

答案是，

这并不是一个好

的插入方式。

Dropout

Dropout

Dropout

Dropout

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

图6-32　不好的例

子：在时序方向上插入Dropout层

如果在时序方向上插入

Dropout，那么当模型学习时，随着

时间的推

图灵社区会员

Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 6章

Gated RNN 260

移，信

息会渐渐丢失。也就是说

，因 Dropout 产生的噪声会随时间

成比例

地积累。考虑到噪

声的积累，最好不要在时

间轴方向上插入 Dropout。因

此，如

图 6-33 所示，我们在深度方向

（垂直方向）上插入 Dropout

层。

图6-33　好

的例子：在深度方向（垂直

方向）上插入Dropout层

Dropout Dropout Dropout

Dropout Dropout Dropout

Dropout Dropout Dropout

LSTM LSTM LSTM

LSTM LSTM LSTM

...

. . . . .

.

. . .

. .

. . . .

...

这样一来

，无论沿时间方向（水平方

向）前进多少，信息都不会

丢失。

Dropout 与时间轴独立，仅在

深度方向（垂直方向）上起

作用。

现在比较一下图 6-31 和

图 6-33。图

6-31 的例子展示了在前

馈神

经网络中使用 Dropout 的情

况，这个例子在深度方向

上应用了

Dropout。以相同的方式

，在图

6-33 中，通过在深度方向

上应用

Dropout，有望和前馈神经

网络时一样，能够抑制过

拟合。

6.5 进一步改进RNNLM 261

如前所

述，“常规的 Dropout”不适合用在时

间方向上。但是，最近的

研

究提出了多种方法来实

现时间方向上的 RNN 正则化

。比如，文献 [36]

中

提出的“变分

Dropout”（variational dropout）就被成功地应用在了时

间

方向上。

除了深度方向

，变分 Dropout

也能用在时间方向

上，从而进一步提高

语言

模型的精度。如图 6-34 所示，它

的机制是同一层的 Dropout 使用

相同

的 mask。这里所说的 mask 是指

决定是否传递数据的随

机布尔值。

图6-34 变分Dropout的例子

：具有相同图示的Dropout使用相

同的mask。像这样，

位于同一层

的Dropout使用相同的mask，对时间方

向上的Dropout也有效果

（参见彩

图）

Dropout

Dropout

Dropout Dropout

Dropout

LSTM LSTM LSTM

LSTM LSTM

LSTM

Dropout Dropout

Dropout Dropout Dropout

Dropout Dropout Dropout

Dropout Dropout ...

...

...

...

...

...

...

如图 6-34 所示，通过同一层

的 Dropout 共用 mask，mask

被“固定”。

如此一来

，信息的损失方式也被“固

定”，所以可以避免常规 Dropout 发

生

的指数级信息损失。

据

说变分

Dropout 比常规 Dropout 的 效 果

更

好。不 过，本 章

并不打算使

用变分 Dropout，而是仍使用常规

Dropout。变分

Dropout

的想法很简单，感兴

趣的读者可以自己尝试

实现一下。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 6章　Gated RNN 262

6.5.3

权重

共享

改进语言模型有一

个非常简单的技巧，那就

是权重共享（weight tying）[37] [38]。

weight tying

可以直译为

“权重绑定”。如图 6-35 所示，其含

义就是共享

权重。

图6-35　语言

模型中共享权重的例子

：Embedding层和Softmax前的Affine层共享权重

Softmax

with Loss

Affine

LSTM

LSTM

Embedding

t

w

Ϙᱰ䛺ڞ

ᢌ๞

6.5

进

一步改进RNNLM 263

如图 6-35 所示，绑定

（共享）Embedding 层和

Affine 层的权重的技

巧

在于权重共享。通过在

这两个层之间共享权重

，可以大大减少学习的参

数数

量。尽管如此，它仍能

提高精度。真可谓一石二

鸟！

现在，我们来考虑一下

权重共享的实现。这里，假

设词汇量为 V，

LSTM 的隐藏状态

的维数为 H，则 Embedding 层的权重形

状为 V

× H，

Affine 层的权重形状为 H ×

V。此

时，如果要使用权重共享

，只需将

Embedding 层权重的转置设

置为 Affine 层的权重。这个非常

简单的技巧可

以带来出

色的结果。

为什么说权重

共享是有效的呢？直观上

，共享权重可以减少需要

学

习的参数数量，从而促

进学习。另外，参数数量减

少，还能收获抑

制过拟合

的好处。论文 [38] 从理论上描

述了权重共享为什么有

用，

感兴趣的读者可以参

考一下。

6.5.4  更好的RNNLM的实现

至

此，我们介绍了 RNNLM 的

3 点有待

改进的地方。接下来，我们

来看一下这些技巧会在

多大程度上有效。这里，将

图 6-36 的层结构实现为

BetterRnnlm 类。

第

6章　Gated RNN 264

Time

Softmax

with Loss

Time Affine

Time

Dropout

Time Dropout

Time Dropout

Time

LSTM

Time LSTM

Time Embedding

ts

ws

Ϙᱰ䛺ڞ

ᢌ๞

图6-36 BetterRnnlm类的网络结构

6.5

进一

步改进RNNLM 265

如图 6-36 所示，改进的

3 点如下：

• LSTM 层的多层化（此处

为 2 层）

•

使用 Dropout（仅应用在深度

方向上）

•　权重共享（Embedding 层和 Affine

层

的权重共享）

现在，我们来

实现进行了这 3 点改进的

BetterRnnlm 类，如下所示

（

ch06/better_rnnlm.py）。

import sys

sys.path.append('..')

from common.time_layers

import *

from common.np import *

from common.base_model import BaseModel

class BetterRnnlm(BaseModel):

def __init__(self, vocab_size=10000, wordvec_size=650,

 hidden_size=650,

dropout_ratio=0.5):

 V, D, H =

vocab_size, wordvec_size, hidden_size

 rn =

np.random.randn

 embed_W = (rn(V, D)

/ 100).astype('f')

 lstm_Wx1 = (rn(D,

4 * H) / np.sqrt(D)).astype('f')

lstm_Wh1 = (rn(H, 4 * H)

/ np.sqrt(H)).astype('f')

 lstm_b1 = np.zeros(4

* H).astype('f')

 lstm_Wx2 = (rn(H,

4 * H) / np.sqrt(H)).astype('f')

lstm_Wh2 = (rn(H, 4 * H)

/ np.sqrt(H)).astype('f')

 lstm_b2 = np.zeros(4

* H).astype('f')

 affine_b = np.zeros(V).astype('f')

# 3点改进!

 self.layers = [

TimeEmbedding(embed_W),

 TimeDropout(dropout_ratio),

 TimeLSTM(lstm_Wx1, lstm_Wh1,

lstm_b1, stateful=True),

 TimeDropout(dropout_ratio),

 TimeLSTM(lstm_Wx2,

lstm_Wh2, lstm_b2, stateful=True),

 TimeDropout(dropout_ratio),

TimeAffine(embed_W.T, affine_b) # 权重

共享!!

第 6章　Gated RNN 266

]

 self.loss_layer = TimeSoftmaxWithLoss()

self.lstm_layers = [self.layers[2], self.layers[4]]

 self.drop_layers

= [self.layers[1], self.layers[3],

self.layers[5]]

 self.params,

self.grads = [], []

 for

layer in self.layers:

 self.params +=

layer.params

 self.grads += layer.grads

def predict(self, xs, train_flg=False):

 for

layer in self.drop_layers:

 layer.train_flg =

train_flg

 for layer in self.layers:

xs = layer.forward(xs)

 return xs

def forward(self, xs, ts, train_flg=True):

score = self.predict(xs, train_flg)

 loss

= self.loss_layer.forward(score, ts)

 return loss

def backward(self, dout=1):

 dout =

self.loss_layer.backward(dout)

 for layer in reversed(self.layers):

dout = layer.backward(dout)

 return dout

def reset_state(self):

 for layer in

self.lstm_layers:

 layer.reset_state()

灰色背景的代

码就是刚才所说的改进

的地方，具体而言，叠加两

个

Time LSTM

层， 使 用 Time Dropout 层，

并 在 Time Embedidng 层 和

Time Affine 层

之间共享权重。

下面进行

改进过的 BetterRnnlm 类的学习。在这

之前，我们稍微改动一

下

将要执行的学习代码。这

个改动是，针对每个 epoch 使用

验证数据评价困

惑度，在

值变差时，降低学习率。这

是一种在实践中经常用

到的技巧，并且

6.5 进一步改

进RNNLM

267

往往能有好的结果。这

里的实现参考了PyTorch的语言

模型的实现示例[39]，

学习代

码如下所示（ ch06/train_better_rnnlm.py）。

import sys

sys.path.append('..')

from common import config

#

在用GPU运行时

，请打开下面的注释（需要

cupy）

# ==============================================

# config.GPU =

True

# ==============================================

from common.optimizer import

SGD

from common.trainer import RnnlmTrainer

from

common.util import eval_perplexity

from dataset import

ptb

from better_rnnlm import BetterRnnlm

#

设定超参数

batch_size = 20

wordvec_size =

650

hidden_size = 650

time_size =

35

lr = 20.0

max_epoch =

40

max_grad = 0.25

dropout =

0.5

# 读入训练数

据

corpus, word_to_id, id_to_word

= ptb.load_data('train')

corpus_val, _, _ =

ptb.load_data('val')

corpus_test, _, _ = ptb.load_data('test')

vocab_size = len(word_to_id)

xs = corpus[:-1]

ts = corpus[1:]

model = BetterRnnlm(vocab_size,

wordvec_size, hidden_size, dropout)

optimizer = SGD(lr)

trainer = RnnlmTrainer(model, optimizer)

图灵社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊

重版权

  第 6章

Gated RNN 268

best_ppl = float('inf')

for epoch in range(max_epoch):

 trainer.fit(xs,

ts, max_epoch=1, batch_size=batch_size,

 time_size=time_size, max_grad=max_grad)

model.reset_state()

 ppl = eval_perplexity(model, corpus_val)

print('valid perplexity: ', ppl)

 if

best_ppl > ppl:

 best_ppl =

ppl

 model.save_params()

 else:

lr /= 4.0

 optimizer.lr =

lr

 model.reset_state()

 print('-' *

50)

这里针对每

个 epoch 使用验证数据评价困

惑度，当它比之前的困惑

度

（best_ppl）低时，将学习率乘以 1/4。为

此，我们用

for 循环反复执行

以下

处理：通过 RnnlmTrainer 类的 fit()

方法

进行一个 epoch 的学习，然后使

用

验证数据评价困惑度

。现在让我们运行一下学

习代码。

这个学习需要相

当长的时间。在用 CPU

运行的

情况下，需要 2 天左

右；而如

果用 GPU 运行，则能在

5 小时左

右完成（在用 GPU 运行时，

需要

去掉文件顶部 import语句中的

#

config.GPU = True这行注释）。

此外，从出版社

网站的本书主页可以获

得学习好的权重。

执行上

面的代码，困惑度平稳下

降，最终在测试数据上获

得了困惑度为

75.76

的结果（每

次运行结果不同）。考虑到

改进前的 RNNLM 的困惑度约

为

136，这个结果可以说提升很

大。通过 LSTM 的多层化提高表

现力，通

过 Dropout 提高泛化能力

，通过权重共享有效利用

权重，从而实现了精度

的

大幅提高。

6.5 进一步改进RNNLM

269

6.5.5  前

沿研究

至此，我们对 RNNLM

的改

进就结束了。通过对 RNNLM 进行

若干改

造，精度显著提升

，在 PTB 数据集的测试数据上

达到了

75 左右的困惑度，

可

以说是一个还算不错的

结果。不过，前沿研究走得

更远。这里我想简单地

介

绍一下最新的研究结果

，让我们来看一下图 6-37。

图6-37

各

模型在PTB数据集上的结果

（摘自文献[34]）。表中的Parameters是参数

总

数，Validation是验证数据的困惑

度，Test是测试数据的困惑度

图 6-37 摘自文献 [34]，该表总结了

过去各个阶段最优语言

模型在 PTB

数据集上的困惑

度结果。由 Test 列可知，随着新

方法被提出，困惑度在下

降，最后一行的结果是 52.8。实

际上，这个 52.8 是一个非常好

的结果。在

PTB 数据集上的困

惑度接近 50，这在几年前还

是无法想象的。

这里只展

示了最先进的研究结果

。当然，我们的模型和它还

有相当的距

离，但是图 6-37

中

的最先进的模型和我们

的模型有很多共同点。比

如，最

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

第 6章　Gated RNN 270

先进的模

型使用了多层

LSTM 模型，并进

行了基于 Dropout 的正则化（变

分

Dropout 和

DropConnectA）和权重共享。在此基础

上，它进一步使用

了最优

化和正则化的几个技巧

，并严格进行了超参数的

调整，最终达成了

52.8 这样惊

人的值。

图6-37中有一个名为

AWD-LSTM 3-layer

LSTM (tied) + continuous 

cache

pointer 的模型。这个 continuous cache pointer 技术基于

第

8 章会详细介绍的 Attention。Attention 是一项

非常重要的技术，

被应用

在许多地方。在语言模型

这个任务中，它也为精度

提高做出

了重大贡献。让

我们期待第

8 章的 Attention。

6.6 小结

本

章的主题是

Gated RNN，我们指出了

上一章的简单 RNN 中存在的

梯度消失（或梯度爆炸）问

题，说明了作为替代层的

Gated RNN（具体指

LSTM

和 GRU 等）的有效性。这

些层使用门这一机制，能

够更好地控制数

据和梯

度的流动。

另外，本章使用

LSTM 层创建了语言模型，并在

PTB

数据集上进行了

学习，评

价了困惑度。另外，通过 LSTM 的

多层化、Dropout 和权重共享

等技

巧，成功地大幅提高了精

度。这些技巧也被实际用

在了

2017 年的最前

沿研究中

。

下一章我们将使用语言

模型生成文本。之后，像机

器翻译一样，我们将

仔细

考察一个将某种语言转

换为另一种语言的模型

。

A

DropConnect是指随机无视权重自身

的方法。

6.6 小结  271

本章所学的

内容

• 在简单 RNN 的学习中，存

在梯度消失和梯度爆炸

问题

• 梯度裁剪对解决梯

度爆炸有效，LSTM、GRU

等 Gated RNN 对解

决梯

度消失有效

•

LSTM 中有 3 个门：输

入门、遗忘门和输出门

• 门

有专门的权重，并使用

sigmoid 函

数输出 0.0 ～ 1.0 的实数

• LSTM 的多层化

、Dropout 和权重共享等技巧可以

有效改进语言模型

• RNN

的正

则化很重要，人们提出了

各种基于 Dropout 的方法

图灵社

区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

第

7章

基于RNN生成文本

不存在

什么完美的文章，就好像

没有完美的绝望。

——村上春

树《且听风吟》

在第

5 章和第

6 章中，我们仔细研究了 RNN 和

LSTM 的结构及其实现。

现在我

们已经在代码层面理解

了它们。在本章，RNN和LSTM将大显

身手，

我们将利用 LSTM 实现几

个有趣的应用。

首先，本章

将使用语言模型进行文

本生成。具体来说，就是使

用在语料

库上训练好的

语言模型生成新的文本

。然后，我们将了解如何使

用改进过的

语言模型生

成更加自然的文本。通过

这项工作，我们可以（简单

地）体验基

于 AI 的文本创作

。

另外，本章还会介绍一种

结构名为 seq2seq

的新神经网络

。seq2seq 是

“(from) sequence to sequence”（从时序到时序）的意思

，即将一个时序数

据转换

为另一个时序数据。本章

我们将看到，通过组合两

个 RNN，可以轻

松实现 seq2seq。seq2seq 可以应

用于多个应用，比如机器

翻译、聊天机器人

和邮件

自动回复等。通过理解这

个简单但聪明强大的

seq2seq，应

用深度学

习的可能性将

进一步扩大。

  第 7章

基于RNN生

成文本 274

7.1 使用语言模型生

成文本

我们已经用几章

的篇幅讨论了语言模型

。如前所述，语言模型可用

于各

种各样的应用，其中

具有代表性的例子有机

器翻译、语音识别和文本

生成。

这里，我们将使用语

言模型来生成文本。

7.1.1  使用

RNN生成文本的步骤

在上一

章中，我们使用 LSTM

层实现了

语言模型，这个语言模型

的网

络结构如图 7-1 所示。顺

便说一下，我们还实现了

整体处理（T 个）时序数

据的

Time

LSTM 层和 Time Affine 层。

图7-1

上一章实现的

语言模型：右图使用整体

处理时序数据的Time层；左图

是将其展

开后的层结构

Softmax

with Loss

Affine

Embedding

Embedding Embedding Time Embedding

LSTM LSTM

LSTM Time LSTM

Affine Affine Time

Affine

Softmax

with Loss Softmax

with

Loss Time Softmax

with Loss

wT

−1 ws w1 w0

ᢌ๞

ᢌ๞

...

...

...

...

...

...

现在我们来说明一下语

言模型生成文本的顺序

。这里仍以“you say 

goobye and i

say hello.”这一在语料库

上学习好的语言模型为

例，考虑将

单词 i 赋给这个

语言模型的情况。此时，这

个语言模型输出图 7-2

中的

概率

分布。

7.1 使用语言模型

生成文本  275

图7-2　语言模型输

出下一个出现的单词的

概率分布

Softmax

Affine

Embedding

LSTM

i

语言模型根据

已经出现的单词输出下

一个出现的单词的概率

分布。在图

7-2 的例子中，语言

模型输出了当给定单词

i 时下一个出现的单词的

概率分

布。那么，它如何生

成下一个新单词呢？

一种

可能的方法是选择概率

最高的单词。在这种情况

下，因为选择的是

概率最

高的单词，所以结果能唯

一确定。也就是说，这是一

种“确定性的”

方法。另一种

方法是“概率性地”进行选

择。根据概率分布进行选

择，这样

概率高的单词容

易被选到，概率低的单词

难以被选到。在这种情况

下，被选

到的单词（被采样

到的单词）每次都不一样

。

这里我们想让每次生成

的文本有所不同，这样一

来，生成的文本富有

变化

，会更有趣。因此，我们通过

后一种方法（概率性地选

择的方法）来 you say goodbye and ihello

.

  第 7章　基于RNN生

成文本

276

选择单词。回到我

们的例子中，如图 7-3 所示，假

设（概率性地）选择了单

词

say。

Softmax

Affine

Embedding

LSTM

i say

图7-3

根据概率分布采样一

个单词

图 7-3 中显示了根据

概率分布进行采样后结

果为 say 的例子。在图

7-3

的概率

分布中，say 的概率最高，所以

它被采样到的概率也最

高。不过请注

意，这里选到

say 并不是必然的（不是确定

性的），而是概率性的。因此

，

say

以外的其他单词根据出

现的概率也可能被采样

到。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

you

say

goodbye

and

i

hello

.

7.1 使用语言模型生

成文本  277

“确定性的”是指（算

法的）结果是唯一确定的

，是可预测的。在上例中，

假

设选择概率最高的单词

，那么这就是一种确定性

的算法。而“概

率性的”算法

则概率性地确定结果，因

此每次实验时选到的单

词都

会有所变化（或者说

，存在变化的可能性）。

接下

来，采样第 2 个单词。这只需

要重复一下刚才的操作

。也就是说，

将生成的单词

say

输入语言模型，获得单词

的概率分布，然后再根据

这个概

率分布采样下一

个出现的单词，如图 7-4 所示

。

Softmax

Affine

Embedding

LSTM

Softmax

Affine

Embedding

LSTM

i say hello

图7-4　重复概率分布的输出

和采样

之后根据需要重

复此过程即可（或者直到

出现

<eos> 这一结尾记号）。 you say goodbye and

ihello .you say goodbye and ihello

.

  第 7章

基于RNN生成文本 278

这样一来

，我们就可以生成新的文

本。

这里需要注意的是，像

上面这样生成的新文本

是训练数据中没有的新

生

成的文本。因为语言模

型并不是背诵了训练数

据，而是学习了训练数据

中单

词的排列模式。如果

语言模型通过语料库正

确学习了单词的出现模

式，我们

就可以期待该语

言模型生成的文本对人

类而言是自然的、有意义

的。

7.1.2

文本生成的实现

下面

我们进行文本生成的实

现。这里基于上一章实现

的 Rnnlm 类

（ ch06/rnnlm.py），来创建继承自它的

RnnlmGen

类，然后向这个类添加生

成

文本的方法。

类的继承

是指继承已有类，创建新

的类。在 Python 中，可以通过

class

New(Base):继承

Base类，创建 New类。

RnnlmGen 类的实现如下

所示（ ch07/rnnlm_gen.py）。

import

sys

sys.path.append('..')

import numpy as np

from common.functions import softmax

from ch06.rnnlm

import Rnnlm

from ch06.better_rnnlm import BetterRnnlm

class RnnlmGen(Rnnlm):

 def generate(self, start_id,

skip_ids=None, sample_size=100):

 word_ids = [start_id]

x = start_id

 while len(word_ids)

< sample_size:

 x = np.array(x).reshape(1,

1)

 score = self.predict(x)

p = softmax(score.flatten())

 sampled =

np.random.choice(len(p), size=1, p=p)

7.1 使用语言模型生成

文本

279

 if (skip_ids is None)

or (sampled not in skip_ids):

x = sampled

 word_ids.append(int(x))

return word_ids

这个类用 generate(start_id, skip_ids, sample_size)

生成本文

。此处，

参数 start_id 是第 1 个单词

ID，sample_size 表

示要采样的单词数量。另

外，

参数 skip_ids是单词 ID 列表（比如

，[12,

20]），它指定的单词将不被采

样。

这个参数用于排除 PTB 数

据集中的 <unk>、N 等被预处理过

的单词。

PTB 数据集对原始文

本进行了预处理，稀有单

词被 <unk>替换，

数字被 N替换。另

外，我们用 <eos>作为文本的分

隔符。

generate() 方法首先通过 model.predict(x) 输出

各个单词的得分（得

分是

正规化之前的值），然后基

于 p

= softmax(score)，使用 Softmax 函

数对得分进行

正规化，这样就获得了我

们想要的概率分布。接下

来，使用

np.random.choice()，根据这个概率分

布

p 采样下一个单词。关于

np.random.

choice()，我们已经在 4.2.6 节说明过了

。

model的

predict()方法进行的是 mini-batch 处理，所

以输入 x必

须是二维数组

。因此，即使在只输入 1

个单

词 ID 的情况下，也要

将它的

批大小视为 1，并将其整理

成形状为 1

× 1 的 NumPy 数组。

现在，使

用这个

RnnlmGen 类进行文本生成

。这里先在完全没有学习

的状

态（即权重参数是随

机初始值的状态）下生成

文本，代码如下所示（ ch07/

generate_text.py）。

import

sys

sys.path.append('..')

from rnnlm_gen import RnnlmGen

图灵

社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第

7章　基于RNN生成文本 280

from dataset import

ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')

vocab_size = len(word_to_id)

corpus_size = len(corpus)

model = RnnlmGen()

# model.load_params('../ch06/Rnnlm.pkl')

#

设定

start单词和skip单词

start_word = 'you'

start_id =

word_to_id[start_word]

skip_words = ['N', '<unk>', '



$']

skip_ids = [word_to_id[w] for w in skip_words]

# 生成文本

word_ids = model.generate(start_id, skip_ids)

txt = ' '.join([id_to_word[i] for i in word_ids])

txt = txt.replace(' <eos>', '.\n')

print(txt)

这里，第 1 个单词是 you，我们将它的单词 ID 设为 start_id，来进行

文本生成。另外，指定不参与采样的单词为 ['N', '<unk>', '$



']。生成文本

的 generate() 方法返回单

词 ID 列表，因此需要将单词

ID

列表转化为句子。

这可以

通过 txt = ' '.join([id_to_word[i]

for i in word_ids]) 这行代码进

行。join()

方法

通过“' 分隔符 '.join( 列表 )”这种形

式连接单词。下面我们

来

看一个具体的例子。

>>> ' '.join(['you', 'say', 'goodbye'])

'you

say goodbye'

运行

一下上面的代码，结果如

下。

you setback best

raised fill steelworkers montgomery kohlberg told

beam

worthy allied ban swedish aichi

mather promptly ramada explicit leslie

bets

discovery considering campaigns bottom petrie warm

large-scale

frequent temple grumman bennett ...

7.1 使用语言模型生成文

本  281

如你所见，输出的文本

是一堆乱七八糟的单词

。不过这可以理解，因为

这

里的模型权重使用的是

随机初始值，所以输出了

没有意义的文本。那么，

如

果换成学习好的语言模

型，结果会怎样呢？我们利

用上一章学习好的权重

来进行文本生成。为此，使

用 model.load_params('../ch06/Rnnlm.pkl') 读入

上一章学习好的

权重参数，并生成文本。我

们来看一下生成的文本

（每次的

结果都不一样）。

you

'll include one of them a

good problems.

 moreover so if

not gene 's corr experience with

the heat of bridges a

new

deficits model is non-violent what it

's a rule must exploit it.

there 's no tires industry could

occur.

 beyond my hours where

he is n't going home says

and japanese letter.

 knight transplants

d.c. turmoil with one-third of voters.

the justice department is ...

虽

然上面的结果中可以看

到多处语法错误和意思

不通的地方，不过

也有几

处读起来已经比较像句

子了。仔细看的话，这个模

型正确生成了主

语和动

词的组合，比如“you’ll include...”“there’ s no tires...”“knight

transplants...”等。再者，它

在一定程度上理解了形

容词和名词的使用方

法

，比如“good problems”“japanese letter”等。另外，开头的“you’ ll

include one of them a good

problems.”也是

一个含义通顺的句子。

如

上所述，上面的实验生成

的文本在某种程度上可

以说是正确的，不过

结果

中仍有许多不自然的地

方，改进空间很大。虽然不

存在“完美的文章”，

但是至

少我们可以追求更自然

的文章。为此，我们应该怎

么做呢？当然是使

用更好

的语言模型！

7.1.3

更好的文本

生成

如果有更好的语言

模型，就可能有更好的文

本。在上一章中，我们改进

了简单的 RNNLM，实现了“更好的

RNNLM”，将模型的困惑度从 136 降

至

75。现在，我们看一下这个“更

好的

RNNLM”生成文本的能力。

  第

7章　基于RNN生成文本 282

在上一

章中，我们进行了 BetterRnnlm类的学

习，并将学习好的

权重保

存为了文件。这里的实验

需要用到这个学习好的

权重文

件，我们可以从出

版社网站的本书主页获

取。将这个权重文件

放到

本书源代码的 ch06

目录下，即

可运行本实验的代码 ch07/

generate_better_text.py。

在

上一章中，我们将更好的

语言模型实现为了 BetterRnnlm 类。这

里，

像刚才一样，继承这个

类，并使之有生成文本的

能力。BetterRnnlmGen 类的

实现和刚刚的

RnnlmGen 类完全一样，此处省略具

体说明。

现在，我们让这个

更好的语言模型生成文

本。和之前一样，第 1

个单词

是 you。这样一来，下述文本会

被生成（ ch07/generate_better_text.py）。

you 've seen

two families and the women and

two other women of students.

the principles of investors that prompted

a bipartisan rule of which

had

a withdrawn target of black men

or legislators interfere with the

number

of plants can do to carry

it together.

 the appeal was

to deny steady increases in the

operation of dna and

educational damage

in the 1950s.

...

可以看出，这个模

型生成了比之前更自然

的文本（可能有些主观）。最

开始的句子“you’ ve

seen two families and the women...”正确使用了

主

语、动词和宾语，并且正

确学习了 and 的使用方法（two families and the

women）。其

他部分读起来总体上也

算说得过去。

虽然这里生

成的文本仍然存在若干

问题（特别是语义方面），但

是从某

种程度上来说，这

个更好的语言模型生成

了更加自然的文本。通过

进一步改

进这个模型，使

用更大规模的语料库，应

该能创造出更加自然的

文本。

最后，我们尝试给这

个更好的语言模型输入

“the meaning

of life is”，

让它生成后续的单词（这

是论文 [35] 中所做的实验）。为

了做这个实验，我

们按顺

序向模型输入 ['the', 'meaning', 'of', 'life']，进行正向

传播。此

时不使用任何输

出的结果，只是让

LSTM 层记住

这些单词的信息。然后，

以

单词 is 作为开始位置，生成

“the meaning

of life is”的后续内容。

7.2 seq2seq模型 283

这个实

验可以用 ch07/generate_better_text.py 实现。每次实验

生成的

文本都不太一样

，这里介绍一个有意思的

结果。

the meaning

of life is not a good

version of paintings.

如上所述，语言模型

给出的回答是“人生的意

义并不是一种状态良好

的

绘画”。虽然搞不懂什么

才是“状态良好的绘画”，不

过说不定这其中有什么

深刻的意义。

7.2

seq2seq模型 

这个世

界充满了时序数据。文本

数据、音频数据和视频数

据都是时序数

据。另外，还

存在许多需要将一种时

序数据转换为另一种时

序数据的任务，

比如机器

翻译、语音识别等。其他的

还有进行对话的聊天机

器人应用、将源

代码转为

机器语言的编译器等。

像

这样，世界上存在许多输

入输出均为时序数据的

任务。从现在开始，

我们会

考察将时序数据转换为

其他时序数据的模型。作

为它的实现方法，我

们将

介绍使用两个 RNN 的 seq2seq

模型。

7.2.1  seq2seq的

原理

seq2seq 模型也称为

Encoder-Decoder 模型。顾

名思义，这个模型有两

个

模块——Encoder（编码器）和 Decoder（解码器）。编

码器对输入数据

进行编

码，解码器对被编码的数

据进行解码。

编码是基于

某些既定规则的信息转

换过程。以字符码为例，将

字符“A”

转换为“1000001”（二进制）就是

一个编码的例子。而解码

则将被编

码的信息还原

到它的原始形态。仍以字

符码为例，这相当于将位

模

式的“1000001”转换为字符“A”。

  第

7章

基于RNN生成文本 284

现在，我们

举一个具体的例子来说

明 seq2seq 的机制。这里考虑将日

语

翻译为英语，比如将“吾

輩は猫である”A

翻译为“I am a cat”。此时，如

图 7-5

所

示，seq2seq 基于编码器和解码器

进行时序数据的转换。

图

7-5　基于编码器和解码器进

行翻译的例子

吾輩は猫である

I

am a cat

㑃⴮க

㼐⴮க

如

图

7-5 所示，编码器首先对“吾

輩は猫である”这句话进行编码，然

后将编码好的信息传递

给解码器，由解码器生成

目标文本。此时，编码器编

码的信息浓缩了翻译所

必需的信息，解码器基于

这个浓缩的信息生成目

标

文本。

以上就是 seq2seq 的全貌

图。编码器和解码器协作

，将一个时序数据转

换为

另一个时序数据。另外，在

这些编码器和解码器内

部可以使用 RNN。

下面我们来

看一下细节。首先来看编

码器，它的层结构如图 7-6 所

示。

图7-6

编码器的层结构

ある 吾

輩 は 猫 で

Embedding Embedding Embedding Embedding Embedding

LSTM

LSTM LSTM LSTM LSTM h

A“吾輩は猫である”是日本著名

作家夏目漱石的代表作

《我是猫》的书名，译为“我是

猫”。——编者注

7.2 seq2seq模型 285

由图 7-6 可以

看出，编码器利用

RNN 将时序

数据转换为隐藏状态 h。

这

里的RNN使用的是LSTM，不过也可

以使用“简单RNN”或者GRU等。

另外

，这里考虑的是将日语句

子分割为单词进行输入

的情况。

图

7-6 的编码器输出

的向量 h 是 LSTM 层的最后一个

隐藏状态，其中

编码了翻

译输入文本所需的信息

。这里的重点是，LSTM 的隐藏状

态 h 是

一个固定长度的向

量。说到底，编码就是将任

意长度的文本转换为一

个固定

长度的向量（图

7-7）。

图

7-7　编码器将文本编码为固

定长度的向量

どこで生れたかとんと見当がつかぬ

吾

輩は猫である

h

㑃⴮க

㑃⴮க

㑃⴮க

名前はまだない

如图 7-7

所示，编码

器将文本转换为固定长

度的向量。那么，解码器是

如何“处理”这个编码好的

向量，从而生成目标文本

的呢？其实，我们已经

知道

答案了。因为我们只需要

直接使用上一节讨论的

进行文本生成的模型即

可，如图 7-8 所示。AB

A《我是猫》中的

一句话，译为“还没有名字

”。——编者注

B《我是猫》中的一句

话，译为“不记得是在哪里

出生的”。——编者注

第 7章　基于

RNN生成文本 286

Softmax

Softmax Softmax Softmax Softmax

Affine Affine

Affine Affine Affine

Embedding Embedding Embedding

Embedding Embedding

LSTM LSTM LSTM LSTM

LSTM h

cat <eos> I am

a

cat <eos> I am a

图7-8　解码器的层

结构

从图 7-8 中可以看出，解

码器的结构和上一节的

神经网络完全相同。

不过

它和上一节的模型存在

一点差异，就是

LSTM 层会接收

向量 h。在

上一节的语言模

型中，LSTM 层不接收任何信息

（硬要说的话，也可以说

LSTM

的

隐藏状态接收“0 向量”）。这个

唯一的、微小的改变使得

普通的语

言模型进化为

可以驾驭翻译的解码器

。

图 7-8 中使用了

<eos>这一分隔符

（特殊符号）。这个分隔符被

用作

通知解码器开始生

成文本的信号。另外，解码

器采样到 <eos>出

现为止，所以

它也是结束信号。也就是

说，分隔符 <eos>可以用

来指示

解码器的“开始

/ 结束”。在其

他文献中，也有使用 <go>、

<start>或者

“_”（下划线）作为分隔符的例

子。

现在我们连接编码器

和解码器，并给出它的层

结构，具体如图 7-9

所示。

7.2 seq2seq模型

287

图7-9 seq2seq的整体的层结构

Softmax

Softmax Softmax Softmax Softmax

Affine Affine

Affine Affine Affine

Embedding Embedding Embedding

Embedding Embedding Embedding Embedding Embedding Embedding

Embedding

LSTM LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM LSTM

cat

<eos>

<eos>

I am a

cat

I am a ある 吾輩

は 猫

で

如图 7-9 所示，seq2seq 由两个 LSTM

层构

成，即编码器的 LSTM 和

解码器

的LSTM。此时，LSTM层的隐藏状态是

编码器和解码器的“桥梁

”。

在正向传播时，编码器的

编码信息通过 LSTM

层的隐藏

状态传递给解码器；

在反

向传播时，解码器的梯度

通过这个“桥梁”传递给编

码器。

7.2.2  时序数据转换的简

单尝试

下面我们来实现

seq2seq，不过在此之前，首先说明

一下我们要处理的

问题

。这里我们将“加法”视为一

个时序转换问题。具体来

说，如图 7-10

所示，在 seq2seq 学习后，如

果输入字符串“57 +

5”，seq2seq 要能正确

回答“62”。顺便说一下，这种为

了评价机器学习而创建

的简单问题，称为

“toy problem”。

图7-10　让seq2seq学

习加法的例子

“57+5” “62” seq2seq

seq2seq

seq2seq

“1149”

“228”

“628+521”

“220+8”

  第

7章　基于

RNN生成文本 288

在我们看来，这

里做的加法运算是非常

简单的问题，但是 seq2seq 对加

法

（更确切地说是加法的逻

辑）一无所知。seq2seq 从加法的例

子（样本）

中学习出现的字

符模式，这样真的可以学

习到加法运算的规则吗

？这正是本

次实验的看头

。

顺便说一下，在之前的 word2vec

和

语言模型中，我们都把文

本以单词

为单位进行了

分割，但并非必须这样做

。对于本节的这个问题，我

们将不以

单词为单位，而

是以字符为单位进行分

割。在以字符为单位进行

分割的情况

下，“57 + 5”这样的输

入会被处理为

['5', '7', '+', '5'] 这样的列

表。

7.2.3

可变长度的时序数据

我们将“加法”视为字符（数

字）列表。这里需要注意的

是，不同的加

法问题（“57 + 5”或者

“628 + 521”等）及其回答（“62”或者“1149”等）

的字

符数是不同的。比如，“57 + 5”共有

4 个字符，而“628 + 521”共有

7 个字符。

如

此，在加法问题中，每个样

本在时间方向上的大小

不同。也就是说，

加法问题

处理的是可变长度的时

序数据。因此，在神经网络

的学习中，在进

行 mini-batch

处理时

，需要想一些应对办法。

在

使用批数据进行学习时

，会一起处理多个样本。此

时，（在我们的

实现中）需要

保证一个批次内各个样

本的数据形状是一致的

。

在基于 mini-batch 学习可变长度的

时序数据时，最简单的方

法是使用

填充（padding）。所谓填充

，就是用无效（无意义）数据

填入原始数据，从

而使数

据长度对齐。就上面这个

加法的例子来说，如图 7-11 所

示，在多余

位置插入无效

字符（这里是空白字符），从

而使所有输入数据的长

度对齐。

7.2

seq2seq模型 289

图7-11　为了进行

mini-batch学习，使用空白字符进行

填充，使输入和输出的大

小对齐

ܧ䓀

ڒ䓀

5

6 2

2 2 0

8

8

+

+

5 2

2

2 2

1

5 6

@

@

@

8

1 9

1 4

7 +

本次的问题处理

的是 0

~ 999 的两个数的加法。因

此，包括“+”在

内，输入的最大

字符数是 7。另外，加法的结

果最大是 4

个字符（最大为

“999 + 999 = 1998”）。因此，对监督数据也进行

类似的填充，从而对齐所

有样本数据的长度。另外

，在本次的问题中，在输出

的开始处加上了分隔符

“_”（下划线），使得输出数据的

字符数统一为 5。这个分隔

符作为通知解码

器开始

生成文本的信号使用。

对

于解码器的输出，可以在

监督标签中插入表示字

符输出结束的

分隔符（比

如“_62_”或“_1149_”）。但是，简单起见，这里

我

们不使用表示字符输

出结束的分隔符。也就是

说，在解码器生成

字符串

时，始终输出固定数量的

字符（这里是包括开始处

的“_”

在内的

5 个字符）。

像这样

，通过填充对齐数据的大

小，可以处理可变长度的

时序数据。但

是，因为使用

了填充，seq2seq 需要处理原本不

存在的填充用字符，所以

如

果追求严谨，使用填充

时需要向

seq2seq 添加一些填充

专用的处理。比如，

在解码

器中输入填充时，不应计

算其损失（这可以通过向

Softmax with 

Loss

层添加 mask 功能来解决）。再比

如，在编码器中输入填充

时，LSTM

层应按原样输出上一

时刻的输入。这样一来，LSTM 层

就可以像不存在填充

一

样对输入数据进行编码

。

这里的内容有一些复杂

，大家即使无法理解也没

有关系。为了便于理解，

本

章我们将填充用字符（空

白字符）作为普通数据处

理，不进行特别处理。

  第 7章

基于RNN生成文本

290

7.2.4  加法数据

集

这里介绍的加法的学

习数据预先存放在了 dataset/addition.txt

中

。如

图 7-12 所示，这个文本文件

中含有 50 000

个加法样本。这份

学习数据的制

作参考了

Keras 的 seq2seq 的实现 [40]。

图7-12　加法的学习

数据：空白字符（空格）用灰

色的点表示

为了使用 Python 轻

松处理 seq2seq

的学习数据（文本

文件），本书提

供了一个专

用模块（dataset/sequence.py），这个模块有 load_data() 和 get_

vocab()

两

个方法。

load_data(file_name, seed) 读入由 file_name 指定的文

本文件，并将文

本转换为

字符 ID，返回训练数据和测

试数据。该方法内部设有

随机数种子

seed 以打乱数据

，分割训练数据和测试数

据。另外，get_vocab() 方法返回

字符与

ID

的映射字典（实际上返回

char_to_id 和 id_to_char）。现在我们来

看一下实

际的使用示例（ ch07/show_addition_dataset.py）。

import

sys

sys.path.append('..')

from dataset import sequence

7.3 seq2seq的实现 291

(x_train, t_train), (x_test,

t_test) = \

 sequence.load_data('addition.txt', seed=1984)

char_to_id, id_to_char = sequence.get_vocab()

print(x_train.shape, t_train.shape)

print(x_test.shape, t_test.shape)

# (45000, 7) (45000,

5)

# (5000, 7) (5000, 5)

print(x_train[0])

print(t_train[0])

# [ 3 0

2 0 0 11 5]

#

[ 6 0 11 7 5]

print(''.join([id_to_char[c] for c in x_train[0]]))

print(''.join([id_to_char[c]

for c in t_train[0]]))

# 71+118

# _189

像

这样，使用 sequence 模块，可以轻松

地读入 seq2seq

用的数据。这

里，x_train 和

t_train 存放的是字符 ID。另外，字符

ID 和字符之间的映

射可以

使用 char_to_id 和 id_to_char。

数据集原本应分

成训练用、验证用和测试

用 3

份。用训练数据进

行学

习，用验证数据进行调参

，最后再用测试数据评价

模型的能力。

而简单起见

，这里只分成训练数据和

测试数据 2 份，用它们进行

模型的训练和评价。

7.3

seq2seq的实

现 

seq2seq 是组合了两个 RNN 的神经

网络。这里我们首先将这

两个

RNN

实现为 Encoder 类和 Decoder 类，然后

将这两个类组合起来，来

实现

seq2seq

类。我们先从 Encoder 类开始

介绍。

7.3.1

Encoder类

如图 7-13 所示，Encoder 类接收

字符串，将其转化为向量

h。

第 7章　基于RNN生成文本 292

图7-13 Encoder类

的输入输出

Encoder h

5 5 7 +

如前所述，我

们使用 RNN 实现编码器。这里

，使用 LSTM 层实现图

7-14

的层结构

。

图7-14　编码器的层结构

Embedding Embedding Embedding

Embedding Embedding Embedding Embedding

LSTM LSTM

LSTM LSTM LSTM LSTM LSTM h

“5” “5” “ ” “ ”

“ ” “7” “+”

如图

7-14 所示，Encoder

类由 Embedding 层和 LSTM 层组成。Embedding

层

将字符（字符

ID）转化为字符

向量，然后将字符向量输

入 LSTM 层。

LSTM 层向右（时间方向）输

出隐藏状态和记忆单元

，向上输出隐藏状

态。这里

，因为上方不存在层，所以

丢弃

LSTM 层向上的输出。如图

7-14

所示，在编码器处理完最

后一个字符后，输出 LSTM 层的

隐藏状态 h。然

后，这个隐藏

状态 h 被传递给解码器。

编

码器只将 LSTM 的隐藏状态传

递给解码器。尽管也可以

把

LSTM 的记忆单元传递给解

码器，但我们通常不太会

把 LSTM 的

记忆单元传递给其

他层。这是因为，LSTM 的记忆单

元被设计为

只给自身使

用。

顺便说一下，我们已经

将时间方向上进行整体

处理的层实现为了 Time 

图灵

社区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

7.3 seq2seq的实现 293

LSTM 层和

Time Embedding 层。使用这些

Time 层，我们的编码器将如

图

7-15 所示。

图7-15　使用Time层实现编码

器

Time LSTM

Time Embedding

h

5 5 7 +

Encoder

类如下所示。这个 Encoder 类有

初始化 __init__()、正向传播

forward() 和反向

传播

backward() 这 3 个方法。首先，我们

来看一下初始化

方法（ ch07/seq2seq.py）。

class Encoder:

 def __init__(self, vocab_size,

wordvec_size, hidden_size):

 V, D, H

= vocab_size, wordvec_size, hidden_size

 rn

= np.random.randn

 embed_W = (rn(V,

D) / 100).astype('f')

 lstm_Wx =

(rn(D, 4 * H) / np.sqrt(D)).astype('f')

lstm_Wh = (rn(H, 4 * H)

/ np.sqrt(H)).astype('f')

 lstm_b = np.zeros(4

* H).astype('f')

 self.embed = TimeEmbedding(embed_W)

self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)

self.params = self.embed.params + self.lstm.params

self.grads = self.embed.grads + self.lstm.grads

self.hs = None

  第

7章

基于RNN生成文本 294

初始化

方法接收 vocab_size、wordvec_size 和 hidden_size

这 3 个参数。

vocab_size 是

词汇量，相当于字符的种

类。顺便说一下，这次共有

13 种字

符（数字 0 ~ 9、“+”、“ ”( 空白字符

)、“_”）。此

外，wordvec_size 对应于

字符向量的维

数，hidden_size 对应于 LSTM 层的隐藏状态

的维数。

这个初始化方法

进行权重参数的初始化

和层的生成。最后，将权重

参

数和梯度分别归纳在

成员变量 params 和 grads 的列表中。因

为这次并不保持

Time LSTM 层的状

态，所以设定 stateful=False。

第 5

章和第 6 章

的语言模型处理的是只

有一个长时序数据的问

题。

那时我们设定 Time LSTM

层的参

数 stateful=True，以在保持

隐藏状态的

同时，处理长时序数据。而

这次是有多个短时序数

据

的问题。因此，针对每个

问题重设 LSTM 的隐藏状态（为

0

向量）。

接着来看 forward() 方法和 backward() 方

法（

ch07/seq2seq.py）。

def forward(self, xs):

 xs

= self.embed.forward(xs)

 hs = self.lstm.forward(xs)

self.hs = hs

 return hs[:,

-1, :]

def backward(self, dh):

dhs = np.zeros_like(self.hs)

 dhs[:, -1,

:] = dh

 dout =

self.lstm.backward(dhs)

 dout = self.embed.backward(dout)

return dout

编码器的正向传播调

用 Time Embedding 层

和 Time LSTM 层 的

forward()

方法，然后取

出 Time LSTM 层的最后一个时刻的

隐藏状态，将

它作为编码

器的 forward()

方法的输出。

在编码

器的反向传播中，LSTM 层的最

后一个隐藏状态的梯度

是 dh，

7.3 seq2seq的实现

295

这个 dh 是从解码

器传来的梯度。在反向传

播的实现中，先生成元素

为 0 的

张量 dhs，再将 dh 存放到这

个 dhs 中的对应位置。剩下的

就是调用

Time 

Embedding 层和 Time LSTM

层的 backward() 方法

。以上就是 Encoder 类的

实现。

7.3.2  Decoder类

下

面，我们继续 Decoder 类的实现。如

图

7-16 所示，Decoder 类接收

Encoder 类输出的

h，输出目标字符串。

图7-16

编码

器和解码器

Encoder Decoder h

5 5

7

6 2

+

如前所述，解

码器可以由 RNN

实现。和编码

器一样，这里也使用

LSTM 层，此

时解码器的层结构如图

7-17 所示。

第 7章　基于RNN生成文本

296

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Affine Affine Affine

Affine

Embedding Embedding Embedding Embedding

LSTM

LSTM LSTM LSTM

6 2

6

2

h

ᢌ๞

图7-17　解码器的层结构（学习

时）

图

7-17 显示了解码器在学

习时的层结构。这里使用

了监督数据 _62 进

行学习，此

时输入数据是 ['_',

'6', '2', ' ']，对应的输

出是 ['6', '2',

' ', ' ']。

在使用 RNN

进行文本生

成时，学习时和生成时的

数据输入方法

不同。在学

习时，因为已经知道正确

解，所以可以整体地输入

时

序方向上的数据。相对

地，在推理时（生成新字符

串时），则只能

输入第 1 个通

知开始的分隔符（本次为

“_”）。然后，基于输出采

样 1 个字

符，并将这个采样出来的

字符作为下一个输入，如

此重

复该过程。

顺便说一

句，在 7.1

节，在进行文本生成

时，我们基于 Softmax 函数

的概率

分布进行了采样，因此生

成的文本会随机变动。因

为这次的问题是加

法，所

以我们想消除这种概率

性的“波动”，生成“确定性的

”答案。为此，

这次我们仅选

择得分最高的字符。也就

是说，是“确定性”地选择，而

不是

“概率性”地选择。图 7-18 显

示了解码器生成字符串

的过程。

7.3 seq2seq的实现 297

图7-18 解码器

生成字符串的步骤：通过

argmax节点从Affine层的输出中选择

最大值

的索引（字符ID）

argmax argmax argmax

argmax

Affine Affine Affine Affine

Embedding

Embedding Embedding Embedding

LSTM LSTM LSTM

LSTM

6 @ 2

6 2

h

如图

7-18 所示，这里出现了新的 argmax 节

点，这是获取最大值的索

引（本例中是字符 ID）的节点

。图

7-18 的结构和上一节展示

的文本生成时

的结构相

同。不过这次没有使用 Softmax 层

，而是从 Affine

层输出的得分

中

选择了最大值的字符 ID。

Softmax 层

对输入的向量进行正规

化。此时，向量元素的值虽

然

被改变，但是它们的大

小关系没有变化。因此，在

图

7-18 的情况

下，可以省略 Softmax 层

。

如上所述，在解码器中，在

学习时和在生成时处理

Softmax

层的方式

是不一样的。因

此，Softmax with Loss 层交给此后实现的 Seq2seq

类

处理。

如图 7-19 所示，Decoder 类仅承担

Time Softmax

with Loss 层之前的部分。

  第

7章　基于

RNN生成文本 298

图7-19 Decoder类的结构

Time

Softmax

with Loss

Time Affine

Time

Embedding

Time LSTM

Decoder

h

@

6 2

6 2

ᢌ๞

由

图

7-19 可以看出，Decoder 类由 Time Embedding、Time LSTM

和

Time Affine 这 3 个

层构成。下面，我们来看一

下

Decoder 层的实现。这里

一起给

出它的初始化 __init__() 方法、正向

传播 forward()

方法和反向传播

backward() 方

法（ ch07/seq2seq.py）。

class Decoder:

def __init__(self, vocab_size, wordvec_size, hidden_size):

V, D, H = vocab_size, wordvec_size,

hidden_size

 rn = np.random.randn

embed_W = (rn(V, D) / 100).astype('f')

lstm_Wx = (rn(D, 4 * H)

/ np.sqrt(D)).astype('f')

 lstm_Wh = (rn(H,

4 * H) / np.sqrt(H)).astype('f')

lstm_b = np.zeros(4 * H).astype('f')

affine_W = (rn(H, V) / np.sqrt(H)).astype('f')

affine_b = np.zeros(V).astype('f')

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊

重版权

7.3 seq2seq的实现 299

 self.embed

= TimeEmbedding(embed_W)

 self.lstm = TimeLSTM(lstm_Wx,

lstm_Wh, lstm_b, stateful=True)

 self.affine =

TimeAffine(affine_W, affine_b)

 self.params, self.grads =

[], []

 for layer in

(self.embed, self.lstm, self.affine):

 self.params +=

layer.params

 self.grads += layer.grads

def forward(self, xs, h):

 self.lstm.set_state(h)

out = self.embed.forward(xs)

 out =

self.lstm.forward(out)

 score = self.affine.forward(out)

return score

 def backward(self, dscore):

dout = self.affine.backward(dscore)

 dout =

self.lstm.backward(dout)

 dout = self.embed.backward(dout)

dh = self.lstm.dh

 return dh

这里仅对

反向传播进行一些补充

说明。在 backward() 的实现中，从上

方

的 Softmax with

Loss 层接收梯度 dscore，然后按 Time Affine 层

、Time

LSTM 层和 Time Embedding 层的顺序传播梯度

。Time LSTM

层将时间

方向上的梯度

保存在 Time LSTM 层的成员变量 dh

中

（具体请参考 6.3 节），

因此取出

时间方向上的梯度 dh，将其

作为 Decoder

类的 backward() 的输出。

如前所

述，Decoder 类在学习时和在生成

文本时的行为不同。上面

的

forward()

方法是假定在学习时

使用的。我们将 Decoder 类生成文

本时的方法

实现为 generate()。

def

generate(self, h, start_id, sample_size):

 sampled

= []

 sample_id = start_id

self.lstm.set_state(h)

  第 7章

基于RNN生成文本 300

for _ in range(sample_size):

 x

= np.array(sample_id).reshape((1, 1))

 out =

self.embed.forward(x)

 out = self.lstm.forward(out)

score = self.affine.forward(out)

 sample_id =

np.argmax(score.flatten())

 sampled.append(int(sample_id))

 return sampled

这个 generate() 方法

有 3 个参数，分别是从编码

器接收的隐藏状态 h、

最开

始输入的字符 ID start_id 和生成的

字符数量 sample_size。这里重复如

下

操作：输入一个字符，选择

Affine

层输出的得分中最大值

的字符 ID。以

上就是 Decoder 类的实

现。

在这次的问题中，需要

将编码器的输出

h设定给

解码器的 Time 

LSTM 层。此时，通过设

置 Time

LSTM 层为 stateful，可以不

重设隐藏

状态，在保持编码器的 h的

同时，进行正向传播。

7.3.3

Seq2seq类

最

后来看 Seq2seq 类的实现。话虽如

此，这里需要做的只是将

Encoder

类和 Decoder

类连接在一起，然后

使用 Time Softmax with Loss 层计算损

失而已。Seq2seq 类

的实现如下所示（ ch07/seq2seq.py）。

class Seq2seq(BaseModel):

def __init__(self, vocab_size, wordvec_size, hidden_size):

V, D, H = vocab_size, wordvec_size,

hidden_size

 self.encoder = Encoder(V, D,

H)

 self.decoder = Decoder(V, D,

H)

 self.softmax = TimeSoftmaxWithLoss()

self.params = self.encoder.params + self.decoder.params

self.grads = self.encoder.grads + self.decoder.grads

7.3

seq2seq的实现

301

 def forward(self, xs, ts):

decoder_xs, decoder_ts = ts[:, :-1], ts[:,

1:]

 h = self.encoder.forward(xs)

score = self.decoder.forward(decoder_xs, h)

 loss

= self.softmax.forward(score, decoder_ts)

 return loss

def backward(self, dout=1):

 dout =

self.softmax.backward(dout)

 dh = self.decoder.backward(dout)

dout = self.encoder.backward(dh)

 return dout

def generate(self, xs, start_id, sample_size):

h = self.encoder.forward(xs)

 sampled =

self.decoder.generate(h, start_id, sample_size)

 return sampled

Encoder 和 Decoder 的各类中已经实现了

主要的处理，因此这里只

是将

它们组合起来。以上

就是 Seq2seq

类的实现。下面我们

使用这个 Seq2seq 类，

来挑战一下

加法问题。

7.3.4

seq2seq的评价

Seq2seq 的学习

和基础神经网络的学习

具有相同的流程。基础神

经网络

的学习流程如下

：

1. 从训练数据中选择一个

mini-batch

2. 基于 mini-batch 计算梯度

3. 使用梯度

更新权重

这里使用 1.4.4 节说

明过的 Trainer 类进行上述操作

。另外，seq2seq 针

对每个 epoch 求解测试

数据（生成字符串），并计算

正确率。seq2seq 的学习

代码如下

所示（ ch07/train_seq2seq.py）。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第

7章　基于RNN生成

文本 302

import sys

sys.path.append('..')

import numpy as np

import matplotlib.pyplot

as plt

from dataset import sequence

from common.optimizer import Adam

from common.trainer

import Trainer

from common.util import eval_seq2seq

from seq2seq import Seq2seq

from peeky_seq2seq

import PeekySeq2seq

# 读入数据集

(x_train, t_train),

(x_test, t_test) = sequence.load_data('addition.txt')

char_to_id, id_to_char

= sequence.get_vocab()

# 设定超

参数

vocab_size =

len(char_to_id)

wordvec_size = 16

hidden_size =

128

batch_size = 128

max_epoch =

25

max_grad = 5.0

# 生成模型/优化器/训

练器

model = Seq2seq(vocab_size, wordvec_size, hidden_size)

optimizer

= Adam()

trainer = Trainer(model, optimizer)

acc_list = []

for epoch in

range(max_epoch):

 trainer.fit(x_train, t_train, max_epoch=1,

batch_size=batch_size, max_grad=max_grad)

 correct_num = 0

for i in range(len(x_test)):

 question,

correct = x_test[[i]], t_test[[i]]

 verbose

= i < 10

 correct_num

+= eval_seq2seq(model, question, correct,

 id_to_char,

verbose)

 acc = float(correct_num) /

len(x_test)

 acc_list.append(acc)

 print('val acc

%.3f%%' % (acc * 100))

7.3

seq2seq的实现 303

这里显示的

代码和基础神经网络的

学习用代码是一样的，不

过这里采

用正确率（正确

回答了多少问题）作为评

价指标。具体来说，就是针

对每个

epoch 对正确回答了测

试数据中的多少问题进

行统计。

为了测量上述实

现的正确率，我们使用 common/util.py 中

的 eval_ 

seq2seq(model,

question, correct, id_to_char, verbose, is_reverse) 方法。

这个方法向模型

输入问题，生成字符串，并

判断它是否与答案相符

。如果模

型给出的答案正

确，则返回 1；如果错误，则返

回 0.

eval_seq2seq(model, question,

correct, id_to_char, verbose, 

is_reverse)方法有 6

个参数。首先是

模型 model、问题（字符 ID 数

组）question、正确

解（字符 ID

列表）correct。之后是进行

字符 ID

与字符映射的字典

id_to_char、指定是否显示结果的 verbose、指

定

是否反转输入语句的

is_reverse。如果设置 verbose

= True，则结

果会显示

在终端上。这次的实验仅

显示最初的 10 份测试数据

。另外，

关于参数

is_reverse，我们稍后

再解释。

运行上述代码后

，图 7-20 所示的结果会显示在

终端（控制台）上 A。

图7-20

终端上

显示的部分结果

A 在Windows环境

中， 的显示可能有所不同

。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重

版权

  第

7章　基于RNN生成文本

304

如图 7-20 所示，在终端上，每个

epoch 显示一次结果。每行有“Q

600 + 257”这

样的问题语句，它的下方

是“T 857”这样的正确答案。这

里

，“ 864”是我们的模型给出的答

案。如果我们的模型给出

了正确答案，

则终端上会

显示“ 864”。

我们再看一下随着

学习的进行，上面的结果

会发生什么样的变化。图

7-21 给出了一个例子。

图7-21

终端

上显示的结果的变化

图

7-21 中展示了随着学习的进

行而出现的几个结果。从

结果中可知，

seq2seq 最开始没能

顺利回答问题。但是，随着

学习不断进行，它在慢慢

靠

近正确答案，然后变得

可以正确回答一些问题

。下面，我们来绘制一下每

个

epoch 的正确率，结果如图 7-22 所

示。

႓΍⮱䔈ᆂ᫦ा

7.4

seq2seq的改进 305

1.0

0.8

0.6

0.4

0.2

0.0

0 5 10 15

epochs

20 25

图7-22　正确率的变

化

由图

7-22 可知，随着学习的

积累，正确率稳步提高。本

次的实验只进

行了 25 次，最

后的正确率约为 10%。从图中

的变化趋势可知，如果继

续学

习，正确率应该还会

进一步上升。不过，为了能

更好地学习相同的问题

（加

法问题），这里我们暂停

本次学习，对 seq2seq 进行一些改

进。

7.4 seq2seq的改进

本节我们对上

一节的 seq2seq 进行改进，以改进

学习的进展。为了达成

该

目标，可以使用一些比较

有前景的技术。本节我们

展示其中的两个方案，

并

基于实验确认它们的效

果。

7.4.1

反转输入数据（Reverse）

第一个

改进方案是非常简单的

技巧。如图 7-23 所示，反转输入

数据的

顺序。

ₐ⶛⢴

第 7章　基于RNN生

成文本 306

䒙ࣺ

5

6 2 8 + 5

2 2 0 + 8

21

12 5 + 826

8+0 2

2

5 7 + 5 7

+ 5

图7-23　反转输入数据

的例子

这个反转输入数

据的技巧是文献 [41]

中提出

来的。据研究，在许多情况

下，使用这个技巧后，学习

进展得更快，最终的精度

也有提高。现在我们来

做

一下实验。

为了反转输入

数据，对于上一节的学习

用代码（ch07/train_seq2seq. 

py），在读入数据集之

后，我们追加下面的代码

。

#

读入数据集

(x_train, t_train), (x_test, t_test) =

sequence.load_data('addition.txt')

...

x_train, x_test = x_train[:,

::-1], x_test[:, ::-1]

...

如上所示，可

以使用 x_train[:,

::-1] 反转数组的排列

。那么，通过反

转输入数据

，正确率可以上升多少呢

？结果如图 7-24 所示。

...

...

7.4 seq2seq的改进 307

图

7-24 seq2seq的正确率的变化：baseline是上一

节的结果，reverse是反转输入数

据

后的结果

1.0

0.8

0.6

0.4

0.2

0.0

0 5 10 15

epochs

baseline

reverse

20 25

从图 7-24

中可知

，仅仅通过反转输入数据

，学习的进展就得到了改

善！

在 25 个 epoch 时，正确率为

50% 左右

。再次重复一遍，这里和上

一次（图

中的 baseline）的差异只是

将数据反转了一下。仅仅

这样，就产生了这么大

的

差异，真是令人吃惊。当然

，虽然反转数据的效果因

任务而异，但是通常

都会

有好的结果。

为什么反转

数据后，学习进展变快，精

度提高了呢？虽然理论上

不是很

清楚，但是直观上

可以认为，反转数据后梯

度的传播可以更平滑。比

如，考

虑将“吾輩 は 猫 で

ある”A 翻译成

“I am a cat”这一问题，单词“吾

輩”和单

词“I”之间有转换关系。此时

，从“吾輩”到“I”的路程必须经

过“は”“猫”“で”“ある”这

4 个单词的 LSTM 层。因此

，在反向传播时，

梯度从“I”抵

达“吾輩”，也要受到这个距

离的影响。

A

译为“我是猫”，不

懂日文的读者只需明白

该句可以拆分为“吾輩”“は”“猫

”“で”“ある”这5个单

词即可。——编者注 ₐ⶛⢴

  第

7章

基于RNN生成文本 308

那么，如

果反转输入语句，也就是

变为“ある で 猫 は

吾輩”，结果

会怎样

呢？此时，“吾輩”和“I”彼此相邻

，梯度可以直接传递。如此

，因

为通过反转，输入语句

的开始部分和对应的转

换后的单词之间的距离

变近

（这样的情况变多），所

以梯度的传播变得更容

易，学习效率也更高。不过

，

在反转输入数据后，单词

之间的“平均”距离并不会

发生改变。

7.4.2

偷窥（Peeky）

接下来是

seq2seq 的第二个改进。在进入正

题之前，我们再看一下编

码

器的作用。如前所述，编

码器将输入语句转换为

固定长度的向量 h，这个 h

集

中了解码器所需的全部

信息。也就是说，它是解码

器唯一的信息源。但

是，如

图 7-25 所示，当前的 seq2seq 只有最开

始时刻的

LSTM 层利用了 h。

我们

能更加充分地利用这个

h 吗？

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Affine

Affine Affine Affine

Embedding Embedding Embedding

Embedding

LSTM LSTM LSTM LSTM

6

5 7 + 5

@

2

h

ᢌ๞

㑃⴮க

图7-25　改进前：只有最开始

的LSTM层接收编码器的输出

h

为了达成该目标，seq2seq

的第二

个改进方案就应运而生

了。具体来说，

就是将这个

集中了重要信息的编码

器的输出 h 分配给解码器

的其他层。我

们的解码器

可以考虑图 7-26

中的网络结

构。

7.4 seq2seq的改进 309

Softmax

with

Loss

Softmax

with Loss

Softmax

with

Loss

Softmax

with Loss

Affine Affine

Affine Affine

Embedding Embedding Embedding Embedding

LSTM LSTM LSTM LSTM

6

5

7 + 5

@

2

6

2

h

ᢌ๞

㑃⴮க

图7-26　改进后：将编

码器的输出h分配给所有

时刻的LSTM层和Affine层

如图 7-26 所示

，将编码器的输出 h 分配给

所有时刻的 Affine

层和

LSTM 层。比较

图 7-26 和图 7-25

可知，之前 LSTM 层专用

的重要信息 h

现在在多个

层（在这个例子中有 8

个层

）中共享了。重要的信息不

是一个人

专有，而是多人

共享，这样我们或许可以

做出更加正确的判断。

这

里的改进是将编码好的

信息分配给解码器的其

他层，这可以解释

为其他

层也能“偷窥”到编码信息

。因为“偷窥”的英语是 peek，所

以

将这个改进了的解码器

称为

Peeky Decoder。同理，将使用了

Peeky Decoder 的 seq2seq

称

为 Peeky seq2seq。这个想法基于文

献 [42]。

在

图

7-26 中，有两个向量同时被

输入到了 LSTM 层和 Affine 层，这实

际

上表示两个向量的拼接

（concatenate）。因此，在刚才的图中，如果

使用

concat 节点拼接两个向量

，则正确的计算图可以绘

制成图 7-27。

第 7章　基于RNN生成文

本 310

图7-27　在Affine层的输入有两个

的情况下（左图），将它们拼

接起来输入Affine层（右图）

下面

给出 PeekyDecoder 类的实现。这里仅显

示初始化 __init__() 方法和

正向传

播

forward() 方法。因为没有特别难

的地方，所以这里省略了

反向传播

backward() 方法和文本生

成 generate() 方法（

ch07/peeky_seq2seq.py）。

class PeekyDecoder:

 def __init__(self,

vocab_size, wordvec_size, hidden_size):

 V, D,

H = vocab_size, wordvec_size, hidden_size

rn = np.random.randn

 embed_W =

(rn(V, D) / 100).astype('f')

 lstm_Wx

= (rn( H + D ,

4 * H) / np.sqrt(H +

D)).astype('f')

 lstm_Wh = (rn(H, 4

* H) / np.sqrt(H)).astype('f')

 lstm_b

= np.zeros(4 * H).astype('f')

 affine_W

= (rn( H + H ,

V) / np.sqrt(H + H)).astype('f')

affine_b = np.zeros(V).astype('f')

 self.embed =

TimeEmbedding(embed_W)

 self.lstm = TimeLSTM(lstm_Wx, lstm_Wh,

lstm_b, stateful=True)

 self.affine = TimeAffine(affine_W,

affine_b)

 self.params, self.grads = [],

[]

 for layer in (self.embed,

self.lstm, self.affine):

 self.params += layer.params

self.grads += layer.grads

 self.cache =

None

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

7.4

seq2seq的改进 311

 def forward(self, xs,

h):

 N, T = xs.shape

N, H = h.shape

 self.lstm.set_state(h)

out = self.embed.forward(xs)

 hs =

np.repeat(h, T, axis=0).reshape(N, T, H)

out = np.concatenate((hs, out), axis=2)

out = self.lstm.forward(out)

 out =

np.concatenate((hs, out), axis=2)

 score =

self.affine.forward(out)

 self.cache = H

return score

PeekyDecoder 的初

始化和上一节的 Decoder 基本上

是一样的，不同之处

仅在

于 LSTM 层权重和 Affine 层权重的形

状。因为这次的实现要接

收编码

器编码好的向量

，所以权重参数的形状相

应地变大了。

接着是 forward() 的实

现。这里首先使用 np.repeat() 根据时

序大小复

制相应份数的

h，并将其设置为

hs。然后，将 hs 和

Embedding 层的输出用

np.concatenate() 拼接，并输入

LSTM

层。同样地，Affine 层的输入也是

hs

和 LSTM 层的输出的拼接。

编码

器和上一节没有变化，因

此直接使用上一节的编

码器。

最后，我们来实现 PeekySeq2seq，不

过这和上一节的 Seq2seq 类基本

相

同，唯一的区别是 Decoder

层。上

一节的 Seq2seq 类使用了 Decoder 类，与

此

相对，这里使用

PeekyDecoder，剩余的逻

辑完全一样。因此，PeekySeq2seq

类的实

现只需要继承上一章的

Seq2seq 类，并修改一下初始化部

分（ ch07/

peeky_seq2seq.py）。

第 7章　基于RNN生成文本 312

from seq2seq

import Seq2seq, Encoder

class PeekySeq2seq(Seq2seq):

def __init__(self, vocab_size, wordvec_size, hidden_size):

V, D, H = vocab_size, wordvec_size,

hidden_size

 self.encoder = Encoder(V, D,

H)

 self.decoder = PeekyDecoder(V, D,

H)

 self.softmax = TimeSoftmaxWithLoss()

self.params = self.encoder.params + self.decoder.params

self.grads = self.encoder.grads + self.decoder.grads

至

此，准备工作就完成了。现

在我们使用这个

PeekySeq2seq 类，再次

挑战加法问题。学习用代

码仍使用上一节的代码

，只需要将 Seq2seq 类换成

PeekySeq2seq 类。

# model = Seq2seq(vocab_size, wordvec_size, hidden_size)

model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)

这里

，我们在第一个改进（反转

输入）的基础上进行实验

，结果如图

7-28 所示。

1.0

0.8

0.6

0.4

0.2

0.0

0 5 10 15

epochs

baseline

reverse

reverse + peeky

20 25

图7-28　“reverse + peeky”是进行

了本节的两个改进的结

果

ₐ⶛⢴

7.5 seq2seq的应用  313

如图

7-28 所示，加上

了 Peeky 的 seq2seq 的结果大幅变好。刚

过

10 个

epoch 时，正确率已经超过

90%，最终的正确率接近 100%。

从上

述实验结果可知，Reverse

和 Peeky 都有

很好的效果。借助反转

输

入语句的 Reverse 和共享编码器

信息的

Peeky，我们获得了令人

满意的

结果！

这样我们就

结束了对 seq2seq 的改进，不过故

事仍在继续。实际上，本

节

的改进只能说是“小改进

”，下一章我们将对

seq2seq 进行“大

改进”。届

时将使用名为 Attention 的

技术，它能使 seq2seq

发生巨大变

化。

这里的实验有几个需

要注意的地方。因为使用

Peeky 后，网络的权重

参数会额

外地增加，计算量也会增

加，所以这里的实验结果

必须考虑到相应

地增加

的“负担”。另外，seq2seq 的精度会随

着超参数的调整而大幅

变化。

虽然这里的结果是

可靠的，但是在实际问题

中，它的效果可能不稳定

。

7.5 seq2seq的应用

seq2seq 将某个时序数据

转换为另一个时序数据

，这个转换时序数据的

框

架可以应用在各种各样

的任务中，比如以下几个

例子。

•　机器翻译：将“一种语

言的文本”转换为“另一种

语言的文本”

•　自动摘要：将

“一个长文本”转换为“短摘

要”

•　问答系统：将“问题”转换

为“答案”

•　邮件自动回复：将

“接收到的邮件文本”转换

为“回复文本”

像这样，seq2seq 可以

用于处理成对的时序数

据的问题。除了自然语言

之外，也可以用于语音、视

频等数据。有些乍一看不

属于 seq2seq 的问题，

通过对输入

输出数据进行预处理，也

可以应用 seq2seq。本节将介绍几

个使

用 seq2seq 的应用。如果读者

能由此感受到 seq2seq

的潜力和

乐趣，那就再

好不过了。

  第

7章　基于RNN生成文本

314

7.5.1  聊天机

器人

聊天机器人是人和

计算机使用文本进行对

话的程序，现在已经被用

于

Facebook、Twitter

等各种各样的服务中

。

毫无疑问，聊天机器人可

以使用 seq2seq。因为对话是由“对

方的发言”

和“本方的发言

”构成的，可以理解为是将

“对方的发言”转换为“本方

的

发言”的问题。也就是说

，如果有对话文本数据，seq2seq 就

可以学习它。

另外，聊天机

器人也可以用在实用场

景中。文献 [43] 进行了将基

于

seq2seq 的聊天机器人应用于 IT

帮

助台的实验，并展示了它

成功解决

有关 VPN 接入问题

的例子。这里，我们将其中

的一部分对话翻译成中

文

（图 7-29）A

。

机器：你好。

人：你好。

机

器：请问你使用的是什么

操作系统？

人：Linux。

……

机器：运行时

报的错误是什么？

人：报了

一个有关 VPN 的错误。

机器：在

用 VPN

联网时，显示了什么错

误消息？

人：连接未成功之

类的。

……

机器：现在你是在公

司还是在家？

人：在家。

机器

：那你登录一下这个网址

<URL>，输入你的用户名和密码

。

人：好了。

机器：试试可不可

以用 <NAME> 正常登录。

人：好像可

以了。

机器：好的。还有其他

问题吗？

人：没了，太感谢了

。

图7-29 使用基于seq2seq的聊天机器

人进行对话的例子（参考

文献[43]）

A 原文是英语对话，本

书日文原版中作者翻译

为了日文，这里译者翻译

为了中文。——译者注

7.5

seq2seq的应用

315

从图 7-29 中可知，机器（聊天机

器人）很好地解决了问题

，遇到 VPN

接入问题的人被引

导到了能解决该问题的

URL。当然，它只能解决与 IT 帮

助

台相关的问题，无法泛化

。但是，基于对话获取答案

或者线索，这一点非

常实

用，应用范围很广。实际上

，这样的服务（简易版）已经

可以在若干网

站上看到

。

7.5.2  算法学习

本章进行的 seq2seq 实

验是加法这样的简单问

题，但理论上它也能处理

更加高级的问题，比如图

7-30

所示的 Python 代码。

图7-30　用Python写的代

码示例：图中的Input是输入，Target是

输出（引自文献[44]）

源代码也

是用字符编写的时序数

据。我们可以将跨行的代

码处理为一

条语句（将换

行视为换行符）。因此，可以

直接将源代码输入 seq2seq，让

seq2seq 对

源代码与目标答案一起

进行学习。

上述包含 for

语句

和 if 语句的问题不太容易

解决。不过，即便是这样

的

问题，也可以在 seq2seq 框架内处

理。通过改造

seq2seq 的结构，可以

期

待这样的问题能够被

解决。

下一章将介绍 RNN 的

扩

展 ——NTM（Neural Turing Machine，

神经图灵机 )

模型。届时

计算机（图灵机）将学习内

存的读写顺序，

重现算法

。

  第 7章

基于RNN生成文本 316

7.5.3  自动

图像描述

到目前为止，我

们只看了处理文本的

seq2seq 的

应用示例，除了文本之

外

，seq2seq 还可以处理图像、语音等

类型的数据。本节我们来

看一下将图

像转换为文

本的自动图像描述（image captioning）[45]

[46]。

自动

图像描述将“图像”转换为

“文本”。如图 7-31 所示，这也可以

在

seq2seq 的框架下解决。

图7-31　用于

自动图像描述的seq2seq的网络

结构示例

Softmax Softmax Softmax Softmax

Softmax

Affine

Affine

CNN

Affine Affine

Affine Affine

Embedding Embedding Embedding Embedding

Embedding

LSTM LSTM LSTM LSTM LSTM

I am a cat <eos>

I

am a cat <eos>

图 7-31

是我们熟悉

的网络结构。实际上，它和

之前的网络的唯一区别

在于，编码器从 LSTM 换成了 CNN（Convolutional Neural Network，卷

积神经网络），而解码器仍

使用与之前相同的网络

。仅通过这点改变（用

CNN 替代

LSTM），seq2seq 就可以处理图像了。

这里

补充说明一下图 7-31 中的

CNN。此

处，CNN 对图像进行编码，

这时

CNN 的最终输出是特征图。因

为特征图是三维（高、宽、通

道）的，

所以需要想一些办

法让解码器的 LSTM

可以处理

它。于是，我们将 CNN 的

特征图

扁平化到一维，并基于全

连接的 Affine 层进行转换。之后

，再将转

换后的数据传递

给解码器，就可以像之前

一样生成文本了。

7.5 seq2seq的应用

317

图 7-31

的 CNN 使用 VGG、ResNet 等成熟网络，并

使用在别的图

像数据集

（ImageNet

等）上学习好的权重，这样

可以获得好的编码，

从而

生成好的文本。

现在我们

看几个基于 seq2seq 的自动图像

描述的例子。图 7-32

显示的

是

由基于 TensorFlow 的 im2txt[47] 生成的例子。此

处使用的网络基于图

7-31，并

在其上进行了若干改进

。

图7-32　自动图像描述的例子

：将图像转换为文本（参考

文献[47]）

A group of

giraffe standing

next to each other.

(一群长颈鹿站在一

起)

A person on a beach

flying a kite.

(在海滩放风筝的人)

A black

and white photo of

a train

on a train track.

(铁

轨上的一辆火车的黑白

照片)

A

person skiing down a

snow covered

slope.

(在积雪的斜坡上滑

雪的人)

由图 7-32 可知，这里得

到了很不错的结果。之所

以能够达到这样的效

果

，是因为存在大量的图像

和说明文字等训练数据

（比如，ImageNet

等大

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 7章　基于

RNN生成文本 318

规模的图像数

据）。再加上可以高效学习

这些训练数据的 seq2seq

的应用

，

最终得到了图 7-32 所示的出

色结果。

7.6 小结

本章我们探

讨了基于 RNN 的文本生成。实

际上，我们只是稍微改动

了一下上一章的基于 RNN 的

语言模型，增加了文本生

成的功能。在本章后

半部

分，我们研究了

seq2seq，并使之成

功学习了简单的加法。seq2seq 模

型拼接了编码器和解码

器，是组合了两个 RNN 的简单

结构。但是，尽管

seq2seq 简单，却具

有巨大的潜力，可以用于

各种各样的应用。

另外，本

章还介绍了改进 seq2seq 的两个

方案——Reverse 和 Peeky。

我们对这两个方

案进行了实现和评价，并

确认了它们的效果。下一

章我们将

继续改进 seq2seq，届时

深度学习中最重要的技

巧之一 Attention 将会出现。

我们将

说明 Attention

的机制，然后基于它

实现更强大的 seq2seq。

7.6 小结  319

本章

所学的内容

• 基于 RNN 的语言

模型可以生成新的文本

• 在进行文本生成时，重复

“输入一个单词（字符），基于

模型的输出

（概率分布）进

行采样”这一过程

• 通过组

合两个 RNN，可以将一个时序

数据转换为另一个时序

数据

（seq2seq）

•

在 seq2seq 中，编码器对输入

语句进行编码，解码器接

收并解码这个

编码信息

，获得目标输出语句

• 反转

输入语句（Reverse）和将编码信息

分配给解码器的多个层

（Peeky）可以有效提高

seq2seq 的精度

• seq2seq 可

以用在机器翻译、聊天机

器人和自动图像描述等

各种各样

的应用中

第8章

Attention

注意力是全部。

——Vaswani 们的论文

标题 [52]

上一章我们使用

RNN 生

成了文本，又通过连接两

个 RNN，将一个时

序数据转换

为了另一个时序数据。我

们将这个网络称为 seq2seq，并用

它成

功求解了简单的加

法问题。之后，我们对这个

seq2seq

进行了几处改进，几

乎完

美地解决了这个简单的

加法问题。

本章我们将进

一步探索 seq2seq 的可能性（以及

RNN 的可能性）。这

里，Attention 这一强大

而优美的技术将登场。Attention 毫

无疑问是近年来

深度学

习领域最重要的技术之

一。本章的目标是在代码

层面理解 Attention

的结构，然后将

其应用于实际问题，体验

它的奇妙效果。

8.1 Attention的结构

如

上一章所述，seq2seq 是一个非常

强大的框架，应用面很广

。这里我

们将介绍进一步

强化 seq2seq

的注意力机制（attention mechanism，简称

Attention）。基于 Attention 机制，seq2seq 可以像我们人

类一样，将“注

意力”集中在

必要的信息上。此外，使用

Attention

可以解决当前 seq2seq

面临的问

题。

本节我们将首先指出

当前 seq2seq 存在的问题，然后一

边说明

Attention

的结构，一边对其

进行实现。

  第 8章

Attention 322

上一章我

们已经对 seq2seq 进行了改进，但

那些只能算是“小改

进”。下

面将要说明的

Attention 技术才是

解决 seq2seq 的问题的“大

改进”。

8.1.1

seq2seq存

在的问题

seq2seq 中使用编码器

对时序数据进行编码，然

后将编码信息传递给解

码器。此时，编码器的输出

是固定长度的向量。实际

上，这个“固定长度”

存在很

大问题。因为固定长度的

向量意味着，无论输入语

句的长度如何（无

论多长

），都会被转换为长度相同

的向量。以上一章的翻译

为例，如图 8-1

所示，不管输入

的文本如何，都需要将其

塞入一个固定长度的向

量中。

h

㑃⴮க

㑃⴮க

“吾䒵 は猫である”

图8-1　无论输入语

句多长，编码器都将其塞

入固定长度的向量中

无

论多长的文本，当前的编

码器都会将其转换为固

定长度的向量。就像

把一

大堆西装塞入衣柜里一

样，编码器强行把信息塞

入固定长度的向量中。

但

是，这样做早晚会遇到瓶

颈。就像最终西服会从衣

柜中掉出来一样，有用

的

信息也会从向量中溢出

。

现在我们就来改进 seq2seq。首先

改进编码器，然后再改进

解码器。A

A《我是猫》中的一句

话，译为“只记得我在一个

昏暗潮湿的地方喵喵地

哭泣着”。——编者注

8.1 Attention的结构

323

8.1.2  编

码器的改进

到目前为止

，我们都只将 LSTM

层的最后的

隐藏状态传递给解码器

，

但是编码器的输出的长

度应该根据输入文本的

长度相应地改变。这是编

码器

的一个可以改进的

地方。具体而言，如图 8-2 所示

，使用各个时刻的 LSTM

层的隐

藏状态。

hs

LSTM LSTM LSTM LSTM

LSTM

Embedding Embedding Embedding Embedding Embedding

图8-2　使用编码器各

个时刻（各个单词）的LSTM层的

隐藏状态（这里表示为hs）

如

图 8-2 所示，使用各个时刻（各

个单词）的隐藏状态向量

，可以获得

和输入的单词

数相同数量的向量。在图

8-2

的例子中，输入了 5 个单词

，此

时编码器输出 5 个向量

。这样一来，编码器就摆脱

了“一个固定长度的向

量

”的制约。

  第 8章　Attention

324

在许多深度

学习框架中，在初始化 RNN 层

（或者 LSTM 层、GRU

层等）时，可以选择

是返回“全部时刻的隐藏

状态向量”，还是返回“最

后

时刻的隐藏状态向量”。比

如，在 Keras 中，在初始化 RNN 层时，

可

以设置 return_sequences为 True或者 False。

图 8-2

中我们

需要关注 LSTM 层的隐藏状态

的“内容”。此时，各个时

刻的

LSTM 层的隐藏状态都充满了

什么信息呢？有一点可以

确定的是，各

个时刻的隐

藏状态中包含了大量当

前时刻的输入单词的信

息。就图

8-2 的

例子来说，输入

“猫”时的 LSTM 层的输出（隐藏状

态）受此时输入的单

词“猫

”的影响最大。因此，可以认

为这个隐藏状态向量蕴

含许多“猫的成

分”。按照这

样的理解，如图 8-3 所示，编码

器输出的 hs 矩阵就可以视

为各

个单词对应的向量

集合。

图8-3 编码器的输出hs具

有和单词数相同数量的

向量，各个向量中蕴含了

各个单词对应

的信息

hs

㑃⴮க

吾

輩は猫である

吾輩

ある

で

猫

は

因为编码器

是从左向右处理的，所以

严格来说，刚才的“猫”向量

中含有“吾輩”“は”“猫”这3个单词

的信息。考虑整体的平衡

性，

最好均衡地含有单词

“猫”周围的信息。在这种情

况下，从两个方

向处理时

序数据的双向RNN（或者双向

LSTM）比较有效。我们后

面再介

绍双向RNN，这里先继续使用

单向LSTM。

8.1 Attention的结构

325

以上就是对

编码器的改进。这里我们

所做的改进只是将编码

器的全部时

刻的隐藏状

态取出来而已。通过这个

小改动，编码器可以根据

输入语句的长

度，成比例

地编码信息。那么，解码器

又将如何处理这个编码

器的输出呢？

接下来，我们

对解码器进行改进。因为

解码器的改进有许多值

得讨论的地

方，所以我们

分

3 部分进行。

8.1.3  解码器的改

进①

编码器整体输出各个

单词对应的

LSTM 层的隐藏状

态向量 hs。然后，

这个 hs 被传递

给解码器，以进行时间序

列的转换（图

8-4）。

hs

I am a cat

㑃⴮க

吾輩は猫である

㼐⴮க

图8-4　编

码器和解码器的关系

顺

便说一下，在上一章的最

简单的

seq2seq 中，仅将编码器最

后的隐藏

状态向量传递

给了解码器。严格来说，这

是将编码器的 LSTM 层的“最后

”

的隐藏状态放入了解码

器的

LSTM 层的“最初”的隐藏状

态。用图来表示

的话，解码

器的层结构如图 8-5 所示。

第

8章　Attention 326

hs Affine Affine

Affine Affine Affine

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

LSTM LSTM LSTM LSTM LSTM

Embedding

<eos>

<eos>

I am a cat

I am a cat

Embedding Embedding

Embedding Embedding

ᢌ๞ 㑃⴮க

吾輩は猫である

图8-5

上一章的解

码器的层结构（学习时）

如

图 8-5 所示，上一章的解码器

只用了编码器的 LSTM 层的最

后的隐

藏状态。如果使用

hs，则只提取最后一行，再将

其传递给解码器。下面我

们改进解码器，以便能够

使用全部 hs。

我们在进行翻

译时，大脑做了什么呢？比

如，在将“吾輩は猫である”

这句话翻

译为英文时，肯定要用到

诸如“吾輩 = I”“猫

= cat”这样的知识

。

也就是说，可以认为我们

是专注于某个单词（或者

单词集合），随时对这个

单

词进行转换的。那么，我们

可以在 seq2seq 中重现同样的事

情吗？确切地

说，我们可以

让 seq2seq 学习“输入和输出中哪

些单词与哪些单词有关

”这

样的对应关系吗？

在机

器翻译的历史中，很多研

究都利用“猫 =cat”这样的单词

对应

关系的知识。这样的

表示单词（或者词组）对应

关系的信息称为对齐

（alignment）。到

目前为止，对齐主要是手

工完成的，而我们将要介

绍的 Attention 技术则成功地将对

齐思想自动引入到了 seq2seq 中

。

这也是从“手工操作”到“机

械自动化”的演变。

从现在

开始，我们的目标是找出

与“翻译目标词”有对应关

系的“翻译

源词”的信息，然

后利用这个信息进行翻

译。也就是说，我们的目标

是仅关

注必要的信息，并

根据该信息进行时序转

换。这个机制称为 Attention，是

本章

的主题。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

8.1 Attention的结构

327

在介

绍 Attention 的细节之前，这里我们

先给出它的整体框架。我

们要

实现的网络的层结

构如图 8-6

所示。

hs

hs⮱ᰭ㏵㵹 LSTM LSTM LSTM

LSTM LSTM

Embedding Embedding Embedding Embedding

Embedding

<eos>

<eos>

I am a

cat

I am a cat

Affine

Affine Affine Affine Affine

Softmax

with

Loss

Softmax

with Loss

Softmax

with

Loss

Softmax

with Loss

Softmax

with

Loss

ᢌ๞

㑃⴮க

᳽⻺䃎ツ

᳽⻺䃎ツ

᳽⻺䃎ツ

᳽⻺䃎ツ

᳽⻺䃎ツ

吾輩は猫である

图8-6　改

进后的解码器的层结构

如图 8-6

所示，我们新增一个

进行“某种计算”的层。这个

“某种计

算”接收（解码器）各

个时刻的 LSTM 层的隐藏状态

和编码器的 hs。然后，

从中选

出必要的信息，并输出到

Affine

层。与之前一样，编码器的

最后的

隐藏状态向量传

递给解码器最初的 LSTM 层。

图

8-6 的网络所做的工作是提

取单词对齐信息。具体来

说，就是从

hs

中选出与各个

时刻解码器输出的单词

有对应关系的单词向量

。比如，当图

8-6 的解码器输出

“I”时，从 hs 中选出“吾輩”的对应

向量。也就是说，

我们希望

通过“某种计算”来实现这

种选择操作。不过这里有

个问题，就是

选择（从多个

事物中选取若干个）这一

操作是无法进行微分的

。

神经网络的学习一般通

过误差反向传播法进行

。因此，如果使用

可微分的

运算构造网络，就可以在

误差反向传播法的框架

内进行

学习；而如果不使

用可微分的运算，基本上

也就没有办法使用误

差

反向传播法。

第 8章　Attention 328

可否将

“选择”这一操作换成可微

分的运算呢？实际上，解决

这个问题

的思路很简单

（但是，就像哥伦布蛋一样

，第一个想到是很难的）。这

个思

路就是，与其“单选”，不

如“全选”。如图 8-7 所示，我们另

行计算表示各

个单词重

要度（贡献值）的权重。

图8-7

对

各个单词计算表示重要

度的权重（后文介绍如何

计算）

hs a

0.8 

0.1

0.03 

0.05 

0.02 ある

吾輩

は

猫

で

如图 8-7

所示，这

里使用了表示各个单词

重要度的权重（记为 a）。此

时

，a 像概率分布一样，各元素

是 0.0 ～

1.0 的标量，总和是 1。然后，计

算这个表示各个单词重

要度的权重和单词向量

hs 的加权和，可以获得目标

向量。这一系列计算如图

8-8 所示。

hs

a

c 0.8 

0.1

0.03 

0.05 

0.02 ̷̸᪴ा䛼ある

吾輩

は

猫

で

图8-8　通过计算

加权和，可以得到上下文

向量

如图 8-8 所示，计算单词

向量的加权和，这里将结

果称为上下文向量，

并用

符号 c 表示。顺便说一下，如

果我们仔细观察，就可以

发现“吾輩”对

8.1 Attention的结构  329

应的

权重为 0.8。这意味着上下文

向量

c 中含有很多“吾輩”向

量的成分，

可以说这个加

权和计算基本代替了“选

择”向量的操作。假设“吾輩

”对应

的权重是 1，其他单词

对应的权重是 0，那么这就

相当于“选择”了“吾輩”

向量

。

上下文向量 c 中包含了当

前时刻进行变换（翻译）所

需的信息。更确

切地说，模

型要从数据中学习出这

种能力。

下面，我们从代码

的角度来看一下目前为

止的内容。这里随意地生

成编

码器的输出 hs 和各个

单词的权重 a，并给出求它

们的加权和的实现，代码

如下所示，请注意多维数

组的形状。

import numpy

as np

T, H = 5,

4

hs = np.random.randn(T, H)

a

= np.array([0.8, 0.1, 0.03, 0.05, 0.02])

ar = a.reshape(5, 1).repeat(4, axis=1)

print(ar.shape)

# (5, 4)

t = hs

* ar

print(t.shape)

# (5, 4)

c = np.sum(t, axis=0)

print(c.shape)

#

(4,)

设时序数据的

长度 T=5，隐藏状态向量的元

素个数 H=4，这里给出了加

权

和的计算过程。我们先关

注代码 ar

= a.reshape(5, 1).repeat(4, axis=1)。

如图 8-9

所示，这行代

码将 a 转化为 ar。

图灵社区会

员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

  第 8章

Attention 330

图

8-9　通过reshape()和repeat()方法从a生成ar（变量

的形状显示在其右侧）

ar(5, 1)

a(5,) repeat reshape

ar(5, 4)

0.03

0.1

0.8 0.8 0.8 0.8

0.1

0.1 0.1

0.05 0.05 0.05 0.05

0.02

0.03

0.1

0.8

0.05

0.02

0.10.8 0.05 0.0 0.03 2

0.02

0.02 0.02

0.03 0.03 0.03

如

图

8-9 所示，我们要做的是复

制形状为 (5,) 的 a，创建 (5,4)

的数组

。

因此，通过 a.reshape(5, 1) 将 a

的形状从 (5,) 转

化为 (5,1)。然后，在第 1

个轴方向

上（axis=0）重复这个变形后的数

组

4 次，生成形状为 (5,4) 的数组

。

repeat()方法复制多维数组的元

素生成新的多维数组。设

x为NumPy

多维数组，则可以像x.repeat(rep,

axis)这

样使用。这里参数rep

指定复

制的次数，axis指定要进行复

制的轴（维度）。比如，在x的形

状为(X, Y, Z)的情况下，x.repeat(3, axis=1)沿x的第1个

轴方向（第

1个维度）进行复

制，生成形状为(X,

3*Y, Z)的多维数

组。

此外，这里也可以不使

用 repeat() 方法，而使用 NumPy

的广播功

能。

此时，令 ar = a.reshape(5, 1)，然后计算

hs * ar。如图

8-10 所示，ar 会

自动扩展以匹配

hs

的形状。

ar ar hs hs

0.03

0.1

0.1

0.3

0.6 2.1

0.9

0.2

1.0

1.3 1.2

−0.3

−0.7

−0.4

−0.8 −0.2

−1.2 −1.7

−0.4

−0.3 −0.4

−0.4 0.8

0.05

0.02

0.03 0.03 0.03

0.1 0.1 0.1

0.8 0.8 0.8

0.05 0.05 0.05

0.02

0.03

0.1

0.8

0.05

0.02

0.02 0.02

0.1

0.3

0.6 2.1

0.9 0.2

1.0

1.3 1.2

−0.3

−0.7

−0.4

−0.8 −0.2

−1.2 −1.7

−0.4 −0.3 −0.4

−0.4

图8-10 NumPy的广播

为了提

高执行效率，这里应该使

用 NumPy 的广播，而不是 repeat() 方

法。但

是，在这种情况下，需要注

意的是，在许多我们看不

见的地方多维数

8.1 Attention的结构

331

组的元素被复制了。由 1.3.4.3 节

可知，这相当于计算图中

的

Repeat 节点。

因此，在反向传播

时，需要执行 Repeat 节点的反向

传播。

如图

8-10 所示，先计算对

应元素的乘积，然后通过

c = np.sum(hs*ar, 

axis=0)

求和。这里，通过参数 axis 可以

指定在哪个轴方向（维度

）上求和。

如果我们注意一

下数组的形状，axis 的使用方

法就会很清楚。比如，当 x

的

形状为 (X, Y, Z) 时，np.sum(x, axis=1)

的输出（和）的形

状为 (X, Z)。这

里的重点是，求和

会使一个轴“消失”。在上面

的例子中，hs*ar 的形状为

(5,4)，通过

消除第

0 个轴，获得了形状

为 (4,) 的矩阵（向量）。

计算加权

和最简单有效的方法是

使用矩阵乘积。就上面的

例子来

说，只需要

np.dot(a, hs)这一行

代码就可以获得目标结

果。不过，

这样只能处理一

笔数据（样本），很难将其扩

展到批处理。如果非

要扩

展，就需要用到“张量积”，这

会使事情变得有些复杂

（在这

种情况下，需要使用

np.tensordot()和 np.einsum()方法）。简单

起见，这里我

们不使用矩阵乘积，而是

通过 repeat()和 sum()方

法来实现加权

和的计算。

下面进行批处

理版的加权和的实现，具

体如下所示（这里随机创

建 hs

和 a）。

N, T, H =

10, 5, 4

hs = np.random.randn(N,

T, H)

a = np.random.randn(N, T)

ar = a.reshape(N, T, 1).repeat(H, axis=2)

# ar = a.reshape(N, T, 1)

# 使用广播

t = hs *

ar

print(t.shape)

# (10, 5, 4)

c = np.sum(t, axis=1)

print(c.shape)

#

(10, 4)

  第 8章

Attention 332

这里

的批处理与之前的实现

几乎一样。只要注意数组

的形状，应该很快

就能确

定 repeat() 和

sum() 需要指定的维度（轴

）。作为总结，我们把加权

和

的计算用计算图表示出

来（图 8-11）。

hs (N,

T, H)

c (N, H)

t

(N, T, H)

Sum

Repeat

ar

(N, T, H)

a(N, T)

图8-11

加权和的计算图

如图 8-11 所示，这里使用 Repeat 节点

复制 a。之后，通过“×”节点

计算

对应元素的乘积，通过 Sum 节

点求和。现在考虑这个计

算图的反向传

播。其实，所

需要的知识都已经齐备

。第 1 章介绍了

Repeat 节点和 Sum 节

点

的反向传播。这里重述一

下要点：“Repeat 的反向传播是

Sum”“Sum 的

反向传播是 Repeat”。只要注意到

张量的形状，就不难知道

应该对哪个轴进

行 Sum，对哪

个轴进行 Repeat。

现在我们将图

8-11 的计算图实现为层，这里

称之为 Weight Sum 层，

其实现如下所

示（

ch08/attention_layer.py）。

class WeightSum:

 def __init__(self):

self.params, self.grads = [], []

self.cache = None

8.1 Attention的结构

333

 def forward(self, hs, a):

N, T, H = hs.shape

ar = a.reshape(N, T, 1).repeat(H, axis=2)

t = hs * ar

c = np.sum(t, axis=1)

 self.cache

= (hs, ar)

 return c

def backward(self, dc):

 hs, ar

= self.cache

 N, T, H

= hs.shape

 dt = dc.reshape(N,

1, H).repeat(T, axis=1) # sum的反向传播

dar = dt * hs

dhs = dt * ar

da = np.sum(dar, axis=2) # repeat的

反向传播

return dhs, da

以上就是计算

上下文向量的 Weight Sum

层的实现

。因为这个层没有要

学习

的参数，所以根据本书的

代码规范，此处为 self.params = []。其他应

该没有特别难的地方，我

们继续往下看。

8.1.4

解码器的

改进②

有了表示各个单词

重要度的权重 a，就可以通

过加权和获得上下文向

量。那么，怎么求这个 a 呢？当

然不需要我们手动指定

，我们只需要做好让

模型

从数据中自动学习它的

准备工作。

下面我们来看

一下各个单词的权重 a 的

求解方法。首先，从编码器

的处

理开始到解码器第

一个 LSTM 层输出隐藏状态向

量的处理为止的流程如

图

8-12 所示。

  第 8章

Attention 334

图8-12　解码器第

1个LSTM层的隐藏状态向量

吾

輩

は

猫

吾輩は猫である

で

ある

hs

hs⮱ᰭ㏵㵹

h

LSTM

Embedding

<eos>

㑃⴮க

在图

8-12 中，用 h 表

示解码器的 LSTM 层的隐藏状

态向量。此时，我

们的目标

是用数值表示这个 h 在多

大程度上和 hs 的各个单词

向量“相似”。

有几种方法可

以做到这一点，这里我们

使用最简单的向量内积

。顺便说一

下，向量 a = (a1, a2, ···

, an) 和向量

b = (b1, b2,

··· , bn) 的内积为：

a ·

b = a1b1 + a2b2 +

··· + anbn (8.1)

式 (8.1)

的含义是两

个向量在多大程度上指

向同一方向，因此使用内

积

作为两个向量的“相似

度”是非常自然的选择。

计

算向量相似度的方法有

好几种。除了内积之外，还

有使用小型的

神经网络

输出得分的做法。文献 [49] 中

提出了几种输出得分的

方法。

下面用图表示基于

内积计算向量间相似度

的处理流程（图 8-13）。

8.1 Attention的结构  335

图

8-13　基于内积计算hs的各行与

h的相似度（内积用dot节点表

示）

hs

h

s

dot

dot

−1.2

0.9

−2.4 −1.9 −2.8

dot dot dot

如图 8-13 所示，这里通过向

量内积算出

h 和 hs 的各个单

词向量之间

的相似度，并

将其结果表示为 s。不过，这

个

s 是正规化之前的值，也

称为

得分。接下来，使用老

一套的 Softmax 函数对 s

进行正规

化（图 8-14）。

  第 8章

Attention 336

s

a

Softmax

−1.2

0.9

0.8

0.1 0.03 0.05 0.02

−2.4 −1.9 −2.8

图8-14　基于Softmax的正规

化

使用

Softmax 函数之后，输出的

a 的各个元素的值在 0.0 ～ 1.0，总和

为

1，这样就求得了表示各

个单词权重的 a。现在我们

从代码角度来看一下

这

些处理。

import sys

sys.path.append('..')

from common.layers import Softmax

import numpy

as np

N, T, H =

10, 5, 4

hs = np.random.randn(N,

T, H)

h = np.random.randn(N, H)

hr = h.reshape(N, 1, H).repeat(T, axis=1)

# hr = h.reshape(N, 1, H)

# 广播

t = hs *

hr

print(t.shape)

# (10, 5, 4)

8.1 Attention的结构  337

s =

np.sum(t, axis=2)

print(s.shape)

# (10, 5)

softmax = Softmax()

a = softmax.forward(s)

print(a.shape)

# (10, 5)

以上

就是进行批处理的代码

。如前所述，此处我们通过

reshape() 和

repeat() 方法生成形状合适的

hr。在使用 NumPy 的广播的情况下

，不需要

repeat()。此时的计算图如

图 8-15

所示。

hs (N, T, H)

a(N,

T)

s (N, T)

Softmax

t

(N, T, H)

hr (N, T,

H)

h(N, H)

Sum

Repeat

图8-15

计算各个单词

权重的计算图

如图 8-15 所示

，这里的计算图由 Repeat 节点、表

示对应元素的乘积的

“×”节

点、Sum 节点和 Softmax 层构成。我们将

这个计算图表示的处理

实

现为 AttentionWeight

类（ ch08/attention_layer.py）。

import sys

sys.path.append('..')

第 8章　Attention 338

from common.np

import * # import numpy as

np

from common.layers import Softmax

class

AttentionWeight:

 def __init__(self):

 self.params,

self.grads = [], []

 self.softmax

= Softmax()

 self.cache = None

def forward(self, hs, h):

 N,

T, H = hs.shape

 hr

= h.reshape(N, 1, H).repeat(T, axis=1)

t = hs * hr

s = np.sum(t, axis=2)

 a

= self.softmax.forward(s)

 self.cache = (hs,

hr)

 return a

 def

backward(self, da):

 hs, hr =

self.cache

 N, T, H =

hs.shape

 ds = self.softmax.backward(da)

dt = ds.reshape(N, T, 1).repeat(H, axis=2)

dhs = dt * hr

dhr = dt * hs

dh = np.sum(dhr, axis=1)

 return

dhs, dh

类似于之

前的 Weight Sum 层，这个实现有

Repeat 和 Sum 运

算。只

要注意到这两个运

算的反向传播，其他应该

就没有特别难的地方。下

面，我

们进行解码器的最

后一个改进。

8.1 Attention的结构  339

8.1.5

解码

器的改进③

在此之前，我们

分两节介绍了解码器的

改进方案。8.1.3 节和 8.1.4 节分

别实

现了

Weight Sum 层和 Attention Weight 层。现在，我们将

这两层组

合起来，结果如

图 8-16 所示。

图8-16　计算上下文向

量的计算图

hr

(N, T, H)

hs (N, T,

H)

t1 (N, T, H)

s

(N, T)

a(N, T)

ar (N,

T, H)

t2 (N, T, H)

c (N, H)

h(N, H)

hs

c

a

h

Sum

Sum

Repeat

Repeat

Softmax

Weight Sum

Attention Weight

图 8-16 显示了用

于获取上下文向量 c 的计

算图的全貌。我们已经分

为

Weight

Sum 层和 Attention Weight 层进行了实现。重

申一下，这里进行的

计算

是：Attention

Weight 层关注编码器输出的

各个单词向量 hs，并计算

各

个单词的权重 a；然后，Weight Sum

层计

算 a 和 hs 的加权和，并输出

上

下文向量

c。我们将进行这

一系列计算的层称为 Attention 层

（图 8-17）。

第 8章　Attention 340

图8-17　将左边的计算

图整合为Attention层

hs

hs

h

a

h

Weight

Sum

Attention Weight

Attention

ܧ䓀

ܧ䓀

以上就是 Attention 技

术的核心内容。关注编码

器传递的信息 hs 中的

重要

元素，基于它算出上下文

向量，再传递给上一层（这

里，Affine

层在上

一层等待）。下面

给出 Attention 层的实现（ ch08/attention_layer.py）。

class

Attention:

 def __init__(self):

 self.params,

self.grads = [], []

 self.attention_weight_layer

= AttentionWeight()

 self.weight_sum_layer = WeightSum()

self.attention_weight = None

 def forward(self,

hs, h):

 a = self.attention_weight_layer.forward(hs,

h)

 out = self.weight_sum_layer.forward(hs, a)

self.attention_weight = a

 return out

def backward(self, dout):

 dhs0, da

= self.weight_sum_layer.backward(dout)

 dhs1, dh =

self.attention_weight_layer.backward(da)

 dhs = dhs0 +

dhs1

 return dhs, dh

8.1

Attention的结构  341

以

上是 Weight Sum

层和 Attention Weight 层的正向传播

和反向传播。

为了以后可

以访问各个单词的权重

，这里设定成员变量 attention_weight，

如此

就完成了 Attention 层的实现。我们

将这个 Attention 层放在 LSTM

层

和 Affine 层的

中间，如图 8-18 所示。

图8-18　具有Attention层

的解码器的层结构

吾輩

は猫である

hs

hs⮱ᰭ㏵ 㵹

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Softmax

with Loss

Affine Affine Affine

Affine Affine

Attention Attention Attention Attention

Attention

LSTM LSTM LSTM LSTM LSTM

Embedding Embedding Embedding Embedding Embedding

I

am a cat

I am a

cat

<eos>

<eos>

ᢌ๞

㑃⴮க

如图

8-18 所示，编码器的输

出 hs 被输入到各个时刻的

Attention 层。

另外，这里将

LSTM 层的隐藏

状态向量输入 Affine 层。根据上

一章的解码

器的改进，可

以说这个扩展非常自然

。如图 8-19

所示，我们将 Attention

信息“添

加”到了上一章的解码器

上。

  第

8章　Attention 342

hs

Affine Affine

Attention

LSTM LSTM

图8-19 上一章的解码

器（左图）和带Attention的解码器（右

图）的比较：选出从LSTM

层到Affine层

的部分

如图 8-19 所示，我们向

上一章的解码器“添加”基

于 Attention 层

的上下文向量信息

。因此，除了将原先的

LSTM 层的

隐藏状态向量传给

Affine 层之

外，追加输入 Attention 层的上下文

向量。

在图 8-19 中，上下文向量

和隐藏状态向量这两个

向量被输入

Affine 层。如前所述

，这意味着将这两个向量

拼接起来，将拼接

后的向

量输入

Affine 层。

最后，我们将在

图 8-18 的时序方向上扩展的

多个 Attention

层整体实

现为 Time Attention 层，如

图 8-20

所示。

图8-20　将多个Attention层整体

实现为Time Attention层

hs hs_enc

hs_dec

Attention Attention Attention Attention Attention

Time Attention

ܧ䓀

8.1 Attention的结构

343

由图 8-20 可

知，Time Attention 层只是组合了多个

Attention 层

，其

实现如下所示（ ch08/attention_layer.py）。

class TimeAttention:

def __init__(self):

 self.params, self.grads =

[], []

 self.layers = None

self.attention_weights = None

 def forward(self,

hs_enc, hs_dec):

 N, T, H

= hs_dec.shape

 out = np.empty_like(hs_dec)

self.layers = []

 self.attention_weights =

[]

 for t in range(T):

layer = Attention()

 out[:, t,

:] = layer.forward(hs_enc, hs_dec[:,t,:])

 self.layers.append(layer)

self.attention_weights.append(layer.attention_weight)

 return out

 def

backward(self, dout):

 N, T, H

= dout.shape

 dhs_enc = 0

dhs_dec = np.empty_like(dout)

 for t

in range(T):

 layer = self.layers[t]

dhs, dh = layer.backward(dout[:, t, :])

dhs_enc += dhs

 dhs_dec[:,t,:] =

dh

 return dhs_enc, dhs_dec

这里仅

创建必要数量的

Attention 层（代码

中为 T 个），各自进行正向

传

播和反向传播。另外，attention_weights 列表

中保存了各个

Attention 层

对各个

单词的权重。

  第

8章　Attention 344

以上，我

们介绍了Attention的结构及其实

现。下面我们使用Attention

来实现

seq2seq，并尝试挑战一个真实问

题，以确认 Attention

的效果。

8.2 带Attention的 seq2seq的

实现

上一节实现了 Attention

层（以

及 Time Attention 层），现在我们使用

这个

层来实现“带Attention的seq2seq”。和上一章

实现了3个类（Encoder、

Decoder

和 seq2seq）一样，这里

我们也分别实现 3 个类（AttentionEncoder、

AttentionDecoder 和

AttentionSeq2seq）。

8.2.1  编码器的实现

首先实现

AttentionEncoder 类。这个类和上一章实现

的 Encoder

类几乎

一样，唯一的区

别是，Encoder 类的 forward() 方法仅返回 LSTM

层

的最后

的隐藏状态向量

，而AttentionEncoder类则返回所有的隐藏

状态向量。因此，

这里我们

继承上一章的 Encoder 类进行实

现。AttentionEncoder 类的实现如

下所示（ ch08/attention_seq2seq.py）。

import sys

sys.path.append('..')

from

common.time_layers import *

from ch07.seq2seq import

Encoder, Seq2seq

from ch08.attention_layer import TimeAttention

class AttentionEncoder(Encoder):

 def forward(self, xs):

xs = self.embed.forward(xs)

 hs =

self.lstm.forward(xs)

 return hs

 def

backward(self, dhs):

 dout = self.lstm.backward(dhs)

dout = self.embed.backward(dout)

 return dout

图

灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

8.2 带Attention的

seq2seq的实现  345

8.2.2  解码器的

实现

接着实现使用了 Attention 层

的解码器。使用了 Attention 的解码

器的

层结构如图

8-21 所示。

[吾

輩は猫である]

[I, am, a,

cat, <eos>]

hs

hs⮱ᰭ㏵㵹

Time Softmax

with Loss

Time Affine

Time Attention

Time LSTM

Time Embedding

[ <eos>,

I, am, a, cat]

ᢌ๞

㑃⴮க

㼐⴮க

图8-21　解码器的层结构

从图 8-21 中可以看出，和上一

章的实现一样，Softmax 层（更确切

地

说，是 Time Softmax with Loss 层）之前的层都作

为解码器。另外，和上

一章

一样，除了正向传播 forward() 方法

和反向出传播 backward() 方法之

外

，还实现了生成新单词序

列（字符序列）的

generate() 方法。这里

仅给

出 Attention Decoder 层的初始化方法

和

forward() 方法的实现，如下所示

（ ch08/attention_seq2seq.py）。

  第

8章　Attention 346

class AttentionDecoder:

def __init__(self, vocab_size, wordvec_size, hidden_size):

V, D, H = vocab_size, wordvec_size,

hidden_size

 rn = np.random.randn

embed_W = (rn(V, D) / 100).astype('f')

lstm_Wx = (rn(D, 4 * H)

/ np.sqrt(D)).astype('f')

 lstm_Wh = (rn(H,

4 * H) / np.sqrt(H)).astype('f')

lstm_b = np.zeros(4 * H).astype('f')

affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')

affine_b = np.zeros(V).astype('f')

 self.embed =

TimeEmbedding(embed_W)

 self.lstm = TimeLSTM(lstm_Wx, lstm_Wh,

lstm_b, stateful=True)

 self.attention = TimeAttention()

self.affine = TimeAffine(affine_W, affine_b)

 layers

= [self.embed, self.lstm, self.attention, self.affine]

self.params, self.grads = [], []

for layer in layers:

 self.params

+= layer.params

 self.grads += layer.grads

def forward(self, xs, enc_hs):

 h

= enc_hs[:,-1]

 self.lstm.set_state(h)

 out

= self.embed.forward(xs)

 dec_hs = self.lstm.forward(out)

c = self.attention.forward(enc_hs, dec_hs)

 out

= np.concatenate((c, dec_hs), axis=2)

 score

= self.affine.forward(out)

 return score

def backward(self, dscore):

 # 参照源代码

def generate(self, enc_hs, start_id, sample_size):

# 参照源

代码

8.3 Attention的评价  347

这里的实现

除使用了新的 Time Attention 层之外，和

上一章的 Decoder 类

没有什么太

大的不同。需要注意的是

，forward() 方法中拼接了 Time Attention

层的输出

和 LSTM

层的输出。在上面的代

码中，使用 np.concatenate() 方

法进行拼接

。

这里省略对 AttentionDecoder

类 的 backward() 和 generate() 方法

的

说明。最后，我们使用 AttentionEncoder 类

和 AttentionDecoder 类来实现

AttentionSeq2seq

类。

8.2.3  seq2seq的实现

AttentionSeq2seq 类

的实现也和上一章实现

的

seq2seq 几乎一样。区别

仅在于

，编码器使用AttentionEncoder类，解码器使

用AttentionDecoder类。

因此，只要继承上一

章的 Seq2seq 类，并改一下初始化

方法，就可以实现

AttentionSeq2seq 类（ ch08/attention_seq2seq.py）。

from ch07.seq2seq import

Encoder, Seq2seq

class AttentionSeq2seq(Seq2seq):

 def

__init__(self, vocab_size, wordvec_size, hidden_size):

 args

= vocab_size, wordvec_size, hidden_size

 self.encoder

= AttentionEncoder(*args)

 self.decoder = AttentionDecoder(*args)

self.softmax = TimeSoftmaxWithLoss()

 self.params =

self.encoder.params + self.decoder.params

 self.grads =

self.encoder.grads + self.decoder.grads

以上

就是带 Attention 的

seq2seq 的实现。

8.3 Attention的评价

下面，我们使用上一节实

现的AttentionSeq2seq类来挑战一个实际

问题。

原本我们应该通过

研究翻译问题来确认 Attention

的

效果，可惜没能找到大

小

适中的翻译数据集。因此

，我们转而通过研究“日期

格式转换”问题（本

图灵社

区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第

8章　Attention 348

质上属于人为创造的

问题，数据量有限），来确认

带 Attention

的 seq2seq

的效果。

WMT 是一个有名

的翻译数据集。这个数据

集中提供了英语和法语

（或

者英语和德语）的平行

学习数据。WMT

数据集在许多

研究中都被作

为基准使

用，经常用于评价 seq2seq 的性能

，不过它的数据量很大（超

过 20 GB），使用起来不是很方便

。

8.3.1  日期格式转换问题

这里

我们要处理的是日期格

式转换问题。这个任务旨

在将使用英语的

国家和

地区所使用的各种各样

的日期格式转换为标准

格式。例如，将人写的

“september

27, 1994”这样

的日期数据转换为“1994-09-27”这样

的标准格

式，如图 8-22 所示。

图

8-22

日期格式转换的例子

september 27, 1994

JUN 17,

2013

1994-09-27

2013-06-17

1993-02-10 2/10/93

这

里采用日期格式转换问

题的原因有两个。首先，该

问题并不像看上去

那么

简单。因为输入的日期数

据存在各种各样的版本

，所以转换规则也相应

地

复杂。如果尝试将这些转

换规则全部写出来，那将

非常费力。

其次，该问题的

输入（问句）和输出（回答）存

在明显的对应关系。具

体

而言，存在年月日的对应

关系。因此，我们可以确认

Attention 有没有正

确地关注各自

的对应元素。

我们事先在

dataset/date.txt 中准备好了要处理的日

期转换数据。如图

8-23 所示，这

个文本文件包含 50 000

个日期

转换用的学习数据。

8.3 Attention的评

价  349

图8-23

用于日期格式转换

的学习数据：空格显示为

灰点

为了对齐输入语句

的长度，本书提供的日期

数据集填充了空格，并将

“_”（下划线）设置为输入和输

出的分隔符。另外，因为这

个问题输出的字

符数是

恒定的，所以无须使用分

隔符来指示输出的结束

。

如上一章所述，本书提供

了一个可以轻松处理上

述 seq2seq 用的学

习数据的 Python 模块

，这个模块位于 dataset/sequence.py中。

8.3.2

带Attention的 seq2seq的

学习

下面，我们在日期转

换用的数据集上进行 AttentionSeq2seq 的

学习，学

习用的代码如下

所示（

ch08/train.py）。

import sys

sys.path.append('..')

import numpy

as np

from dataset import sequence

from common.optimizer import Adam

from common.trainer

import Trainer

from common.util import eval_seq2seq

第 8章　Attention 350

from

attention_seq2seq import AttentionSeq2seq

from ch07.seq2seq import

Seq2seq

from ch07.peeky_seq2seq import PeekySeq2seq

#

读入数据

(x_train, t_train), (x_test, t_test) =

sequence.load_data('date.txt')

char_to_id, id_to_char = sequence.get_vocab()

#

反转

输入语句

x_train, x_test = x_train[:, ::-1],

x_test[:, ::-1]

# 设定超参数

vocab_size =

len(char_to_id)

wordvec_size = 16

hidden_size =

256

batch_size = 128

max_epoch =

10

max_grad = 5.0

model =

AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)

optimizer = Adam()

trainer = Trainer(model, optimizer)

acc_list =

[]

for epoch in range(max_epoch):

trainer.fit(x_train, t_train, max_epoch=1,

 batch_size=batch_size, max_grad=max_grad)

correct_num = 0

 for i

in range(len(x_test)):

 question, correct =

x_test[[i]], t_test[[i]]

 verbose = i

< 10

 correct_num += eval_seq2seq(model,

question, correct,

 id_to_char, verbose, is_reverse=True)

acc = float(correct_num) / len(x_test)

acc_list.append(acc)

 print('val acc %.3f%%' %

(acc * 100))

model.save_params()

8.3 Attention的

评价

351

这里显示的代码和

上一章的加法问题的学

习用代码几乎一样。区别

在

于，它读入日期数据作

为学习数据，使用AttentionSeq2seq作为模

型。另外，

这里还使用了反

转输入语句的技巧（Reverse）。之后

，在学习的同时，每个

epoch 使用

测试数据计算正确率。为

了查看结果，我们将前

10 个

问题的问

句和回答输出

到终端。

现在我们运行一

下上面的代码。随着学习

的进行，结果如图 8-24 所示。

图

8-24　显示在终端上的结果的

演变

如图 8-24 所示，随着学习

的深入，带 Attention

的 seq2seq 变聪明了。

实

际上，没过多久，它就对大

多数问题给出了正确答

案。此时，测试数据的

正确

率（代码中的 acc_list）如图

8-25 所示。

图

灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版

权

႓΍⮱䔈ᆂ᫦ा

  第 8章　Attention

352

图8-25　正确率的演变

1.0

0.8

0.6

0.4

0.2

0.0

0 2 4

epochs

6 8

如图 8-25 所示，从第 1

个 epoch 开始，正

确率迅速上升，到第 2 个

epoch

时

，几乎可以正确回答所有

问题。这可以说是一个很

好的结果。我们

将这个结

果与上一章的模型比较

一下，如图 8-26 所示。 ₐ⶛⢴

8.3

Attention的评价  353

图

8-26 与其他模型的比较：图中

的baseline是上一章中的简单的

seq2seq，peeky是

使用“偷窥”技术改进过

的seq2seq（所有的模型都反转了

输入语句）

1.0

0.8

0.6

baseline

peeky

attention

0.4

0.2

0.0

0 2 4

epochs

6 8

从图 8-26 的结果可

知，简单的

seq2seq（图中的 baseline）完全没

法用。

即使经过了 10 个 epoch，大多

数问题还是不能回答正

确。而使用了“偷窥”

技术的

Peeky 给出了良好的结果，从第

3 个 epoch 开始，模型的正确率开

始上升，在第 4

个 epoch 时，正确率

达到了 100%。但是，就学习速度

而言，

Attention 稍微有些优势。

在这

次的实验中，就最终精度

来看，Attention 和 Peeky 取得了差

不多的

结果。但是，随着时序数据

变长、变复杂，除了学习速

度之外，

Attention

在精度上也会变

得更有优势。

8.3.3  Attention的可视化

接

下来，我们对 Attention

进行可视化

。在进行时序转换时，实际

观察

Attention 在注意哪个元素。因

为在 Attention 层中，各个时刻的 Attention

权

重均保存到了成员变量

中，所以我们可以轻松地

进行可视化。 ₐ⶛⢴

  第 8章

Attention 354

在我们

的实现中，Time Attention 层中的成员变

量 attention_weights

保

存了各个时刻的 Attention 权

重，据此可以将输入语句

和输出语句的各个单词

的对应关系绘制成一张

二维地图。这里，我们针对

学习好的 AttentionSeq2seq，

对进行日期格

式转换时的

Attention 权重进行可

视化。此处，我们不给出代

码，

仅将结果显示在图 8-27 上

（ ch08/visualize_attention.py）。

图8-27 使用学习好的模型，对

进行时序转换时的Attention权重

进行可视化。横轴是

模型

的输入语句，纵轴是模型

的输出语句。地图中的元

素越接近白色，其值越大

（接近1）

图 8-27 是

seq2seq 进行时序转换

时的 Attention 权重的可视化结果

。例

如，我们可以看到，当 seq2seq

输

出第 1 个“1”时，注意力集中在

输入语句

的“1”上。这里需要

特别注意年月日的对应

关系。仔细观察图中的结

果，纵

轴（输出）的“1983”和“26”恰好对

应于横轴（输入）的“1983”和“26”。

另外

，输入语句的“AUGUST”对应于表示

月份的“08”，这一点也很令人

惊

讶。这表明 seq2seq 从数据中学

习到了“August”和“8 月”的对应关系

。图

8-28 中给出了其他一些例

子，从中也可以很清楚地

看到年月日的对应关系

。

8.3 Attention的评价  355

C

M

Y

CM

MY

CY

CMY

K

8-28.pdf 1 2020/7/19 16:02:51

图8-28 Attention权重的例子

像

这样，使用 Attention，seq2seq 能像我们人一

样将注意力集中在必

要

的信息上。换言之，借助 Attention，我

们理解了模型是如何工

作的。

我们没有办法理解

神经网络内部进行了什

么工作（基于何种逻辑工

作），

而 Attention 赋予了模型“人类可

以理解的结构和意义”。在

上面的

例子中，通过 Attention，我们

看到了单词和单词之间

的关联性。由此，

我们可以

判断模型的工作逻辑是

否符合人类的逻辑。

以上

就是关于 Attention 的评价的内容

。通过这里的实验，我们体

验了

Attention 的奇妙效果。至此，Attention 的

核心话题就要告一段落

了，但

是关于 Attention 的其他内容

还有不少。下一节我们继

续围绕 Attention，

介绍它的几个高

级技巧。

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专

享 尊重版权

  第

8章　Attention 356

8.4 关于Attention的

其他话题

到目前为止，我

们研究了

Attention（正确地说，是带

Attention 的

seq2seq），本节我们介绍几个之

前未涉及的话题。

8.4.1  双向RNN

这

里我们关注 seq2seq 的编码器。首

先复习一下，上一节之前

的编码器

如图 8-29 所示。

图8-29　基

于LSTM层输出hs

hs

LSTM LSTM LSTM

LSTM LSTM

Embedding Embedding Embedding Embedding

Embedding

吾輩 は 猫 で ある

如图8-29所

示，LSTM中各个时刻的隐藏状

态向量被整合为hs。这里，

编

码器输出的 hs 的各行中含

有许多对应单词的成分

。

需要注意的是，我们是从

左向右阅读句子的。因此

，在图 8-29

中，单

词“猫”的对应向

量编码了“吾輩”“は”“猫”这 3 个单

词的信息。如果考

图灵社

区会员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

8.4 关

于Attention的其他话题  357

虑整体的

平衡性，我们希望向量能

更均衡地包含单词“猫”周

围的信息。

在这次的翻译

问题中，我们获得了所有

的时序数据（需要翻译的

文本）。因此，我们既可以从

左向右读取文本，也可以

从右向左

读取文本。

为此

，可以让 LSTM 从两个方向进行

处理，这就是名为双向

LSTM 的

技术，如图 8-30 所示。

hs

LSTM

LSTM

LSTM LSTM LSTM LSTM

LSTM

LSTM LSTM LSTM

Embedding Embedding Embedding

Embedding Embedding

吾輩 は 猫 で

ある

图

8-30　基于双向LSTM进行编码的例

子（这里简化了LSTM层）

如图 8-30 所

示，双向

LSTM 在之前的 LSTM 层上添

加了一个反方向

处理的

LSTM 层。然后，拼接各个时刻的

两个

LSTM 层的隐藏状态，将

其

作为最后的隐藏状态向

量（除了拼接之外，也可以

“求和”或者“取平

均”等）。

通过

这样的双向处理，各个单

词对应的隐藏状态向量

可以从左右两个方

向聚

集信息。这样一来，这些向

量就编码了更均衡的信

息。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

第 8章　Attention 358

双向 LSTM

的实现

非常简单。一种实现方式

是准备两个 LSTM 层（本

章中是

Time LSTM 层），并调整输入各个层的

单词的排列。具体而言，其

中一个层的输入语句与

之前相同，这相当于从左

向右处理输入语句的常

规的

LSTM 层。而另一个 LSTM 层的输

入语句则按照从右到左

的顺序输入。如

果原文是

“A B

C D”，就改为“D C B A”。通过输入改变了

顺序的输入

语句，另一个

LSTM

层从右向左处理输入语

句。之后，只需要拼接这两

个

LSTM 层的输出，就可以创建

双向 LSTM 层。

为了便于理解，本

章使用了单向

LSTM 作为编码

器。不过，显然也

可以将此

处介绍的双向 LSTM 用作编码

器。感兴趣的读者可以尝

试实现使用双向 LSTM

的带 Attention 的

seq2seq。另外，common/

time_layers.py的 TimeBiLSTM类中有双向 LSTM

的实

现，感兴趣的

读者可以参

考一下。

8.4.2  Attention层的使用方法

接

下来，我们思考

Attention 层的使用

方法。截止到目前，我们使

用的

Attention 层的层结构如图 8-31 所

示。

8.4 关于Attention的其他话题  359

图8-31　上

一节之前使用的带Attention的seq2seq的

层结构

吾輩は猫である

hs

hs⮱ᰭ㏵݄

Softmax

with Loss

Softmax

with Loss

Affine

Attention

LSTM

Embedding

Affine

Attention

LSTM

Embedding

I

I

am

<eos>

ᢌ๞

㑃⴮க

如图

8-31 所示

，我们将 Attention 层插入了 LSTM 层和

Affine 层

之间，

不过使用 Attention 层的方式

并不一定非得像图 8-31

那样

。实际上，使用

Attention 的模型还有

其他好几种方式。比如，文

献 [48] 中以图 8-32

的结构

使用了

Attention。

  第 8章

Attention 360

hs hs

hs⮱ᰭ㏵݄

吾輩は猫である

I

I

am

<eos>

Softmax

with

Loss

Softmax

with Loss

Affine Affine

Attention Attention

LSTM LSTM

Embedding Embedding

ᢌ๞

㑃⴮க

图8-32 Attention层的其他

使用示例（这里参考了文

献[48]，并简化了网络结构）

在

图 8-32

中，Attention 层的输出（上下文向

量）被连接到了下一时刻

的 LSTM 层的输入处。通过这种

结构，LSTM 层得以使用上下文

向量的信

息。相对地，我们

实现的模型则是

Affine 层使用

了上下文向量。

那么，Attention 层的

位置的不同对最终精度

有何影响呢？答案要试一

下才知道。实际上，这只能

使用真实数据来验证。不

过，在上面的两个模型

中

，上下文向量都得到了很

好的应用。因此，在这两个

模型之间，我们可能

看不

到太大的精度差异。

从实

现的角度来看，前者的结

构（在 LSTM 层和 Affine 层之间插入

Attention

层

）更加简单。这是因为在前

者的结构中，解码器中的

数据是从

下往上单向流

动的，所以 Attention 层的模块化会

更加简单。实际上，我们

轻

松地将其模块化为了 Time

Attention 层

。

8.4.3  seq2seq的深层化和 skip

connection

在诸如翻译

这样的实际应用中，需要

解决的问题更加复杂。在

这种情

8.4 关于Attention的其他话题

361

况下，我们希望带

Attention 的 seq2seq 具有

更强的表现力。此时，首先

可以考虑到的是加深 RNN 层

（LSTM

层）。通过加深层，可以创建

表现力

更强的模型，带 Attention 的

seq2seq 也是如此。那么，如果我们

加深带

Attention

的 seq2seq，结果会怎样呢

？图 8-33 给出了一个例子。

…

…

…

…

…

…

Softmax

with

Loss

Affine

Attention

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM LSTM LSTM LSTM

LSTM

LSTM LSTM LSTM LSTM

LSTM

LSTM LSTM LSTM

Embedding Embedding Embedding

Embedding Embedding Embedding

hs I

<eos>

吾輩

は 猫 で ある

ᢌ๞

图8-33

使用了3层LSTM层的带Attention的

seq2seq

在图 8-33 的模型中，编码器和

解码器使用了 3 层

LSTM 层。如本

例所

示，编码器和解码器

中通常使用层数相同的

LSTM 层。另外，Attention 层

的使用方法有

许多变体。这里将解码器

LSTM

层的隐藏状态输入 Attention

层，然

后将上下文向量（Attention 层的输

出）传给解码器的多个层

（LSTM

层和 Affine

层）。

图 8-33 的模型只是一

个例子。除了这个例子之

外，还有很多方式，

比如使

用多个 Attention

层，或者将 Attention 的输出

输入给下

一个时刻的 LSTM 层

等。另外，如上一章所述，在

加深层时，避

免泛化性能

的下降非常重要。此时，Dropout、权

重共享等技术

可以发挥

作用。

另外，在加深层时使

用到的另一个重要技巧

是残差连接（skip connection，

第 8章　Attention 362

也称为

residual connection

或 shortcut）。如图 8-34 所示，这是一种“跨

层

连接”的简单技巧。

LSTM

LSTM

图8-34 LSTM层

中的skip connection的例子

如图 8-34

所示，所

谓残差连接，就是指“跨层

连接”。此时，在残差连

接的

连接处，有两个输出被相

加。请注意这个加法（确切

地说，是对应元素

的加法

）非常重要。因为加法在反

向传播时“按原样”传播梯

度，所以残差

连接中的梯

度可以不受任何影响地

传播到前一个层。这样一

来，即便加深了

层，梯度也

能正常传播，而不会发生

梯度消失（或者梯度爆炸

），学习可以

顺利进行。

在时

间方向上，RNN 层的反向传播

会出现梯度消失或梯度

爆炸的问

题。梯度消失可

以通过 LSTM、GRU 等 Gated

RNN 应对，梯度爆

炸

可以通过梯度裁剪应对

。而对于深度方向上的梯

度消失，这里介

绍的残差

连接很有效。

图灵社区会

员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权

8.5 Attention的应用

363

8.5

Attention的应用

到目前为止，我们

仅将 Attention 应用在了 seq2seq 上，但是

Attention

这

一想法本身是通用的，在

应用上还有更多的可能

性。实际上，在近些年

的深

度学习研究中，作为一种

重要技巧，Attention 出现在了各种

各样的场

景中。本节我们

将介绍 3

个使用了 Attention 的前沿

研究，以使读者感受到

Attention 的

重要性和可能性。

8.5.1

GNMT

回看机

器翻译的历史，我们可以

发现主流方法随着时代

的变迁而演变。

具体来说

，就是从“基于规则的翻译

”到“基于用例的翻译”，再到

“基于

统计的翻译”。现在，神

经机器翻译（Neural Machine Translation）取代

了这些

过往的技术，获得了广泛

关注。

神经机器翻译这个

术语是出于与之前的基

于统计的翻译进行对比

而

使用的，现在已经成为

使用了 seq2seq 的机器翻译的统

称。

从

2016 年开始，谷歌翻译就

开始将神经机器翻译用

于实际的服务，其

机器翻

译系统称为 GNMT（Google Neural Machine

Translation，谷歌神

经机

器翻译系统）。关于 GNMT 的技术

细节，文献 [50] 中有具体介绍

。这

里，我们以层结构为中

心来看一下 GNMT 的架构，如图

8-35 所示。

第 8章　Attention 364

图8-35 GNMT的层结构（引

自文献[50]）（参见彩图）

层

GNMT 和本

章实现的带 Attention 的 seq2seq

一样，由编

码器、解码

器和 Attention 构成。不过

，与我们的简单模型不同

，这里可以看到许多为

了

提高翻译精度而做的改

进，比如 LSTM

层的多层化、双向

LSTM（仅编

码器的第 1 层）和 skip connection

等。另

外，为了提高学习速度，还

进行

了多个 GPU 上的分布式

学习。

除了上述在架构上

下的功夫之外，GNMT 还进行了

低频词处理、用于

加速推

理的量化（quantization）等工作。利用这

些技巧，GNMT 获得了非

常好的

结果，实际报告出来的结

果如图 8-36 所示。

8.5

Attention的应用  365

图8-36 GNMT的

精度评价：纵轴是人按照

0~6对翻译质量进行的评价

（引自文献[51]）

（参见彩图）

㠞䄚

㺬⤚➆䄚

㠞䄚

∂䄚

㠞䄚

͚᪴

㺬⤚➆䄚

㠞䄚 㠞䄚 㠞䄚

∂䄚

㔨䃾὎ಸ

͚᪴

ϧጒ㔨䃾

⺋㏼㔨䃾喍(/.5喎

ധλⴚ䄚喍1#.5喎

Ⴙ㒻㔨䃾

如图 8-36

所示，与基于短

语的机器翻译（基于统计

的机器翻译的一种）

这种

传统方法相比，GNMT 成功地提

高了翻译精度，其精度进

一步接近了

人工翻译的

精度。像这样，GNMT 给出了出色

的结果，充分展示了神经

翻

译的实用性和可能性

。不过，但凡用过谷歌翻译

的人都知道，它仍存在许

多

不自然的翻译以及人

绝对不会犯的错误。机器

翻译的研究仍在继续。实

际

上，GNMT 只是一个开始，目前

围绕神经翻译的研究非

常活跃。

实现 GNMT 需要大量的

数据和计算资源。根据文

献

[50]，GNMT

使用了大量的训练数

据，（1 个模型）在将近 100 个 GPU

上学

习了 6 天。

另外，GNMT 也在设法基

于可以并行学习 8

个模型

的集成学习和强

化学习

等技术进一步提高精度

。虽然这些事情不是一个

人可以完成的，

但是我们

已经学习了需要用到的

技术的核心部分。

8.5.2  Transformer

到目前

为止，我们在各种地方使

用了 RNN（LSTM）。从语言模型到

文本

生成，从 seq2se 到带 Attention

的 seq2seq 及其组成

部分，RNN 都会

出现。使用 RNN

可以

很好地处理可变长度的

时序数据，（在大多数情况

下）

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊

重版权

㔨䃾㇫Ꮣ

第 8章　Attention 366

能够获得良

好的结果。但是，RNN

也有缺点

，比如并行处理的问题。

RNN 需

要基于上一个时刻的计

算结果逐步进行计算，因

此（基本）不

可能在时间方

向上并行计算 RNN。在使用了

GPU 的并行计算环境下进行

深

度学习时，这一点会成

为很大的瓶颈，于是我们

就有了避开 RNN 的动机。

在这

样的背景下，现在关于去

除 RNN 的研究（可以并行计算

的

RNN 的研究）很活跃，其中一

个著名的模型是 Transformer[52] 模型。

Transformer 是

在“Attention

is all you need”这篇论文中提出来的

方法。

如论文标题所示，Transformer 不

用

RNN，而用 Attention 进行处理。这

里，我

们简单地看一下这个 Transformer。

除

了

Transformer 之外，还有多个研究致

力于去除 RNN，比如用

卷积层

（Convolution 层）代替 RNN

的研究 [54]。这里我们

不去探讨

该研究的细节

，基本上就是用卷积层代

替 RNN 来构成 seq2seq，并

据此实现并

行计算。

Transformer 是基于 Attention 构成的，其

中使用了 Self-Attention

技

巧，这一点很

重要。Self-Attention 直译为“自己对自己

的 Attention”，也

就是说，这是以一个

时序数据为对象的 Attention，旨在

观察一个时序数

据中每

个元素与其他元素的关

系。用 Time Attention 层来说明的话，Self￾Attention 如图

8-37 所示。

hs hs_dec

hs_enc Time Attention Time

Attention

图8-37　左图是常规的Attention，右

图是Self-Attention

8.5 Attention的应用

367

在此之前，我

们用 Attention 求解了翻译这种两

个时序数据之间的对应

关系。如图 8-37 的左图所示，Time

Attention 层

的两个输入中输入的是

不

同的时序数据。与之相

对，如图 8-37 的右图所示，Self-Attention 的两

个输

入中输入的是同一

个时序数据。像这样，可以

求得一个时序数据内各

个元素

之间的对应关系

。

至此，对 Self-Attention 的说明就结束了

，下面我们看一下 Transformer

的层结

构，如图 8-38 所示。

图8-38 Transformer的层结构

（这里参考了文献[52]，并简化

了模型）

“吾輩は猫である”

Nx

Nx

Time Embedding

Time Attention

Time Attention

Time Affine

Time Attention

Feed Forward

Feed Forward

Time Embedding

Time Softmax

with Loss

“<eos> I

am a cat”

“I am a

cat”

ᢌ๞

图灵社区

会员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

第 8章

Attention 368

Transformer 中用

Attention 代替了 RNN。实际上，由图

8-38 可知，

编码器和解码器两

者都使用了 Self-Attention。图

8-38 中的 Feed Forward

层表

示前馈神经网络（在时间

方向上独立的网络）。具体

而言，使用具有一

个隐藏

层、激活函数为

ReLU 的全连接

的神经网络。另外，图中的

Nx 表

示灰色背景包围的元

素被堆叠了 N 次。

图 8-38 显示的

是简化了的 Transformer。实际上，除了

这个架构

外，Skip Connection、Layer

Normalization[8]等技巧也会

被用到。

其他常见的技巧

还有，（并行）使用多个 Attention、编码

时序数据

的位置信息（Positional Encoding，位

置编码）等。

使用

Transformer 可以控制

计算量，充分利用 GPU 并行计

算带来的好

处。其结果是

，与 GNMT

相比，Transformer 的学习时间得以

大幅减少。

在翻译精度方

面，如图 8-39 所示，也实现了精

度提升。

图8-39

使用基准翻译

数据WMT，评价英法翻译的精

度。纵轴是翻译精度的指

标

BLEU值，这个值越高越好（参

考文献[53]）

㠞∂㔨䃾⮱㇫Ꮣ

图 8-39 比较了

3 种方法

。结果是，使用卷积层的 seq2seq（图

中记为

ConvS2S）比 GNMT 精度高，而

Transformer 比使

用卷积层的 seq2seq 还

要高。如此

，不仅仅是计算量，从精度

的角度来看，Attention 也是很有前

途的技术。

8.5 Attention的应用  369

我们之

前组合使用了 Attention

和 RNN， 但 是 由

这 个

研 究 可 知，

Attention 其实可以

用来替换

RNN。这样一来，利用

Attention 的机会可能

会进一步增

加。

8.5.3  NTM

我们在解决复杂问题

时，经常使用纸和笔。从另

一个角度来看，这可以

解

释为基于纸和笔这样的

“外部存储装置”，我们的能

力获得了延伸。同样

地，利

用外部存储装置，神经网

络也可以获得额外的能

力。本节我们讨论的

主题

就是“基于外部存储装置

的扩展”。

RNN 和

LSTM 能够使用内部

状态来存储时序数据，但

是它们的内

部状态长度

固定，能塞入其中的信息

量有限。因此，可以考虑在

RNN 的外部配置存储装置（内

存），适当地记录必要信息

。

在带 Attention

的 seq2seq 中，编码器对输入

语句进行编码。然后，解

码

器通过 Attention 使用被编码的信

息。这里需要注意的仍是

Attention

的

存在。基于 Attention，编码器和解

码器实现了计算机中的

“内存操作”。换

句话说，这可

以解释为，编码器将必要

的信息写入内存，解码器

从内存中读

取必要的信

息。

可见计算机的内存操

作可以通过神经网络复

现。我们可以立刻想到一

个方法：在

RNN 的外部配置一

个存储信息的存储装置

，并使用 Attention

向这个存储装置

读写必要的信息。实际上

，这样的研究有好几个，NTM

（Neural Turing

Machine，神

经图灵机）[55] 就是其中比较

有名的一个。

NTM 是 DeepMind 团队进行

的一项研究，后来被改进

成名为

DNC

（Differentiable Neural Computers，可微分神经计算

机 )[56] 的方

法。关于 DNC 的论文发

表在了学术期刊《自然》上

。DNC 可以认

为是强化了内存

操作的 NTM，但它们的核心技

术是一样的。

在解释 NTM 的内

容之前，我们先来看一下

NTM 的整体框架。图 8-40

第 8章　Attention 370

这张

有趣的图片非常适合用

于这一目的。这是 NTM

所进行

的处理的概念表

示，很好

地总结了 NTM 的精髓（准确地

说，这是发展了 NTM 的

DNC 的

一篇

解说文章 [57] 中用到的图）。

图

8-40

NTM的概念图（引自文献[57]）

现在

我们看一下图 8-40。这里需要

注意的是图中间的一个

被称为“控

制器”的模块。这

是处理信息的模块，我们

假定它使用神经网络（或

者

RNN）。从图中可以看出，数据

“0”和“1”一个接一个地流入这

个控制

器，控制器对其进

行处理并输出新的数据

。

这里重要的是，在这个控

制器的外侧有一张“大纸

”（内存）。基于这个

内存，控制

器获得了计算机（图灵机

）的能力。具体来说，这个能

力是指，

在这张“大纸”上写

入必要的信息、擦除不必

要的信息，以及读取必要

信息

8.5 Attention的应用

371

的能力。顺便

说一下，因为图 8-40 的“大纸”是

卷式的，所以各个节点可

以在需要的地方读写数

据。换句话说，就是可以移

动到目标地点。

像这样，NTM 在

读写外部存储装置的同

时处理时序数据。NTM

的有

趣

之处在于使用“可微分”的

计算构建了这些内存操

作。因此，它可以从数

据中

学习内存操作的顺序。

计

算机根据人编写的程序

进行动作。与此相对，NTM 从数

据中学

习程序。也就是说

，这意味着它可以从“算法

的输入和输出”中学

习“算

法自身”（逻辑）。

NTM 像计算机一

样读写外部存储装置，其

层结构可以简单地绘制

成

图 8-41。

图8-41

NTM的层结构：新出现

了Write Head层和Read Head层，它们进行内存

读写

…

…

…

…

…

Softmax

output[t]

input[t] input[t+1]

output[t+1]

Softmax

Affine Affine

Write Head

Read Head

Embedding Embedding

LSTM LSTM

ႅڲ

  第 8章　Attention

372

图 8-41 是简化版的

NTM 的层结构。这里 LSTM

层是控制

器，执行

NTM 的主要处理。Write Head 层接

收 LSTM

层各个时刻的隐藏状

态，将

必要的信息写入内

存。Read Head 层从内存中读取重要

信息，并传递给下

一个时

刻的 LSTM

层。

那么，图 8-41 的 Write Head

层和 Read Head 层

如何进行内存操作呢？

当

然是使用 Attention。

重申一下，在读

取（或者写入）内存中某个

地址上的数据时，我们

需

要先“选择”数据。这个选择

操作自身是不能微分的

，因此先

使用 Attention 选择所有地

址上的数据，再利用权重

表示每个数

据的贡献，这

样就能够利用可微分的

计算替代选择这个操作

。

为了模仿计算机的内存

操作，NTM 的内存操作使用了

两个 Attention，

分别是“基于内容的

Attention”和“基于位置的 Attention”。基于内容

的

Attention

和我们之前介绍的 Attention 一

样，用于从内存中找到某

个向量

（查询向量）的相似

向量。

而基于位置的 Attention

用于

从上一个时刻关注的内

存地址（内存的各

个位置

的权重）前后移动。这里我

们省略对其技术细节的

探讨，具体可以通

过一维

卷积运算实现。基于内存

位置的移动功能，可以再

现“一边前进（一

个内存地

址）一边读取”这种计算机

特有的活动。

NTM 的内存操作

比较复杂。除了上面说到

的操作以外，还包括

锐化

Attention 权重的处理、加上上一个

时刻的 Attention 权重

的处理等。

通

过自由地使用外部存储

装置，NTM

获得了强大的能力

。实际上，对

于 seq2seq 无法解决的

复杂问题，NTM 取得了惊人的

成绩。具体而言，

NTM

成功解决

了长时序的记忆问题、排

序问题（从大到小排列数

字）等。

如此，NTM 借助外部存储

装置获得了学习算法的

能力，其中 Attention

作为一项重要

技术而得到了应用。基于

外部存储装置的扩展技

术和 Attention

8.6 小结  373

会越来越重要

，今后将被应用在各种地

方。

8.6

小结

本章我们学习了

Attention 的结构，并实现了 Attention 层。然后

，我

们使用

Attention 实现了 seq2seq，并通过

简单的实验，确认了 Attention

的出

色效果。另外，我们对模型

推理时的 Attention

的权重（概率）进

行了

可视化。从结果可知

，具有 Attention 的模型以与人类相

同的方式将注意力

放在

了必要的信息上。

另外，本

章还介绍了有关

Attention 的前沿

研究。从多个例子可知，

Attention 扩

展了深度学习的可能性

。Attention 是一种非常有效的技术

，

具有很大潜力。在深度学

习领域，今后

Attention 自己也将吸

引更多的“注

意力”。

图灵社

区会员 Kensuke(cpy4ever@gmail.com) 专享

尊重版权

  第

8章　Attention 374

本章所学的内容

• 在翻

译、语音识别等将一个时

序数据转换为另一个时

序数据的任务

中，时序数

据之间常常存在对应关

系

• Attention

从数据中学习两个时

序数据之间的对应关系

• Attention 使用向量内积（方法之一

）计算向量之间的相似度

，并输

出这个相似度的加

权和向量

• 因为

Attention 中使用的

运算是可微分的，所以可

以基于误差反向传

播法

进行学习

• 通过将 Attention

计算出

的权重（概率）可视化，可以

观察输入与输

出之间的

对应关系

• 在基于外部存

储装置扩展神经网络的

研究示例中，Attention 被用来

读写

内存

附录 A

sigmoid函数和 tanh函数的

导数

神经网络使用各种

各样的激活函数，这里将

讨论具有代表性的 sigmoid

函数

和 tanh 函数，使用两种不同的

方法计算这两个函数的

导数。具体来说，

就是使用

计算图计算 sigmoid 函数的导数

，使用数学式计算

tanh 函数的

导

数。通过理解各个方法

，来熟悉导数的计算。

A.1 sigmoid函数

sigmoid 函数可用下式表示：

y = 1

1 + exp(−x)

(A.1)

此时

，式 (A.1) 的计算图如图 A-1 所示。

x −x

y

exp(−x) 1+exp(−x)

exp

−1 1

1+exp(−x)

1

图

A-1 Sigmoid层的计算图

附录A　sigmoid函数和

tanh函数的导数 376

在图 A-1

中，除了

“×”和“+”节点之外，还有 exp 和“/”节点

。

exp 节点进行 y

= exp (x) 的计算，“/”节点进

行 y =

x

1 的计算。现在，我

们使用

计算图，来逐一确认它们

的反向传播。

步骤 1

“/”节点表

示 y = x

1，它的导数可以解析性

地如下表示：

y

x = − 1

x2 =

−

 x

1

2

= −y

2 (A.2)

基于式

(A.2)，在反

向传播时，对上游的梯度

乘以 −y2

（正向传播的输

出的

平方取负），再传递给下游

，如图 A-2 所示。

x −x y exp(−x) 1+exp(−x)

−1

1

exp

− L

y y2

L

y

图A-2　反向传播的

步骤1

步骤 2

“+”节点将上游的

值原样传递给下游，它的

计算图如图 A-3 所示。

x −x y

exp(−x) 1+exp(−x)

−1 1

exp

−

L

y y2 L

y −

L

y y2

图A-3　反向

传播的步骤2

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

A.1

sigmoid函数  377

步骤 3

exp

节

点表示 y = exp (x)，它的导数可用下

式表示：

y

x = exp(x) （A.3）

在计算图中，对上

游的梯度乘以正向传播

时的输出（此处为 exp

(−x)），

再传递

给下游（图 A-4）。

图A-4　反向传播的

步骤3

x

−x y

− L

y y2

exp(−x) − L

y y2 −

L

y y2 L

y

exp(−x)

1+exp(−x)

−1 1

exp

步骤 4

“×”节点乘以将正

向传播时的输入交换后

的值，这里乘以 −1（图 A-5）。

图A-5 sigmoid函数

的计算图（反向传播）

x

−x

−1 1

y

− L

y y2 exp(−x) − L

y

y2 − L

y y2 L

y

L

y y2 exp(−x)

exp(−x)

exp

1+exp(−x)

如上

所示，Sigmoid 层可以按图 A-5 的计算

图进行反向传播。从图

A-5

的

结果可知，反向传播的输

出是 L

y y2 exp(−x)，这个值被传递给下

游节

点。这个 L

y y2 exp(−x)可以进一步

整理如下：

图灵社区会员

Kensuke(cpy4ever@gmail.com)

专享 尊重版权

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

  附录A　sigmoid函数

和

tanh函数的导数 378

L

y

y

2

exp(−x) = L

y

1

(1

+ exp(−x))2 exp(−x)

= L

y

1

1 + exp(−x)

exp(−x)

1

+ exp(−x)

= L

y

y(1

− y)

(A.4)

由式 (A.4) 的展

开可知，仅根据正向传播

时的输出，就可以计算

sigmoid

函

数的反向传播。综上所述

，sigmoid 函数的计算图可以画成

图 A-6。

图A-6 sigmoid函数的计算图

x y

L

y y(1 −

y) L

y

sigmoid

以上

就是 sigmoid

函数的导数。这里使

用计算图计算了 sigmoid 函数的

导数。下面来看一下如何

解析性地计算 tanh 函数的导

数。

A.2

tanh函数

tanh 函数也称为双曲

正切（hyperbolic tangent）函数，可以用式 (A.5)

表示

：

y = tanh(x) = ex −

e−x

ex + e−x (A.5)

我们的目标是对式

(A.5) 求 x

y。因

此，这里使用下面的导数

公式：

图灵社区会员 Kensuke(cpy4ever@gmail.com)

专享

尊重版权

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

∂

A.2 tanh函数

379



f

g(

(

x

x

)

)

= f (x)g(x) −

f(x)g (x)

g(x)2 (A.6)

式

(A.6) 是分数

函数的导数公式。为了便

于观察，这里将 f

g(

(

x

x

)

)

记为 f(x)

g(x)



 。

同样

地，将 f(x) 关于 x 的导数记为 f



(x)。

另

外，关于纳皮尔数（e），可以解

析性地推导出下面的导

数：

ex

x =

ex (A.7)

e−x

x = −e

−x (A.8)

通过使用上面的式 (A.6)、式

(A.7) 和式 (A.8)，tanh

函数的导数可以

如

下求出：

tanh(x)

∂x = (ex

+ e−x)(ex + e−x) − (ex

− e−x)(ex − e−x)

(ex +

e−x)2

= 1 − (ex −

e−x)(ex − e−x)

(ex + e−x)2

= 1 −

 (e

(ex

x + e

− e−

−

x

x

)

)

2

= 1 − tanh(x)

2

= 1 − y

2

(A.9)

如式 (A.9) 所示，使用分

数函数的导数公式，并对

公式进行简单的整

理，就

可以求出 tanh 函数的导数，结

果是

1 − y2

。根据这个结果，tanh 函

数

的计算图可以绘制成图

A-7。

∂

∂

∂

∂

∂

附录A　sigmoid函数和 tanh函数的导数

380

图A-7 tanh函数的计算图

x

y

L

y (1 − y2)

L

y

tanh

以上就

是 tanh 函数的导数的推导过

程。我们通过展开数学式

，简洁明

了地求得了导数

。

A.3 小结

以上我们分别用计

算图和解析性的方法求

解了导数。两种方法殊途

同

归，读者可以根据不同

的问题选择合适的方法

。不过，在习惯之后，可能会

觉得使用数学式的方法

更加方便。而在刚开始时

，可能计算图这种直观的

方

法比较好（特别是心中

有疑惑时）。能用多个方法

解决问题非常重要！就像

本附录这样，在解决问题

时，有时使用数学式，有时

使用计算图，这可以加

深

我们的理解。

图灵社区会

员 Kensuke(cpy4ever@gmail.com) 专享 尊重版权

∂

∂

∂

∂

附录B

运

行WordNet

在本附录中，我们将实

际运行一下

WordNet，来看一下 WordNet 中

存

在什么样的“知识”。另外

，这里的实现只是为了让

读者对该同义词库有所

了解，所以进行的实验非

常简单。

本附录将简单介

绍 WordNet

和 NLTK。《自然语言处理入门

》[14]

一书中有关于 NLTK 的详细说

明，感兴趣的读者可以参

考一下。

B.1

NLTK的安装

通 过 Python 利 用

WordNet，可以使用

NLTK（Natural Language 

Toolkit，自然语言处理

工具包）这个库。NLTK 是用于自

然语言处理的

Python

库，其中包

含许多自然语言处理相

关的便捷功能，比如词性

标注、

句法分析、信息抽取

和语义分析等。

现在我们

就来安装 NLTK。安装方法有很

多，这里介绍如何使用 pip 进

行安装（其他的安装方法

，请读者根据自身环境自

行尝试）。

要安装 NLTK，需要向终

端输入下面一行代码。



pip install nltk

附录B　运行WordNet 382

这样 NLTK 的安装就

结束了（安装需要一些时

间）。在安装结束后，导

入 NLTK，以

确认安装是否成功。

>>> import nltk

>>>

这里

，启动 Python 解释器，导入 NLTK。如果 NLTK

安

装正确，则

如上所示，不显

示任何错误。

B.2 使用WordNet获得同

义词

下面，我们来实际使

用一下 WordNet。首先，从

nltk.corpus 中导入

wordnet 模

块。

>>> from

nltk.corpus import wordnet 

准备完毕。我们试着查

看一下单词 car

的同义词，在

此之前，我们先看

一下单

词 car 存在多少个不同的含

义。为此，使用 wordnet.synsets() 方法。

在 WordNet 中，每

个单词都被归类到了名

为 synset 的同义词簇中。

为了得

到

car 的同义词簇，只需要调

用 wordnet.synset()方法。这

里有一点需要

注意，那就是单词 car（和其他

许多单词一样）存在多

个

含义。具体来说，除了“汽车

”的含义之外，它还有“（火车

）车

厢”“吊舱”的含义。因此，在

获得同义词时，需要（从多

个含义中）

指定是哪个含

义。

现在，我们尝试用 WordNet 获得

car 的同义词。

>>> wordnet.synsets('car') 

[Synset('car.n.01'),

Synset('car.n.02'),

 Synset('car.n.03'),

 Synset('car.n.04'),

Synset('cable_car.n.01')]

B.2 使用WordNet获得同义

词  383

这里输出了一个包含

5

个元素的列表，这表示 car 这

个单词被定义了 5

种不同

的含义（严格来说，是 5

个不

同的同义词簇）。

还有一点

需要注意的是，上面的列

表中的元素显示的是 car 的

“标题

词”。如图 B-1

所示，WordNet 中使用

的标题词由被“.”切分的 3 个

元素指

定。比如，“car.n.01”这个标题

词表示“car 的第

1 个名词”的含

义（簇）。

图B-1 WordNet中的标题词的解

读方法：n是noun（名词）的首字母

口ऺ䃺ࢂ

ᆋᕔ喍ऺ䃺Ƞߕ䃺ぶ喎

ㄴ⮱㉏ᑂ

DBS

O





许多单词都有多

个含义。WordNet 中使用标题词从

单词的多个

含义中指定

某个特定的含义。因此，一

般情况下，在 WordNet

的方法中需

要给参数指定单词名称

时，不是指定“car”，而是指

定“car.n.01”或

者“car.n.02”这样的标题词。

下面，我

们来确认“car.n.01”这一标题词指

定的同义词的含义。为此

，

使用

wordnet.synset() 方法，获取“car.n.01”的同义词

簇。另外，对该同

义词簇调

用 definition() 方法。

>>>

car = wordnet.synset('car.n.01') # 同义词簇

>>>

car.definition()

'a motor vehicle with four

wheels; usually propelled by an internal

combustion engine' 

上面的

结果可直译为“使用内燃

机驱动的有 4 个车轮的机

动车”，这就

是“car.n.01”这个同义词

簇的含义。另外，这里使用

的 definition() 方法

主要是用来帮助

人（而非计算机）理解该单

词的。

现在我们来实际看

一下“car.n.01”这个标题词的同义

词簇中有什么样

的单词

。为此，使用

lemma_names() 方法。

  附录B　运行

WordNet

384

>>> car.lemma_names()

['car', 'auto', 'automobile',

'machine', 'motorcar'] 

如此，使用 lemma_names() 方法，可以获得

同义词簇中存在的单词

名称。

从上面的结果可知

，在 car 这个单词（严格地讲，是

“car.n.01”这个标题

词）中，有 auto、automobile、machine 和

motorcar 这 4 个

单词被定义为

了同义词

。

B.3

WordNet和单词网络

接下来，我们

使用 car 的单词网络，查看一

下它和其他单词在语义

上的

上下位关系。为此，可

以使用 hypernym_paths()

方法。hypernym 主要是语

言

学中用到的单词，意思是

“上位词”。

>>> car.hypernym_paths()[0]

[Synset('entity.n.01'),

Synset('physical_entity.n.01'), 

Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'),

Synset('instrumentality.n.03'), Synset('container.n.01'), 

Synset('wheeled_vehicle.n.01'), Synset('self-propelled_vehicle.n.01'),

Synset('motor_vehicle.n.01'), Synset('car.n.01')] 

从上面的结果可

知，car 这个单词从 entity

这个单词

出发，经过了

“entity → physical_entity → object

→ ... → motor_vehicle → car”这一

路径（此

处省略了“标题词”的标记

）。这里具体看一下各个单

词，car 的

上一层是 motor vehicle（机动车），再

上一层是 self-propelled

vehicle（自

走式车辆），继

续往上是 object、entity 等抽象单词。在

构成 WordNet 的单

词网络中，各个

单词被配置成越往上走

越抽象，越往下走越具体

。

在上面的例子中，car.hypernym_paths()返回列

表，该列表元素

中包含了

具体的路径信息。为什么

要返回列表呢？因为单词

之间

的路径可能存在多

个。就上面的例子来说，从

起点单词 entity 到

终点单词 car 有

多条路径（有的单词可能

只有一条路径）。

B.4 基于WordNet的语

义相似度

385

B.4 基于WordNet的语义相

似度

如前所述，WordNet 中许多单

词根据同义词（近义词）被

分组。另外，

单词之间被构

建了语义网络。这些单词

之间的关联知识可以应

用在很多问题

中。这里，我

们来看一个计算单词之

间的相似度的例子。

求单

词之间的相似度，可以使

用 path_similarity() 方法。这个方法返

回单

词之间的相似度，其返回

值是 0

～ 1 的实数（数值越大，越

相似）。现

在我们实际地求

一下单词之间的相似度

。这里分别求单词 car（汽车）和

novel（小说）、dog（狗）、motorcycle（摩托车）之间的相

似度。

>>>

car = wordnet.synset('car.n.01')

>>> novel =

wordnet.synset('novel.n.01')

>>> dog = wordnet.synset('dog.n.01')

>>>

motorcycle = wordnet.synset('motorcycle.n.01') 

>>> car.path_similarity(novel)

0.05555555555555555

>>> car.path_similarity(dog) 

0.07692307692307693

>>> car.path_similarity(motorcycle) 

0.3333333333333333 

从上面的结果可知

，对于单词

car，单词 motorcycle 的相似度

最高，其

次是 dog，最不相似的

单词是 novel。从相似度的值来

看，car

和 motorcycle

的相似度很大，值比

其他两个单词大很多倍

。这些结果可以说很接近

我们的

感觉。

上例中用到

的 path_similarity()

方法会在内部基于图

B-2 所示的单词

网络的公共

路径计算单词之间的相

似度。

  附录B

运行WordNet 386

图B-2　基于单

词网络的公共路径计算

单词的语义相似度（虚线

表示途中还有多个单词

）

entity

abstraction

object

whole living thing

motor vehicle

dog car novel motorcycle

physical entity

图 B-2 展示了 WordNet 的部分单词网

络（省略了途中的单词）。从

该

图可知，car

和 motorcycle 之间有许多

公共路径。实际上，它们到

倒数（从

底部数）第二个单

词 motor vehicle

的路径是相同的。比较

car 和 dog 可以

发现，它们的路径

在单词 object

处分叉了。再比较

car 和 novel 可以发现，

它们的路径

在最上面的单词 entity

处就已

经分叉了。path_similarity() 方

法基于这些

信息计算单词之间的相

似度，（在本例中）其结果很

接近我们的

感觉。

像这样

，使用单词网络，可以计算

两个单词之间的相似度

。计算单词之

间的相似度

意味着我们可以测量单

词和单词在语义上的距

离。而如果没有理

解单词

含义，就没有办法正确完

成此类任务。从这一点来

看，同义词库可以

说（间接

）赋予了计算机理解单词

含义的能力。

除了 path_similarity()方法之

外，WordNet 中还提供了几个用来

测量相似度的方法（比如

Leacock-Chodorow 相似度、Wu-Palmer

相似度等）。感兴趣

的读者请参考WordNet 的Web 文档 [18]。

附

录C

GRU

第 6 章详细介绍了 Gated RNN 的

LSTM。LSTM 是

一个非常好的层，

但是它

的参数太多，计算需要很

长时间。因此，最近业界又

提出了很多用来

替代 LSTM 的

Gated

RNN。这里，我们介绍一下 GRU（Gated Recurrent 

Unit，门控

循环单元）[42] 这个有名的

Gated RNN。GRU 保

留了 LSTM

使用门的理念，但是

减少了参数，缩短了计算

时间。现在，我们来看一下

GRU 的内部结构。

C.1 GRU的接口

LSTM 的重

点是使用门，因此学习时

梯度的流动平稳，梯度消

失得以

缓解。GRU 也继承了这

一想法。不过，LSTM

和 GRU 存在几个

差异，主

要区别在于层的

接口，如图 C-1 所示。

附录C　GRU 388

图C-1 LSTM和

GRU的比较

ht ht

ht

ht

xt xt

ht−1

ht−1

c t c t−1

LSTM GRU

如图 C-1 所示，相对于

LSTM 使用隐藏状态和记忆单

元两条线，GRU

只使用隐藏状

态。顺便说一下，这和第 5 章

讨论的“简单 RNN”的接口

相同

。

LSTM

的记忆单元是私有存储

，对其他层不可见。LSTM 将必要

信

息记录在记忆单元中

，并基于记忆单元的信息

计算隐藏状态。与此

相对

，GRU 中不需要记忆单元这样

的额外存储。

C.2

GRU的计算图

现

在，我们看一下 GRU 内部进行

的计算。这里用数学式表

示 GRU

中进行的计算，并给出

与之对应的计算图。另外

，计算图使用在第

6 章的

LSTM 的

计算图中使用的 σ 和

tanh 等简

化版节点。

z = σ(xtWx

(z)

+ ht−1Wh

(z) + b(z)) （C.1）

r = σ(xtWx

(r) + ht−1Wh

(r) + b(r)) （C.2）

h˜ =

tanh(xtWx + (r  ht−1)Wh

+ b) （C.3）

ht = (1

− z)  ht−1 +

z  h˜ （C.4）

C.2

GRU的计算图  389

GRU 中进

行的计算由上述 4

个式子

表示（这里 xt和 ht−1 都是行向量

），

对应的计算图如图 C-2

所示

。

图C-2 GRU的计算图：σ节点和tanh节点

有专用的权重，节点内部

进行仿射变换（“1−”

节点输入

x，输出1 − x）

ht−1

ht

ht

xt

r z

1−

tanh σ σ

+

×

×

×

h˜

如图 C-2 所示，GRU

没有记忆

单元，只有一个隐藏状态

h 在时间方向

上传播。这里

使用 r 和 z

共两个门（LSTM 使用 3 个

门），r 称为 reset

门，

z 称为 update 门。

r（reset门）决定

在多大程度上“忽略”过去

的隐藏状态。根据式(C.3)，

如果

r 是 0，则新的隐藏状态 h˜ 仅取

决于输入 xt。也就是说，此时

过去的隐

藏状态将完全

被忽略。

而 update 门是更新隐藏

状态的门，它扮演了 LSTM 的

forget 门

和

input 门两个角色。式 (C.4) 的

(1 − z)  ht−1 部分

充当

forget 门的功能。

根据这个

计算，从过去的隐藏状态

中删除应该被遗忘的信

息。z  h˜

的部分

充当 input 门的功能

，对新增的信息进行加权

。

附录C　GRU 390

综上，GRU 是简化了 LSTM

的架

构，与 LSTM 相比，可以减少计算

成本和参数。这里，我们不

进行 GRU 层的实现，它的代码

在 common/time_

layers.py 中，感兴趣的读者可以

参考一下。

那么，我们应该

使用 LSTM 和 GRU

中的哪一个呢？由

文献 [32] 和

文献 [33] 可知，根据不

同的任务和超参数设置

，结论可能不同。在

最近的

研究中，LSTM（以及 LSTM 的变体）被大

量使用，而 GRU

的人气也在稳

步上升。因为 GRU

的超参数少

、计算量小，所以特

别适合

用于数据集较小、设计模

型需要反复实验的场景

。

图灵社区会员 Kensuke(cpy4ever@gmail.com) 专享 尊重

版权

后记

为了登上险峰

，一开始要走得慢一点。

——莎

士比亚

至此，我们在深度

学习的世界中探索了自

然语言处理这一主题。实

际

上，我们实现了关于自

然语言处理的各种代码

，并通过大量实验学习了

若干

个重要技术，希望读

者能够从这些经历中学

到一些东西。如果读者感

受到了

其中的乐趣与深

奥，那笔者将感到无比开

心。

现在，深度学习领域正

在加速发展，几乎每天都

有大量论文发表，新的

想

法接连不断地被提出来

。遗憾的是，我们没有办法

把所有论文都看一遍。

今

后深度学习将如何发展

，谁也无法准确预测。

此外

，深度学习中的重要技术

（在某种程度上）正在固定

下来。我相信，

本书介绍的

围绕自然语言处理的技

术今后仍将十分重要。希

望读者能够立足

于从本

书学到的知识，在更广阔

的深度学习世界里走得

更扎实。

本书到此结束。非

常感谢大家阅读本书。从

历史的角度来看，我们这

一

代目睹了深度学习如

何一步一步地渗入世界

，并改变世界。笔者只是幸

运地

出生在这一时代，并

恰巧写了关于这一主题

的书。很高兴有此缘分让

笔者通

过本书和大家进

行了交流。十分感谢！

392

后记

致谢

本书的存在离不开

伟大的先驱们对深度学

习、人工智能和自然科学

的推

动。首先，笔者想感谢

他们。其次，还要感谢身边

朋友的支持和帮助。他们

直接或者间接地鼓励、支

持着我。谢谢！

作为一个新

的尝试，本书使用了“公开

审稿”的校稿方式。在这次

公

开审稿中，原稿被公开

在

Web 上，任何人都可以阅读

和评论。结果，仅一

个月的

审稿时间，我们就收到了

1500 多条有用的评论。感谢所

有参加审稿

的朋友！毫无

疑问，正是大家的评论让

本书得到了进一步的完

善。谢谢大

家！当然，本书中

存在的不完善之处或者

错误，均是笔者的责任，与

评论者

无关。

最后，本书的

出版还要归功于世界各

地人民。笔者每天都在受

到许多不

知道名字的人

的影响，缺少其中任何一

个人，本书可能就不存在

了（至少不

是现在这种状

态）。这些话也同样可以对

周围的自然界说，河流、树

木、大

地、天空中都有我的

日常生活。感谢这些无名

的人，无名的自然。

斋藤康

毅

2018 年 6 月 1 日

393 后记 

审稿

寄田

明宏 玉川晃之

吉永彰成

高山笃史 佐藤太树

小林

大将 高屋英知 江崎大嗣

兵头冲 铃木骏

藤冈秀明

田岛英朗 原田秀隆 河村

英幸 谷冈广树

荒井浩 布

施快

渊上纮行 濑户口久

雄 藤田勇

石川敬规 上久

保景幸 神户宏之

金泽隆

史 永田员广

石津和纪 古

贺一德 佐佐木知哉 大滝

启介

山内健太

小泽学 寺

田学 长谷川正彦 山本正

喜 藤井昌纪

水谷穰 三浦

康幸 野口宗之 内藤亮介

米泽直记

佐藤尚至 野口

聪明

伊藤宣博 山下修 佐

藤隆佑

佐藤温 佐藤洁 新

谷正领

小林茂 清水丰

永

田晋介 杉田臣辅 和田信

也 白木宏朋

中村翔

千田

翔太 藤原秀平 铃木英太

阿部考志 宫阪健夫

若杉

武史

铃木琢也 新村拓也

田中智史 蔡天星

石川哲

朗 鹰城彻 森原利之

大野

翼 土井泰法

西田泰士 高

野泰朋 长谷部阳一郎 荻

原义宽

林悠大

增宫雄一

小池祐二 飞永由夏 宵勇

树 藤本裕介

古川悠介

西

口祐介 吉成祐人 水原悠

TAICHI KAWABATA AIMY——山形县人工智能协会

制

作

武藤健志

增子萌

编辑

宫川直树

岩佐未央

小柳

彩良

参考文献

Python 相关

[1] Broadcasting.

[2]

100 numpy exercises.

[3] CuPy web

page.

[4] CuPy install page.

深度

学习基础

[5] 斎藤康毅 . ゼロから作る Deep Learning

—— Python で学

ぶディープ

ラーニングの理論と実装 [M]. 東京：オライリー·ジャパン，2016.

[6] Gupta, Suyog, et al. Deep

learning with limited numerical precision[J].

Proceedings of the 32nd International Conference

on Machine 

Learning (ICML-15), 2015.

[7] Jouppi, Norman P., et al.

In-datacenter performance analysis 

of a

tensor processing unit[J]. Proceedings of the

44th Annual 

International Symposium on

Computer Architecture. ACM, 2017.

[8] Ba,

Jimmy Lei, Jamie Ryan Kiros, Geoffrey

E. Hinton.Layer 

Normalization[J]. arXiv preprint

arXiv: 1607.06450, 2016.

[9] Srivastava, Nitish,

et al. Dropout: a simple way

to prevent neural

参考文献

396

networks from overfitting[J]. Journal of

machine learning research, 

2014, 15(1):

1929-1958.

基于深度学习的自然语

言处理

[10] Stanford University CS224d:

Deep Learning for Natural Language

Processing.

[11] Oxford Deep NLP 2017

course.

[12] Young, D. Hazarika, S.

Poria, E. Cambria.Recent trends in deep

learning based natural language processing[J]. in

arXiv preprint 

arXiv: 1708.02709, 2017.

[13] 坪井祐太，海野裕

也，鈴木潤 . 深層学習による自然

言語処理（機械学

習プロフェッショナルシリーズ）[M]. 東京

：講談社，2017.

深度学习出现之

前的自然语言处理

[14] Steven Bird, Ewane Klein,

Edward Loper. 入門

自然言語処理 [M]. 萩

原正人

，中山敬広，水野貴明，訳.

東

京：オライリー·ジャパン，

2010.

[15] Jeffrey E. F.

Friedl. 詳説正規表現第 3 版 [M]. 株

式会社ロングテール，

長尾高弘，訳. 東京

：オライリー·ジャパン，2008.

[16] Christopher D. Manning,

Hinrich Sch¨utze. 統計的自然言語処理の基

礎 [M]. 加藤恒昭，菊井玄一郎

，林良彦，森辰則，訳. 東京：共

立出版，

2017.

[17] Miller, George A. WordNet:

a lexical database for English[J].

Communications

of the ACM, 1995, 38(11): 39-41.

[18] WordNet Interface.

基于计数的方法

的单词向量

[19] Church,

Kenneth Ward, and Patrick Hanks.Word association

参考文献  397

norms,mutual information, and

lexicography[J]. Computational 

linguistics, 1990, 16(1):

22-29.

[20] Deerwester, Scott, et al.Indexing

by latent semantic analysis[J]. 

Journal

of the American society for information

science, 1990, 41(6): 

391.

[21]

TruncatedSVD.

word2vec 相

关

[22] Mikolov, Tomas,

et al.Efficient estimation of word representations

in 

vector space[J]. arXiv preprint

arXiv:1301.3781, 2013.

[23] Mikolov, Tomas, et

al.Distributed representations of words and

phrases and their compositionality[J]. Advances in

neural information 

processing systems. 2013.

[24] Baroni, Marco, Georgiana Dinu, and

Germ´an Kruszewski. Don’ t 

count,

predict! A systematic comparison of context-counting

vs. 

context-predicting semantic vectors[J]. ACL

(1), 2014.

[25] Levy, Omer, Yoav

Goldberg, Ido Dagan. Improving distributional

similarity with lessons learned from word

embeddings[J]. 

Transactions of the Association

for Computational Linguistics, 2015, 

3:

211-225.

[26] Levy, Omer, Yoav Goldberg.Neural

word embedding as implicit 

matrix

factorization[J]. Advances in neural information processing

systems, 2014.

[27] Pennington, Jeffrey, Richard

Socher, Christopher D. Manning.Glove: 

Global

Vectors for Word Representation[J]. EMNLP. Vol.14.

2014.

[28] Bengio, Yoshua, et al.

A neural probabilistic language model[J].

Journal of machine learning research, 2003,

3(Feb): 1137-1155.

  参考文献 398

RNN 相关

[29] Talathi, Sachin S.,

Aniket Vartak. Improving performance of

recurrent neural network with relu nonlinearity[J].

arXiv preprint 

arXiv:1511.03771, 2015.

[30]

Pascanu, Razvan, Tomas Mikolov, Yoshua Bengio.On

the difficulty 

of training recurrent

neural networks[J]. International Conference on

Machine Learning, 2013.

[31] Understanding LSTM

Networks.

[32] Chung, Junyoung, et al.Empirical

evaluation of gated recurrent 

neural

networks on sequence modeling[J]. arXiv preprint

arXiv:1412.3555, 2014.

[33] Jozefowicz, Rafal, Wojciech

Zaremba, Ilya Sutskever.An empirical 

exploration

of recurrent network architectures[J]. International

Conference on Machine Learning, 2015.

基于

RNN 的

语言模型

[34] Merity, Stephen, Nitish

Shirish Keskar, Richard Socher.Regularizing 

and

optimizing LSTM language models[J]. arXiv preprint

arXiv:1708.02182, 2017.

[35] Zaremba, Wojciech, Ilya

Sutskever, Oriol Vinyals.Recurrent neural 

network

regularization[J]. arXiv preprint arXiv:1409.2329, 2014.

[36]

Gal, Yarin, Zoubin Ghahramani.A theoretically grounded

application 

of dropout in recurrent

neural networks[J].Advances in neural 

information

processing systems, 2016.

[37] Press, Ofir,

Lior Wolf.Using the output embedding to

improve 

language models[J].arXiv preprint arXiv:1608.05859,

2016.

[38] Inan, Hakan, Khashayar Khosravi,

Richard Socher.Tying Word 

图灵社区会员

Kensuke(cpy4ever@gmail.com) 专享

尊重版权

参考文献

399

Vectors and Word

Classifiers: A Loss Framework for Language

Modeling[J]. arXiv preprint arXiv:1611.01462, 2016.

[39]

Word-level language modeling RNN.

seq2seq 相关

[40] Implementation of sequence to sequence

learning for performing 

addition of

two numbers (as strings).

[41] Sutskever,

Ilya, Oriol Vinyals, Quoc V. Le.Sequence

to sequence 

learning with neural

networks[J]. Advances in neural information

processing systems, 2014.

[42] Cho, Kyunghyun,

et al.Learning phrase representations using RNN

encoder-decoder for statistical machine translation[J]. arXiv

preprint 

arXiv:1406.1078, 2014.

[43] Vinyals,

Oriol, Quoc Le.A neural conversational model[J].

arXiv 

preprint arXiv:1506.05869, 2015.

[44]

Zaremba, Wojciech, Ilya Sutskever.Learning to execute[J].

arXiv 

preprint arXiv:1410.4615, 2014.

[45]

Vinyals, Oriol, et al.Show and tell:

A neural image caption 

generator[J].Computer

Vision and Pattern Recognition (CVPR),

2015 IEEE Conference on. IEEE, 2015.

[46] Karpathy, Andrej, Li Fei-Fei.Deep visual-semantic

alignments for 

generating image descriptions[J].

Proceedings of the IEEE conference

on computer vision and pattern recognition,

2015.

[47] Show and Tell: A

Neural Image Caption Generator.

Attention 相关

[48] Bahdanau, Dzmitry, Kyunghyun Cho, Yoshua

Bengio：Neural 

machine translation by jointly

learning to align and translate[J].

参考文献 400

arXiv preprint arXiv:1409.0473,

2014.

[49] Luong, Minh-Thang, Hieu Pham,

Christopher D. Manning.Effective 

approaches to

attention-based neural machine translation[J]. arXiv

prelprint arXiv:1508.04025, 2015.

[50] Wu, Yonghui,

et al. Google’s neural machine translation

system: 

Bridging the gap between

human and machine translation[J]. arXiv

preprint arXiv:1609.08144, 2016.

[51] A Neural

Network for Machine Translation, at Production

Scale.

[52] Vaswani, Ashish, et al.Attention

Is All You Need[J].arXiv preprint

arXiv:1706.03762, 2017.

[53] Transformer: A Novel

Neural Network Architecture for Language

Understanding.

[54] Gehring, Jonas, et al.Convolutional

Sequence to Sequence 

Learning[J]. arXiv

preprint arXiv:1705.03122, 2017.

带外

部存储的 RNN

[55]

Graves, Alex, Greg Wayne, Ivo Danihelka,Neural

turing machines[J]. 

arXiv preprint arXiv:1410.5401,

2014.

[56] Graves, Alex, et al.Hybrid

computing using a neural network with

dynamic external memory[J]. 2016, Nature 538(7626):

471.

[57] Differentiable neural computers.

Dropout

Dropout

Dropout Dropout Dropout

LSTM LSTM

LSTM

LSTM LSTM LSTM

Dropout Dropout

Dropout Dropout Dropout

Dropout Dropout Dropout

Dropout Dropout ...

. . .

. . .

. . .

...

...

...

图6-34 变分Dropout的例子

：具有相同图示的Dropout使用相

同的mask。像这样，

位于同一层

的Dropout使用相同的mask，对时间方

向上的Dropout也有效果

h0 h1 h9

h10 h11 h19

h20 h21 h29

x0 x1 x9

x10 x11 x19

x20 x21 x29

RNN RNN RNN

RNN RNN RNN

RNN RNN RNN

h10

h0 h1

h9 h20 h21 h29 h30

h0

h1 h9 h10 h11 h19

h11

h19 h20 h21 h29 h30

x10

x11 x19 x20 x21 x29 x30

RNN

RNN

RNN RNN RNN RNN

RNN RNN

RNN RNN RNN RNN

RNN

RNN RNN

RNN RNN RNN

RNN RNN RNN

x0 x1 x9

x20 x21 x29 x30

x0 x1

x9 x10 x11 x19

h9

h19

h30

x30

图5-14 Truncated BPTT的数

据处理顺序 䔈㵹႓΍⮱䶧Ꮌ

㠞䄚 㺬⤚➆䄚 㠞䄚 ∂䄚 㠞䄚 ͚᪴

㺬⤚➆䄚

㠞䄚 㠞䄚 㠞䄚

∂䄚

㔨䃾὎ಸ

͚᪴

ϧጒ㔨䃾

⺋㏼㔨䃾喍(/.5喎

ധλⴚ䄚喍1#.5喎

Ⴙ㒻㔨䃾

图

8-36

GNMT的精度评价：纵轴是人按

照0~6对翻译质量进行的评

价（引自文献[51]）

图8-35 GNMT的层结构

（引自文献[50]）

层㔨䃾㇫Ꮣ

图灵社区会

员 Kensuke(cpy4ever@gmail.com)

专享 尊重版权
