内 容 简 介

深度强化学习

结合深度学习与强化学

习算法各自的优势解决

复杂的决策任务。得益于

DeepMind AlphaGo

和 OpenAI Five 成功的案例，深度强化

学习受到大量的关注，相

关技术广泛应用于不同

的领域。

本书分为三大部

分，覆盖深度强化学习的

全部内容。第一部分介绍

深度学习和强化学习的

入门知识、一

些非常基础

的深度强化学习算法及

其实现细节，包括第

1～6 章。第

二部分是一些精选的深

度强化学习研究

题目，这

些内容对准备开展深度

强化学习研究的读者非

常有用，包括第 7～12 章。第三部

分提供了丰富的

应用案

例，包括

AlphaZero、让机器人学习跑

步等，包括第 13～17 章。

本书是为

计算机科学专业背景、希

望从零开始学习深度强

化学习并开展研究课题

和实践项目的学生准

备

的。本书也适合没有很强

的机器学习背景、但是希

望快速学习深度强化学

习并将其应用到具体产

品中的

软件工程师阅读

。

未经许可，不得以任何方

式复制或抄袭本书之部

分或全部内容。

版权所有

，侵权必究。

图书在版编目

（CIP）数据

深度强化学习：基础

、研究与应用 / 董豪等著.—北

京：电子工业出版社，2020.7

ISBN 978-7-121-41188-5

Ⅰ. ①深… Ⅱ. ①董

…

Ⅲ. ①机器学习 Ⅳ. ①TP181

中国版本图书

馆 CIP

数据核字（2021）第 093628 号

责任编

辑：孙学瑛

印

刷：

装　　订：

出版

发行：电子工业出版社

北

京市海淀区万寿路

173 信箱

邮编：100036

开　　本：787×980 1/16

印张：32.5 字数：745 千字

彩插：7

版　　次：2021

年 7 月第 1 版

印

次

：2021 年 7 月第 1 次印刷

定　　价：129.00 元

凡

所购买电子工业出版社

图书有缺损问题，请向购

买书店调换。若书店售缺

，请与本社发行部联系，联

系及邮购电话：（010）88254888，88258888。

质量投诉

请发邮件至

zlts@phei.com.cn，盗版侵权举

报请发邮件至 dbqq@phei.com.cn。

本书咨询

联系方式：（010）51260888-819，faq@phei.com.cn。

专家赞誉

郭毅

可（香港浸会大学副校长

、教授，帝国理工学院教授

，数据科学研究所所

长，英

国皇家工程院院士，欧洲

科学院院士）

我对这本书

覆盖内容的范围之广印

象深刻。从深度强化学习

的基础理论知识，

到包含

代码细节的技术实现描

述，作者们花了大量的精

力致力于提供综合且

广

泛的内容。这样的书籍是

初学者和科研人员非常

好的学习材料。拥抱开源

社区是深度学习得到快

速发展不可或缺的一个

原因。我很欣慰这本书提

供了

大量的开源代码。我

也相信这本书将会对那

些希望深入这个领域的

研究人员非常有用，也对

那些

希望通过开源例子

快速上手的工程师提供

良好的基础。

陈宝权（北京

大学教授，前沿计算研究

中心执行主任，IEEE

Fellow）

本书提供

的深度强化学习内容非

常可靠，缩小了基础理论

和实践之间的差距，

以提

供详细的描述、算法实现

、大量技巧和速查表为特

色。本书作者均是研究

强

化学习的知名大学研究

者和将技术用在各类应

用中的开源社区实践者

。这

本书为不同背景和阅

读目的的读者提供了非

常有用的资源。

金驰（普林

斯顿大学助理教授）

这是

一本在深度强化学习这

个重要领域出版得非常

及时的书。本书以一种简

明清晰的风格提供了详

尽的工具，包括深度强化

学习的基础和重要算法

、具体

实现细节和前瞻的

研究方向。对任何愿意学

习深度强化学习、将深度

强化学

习算法运用到某

些应用上或开始进行深

度强化学习基础研究的

人来说，这本

书都是理想

的学习材料。

III

专家赞誉

李

克之（伦敦大学学院助理

教授）

这本书是为强化学

习、特别是深度强化学习

的忠实粉丝提供的。从 2013 年

开

始，深度强化学习已经

渐渐地以多种方式改变

了我们的生活和世界，比

如会下

棋的

AlphaGo 技术展示了

超过专业选手的理解能

力的“围棋之美”。类似的情

况也会发生在技术、医疗

和金融领域。深度强化学

习探索了一个人类最基

本的

问题：人类是如何通

过与环境交互进行学习

的？这个机制可能成为逃

出“大数

据陷阱”的关键因

素，作为一条强人工智能

的必经之路，通向人类智

慧尚未企及的地方。本书

由

一群对机器学习充满

热情的年轻研究人员编

著，它将向你展示深度强

化学习的世界，通过实例

和

经验介绍加深你对深

度强化学习的理解。向所

有想把未来智慧之匙揣

进口袋的学习者推荐此

书。

IV

前 言

为什么写作本书

人工智能已经成为当今

信息技术发展的主要方

向，国务院印发的《新一代

人工智能发展规

划》中指

出：2020 年我国人工智能核心

产业规模超过

1500 亿元，带动

相关产业规模超过 1 万亿

元；2030 年人工智能核心产业

规模超过 1

万亿元，带动相

关产业规模超过 10 万亿元

。深度强化

学习将结合深

度学习与强化学习算法

各自的优势来解决复杂

的决策任务。

近年来，归功

于 DeepMind

AlphaGo 和 OpenAI Five 这类成功的案例，深

度强化学习受到大

量的

关注，相关技术广泛用于

金融、医疗、军事、能源等领

域。为此，学术界和产业界

急需大量

人才，而深度强

化学习作为人工智能中

的智能决策部分，是理论

与工程相结合的重要研

究方向。

本书将以通俗易

懂的方式讲解相关技术

，并辅以实践教学。

本书主

要内容

本书分为三大部

分，以尽可能覆盖深度强

化学习所需要的全部内

容。

第一部分介绍深度学

习和强化学习的入门知

识、一些非常基础的深度

强化学习算法及其实现

细节，请见第 1～6

章。

第二部分

是一些精选的深度强化

学习研究题目，请见第 7～12 章

，这些内容对准备开展深

度

强化学习研究的读者

非常有用。

为了帮助读者

更深入地学习深度强化

学习，并把相关技术用于

实践，本书第三部分提供

了丰

富的例子，包括 AlphaZero、让机

器人学习跑步等，请见第

13～17 章。

如何阅读本书

本书是

为计算机科学专业背景

、希望从零学习深度强化

学习并开展研究课题和

实践项目的学

生准备的

。本书也适用于没有很强

机器学习背景、但是希望

快速学习深度强化学习

并把它应用到

具体产品

中的软件工程师。

V

前言

鉴

于不同的读者情况会有

所差异（比如，有的读者可

能是第一次接触深度学

习，而有的读者

可能已经

对深度学习有一定的了

解；有的读者已经有一些

强化学习基础；有的读者

只是想了解强

化学习的

概念，而有的读者是准备

长期从事深度强化学习

研究的），这里根据不同的

读者情况给

予不同的阅

读建议。

1. 要了解深度强化

学习。

第 1～6 章覆盖了深度强

化学习的基础知识，其中

第

2 章是最关键、最基础的

内容。如果您

已经有深度

学习基础，可以直接跳过

第 1 章。第 3

章、附录 A 和附录 B 总

结了不同的算法。

2.

要从事

深度强化学习研究。

除了

深度学习的基础内容，第

7 章介绍了当今强化学习

技术发展遇到的各种挑

战。您可以

通过阅读第 8～12 章

来进一步了解不同的研

究方向。

3. 要在产品中使用

深度强化学习。

如果您是

工程师，希望快速地在产

品中使用深度强化学习

技术，第 13～17 章是您关注的重

点。您可以根据业务场景

中的动作空间和观测种

类来选择最相似的应用

例子，然后运用到

您的业

务中。

董豪

2021 年 4 月

VI

关于本书

作者

本书编著方式与其

他同类书籍不同，是由人

工智能开源社区发起的

，我们非常感谢 TensorLayer 中文社区

的支持。最重要的是感谢

家人对我们工作的支持

和强大的祖国对人工智

能产业的

重视。下表列出

了所有章节的作者。

各章

节作者列表

章节 标题 作

者

– 前言 董豪

– 数学符号 张

敬卿

– 序言 董豪、仉尚航

– 基

础部分 董豪

1 深度学习入

门 张敬卿、袁航、廖培元、董

豪

2 强化学习入门 丁子涵

、黄彦华、袁航、董豪、仉尚航

3 强化学习算法分类 张鸿

铭、余天洋

4

深度 Q 网络 黄彦

华、余天洋

5 策略梯度

仉尚

航、黄锐桐、余天洋、丁子涵

6 深度 Q 网络和 Actor-Critic 的结合

张鸿

铭、余天洋、黄锐桐

– 研究部

分 丁子涵

7 深度强化学习

的挑战

丁子涵、董豪

8 模仿

学习 丁子涵

9 集成学习和

规划

张华清、黄锐桐、仉尚

航

10 分层强化学习 黄彦华

、仉尚航、余天洋

11 多智能体

强化学习

张华清、仉尚航

12 并行计算 张华清、余天洋

– 应用部分 董豪、丁子涵

13

Learning to Run 丁

子涵、董豪

VII

关于本书作者

各章节作者列表（续表）

章

节 标题 作者

14 鲁棒的图像

增强 黄彦华、仉尚航、余天

洋

15 AlphaZero 张鸿铭、余天洋

16 模拟环

境中机器人学习 丁子涵

、董豪

17 Arena：多智能体强化学习

平台 丁子涵

18 深度强化学

习应用实践技巧 丁子涵

、董豪

– 总结部分 董豪

– 算法

总结表 丁子涵

– 算法速查

表 丁子涵

VIII

作者简介

董

豪

北京大学计算机系前沿

计算研究中心助理教授

、深圳鹏城实验室双聘成

员。于 2019 年

秋获得英国帝国

理工学院博士学位。研究

方向主要涉及计算机视

觉和生成模型，目的是降

低学习

智能系统所需要

的数据。致力于推广人工

智能技术，是深度学习开

源框架 TensorLayer

的创始人，

此框架

获得 ACM MM 2017 年度最佳开源软件

奖。在英国帝国理工学院

和英国中央兰开夏大学

获

得一等研究生和一等

本科学位。

丁子涵 英国帝

国理工学院硕士。获普林

斯顿大学博士生全额奖

学金，曾在加拿大 Borealis AI、

腾讯

Robotics X 实

验室有过工作经历。本科

就读于中国科学技术大

学，获物理和计算机双学

位。

研究方向主要涉及强

化学习、机器人控制、计算

机视觉等。在 ICRA、NeurIPS、AAAI、IJCAI、

Physical

Review 等顶级期刊

与会议发表多篇论文，是

TensorLayer-RLzoo、TensorLet 和 Arena 开源

项目的贡献者。

仉

尚航 加州大学伯克利分

校 BAIR 实验室（Berkeley AI Research

Lab）博士后研究员

。于 2018

年获得卡内基·梅隆大

学博士学位。研究方向主

要涉及深度学习、计算机

视觉及强化学习。在

NeurIPS、CVPR、ICCV、TNNLS、AAAI、IJCAI 等人

工智能顶级期刊和会议

发表多篇论文。目前

主要

从事

Human-inspired sample-efficient learning 理论与算法研究，包

括 low-shot learning、domain

adaptation、self learning 等。获得 AAAI 2021 Best

Paper Award、美国 2018 Rising Stars in

EECS、

Adobe Collaboration Fund、Qualcomm Innovation Fellowship

Finalist Award 等奖励。

袁

航 英国牛津大学计算机

科学博士在读、李嘉诚奖

学金获得者，主攻人工智

能安全和深度学

习在健

康医疗中的运用。曾在欧

美各大高校和研究机构

研习，如帝国理工学院、马

克斯普朗克研

究所、瑞士

联邦理工和卡内基·梅隆

大学。

张鸿铭 中国科学院

自动化研究所算法工程

师。于 2018 年获得北京大学硕

士研究生学位。本科

就读

于北京师范大学，获理学

学士学位。研究方向涉及

统计机器学习、强化学习

和启发式搜索。

张敬卿 英

国帝国理工学院计算机

系博士生，师从帝国理工

学院数据科学院院长郭

毅可院士。主

要研究方向

为深度学习、机器学习、文

本挖掘、数据挖掘及其应

用。曾获得中国国家奖学

金。2016

IX

作者简介

年于清华大

学计算机科学与技术系

获得学士学位，2017

年于帝国

理工学院计算机系获得

一等研

究性硕士学位。

黄

彦华 就职于小红书，负责

大规模机器学习及强化

学习在推荐系统中的应

用。2016 年在华东

师范大学数

学系获得理学学士学位

。曾贡献过开源项目

PyTorch、TensorFlow 和 Ray。

余

天洋 启元世界算法工程

师，负责强化学习在博弈

场景中的应用。硕士毕业

于南昌大学，是

TensorLayer-RLzoo

开源项目

的贡献者。

张华清 谷歌公

司算法和机器学习工程

师，侧重于多智能体强化

学习和多层次结构博弈

论方向研

究，于华中科技

大学获得学士学位，后于

2017 年获得休斯敦大学博士

学位。

黄锐桐

Borealis AI （加拿大皇家

银行研究院）团队主管。于

2017 年获得阿尔伯塔大学统

计机

器学习博士学位。本

科就读于中国科学技术

大学数学系，后于滑铁卢

大学获得计算机硕士学

位。

研究方向主要涉及在

线学习、优化、对抗学习和

强化学习。

廖培元 目前本

科就读于卡内基·梅隆大

学计算机科学学院。研究

方向主要涉及表示学习

和多模

态机器学习。曾贡

献过开源项目 mmdetection 和 PyTorch

Cluster，在 Kaggle 数据

科学社区曾获

Competitions Grandmaster 称号，最高

排名全球前

25 位。

读者服务

微信扫码回复：41188

• 获取本书

参考链接

•

加入“人工智能

”读者交流群，与更多读者

互动

• 获取各种共享文档

、线上直播、技术分享等免

费资源

• 获取博文视点学

院在线课程、电子书 20

元代

金券

X

致 谢

首先，我们感谢

我国对人工智能事业的

大力支持和对开源建设

的关注，感谢前辈们的指

导，

这鼓励了我们更加大

胆地尝试、创新与实践。我

们也非常感谢开源用户

源源不断地向我们提供

反

馈，为我们不断前进提

供了方向。

在此，我们特别

感谢在本书写作过程中

为我们提供建议的朋友

们，包括：来自 Mila 的付杰，

帝国

理工学院的王剑红和刘

世昆，北京大学的陈坤，加

州大学圣地亚哥分校的

宋萌，阿尔伯塔大

学的马

辰、肖晨骏、梅劲骋、王琰和

杨斌，三星研究院的于桐

，复旦大学的罗旭，休斯敦

大学的

史典，上海交通大

学的张卫鹏，乔治亚理工

学院的康亚舒，华东师范

大学的赵晨萧，弗雷德里

希

米歇尔研究所的刘天

霖，Borealis AI 的丁伟光，小红书的苏

睿龙，启元世界的彭鹏，清

华大学的

周仕佶、常恒、陈

泽铭、毛忆南、袁新杰、叶佳

辉，以及中科院自动化所

的裴郢郡、张清扬、胡金

城

。我们也感谢耶鲁大学的

Jared

Sharp 帮助本书英文版本的语

言检查，感谢林嘉媛的封

面设计。

此外，很多开源社

区的贡献者对本书的代

码库做出了贡献，包括北

京大学的吴睿海和吴润

迪、

鹏城实验室的赖铖、爱

丁堡大学的麦络、帝国理

工的李国、英伟达的 Jonathan Dekhtiar

等，他

们

为维护 TensorLayer 和强化学习实

例库做了很多工作。

董豪

特别感谢北京大学前沿

计算研究中心和深圳鹏

城实验室对 TensorLayer

的开发维护

，以

及探索下一代 AI 开源软

件的支持。感谢广东省重

点领域研发计划资助（编

号 2019B121204008）、

北大新聘学术人员科

启（编号

7100602564）和人工智能算法

研究（编号 7100602567）项目的经

费支

持。特别感谢郭毅可院士

对他研究生和博士工作

的指导。

丁子涵特别感谢

帝国理工学院 Edward Johns

教授对他

硕士研究生工作的指导

。

仉尚航特别感谢加州大

学伯克利分校 Kurt Keutzer 教授和 Trevor

Darrell 教

授对她博士后研究

工作

的指导，卡内基·梅隆大学

José M. F. Moura

教授对她博士研究工作

的指导，北京大学高文

教

授和解晓东教授对她研

究生工作的指导，以及清

华大学朱文武教授的指

导与合作。

XI

前导知识

自从

1946 年第一台真正意义上的

计算机发明以来，人们一

直致力于建造更加智能

的计算机。

随着算力的提

高和数据的增长，人工智

能（Artificial Intelligence，AI）获得了空前的发展，在

一

些任务上的表现甚至

已经超越人类，比如围棋

、象棋，以及一些疾病诊断

和电子游戏等。人工智

能

技术还能被广泛用于其

他应用中，比如药物发现

、天气预测、材料设计、推荐

系统、机器感知

与控制、自

动驾驶、人脸识别、语音识

别和对话系统。

近十年来

，很多国家，比如中国、英国

、美国、日本、德国，对人工智

能进行了大量的投入。

与

此同时，还有很多科技巨

头，比如 Google、Facebook、Microsoft、Apple、百度、华为、腾讯、字

节跳动和阿里巴巴等，也

都积极地参与其中。人工

智能在我们的日常生活

中正变得无处不在，如

自

动驾驶汽车、人脸 ID 和聊天

机器人。毫无疑问，人工智

能对人类社会的发展至

关重要。

在我们深入阅读

本书之前，第一步应该先

了解人工智能领域不同

的子领域，如机器学习（Machine

Learning，ML）、深

度学习（Deep Learning，DL）、强化学习（Reinforcement Learning，RL），

以及本

书的主题——深度强化学习

（Deep Reinforcement Learning，DRL）。图

1 用韦恩图（Venn

Diagram）展示了它们

之间的关系，下面将会逐

一介绍它们。

人工智能

虽

然科学家一直以来都在

努力让计算机变得越来

越智能，但是“智能”的定义

直到今天依

然是非常模

糊的。在这个问题上，Alan

Turing 最早

在他 1950 年曼城大学时的文

章 Computing

Machinery

and Intelligence 中介绍了图灵测试（Turing Test）。图

灵测试可以用来衡量机

器模拟人

类行为的能力

大小。具体来说，它描述了

一个“imitation game”，一个质问者向一个

人和一台计

算机提出一

系列问题，用以判断哪个

是人，哪个是机器。当且仅

当质问者不能分辨出人

和机器时，

图灵测试就通

过了。

人工智能的概念最

早是由 John McCarthy 在

1956 年夏天的达特

茅斯（Dartmouth）会议上提

出的。这次

会议被认为是人工智能

正式进入计算机科学领

域的开端。最早期的人工

智能算法主要

用于解决

可以被数学符号和逻辑

规则公式化的问题。

XII

前导

知识

人工智能

Artificial	Intelligence

机器学习

Machine	Learning

深度学习

Deep	Learning

深度强化学习

Deep		Reinforcement	Learning

强化学习

Reinforcement	Learning

图 1 人工智能、机

器学习、深度学习、强化学

习及深度强化学习之间

的关系

机器学习

机器学

习（Machine Learning，ML）的概念和名字是由 Arthur Samuel（Bell Labs,

IBM, Stanford）

在

1959 年首次提出来的。一个人

工智能系统需要具备从

原始数据中学习知识的

能力，这个能力

就称为机

器学习。很多人工智能问

题可以被这样解决：通过

设计有针对性的模式识

别算法来从原

始数据中

提取有效特征，然后用机

器学习算法使用这些特

征。

比如，在早期的人脸识

别算法中，我们需要特殊

的人脸特征提取算法。最

简单的方法就是使

用主

成分分析（Principal Component Analysis，PCA）降低数据的维

度，然后把低维度特征输

入一

个分类器获得结果

。长期以来，人脸识别需要

纯手工设计的特征工程

算法。针对不同问题设计

特

征提取算法的过程非

常耗时，而且在很多任务

中设计有针对性的特征

提取算法的难度非常大

。比

如，语言翻译的特征提

取需要语法的知识，这需

要很多语言学专家帮助

。然而，一个通用的算法

应

该具备从对不同任务自

行学习出特征提取算法

，以大大降低算法开发过

程中所需的人力的先验

知识。

学术界有很多研究

，使得机器学习能自动学

习数据的表征。表征学习

的智能化不仅可以提升

性能，还能降低解决人工

智能问题的成本。

深度学

习

深度学习是机器学习

中的一个子领域，与其他

算法不同，它主要基于人

工神经网络（Artificial

Neural

Network，ANN）(Goodfellow et al., 2016) 来实现。我们

之所以称它为神经网络

，是因为它

是由生物神经

网络启发设计的。Warren

Sturgis McCulloch 和 Walter Pitts 在

1943 年

共同发表的 A

Logical Calculus of

the Ideas Immanent in Nervous Activity

(McCulloch et al., 1943) 被视为人工

神经

XIII

前导知识

网络的开

端。至此，人工神经网络作

为一种全自动特征学习

器，使得我们不需要对不

同数据开发

特定的特征

提取算法，从而大大提高

了开发算法的效率。

深度

神经网络（Deep Neural Network，DNN）是人工神经网

络的“深度”版本，有很多的

神

经网络层，深层的网络

相比浅层的网络具有更

强的数据表达能力。图 2 展

示了深度学习方法与非

深度学习方法的主要区

别。深度学习方法让开发

者不再需要针对特定数

据来设计纯手工的特征

提

取算法。我们因此也称

这些学习算法为端到端

（End-to-end）方法。但值得注意的是，很

多人质

疑，深度学习方法

是一个黑盒子（Black-box），我们并不

知道它是如何学到数据

特征表达的，往

往缺乏透

明性和可解析性。

数据 特

征工程 学习算法 结果

数

据 深度学习算法

结果

图

2 深度学习方法与非深度

学习方法的区别

虽然现

在看来，深度学习非常流

行，但是在人工神经网络

早期发展阶段，受制于当

时计算机

算力和黑盒子

问题，实际应用很少，并未

受到学术界的广泛关注

。

这种情况直到

2012 年才得到

了改变，当年一个叫 Alexnet (Krizhevsky et al.,

2012) 的模

型在

ImageNet 图像分类竞赛 (Russakovsky et

al., 2015) 中取

得了超过其他方法 10% 以上

的性能。从此，

深度学习开

始受到越来越多的关注

，深度学习方法开始在很

多不同领域超越非深度

学习方法，比

如大家熟悉

的计算机视觉 (Girshick, 2015; Johnson et al.,

2016; Ledig et al., 2017; Pathak

et al., 2016;

Vinyals et al.,

2016) 和自然语

言处理 (Bahdanau et al., 2015)。

强化学习

深度学

习虽然具有了很强大的

数据表达能力，但不足以

建立一个智能的人工智

能系统。这是

因为人工智

能系统不仅需要从给定

的数据中学习，而且还要

像人类那样学习与真实

世界交互。强

化学习作为

机器学习的一个分支，即

可让计算机与环境进行

交互学习。

简单来说，强化

学习把世界分为两个部

分：环境（Environment）与智能体（Agent）。智能体

通过执行动作（Action）来与环境

交互，并获得环境的反馈

。在强化学习中，环境的反

馈是以奖

励（Reward）形式体现的

。智能体学习如何“更好”地

与环境交互，以尽可能获

得更大的奖励。

这个学习

过程建立了环境与智能

体间的环路，通过强化学

习算法来提升智能体的

能力。

XIV

参考文献

深度强化

学习

深度强化学习结合

了深度学习和强化学习

各自的优点来建立人工

智能系统，主要在强化学

习

中使用深度神经网络

的强大数据表达能力，例

如价值函数（Value

Function）可以用神经

网络来近

似，以实现端到

端的优化学习。

DeepMind 是一家成

立于伦敦、以科研为主导

的人工智能技术公司，在

深度强化学习历史上

具

有非常重要的地位。2013 年，仅

在

AlexNet 提出一年以后，他们就

发表了论文 Playing Atari with

Deep

Reinforcement Learning，该文基于电

子游戏的原始画面作为

输入，学习了 7 种游戏。DeepMind

的方

法不需要手工设计特征

提取算法，在 6

个游戏中优

于之前的方法，甚至在 1 个

游戏中赢了

人类。

2017 年，DeepMind

的 AlphaGO 围

棋算法在中国打败了世

界第一围棋大师——柯洁。该

事件

标志着人工智能具

备比人类更好表现的潜

力。深度强化学习是机器

学习的一个子领域，具有

实现

通用人工智能（Artificial General

Intelligence，AGI）的潜

力。但是还有很多的挑战

需要我们解决，

才能真正

地实现这个理想的目标

。

TensorLayer

强化学习的算法很多，而

且从学习算法到实现算

法有一定的距离。因此，本

书中很多章节会

有实现

教学，我们会展示一些算

法中的关键部分是如何

实现的。自从深度学习变

得流行以来，出

现了很多

开源的框架，比如

TensorFlow、Chainer、Theano 和 PyTorch 等，以

支持神经网络的自动

优

化。在本书中，我们选择 TensorLayer，一

个为科研人员和专业工

程师设计的深度学习与

强化学

习库。该库获得了

ACM Multimedia 2017 年度最佳开源软件奖。在

本书定稿时，TensorLayer 2.0

支持

TensorFlow 2.0 作为后

端计算引擎，而在下一版

本中，TensorLayer 将会支持更多的其

他计算

引擎，如华为 MindSpore，以更

好地支持国内外的

AI 训练

芯片。更多关于 TensorLayer 的最新信

息，请访问 GitHub 页面1。

参考文献

BAHDANAU D, CHO K, BENGIO Y,

2015. Neural machine translation by jointly

learning to align and

translate[C]//Proceedings of

the International Conference on Learning Representations

(ICLR).

GIRSHICK R, 2015. Fast R-CNN[C]//Proceedings

of the IEEE International Conference on

Computer

Vision (ICCV). 1440-1448.

GOODFELLOW I,

BENGIO Y, COURVILLE A, 2016. Deep

learning[M]. MIT Press.

1链接见读者服务

XV

前导知

识

JOHNSON J, ALAHI A, FEI-FEI L,

2016. Perceptual Losses for Real-Time Style

Transfer and SuperResolution[C]//Proceedings of the European

Conference on Computer Vision (ECCV).

KRIZHEVSKY

A, SUTSKEVER I, HINTON G E,

2012. Imagenet classification with deep convolutional

neural networks[C]//Proceedings of the Neural Information

Processing Systems (Advances in Neural

Information

Processing Systems). 1097-1105.

LEDIG C, THEIS

L, HUSZAR F, et al., 2017.

Photo-Realistic Single Image Super-Resolution Using

a

Generative Adversarial Network[C]//Proceedings of the IEEE

Conference on Computer Vision and

Pattern

Recognition (CVPR).

MCCULLOCH W S, PITTS

W, 1943. A logical calculus of

the ideas immanent in nervous activity[J].

The bulletin of mathematical biophysics, 5(4):

115-133.

PATHAK D, KRAHENBUHL P, DONAHUE

J, et al., 2016. Context encoders:

Feature learning by

inpainting[C]//Proceedings of the

IEEE Conference on Computer Vision and

Pattern Recognition

(CVPR). 2536-2544.

RUSSAKOVSKY O,

DENG J, SU H, et al.,

2015. Imagenet Large Scale Visual Recognition

Challenge[J].

International Journal of Computer Vision

(IJCV), 115(3): 211-252.

VINYALS O, TOSHEV

A, BENGIO S, et al., 2016.

Show and tell: Lessons learned from

the 2015

mscoco image captioning challenge[J].

IEEE Transactions on Pattern Analysis and

Machine Intelligence

(PAMI).

XVI

数学符号

本书尽可能

地减少了和数学相关的

内容，以帮助读者更加直

观地理解深度强化学习

。本书的

数学符号约定如

下。

基础符号

x scalar，标量

x vector，向量

X matrix，矩

阵

R the set of

real numbers，实数集

dy

dx

derivative of

y with respect to x，标量的导数

∂y

∂x partial derivative of y with

respect to x，标

量的偏导数

∇xy gradient of

y with respect to x，向量的梯度

∇Xy matrix

derivatives of y with respect to

X，矩阵的导数

P(X) a probability distribution over

a discrete variable，离散变量的

概率分布

p(X) a probability

distribution over a continuous variable, or

over a variable whose type has

not been specified，连续变量（或者

未定义连续或者离散的

变量）的概率分布

X ∼ p

the random variable X has distribution，随机变

量

X 满足概率分布 p

E[X] expectation of

a random variable，随机变

量的期望

Var[X] variance of

a random variable，随机变量的方

差

Cov(X, Y )

covariance of two random variables，两个随机变量的协方

差

XVII

数学符号

DKL(P∥Q) Kullback-Leibler divergence of P

and Q，两个概率分

布的 KL 散度

N (x;

µ, Σ) Gaussian distribution over x

with mean µ and covariance Σ，平均值为

µ 且协

方差

为 Σ 的多元高斯分布

强化学习符号

s,

s′

states，状态

a action，动作

r reward，奖励

R

reward function，奖励函数

S set of all

non-terminal states，非终结状

态

S

+ set of

all states, including the terminal state，全部状态，包括终结状

态

A set of actions，动作集合

R set

of all possible rewards，奖励集合

P transition

matrix，转

移矩阵

t discrete time step，离散时间步

T

final time step of an episode，回合

内最终时间步

St state at time t，时间 t

的状

态

At action at time t，时间

t 的动作

Rt reward at time

t, typically due, stochastically, to At

and St，时间 t 的奖

励，通常为

随机量，且由 At

和

St 决定

Gt return following time

t，回报

G

(n)

t n-step return

following time t，n 步回报

Gλ

t

λ-return following time t，λ-回报

π policy,

decision-making rule，策

略

π(s) action taken in

state s under deterministic policy π，根据确定性策略

π，状态

s 时

的动作

XVIII

数学符号

π(a|s)

probability of taking action a in

state s under stochastic policy π，根据

随机性策略

π，状态 s 时执行

动作 a 的概率

p(s

′

, r|s, a) probability of

transitioning to state s

′

,

with reward r, from state s

and action a，根据

状态 s 和

动作

a，使得状态转移成 s

′ 且

获得奖励 r 的概率

p(s

′

|s, a) probability of

transitioning to state s

′

,

from state s taking action a，根据状

态

s 和动

作 a，使得状态转移

成 s

′

的概率

vπ(s) value of state s

under policy π (expected return)，根据策略 π，状态

s

的价值（回

报期望）

v∗(s) value of state

s under the optimal policy，根据最

优策略，状态 s

的价值

qπ(s, a) value of taking

action a in state s under

policy π，根据

策略 π，在状态 s 时执行动

作

a

的价值

q∗(s, a) value of taking

action a in state s under

the optimal policy，根据最优策略，在

状态

s 时执行动作 a

的价值

V, Vt estimates of state-value function

vπ(s) or v∗(s)，状态价值函数的估计

Q, Qt estimates

of action-value function qπ(s, a) or

q∗(s, a)，动

作价值函数的估计

τ trajectory, which is

a sequence of states, actions and

rewards,

τ = (S0, A0, R0,

S1, A1, R1, · · ·)，状态

、动作、奖励的轨迹

γ reward discount factor, γ ∈

[0, 1]，奖励折

扣因子

ϵ probability of taking

a random action in ϵ-greedy policy，根据

ϵ-贪婪策略，执

行随

机动作的概率

α, β step-size parameters，步长

λ

decay-rate parameter for eligibility traces，资格迹的衰减速率

强化

学习中术语总结

除了在

本书开头的数学符号法

则中定义的术语，强化学

习中常见内容的相关术

语总结如下：

R 是奖励函数

，Rt = R(St) 是

MRP 中状态 St 的奖励，Rt = R(St,

At) 是 MDP 中的

奖励，

St ∈

S。

R(τ ) 是轨迹 τ 的

γ-折扣化回

报，R(τ ) = P∞

t=0 γ

tRt。

XIX

数学符号

p(τ ) 是轨迹的概

率：

– p(τ ) = ρ0(S0)

QT

−1

t=0 p(St+1|St) 对于 MP 和

MRP，ρ0(S0) 是起始状态分

布（Start-State

Distribution）。

– p(τ |π)

= ρ0(S0)

QT −1

t=0 p(St+1|St,

At)π(At|St) 对于 MDP，ρ0(S0) 是起始状态分布

。

J(π) 是策略

π 的期望回报，J(π) = R

τ

p(τ

|π)R(τ ) = Eτ∼π[R(τ )]。

π

∗ 是最

优策略：π

∗ = arg maxπ

J(π)。

vπ(s) 是状态 s 在策略 π

下

的价值（期望回报）。

v∗(s) 是状态

s 在最优策略下的价值（期

望回报）。

qπ(s, a)

是状态 s 在策略 π 下

采取动作 a

的价值（期望回

报）。

q∗(s, a) 是状态 s 在最优策略下

采取动作

a 的价值（期望回

报）。

V (s) 是对 MRP

中从状态 s 开始的

状态价值的估计。

V

π

(s)

是对 MDP 中

在线状态价值函数的估

计，给定策略 π，有期望回报

：

– V

π

(s) ≈ vπ(s) = Eτ∼π[R(τ

)|S0 = s]

Qπ

(s, a)

是对 MDP 下在线动作价值函

数的估计，给定策略 π，有期

望回报：

– Qπ

(s, a) ≈ qπ(s, a) =

Eτ∼π[R(τ )|S0 = s, A0 =

a]

V

∗

(s) 是对 MDP

下最优动作

价值函数的估计，根据最

优策略，有期望回报：

– V

∗

(s) ≈

v∗(s) = maxπ Eτ∼π[R(τ )|S0 =

s]

Q∗

(s, a) 是对

MDP 下最优动作价值函数的

估计，根据最优策略，有期

望回报：

– Q∗

(s, a) ≈ q∗(s,

a) = maxπ Eτ∼π[R(τ )|S0 =

s, A0 = a]

Aπ

(s,

a) 是对状态 s 和动作

a 的优势估计函数：

–

Aπ

(s, a) = Qπ

(s,

a) − V

π

(s)

在线状

态价值函数

vπ(s) 和在线动作

价值函数 qπ(s, a) 的关系：

–

vπ(s) = Ea∼π[qπ(s, a)]

最优状

态价值函数 v∗(s)

和最优动作

价值函数 q∗(s, a) 的关系：

– v∗(s)

= maxa q∗(s, a)

a∗(s) 是状态

s

下根据最优动作价值函

数得到的最优动作：

– a∗(s) = arg maxa

q∗(s, a)

对于

在线状态价值函数的贝

尔曼方程：

– vπ(s) =

Ea∼π(·|s),s′∼p(·|s,a)

[R(s, a) + γvπ(s

′

)]

对于在线动作

价值函数的贝尔曼方程

：

– qπ(s, a) =

Es

′∼p(·|s,a)

[R(s, a) + γEa′∼π(·|s

′)

[qπ(s

′

, a′

)]]

XX

数学符号

对于最优状态

价值函数的贝尔曼方程

：

– v∗(s) =

maxa Es

′∼p(·|s,a)

[R(s, a) +

γv∗(s

′

)]

对于最优动作价值函数

的贝尔曼方程：

– q∗(s,

a) = Es

′∼p(·|s,a)

[R(s, a)

+ γ maxa′ q∗(s

′

,

a′

)]

XXI

目录

基础

部分 1

第 1 章 深度学习入门

2

1.1 简介

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 2

1.2 感知器

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 3

1.3 多层感知器

. . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 7

1.4 激活函数 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 9

1.5 损失函数 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 11

1.6 优化

. .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 13

1.6.1 梯度下降和误差的反向

传播 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 13

1.6.2 随机梯度下降和自

适应学习率 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

15

1.6.3 超参数筛选

. . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 17

1.7 正则化 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

18

1.7.1 过拟合 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

18

1.7.2 权重衰减

. . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 18

1.7.3

Dropout . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 20

1.7.4

批标准化 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 20

1.7.5 其他缓和过拟

合的方法

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

21

1.8 卷积神经网络

. . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 22

1.9 循环神经网络 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 25

1.10 深度学习

的实现样例 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 28

1.10.1 张量和梯度

. . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 28

1.10.2 定义模型

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 29

1.10.3 自定义层 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 31

1.10.4 多层

感知器：MNIST 数据集上的图像

分类 .

. . . . . .

. . . . . .

. . . . . .

33

XXII

目录

1.10.5 卷积神经网络

：CIFAR-10 数据集上的图像分类

. . . . . .

. . . . . .

. . . . 35

1.10.6

序

列到序列模型：聊天机器

人 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 36

第 2

章 强化学习入门 43

2.1 简

介 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 43

2.2 在线预测和在线学习

. .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 46

2.2.1 简介

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 46

2.2.2

随机多臂赌博机 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 48

2.2.3 对

抗多臂赌博机 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

50

2.2.4 上下文赌

博机 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 51

2.3 马尔可夫过程 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 52

2.3.1 简介

. .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 52

2.3.2 马尔可夫奖励过程 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 54

2.3.3 马尔

可夫决策过程 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

57

2.3.4 贝尔曼方

程和最优性 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 61

2.3.5

其他重要概

念 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

64

2.4 动态规划 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 64

2.4.1 策略迭代

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 65

2.4.2 价

值迭代 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 67

2.4.3 其他 DPs：异步 DP、近似

DP 和

实时 DP . . .

. . . . . .

. . . . . .

. . . . 68

2.5

蒙特卡罗 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 70

2.5.1 蒙特卡罗

预测 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 70

2.5.2 蒙特卡罗控制

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 71

2.5.3 增量

蒙特卡罗 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 72

2.6 时间差分学习

. . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

73

2.6.1 时间差分预测 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 73

2.6.2 Sarsa：在线策略

TD 控制

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 77

2.6.3 Q-Learning：离线策略 TD 控制 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 80

2.7 策略

优化 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 80

2.7.1 简介 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 80

2.7.2 基于价值的优

化

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 84

2.7.3

基于策略的优化 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 89

2.7.4 结合

基于策略和基于价值的

方法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . 105

XXIII

目录

第

3 章 强化学习

算法分类 110

3.1 基于模型的方

法和无模型的方法

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 111

3.2 基于

价值的方法和基于策略

的方法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 113

3.3 蒙特卡罗方法和

时间差分方法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 114

3.4 在线策略

方法和离线策略方法 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 115

第

4 章 深度 Q 网络 119

4.1 Sarsa 和 Q-Learning . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 121

4.2

为什么使

用深度学习: 价值函数逼

近 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 121

4.3 DQN . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 123

4.4 Double DQN . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 124

4.5 Dueling DQN .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 125

4.6 优先经验回放

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 127

4.7 其他改

进内容：多步学习、噪声网

络和值分布强化学习

. . . . . .

. . . . . .

. . . 128

4.8 DQN

代

码实例 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 131

第

5 章 策略梯度 146

5.1 简

介

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 146

5.2 REINFORCE：初版策略梯度

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 147

5.3 Actor-Critic . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 149

5.4 生成对

抗网络和

Actor-Critic . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 150

5.5 同步优势 Actor-Critic

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 152

5.6 异步

优势 Actor-Critic . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 153

5.7 信赖域策略优化 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 154

5.8 近

端策略优化 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 157

5.9 使用 Kronecker 因子化

信赖域的

Actor-Critic . . . . .

. . . . . .

. . . . . .

. . . . . 159

5.10 策略梯度代码

例子 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 162

5.10.1 相关的

Gym 环境 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 162

5.10.2 REINFORCE: Atari Pong 和 CartPole-V0

. . . . . .

. . . . . .

. . . . . .

. . . 165

5.10.3 AC:

CartPole-V0 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 173

5.10.4 A3C: BipedalWalker-v2 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 176

5.10.5 TRPO:

Pendulum-V0 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 181

5.10.6 PPO:

Pendulum-V0 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 192

XXIV

目录

第 6 章 深度 Q 网络和

Actor-Critic 的结合

200

6.1 简介 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 200

6.2 深度确定性策略梯

度算法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 201

6.3 孪生延迟 DDPG 算法

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 203

6.4 柔

性 Actor-Critic 算法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 206

6.4.1 柔性策略迭代

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 206

6.4.2 SAC . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 207

6.5 代

码例子 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 209

6.5.1

相关的 Gym 环境 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

209

6.5.2 DDPG: Pendulum-V0 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 209

6.5.3 TD3: Pendulum-V0 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

215

6.5.4 SAC: Pendulum-v0 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 225

研究

部分 236

第 7

章 深度强化学习

的挑战 237

7.1 样本效率 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 237

7.2 学习稳

定性 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 240

7.3 灾难性遗忘 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 242

7.4 探索

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 243

7.5 元

学习和表征学习

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 245

7.6 多智能

体强化学习 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 246

7.7 模拟到现实

. . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

247

7.8 大规模强化学习 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 251

7.9 其他挑

战 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 252

第 8 章

模仿学习 258

8.1 简介 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 258

8.2 行

为克隆方法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 260

8.2.1 行为克隆方

法的挑战 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 260

8.2.2 数据集聚合 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 261

8.2.3 Variational Dropout

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 262

8.2.4 行

为克隆的其他方法 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

262

8.3 逆向

强化学习方法 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 263

8.3.1

简介 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 263

8.3.2 逆向

强化学习方法的挑战 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

264

XXV

目

录

8.3.3 生成对抗模仿学习 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 265

8.3.4 生

成对抗网络指导性代价

学习 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

266

8.3.5 对抗性逆向强化学

习 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 268

8.4

从观察量进行模仿学

习 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 269

8.4.1 基于模型方法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 269

8.4.2 无模型

方法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 272

8.4.3

从观察量模仿学习

的挑战 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 277

8.5 概率性方法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 277

8.6 模仿

学习作为强化学习的初

始化 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 279

8.7 强化学习中利用示

范数据的其他方法

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 280

8.7.1 将示

范数据导入经验回放缓

存 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 280

8.7.2 标准化 Actor-Critic .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 281

8.7.3 用示范数据进

行奖励塑形 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 282

8.8 总结 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 282

第 9 章

集

成学习与规划 289

9.1 简介 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 289

9.2 基于

模型的方法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

290

9.3 集成模式架

构 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

292

9.4 基于模拟的搜索 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 293

9.4.1 朴素

蒙特卡罗搜索 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 294

9.4.2

蒙特卡罗

树搜索 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 294

9.4.3 时间差分搜索 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 295

第

10 章 分层强化学习 298

10.1 简介 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 298

10.2 选

项框架 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 299

10.2.1 战略专注作家 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 300

10.2.2

选

项-批判者结构 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 303

10.3

封建制强

化学习 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 305

10.3.1 封建制网络

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

305

10.3.2 离线

策略修正 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 307

10.4 其他工作 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 309

XXVI

目录

第 11 章 多智能体强化学习

315

11.1

简介 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 315

11.2

优化和均衡 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

316

11.2.1 纳什均

衡 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 317

11.2.2 关联性均衡 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 318

11.2.3 斯塔克尔

伯格博弈 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 320

11.3 竞争与合作 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 321

11.3.1 合

作

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 321

11.3.2

零和博弈 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 321

11.3.3 同时决策下

的竞争

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 322

11.3.4 顺序决策下的竞

争 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 323

11.4 博弈分析架构 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 324

第 12 章 并

行计算

326

12.1 简介 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

326

12.2 同步和异步

. . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 327

12.3 并行计算网络 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 329

12.4 分布式强

化学习算法 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 330

12.4.1 异步优势

Actor-Critic . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 330

12.4.2 GPU/CPU 混

合式异步优势

Actor-Critic . . . . .

. . . . . .

. . . . . .

. . . . 332

12.4.3

分布式近

端策略优化 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 333

12.4.4 重要性加权

的行动者-学习者结构和

可扩展高效深度强化学

习 .

. . . . . .

. 336

12.4.5 Ape-X、回溯-行动者和分布式

深度循环回放 Q 网络

. . . . . .

. . . . . .

. 338

12.4.6 Gorila . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 340

12.5 分布

式计算架构 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 340

应用部分 343

第

13 章

Learning to Run 344

13.1 NeurIPS

2017 挑战：Learning to Run . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 344

13.1.1 环境介绍 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

344

13.1.2 安装 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 346

13.2 训

练智能体 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 347

13.2.1 并行训练

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 348

13.2.2 小技

巧 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 351

13.2.3 学习结果 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 352

XXVII

目录

第 14

章 鲁

棒的图像增强 354

14.1 图像增强

. .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 354

14.2

用于鲁棒处理的强化学

习 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 356

第

15 章 AlphaZero 366

15.1 简介

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 366

15.2 组合博弈

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

367

15.3 蒙

特卡罗树搜索 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 370

15.4 AlphaZero：棋类游戏

的通用算法 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

376

第 16 章 模拟环

境中机器人学习 388

16.1 机器人

模拟 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 389

16.2 强化学习用于机器

人学习任务 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 405

16.2.1

并行训练 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 407

16.2.2 学

习效果

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 407

16.2.3 域随机化 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 408

16.2.4 机器人

学习基准 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 409

16.2.5 其他模拟器 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 409

第

17 章 Arena：多智能体强化学习平

台 412

17.1 安装

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 413

17.2 用

Arena 开发游戏 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 413

17.2.1 简单

的单玩家游戏

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 414

17.2.2 简单的使

用奖励机制的双玩家游

戏 .

. . . . . .

. . . . . .

. . . . . .

. . . . 416

17.2.3

高级设置 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 420

17.2.4 导出二进制

游戏

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 424

17.3

MARL 训练 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

427

17.3.1 设置 X-Server . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 427

17.3.2 进行训练

.

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 429

17.3.3 可视化 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 431

17.3.4 致谢 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 431

第 18 章

深度强

化学习应用实践技巧 433

18.1 概

览：如何应用深度强化学

习 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 433

18.2

实现阶段 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. 434

18.3 训练和调试

阶段 . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

440

XXVIII

目录

总结部分 445

附录

A

算法总结表 446

附录 B 算法速

查表 451

B.1 深度学习 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 451

B.1.1 随机梯度

下降 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 451

B.1.2

Adam 优化器 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 452

B.2 强化学习 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 452

B.2.1 赌

博机 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 452

B.2.2 动态规划 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 453

B.2.3

蒙特卡罗

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 454

B.3 深度强化学习 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . 458

B.4 高等深度

强化学习 .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

467

B.4.1 模仿学习 . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 467

B.4.2 基于

模型的强化学习 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 468

B.4.3 分层强

化学习

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . 470

B.4.4 多智能体强化学

习 . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . 471

B.4.5

并行计算 . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . 472

附录 C

中英文

对照表 476

XXIX



基础部分

本书第

一部分包括

6 个章节，介绍

深度学习、强化学习及广

泛应用的深度强化学习

算法及其

实现。具体来说

，前两章介绍深度学习和

强化学习的基本概念，以

及少量深度强化学习的

基本知

识，这些内容对读

者阅读后续章节非常重

要。如果您已经掌握了这

些基本知识，完全可以跳

过这

两个章节。但我们还

是建议您阅读第 2

章，这有

助于熟悉本书的术语和

数学公式。

第 3 章介绍了强

化学习算法的分类，以帮

助大家从不同的角度来

对深度强化学习算法有

全

局的认识。分类包括基

于模型的（Model-Based）与无模型的（Model-Bree）方

法、基于策略的

（Policy-Based）与基于价

值的（Value-Based）方法、蒙特卡罗（Monte

Carlo，MC）与时

间差分

（Temporal Difference，TD）方法、在线策略（On-Policy）与

离线策略（Off-Policy）方法，等等。

如果

读者在阅读本书其他章

节时，对算法的分类与属

性有困惑，可回到第 3 章仔

细思考。我们会

在第 4～6 章详

细介绍一些常见的深度

强化学习算法，通过实例

代码帮助大家深入理解

算法的细

节和实现技巧

。

1

1

深度学习入门

深度学习

是深度强化学习的重要

构成部分。本章将首先简

要介绍深度学习的基础

知识，会从

简单的单层神

经网络开始，逐渐引入更

加复杂且学习能力更强

的神经网络模型，比如卷

积神经网

络和循环神经

网络的模型。本章在最后

将提供一些代码样例，用

于介绍深度学习的实现

过程。

1.1 简介

如果您已经非

常熟悉深度学习，则可以

从第 2 章开始阅读。如果您

想对深度学习中的部分

内容进行深入的学习和

了解，推荐您参阅其他相

关图书，例如 Pattern Recognition and

Machine

Learning (Bishop, 2006) 和 Deep

Learning (Goodfellow et al., 2016)。与经典强

化学习不同的是，深

度强

化学习是基于深度学习

模型，即深度神经网络，来

利用大数据和高性能计

算强大优势的。我

们可以

大致将深度学习模型分

为以下两大类。

判别模型

用于建模条件概率 p(y|x)，其中

x 代表输入数据，而 y 代表输

出目标。也就是说，

判别模

型基于输入数据 x，预测相

对应的标签 y。顾名思义，判

别模型大多应用于需要

进行判断

的任务，例如，分

类任务和回归任务。具体

来说，在分类任务中，模型

需要根据输入数据从备

选

类别中选择正确的目

标类别。如果一个任务中

仅有两个备选类别且模

型只需要从中选取一个

正确

的目标类别，则为二

分类任务，是最为基本的

分类任务。例如，在情感分

析中

(Maas et al., 2011)，

根据文本内容，判断

文本表达了正面的情绪

还是负面的情绪，即二分

类任务。与之相对应的，在

多标签分类任务中，备选

类别中可能同时有多个

正确的目标类别。

在很多

情况下，一个分类模型并

不直接指定目标类别，而

会给每一个备选类别计

算一个概率。

例如，模型根

据某个数据样例，认为它

有 80% 的概率来自类别 A，而另

有 15% 的概率来自类别

B，5% 的概

率来自类别 C。之所以使用

这种基于概率的表征，主

要是为了便于在训练阶

段对模型

2

1.2 感知器

进行优

化。深度学习已经在很多

像图像分类 (Krizhevsky et al., 2009) 和文本分类

(Yang

et al., 2019)

的分类任务上取得了巨

大的成功。

分类任务的输

出均为离散的类别标签

，而回归任务则不同。回归

任务的输出是连续的数

值，

如利用过去的交通数

据来预测未来一段时间

内的车速

(Liao et al., 2018a,b)。只要回归模型

是基于

条件概率建模的

，我们就认为它是判别模

型。

生成模型用于建模联

合概率

p(x, y)。生成模型通常对

可观测数据的分布进行

建模，从而达

到生成可观

测数据的目的。生成对抗

网络（Generative Adversarial Networks，GANs）(Goodfellow

et

al., 2014) 就是这样一个例子

，它被用于生成图像、重构

图像和对图像去噪。然而

，类似于 GANs

的深度学习技术

与可观测数据的分布并

没有显式的关系，因为深

度学习技术更关注生成

的样本和

可观测的真实

样本之间的相似程度。与

此同时，像朴素贝叶斯（Naive

Bayes）的

生成模型也用于

解决分

类任务 (Ng et al., 2002;

Rish et al., 2001)。尽管生成模型和

判别模型都可以用于解

决分类

任务，判别模型关

注的是哪一个标签更适

合可观测数据，而生成模

型则尝试建模可观测数

据的分

布。下面举两个例

子来说明它们的不同。朴

素贝叶斯对似然概率（Likelihood）p(x|y)

建

模，也

就是可观测数据在

给定标签情况下的条件

概率。生成模型先学会创

造数据，再去学习如何判

别数

据，当学习了联合概

率分布 p(x, y) 后即可以学会判

别，比如给定观测输入

x，输

出目标为 1 的概

率为 p(y =

1|x) = p(x,y=1)

p(x) 。

大多

数深度神经网络都是判

别模型，无论其目的是用

于判别类任务还是生成

类任务。这是因

为很多生

成类任务在具体实现中

都可以简化为分类或者

回归问题。例如，问答系统

(Devlin et al.,

2019) 可以简化为根据问题选

择文本中相应的段落；自

动摘要 (Zhang

et al., 2019b) 可以简化为从

词

表中根据概率选择单词

，并组合成摘要。在这两种

场景下，它们都在尝试生

成文本，但是一个

使用了

分类的方法，另一个则使

用了回归的方法。

具体来

说，本章将介绍深度学习

相关的基本元素和技术

，例如构造深度神经网络

必需的神经

元、激活函数

和优化器等，同时将介绍

深度学习相关的应用。本

章也将介绍基础的深度

神经网络，

例如多层感知

器（Multilayer Perceptron，MLP）、卷积神经网络（Convolutional Neural Networks，

CNNs），以及循

环神经网络（Recurrent Neural Networks，RNNs）。最后，1.10 节将基

于 TensorFlow

和

TensorLayer 介绍深度神经网络

的实现样例。

1.2 感知器

单输

出

神经元或节点是深度

神经网络最基本的单元

。神经元的概念最初是基

于大脑中生物神经元提

出的，也是生物神经元的

一种抽象表示。在大脑中

，生物神经元通过树突接

受电信号，当生物神

经元

被激活后，通过轴突将电

信号传播给其他附近的

生物神经元。在真实的生

物系统中，神经元

的信息

传递并不是在一瞬间发

生的，而是需要经过一步

一步传递的过程，这个过

程可以形象地理

3

第 1 章

深

度学习入门

解成激活一

个神经网络。当前，深度学

习的研究更多地依赖深

度神经网络（Deep Neural Networks，

简称 DNNs），亦称人

造神经网络（Artificial

Neural Networks，简称 ANNs）。深度神

经网络中的神经

元的输

入和输出都是数值。一个

神经元可以跟下一层的

多个神经元同时相连，也

可以跟上一层的

多个神

经元同时相连。具体来说

，每个神经元将上一层神

经元的输出进行聚合，再

通过激活函数

决定其最

终的输出。如果这些聚集

的输入信号足够强，那么

这个激活函数将会“激活

”（Activate）

这个神经元，然后这个神

经元会将一个有高数值

的信号传递给下一层网

络。相对地，如果输入信

号

不够强，那么一个低数值

信号将被传递下去。

一个

神经网络可以有任意多

个神经元，而这些神经元

彼此可以有很多随机的

连接。但是为了

运算更加

容易，神经元往往是层层

递进的。一般来说，一个神

经网络至少会有两层：输

入层和输

出层（见图 1.1）。这个

网络可以被公式

(1.1) 描述，它

可以做一些简单的决定

任务，比如帮助几

个学生

根据天气的情况具体决

定他们是否外出踢足球

，网络输出的 z 是一个分数

，分数越高则代

表越可以

去踢足球。这个分数取决

于三个因素：1）足球场的使

用费用

x1；2）天气 x2；3）去球

场的时

间 x3。如果天气对大家做这

个决定比较重要，则其相

对应的网络权重 w2 会有较

大的绝对

值。同样地，那些

对做这个决定影响较小

的因素，所对应的网络权

重的绝对值就会较小。如

果一

个权重被设置为零

，那么它所对应的输入就

对最终的结果完全没有

影响。比如，有的学生有钱

，

不在乎足球场的费用，则

w1 为 0。我们把具有这样结构

的网络叫作单一层网络

，也叫作感知器

（Perceptron）。

z = w1x1 + w2x2 +

w3x3 (1.1)

图 1.1 有三个

输入神经元和一个输出

神经元的神经网络

偏差

与决策边界

偏差（Bias）是神经

元所附带的一个额外的

标量，用来偏移神经网络

的输出。图 1.2 所示的

一个有

偏差 b 的单层神经网络可

以用公式

(1.2) 表达：

4

1.2 感知器

z

= w1x1 + w2x2 + w3x3

+ b (1.2)

图

1.2 一个有偏差的单层神经

网络

偏差可以帮助一个

神经网络更好地学习数

据。我们不妨定义以下二

分类问题：对于输出

z，

当且

仅当 z 为正数，其所对应的

标签 y 为

1，反之为 0：

y =







1 z > 0

0

z ⩽ 0

(1.3)

二分类任

务的样本数据分布例子

如图 1.3

所示。我们现在需要

找到最符合这些数据的

权重和

偏差。我们把这些

样本数据分成两个不同

的类别的边界定义为决

策边界。正式来说，这个边

界是

{x1, x2, x3|w1x1 +

w2x2 + w3x3 + b =

0}。

我们首先把这个问

题简化到只有两个输入

的情况下，即 z = w1x1 +

w2x2 + b。如图 1.3 左

所示

，如果没有偏差值，也就是

说

b = 0，那么决策边界必须穿

过坐标系的原点（左下的

线）。但

是，这样很明显不符

合数据的分布，因为我们

的数据点都是在这个边

界的一侧。如果偏差值不

是

0，那么决策边界与两个

轴的交点就为 (0,

−

b

w2

) 和 (−

b

w1

, 0)。这样来

看，如果我们的权重和偏

差

值选得好，那么决策边

界就能更好地符合数据

分布。

进一步来说，当一个

神经元有三个输入的时

候，z

= w1x1 + w2x2 + w3x3

+ b，此时的边界

就会变成

如图 1.3 右所示的平面。在一

个如单层神经网络（见公

式 (1.2)）的线性模型中，这样的

一个平面也被称为超平

面（Hyperplane）。

5

第 1 章 深度学习入门

O

O

图

1.3 线性模型分别在两个输

入和三个输入场景下的

决策边界。左：z = w1x1 +

w2x2 + b。右：

z = w1x1

+ w2x2 + w3x3 + b。若没有偏

差，则决策边界必须经过

原点，不能很好地分类

多

输出

单层神经网络可以

有多个神经元。图 1.4 展示了

一个有两个输出神经元

的单层网络，由公

式 (1.4)

所得

。因为每一个输出都和全

部输入相连，所以输出层

也被称为密集层（Dense Layer）或

者全

连接层（Fully-Connected (FC) Layer）：

z1

= w11x1 + w12x2 + w13x3

+ b1

z2 = w21x1 +

w22x2 + w23x3 + b2 (1.4)

𝑥#

		𝑥



$

		𝑥%

z1

		𝑤##

		𝑤$



#

		𝑤



$%

z2

		𝑤#$



𝑤#%

		𝑤



$$



𝑏#

		𝑏



$

图 1.4 一个有三个输入和两个输出的神经元的神经网络

6

1.3 多层感知器

在实践中，全连接层也可以被矩阵乘法实现：

z = W x + b (1.5)

式中，W ∈ R

m×n 是用来表示权重的矩阵，z ∈ R

m, x ∈ R

n, b ∈ R

m 分别用来表示输出、输

入和偏差的向量。在公式 (1.5) 里的例子中，m = 2，n = 3，即 W ∈ R

2×3。







z1

z2





 =







w11 w12 w13

w21 w22 w23





















x1

x2

x3















+







b1

b2





 (1.6)

1.3 多层感知器

多层感知器（Multi-Layer Perceptron，MLP）(Rosenblatt, 1958; Ruck et al., 1990) 最初指至少有

两个全连接层的网络。图 1.5 展现了一个有四个全连接层的多层感知器。那些在输入层和输出层

中间的网络层被隐藏（Hidden）了，因为一般来说从网络外面是没有办法直接接触它们的，所以

被统称为隐藏层（Hidden Layers）。相比只有一个全连接层的网络，MLP 可以从更复杂的数据中

学习。从另外一个角度来看，MLP 的学习能力是大于单一层网络的学习能力的。但是拥有更多的

隐藏层并不意味着一个网络会有更强的学习能力。通用近似定理说的是：一个有一层隐藏层的神

经网络（类似于有一层隐藏层的 MLP）和任何可挤压的激活函数（见后文的 sigmoid 和 tanh）在

这一层网络有足够多神经元的情况下，可以估算出任何博莱尔可测函数 (Goodfellow et al., 2016;

Hornik et al., 1989; Samuel, 1959)。但是实际上，这样的网络可能会非常难以训练或者容易过拟合

（Overfit）（见后文）。因为隐藏层非常大，所以一般的深度神经网络都会有几层隐藏层来降低训

练难度。

为什么需要多层网络？为了回答这个问题，我们首先通过逻辑运算的几个例子来展示一个网

络是怎么估算一个方程的。我们会考虑的逻辑运算有：与（AND）、或（OR）、同或（XNOR）、

异或（XOR）、或非（NOR）、与非（NAND）。这些运算输入都是两个二进制数字，然后输出为 1

或者 0。如与（AND），只有两个输入同时为 1，AND 才会输出 1。这些简单的逻辑计算可以很容

易就被感知器学习，就像公式 (1.7) 里展现的那样。

f(x) =







1 如果 z > 0

0 其他情况

, z = w1x1 + w2x2 + b (1.7)

图 1.6 展示了被感知器定义的决策边界可以很轻松地把 AND、OR、NOR 和 NAND 运算的 0

7

第 1 章 深度学习入门

和 1 分开出来，但是，XOR 或 XNOR 的决策边界是不可能被找到的。

图 1.5 一个具有三个隐藏层和一个输出层的多层感知器。图中使用 a

l

i 表示神经元，其中 l 代表

层的索引，i 代表输出的索引

1 1 2 2

O O

O O O O

图 1.6 左上：有两个输入和一个输出的感知器。剩下的是：不同的用来把 0（×）和 1（•）分开

的决策边界。在这个单层感知器中，能找到 AND、OR、NOR 和 NAND 的决策边界，但找

不到可以实现 XOR 和 XNOR 的决策边界

因为我们不能用一个线性模型像单个感知器那样直接估算 XOR，所以必须要转化输入。图 1.7

展现了一个用有一层隐藏层的 MLP 去估算 XOR，这个 MLP 首先将通过估计 OR 和 NAND 运算

把 x1, x2 转换到了一个新的空间，然后在这个转换过的空间里，这些点就可以被一条估算 AND

的平面分开了。这个被转换过后的空间也被称为特征空间。这个例子说明了怎么通过特征的学习

来改善一个模型的学习能力。

8

1.4 激活函数

O O

图 1.7 左：一个可以估算 XOR 的 MLP。中和右：把原始数据点转化到特征空间，从而使得这些

数据点变得线性可分离

1.4 激活函数

矩阵的加减和乘除运算都是线性运算符，但是一个线性模型的学习能力还是相对有限的。举

例来说，线性模型不能轻易地估算一个余弦函数。因为大多数深度神经网络解决的真实问题都不

可能被简单地映射到一个线性转换，所以非线性在深度神经网络里至关重要。

实际上，深度学习网络的非线性是通过激活函数来介入的。这些激活函数都是针对每一个元

素（Element-Wise）运算的。我们需要这些激活函数来帮助模型获得有任意数值的概率向量。激

活函数的选择要根据具体的运用场景来考虑。虽然有一些激活函数在大多数的情况下效果都是不

错的，但是在具体的实际运用中，可能还有更好的选择。所以激活函数的设计至今都还是一个活

跃的研究方向。本节主要介绍四种非常常见的激活函数：sigmoid、tanh、ReLU 和 softmax。

逻辑函数 sigmoid 在作为激活函数时，将输入控制在了 0 和 1 之间，如公式 (1.8) 所示。sigmoid

方程可以在网络的最后一层，使用来做一些分类的任务，以代表 0%～100% 的概率。比如说，一

个二维的分类器可以把 sigmoid 方程放在最后一层，来把其数值局限在 0 和 1 之中，然后我们可

以用一个简单的临界值决定最终输出的标签是什么（0 或 1）。

f(z) = 1

1 + e−z

(1.8)

与 sigmoid 函数类似的是，hyperbolic tangent （tanh）把输出值控制到了 −1 和 1 之间，就

如公式 (1.9) 所定义那样。tanh 函数可以在隐藏层中使用来提高非线性 (Glorot et al., 2011)。它也

可以在输出层中使用，比如网络可以输出像素数值在 −1 和 1 的图像。

f(z) = e

z − e

−z

e

z + e−z

(1.9)

9

第 1 章 深度学习入门

图 1.8 展现三个元素单位运用的方程：sigmoid、tanh 和 ReLU。sigmoid 把数值限制在了 0 和 1 之

间，而 tanh 则把数值限制在了 −1 和 1 之间。当输入是负数时，ReLU 则输出 0，但当输入

是正数时，其输出等于输入

在公式 (1.10) 中，我们定义了整流线性单元（Rectified Linear Unit，ReLU）函数，也叫作

rectifier。ReLU 被广泛地使用于不同的研究当中 (Cao et al., 2017; He et al., 2016; Noh et al., 2015)，

在很多层的网络中 ReLU 通常会比 sigmoid 和 tanh 性能更好 (Glorot et al., 2011)。

f(z) =







0 当 z ⩽ 0

z 当 z > 0

(1.10)

在实际运用中，ReLU 有以下优势。

• 更易实现和计算：在实现 ReLU 的过程中，首先我们只需要把其数值和 0 做对比，然后根

据结果来设定输出是 0 还是 z。而我们在实现 sigmoid 和 tanh 的过程当中，指数函数在大型

网络中会更难以计算。

• 网络更好优化：ReLU 接近于线性，因为它是由两个线性函数组成的。这种性质就使得它更

容易被优化，我们在本章后面讲解优化细节时再讨论。

然而 ReLU 把负数变成 0，可能会导致输出中信息的丧失。这可能是因为一个不合适的学习

速率或者负的偏差而导致的。带泄漏的（Leaky）ReLU 则解决了这个问题 (Xu et al., 2015)。我们

在公式 (1.11) 中对它进行了定义。标量 α 是一个较小的正数来控制斜率，使得来自负区间的信息

也可以被保留下来。

f(z) =







αz 当 z ⩽ 0

z 当 z > 0

(1.11)

10

1.5 损失函数

有参数的 ReLU（PReLU）(He et al., 2015) 和 Leaky ReLU 很近似，它把 α 看作一个可以训练

的参数。目前我们还没有具体的证据表明 ReLU、Leaky ReLU 或 PReLU 哪个是最好的，它们在

不同应用中往往有不同的效果。

不像上述的其他激活函数，在公式 (1.12) 中定义的 softmax 函数会根据前一层网络的输出提

供归一化。softmax 函数首先计算指数函数 e

z，然后每一项都除以这个值进行归一。

f(z)i =

e

zi

PK

k=1 e

zk

(1.12)

在实际运用当中，softmax 函数只在最后的输出层用来归一输出向量 z，使其变成一个概率

向量。这个概率向量的每一个值都为非负数，然后它们的和最终会为 1。所以，softmax 函数在多

分类任务中被广泛使用，用以输出不同类别的概率。

1.5 损失函数

到目前为止，我们了解了神经网络结构的基础知识，那么网络的参数是怎么自动学习出来的

呢？这需要损失函数（Loss Function）来引导。具体来说，损失函数通常被定义为一种计算误差

的量化方式，也就是计算网络输出的预测值和目标值之间的损失值或者代价大小。损失值被用来

作为优化神经网络参数的目标，我们优化的参数包括权重和偏差等。在本节里，我们会介绍一些

基本的损失函数，1.6 节会介绍如何使用损失函数优化网络参数。

交叉熵损失

在介绍交叉熵损失之前，首先来看一个类似的概念：Kullback-Leibler (KL) 散度，其作用是衡

量两个分布 P(x) 和 Q(x) 的相似度：

DKL(P∥Q) = Ex∼P



log P(x)

Q(x)



= Ex∼P [log P(x) − log Q(x)] (1.13)

KL 散度是一个非负的指标，并且只有在 P 和 Q 两个分布一样时才取值为 0。因为 KL 散度

的第一个项和 Q 没有关系，我们引入交叉熵的概念并把公式的第一项移除。

H(P, Q) = −Ex∼P log Q(x) (1.14)

因此，通过 Q 来最小化交叉熵就等同于最小化 KL 散度。在多类别分类任务中，深度神经网

络通过 softmax 函数输出的是不同类别概率的分布，而不是直接输出一个样本属于的类别。所以，

我们可以用交叉熵来测量预测分布有多好，从而训练网络。

以一个二分类任务为例。在二分类中，每一个数据样本 xi 都有一个对应的标签 yi（0 或 1）。

11

第 1 章 深度学习入门

一个模型需要预测样本是 0 或者 1 的概率，用 yˆi,1，yˆi,2 来表示。因为 yˆi,1 + ˆyi,2 = 1，可以把它

们改写为 yˆi 和 1 − yˆi。前者可以代表一个类别的概率，后者可以代表另外一个类别的概率。因此，

一个二分类的神经网络可以只有一个输出，且最后一层使用 sigmoid。根据交叉熵的定义，我们

有：

L = −

1

N

X

N

i=1



yi

log yˆi + (1 − yi)log(1 − yˆi)



(1.15)

式中，N 代表了总数据样本的大小。因为 yi 是一个 1 或者 0 的值，因此在 yi

log yˆi 和

(1 − yi)log(1 − yˆi) 中，对于每一个新样本，两个表达式的值只有一个不为零。若 ∀i, yi = ˆyi，则

交叉熵就为 0。

在多类别分类任务中，每一个样本 xi 都会被分到 3 个或者更多的类别中的一个。这时，一

个模型需预测每一个类别的概率 {yˆi,1, yˆi,2, · · · , yˆi,M}，且符合条件 M ⩾ 3 和

PM

j=1 yˆi,j = 1。在

这里，每一个样本的目标写作 ci，它的值域为 [1, M]。同时，它也可以被转换成为一个独热编码

yi = [yi,1, yi,2, · · · , yi,M]，其中只有 yi,ci = 1，其他的都是 0。我们现在就可以把多类别分类的交

叉熵写成以下形式：

L = −

1

N

X

N

i=1

X

M

j=1

yi,j log yˆi,j = −

1

N

X

N

i=1

(0 + · · · + yi,ci

log yˆi,ci + · · · + 0)

= −

1

N

X

N

i=1

log yˆi,ci

(1.16)

Lp 范式

向量 x 的 p-范式用来测量其数值幅度大小：如果一个向量的值更大，它的 p-范式也会有一

个更大的值。p 是一个大于或等于 1 的值，p-范式定义为

∥x∥p =





X

N

i=1

|xi

|

p





1/p

i.e., ∥x∥

p

p =

X

N

i=1

|xi

|

p

(1.17)

p-范式在深度学习中往往用来测量两个向量的差别大小，写作 Lp，如在公式 (1.18) 一样，其

中 y 为目标值向量，yˆ 为预测值向量。

12

1.6 优化

Lp = ∥y − yˆ∥

p

p =

X

N

i=1

|yi − yˆi

|

p

(1.18)

均方误差

均方误差 (Mean Squared Error，MSE) 是由公式 (1.19) 所定义的 L2 范式的平均值。均方误

差可以在网络输出是连续值的回归问题中使用。比如说，两个不同图像在像素上的区别就可以用

MSE 来测量：

L =

1

N

∥y − yˆ∥

2

2 =

1

N

X

N

i=1

(yi − yˆi)

2

(1.19)

其中 N 是样本数据的大小，y 和 yˆ 分别为目标值向量和预测值向量。

平均绝对误差

与均方误差类似，平均绝对误差 (Mean Absolute Error，MAE) 也可以被用来做回归任务，它

被定义为 L1 范式的平均。

L =

1

N

X

N

i=1

|yi − yˆi

| (1.20)

均方误差和平均绝对误差都可衡量 y 和 yˆ 的误差，用以优化网络模型。其中，均方误差提

供了更好的数学性质，从而让我们能更简便地计算梯度下降所需要的偏导数。而在平均绝对误差

中，当 yi = ˆyi 时，我们注意到上面公式中的绝对值项无法求导，这对平均绝对误差来说是一个

无法解决且需要规避的问题。另外，当 yi 和 yˆi 的绝对差大于 1 时，均方误差相对平均绝对误差

来说误差值更大。显然地，当 (yi − yˆi) > 1 时，(yi − yˆi)

2 > |yi − yˆi

|。

1.6 优化

在这一小节里，我们将描述深度神经网络的优化，即深度神经网络参数训练。本节包含了反

向传播算法、梯度下降、随机梯度下降和超参数的选择等内容。

1.6.1 梯度下降和误差的反向传播

如果我们有一个神经网络和一个损失函数，那么对于这个网络的训练的意义是通过学习它的

θ 使得损失值 L 最小化。最暴力的方法是通过寻找一组参数 θ，使它满足 ▽θL = 0，以找到损失

值的最小值。但这种方法在实际中很难实现，因为通常深度神经网络参数很多、非常复杂。所以

13

第 1 章 深度学习入门

我们需要考虑一种叫作梯度下降（Gradient Descent）的方法，它是通过逐步优化来一步一步地寻

找更好的参数来降低损失值的。

图 1.9 展示了两个梯度下降的例子。梯度下降的学习过程从一个随机指定的参数开始，其损

失值 L 随参数的更新而逐步下降，其过程如箭头所示。具体来说，在神经网络中，参数通过偏导

数 ∂L

∂θ 被逐步优化，优化过程为 θ := θ − α

∂L

∂θ ，其中 α 为学习率，用以控制步长幅度。可见，梯

度下降法的关键是计算出偏导数 ∂L

∂θ 。

O O

图 1.9 梯度下降的示例：在左图中，我们有一个可以训练的参数 θ = w；在右图中，我们有两个

可以训练的参数 θ = [w1, w2]。在梯度下降里，整个学习过程的初始化参数是随机的。在

每一步对参数调整之后，损失 L 会慢慢地减少，但无法保证最后能找到全局最小的损失

值，在大多数情况下，我们能找到的都是局部最小值

反向传播（Back-Propagation）(LeCun et al., 2015; Rumelhart et al., 1986) 是一种计算神经网络

中偏导数 ∂L

∂θ 的方法。为了使得表示对 ∂L

∂θ 的计算更加清晰，这种方法引入一个中间量 δ =

∂L

∂z ，

用来表示损失函数 L 对于神经网络输出 z 的偏导数。因此，这种方法可以通过中间量 δ 来计算

损失函数 L 对于每个参数的偏导数，并最终共同组成 ∂L

∂θ 。

网络层的序号为 l = 1, 2, · · · , L，其中输出层的序号为 L。对于每个网络层，我们有输出 z

l，

中间值 δ

l =

∂L

∂zl 和一个激活值输出 a

l = f(z

l

) （其中 f 为激活函数）。下面是一个使用均方误

差和 sigmoid 激活函数的多层感知器的例子：已知 z

l = Wla

l−1 + b

l

, a

l = f(z

l

) = 1

1+e−zl 和

L =

1

2

∥y − a

L∥

2

2，可以得出激活值输出对于原先输出的偏导数 ∂a

l

∂zl = f

′

(z

l

) = f(z

l

)(1 − f(z

l

)) =

a

l

(1 − a

l

)，以及损失函数对于激活值输出的偏导数 ∂L

∂aL = (a

L − y)。然后，为了计算损失函数

对于输出层的偏导数，可以使用链式法则，具体如下：

从输出层开始向后传播误差，先计算输出层的中间量：

• δ

L =

∂L

∂zL =

∂L

∂aL

∂a

L

∂zL = (a

L − y) ⊙ (a

L(1 − a

L))

然后计算损失函数对于后一层输出的偏导数，如（l = 1, 2, · · · , L − 1）：

• 已知 z

l+1 = Wl+1a

l + b

l+1，则 ∂z

l+1

∂al = Wl+1；且 ∂a

l

∂zl = a

l

(1 − a

l

)

14

1.6 优化

• 那么 δ

l =

∂L

∂zl =

∂L

∂zl+1

∂z

l+1

∂al

∂a

l

∂zl = (Wl+1)

Tδ

l+1 ⊙ (a

l

(1 − a

l

))

从输出层开始向后传播，计算出所有层的中间值 δ

l 后，反向传播算法的第二步是在中间值

δ

l 的基础上计算损失函数对于每层参数 ∂L

∂Wl 和 ∂L

∂bl 的偏导数。

• 若有 z

l = Wla

l−1 + b

l，我们有 ∂z

l

∂Wl = a

l−1 和 ∂z

l

∂bl = 1

• 那么 ∂L

∂Wl =

∂L

∂zl

∂z

l

∂Wl = δ

l

(a

l−1

)

T， ∂L

∂bl =

∂L

∂zl

∂z

l

∂bl = δ

l

最后，我们用 ∂L

∂Wl 和 ∂L

∂bl 及梯度下降更新 Wl 和 b

l：

• Wl

:= Wl − α

∂L

∂Wl

• b

l

:= b

l − α

∂L

∂bl

可见，有了偏导数 ∂L

∂θ = [ ∂L

∂Wl

,

∂L

∂bl

]，我们可以使用梯度下降来对参数进行迭代，直到其收敛

到了损失函数中的一个最小值，如图 1.9 所示。在实践中，我们最终得到的最小值往往是一个局

部最小值，而不是全局最小值。但是，因为深度神经网络往往可以提供一个很强的表示能力，这

些局部最小值通常会很接近全局最小值 (Goodfellow et al., 2016)，使得损失值足够小。

这里额外介绍 sigmoid 的问题，当使用 sigmoid 时，∂a

l

∂zl = a

l

(1 − a

l

)，当 a 接近于 0 或者 1

时，∂a

l

∂zl 会非常小，从而导致 δ

l 非常小。在网络很深的情况下，反向传播时 δ 会越来越小，出现

梯度消失（Vanishing Gradient）问题，导致模型靠近输入部分的参数很难被更新，模型无法训练

起来。而 ReLU 的 ∂a

l

∂zl 在 a 大于 0 时衡为 1，就不会有这个问题，这也是现在的深度模型往往在

隐藏层中使用 ReLU 而不再使用 sigmoid 的原因。

在梯度下降中，如果数据集的大小（即数据样本的数量）N 较大，则在每个迭代中计算损失

函数 L 的计算开销可能会较高。拿之前的均方误差举例，我们可以把上式展开成

L =

1

2

∥y − a

L

∥

2

2 =

1

2

X

N

i=1

(yi − a

L

i

)

2

(1.21)

在实践中，数据集很有可能会很大，梯度下降因需要计算 L 而变得十分低效。随机梯度下降

应运而生，其他对于 L 的计算只包含少量的数据样本。

1.6.2 随机梯度下降和自适应学习率

与其是在每个迭代中对全部训练数据计算损失函数 L，随机梯度下降（Stochastic Gradient Descent, SGD）(Bottou et al., 2007) 计算损失值时随机选取一小部分的训练样本。这些小

样本被称为小批量 (Mini-batch)，而在这些小批量的具体大小被称为批大小 (Batch Size) B。然

后，我们就可以用批大小 B 和 B ≪ N 重写公式 (1.21)，得到公式 (1.22)，以改进计算 L 的

效率：

L =

1

2

∥y − a

L

∥

2

2 =

1

2

X

B

i=1

(yi − a

L

i

)

2

(1.22)

15

第 1 章 深度学习入门

随机梯度下降的训练过程请见算法 1.1。如果参数在算法 1.1 中更新了足够多的次数，那么

小批量可以覆盖整个训练集。

算法 1.1 随机梯度下降的训练过程

Input: 参数 θ，学习率 α，训练步数/迭代次数 S

1: for i = 0 to S do

2: 计算一个小批量的 L

3: 通过反向传播计算 ∂L

∂θ

4: ▽θ ← −α ·

∂L

∂θ

;

5: θ ← θ + ▽θ 更新参数

6: end for

7: return θ；返回训练好的参数

学习率 (Learning Rate) 控制了随机梯度下降中每次更新的步长。如果学习率过大，随机梯度

下降可能无法找到最小值，如图 1.10 所示。另一方面，如果学习率过小，随机梯度下降的收敛速

率将会变得十分缓慢。如何决定学习率是一个很困难的过程。为了解决这个问题，需要使用自适

应学习率算法，如 Adam (Kingma et al., 2014)、RMSProp (Tieleman et al., 2017) 和 Adagrad (Duchi

et al., 2011) 等。其作用为通过自动、自适应的方法来调整学习率，从而加速训练算法的收敛速度。

这些算法的原理在于，当参数收到了一个较小的梯度时，算法会转到一个更大的步长；反之，如

果梯度过大，算法就会给出一个较小的步长。其中，Adam 是最常见的自适应学习率算法。与其

直接用梯度更新参数，Adam 首先会计算梯度的滑动平均和二阶动量。然后，如算法 1.2 所示，这

些新计算的数值会被用来更新我们想要训练的参数。算法 1.2 中的 β1 和 β2 为梯度的遗忘因子，

或者分别是其动量和二阶动量。在默认设置下，β1 和 β2 的值分别是 0.9 和 0.999 (Kingma et al.,

2014)。

图 1.10 一个很大的学习率可能会加速训练过程，但会导致模型很难训练至一个理想的参数。如

左图所示，因为其学习率较右图更大，其损失函数有可能在参数更新后增加，因此更难

以接近最小值。同样地，右图的优化有一个更小的学习率，能更好地找到低点，但训练

速度较慢

16

1.6 优化

算法 1.2 Adam 优化器的训练过程

Input: 参数 θ，学习率 α，训练步数/迭代次数 S，β1 = 0.9，β2 = 0.999，ϵ = 10−8

1: m0 ← 0; 初始化一阶动量

2: v0 ← 0; 初始化二阶动量

3: for t = 1 to S do

4: ∂L

∂θ

; 用一个随机的小批量计算梯度

5: mt ← β1 · mt−1 + (1 − β1) ·

∂L

∂θ

; 更新一阶动量

6: vt ← β2 · vt−1 + (1 − β2) · (

∂L

∂θ

)

2

; 更新二阶动量

7: mˆt ← mt

1−β

t

1

; 计算一阶动量的滑动平均

8: vˆt ← vt

1−β

t

2

; 计算二阶动量的滑动平均

9: ▽θ ← −α · √mˆt

vˆt+ϵ

10: θ ← θ + ▽θ; 更新参数

11: end for

12: return θ; 返回训练好的参数

1.6.3 超参数筛选

在深度学习中，超参数（Hyper-Parameters）指和设置相关的参数，比如层的数量，以及训练

过程的设置参数，如更新步的数量、批大小和学习率。这些设置参数会在很大程度上影响模型的

表现，因此它们是组成一个理想模型的重要因素。

为了衡量不同超参数对于模型表现的影响，我们通常将数据集划分为训练集（Training Set）、

验证集（Validation Set）和测试集（Testing Set）。不同的超参数设置分别用训练集训练出不同的

模型，然后在验证集上进行性能评估。最后，我们用在验证集上表现最好的超参数在测试集上做

最后的性能评估。在这里需要注意的是，我们不能用测试集调整超参数，不然就是已知考卷试题

的作弊行为。

交叉验证

在一个小数据集上，把数据集分为训练集、验证集和测试集的做法会浪费宝贵的数据。具体

来说，如果训练集分得过小，可能会因为训练数据不足而让训练出来的模型表现不佳。从另一方

面来说，如果训练集分得过多、验证集过小，模型也不能在一个小数据集上被充分地评估。为了

解决这个问题，可使用交叉验证（Cross Validation），所有数据都能被用来训练模型，不再需要验

证集，以充分利用数据。

在一个 k 折交叉验证策略中，一个数据集将会被分成 k 个互相不重复的子集，并且每个子集

包含同样数量的数据。我们将重复训练模型 k 次，其中每次训练时，一个子集将会被选为测试集，

而剩下的数据将会被用来训练模型。最后用来评估的结果则是：k 次训练后，模型输出性能（如

准确度）的平均值。图 1.11 展示了一个四折交叉验证示例。

17

第 1 章 深度学习入门

图 1.11 四折交叉验证（Four-Fold Cross-Validation）示例。数据集被划分为四个子集（为了展示目

的，每一行为一个子集）。在每次训练中，而加框的子集被当作测试数据，其他被当作

训练数据。最后模型评估的结果则是四次训练预测的平均

1.7 正则化

我们把那些用来使得一个模型在训练集和测试集都有很好效果的方法叫作正则化办法。本节

主要介绍过拟合和一些不同的正则化方法，如权重衰减、Dropout 和批标准化。

1.7.1 过拟合

一个机器学习的模型为了减少训练集上的损失而进行的优化，并不能保证它在测试集上的效

果良好。一个被过度优化了的模型会有很小的训练集误差，但有很大的测试集误差，这种现象为

过拟合（Overfitting）。

图 1.12 中，虚线代表的多项式模型就存在过拟合的问题。这个模型在训练集上过度一致，而

在测试集上就不太符合。当使用一个这样过拟合的模型在现实应用中应用新的数据时，是不可靠

的。相反地，由实线代表的线性模型虽有很少的参数，但是却更符合测试数据的趋势。

和过拟合相对的是欠拟合（Underfitting），即模型在训练集和测试集上都有了很大的误差。但

是在现实中，欠拟合很容易解决，比如可以用一个更大的模型来解决（更多网络层及更多的参数

等），而解决过拟合会更加棘手。最简单的一个方法就是使用更多的训练数据，但这不是一个万

能药，因为数据的获取和标记都需要代价。

1.7.2 权重衰减

权重衰减（Weight Decay）是一种简单却有效的用于解决过拟合的正则化方法。它用了一个

正则项作为惩戒，使得 θ 有更小的绝对值。以图 1.12 为例，如果多项式模型从 c 到 h 的参数有

更小的绝对值，那这个模型的上下摇摆幅度就会减小，能更好地拟合数据。用参数范式作为惩戒

的损失函数的定义为

Ltotal = L(y, yˆ) + λΩ(θ) (1.23)

18

1.7 正则化

图 1.12 一个过拟合的例子：深色点代表了训练集，浅色点代表了测试集。虽然由实线代表的线

性模型在训练集上有一个更大的损失值，但实线的模型比虚线代表的多项式模型在测试

集上误差更小。我们可以说这个多项式模型对训练集过拟合了

其中 L(y, yˆ) 是从使用目标 y 和预测 yˆ 来计算的损失函数，Ω 是模型的参数范式惩戒函数，

λ 是有比较小的值，以控制参数范式惩戒函数的幅度。

两种最常见的参数范式惩戒函数是 L1 = ∥W∥ 和 L2 = ∥W∥

2

2。深度神经网络的参数的绝对

值通常小于 1，所以 L1 会比 L2 输出一个更大的惩戒，因为当 |w| < 1 时，|w| > w2。可见，L1

函数用来作为参数范式惩戒函数时，会让参数偏向于更小的值甚至为 0。这是模型隐性地选择特

征的方法，把那些不重要特征的相对应参数设为一个很小的值或者是 0。

我们可以进一步通过几何方法来看看 L1 和 L2 的区别。由图 1.13 所示的坐标系里，有两个

模型参数 w1, w2。w1

2 + w2

2 = r

2 是一个半径为 r 的圆（图 1.13 左）而 |w1| + |w2| = r 是一个对

图 1.13 左图：原始损失值的轮廓线（红色）还有 L2 损失值（蓝色）。右图：原始损失值的轮廓

线（红色）还有 L1 损失值（蓝色）。从红色轮廓线和蓝色轮廓线交接的地方可见，L1 更

有可能使得参数为 0

19

第 1 章 深度学习入门

角线长为 2r 的正方形（图 1.13 右）。它们两个都被蓝色轮廓线表示。在图中，红色的线代表的是

初始的损失 L(y, yˆ)。初始损失和参数范式惩戒的交点用“叉”标记了出来。L1 更有可能使得参

数为 0，两个轮廓的交接位于正方形的顶点上。

1.7.3 Dropout

Dropout 是另一个很受欢迎的用来解决过拟合问题方法 (Hinton et al., 2012; Srivastava et al.,

2014)。当神经元数量非常多时，网络会出现共适应的问题，从而会有过拟合的现象。神经元的共

适应指神经元之间会互相依赖。故而，造成一旦有一个神经元失效了，就有可能所有依赖它的神

经元都会失效，以至于整个网络瘫痪的局面。为了避免参数过多导致的共适应，Dropout 在训练

的过程中，将隐藏层的输出按比例随机设为 0。就像图 1.14 中所示一样，每一层会有几个神经元

随机地失去和其他层的连接。

图 1.14 训练过程中对一个神经网络使用 Dropout，让它的某些连接消失

在反向传播当中，如果有的输出 a

l 为 0，那么其相对应的那一层的偏导数 δ

l 也是 0。只有还有

连接的神经元会被更新。所以 Dropout 法其实是在训练很多不同的小的网络，且共用参数 (Hinton

et al., 2012)。在测试过程当中，Dropout 就不能被使用了，没有输出会被设为 0。这就意味着是

所有网络一起来预测最终的结果。集成学习（Ensemble Learning）就是这样一个例子 (Hara et al.,

2016)，它用很多模型学会做同一个任务，然后测试的时候使用所有模型输出的结果来提高准确

性。关于 Dropout 的理论证明在原始的论文里是没有的 (Hinton et al., 2012)，但是近期有了些新的

结果，比如说 (Hara et al., 2016) 就证明了它在集成学习里的有效性，以及 (Gal et al., 2016) 证明了

它在贝叶斯里的有效性。

1.7.4 批标准化

批标准化（Batch Normalization）(Ioffe et al., 2015) 层标准化了网络的输出，也就是让输出的

平均值变为 0，方差变为 1。这样做的目的是提高训练的稳定性。在训练的过程中，批标准化层会

用一个移动平均的办法来计算每一批输入的平均值和方差，以估计整个训练集的平均值和方差。

20

1.7 正则化

每一批输入的平均值和方差会被用来标准化这一批输入。在模型测试的过程当中，我们会保持移

动平均值和方差不变来标准化输入。

除了提高性能和稳定性，批标准化也可以提升正则化的作用。和 Dropout 里对隐藏层加一个

不确定性一样，移动平均值和方差也同样地引入了一定的随机性，因为在每一个回合当中，它都

是根据具体的那一批的随机样本来决定更新的。因此，在训练中有了这样一个变化的神经网络会

变得更加鲁棒。

1.7.5 其他缓和过拟合的方法

我们有很多其他方法来预防过拟合，比如说，早停法（Early Stopping）或者数据增强（Data

Augmentation）。早停法会当网络在满足一定的实际条件时停止训练，比如说在验证集上有了足够

高的精确度。图 1.16 描述了损失在训练过程可能会慢慢增加，也就是过拟合的开始，不过我们可

以用早停法在过拟合开始前的那个点停止训练。

图 1.15 一个图像数据增强的例子。左上角的是原图，其他图片是通过随机的反转、平移、缩近

等运算得到的

数据增强即增加现有训练数据的大小，如运用反转、旋转、移动和放缩等运算合理生成数据，

以减少过拟合，从而提高网络性能 (Dong et al., 2017; He et al., 2016; Howard et al., 2017; Simonyan

et al., 2015)。和图像数据一样，音频数据也一样可以通过增加噪声或者其他改变来增强。最近研

究表明，通过改变音频速度来增强，可以提高语音识别算法的性能 (Ko et al., 2015)。

但是我们不能把同样的方法运用在字符信息上面，因为字符的大小和排序有它特定的意思。

比如说，“人类喜欢狗狗”和“狗狗喜欢人类”的意思是不一样的。一个可以增强字符数据的现实

方法是用规定的同义词来复述句子 (Zhang et al., 2015)，也可以不增强原始数据，文献 (Reed et al.,

2016) 利用两个随机句子的向量表征的内插来进行数据增强。

21

第 1 章 深度学习入门

图 1.16 过拟合的训练曲线。我们可以用早停法来让训练过程在开始过拟合之时就停止

1.8 卷积神经网络

卷积神经网络（Convolutional Neural Network，CNN）(LeCun et al., 1989) 是前向神经网络的

一种，它在很多不同的领域里都有很大的作用，如计算机视觉 (He et al., 2016; Krizhevsky et al.,

2012; Simonyan et al., 2015)、时序预测 (van den Oord et al., 2016)、自然语言处理 (Yin et al., 2017;

Zhang et al., 2019a) 和强化学习 (James et al., 2019; Rusu et al., 2016)。很多已经在现实世界落地的

机器学习系统都是基于 CNN 之上的。本节介绍两种网络层：卷积层和池化层，它们都是 CNN 结

构的一部分。

卷积层可能是 CNN 最有识别度的一个特征。其主要思想来自对人脑中并排处理视觉输入的

学习。和图 1.17 所示的一样，卷积输出使用了四个不同的神经元来处理同样的输入图像区间。不

同的神经元可能负责的处理任务不一样，如处理边缘、颜色和角度等任务。在卷积层的神经元只

是和局部有连接，并不是和前一层的所有单元都有连接。卷积层可以被层层地叠加在一起，也就

是说，一个卷积层的输出可以作为另外一个卷积层的输入。卷积层最大的优点是，相对于全连接

层，它需要的参数会少得多，能更快地被训练出来。图 1.17 展示了在卷积层的每一个神经元有关

于局部输入的所有通道的信息。如果说一个 RGB 图片是输入，那么一个在卷积层的神经元就能

知道卷积核运算之后的一个局部区域的所有 RGB 通道。

在卷积层里的卷积运算使用了不同的卷积核来提取各种各样重要的特征。当其中一层网络的

输入是高/宽为 W 的向量，并且我们使用一个大小为 F 的卷积核，卷积运算将输入的向量切分为

若干小区间，然后每个小区间依次和卷积核进行点乘计算。其中步长 S 规定了每个小区间之间的

距离。若步长为 2 (S = 2)，卷积核则会跟每个距离为 2 的小区间进行点乘运算。如果要确保边缘

的数值也被很好地考虑在内的话，那么就需要在边缘填充零（Zero Padding）。若使填充的大小为

22

1.8 卷积神经网络

图 1.17 从样本图片计算卷积层的方法：蓝色的是输入层，绿色为卷积输出层。假如输入是一个

有三个通道的 RGB 图片，那么卷积层输出也有不同通道。和全连接层不同的是，卷积层

的神经元只和输入层的部分区域所连接，而不是和所有输入都有连接。图中展示了卷积

层的不同通道是怎么和输入层有局部连接的（见彩插）

P，则一个卷积层的输出层大小就可以用公式 (1.24) 来进行计算。



W − F + 2P

S

+ 1

(1.24)

输出层的深度（输出通道的数量）和卷积核的数量是一致的。图 1.18 具体地展示了卷积运算

的流程。在图 1.18 中，有一个大小为 4 × 4 （高 × 宽）的 RBG 图片、一个大小为 3 × 3 × 3 （高

× 宽 × 输入通道）的卷积核，步长 S = 1，边缘填充 P = 0。根据公式 (1.24)，输出值的高/宽为

(4 − 3 + 0)/1 + 1 = 2。输出的深度（卷积核的通道数）是 1（因为只有一个卷积核）。为了计算

在每一个通道左上角的那个数值，首先计算输入图片和卷积核的点乘，得到三个值，这三个值的

和就是左上角的数值。卷积运算所得到的输出可以通过一层激活函数来引入非线性。

池化层利用了图片相邻像素类似的性质来进行下取样。我们认为，合适的像素只留取一个区

域里的最大值或者平均值的下取样，会在建模当中有很多益处。通常有两种池化方法来减少数据

大小：最大值池化和平均值池化。在图 1.19 中，在一个 4 × 4 的输入上和在步长是 2 的情况下，

演示了最大值池化和平均值池化的例子。池化层可以很明显地减少输出大小，提高之后层的计算

效率。比如说，在一个卷积层以后会有数以百计的通道，在输出被传递给全连接层之前，使用池

化层来减小输出大小会减小计算量。

通常来说，卷积层、池化层和全连接层是 CNN 的核心构建部分。图 1.20 展示了一个有两个

卷积层、一个最大值池化层和一个全连接层的网络。这里需要注意的是激活函数可以同样地用在

卷积层上。

和前向神经网络不同的是，CNN 借用了参数共享的概念。在模型的不同部分使用参数共享，

让整个模型更加高效（更少的参数和内存需求），然后它也可以用来处理不同的数据形式（不同

大小或者长度）。回想一下，在一个全连接层中有一个权重矩阵，里面的元素 wij 代表着前一层

第 i 神经元和当前层第 j 神经元连接。但在一个卷积层里，卷积核其实就是权重，它们在运算输

出的时候是被重复使用的。对卷积核的重复使用就减少了在卷积网络里对参数的需求，这也就是

为什么在输入和输出大小类似的情况下，卷积层比全连接层所需要的参数更少。

23

第 1 章 深度学习入门

图 1.18 卷积运算的示意图，在这个例子里有一个大小为 3 × 3 × 3 × 1 的卷积核（Filter，也称为

Kernel）（尺寸为：高 × 宽 × 输入通道数 × 输出通道数）被用到了一个大小为 4 × 4（高

× 宽）的有 3 个输入通道的 RBG 图片上。图片和卷积核的点乘在不同的通道上都会应用。

点乘所获得的值最终会被求和，然后得到输出的左上角的那个值

1 3 1 2

4 5 4 2

1 5 6 3

4 2 0 5

5 4

5 6

3.25 2.25

3 3.5

最大值池化 平均值池化

输入

图 1.19 2 × 2 最大值池化和平均值池化的例子，它们的步长为 2，输入大小是 4 × 4

我们可以进一步地通过批标准化（批标准化层），即内部的样例迁移，来提高 CNN 的训练效

率 (Ioffe et al., 2015)。我们之前提过，一个批标准化层是通过一个平均值和一个方差来进行标准

化且独立于其他层的。也就是说，批标准化简化了在梯度更新的时候不同层之间的关系，从而可

以用更大的学习速率来加快学习过程。

24

1.9 循环神经网络

图 1.20 一个有两个卷积层、一个池化层和一个全连接层的网络。图片使用 NN-SVG 构造

1.9 循环神经网络

循环神经网络（Recurrent Neural Networks，RNN）(Rumelhart et al., 1986) 是另一种深度学习

模型结构，主要用于处理序列数据。图像数据可以用网格加数值来表示，而序列数据作为另一种

常见的数据类型，则被定义为一串元素 {x1, x2, · · · , xn}。例如，文本是由一串单词组成的，而股

票的价格也可以用一串交易金额来表示。

序列数据的一个重要特点是，这一串元素之间有互相影响。例如，人们可以轻松地根据文章

的开头，大致推测出文章接下来的内容。然而，针对这种元素之间的影响进行建模是相当具有挑

战性的，尤其是当这一串序列非常长的时候。因此，循环神经网络需要能够有效地积累序列信息，

并且考虑前序信息和后序信息之间的影响。

与卷积神经网络类似，循环神经网络同样使用了参数共享。参数共享得以让循环神经网络对

序列上不同位置的元素重复使用同一组权重。我们来一起看一个例子，卷积神经网络需要能够学

到“深度学习从 2010 年开始受到追捧”和“从 2010 年开始，深度学习受到追捧”这两句话其实

表达的是相同的意思，尽管两句话的语序并不相同。同样，当卷积神经网络对猫的图片进行分类

的时候，猫在图片中的位置也不应该影响模型做出正确的判断。

循环神经网络可以处理任意长度的输入序列，这一点与卷积神经网络可以处理任意长宽的输

入图像相似。之所以如此，是因为循环神经网络使用了循环单元（Cell）作为基本的计算单元。针

对输入序列中的每个数据元素，循环单元会被依次反复调用并进行计算。因此循环单元中会维护

一个隐状态（Hidden State），用于记录序列中的信息。循环单元的计算包含两个输入，分别是序列

中的当前数据及循环单元之前的隐状态。循环单元根据两个输入计算新的隐状态作为输出，新的

隐状态也将用在下一轮计算当中，如图 1.21 所示。最简单的循环单元使用线性变换（公式 (1.25)）：

ht = W[xt; ht−1] + b (1.25)

25

第 1 章 深度学习入门

图 1.21 循环神经网络结构示意图。循环单元（cell）接收数值 xt 和前序信息的隐状态 ht−1，然

后输出新的隐状态 ht.

公式 (1.25) 中，隐状态 ht−1 与输入数据 xt 组合在一起，然后与线性核 W 做矩阵乘法，同时

偏置 b 也可以加入新的隐状态当中。由于线性核 W 会被反复计算，循环神经网络实际上构建了

一个深度计算图，而深度较大的计算图可能导致梯度爆炸或者梯度消失。当 W 的特征值幅度大

于 1 时，可能导致梯度爆炸，而梯度爆炸会让学习过程完全失效。与之相反，若 W 的特征值幅度

小于 1，则将可能导致梯度消失，梯度消失会让模型无法有效地根据学习目标进行优化。如果输

入序列很长，那么使用简单循环单元的循环神经网络将有可能遇到这两种梯度问题的其中之一。

简单循环单元有严重的遗忘问题，当给定句子“我是中国人，我的母语是 ____”，简单循环

单元会很容易预测出结果是“中文”，但是当句子很长时，如“我是中国人，我去英国读书，后

来在法国工作，我的母语是 ____”，隐状态被多次更新后，简单循环单元很可能无法预测出正确

的结果。长短期记忆（Long Short-Term Memory，LSTM）(Hochreiter et al., 1997) 是一种更加先进

的循环单元，并常用于处理长序列中元素之间的影响。使用 LSTM 作为循环单元的循环神经网络

亦常被简称为 LSTM。

与简单循环单元不同，LSTM 循环单元有两个状态量：单元状态（Cell State），记为 Ct；隐

状态（Hidden State），记为 ht。计算单元状态的过程实际上构建了一条信息高速路（如图 1.22 所

示），这条信息高速路贯穿整个序列并且只使用了简单的计算过程。由于这条信息高速路让信息

流可以较为便捷地穿越整个序列，因此 LSTM 可以较好地考虑长序列当中两个距离较远的元素

之间的影响，即长期记忆。与此同时，LSTM 基于门（Gate）的机制计算隐状态。这种基于门的

计算机制利用 sigmoid 激活函数来控制信息的遗忘或者叠加，因为 sigmoid 函数的值域介于 0 和

1 之间。也就是说，当 sigmoid 函数输出为 1 时，相对应的信息会被完整地保留。与之相反，当

sigmoid 函数输出为 0 时，相对应的信息会完全丢失。

在 LSTM 当中，一共有三个基于门的计算机制，分别是遗忘门（Forget Gate）、输入门（Input

Gate）和输出门（Output Gate）。首先，遗忘门根据新的输入来决定单元状态当中是否有部分信息

应该被遗忘。其次，输入门决定哪些输入信息应该被加入单元状态中，目的是长期存储这部分信

息并取代被遗忘的信息。最后的输入门根据最新的单元状态，决定 LSTM 循环单元的输出。这三

26

1.9 循环神经网络

“信息高速路”

图 1.22 使用 LSTM 循环单元的循环神经网络示意图。LSTM 循环单元包括两个状态，即单元状态

（Cell State）Ct 和隐状态（Hidden State）ht。除此之外，还有三个门（Gate）用于控制

信息的取舍。本图依据文献 (Olah, 2015) 重新绘制

个基于门的计算机制可以用方程 (1.26) 定义，其中 σ 代表 simoid 函数。

遗忘门： ft = σ(Wf [ht−1; xt] + bf )

输入门： it = σ(Wi

[ht−1; xt] + bi)

输出门： ot = σ(Wo[ht−1; xt] + bo)

更新单元状态： Ct = ft × Ct−1 + it × tanh(WC [ht−1; xt] + bC )

更新隐状态： ht = ot × tanh(Ct) (1.26)

循环神经网络有很多种，而 LSTM 是其中之一，还有 GRU（Gated Recurrent Units）。最近的

研究工作尝试对比了不同结构的循环神经网络，但是关于哪一种结构更优，目前尚无定论 (Cho

et al., 2014; Jozefowicz et al., 2015)。

在深度学习中，循环神经网络主要用于处理序列数据，如自然语言和时间序列 (Chung et al.,

2014; Liao et al., 2018b; Mikolov et al., 2010)，同时也会用于处理强化学习的问题 (Peng et al., 2018;

Wierstra et al., 2010)。根据输入和输出的关系，循环神经网络的结构在不同的场景也会有些许变

化。例如，在文本分类的问题中，循环神经网络的输入是一串单词序列，而输出是单个代表类别

的标签 (Lee et al., 2016; Zhang et al., 2019a)。在机器翻译 (Bahdanau et al., 2015; Luong et al., 2015;

Sutskever et al., 2014) 或者自动摘要 (Nallapati et al., 2017) 的任务中，循环神经网络的输入和输出

均是一串单词序列。关于更多的细节，感兴趣的读者可以查看我们其他的讲义，链接见读者服务。

27

第 1 章 深度学习入门

1.10 深度学习的实现样例

本节将介绍深度学习的实现样例，其中模型的代码将基于 Python 3、TensorFlow 2.0 和 TensorLayer 2.0。

1.10.1 张量和梯度

张量（Tensor）是 TensorFlow 中最基本的计算单元，特指计算函数的输出，由计算函数生

成，如 tf.constant，tf.matmul 等。这些张量本身并不存储计算结果，而是为获取 TensorFlow

session 中产生该结果的计算过程提供便利。在 TensorFlow 2.0 中，无须手动运行会话（Session），

因为在 Eager execution 设计思想下，运算图和会话的运行细节仅在后端可见。比如，在下面的矩

阵乘法示例中，我们可以通过 tf.constant 创建矩阵，并通过 tf.matmul 计算输出为另一个矩

阵的乘法。

代码 1.1 TensorFlow 中基于张量的矩阵乘法

>>> import tensorflow as tf

>>> a = tf.constant([[1, 2], [1, 2]])

# tf.Tensor(

# [[1 2]

# [1 2]], shape=(2, 2), dtype=int32)

>>> b = tf.constant([[1], [2]])

# tf.Tensor(

# [[1]

# [2]], shape=(2, 1), dtype=int32)

>>> c = tf.matmul(a, b)

# tf.Tensor(

# [[5]

# [5]], shape=(2, 1), dtype=int32)

在深度神经网络的前向传播中，Tensors 实例会自动相互连接，从而形成一个运算图。因此，我

们可以通过 TensorFlow 自带的自动差分和运算图相关功能，在反向传播时计算梯度。TensorFlow

2.0 更是提供了 tf.GradientTape 方法，用于计算输入变量对被记录操作的梯度。

神经网络的前向传播和损失函数的计算应当在 tf.GradientTape 作用域之内，而反向传

播和权重更新则可以在作用域之外。tf.GradientTape 将所有在作用域内执行的运算都记录

到 Tape 中，然后通过反向自动差分机制，计算每个运算符和输入变量相对应的梯度。直到

tape.gradient() 被调用后，tf.GradientTape 所占用的资源才会被释放。

28

1.10 深度学习的实现样例

代码 1.2 TensorFlow 和 TensorLayer 中的梯度计算

import tensorflow as tf

import tensorlayer as tl

def train(model, dataset, optimizer):

# 给定一个 TensorLayer 模型

# 遍历数据，其中 x 为输入，y 为输出

for x, y in dataset:

# 构建 tf.GradientTape 的作用域

with tf.GradientTape() as tape:

prediction = model(x) # 前向传播

loss = loss_fn(prediction, y) # 损失函数

# 反向传播并计算梯度

# 然后释放 tf.GradientTape 所占用的资源

gradients = tape.gradient(loss, model.trainable_weights)

# 根据梯度，利用优化器更新权重

optimizer.apply_gradients(zip(gradients, model.trainable_weights))

1.10.2 定义模型

在 TensorLayer 2.0 中，模型（Model）是一个包含多个 Layer 的实体，并且定义了 Layer

之间传播运算。TensorLayer 2.0 提供了两套定义模型的接口，其中静态模型接口让用户可以更加

流畅地定义模型，而动态模型接口让前向传播更加灵活。静态模型需要用户手动构建运算图并编

译，模型一旦编译后，前向传播将不能修改。与之不同的是，动态模型可以像普通 Python 代码一

样即刻执行（Eager Execution），而且前向传播是可以修改的。

如下面的实现样例所示，我们可以将静态模型和动态模型的差别总结成两个方面。首先，静

态模型中的 Layer 在声明的同时也会定义与其他 Layer 的连接关系（即前向传播）。根据 Layer 之

间的连接关系，TensorLayer 可以自动推断每个 Layer 输入变量的大小，并相应构建权重。因此，

当 Model 初始化的时候，只需要明确模型的输入和输出即可，而 TensorLayer 将自动根据 Layer

之间的连接构建运算图。然而，动态模型则不同，前向传播的顺序（即 Layer 之间的连接关系）

在动态模型初始化时是不需要明确的，因为动态模型的前向传播直到前向函数 forward 被实际

调用的时候才能确定。因此，动态模型无法自动推断每个 Layer 输入变量的大小，必须通过输入

参数 in_channels 显式地明确 Layer。

其次，静态模型的前向传播一旦定义即固定，因此更加易于加速计算过程。TensorFlow 2.0 提

供了一个新的功能，即 tf.function，可作为装饰器套在函数上，加速函数内的计算。与静态模

型不同的是，动态模型的前向传播更加灵活。例如，用户可以根据不同的输入和参数来控制前向

传播，同时也可以根据需要选择执行或者跳过部分 Layer 的计算。

29

第 1 章 深度学习入门

代码 1.3 静态模型样例：多层感知器（MLP）

import tensorflow as tf

from tensorlayer.layers import Input, Dense

from tensorlayer.models import Model

# 包含了三个全连接层的多层感知器模型

def get_mlp_model(inputs_shape):

ni = Input(inputs_shape)

# 因为明确定义了 Layer 之间的连接关系

# 可以自动推断每个 Layer 的 in_channels

nn = Dense(n_units=800, act=tf.nn.relu)(ni)

nn = Dense(n_units=800, act=tf.nn.relu)(nn)

nn = Dense(n_units=10, act=tf.nn.relu)(nn)

# 根据连接关系自动构建模型

M = Model(inputs=ni, outputs=nn)

return M

MLP = get_mlp_model([None, 784])

# 开启 eval 模式

MLP.eval()

# 给定输入数据

# 该计算过程可以通过 TensorFlow 2.0 中的 @tf.function 加速

outputs = MLP(data)

代码 1.4 动态模型样例：多层感知器（MLP）

import tensorflow as tf

from tensorlayer.layers import Input, Dense

from tensorlayer.models import Model

class MLPModel(Model):

def __init__(self):

super(MLPModel, self).__init__()

# 因为无法明确 Layer 之间的连接关系，必须手动提供 in_channels

# 给定输入数据的大小，即 784

self.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784)

self.dense2 = Dense(n_units=800, act=tf.nn.relu, in_channels=800)

self.dense3 = Dense(n_units=10, act=tf.nn.relu, in_channels=800)

30

1.10 深度学习的实现样例

def forward(self, x, foo=False):

# 定义前向传播

z = self.dense1(z)

z = self.dense2(z)

out = self.dense3(z)

# 灵活控制前向传播

if foo:

out = tf.nn.softmax(out)

return out

MLP = MLPModel()

# 开启 eval 模式

MLP.eval()

# 给定输入数据

# 通过参数 foo 控制前向传播

outputs_1 = MLP(data, foo=True) # 使用 softmax

outputs_2 = MLP(data, foo=False) # 不使用 softmax

1.10.3 自定义层

TensorLayer 2.0 为用户提供了大量的神经网络层，也支持 Lambda Layer 以方便用户创造高

度自定义的层。如下所示，最简单的例子是把一个 lambda 表达式直接传入 Lambda Layer。用户

可以通过一个自定义输入参数的函数和 fn_args 选项来初始化或者调用 Lambda Layer。

import tensorlayer as tl

x = tl.layers.Input([8, 3], name=’input’)

y = tl.layers.Lambda(lambda x: 2*x)(x) # 没有可训练的权重

def customize_fn(input, foo): # 参数可以通过 Lambda Layer 的 fn_args 定义

return foo * input

z = tl.layers.Lambda(customize_fn, fn_args={’foo’: 42})(x) # this layer has no weights.

Lambda Layer 拥有可训练的权重。下面的示例可以展示如何在自定义函数外定义权重，并

通过 fn_weights 选项传入 Lambda Layer。

31

第 1 章 深度学习入门

import tensorflow as tf

import tensorlayer as tl

a = tf.Variable(1.0) # 自定义函数作用域之外的权重

def customize_fn(x):

return x + a

x = tl.layers.Input([8, 3], name=’input’)

y = tl.layers.Lambda(customize_fn, fn_weights=[a])(x) # 通过 fn_weights 传递权重

此外，Lambda Layer 还可以使 Keras 与 TensorLayer 兼容。用户可以定义一个 Keras 模型，

并将其以一个函数的形式传入 Lambda Layer，因为 Keras 的模型是可被调用的。同时，为了让

自定义模型和 Keras 模型一起被训练，Keras 模型中可被训练的权重需要被手动提取，然后传入

Lambda Layer 中。

import tensorflow as tf

import tensorlayer as tl

# 定义一个 Keras 模型

layers = [

tf.keras.layers.Dense(10, activation=tf.nn.relu),

tf.keras.layers.Dense(5, activation=tf.nn.sigmoid),

tf.keras.layers.Dense(1, activation=tf.identity)

]

perceptron = tf.keras.Sequential(layers)

# 获得 Keras 模型的可被训练的权重

_ = perceptron(np.random.random([100, 5]).astype(np.float32))

class CustomizeModel(tl.models.Model):

def __init__(self):

super(CustomizeModel, self).__init__()

self.dense = tl.layers.Dense(in_channels=1, n_units=5)

self.lambdalayer = tl.layers.Lambda(perceptron, perceptron.trainable_variables)

# 将可以训练的权重传递给 Lambda Layer

def forward(self, x):

z = self.dense(x)

z = self.lambdalayer(z)

return z

32

1.10 深度学习的实现样例

1.10.4 多层感知器：MNIST 数据集上的图像分类

用户可以通过 TensorLayer 2.0 中提供的 Model、Layer 和其他支持性的 API 来灵活、直观地

设计和实现自己的深度学习模型。为了帮助读者更好地了解如何用 TensorLayer 实现一个深度学

习模型，这里首先介绍一个利用多层感知器在 MNIST 数据集 (LeCun et al., 1998) 上分类图片的示

例。该数据集包含了 70,000 张手写数字的图片。一个深度学习模型的建立通常会包含五个步骤，

分别是数据加载、模型定义、训练、测试和模型存储。

TensorLayer 在 tl.files 中提供了多个常用数据集的 API，包括 MNIST、CIFAR10、PTB、

CelebA等。比如说，我们可以用 tl.files.load_mnist_dataset和一个具体的shape加载MNIST

数据集。通常来说，数据集会被划分为三个子集：训练集、验证集和测试集。

# 通过 TensorLayer 加载 MNIST 数据集

X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,

784)) # 每个 MNIST 图像的原始尺寸为 28 * 28，即一共有 784 个像素点

就像在 1.10.2 节里提到的一样，在 TensorLayer 2.0 中，一个多层感知器模型可以通过静态模

型或者动态模型两种方法来实现。在这个例子中，我们的模型有三个 Dense 层，且为静态模型，

同时，用 Dropout 来防止过拟合现象的产生。

# 构建模型

ni = tl.layers.Input([None, 784]) # 根据输入数据定义尺寸

# 多层感知器

nn = tl.layers.Dropout(keep=0.8)(ni)

nn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)

nn = tl.layers.Dropout(keep=0.5)(nn)

nn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)

nn = tl.layers.Dropout(keep=0.5)(nn)

nn = tl.layers.Dense(n_units=10, act=None)(nn)

# 给定输入和输出，构建模型

network = tl.models.Model(inputs=ni, outputs=nn, name="mlp")

多层感知器在MNIST数据集上的训练是指对其权重的学习。用户可以通过调用 tl.utils.fit

函数来触发训练过程。除此之外，我们还需要通过 tl.utils.test 函数来验证模型的性能。

# 定义一个函数来评估模型的准确度

# 与损失函数不同，这个函数不用于更新模型

def acc(_logits, y_batch):

return tf.reduce_mean(

tf.cast(

33

第 1 章 深度学习入门

tf.equal(

tf.argmax(_logits, 1),

tf.convert_to_tensor(y_batch, tf.int64)),

tf.float32),

name=’accuracy’

)

# 训练

tl.utils.fit(

network, # 模型

train_op=tf.optimizers.Adam(learning_rate=0.0001), # 优化器

cost=tl.cost.cross_entropy, # 损失函数

X_train=X_train, y_train=y_train, # 训练集

acc=acc, # 评估指标

batch_size=256, # 批样本数量

n_epoch=20, # 训练轮数

X_val=X_val, y_val=y_val, eval_train=True, # 验证集

)

# 测试

tl.utils.test(

network, # 训练好的模型

acc=acc, # 评估指标

X_test=X_test, y_test=y_test, # 测试集

batch_size=None, # 批样本数量，如果为 None 则将测试集一起输入模型，因此当且仅当测试集

# 很小的时候可以将此设置为 None

cost=tl.cost.cross_entropy # 损失函数

)

最后，多层感知器模型的权重可以保存至本地的一个文件中，使得我们可以在后面需要的时

候恢复模型参数，用于推理，该多层感知器示例的完整实现代码链接见读者服务。

# 将模型权重保存到文件中

network.save_weights(’model.h5’)

34

1.10 深度学习的实现样例

1.10.5 卷积神经网络：CIFAR-10 数据集上的图像分类

CIFAR-10 数据集 (Krizhevsky et al., 2009) 是一个通用的、具有一定挑战性的图像分类基准测

试。此数据集一共包含 10 类数据，其中每类分别有 6000 张 32 × 32 RGB 图片，且每张图片只

专注于描述单个物体，如狗、飞机、船舶等。使用 TensorLayer 2.0 中的 Dataset 和 Dataloader

APIs，我们可以很简单地加载 CIFAR-10 并对其做数据增强。

# 定义数据增强

def _fn_train(img, target):

# 1. 随机切割长宽均为 24 的一小块图片

img = tl.prepro.crop(img, 24, 24, False)

# 2. 随机水平翻转图片

img = tl.prepro.flip_axis(img, is_random=True)

# 3. 正则化：减去像素点的平均值并除以方差

img = tl.prepro.samplewise_norm(img)

target = np.reshape(target, ())

return img, target

# 加载训练集

train_ds = tl.data.CIFAR10(train_or_test=’train’, shape=(-1, 32, 32, 3))

# dataloader 加载数据集和数据增强算法

train_dl = tl.data.Dataloader(train_ds, transforms=[_fn_train], shuffle=True,

batch_size=batch_size, output_types=(np.float32, np.int32))

# 加载测试集

test_ds = tl.data.CIFAR10(train_or_test=’test’, shape=(-1, 32, 32, 3))

# dataloader 加载测试集

test_dl = tl.data.Dataloader(test_ds, batch_size=batch_size)

# 遍历数据集

for X_batch, y_batch in train_dl:

# 训练、测试模型的代码

在这个示例里，我们将使用带有批标准化 (Ioffe et al., 2015) 的卷积神经网络来对 CIFAR-10

中的图片进行分类。该模型有两个卷积模块，其中每个模块含有一个批标准化层。模型的最后包

含了三个全连接层。该卷积网络示例的完整实现代码链接请见读者服务。

# 包含了 BatchNorm 的卷积神经网络

def get_model_batchnorm(inputs_shape):

35

第 1 章 深度学习入门

# 自定义权重初始化

W_init = tl.initializers.truncated_normal(stddev=5e-2)

W_init2 = tl.initializers.truncated_normal(stddev=0.04)

b_init2 = tl.initializers.constant(value=0.1)

# 输入层

ni = Input(inputs_shape)

# 第一个卷积层 Conv2d，以及 BatchNorm 和池化层 MaxPool

nn = Conv2d(64, (5, 5), (1, 1), padding=’SAME’, W_init=W_init, b_init=None)(ni)

nn = BatchNorm2d(decay=0.99, act=tf.nn.relu)(nn)

nn = MaxPool2d((3, 3), (2, 2), padding=’SAME’)(nn)

# 第二个卷积层 Conv2d，以及 BatchNorm 和池化层 MaxPool

nn = Conv2d(64, (5, 5), (1, 1), padding=’SAME’, W_init=W_init, b_init=None)(nn)

nn = BatchNorm2d(decay=0.99, act=tf.nn.relu)(nn)

nn = MaxPool2d((3, 3), (2, 2), padding=’SAME’)(nn)

# 卷积层的输出传递给三个全连接层

nn = Flatten()(nn)

nn = Dense(384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2)(nn)

nn = Dense(192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2)(nn)

nn = Dense(10, act=None, W_init=W_init2)(nn)

# 给定输入和输出，构建模型

M = Model(inputs=ni, outputs=nn, name=’cnn’)

return M

1.10.6 序列到序列模型：聊天机器人

聊天机器人（Chatbot）的设计通常涵盖了语音和文字对话的应用。在这个示例中，我们将简

化这一设计，并考虑文字输入和反馈的情形。因此，序列到序列模型（Seq2seq）(Sutskever et al.,

2014) 是实现聊天机器人的一个很好的选择。该模型需要序列作为输入和输出，因此，我们可以在

此把聊天机器人的输入和输出定义为句子，又可被理解为是文字的序列。seq2seq 模型会被训练

去对输入句子以另一句话的形式做适当的回应。虽然 seq2seq 模型在提出的时候主要应用于机器

翻译，但在其他序列-序列应用场景中同样具有良好的应用前景，如交通预测 (Liao et al., 2018a,b)、

文本自动摘要 (Liu et al., 2018; Zhang et al., 2019b) 等。

36

参考文献

在实践中，一个 seq2seq 模型由两个 RNN 组成，其一为编码 RNN，其二为解码 RNN。编

码 RNN 会学习一个对于输入语句的表示，然后解码 RNN 便可尝试生成一个针对输入的回应。

TensorLayer 库提供的 API 可以在一行以内生成一个 Seq2seq 模型。

# Seq2seq 模型

model_ = Seq2seq(

decoder_seq_length=decoder_seq_length, # 解码的最大长度

cell_enc=tf.keras.layers.GRUCell, # 编码 RNN 的循环单元

cell_dec=tf.keras.layers.GRUCell, # 解码 RNN 的循环单元

n_layer=3, # 编码 RNN 和解码 RNN 的层数

n_units=256, # RNN 的隐状态大小

embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size,

embedding_size=emb_dim), # 编码 RNN 的嵌入层

)

下面展示了一些基于 Seq2seq 的聊天机器人模型的结果，聊天机器人的完整实现代码链接请

见读者服务。该模型可以在获取一个输入句子后输出多种可能的结果。

Query > happy birthday have a nice day

> thank you so much

> thank babe

> thank bro

> thanks so much

> thank babe i appreciate it

参考文献

BAHDANAU D, CHO K, BENGIO Y, 2015. Neural machine translation by jointly learning to align and

translate[C]//Proceedings of the International Conference on Learning Representations (ICLR).

BISHOP C M, 2006. Pattern recognition and machine learning[M]. springer.

BOTTOU L, BOUSQUET O, 2007. The Tradeoffs of Large Scale Learning.[C]//Proceedings of the Neural

Information Processing Systems (Advances in Neural Information Processing Systems) Conference:

volume 20. 161-168.

CAO Z, SIMON Z, WEI S E, et al., 2017. Realtime multi-person 2d pose estimation using part affinity

fields[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

37

第 1 章 深度学习入门

CHO K, VAN MERRIËNBOER B, GULCEHRE C, et al., 2014. Learning phrase representations using

RNN encoder-decoder for statistical machine translation[C]//Proceedings of the Empirical Methods in

Natural Language Processing (EMNLP) Conference.

CHUNG J, GULCEHRE C, CHO K, et al., 2014. Empirical evaluation of gated recurrent neural networks

on sequence modeling[J]. arXiv preprint arXiv:1412.3555.

DEVLIN J, CHANG M W, LEE K, et al., 2019. BERT: Pre-training of deep bidirectional transformers for

language understanding[C/OL]//Proceedings of the 2019 Conference of the North American Chapter

of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long

and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics: 4171-4186.

DOI: 10.18653/v1/N19-1423.

DONG H, ZHANG J, MCILWRAITH D, et al., 2017. I2t2i: Learning text to image synthesis with textual

data augmentation[C]//Proceedings of the IEEE International Conference on Image Processing (ICIP).

DUCHI J, HAZAN E, SINGER Y, 2011. Adaptive subgradient methods for online learning and stochastic

optimization[J]. Journal of Machine Learning Research (JMLR), 12(Jul): 2121-2159.

GAL Y, GHAHRAMANI Z, 2016. Dropout as a bayesian approximation: Representing model uncertainty

in deep learning[C]//Proceedings of the International Conference on Machine Learning (ICML). 1050-

1059.

GLOROT X, BORDES A, BENGIO Y, 2011. Deep sparse rectifier neural networks[C]//Proceedings of

the International Conference on Artificial Intelligence and Statistics (AISTATS). 315-323.

GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al., 2014. Generative Adversarial Nets[C]//

Proceedings of the Neural Information Processing Systems (Advances in Neural Information Processing

Systems) Conference.

GOODFELLOW I, BENGIO Y, COURVILLE A, 2016. Deep learning[M]. MIT Press.

HARA K, SAITOH D, SHOUNO H, 2016. Analysis of dropout learning regarded as ensemble learning[C]//Proceedings of the International Conference on Artificial Neural Networks (ICANN). Springer:

72-79.

HE K, ZHANG X, REN S, et al., 2015. Delving deep into rectifiers: Surpassing human-level performance

on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision.

1026-1034.

38

参考文献

HE K, ZHANG X, REN S, et al., 2016. Deep Residual Learning for Image Recognition[C]//Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

HINTON G E, SRIVASTAVA N, KRIZHEVSKY A, et al., 2012. Improving neural networks by preventing

co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580.

HOCHREITER S, HOCHREITER S, SCHMIDHUBER J, et al., 1997. Long Short-Term Memory.[J].

Neural Computation, 9(8): 1735-80.

HORNIK K, STINCHCOMBE M, WHITE H, 1989. Multilayer feedforward networks are universal

approximators[J]. Neural networks, 2(5): 359-366.

HOWARD A G, ZHU M, CHEN B, et al., 2017. Mobilenets: Efficient convolutional neural networks for

mobile vision applications[J]. Computing Research Repository (CoRR).

IOFFE S, SZEGEDY C, 2015. Batch normalization: Accelerating deep network training by reducing

internal covariate shift[J]. arXiv preprint arXiv:1502.03167.

JAMES S, WOHLHART P, KALAKRISHNAN M, et al., 2019. Sim-to-real via sim-to-sim: Dataefficient robotic grasping via randomized-to-canonical adaptation networks[C]//Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition. 12627-12637.

JOZEFOWICZ R, ZAREMBA W, SUTSKEVER I, 2015. An empirical exploration of recurrent network

architectures[C]//International Conference on Machine Learning. 2342-2350.

KINGMA D, BA J, 2014. Adam: A method for stochastic optimization[C]//Proceedings of the International Conference on Learning Representations (ICLR).

KO T, PEDDINTI V, POVEY D, et al., 2015. Audio augmentation for speech recognition[C]//Annual

Conference of the International Speech Communication Association.

KRIZHEVSKY A, HINTON G, et al., 2009. Learning multiple layers of features from tiny images[R].

Citeseer.

KRIZHEVSKY A, SUTSKEVER I, HINTON G E, 2012. Imagenet classification with deep convolutional

neural networks[C]//Advances in Neural Information Processing Systems. 1097-1105.

LECUN Y, BOSER B, DENKER J S, et al., 1989. Backpropagation applied to handwritten zip code

recognition[J]. Neural computation, 1(4): 541-551.

LECUN Y, BOTTOU L, BENGIO Y, et al., 1998. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 86(11): 2278-2324.

39

第 1 章 深度学习入门

LECUN Y, BENGIO Y, HINTON G, 2015. Deep learning[J]. Nature, 521(7553): 436.

LEE J Y, DERNONCOURT F, 2016. Sequential short-text classification with recurrent and convolutional

neural networks[C/OL]//Proceedings of the 2016 Conference of the North American Chapter of the

Association for Computational Linguistics: Human Language Technologies. San Diego, California:

Association for Computational Linguistics: 515-520. DOI: 10.18653/v1/N16-1062.

LIAO B, ZHANG J, CAI M, et al., 2018a. Dest-ResNet: A deep spatiotemporal residual network for

hotspot traffic speed prediction[C]//2018 ACM Multimedia Conference on Multimedia Conference.

ACM: 1883-1891.

LIAO B, ZHANG J, WU C, et al., 2018b. Deep sequence learning with auxiliary information for

traffic prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge

Discovery & Data Mining. ACM: 537-546.

LIU P J, SALEH M, POT E, et al., 2018. Generating wikipedia by summarizing long sequences[C]//

International Conference on Learning Representations.

LUONG T, PHAM H, MANNING C D, 2015. Effective approaches to attention-based neural machine

translation[C/OL]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language

Processing. Lisbon, Portugal: Association for Computational Linguistics: 1412-1421. DOI: 10.18653/

v1/D15-1166.

MAAS A L, DALY R E, PHAM P T, et al., 2011. Learning word vectors for sentiment analysis[C]//

HLT ’11: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:

Human Language Technologies - Volume 1. Stroudsburg, PA, USA: Association for Computational

Linguistics: 142-150.

MIKOLOV T, KARAFIÁT M, BURGET L, et al., 2010. Recurrent neural network based language

model[C]//Interspeech.

NALLAPATI R, ZHAI F, ZHOU B, 2017. Summarunner: A recurrent neural network based sequence

model for extractive summarization of documents[C]//AAAI’17: Proceedings of the Thirty-First AAAI

Conference on Artificial Intelligence. San Francisco, California, USA: AAAI Press: 3075–3081.

NG A Y, JORDAN M I, 2002. On discriminative vs. generative classifiers: A comparison of logistic

regression and naive bayes[C]//Proceedings of the Neural Information Processing Systems (Advances

in Neural Information Processing Systems) Conference. 841-848.

NOH H, HONG S, HAN B, 2015. Learning deconvolution network for semantic segmentation[C]//

Proceedings of the International Conference on Computer Vision (ICCV). 1520-1528.

40

参考文献

OLAH C, 2015. Understanding lstm networks[Z].

PENG X B, ANDRYCHOWICZ M, ZAREMBA W, et al., 2018. Sim-to-real transfer of robotic control

with dynamics randomization[C]//2018 IEEE International Conference on Robotics and Automation

(ICRA). IEEE: 1-8.

REED S, AKATA Z, YAN X, et al., 2016. Generative Adversarial Text to Image Synthesis[C]//Proceedings

of the International Conference on Machine Learning (ICML).

RISH I, et al., 2001. An empirical study of the naive bayes classifier[C]//International Joint Conference on

Artificial Intelligence 2001 workshop on empirical methods in artificial intelligence: volume 3. 41-46.

ROSENBLATT F, 1958. The perceptron: a probabilistic model for information storage and organization

in the brain.[J]. Psychological Review, 65(6): 386.

RUCK D W, ROGERS S K, KABRISKY M, et al., 1990. The multilayer perceptron as an approximation

to a bayes optimal discriminant function[J]. IEEE Transactions on Neural Networks, 1(4): 296-298.

RUMELHART D E, HINTON G E, WILLIAMS R J, 1986. Learning representations by back-propagating

errors[J]. Nature, 323(6088): 533.

RUSU A A, RABINOWITZ N C, DESJARDINS G, et al., 2016. Progressive neural networks[J]. arXiv

preprint arXiv:1606.04671.

SAMUEL A, 1959. Some studies in machine learning using the game of checkers[C]//IBM Journal of

Research and Development.

SIMONYAN K, ZISSERMAN A, 2015. Very deep convolutional networks for large-scale image recognition[C]//Proceedings of the International Conference on Learning Representations (ICLR).

SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al., 2014. Dropout: A simple way to prevent neural

networks from overfitting[J]. Journal of Machine Learning Research (JMLR), 15(1): 1929-1958.

SUTSKEVER I, VINYALS O, LE Q V, 2014. Sequence to sequence learning with neural networks[C]//

Proceedings of the Neural Information Processing Systems (Advances in Neural Information Processing

Systems) Conference. 3104-3112.

TIELEMAN T, HINTON G, 2017. Divide the gradient by a running average of its recent magnitude.

coursera: Neural networks for machine learning[R]. Technical Report.

VAN DEN OORD A, DIELEMAN S, ZEN H, et al., 2016. WaveNet: A generative model for raw

audio[C]//Arxiv.

41

第 1 章 深度学习入门

WIERSTRA D, FÖRSTER A, PETERS J, et al., 2010. Recurrent policy gradients[J]. Logic Journal of

the IGPL, 18(5): 620-634.

XU B, WANG N, CHEN T, et al., 2015. Empirical evaluation of rectified activations in convolutional

network[C]//Proceedings of the International Conference on Machine Learning (ICML) Workshop.

YANG Z, DAI Z, YANG Y, et al., 2019. Xlnet: Generalized autoregressive pretraining for language

understanding[C]//Advances in Neural Information Processing Systems. 5754-5764.

YINW, KANN K, YU M, et al., 2017. Comparative study of cnn and rnn for natural language processing[J].

arXiv preprint arXiv:1702.01923.

ZHANG J, LERTVITTAYAKUMJORN P, GUO Y, 2019a. Integrating semantic knowledge to tackle zeroshot text classification[C/OL]//Proceedings of the 2019 Conference of the North American Chapter of

the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and

Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics: 1031-1040. DOI:

10.18653/v1/N19-1108.

ZHANG J, ZHAO Y, SALEH M, et al., 2019b. PEGASUS: Pre-training with extracted gap-sentences for

abstractive summarization[J]. arXiv preprint arXiv:1912.08777.

ZHANG X, ZHAO J, LECUN Y, 2015. Character-level convolutional networks for text classification[C]//

Advances in Neural Information Processing Systems. 649-657.

42

2 强化学习入门

本章将介绍传统强化学习的基础，并概览深度强化学习。我们将从强化学习中的基本定义

和概念开始，包括智能体、环境、动作、状态、奖励函数、马尔可夫（Markov）过程、马尔可夫

奖励过程和马尔可夫决策过程，随后会介绍一个经典强化学习问题——赌博机问题，给读者提供

对传统强化学习潜在机理的基本理解。这些概念是系统化表达强化学习任务的基石。马尔可夫

奖励过程和价值函数估计的结合产生了在绝大多数强化学习方法中应用的核心结果——贝尔曼

（Bellman）方程。最优价值函数和最优策略可以通过求解贝尔曼方程得到，还将介绍三种贝尔曼

方程的主要求解方式：动态规划（Dynamic Programming）、蒙特卡罗（Monte-Carlo）方法和时间

差分（Temporal Difference）方法。

我们进一步介绍深度强化学习策略优化中对策略和价值的拟合。策略优化的内容将会被分为

两大类：基于价值的优化和基于策略的优化。在基于价值的优化中，我们介绍基于梯度的方法，

如使用深度神经网络的深度 Q 网络（Deep Q-Networks）；在基于策略的优化中，我们详细介绍确

定性策略梯度（Deterministic Policy Gradient）和随机性策略梯度（Stochastic Policy Gradient），并

提供充分的数学证明。结合基于价值和基于策略的优化方法产生了著名的 Actor-Critic 结构，这

导致诞生了大量高级深度强化学习算法。

2.1 简介

本章介绍强化学习和深度强化学习的基础知识，包括基本概念的定义和解释、强化学习的一

些基本理论证明，这些内容是深度强化学习的基础。因此，我们鼓励读者能够掌握本章的内容后

再去学习之后的章节。下面，从强化学习的基本概念开始学习。

如图 2.1 所示，智能体（Agent）与环境（Environment）是强化学习的基本元素。环境是智能

43

第 2 章 强化学习入门

体与之交互的实体。如图 2.2 右边所示，一个环境可以是一个雅达利乒乓球游戏（Pong Game）。

智能体控制一个球拍来反弹小球，从而使环境产生变化。智能体的“交互”是通过预先定义好的

动作集合（Action Set）A = {A1, A2...} 来实现的。动作集合描述了所有可能的动作。在这个乒乓

球游戏中，动作集合是球拍 {向上移动, 向下移动}。那么强化学习的目的就是教会智能体如何很

好地与环境交互，从而在预先定义好的评价指标（Evaluation Metric）下获得好的成绩。在乒乓球

游戏中，评价指标是玩家获得的分数。若小球穿过了对手的防线，智能体则获得奖励 r = 1。相

反，若小球穿过了智能体的防线，则智能体获得奖励 r = −1。

图 2.1 智能体与环境

图 2.2 两类游戏环境：围棋（左边）的观测包含了环境状态的所有信息，这个环境是完全可观

测的。雅达利乒乓球（右边）的观测如果只有单帧画面，不能包含小球的速度和运动方

向，这个环境是部分可观测的

我们现在通过图 2.1 来看看智能体与环境的关系细节。在任意的一个时间步（Time Step）

t，智能体首先观测到当前环境的状态 St，以及当前对应的奖励值 Rt。基于这些状态和奖励信

息，智能体决定如何行动。智能体要执行的动作 At 从环境得到新的反馈，获得下一时间步的

状态 St+1 和奖励 Rt+1。对环境状态 s（s 是一个与时间步 t 无关的通用状态表示符号）的观

测（Observation）并不一定能保证包含环境的所有信息。如果观测只包含了环境的局部状态信息

44

2.1 简介

（Partial State Information），这个环境是部分可观测的（Partially Observable）。而如果观测包含了

环境的全部状态信息（Complete State Information），这个环境是完全可观测的（Fully Observable）。

在实践中，观测通常是系统真实状态的函数，使得我们有时很难辨别观测是否包含了状态的所有

信息。一个更容易理解的方法是从信息角度，一个完全可观测的环境不应从整个环境的潜在状态

中遗漏任何信息，而应该可以把所有信息提供给智能体。

为了更好地理解部分可观测环境和完全可观测环境的区别，我们来看两个例子：图 2.2 左边

的围棋游戏是一个典型的完全可观测环境的例子，环境的信息是所有的棋子的位置。而在图 2.2

右边的雅达利乒乓球游戏中，如果观测是单帧画面，就是一个部分可观测环境。这是因为小球的

速度和运动方向并不能从单帧画面中获得。

在很多强化学习的文献中，在环境对智能体是完全可观测的条件下，动作 a（a 是一个与时

间步 t 无关的通用动作表示符号）通常是基于状态 s 表示的智能体动作。而如果环境对智能体是

部分可观测的，智能体不能直接获得环境潜在状态（Underlying State）的信息，因此在没有其他

处理时，动作是基于观测量（Observation）而不是真正状态 s 的。

为了从环境中给智能体提供反馈，一个奖励函数（Reward Function）记为 R，会根据环境状

态而在每一个时间步上产生一个立即奖励（Immediate Reward）Rt，并将其发送给智能体。在一

些情况下，奖励函数只取决于当前的状态，即 Rt = R(St)。例如，在乒乓球游戏中，如果小球穿

过了对手的防线，玩家会立即获得正数的奖励。这个例子中，奖励函数只取决于当前状态，但是

在很多情况下，奖励函数不仅取决于当前状态，而且取决于当前的动作，甚至可能是之前的状态

和动作。一个简单的例子是：如果我们需要一个智能体记住环境中另一个智能体的一系列连续动

作，并重复模仿执行。一个动作的偏差会导致后续状态和动作都难以对齐，那么这个奖励不仅需

要考虑另一个智能体和这个智能体运动过程中的状态-动作对（State-Action Pair），而且需要考虑

状态-动作对的序列。这时，基于当前状态的奖励函数，或者基于当前状态和动作的函数，都无法

对智能体模仿整个连续序列有足够的指导性意义。

在强化学习中，轨迹（Trajectory）是一系列的状态、动作和奖励：

τ = (S0, A0, R0, S1, A1, R1, · · ·)

用以记录智能体如何和环境交互。轨迹的初始状态 S0，是从起始状态分布（Start-State Distribution）

中随机采样而来的，该状态分布记为 ρ0，从而有 S0 ∼ ρ0(·)。例如，雅达利乒乓球游戏开始的状

态总是小球在画面的正中间。而围棋的开始状态则可以是棋子在棋盘上的任意位置。

一个状态到下一个状态的转移（Transition）可以分为：要么是确定性转移过程（Deterministic

Transition Process），要么是 随机性转移过程（Stochastic Transition Process）。对于确定性转移过

程，下一时刻的状态 St+1 由一个确定性函数支配：

St+1 = f(St, At), (2.1)

45

第 2 章 强化学习入门

其中 St+1 是唯一的下一个状态。而对于随机性转移过程，下一时刻的状态 St+1 是用一个概

率分布（Probabilistic Distribution）来描述的：

St+1 ∼ p(St+1|St, At) (2.2)

而下一时刻的实际状态是从其概率分布中采样得到的。

一个轨迹有时候也称为片段（Episode）或者回合，是一个从初始状态（Initial State）到最终

状态（Terminal State）的序列。比如，玩一整盘游戏的过程可以看作一个片段，若智能体赢了或

者输了这盘游戏，则到达最终状态。在一些时候，一个片段可以是由多局子游戏（Sub-Games）组

成的（而不仅仅是一盘游戏）。比如在雅达利乒乓球游戏中，一个片段可以包含多个回合。

我们用两个重要的概念来结束本小节：探索（Exploration）与利用（Exploitation，有时候也

叫守成），以及一个著名的概念：探索-利用的权衡（Exploration-Exploitation Trade-off）。利用指的

是使用当前已知信息来使智能体的表现达到最佳，而智能体的表现通常是用期望奖励（Expected

Reward）来评估的。举例来说，一个淘金者发现了一个每天能提供两克黄金的金矿，同时他也知

道最大的金矿可以每天提供五克黄金。但是如果他花费时间去找更大的金矿，就需要停下挖掘当

前的金矿，这样的话如果找不到更大的金矿，那么在找矿耗费的时间中就没有任何收获。基于这

位淘金者的经验，去探索新的金矿会有很大的风险，淘金者于是决定继续挖掘当前的金矿来最大

化他的奖励（这个例子中奖励是黄金的数量），他放弃了探索而选择了利用。淘金者选择的策略

（Policy）是贪心（Greedy）策略，即智能体持续地基于当前已有的信息来执行能够最大化期望奖

励的动作，而不去做任何的冒险行为，以免导致更低的期望奖励。

探索是指通过与环境交互来获得更多的信息。回到淘金者的例子中，探索指的是淘金者希望

花费一些时间来寻找新的金矿，而如果他找到更大的金矿，那么他每天能获得更多的奖励。但是

为了获得更大的长期回报（Long-Term Return），短期回报（Short-Term Return）可能会被牺牲。淘

金者需要面对在探索与利用间抉择的难题，要决定当一个金矿产量为多少时应当进行利用而少于

多少时应当开始探索。上述的例子描述了探索-利用的权衡问题，这个问题关乎智能体如何平衡探

索和利用，是强化学习研究非常重要的问题。我们下面进一步通过赌博机问题（Bandit Problem）

来讨论它。

2.2 在线预测和在线学习

2.2.1 简介

在线预测（Online Prediction）问题是一类智能体需要为未来做出预测的问题。假如你在夏威

夷度假一周，需要预测这一周是否会下雨；或者根据一天上午的石油价格涨幅来预测下午石油的

价格。在线预测问题需要在线解决。在线学习和传统的统计学习有以下几方面的不同：

• 样本是以一种有序的（Ordered）方式呈现的，而非无序的批（Batch）的方式。

46

2.2 在线预测和在线学习

• 我们更多需要考虑最差情况而不是平均情况，因为我们需要保证在学习过程中随时都对事

情有所掌控。

• 学习的目标也是不同的，在线学习企图最小化后悔值（Regret），而统计学习需要减少经验

风险。我们会稍后对后悔值进行介绍。

如图 2.3 左侧所示，单臂赌博机（Single-Armed Bandit）是一种简单的赌博机，智能体通过下

拉机械手臂来和这个赌博机进行互动。当这个机器到达头奖的时候，这个智能体就会得到一个奖

励。在赌场中，我们常常能看见很多赌博机被摆在一排。一个智能体就可以选择去下拉其中任何

一只手臂。奖励值 r 的分布 P(r|a) 以动作 a 为条件，它对于不同的赌博机来说是不同的，但是

对某一台赌博机来说是固定的。智能体在一开始是不知道奖励分布的，而只能通过不断的实验和

尝试来增进对分布的了解。智能体的目标是将其做出一些选择后所得到的奖励最大化。智能体需

要在每个时间步上从众多的赌博机中进行选择，我们把这种游戏称为多臂赌博机（Multi-Armed

Bandit，MAB），如图 2.3 中右侧所示。MAB 给予了一个智能体有策略地选择拉下哪一根拉杆的

自由。

图 2.3 单臂赌博机（左）与多臂赌博机（右）

我们尝试通过一般的强化学习方法来解决 MAB 问题。智能体的动作 a 用来选择具体拉哪一

根拉杆。在这个动作完成以后，它会得到一个奖励值。在时间步 t 的一个动作 a 的价值定义为

q(a) = E[Rt|At = a]

我们试图用它来选择动作。如果我们知道了每个动作 a 的真实的动作值 q(a)，那么解决这个问题

就很简单，只需始终选择对应最大 q 值的动作即可。然而，现实中我们往往要估计 q 值，把它的

估计值写为 Q(a)，而 Q(a) 值应当尽可能接近 q(a) 的值。

对于展示探索-利用的权衡问题，MAB 可以作为一个很好的例子。当我们已经对一些状态的

q 值进行估计之后，如果一个智能体一直选择有最大 Q 值的动作的话，那么这个智能体就是贪心

的（Greedy），因为它一直在利用已经估计过的 q 值。如果一个智能体总是根据最大化 Q 值来选

取动作，那么我们认为这样的智能体是有一定探索（Exploration）性的。只做探索或者只对已有

估计值进行利用（Exploitation），在大多数情况下都不能很好地改善策略。

47

第 2 章 强化学习入门

一个种单的基于动作价值的（Action-Value Based）方法是，通过将在时间 t 前选择动作 a 所

获得的总体奖励除以这个动作被选择的次数来估算 Qt(a) 的值：

Qt(a) = 在时间 t 前选择动作 a 的奖励值的总和

在时间 t 前动作 a 被选择的次数 =

Pt−1

i=0 Ri

· 1Ai=a Pt−1

i=0 1Ai=a

1x 的值在 x 为真时为 1，否则为 0。一种贪心的策略可以写成：

At = arg max

a

Qt(a) (2.3)

然而，我们也可以把这个贪心策略转化成有一定探索性的策略，即让它以 ϵ 的概率去探索其

他动作。我们把这种方法叫作 ϵ-贪心（ϵ-Greedy），因为它在概率为 ϵ 的情况下随机选择一个动作，

而在其他情况下，它的动作是贪心的。如果我们有无限的时间步长，那么就可以保证 Qt(a) 收敛

为 q(a)。更重要的是，这个简单的基于动作-价值的方法也是一种基于在线学习（Online Learning）

的方法。

让我们以多臂赌博机问题为例来具体介绍在线学习。假设我们在每个时间步 t 上观测到了回

报 Rt，一个简单的用来找最佳动作的想法是通过 Rt 和 At 来更新 q 的估计。之前介绍的用来计

算平均值的办法是对所有在时间 t 之前选择 At 的奖励值求和，然后除以 At 出现的次数。这样

更像一个批量学习，因为每一次我们都得对一批数据点进行重新计算。在线学习的方法则利用一

个移动的平均值，每次运算都基于之前的估算结果，如 Qi(At) = Qi(At) − Qi(At)/N; Qi+1(At)

= Qi(At) + Rt/N。Qi 是在 At 被选择过 i 次以后的 q 估计值，N 是 At 被选择的次数。

2.2.2 随机多臂赌博机

当我们有 K ⩾ 2 只机器手臂时，我们需要在每个时间步上 t = 1, 2, · · · , T 下拉一只手臂。在

任何时间 t，如果我们下拉的手臂是第 i 只，那么相应地也会观察到奖励 Ri

t。

算法 2.3 多臂赌博机学习

初始化 K 只手臂

定义总时长为 T

每一只手臂都有一个对应的 vi ∈ [0, 1]。每一个奖励都是独立同分布地从 vi 中采样得到的

for t = 1, 2, · · · , T do

智能体从 K 只手臂中选择 At = i

环境返回奖励值向量 Rt = (R1

t

, R2

t

, · · · , RK

t

)

智能体观测到 Ri

t

end for

从传统意义上来说，我们会尝试最大化奖励值。但是在随机多臂赌博机（Stochastic MultiArmed Bandit）里，我们会关注另外一个指标（Metric），即后悔值（Regret）。在 n 步之后的后悔

48

2.2 在线预测和在线学习

值被定义为

REn = max

j=1,2,··· ,K

Xn

t=1

R

j

t −

Xn

t=1

R

i

t

第一项是我们走到 n 步之后，每一次都能获得的最大奖励值之和；第二项是在 n 步中，真实获得

的奖励之和。

因为我们的动作和回报带来了随机性，为了选择最好的动作，我们应该尝试最小化后悔值的

期望值。我们需要把两种不同的后悔值的期望值区分开来：后悔值和伪后悔值（Pseudo-Regret）

的期望值。我们将后悔值的期望值定义为

E[REn] = E[ max

j=1,2,··· ,T

Xn

t=1

R

j

t −

Xn

t=1

R

i

t

] (2.4)

我们将伪后悔值的期望值定义为

REn = max

j=1,2,··· ,T

E





Xn

t=1

R

j

t −

Xn

t=1

R

i

t



 (2.5)

以上两种后悔值最主要的区别在于它们最大化和计算期望值的顺序是不一样的。后悔值的期

望值会相对更难计算一些，这是因为对于伪后悔值来说，我们只需要优化后悔值的期望值；而对

于后悔值的期望值来说，我们则需要每次试验时都找到最优的后悔值再取期望值。而这两个值满

足一定关系，即 E[REn] ⩾ REn。

定义 µi 为 vi 的平均值，而 vi 是第 i 只手臂的奖励值，µ

∗ = maxi=1,2,··· ,T µi。在一个随机的

环境下，我们把公式 (2.5) 改写为

REn = nµ∗ − E





Xn

t=1

R

i

t



 (2.6)

一种最小化伪后悔值的方法是选择最好的那只手臂来下拉，并通过之前介绍的 ϵ-贪心策略来

获得样本。一种更先进的方法叫作置信上界（Upper Confidence Bound，UCB）算法。置信上界算

法使用霍夫丁引理（Hoeffding’s Lemma）来估计置信上界，然后选择那只基于目前估计对应最大

奖励平均值的手臂。

我们现在开始介绍置信上界策略。具体关于置信上界算法在随机多臂赌博机中对后悔值的优

化可以在 (Bubeck et al., 2012) 中找到。我们现在来具体看一看置信上界基于奖励进行策略优化的

过程。尽管在随机 MAB 里，奖励是从一个分布中采样得到的，这个奖励函数分布在时间上是稳

定的。以 ϵ-贪心策略为例，ϵ-贪心以一定概率（值为 ϵ）来探索那些非最优动作，但问题是，它认

49

第 2 章 强化学习入门

为所有的非最优动作都是一样的，从而不对这些动作进行任何区别对待。如果我们想尝试每一个

动作，则可能需要优先尝试那些没有采用过的或者采用次数更少的动作。置信上界算法通过改写

公式 (2.3) 来解决这个问题:

At = arg max

a



Qt(a) + c

s

ln t

Nt(a)



 (2.7)

式中，Nt(a) 是动作 a 在到时间 t 前被选择的次数；c 是一个决定还需要进行多少次探索的正实

数。如果我们有一个稳定的奖励函数分布，可以通过公式 (2.7) 来选择动作。当 Nt(a) 为零时，认

为动作 a 有最大值。为了更好地了解置信上界算法的具体运作方式，平方根项反映了我们对 a 的

q 值估算的不确定性：随着 a 被选择的次数增加，它的不确定性也在减小。同样地，当除 a 外的

动作被选择后，不确定性就变大了，因为 ln t 增大但是 Nt(a) 保持不变。t 的自然对数使得新的

时间步的影响越来越小。置信上界算法给出动作 q 值的上限，而 c 表示置信程度。

2.2.3 对抗多臂赌博机

随机 MAB 的回报函数是用随时间不变的概率分布来表示的。但是在现实中，这个条件往往不

成立。因此，在奖励函数不再简简单单地由一些稳定概率分布决定，而由一个对抗者（Adversary）

决定的情况下，我们需要研究对抗多臂赌博机（Adversarial Multi-Armed Bandit）。在对抗多臂赌

博机的情景中，第 i 只手臂在时间 t 上的奖励为 Ri

t ∈ [0, 1]。同时一个玩家在 t 时所拉的手臂会被

写作 It ∈ {1, 2, · · · , K}。

有人可能会想，万一对抗者干脆把所有的奖励都设为 0 了呢？如果这种情况发生，就没有人

可以得到任何的奖励。事实上，就算对抗者可以自由决定奖励的多少，也不会把所有的奖励都设

为 0，反之给玩家足够多的奖励作为诱惑，让他们有赢的感觉，但是其实玩了许多轮后，最终还

是对抗者获利。

算法 2.4 是对抗多臂赌博机的基本设定。在每一个时间步上，智能体都会选择一只手臂 It，

而对抗者会决定在这个时刻的奖励值向量 Rt。这个智能体有可能只能观测到它所选择的手臂的

奖励 R

It

t ，也有可能观测到每一个机器的奖励 Rt(·)。分两点来完整描述这个问题。第一点是，

一个对抗者到底对一个玩家之前的动作选择了解多少。这个很重要，对抗者可能会为了获得更

多的利益根据玩家的动作来调整机器。我们将那些不考虑过去玩家历史的对抗者叫作健忘对抗

者（Oblivious Adversary），而将那些考虑过去历史的对抗者叫作非健忘对抗者（Non-Oblivious

Adversary）。第二点是一个玩家能够了解到奖励值向量的多少内容。我们将那些玩家知道关于奖

励值向量的全部信息的情况叫作全信息博弈（Full-Information Game），而将那些玩家只知道一部

分回报向量信息的情况叫作部分信息博弈（Partial-Information Game）。

健忘对抗者和非健忘对抗者的区别，只对一个非确定性（Non-Deterministic）玩家才显现出来。

如果我们有一个确定性玩家，或者一个玩家的策略不变，一个对抗者就很容易让后悔值 RE ⩾ n/2，

50

2.2 在线预测和在线学习

其中 n 代表这个玩家下拉手臂的次数。所以，全信息非确定性玩家更有研究价值，可以使用 Hedge

算法来解决这个问题。

算法 2.4 对抗多臂赌博机

初始化 K 只机器手臂

for t = 1, 2, · · · , T do

智能体在 K 只手臂当中选中 It

对抗者选择一个奖励值向量 Rt = (R1

t

, R2

t

, · · · , RK

t

) ∈ [0, 1]K

智能体观察到奖励 R

It

t （根据具体的情况也有可能看到整个奖励值向量）

end for

在算法 2.5 中，我们首先把每只手臂的函数 G 都设为零，然后使用 Softmax 来获得一个新

动作的概率密度函数（Probability Density Function）。η 是一个用来控制温度的正值参数。G 函数

更新是通过把所有的手臂的新奖励值都加起来，从而使有最高奖励值的手臂有最大的概率被选中

的。我们把这个算法叫作 Hedge。Hedge 也是部分信息博弈方法的一个基础。如果我们把一个智能

体的观察局限到只有 Ri

t，那么就需要把奖励标量扩展成一个向量，这样它才可以被 Hedge 使用。

探索和利用的指数加权算法（Exponential-Weight Algorithm for Exploration and Exploitation，Exp3）

即为一个基于 Hedge 来解决不完全信息博弈的算法。它进一步利用了 p(t) 和平均分布（Uniform

Distribution）的结合来确保所有的机器都会被选到，达到了平衡探索和利用的目的。文献 (Auer

et al., 1995) 中有关于探索和利用的指数加权算法更详尽的介绍。

算法 2.5 针对对抗多臂赌博机的 Hedge 算法

初始化 K 只手臂

Gi(0) for i = 1, 2, · · · , K

for t = 1, 2, · · · , T do

智能体从 p(t) 分布中选择 At = it，其中

pi(t) = exp(ηGi(t − 1))

PK

j

exp(ηGj (t − 1))

智能体观测到奖励 gt

让 Gi(t) = G(t − 1) + g

i

t

, ∀i ∈ [1, K]

end for

2.2.4 上下文赌博机

上下文赌博机（Contextual Bandit）有的时候也被叫作关联搜索（Associative Search）任务。我

们把关联搜索任务和非关联搜索（Non-Associative Search）任务放在一起，以更好地了解它们的

意义。我们刚刚所描述的多臂赌博机就是一个非关联搜索任务。当一个任务的奖励函数是稳定的

51

第 2 章 强化学习入门

时候，我们只需要找到那个最好的动作。当一个任务是不稳定的时候，我们就需要把它的变化记

录下来，这个是非关联搜索任务的范畴。对于强化学习问题，事情会变得复杂很多。假设有几个

多臂赌博机任务，我们需要在每一个时间点来选择其中的一个任务。虽然我们仍然可以估算奖励

的期望值，得到的表现未必会达到最优。在这种情况下，我们最好把一些特征和赌博机已经学习

到的奖励期望值联系起来。试想一下，如果每一个机器在不同时间都有一个 LED 灯来发出不同

颜色的灯光，如果当赌博机亮红灯时总是比亮蓝灯时给出更大的奖励值，那么我们就可以把这些

信息和动作选择策略联系起来辅助动作选择，即可以选择那些红灯亮得更多的机器。

上下文赌博机是介于多臂赌博机和完整的强化学习两者之间的问题。它和多臂赌博机有很多

类似点，比如它们的动作都只影响立即奖励（Immediate Reward）。上下文赌博机也和完整强化学

习设置类似，因为两者都需要学习一个策略函数。如果要把一个上下文赌博机变成一个完整的强

化学习任务，那么动作将不只是影响立即奖励，也会影响未来的环境状态。

2.3 马尔可夫过程

2.3.1 简介

马尔可夫过程（Markov Process，MP）是一个具备马尔可夫性质（Markov Property）的离散随

机过程（Discrete Stochastic Process）。图 2.4 展示了一个马尔可夫过程的例子。每个圆圈表示一个

状态，每个边（箭头）表示一个状态转移（State Transition）。这个图模拟了一个人做两种不同的

任务（Tasks），以及最后去床上睡觉的这样一个例子。为了更好地理解这个图，我们假设这个人

当前的状态是在做“Task1”，他有 0.7 的概率会转到做“Task2”的状态；如果他进一步从“Task2”

以 0.6 的概率跳转到“Pass”状态，则这个人就完成了所有任务可以去睡觉了，因为“Pass”到

“Bed”的概率是 1。

图 2.5 用概率图模型（Probabilistic Graphical Model）来表示马尔可夫过程，后面的章节会经

常使用这种表达方式。在概率图模型中，本书统一使用圆形来表达变量，单向箭头来表达两个变

量的关系。例如，“a → b”表示的是变量 b 依赖于变量 a。空白圆形中的变量表示一个常规变量，

而有阴影圆形的变量表示一个观测变量（Observed Variable）（这在随后的 2.7 节图片中展示），观

测变量可以为其他常规变量的推理过程提供了信息。包含一些变量圆圈在内的实体黑色方框表示

这些变量是重复的，同样可以在随后的图片中看到。概率图模型可以帮助我们对强化学习中变量

关系有更直观的理解，以及在我们对沿着 MP 链的不同变量求导梯度时提供细致的参考。

马尔可夫过程基于马尔可夫链（Markov Chain）的假设，下一状态 St+1 只取决于当前状态

St。一个状态跳转到下一状态的概率如下：

p(St+1|St) = p(St+1|S0, S1, S2, · · · , St) (2.8)

52

2.3 马尔可夫过程

Task2

s = t2

Task1

s = t1

Pass

s = p

Rest

s = r

Bed

s = b

Game

s = g

0.9

0.1

1

0.3

0.7

0.3

0.6

0.1

1

0.1

0.9

图 2.4 马尔可夫过程例子。s 表示当前状态，箭头上的数值表示从一个状态转移到另一个状态的

概率

St−1 St St+1

p(St|St−1) p(St+1|St)

图 2.5 马尔可夫过程的概率图模型：t 表示时间步，p(St+1|St) 表示状态转移概率

这个式子描述了“无记忆的（Memoryless）”的特性，即马尔可夫链的马尔可夫性质（Markov

Property）。如果 p(St+2 = s

′

|St+1 = s) = p(St+1 = s

′

|St = s) 对任意时间步 t 和所有可能状态

成立，那么它是一个沿时间轴的稳定转移函数（Stationary Transition Function），称为时间同质性

（Time-Homogeneous），而相应的马尔可夫链为时间同质马尔可夫链（Time-Homogeneous Markov

Chain）。

我们也常用 s

′ 来表示下一个状态，在一个时间同质马尔可夫链中，在时间 t 由状态 s 转移到

时间 t + 1 的状态 s

′ 的概率满足：

p(s

′

|s) = p(St+1 = s

′

|St = s) (2.9)

时间同质性是对本书中大多数推导的一个基本假设，我们在后续绝大多数情况中默认满足

这一假设而不再提及。然而，实践中，时间同质性可能不总是成立的，尤其是对非稳定的（NonStationary）环境、多智能体强化学习（Multi-Agent Reinforcement Learning）等，而这些时候会涉

及时间不同质（Time-Inhomogeneous）的情况。

给定一个有限的状态集（State Set）S，我们有一个状态转移矩阵（State Transition Matrix）P。

53

第 2 章 强化学习入门

比如，图 2.4 例子中对应的 P 如下所示：

g t1 t2 r p b

P =



































0.9 0.1 0 0 0 0

0.3 0 0.7 0 0 0

0 0 0 0.1 0.6 0.3

0 0.1 0.9 0 0 0

0 0 0 0 0 1

0 0 0 0 0 1



































g

t1

t2

r

p

b

其中 Pi,j 是当前状态 Si 到下一状态 Sj 的转移概率。例如，图 2.4 中状态 s = r 跳转到状态

s = t1 有 0.1 的概率，跳转到状态 s = t2 的概率为 0.9。P 是一个方矩阵，每一行的和为 1。这

个转移概率矩阵表示整个转移过程是随机的（Stochastic）。马尔可夫过程可以用一个元组来表示

< S, P >。现实中的很多简单过程可以用这样一个随机过程来近似，而这也正是强化学习方法的

基础。数学上来说，下一时刻状态可以从 P 中采样，如下：

St+1 ∼ PSt,· (2.10)

其中符号 ∼ 表示下一个状态 St+1 是随机地从类别分布（Categorical Distribution）PSt,· 中采

样得到的。

对于状态集合无限大的情况（例如说状态空间是连续的），一个有限的矩阵无法完整地表达

这样状态转移的关系。因此可以使用转移函数 p(s

′

|s)，其与有限状态时的转移矩阵有对应关系，

如 p(s

′

|s) = Ps,s′。

2.3.2 马尔可夫奖励过程

在马尔可夫过程中，虽然智能体可以通过状态转移矩阵 Ps,s′ = p(s

′

|s) 来实现与环境的交

互，但是马尔可夫过程并不能让环境提供奖励反馈给智能体。为了提供反馈，马尔可夫奖励过程

（Markov Reward Process，MRP）把马尔可夫过程从 < S, P > 拓展到 < S, P , R, γ >。其中 R 和

γ 分别表示奖励函数（Reward Function）和奖励折扣因子（Reward Discount Factor）。图 2.6 是一

个马尔可夫奖励过程的例子。图 2.7 是马尔可夫奖励过程的图模型，奖励函数取决于当前的状态：

Rt = R(St) (2.11)

54

2.3 马尔可夫过程

r = −2

Task2

s = t2

r = −2

Task1

s = t1

r = 10

Pass

s = p

r = 1

Rest

s = r

r = 0

Bed

s = b

r = −1

Game

s = g

0.9

0.1

1

0.3

0.7

0.3

0.6

0.1

1

0.1

0.9

图 2.6 马尔可夫奖励过程的例子：s 表示当前的状态，r 表示每一个状态的立即奖励（Immediate

Reward）。箭头边上的数值表示从一个状态到另一个状态的概率

St−1 St St+1

Rt−1 Rt Rt+1

p(St|St−1) p(St+1|St)

图 2.7 马尔可夫奖励过程的图模型

在这个模型中，奖励仅取决于当前状态，而当前状态是基于之前状态和之前动作产生的结果。

为了更好地理解奖励是状态的函数，我们看看如下的例子。如果智能体能通过（Pass）考试，智能

体可以获得的立即奖励为 10，休息（Rest）能获得立即奖励为 1，但如果智能体执行任务（Task）

会损失值为 2 的立即奖励。给定一个轨迹 τ 上每个时间步的立即奖励 r，回报（Return）是一个

轨迹的累积奖励（Cumulative Reward）。严格来说，非折扣化的回报（Undiscounted Return）在一

个有 T 时间步长的有限过程中的值如下：

Gt=0:T = R(τ ) = X

T

t=0

Rt (2.12)

其中 Rt 是 t 时刻的立即奖励，T 是最终状态的步数，或者是整个片段的步数，例如，轨迹

(g, t1, t2, p, b) 的非折扣化的回报是 5 = −1 − 2 − 2 + 10。需要注意的是，一些文献使用 G 来表示

55

第 2 章 强化学习入门

回报，而用 R 来表示立即奖励，但在本书中，我们用 R 来表示奖励函数（Reward Function）。因

此，在本书中 Rt = R(St) 是时间步 t 时候的立即奖励，而 R(τ ) = Gt=0:T 表示长度为 T 的轨迹

τ0:T 的回报，r 是立即奖励的通用表达。

通常来说，距离更近的时间步比相对较远的时间步会产生更大的影响。这里我们介绍折扣化

回报（Discounted Return）的概念。折扣化回报是奖励值的加权求和，它对更近的时间步给出更

大的权重。定义折扣化回报如下：

Gt=0:T = R(τ ) = X

T

t=0

γ

tRt. (2.13)

其中奖励折扣因子（Reward Discount Factor）γ ∈ [0, 1] 被用来实现随着时间步的增加而减

小权重值。举例来说，图 2.6 中，当 γ = 0.9，且轨迹为 (g, t1, t2, p, b) 时，折扣回报为 2.87 =

−1 − 2 × 0.9 − 2 × 0.9

2 + 10 × 0.9

3。如果 γ = 0，则回报值只与当前的立即奖励有关，智能体会

非常“短视”。如果 γ = 1，就是非折扣化的回报。当处理无限长 MRP 情况时，这个折扣因子会

非常关键，因为它能避免回报值随着时间步增大到无穷而增大到无穷，从而使得无限长 MRP 过

程是可评估的。

对折扣因子的 γ 另一个理解角度是：为了简便，奖励折扣因子 γ 有时在文献 (Levine, 2018)

中的离散时间有限范围 MRP 的情况下被省去，而这时折扣因子也可以理解为被并入了动态过程

中，通过直接修改转移动态函数来使得任何产生转移至一个吸收状态（Absorbing State）的动作

都有概率 1 − γ，而其他标准的转移概率都乘以 γ。

价值函数（Value Function）V (s) 是状态 s 的期望回报（Expected Return）。举例来说，如果

下一步有两个不同的状态 S1 和 S2，基于当前策略评估它们的价值分别为 V

π

(S1) 和 V

π

(S2)。智

能体的策略通常是选择价值更高的状态作为下一步。如果智能体的行动基于某种策略 π，我们把

相应的价值函数写为 V

π

(s)：

V

π

(s) = Eτ∼π[R(τ )|S0 = s] (2.14)

对于状态 s 而言，它的价值是以它为初始状态下回报的期望，而这个期望是对策略 π 给出

的轨迹所求的。一种估计价值 V (s) 的简单方法是蒙特卡罗法，给定一个状态 s，我们用状态转

移矩阵 P 随机采样大量的轨迹，来求近似期望。以图 2.6 为例，给定 γ = 0.9 和 P，如何估计

V

π

(s = t2)？我们可以如下随机采样出 4 个轨迹（注意，实际中采样的轨迹要远大于 4，但这里

为了描述方法，我们只采样 4 个轨迹）：

• s = (t2, b), R = −2 + 0 × 0.9 = −2

• s = (t2, p, b), R = −2 + 10 × 0.9 + 0 × 0.9

2 = 7

• s = (t2, r, t2, p, b), R = −2 + 1 × 0.9 − 2 × 0.9

2 + 10 × 0.9

3 + 0 × 0.9

4 = 4.57

• s = (t2, r, t1, t2, b), R = −2 + 1 × 0.9 − 2 × 0.9

2 − 2 × 0.9

3 + 0 × 0.9

4 = −0.178

56

2.3 马尔可夫过程

给定这些 s = t2 为初始状态的轨迹，我们可以计算每个轨迹的回报 R，然后估计出状态

s = t2 的期望回报 V (s = t2) = (−2 + 7 + 4.57 − 0.178)/4 = 2.348，作为状态 s = t2 的价值衡量。

图 2.8 用这个方法估算出每个状态的期望回报。给定这些期望回报，一个最简单的智能体策

略是每一步都往期望回报更高的状态移动。这样所产生的动作就是最大化期望回报，见图 2.8 的

虚线箭头。除了蒙特卡罗方法，还有很多方法可以用来计算 V (s)，比如贝尔曼期望方程（Bellman

Expectation Equation）、逆矩阵方法（Inverse Matrix Method）等，我们将会在稍后逐一介绍。

图 2.8 马尔可夫奖励过程和价值估计函数 V (s)：每个状态都随机采样 4 个轨迹，用蒙特卡罗方

法估算每个状态的价值。虚线箭头表示学习出的简单策略，则智能体往价值更高的状态

移动

2.3.3 马尔可夫决策过程

马尔可夫决策过程（Markov Decision Process，MDP）从 20 世纪 50 年代已经开始被广泛地

研究，在包括经济、控制理论和机器人等很多领域都有应用。在模拟序列决策过程的问题上，马

尔可夫决策过程比马尔可夫过程和马尔可夫奖励过程要好用。如图 2.9 所示，和马尔可夫奖励过

程不同的地方在于，马尔可夫奖励过程的立即奖励只取决于状态（奖励值在节点上），而马尔可

夫决策过程的立即奖励与状态和动作都有关（奖励值在边上）。同样地，给定一个状态下的一个

动作，马尔可夫决策过程的下一个状态不一定是固定唯一的。举例来说，如图 2.10 所示，当智能

体在状态 s = t2 时执行休息（rest）动作后，下一时刻的状态有 0.8 的概率保留在状态 s = t2 下，

有 0.2 的概率变为 s = t1。

57

第 2 章 强化学习入门

play

play sleep sleep

work

work

rest

work

图 2.9 马尔可夫决策过程例子。在马尔可夫奖励过程中，立即奖励只与状态有关。而马尔可夫

决策过程的立即奖励与状态和动作都有关

St−1 St St+1

p(St|St−1, At−1) p(St+1|St, At)

p(At−1|St−1) At−1 At At+1

Rt−1 Rt Rt+1

图 2.10 马尔可夫决策过程的图模型：t 表示时间步，p(At|St) 表示根据当前状态 St 选择的动作

At 的概率，p(St+1|St, At) 是基于当前状态和动作下的状态转移概率。虚线表示智能体

的决策过程

之前说过，马尔可夫过程可以看成一个元组 <S, P >，而马尔可夫奖励过程是 <S, P , R, γ>，

其中状态转移矩阵的元素（Element）值是 Ps,s′ = p(s

′

|s)。这个表示将有限维（Finite-Dimension）

状态转移矩阵拓展成无穷维（Infinite-Dimension）概率函数。这里，马尔可夫决策过程是 <S, A, P ,

R, γ>，其状态转移矩阵的元素变为

p(s

′

|s, a) = p(St+1 = s

′

|St = s, At = a) (2.15)

58

2.3 马尔可夫过程

例如图 2.9 中很多状态转移概率为 1，比如 p(s

′ = t2|s = t1, a = work) = 1；但是也有一些不

是，比如 p(s

′

|s = t2, a = rest) = [0.2, 0.8]，它表示的是，如果智能体在状态 s = t2 下执行动作

a = rest，它有 0.2 的概率会跳到状态 s

′ = t1，而有 0.8 的概率会保持原来的状态。那些不存在的

边代表转移概率为 0，比如 p(s

′ = t2|s = t1, a = rest) = 0。

A 表示有限的动作集合（Finite Action Set）{a1, a2, · · · }，则立即奖励变成

Rt = R(St, At) (2.16)

一个策略（Policy）表示智能体根据它对环境的观测来行动的方式。具体来说，策略是从每

一个状态 s ∈ S 和动作 a ∈ A 到动作概率分布 π(a|s) 的映射，这个概率分布是在状态 s 下采取动

作 a 的概率，可以写为

π(a|s) = p(At = a|St = s), ∃t (2.17)

期望回报（Expected Return）是在一个策略下给定所有可能轨迹的回报的期望值，强化学习

的目的就是通过优化策略来使得期望回报最大化。数学上来说，给定起始状态分布 ρ0 和策略 π，

马尔可夫决策过程中一个 T 步长的轨迹的发生概率是：

p(τ |π) = ρ0(S0)

T

Y−1

t=0

p(St+1|St, At)π(At|St) (2.18)

给定奖励函数 R 和所有可能的轨迹 τ，期望回报 J(π) 可以定义为

J(π) = Z

τ

p(τ |π)R(τ ) = Eτ∼π[R(τ )] (2.19)

其中 p 表示轨迹发生的概率，发生概率越高，则对期望回报计算的权重越大。强化学习优

化问题（RL Optimization Problem）通过优化方法来提升策略，从而最大化期望回报。最优策略

（Optimal Policy）π

∗ 可以表示为

π

∗ = arg max

π

J(π) (2.20)

其中 ∗ 符号在本书中表示“最优的”含义。

给定一个策略 π，价值函数 V (s)，即给定状态下的期望回报，可以定义为

V

π

(s) =Eτ∼π[R(τ )|S0 = s]

59

第 2 章 强化学习入门

=EAt∼π(·|St)





X∞

t=0

γ

tR(St, At)|S0 = s



 (2.21)

其中 τ ∼ π 表示轨迹 τ 是通过策略 π 采样获得的，At ∼ π(·|St) 表示动作是在一个状态下从

策略中采样得到的（如果策略是有随机性的），下一个状态取决于状态转移矩阵 P 及其状态 s 和

动作 a。

在马尔可夫决策过程中，给定一个动作，就有动作价值函数（Action-Value Function），这个

函数依赖于状态和刚刚执行的动作，是基于状态和动作的期望回报。如果一个智能体根据策略 π

来运行，则把动作价值函数写为 Qπ

(s, a)，其定义为

Q

π

(s, a) =Eτ∼π[R(τ )|S0 = s, A0 = a]

=EAt∼π(·|St)





X∞

t=0

γ

tR(St, At)|S0 = s, A0 = a



 (2.22)

我们需要记住的是，Qπ

(s, a) 是基于策略 π 来估计的，因为对值的估计是策略 π 所决定的轨

迹上的期望。也就是说，如果策略 π 改变了，Qπ

(s, a) 也会相应地跟着改变。因此我们通常称基

于一个特定策略估计的价值函数为在线价值函数（On-Policy Value Function），来与用最优策略估

计的最优价值函数（Optimal Value Function）进行区分。

我们可以发现价值函数 vπ(s) 和动作价值函数 qπ(s, a) 之间有如下关系：

qπ(s, a) = Eτ∼π[R(τ )|S0 = s, A0 = a] (2.23)

vπ(s) = Ea∼π[qπ(s, a)] (2.24)

有两种简单方法来计算价值函数 vπ(s) 和动作价值函数 qπ(s, a)：第一种方法是穷举法（exhaustive method），如公式 (2.18) 所示，首先计算出从一个状态开始的所有可能轨迹的概率，然后

用公式 (2.21) 和 (2.22) 来计算出这个状态的 V

π

(s) 和 Qπ

(s, a)。每个状态都用穷举法来单独计算。

然而实际中，可能的轨迹数量是非常大的，甚至是无穷个的。因此除了使用所有可能的轨迹，第

二种方法是使用之前介绍的蒙特卡罗方法通过采样大量的轨迹来估计 V

π

(s)。这两种方法都非常

简单，但都有各自的缺点。而实际上，估计价值函数的公式可以根据马尔可夫性质做进一步的简

化，即下一小节要介绍的贝尔曼方程。

60

2.3 马尔可夫过程

2.3.4 贝尔曼方程和最优性

贝尔曼方程

贝尔曼方程（Bellman Equation），也称为贝尔曼期望方程，用于计算给定策略 π 时价值函

数在策略指引下所采轨迹上的期望。我们称之为“在线（On-Policy）”估计方法（注意它与之后

的在线策略和离线策略更新区分），因为强化学习中的策略一直是变化的，而价值函数（Value

Function）是以当前策略为条件或者用其估计的。

回想状态价值函数或动作价值函数（Action-Value Function）的定义，即vπ(s) = Eτ∼π [R(τ )|S0 = s]

和 qπ(s, a) = Eτ∼π[R(τ )|S0 = s, A0 = a]，我们可以利用递归关系得出在线状态价值函数（OnPolicy State-Value Function）的贝尔曼方程：

vπ(s) = Ea∼π(·|s),s′∼p(·|s,a)

[R(τt:T )|St = s]

= Ea∼π(·|s),s′∼p(·|s,a)

[Rt + γRt+1 + γ

2Rt+2 + · · · + γ

TRT |St = s]

= Ea∼π(·|s),s′∼p(·|s,a)

[Rt + γ(Rt+1 + γRt+2 + · · · + γ

T −1RT )|St = s]

= Ea∼π(·|s),s′∼p(·|s,a)

[Rt + γRτt+1:T

|St = s]

= EAt∼π(·|St),St+1∼p(·|St,At)

[Rt + γEa∼π(·|s),s′∼p(·|s,a)

[Rτt+1:T

]|St = s]

= EAt∼π(·|St),St+1∼p(·|St,At)

[Rt + γvπ(St+1)|St = s]

= Ea∼π(·|s),s′∼p(·|s,a)

[r + γvπ(s

′

)] (2.25)

上式最后一个等式成立，是因为 s, a 是对状态和动作的一般表示，而 St, At 是状态和动作只

在时间步 t 上的表示。在上面的一些公式中，St, At 有时从一般表示 s, a 分离出来，从而更清楚

地展示期望是关于哪些变量求得的。

注意上面的推导过程中，我们展示了基于 MDP 的贝尔曼方程，然而，对 MRP 的贝尔曼方程

可以直接通过从中去掉动作来得到：

v(s) = Es

′∼p(·|s)

[r + γv(s

′

)] (2.26)

除上述外，也有基于在线动作价值函数（On-Policy Action-Value Function）的贝尔曼方程：

qπ(s, a) = Es

′∼p(·|s,a)

[R(s, a) + γEa′∼π(·|s

′)

[qπ(s

′

, a′

)]]，可以通过如下推导得到：

qπ(s, a) = Ea∼π(·|s),s′∼p(·|s,a)

[R(τt:T )|St = s, At = a]

= Ea∼π(·|s),s′∼p(·|s,a)

[Rt + γRt+1 + γ

2Rt+2 + · · · + γ

TRT |St = s, At = a]

= Ea∼π(·|s),s′∼p(·|s,a)

[Rt + γ(Rt+1 + γRt+2 + · · · + γ

T −1RT )|St = s, At = a]

= ESt+1∼p(·|St,At)

[Rt + γEa∼π(·|s),s′∼p(·|s,a)

[Rτt+1:T

]|St = s]

61

第 2 章 强化学习入门

= ESt+1∼p(·|St,At)

[Rt + γEAt+1∼π(·|St+1)

[qπ(St+1, At+1)]|St = s]

= Es

′∼p(·|s,a)

[R(s, a) + γEa′∼π(·|s

′)

[qπ(s

′

, a′

)]] (2.27)

上面的推导是基于最大长度为 T 的有限 MDP，然而，这些等式对无穷长度 MDP 也成立，只

要将 T 用“∞”替代即可。同时，这两个贝尔曼方程也不依赖于策略的具体形式，这意味着它们

对随机性策略 π(·|s) 和确定性策略 π(s) 都有效。这里 π(·|s) 的使用是为了简化。而且，在确定性

转移过程中，我们有 p(s

′

|s, a) = 1。

贝尔曼方程求解

如果转移函数或转移矩阵是已知的，公式 (2.26) 中对 MRP 的贝尔曼方程可以直接求解，称为

逆矩阵方法（Inverse Matrix Method）。我们用矢量形式对离散有限状态空间的情况将公式 (2.26)

改写为

v = r + γP v (2.28)

其中 v 和 r 矢量，其单元 v(s) 和 R(s) 是对所有 s ∈ S 的，而 P 是转移概率矩阵，其元素 p(s

′

|s)

对所有 s, s′ ∈ S 成立。

由 v = r + γP v，我们可以直接对它求解：

v = (I − γP )

−1

r (2.29)

求解的复杂度是 O(n

3

)，其中 n 是状态的数量。因此这种方法对有大量状态的情况难以求解，这

意味着它可能对大规模或连续值问题不适用。幸运的是，有一些迭代方法可以在实践中解决大规

模的 MRP 问题，比如动态规划（Dynamic Programming）、蒙特卡罗估计（Monte-Carlo Estimation）

和时间差分（Temporal Difference）学习法，这些方法将在随后的小节中详细介绍。

最优价值函数

由于在线价值函数是根据策略本身来估计的，即使是在相同的状态和动作集合上，不同的策

略也将会带来不同的价值函数。对于所有不同的价值函数，我们定义最优价值函数为

v∗(s) = max

π

vπ(s), ∀s ∈ S, (2.30)

这实际是最优状态价值函数（Optimal State-Value Function）。我们也有最优动作价值函数（Optimal

Action-Value Function）：

62

2.3 马尔可夫过程

q∗(s, a) = max

π

qπ(s, a), ∀s ∈ S, a ∈ A, (2.31)

它们之间的关系为

q∗(s, a) = E[Rt + γv∗(St+1)|St = s, At = a], (2.32)

上式可以直接通过对式 (2.84) 的最后一个等式最大化并代入式 (2.24) 和 (2.30) 来得到：

q∗(s, a) = E[R(s, a) + γ max

π

E[qπ(s

′

, a′

)]]

= E[R(s, a) + γ max

π

vπ(s

′

)]

= E[Rt + γv∗(St+1)|St = s, At = a]. (2.33)

它们之间的另一种关系为

v∗(s) = max

a∼A

q∗(s, a) (2.34)

这可以直接通过最大化式 (2.24) 的两边来得到。

贝尔曼最优方程

在上面小节中，我们介绍了一般在线价值函数的贝尔曼方程，以及最优价值函数的定义。因

此我们可以在预定义的最优价值函数上使用贝尔曼方程，这会得到贝尔曼最优方程（Bellman

Optimality Equation），或称对最优价值函数的贝尔曼方程（Bellman Equation for Optimal Value

Functions），推导如下。

对最优状态价值函数的贝尔曼方程为

v∗(s) = max

a

Es

′∼p(·|s,a)

[R(s, a) + γv∗(s

′

)], (2.35)

它可以通过下面推导来得到：

v∗(s) = max

a

Eπ∗,s′∼p(·|s,a)

[R(τt:T )|St = s]

= max

a

Eπ∗,s′∼p(·|s,a)

[Rt + γRt+1 + γ

2Rt+2 + · · · + γ

TRT |St = s]

= max

a

Eπ∗,s′∼p(·|s,a)

[Rt + γRτt+1:T

|St = s]

= max

a

Es

′∼p(·|s,a)

[Rt + γ max

a′

Eπ∗,s′∼p(·|s,a)

[Rτt+1:T

]|St = s]

= max

a

Es

′∼p(·|s,a)

[Rt + γv∗(St+1)|St = s]

63

第 2 章 强化学习入门

= max

a

Es

′∼p(·|s,a)

[R(s, a) + γv∗(s

′

)] (2.36)

最优动作价值函数的贝尔曼方程为

q∗(s, a) = Es

′∼p(·|s,a)

[R(s, a) + γ max

a′

q∗(s

′

, a′

)], (2.37)

上式可以通过与前面类似的方式得到。读者可以练习完成这个证明。

2.3.5 其他重要概念

确定性和随机性策略

在之前的小节中，策略用概率分布 π(a|s) = p(At = a|St = s) 表示，其中智能体的动作是

从分布中采样得到的。一个动作从概率分布中采样的策略称为随机性策略分布（Stochastic Policy

Distribution），其动作为

a ∼ π(·|s) (2.38)

然而，如果我们减少随机性策略分布的方差并将其范围缩窄到极限情况，则将得到一个狄拉

克函数（δ 函数）作为其分布，即为一个确定性策略（Deterministic Policy）π(s)。确定性策略 π(s)

也意味着给定一个状态，将得到唯一的动作，如下：

a = π(s) (2.39)

注意确定性策略不再是从状态和动作到条件概率分布（Conditional Probability Distribution）

的映射，而是一个从状态到动作的直接映射。这点不同将导致随后介绍的策略梯度方法中的一些

推导过程的不同。更多关于强化学习中策略类别的细节，尤其是深度强化学习中的参数化策略，

将在 2.7.3 节中介绍。

部分可观测马尔可夫决策过程

如前面小节中所述，当强化学习环境中的状态无法由智能体的观测量完全表示的时候，环境是

部分可观测的。对于一个马尔可夫决策过程，它被称为部分可观测的马尔可夫决策过程（Partially

Observed Markov Decision Process，POMDP），而这构成了一个利用不完整环境状态信息来改进策

略的挑战。

2.4 动态规划

20 世纪 50 年代，Richard E. Bellman 首次提出动态规划（Dynamic Programming）的概念。随

后，动态规划算法被成功地应用到一系列有挑战的场景中。在“动态规划”一词中，“动态”指求

64

2.4 动态规划

解的问题是序列化的，“规划”指优化策略。动态规划将复杂的动态问题拆解为子问题，提供了一

种通用的求解框架。例如，在斐波那契数列中的每一个数字由两个先前的数字相加得到，从 0 和

1 开始。如第 4 个数 F4 可以写为前两个数 F3、F2 之和 F4 = F3 + F2。在这个算式中，我们可以

进一步将 F3 拆解为 F3 = F2 + F1，从而得到 F4 = (F2 + F1) + F2，于是我们用朴素的子问题 F1

和 F2 表示了 F4。动态规划需要知道求解问题的全部信息，例如，强化学习问题中的奖励机制和

状态转移方程，但是在强化学习的场景中，这些信息是很难被获取的。尽管如此，动态规划依旧

提供了一种通过在马尔可夫过程中进行交互来学习的基本思路，被大多数强化学习算法所沿用。

可以应用动态规划的问题必须具备两个性质：最优子结构（Optimal Substructure）和重叠子

问题（Overlapping Sub-Problems）。最优子结构是指一个给定问题的最优解可以分解成它的子问

题的解。重叠子问题是指子问题的数量是有限的，以及子问题递归地出现，使其可以被存储和重

用。有限动作和状态空间的 MDP 满足以上两个性质，贝尔曼方程实现了递归式的分解，价值函

数存储了子问题的最优解。因此在本小节中，我们假设状态集和动作集都是有限的，并且有一个

环境的理想化模型。

2.4.1 策略迭代

策略迭代（Policy Iteration）的目的在于直接操控策略。从任意策略 π 开始，我们可以通过递

归地调用贝尔曼方程来评估策略：

vπ(s) = Eπ[Rt + γvπ(St+1)|St = s ] (2.40)

这里的期望是针对基于环境全部知识的所有可能的转移。一个获得更好策略的自然想法是根据

vπ 来贪心地执行动作：

π

′

(s) = greedy(vπ) = arg max

a∈A

qπ(s, a). (2.41)

这样的提升可以由以下证明：

vπ(s) = qπ(s, π(s))

⩽ qπ(s, π′

(s))

= Eπ′ [Rt + γvπ(St+1)|St = s ]

⩽ Eπ′ [Rt + γqπ(St+1, π′

(St+1))|St = s ]

⩽ Eπ′ [Rt + γRt+1 + γ

2

qπ(St+2, π′

(St+2))|St = s ]

⩽ Eπ′ [Rt + γRt+1 + γ

2Rt+2 + · · · |St = s ] = vπ′ (s). (2.42)

65

第 2 章 强化学习入门

接连地使用以上的策略评估和贪心提升，直到 π = π

′ 形成策略迭代。一般地，策略迭代的

过程可以总结如下：给定任意一个策略 πt，对于每一次迭代 t 中的每一个状态 s，我们首先评估

vπt

(s)，然后找到一个更好的策略 πt+1。我们把前一个阶段称为策略评估（Policy Evaluation），把

后一个阶段称为策略提升（Policy Improvement）。此外，我们使用术语泛化策略迭代（Generalized

Policy Iteration，GPI）来指代一般的策略评估和策略提升交互过程，如图 2.11 所示。

图 2.11 泛化策略迭代

一个基本的问题是，策略迭代的过程是否在最优值 v∗ 上收敛。在策略评估的每一次迭代中，

对于固定的、确定性的策略 π，价值函数更新可以被贝尔曼期望回溯算子 T

π 重写为

(T

πV )(s) = (Rπ + γP

πV )(s) = X

r,s′

(r + γV (s

′

))P(r, s′

|s, π(s)). (2.43)

那么对于任意的价值函数 V 和 V

′，我们对于 T

π 有如下的收缩（Contraction）证明：

|T πV (s) − T πV

′

(s)| =













X

r,s′

(r + γV (s

′

))P(r, s′

|s, π(s)) −

X

r,s′

(r + γV ′

(s

′

))P(r, s′

|s, π(s))













=













X

r,s′

γ(V (s

′

) − V

′

(s

′

))P(r, s′

|s, π(s))













⩽

X

r,s′

γ|V (s

′

) − V

′

(s

′

)|P(r, s′

|s, π(s))

⩽

X

r,s′

γ∥V − V

′

∥∞P(r, s′

|s, π(s))

= γ∥V − V

′

∥∞, (2.44)

此处 ∥V − V

′∥∞ 是 ∞ 范数。通过收缩映射定理（Contraction Mapping Theorem，即巴拿赫不动点

66

2.4 动态规划

定理，Banach Fixed-Point Theorem），迭代策略评估会收敛到唯一的固定点 T

π。由于 T

πvπ = vπ

是固定点，迭代策略评估会收敛到 vπ。需要指出的是，策略提升是单调的，并且在有限 MDP 中

的价值函数只对应于有限个数的贪心策略。策略提升会在有限步数后停止，也就是说，策略迭代

会收敛到 v∗。

算法 2.6 策略迭代

对于所有的状态初始化 V 和 π

repeat

//执行策略评估

repeat

δ ← 0

for s ∈ S do

v ← V (s)

V (s) ←

P

r,s′ (r + γV (s

′

))P(r, s′

|s, π(s))

δ ← max(δ, |v − V (s)|)

end for

until δ 小于一个正阈值

//执行策略提升

stable ← true

for s ∈ S do

a ← π(s)

π(s) ← arg maxa

P

r,s′ (r + γV (s

′

))P(r, s′

|s, a)

if a ̸= π(s) then

stable ← false

end if

end for

until stable = true

return 策略 π

2.4.2 价值迭代

价值迭代（Value Iteration）的理论基础是最优性原则（Principle of Optimality）。这个原则告

诉我们当且仅当 π 取得了可以到达的任何后续状态上的最优价值时，π 是一个状态上的最优策

略。因此如果我们知道子问题 v∗(s

′

) 的解，就可以通过一步完全回溯（One-Step Full Backup）找

到任意一个初始状态 s 的解：

v∗(s) = max

a∈A

R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)v∗(s

′

). (2.45)

价值迭代的过程是将上面的更新过程从最终状态开始，一个一个状态接连向前进行。和策略迭代

中的收敛证明类似，贝尔曼最优算子 T

∗ 为

67

第 2 章 强化学习入门

(T

∗V )(s) = (max

a∈A

Ra + γP

aV )(s) = max

a∈A

R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V (s

′

) (2.46)

这也是对于任意价值函数 V 和 V

′ 的收缩映射：

|T ∗V (s) − T ∗V

′

(s)|

=













max

a∈A



R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V (s

′

)



 − max

a∈A



R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V

′

(s

′

)

















⩽ max

a∈A













R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V (s

′

) − R(s, a) − γ

X

s

′∈S

P(s

′

|s, a)V

′

(s

′

)













= max

a∈A













γ

X

s

′∈S

P(s

′

|s, a)(V (s

′

) − V

′

(s

′

))













⩽ max

a∈A

γ

X

s

′∈S

P(s

′

|s, a)|V (s

′

) − V

′

(s

′

)|

⩽ max

a∈A

γ

X

s

′∈S

P(s

′

|s, a)∥V − V

′

∥∞

= γ∥V − V

′

∥∞ max

a∈A

X

s

′∈S

P(s

′

|s, a)

= γ∥V − V

′

∥∞. (2.47)

由于 v∗ 是 T

∗ 的一个固定点，价值迭代会收敛到最优值 v∗。需要指出的是，在价值迭代中，只有

后续状态的实际价值是已知的。换句话说，价值是不完整的，因此，我们在以上的证明中使用估

计价值函数 V ，而不是真实价值 v。

何时停止价值迭代算法不是显而易见的。文献 (Williams et al., 1993) 在理论上给出了一个充

分的（Sufficient）停止标准：如果两个连续价值函数的最大差异小于 ϵ，那么在任意状态下，贪心

策略的价值与最优策略的价值函数的差值不会超过 2ϵγ

1−γ。

2.4.3 其他 DPs：异步 DP、近似 DP 和实时 DP

目前描述的 DP 方法均使用同步回溯（Synchronous Backup），即每个状态的价值基于系统性

的扫描（Systematic Sweeps）来回溯。一种有效的变体是异步的更新（Asynchronous Update），而

这也是速度和准确率之间的权衡。异步 DP 对于强化学习的设定也是适用的，且如果所有状态被

持续选择的话，可以保证收敛。异步 DP 有三种简单的思路:

68

2.4 动态规划

算法 2.7 价值迭代

为所有状态初始化 V

repeat

δ ← 0

for s ∈ S do

u ← V (s)

V (s) ← maxa

P

r,s′ P(r, s′

|s, a)(r + γV (s

′

))

δ ← max(δ, |u − V (s)|)

end for

until δ 小于一个正阈值

输出贪心策略 π(s) = arg maxa

P

r,s′ P(r, s′

|s, a)(r + γV (s

′

))

1. 在位更新（In-Place Update）

同步价值迭代（Synchronous Value Iteration）存储价值函数 Vt+1(·) 和 Vt(·) 的两个备份：

Vt+1(s) ← max

a∈A

R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)Vt(s

′

). (2.48)

在位价值迭代只存储价值函数的一个备份：

V (s) ← max

a∈A

R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V (s

′

). (2.49)

2. 优先扫描（Prioritized Sweeping）

在异步 DP 中，另一个需要考虑的事情是更新顺序。给定一个转移 (s, a, s′

)，优先扫描将它

的贝尔曼误差（Bellman Error）的绝对值作为它的大小：













V (s) − max

a∈A

(R(s, a) + γ

X

s

′∈S

P(s

′

|s, a)V (s

′

))













. (2.50)

它可以通过保持一个优先权队列来有效地实现，该优先权队列在每个回溯后存储和更新每

个状态的贝尔曼误差。

3. 实时更新（Real-Time Update）

在每个时间步 t 之后，不论采用哪个动作，实时更新将只会通过以下方式回溯当前状态 St：

V (St) ← max

a∈A

R(St, a) + γ

X

s

′∈S

P(s

′

|St, a)V (s

′

). (2.51)

它可以被视为根据智能体的经验来指导选择要更新的状态。

同步 DP 和异步 DP 都在全部状态集上回溯，估计下一个状态的预期回报。从概率的角度来

69

第 2 章 强化学习入门

看，一个有偏差的但有效的选择是使用采样的数据。我们将在下一个小节中深入讨论此问题。

2.5 蒙特卡罗

和动态规划不同的是，蒙特卡罗 (Monte Carlo, MC) 方法不需要知道环境的所有信息。蒙特

卡罗方法只需基于过去的经验就可以学习。它也是一种基于样本的（Sampling-Based）方法。蒙

特卡罗可以在对环境只有很少的先验知识时从经验中学习来取得很好的效果。“蒙特卡罗”可以

用来泛指那些有很大随机性的算法。

当我们在强化学习中使用蒙特卡罗方法的时候，需要对来自不同片段中的每个状态-动作对

（State-Action Pair）相应的奖励值取平均。一个例子是，在本章之前内容中介绍的上下文赌博机

（Contextual Bandit）问题中，如果在不同的机器上有一个 LED 灯，那么玩家就可以逐渐地学习

LED 灯状态信息和回报之间的联系。我们在这里把一种灯的排列组合作为一种状态，那么其可

能的奖励值就作为这个状态的价值。最开始，我们可能无法对状态价值有一个很好的预估，但是

当我们做出更多的尝试以后，平均状态价值会向它们的真实值靠近。在这个章节，我们会探索我

们怎么更合理地做出估算。假设问题是回合制的（Episodic），因而不论一个玩家做出了哪些的动

作，一个回合最后都会终止。

2.5.1 蒙特卡罗预测

首先，我们一起来看给定一个策略 π 如何用蒙特卡罗方法来评估状态价值函数。直观上的一

种方式是，通过对具体策略产生的回报取平均值来从经验中评估状态价值函数。更具体地，让函

数 vπ(s) 作为在策略 π 下的状态价值函数。我们接着收集一组经过状态 s 的回合，并把每一次状

态 s 在一个回合里的出现叫作一次对状态 s 的访问。这样一来，我们就有两种估算方式：首次蒙

特卡罗（First-Visit Monte Carlo）和 每次蒙特卡罗（Every-Visit Monte Carlo）。首次蒙特卡罗只考

虑每一个回合中第一次到状态 s 的访问，而每次蒙特卡罗就是考虑每次到状态 s 的访问。这两种

方式有很多的相似点，但是也有一些理论上的不同。在算法 2.8，我们展示了如何用首次蒙特卡罗

来对 vπ(s) 估算。把首次蒙特卡罗变成每次蒙特卡罗在操作上很简单，我们只需要把对首次访问

检查条件去掉即可。假如我们对状态 s 有无限次访问的话，那最终这两种方式都会收敛到 vπ(s)。

蒙特卡罗方法可以独立地对不同的状态值进行估算。和动态规划不同的是，蒙特卡罗不使用

自举（Bootstrapping），也就是说，它不用其他状态的估算来估算当前的状态值。这个独特的性质

可以让我们直接通过采样的回报来对状态值进行估算，从而有更小的偏差但会有更大的方差。

当我们有了环境的模型以后，状态价值函数就会很有用处了，因为我们就可以通过比较对一

个状态的不同动作的价值平均值来选择在任意状态下的最好动作，就和在动态规划里一样。当模

型未知时，我们需要把状态-动作价值估算出来。每一个状态-动作值需要被分别估计。现在，我

们的学习目标就变成了 qπ(s, a)，即在状态 s 下根据策略 π 采取动作 a 时的预期回报。这在本质

上与对状态价值函数的估计基本一致，而我们现在只是取状态 s 在动作 a 上的平均值而已。不过

70

2.5 蒙特卡罗

有时，可能会有一些状态从来都没有被访问过，所以就没有回报。为了选择最优的策略，我们必

须要探索所有的状态。一个简单的方法是直接选择那些没有可能被选择的状态-动作对来作为初

始状态。这样一来，就可以保证在足够的回合数过后，所有的状态-动作对都是可以被访问的。我

们把这样的一个假设叫作叫作探索开始（Exploring Starts）。

算法 2.8 首次蒙特卡罗预测

输入：初始化策略 π

初始化所有状态的 V (s)

初始化一列回报：Returns(s) 对所有状态

repeat

通过 π: S0, A0, R0, S1, · · · , ST −1, AT −1, Rt 生成一个回合

G ← 0

t ← T − 1

for t >= 0 do

G ← γG + Rt+1

if S0, S1, · · · , St−1 没有 St then

Returns(St).append(G)

V (St) ← mean(Returns(St))

end if

t ← t − 1

end for

until 收敛

2.5.2 蒙特卡罗控制

现在我们可以把泛化策略迭代运用到蒙特卡罗中去，来看看它是怎么用来控制的。泛化策略

迭代有两个部分：策略评估（Policy Evaluation）和策略提升（Policy Improvement）。策略评估的

过程与之前小节中介绍的动态规划是一样的，所以我们主要来介绍策略提升。我们会对状态-动

作值使用贪心策略，在这种情况下不需要使用环境模型。贪心策略会一直选择在一个状态下有最

高价值的动作：

π(s) = arg max

a

q(s, a) (2.52)

对于每一次策略提升，我们都需要根据 qπt 来构造 πt+1。这里展示策略提升是怎么实现的：

qπt

(s, πt+1(s)) = qπt

(s, arg max

a

qπt

(s, a))

= max

a

qπt

(s, a)

⩾ qπt

(s, πt(s))

71

第 2 章 强化学习入门

⩾ vπt

(s) (2.53)

上面的式子证明了 πt+1 不会比 πt 差，而我们会在迭代策略提升后最终找到最优策略。这也

意味着，我们可以对环境没有太多了解而只有采样得到的回合才使用蒙特卡罗。这里我们需要解

决两个假设。第一个假设是探索开始，第二个是假设有无穷多个回合。我们先跳过第一个假设，

从第二个假设开始。简化这个假设的一种简单方法是，通过直接在单个状态的评估和改进之间交

替变更，来避免策略评估所需的无限多的片段（Episodes）。

2.5.3 增量蒙特卡罗

从算法 2.8 和算法 2.9 中可以看出，我们需要对观察到的回报序列求平均值，并且将状态价值

和状态-动作价值的估计分开。其实我们还有一种更加高效的计算办法，它能让我们把回报序列省

去，从而简化均值计算步骤。这样一来，我们就需要一个回合一个回合地更新。我们让 Q(St, At)

作为它已经被选中 t − 1 次以后的状态-动作价值的估计，从而将其改写为

Q(St, At) = G1 + G2 + · · · + Gt−1

t − 1

(2.54)

算法 2.9 蒙特卡罗探索开始

初始化所有状态的 π(s)

对于所有的状态-动作对，初始化 Q(s, a) 和 Returns(s, a)

repeat

随机选择 S0 和 A0，直到所有状态-动作对的概率为非零

根据 π: S0, A0, R0, S1, · · · , ST −1, AT −1, Rt 来生成 S0, A0

G ← 0

t ← T − 1

for t >= 0 do

G ← γG + Rt+1

if S0, A0, S1, A1 · · · , St−1, At−1 没有 St, At then

Returns(St, At).append(G)

Q(St, At) ← mean(Returns(St, At))

π(St) ← arg maxa Q(St, a)

end if

t ← t − 1

end for

until 收敛

对该式的一个简单实现是将所有的回报 G 值都记录下来，然后将它的和值除以它的访问次

数。然而，我们同样也可以通过以下的公式来计算这个值：

72

2.6 时间差分学习

Qt+1 =

1

t

Xt

i=1

Gi

=

1

t



Gt +

Xt−1

i=1

Gi





=

1

t



Gt + (t − 1) 1

t − 1

Xt−1

i=1

Gi





=

1

t

(Gt + (t − 1)Qt)

= Qt +

1

t

(Gt − Qt) (2.55)

这个形式可以让我们在计算回报的时候更加容易操作。它的通用形式是：

新估计值 ← 旧估计值 + 步伐大小 · (目标值 − 旧估计值) (2.56)

“步伐大小”是我们用来控制更新速度的一个参数。

2.6 时间差分学习

时间差分（Temporal Difference，TD）是强化学习中的另一个核心方法，它结合了动态规划和

蒙特卡罗方法的思想。与动态规划相似，时间差分在估算的过程中使用了自举（Bootstrapping），

但是和蒙特卡罗一样，它不需要在学习过程中了解环境的全部信息。在这章中，我们首先介绍如

何将时间差分用于策略评估，然后详细阐释时间差分、蒙特卡罗和动态规划方法的异同点。最后，

我们会介绍 Sarsa 和 Q-Learning 算法，这是一个在经典强化学习中很有用的算法。

2.6.1 时间差分预测

从这个方法的名字可以看出，时间差分利用差异值进行学习，即目标值和估计值在不同时间

步上的差异。它使用自举法的原因是它需要从观察到的回报和对下个状态的估值中来构造它的目

标。具体来说，最基本的时间差分使用以下的更新方式：

V (St) ← V (St) + α[Rt+1 + γV (St+1) − V (St)] (2.57)

这个方法也被叫作 TD(0), 或者是单步 TD。也可以通过将目标值改为在 N 步未来中的折扣

回报和 N 步过后的估计状态价值（Estimated State Value）来实现 N 步 TD。如果我们观察得足

够仔细，蒙特卡罗在更新时的目标值为 Gt，这个值只有在一个回合过后才能得知。但是对于 TD

73

第 2 章 强化学习入门

来说，这个目标值是 Rt+1 + γV (St+1)，而它可以在每一步都算出。在算法 2.10 中，我们展示了

TD(0) 是如何用来做策略评估的。

算法 2.10 TD(0) 对状态值的估算

输入策略 π

初始化 V (s) 和步长 α ∈ (0, 1]

for 每一个回合 do

初始化 S0

for 每一个在现有回合的 St do

At ← π(St)

Rt+1, St+1 ← Env(St, At)

V (St) ← V (St) + α[Rt+1 + γV (St+1) − V (St)]

end for

end for

这里分析一下动态规划、蒙特卡罗和时间差分方法的异同点。它们都是在现代强化学习中的

核心算法，而且经常是被结合起来使用的。它们都可以被用于策略评估和策略提升，它们之间区

别却是深度强化学习效果不同的主要来源之一。

这三种方法都涉及泛化策略迭代（GPI），它们主要区别在于策略评估的过程，其中最明显的

区别是，动态规划和时间差分都使用了自举法，而蒙特卡罗没有。动态规划需要整个环境模型的

所有信息，但是蒙特卡罗和时间差分不需要。进一步地，我们来看一下它们的学习目标的区别。

vπ(s) = Eπ[Gt|St = s] (2.58)

= Eπ[Rt+1 + γGt+1|St = s] (2.59)

= Eπ[Rt+1 + γvπ(St+1)|St = s] (2.60)

公式 (2.58) 是蒙特卡罗方法的状态价值估计方式，公式 (2.60) 是动态规划的。它们都不是真

正的状态值而是估计值。时间差分则把蒙特卡罗的采样过程和动态规划的自举法结合了起来。现

在我们就简单解释实践中时间差分可以比动态规划或者蒙特卡罗更有效的原因。

首先，时间差分不需要一个模型而动态规划需要。将时间差分与蒙特卡罗做比较，时间差分

使用的是在线学习，这也就意味着它每一步都可以学习，但是蒙特卡罗却只能在一个回合结束以

后再学习，这样回合很长时会比较难以处理。当然，也存在一些连续性的问题无法用片段式的形

式来表示一个回合。另外，时间差分在实践中往往收敛得更快，因为它的学习是来自状态转移的

信息而不需要具体动作信息，而蒙特卡罗往往需要动作信息。在理想情况下，两种方法最终都会

渐进收敛到 vπ(s)。

这里我们介绍时间差分和蒙特卡罗方法中的偏差和方差的权衡（Bias and Variance Trade-off）。

我们知道在监督学习的设置下，较大的偏差往往意味着这个模型欠拟合（Underfitting），而较大的

方差伴随较低的偏差往往意味着一个模型过拟合（Overfitting）。一个拟合器（Estimator）的偏差

74

2.6 时间差分学习

是估计值和真正值间的差异。我们对状态价值进行估计时，偏差可以被定义为 E[V (St)] − V (St)。

拟合器的方差描述了这个拟合有多大的噪声。同样对于状态价值估计，方差定义为 E[(E[V (St)]−

V (St))2

]。在预测时，不管它是状态价值估计，还是状态-动作价值估计，时间差分和蒙特卡罗的

更新都有如下形式：

V (St) ← V (St) + α[TargetValue − V (St)]

实质上，我们对不同回合进行了加权平均计算。时间差分法和蒙特卡罗法在处理目标值时分

别采用不同的方式。蒙特卡罗法直接估算到一个回合结束累计的回报。这也正是状态值的定义，

它是没有偏差的。而时间差分法会有一定的偏差，因为它的目标值是由自举法估计得到的，如

Rt+1 + γvπ(St+1)。另一方面，蒙特卡罗法，所以在不同回合中积累到最后的回报会有较大的方

差由于不同回合的经过和结果都不同。时间差分法通过关注局部估计的目标值来解决这个问题，

只依赖当前的奖励和下一个状态或动作价值的估计。自然地，时间差分法方差更小。

我们可以在动态规划和蒙特卡罗之间找到一个中间方法来更有效地解决问题，即 TD (λ)。在

此之前，我们需要先介绍资格迹（Eligibility Trace）和 λ-回报（λ-Return）概念。

简单来说，资格迹可以给我们带来一些计算优势。为了更好地了解其优势，我们需要介绍半

梯度（Semi-Gradient）方法，然后再来看如何使用资格迹。关于策略梯度方法在 2.7 节中有介绍，

而这里我们简单地使用一些策略梯度方法中的概念来方便解释资格迹。假如说我们的状态价值函

数不是表格（Tabular）形式而是一种函数形式，这个函数由矢量 w ∈ R

n 参数化。比如 w 可以是

一个神经网络的权重。为了得到 V (s, w) ≈ vπ(s)，我们使用随机梯度更新来减小估计值和真正

的状态价值的平方损失（Quadratic Loss）。权重向量的更新规则就可以写为

wt+1 = wt −

1

2

α∇wt

[vπ(St) − V (St, wt)]2

= wt + α[vπ(St) − V (St, wt)]∇wtV (St, wt) (2.61)

其中 α 为一个正值的步长变量。

资格迹是一个向量：zt ∈ R

n，在学习的过程中，每当 wt 的一个部分被用于估计，则它在 zt

里的那个相对应的部分需要随之增加，而在增加以后它又会慢慢递减。如果轨迹上的资格值回落

到零之前，有一定的 TD 误差，就进行学习。首先把所有资格值都初始化为 0，然后使用价值函

数的梯度来增加资格迹，而资格值递减的速度是 γλ。资格迹的更新满足如下公式：

z−1 = 0 (2.62)

zt = γλzt−1 + ∇wtV (St, wt) (2.63)

如算法 2.11 所示，TD(λ) 使用资格迹来更新其价值函数估计。易见，当 λ = 1 时，TD(λ) 变为蒙

75

第 2 章 强化学习入门

特卡罗法；而当 λ = 0 时，它就变成了一个单步 TD（One-Step TD）法。因此，资格迹可以看作

是把时间差分法和蒙特卡罗法相结合的一个方法。

算法 2.11 状态值半梯度 TD(λ)

输入策略 π

初始化一个可求导的状态值函数 v、步长 α 和状态值函数权重 w

for 对每一个回合 do

初始化 S0

z ← 0

for 每一个本回合的步骤 St do

使用 π 来选择 At

Rt+1, St+1 ← Env(St, At)

z ← γλz + ∇V (St, wt)

δ ← Rt+1 + γV (St+1, wt) − V (St, wt)

w ← w + αδz

end for

end for

λ-回报是之后 n 步中的估计回报值。λ-回报是 n 个已经折扣化的回报和一个在最后一步状态

下的估计值相加得到的。我们可以把它写作：

Gt:t+n = Rt+1 + γRt+2 + · · · + γ

n−1Rt+n + γ

n

v(St+n, wt+n−1) (2.64)

t 是一个不为零的标量，它小于或等于 T − n。我们可以使用加权平均回报来估算，只要它们

的权重满足和为 1。TD(λ) 在其更新中使用了加权平均（λ ∈ [0, 1]）：

G

λ

t = (1 − λ)

X∞

n=1

λ

n−1Gt:t+n (2.65)

直观地讲，这就意味着下一步的回报将有最大的权重 1 − λ，下两步回报的权重是 (1 − λ)λ。

每一步权重递减的速率是 λ。为了有更清晰的理解，我们让结束状态发生于时间 T，从而上面的

公式可以改写成

G

λ

t = (1 − λ)

T

X−t−1

n=1

λ

n−1Gt:t+n + λ

T −t−1Gt (2.66)

TD 误差 δt 可以被定义为

δt = Rt+1 + γV (St+1, wt) − V (St, wt) (2.67)

76

2.6 时间差分学习

这个更新规则是基于 TD 误差和迹的比重的。算法 2.11 里有其细节。

2.6.2 Sarsa：在线策略 TD 控制

对于 TD 控制，我们使用的方法和预测任务一样，唯一的不同是，我们需要将从状态到状态

的转移变为状态-动作对的交替。这样的更新规则就可以被写为

Q(St, At) ← Q(St, At) + α[Rt+1 + γQ(St+1, At+1) − Q(St, At)] (2.68)

当 St 是终止状态（Terminal State）的时候，下一个状态-动作对的 Q 值就会变成 0。我们用

首字母缩写 Saras 来表示这个算法，因为我们有这样的一个行为过程：首先在一个状态（S）下，

选择了一个动作（A），同时也观察到了回报（R），然后我们就到了另外一个状态（S）下，需要

选择一个新的动作（A）。这样的过程让我们可以做一个简单的更新步骤。对于每一个转移，状态

价值都得到更新，更新后的状态价值会影响决定动作的策略，即在线策略法。在线策略法一般用

来描述这样一类算法，它们的更新策略和行动策略（Behavior Policy）同样。而离线策略法往往

是不同的。Q-Learning 就是离线策略算法的一个例子。我们会在之后的章节中提到。Q-Learning

在更新 Q 函数时假设了一种完全贪心的方法，而它在选择其动作时实际上用的是另外一种类似

于 ϵ-贪心（ϵ-Greedy）的方法。现在我们在算法 2.12 中列出 Sarsa 的细节。在每一个状态-动作对

都会被访问无数次的假设下，会有最优策略和状态动作价值的收敛性保证。

算法 2.12 Sarsa （在线策略 TD 控制）

对所有的状态-动作对初始化 Q(s, a)

for 每一个回合 do

初始化 S0

用一个基于 Q 的策略来选择 A0

for 每一个在当前回合的 St do

用一个基于 Q 的策略从 St 选择 At

Rt+1, St+1 ← Env(St, At)

从 St+1 中用一个基于 Q 的策略来选择 At+1

Q(St, At) ← Q(St, At) + α[Rt+1 + γQ(St+1, At+1) − Q(St, At)]

end for

end for

上面展示的方法只有一步的时间范围，这就意味着它的估算只需要考虑下一步的状态-动作

价值。我们把它叫作单步 Sarsa 或者 Sarsa(0)。我们可以简单地使用自举法把未来的步骤也都容

纳到目标值中从而减少它的偏差。从图 2.12 的回溯树展示中，我们可以看见 Sarsa 很多不同的变

体。从最简单的一步 Sarsa 到无限步 Sarsa，也就是蒙特卡罗方法的另外一个形态。为了把这样的

77

第 2 章 强化学习入门

一个变化融入原来的方法，我们需要把折扣回报写为

Gt:t+n = Rt+1 + γRt+2 + · · · + γ

n−1Rt+n + γ

nQt+n−1(St+n, At+n) (2.69)

图 2.12 对于 n 步 Sarsa 方法的回溯树。每一个黑色的圆圈都代表了一个状态，每一个白色的圆

圈都代表了一个动作。在这个无穷多步的 Sarsa 里，最后一个状态就是它的终止状态

n 步 Sarsa 已经在算法 2.13 中有所描述了。和单步版本最大的不同是，它需要回到过去的时

间来做更新，而单步的版本只需要一边向前进行一边更新即可。

现在讨论 Sarsa 算法在有限的动作空间里的收敛理论。我们首先需要以下的几个条件。

定义 2.1 一个学习策略被定义为：在无限的探索中的极限贪婪（Greedy in the Limit with Infinite

Exploration, GLIE）。如果它能够满足以下两个性质：1. 如果一个状态被无限次访问，那么在该状态

下的每个可能的动作都应当被无限次选择，即 limk→∞ Nk(s, a) = ∞, ∀a, if limk→∞ Nk(s) = ∞。

2. 策略根据学习到的 Q 函数在 t → ∞ 的极限下收敛到一个贪婪策略，即 limk→∞ πk(s, a) =

1(a == arg maxa′∈A Qk(s, a′

))，其中“==”是一个比较算子，当 1(a == b) 的括号内为真时，它

的值为 1，否则为 0。

GLIE 是学习策略收敛的一个条件，对于任何收敛到最优价值函数且估计值都有界（Bounded）

的强化学习算法来说，它都成立。举例来说，我们可以通过 ϵ 贪心方法来推导出一个 GLIE 的策

略，如下：

引理 2.1 如果 ϵ 以 ϵk =

1

k 的形式随 k 增大而渐趋于零，那么 ϵ-贪心是 GLIE。

78

2.6 时间差分学习

算法 2.13 n 步 Sarsa

对所有的状态-动作对初始化 Q(s, a)

初始化步长 α ∈ (0, 1]

决定一个固定的策略 π 或者使用 ϵ-贪心策略

for 每一个回合 do

初始化 S0

使用 π(S0, A) 来选择 A0

T ← INTMAX （一个回合的长度）

γ ← 0

for t ← 0, 1, 2, · · · until γ − T − 1 do

if t < T then

Rt+1, St+1 ← Env(St, At)

if St+1 是终止状态 then

T ← t + 1

else

使用 π(St, A) 来选择 At+1

end if

end if

τ ← t − n + 1 （更新的时间点。这是 n 步 Sarsa，只需更新 n + 1 前的一步，持续下去直

到所有状态都被更新。）

if τ ⩾ 0 then

G ←

Pmin(r+n,T)

i=τ+1 γ

i−γ−1Ri

if γ + n < T then

G ← G + γ

nQ(St+n, Aγ+n)

end if

Q(Sγ, Aγ) ← Q(Sγ, Aγ) + α[G − Q(Sγ, Aγ)]

end if

end for

end for

因而就有了 Sarsa 算法的收敛定理。

定理 2.1 对于一个有限状态-动作的 MDP 和一个 GLIE 学习策略，其动作价值函数 Q 在时间步 t

上由 Sarsa（单步的）估计为 Qt，那么如果以下两个条件得到满足，Qt 会收敛到 Q∗ 并且学习策

略 πt，也会收敛到最优策略 π

∗：

1. Q 的值被存储在一个查找表（Lookup Table）里；

2. 在时间t与状态-动作对(s, a)相关的学习速率（Learning Rate）αt(s, a)满足0 ⩽ αt(s, a) ⩽ 1，

P

t αt(s, a) = ∞，

P

t α

2

t

(s, a) < ∞，并且 αt(s, a) = 0 除非 (s, a) = (St, At);

3. 方差 Var[R(s, a)] < ∞。

符合第二个条件对学习速率的要求的一个典型数列是 αt(St, At) = 1

t 。我们在这里对上面定

理的证明不做介绍，有兴趣的读者可以查看文献 (Singh et al., 2000)。

79

第 2 章 强化学习入门

2.6.3 Q-Learning：离线策略 TD 控制

Q-Learning 是一种离线策略方法，与 Saras 很类似，在深度学习应用中有很重要的作用，如

深度 Q 网络（Deep Q-Networks）。如公式 (2.70) 所示，Q-Learning 和 Sarsa 主要的区别是，它的

目标值现在不再依赖于所使用的策略，而只依赖于状态-动作价值函数。

Q(St, At) ← Q(St, At) + α[Rt+1 + γ max

a

Q(St+1, a) − Q(St, At)] (2.70)

在算法 2.14 中，我们展示了如何用 Q-Learning 控制 TD。将 Q-Learning 变成 Sarsa 算法也

很容易，可以先基于状态和回报选择动作，然后在更新步中将目标值改为估计的下一步动作价

值。上面展示的是单步 Q-Learning，我们也可以把 Q-Learning 变成 n 步的版本。具体做法是将公

式 (2.70) 里的目标值加入未来的折扣后的回报。

算法 2.14 Q-Learning （离线策略 TD 控制）

初始化所有的状态-动作对的 Q(s, a) 及步长 α ∈ (0, 1]

for 每一个回合 do

初始化 S0

for 每一个在当前回合的 St do

使用基于 Q 的策略来选择 At

Rt+1, St+1 ← Env(St, At)

Q(St, At) ← Q(St, At) + α[Rt+1 + γ maxa Q(St+1, a) − Q(St, At)]

end for

end for

Q-Learning的收敛性条件和Sarsa算法的很类似。除了对策略有的GLIE条件，Q-Learning中Q

函数的收敛还对学习速率和有界奖励值要求，这里不再复述，具体的证明可以在文献 (Szepesvári,

1998; Watkins et al., 1992) 中找到。

2.7 策略优化

2.7.1 简介

在强化学习中，智能体的最终目标是改进它的策略来获得更好的奖励。在优化范畴下的策略

改进叫策略优化（图 2.13）。对深度强化学习而言，策略和价值函数通常由深度神经网络中的变量

来参数化，因此可以使用基于梯度的优化方法。举例来说，图 2.14 展示了使用参数化策略的 MDP

的概率图模型（Graphical Model），其中策略由变量 θ 参数化，在离散时间范围 t = 0, · · · , N − 1

内。奖励函数表示为 Rt = R(St, At)，而动作表示为 At ∼ π(·|St; θ)。图模型中变量的依赖关系

可以帮助我们理解 MDP 估计中的潜在关系，而且可以有助于我们在依赖关系图中对最终目标求

80

2.7 策略优化

导而优化变量有帮助，因此我们将在本章展示所有的图模型来帮助理解推导过程，尤其对那些可

微分的过程。近来，文献 (Levine, 2018) 和文献 (Fu et al., 2018) 提出了一种“推断式控制（Control

as Inference）”的方法，这个方法在 MDP 的图模型上添加了额外的表示最优性（Optimality）的变

量，从而将概率推断或变分推断（Variational Inference）的框架融合到有相同目标的最大熵强化

学习（Maximum Entropy Reinforcement Learning）中。这个方法使得推断类工具（Inference Tools）

可以应用到强化学习的策略优化过程中。但是关于这些方法的具体细节超出了本书范围。

基于价值 基于策略

Actor-Critic

(Q-Learning, DQN等) (REINFORCE, CE方法)

(QAC, A2C,

A3C 等)

QT-Opt

价值函数 策略

图 2.13 强化学习中策略优化概览

θ At At+1

Rt Rt+1

St St+1

t = 0, 1, · · · , T − 1

图 2.14 使用参数化策略的 MDP 概率图模型

除了一些线性方法，使用深度神经网络对价值函数参数化是一种实现价值函数拟合（Value

Function Approximation）的方式，而这是现代深度强化学习领域中最普遍的方式，而在多数实际

情况中，我们无法获得真实的价值函数。图 2.15 展示了使用参数化策略 πθ 和参数化价值函数

V

π

w (St) 的 MDP 概率图模型，它们的参数化过程分别使用了参数 θ 和 w。图 2.16 展示了使用参

81

第 2 章 强化学习入门

数化策略 πθ 和参数化 Q 值函数 Qπ

w(St, At) 的 MDP 概率图模型。一般通过在强化学习术语中

被称为策略梯度（Policy Gradient）的方法改进参数化策略。然而，也有一些非基于梯度的方法

（Non-Gradient-Based Methods）可以优化不那么复杂的参数化策略，比如交叉熵（Cross-Entropy，

CE）方法等。

θ At At+1

Rt Rt+1

St St+1

V (St) V (St+1)

w

t = 0, 1, · · · , T − 1

图 2.15 使用参数化策略和参数化价值函数的

MDP 概率图模型

θ At At+1

Rt Rt+1

St St+1

Q(St, At) Q(St+1, At+1)

w

t = 0, 1, · · · , T − 1

图 2.16 使用参数化策略和参数化 Q 值函数的

MDP 概率图模型

如图 2.13 所示，策略优化算法往往分为两大类：（1）基于价值的优化（Value-Based Optimization）方法，如 Q-Learning、DQN 等，通过优化动作价值函数（Action-Value Function）来获得对

动作选择的偏好；（2）基于策略的优化（Policy-Based Optimization）方法，如 REINFORCE、交

叉熵算法等，通过根据采样的奖励值来直接优化策略。这两类的结合被人们 (Kalashnikov et al.,

2018; Peters et al., 2008; Sutton et al., 2000) 发现是一种更加有效的方式，而这构成了一种在无模型

（Model-Free）强化学习中应用最广的结构，称为 Actor-Critic。Actor-Critic 方法通过对价值函数

的优化来引导策略改进。在这类结合型算法中的典型包括 Actor-Critic 类的方法和以其为基础的

82

2.7 策略优化

其他算法，后续有关于这些算法的详细介绍。

回顾强化学习梗概

在线价值函数（On-Policy Value Function），vπ(s)，给出以状态 s 为起始并在后续过程始终遵

循策略 π 的期望回报（Expected Return）：

vπ(s) = Eτ∼π[R(τ )|S0 = s] (2.71)

强化学习的优化问题可以被表述为

π∗ = arg max

π

J(π) (2.72)

最优价值函数（Optimal Value Function），v

∗

(s)，给出以状态 s 为起始并在后续过程始终遵循

环境中最优策略的期望回报：

v∗(s) = max

π

vπ(s) (2.73)

v∗(s) = max

π

Eτ∼π[R(τ )|S0 = s] (2.74)

在线动作价值函数（On-Policy Action-Value Function），qπ(s, a)，给出以状态 s 为起始并采取

任意动作 a（有可能不来自策略），而随后始终遵循策略 π 的期望回报：

qπ(s, a) = Eτ∼π[R(τ )|S0 = s, A0 = a] (2.75)

最优动作价值函数（Optimal Action-Value Function），q∗(s, a)，给出以状态 s 为起始并采取任

意动作 a，而随后始终遵循环境中最优策略的期望回报：

q∗(s, a) = max

π

qπ(s, a) (2.76)

q∗(s, a) = max

π

Eτ∼π[R(τ )|S0 = s, A0 = a] (2.77)

价值函数（Value Function）和动作价值函数（Action-Value Function）的关系：

vπ(s) = Ea∼π[qπ(s, a)] (2.78)

v∗(s) = max

a

q∗(s, a) (2.79)

83

第 2 章 强化学习入门

最优动作：

a∗(s) = arg max

a

q∗(s, a) (2.80)

贝尔曼方程:

对状态价值和动作价值的贝尔曼方程分别为：

vπ(s) = Ea∼π(·|s),s′∼p(·|s,a)

[R(s, a) + γvπ(s

′

)] (2.81)

qπ(s, a) = Es

′∼p(·|s,a)

[R(s, a) + γEa′∼π(·|s

′)

[qπ(s

′

, a′

)]] (2.82)

贝尔曼最优方程：

对状态价值和动作价值的贝尔曼最优方程分别为：

v∗(s) = max

a

Es

′∼p(·|s,a)

[R(s, a) + γv∗(s

′

)] (2.83)

q∗(s, a) = Es

′∼p(·|s,a)

[R(s, a) + γ max

a′

q∗(s

′

, a′

)] (2.84)

2.7.2 基于价值的优化

基于价值的优化（Value-Based Optimization）方法经常需要在（1）基于当前策略的价值函数

估计和（2）基于所估计的价值函数进行策略优化这两个过程之间交替。然而，估计一个复杂的

价值函数并不容易，如图 2.17 所示。

从之前小节中我们可以看到，Q-Learning 可以被用来解决强化学习中一些简单的任务。然而，

现实世界或者即使准现实世界中的应用也都可能有更大和更复杂的状态动作空间，而且实际应用

中很多动作是连续的。比如，在围棋游戏中有约 10170 个状态。在这些情况下，Q-Learning 中的

传统查找表（Lookup Table）方法因为每个状态需要有一条记录（Entry）而每个状态-动作对也需

要一条 Q(s, a) 记录而使其可扩展性（Scalability）有待提升。实践中，这个表中的值需要一个一

个地更新。所以基于表格（Tabular-Based）的 Q-Learning 对内存和计算资源的需求可能是巨大的。

此外，在实践中，状态表征（State Representations）通常也需要人为指定成相匹配的数据结构。

价值函数拟合

为了将基于价值的强化学习应用到相对大规模的任务上，函数拟合器（Function Approximators）可用来应对上述限制条件（图 2.18）。图 2.18 总结了不同类型的价值函数拟合器。

• 线性方法（Linear Methods）：拟合函数是权重 θ 和特征实数向量 φ(s) = (ϕ1(s), ϕ2(s)), · · · ,

ϕn(s))T 的线性组合，其中 s 是状态。拟合函数表示为 v(s, θ) = θ

Tφ(s)。TD(λ) 方法因使

用线性函数拟合器而被证明在一定条件下可以收敛 (Tsitsiklis et al., 1997)。尽管线性方法的

84

2.7 策略优化

动作或观察量空间

多项式、傅立叶基、

粗略编码、地砖编

码、径向基函数等

简单且离散 复杂或连续

表格方法

线性方法 非线性方法 其他方法

神经网络

价值函数拟合

决策树、最近邻、

基于核的方法等

图 2.17 求解价值函数的方法概览

V (s; w) Q (s,a; w) Q (s,a1

; w) ... Q (s,an

; w)

w w w

s s a s

状态价值函数 动作价值函数

（连续动作）

动作价值函数

（离散动作）

图 2.18 不同的价值函数拟合方式。内含参数 w 的灰色方框是函数拟合器

收敛性保证很诱人，但实际上在使用该方法时特征选取或特征表示 φ(s) 有一定难度。如下

是线性方法中构建特征的不同方式：

– 多项式（Polynomials）：基本的多项式族（Polynomial Families）可以用作函数拟合的特

征矢量（Feature Vectors）。假设每一个状态 s = (S1, S2, · · · , Sd)

T 是一个 d 维向量，那

么我们有一个 d 维的多项式基（Polynomial Basis）ϕi(s) = Qd

j=1 S

ci,j

j ，其中每个 ci,j

是集合 {0, 1, · · · , N} 中的一个整数。这构成秩（Order）为 N 的多项式基和 (N + 1)d

个不同的函数。

85

第 2 章 强化学习入门

– 傅立叶基（Fourier Basis）：傅立叶变换（Fourier Transformation）经常用于表示在时间

域或频率域的序列信号。有 N + 1 个函数的一维秩为 N 的傅立叶余弦（Cosine）基为

ϕi(s) = cos(iπs)，其中 s ∈ [0, 1] 且 i = 0, 1, · · · , N。

– 粗略编码（Coarse Coding）：状态空间可以从高维缩减到低维，例如用一个区域覆盖决

定过程（Determination Process）来进行二值化表示（Binary Representation），这被称为

粗略编码。

– 瓦式编码（Tile Coding）：在粗略编码中，瓦式编码对于多维连续空间是一种高效的特

征表示方式。瓦式编码中特征的感知域（Receptive Field）被指定成输入空间的不同分

割（Partitions）。每一个分割称为一个瓦面（Tilling），而分割中的每一个元素称为一个

瓦片（Tile）。许多有着重叠感知域的瓦面往往被结合使用，以得到实际的特征矢量。

– 径向基函数（Radial Basis Functions，RBF）：径向基函数自然地泛化了粗略编码，粗

略编码是二值化的，而径向基函数可用于 [0, 1] 内的连续值特征。典型的 RBF 是以高

斯函数（Gaussian）的形式 ϕi(s) = exp(−

∥s−ci∥

2

2σ

2

i

)，其中 s 是状态，ci 是特征的原型

（Prototypical）或核心状态（Center State），而 σi 是特征宽度（Feature Width）。

• 非线性方法（Non-Linear Methods）：

– 人工神经网络（Artificial Neural Networks）：不同于以上的函数拟合方法，人工神经网络

被广泛用作非线性函数拟合器，它被证明在一定条件下有普遍的拟合能力（Universal

Approximation Ability）(Leshno et al., 1993)。基于深度学习技术，人工神经网络构成了

现代基于函数拟合的深度强化学习方法的主体。一个典型的例子是 DQN 算法，使用

人工神经网络来对 Q 值进行拟合。

• 其他方法：

– 决策树（Decision Trees）：决策树 (Pyeatt et al., 2001) 可以用来表示状态空间，通过使

用决策节点（Decision Nodes）对其分割。这构成了一种重要的状态特征表示方法。

– 最近邻（Nearest Neighbor）方法：它测量了当前状态和内存中之前状态的差异，并用

内存中最接近状态的值来近似当前状态的值。

使用价值函数拟合的好处不仅包括可以扩展到大规模任务，以及便于在连续状态空间中进

行从所见状态到未见过状态的泛化，而且可以减少或缓解人为设计特征来表示状态的需要。对

于无模型方法，拟合器的参数 w 可以用蒙特卡罗（Monte-Carlo，MC）或时间差分（Temporal

Difference，TD）学习来更新，可以对批量样本进行参数更新而非像基于表格的方法一样逐个

更新。这使得处理大规模问题时有较高的计算效率。对基于模型的方法，参数可以用动态规划

（Dynamic Programming，DP）来更新。关于 MC、TD 和 DP 的细节在之前已经有所介绍。

可能的函数拟合器包括特征的线性组合、神经网络、决策树和最近邻方法等。神经网络因其

很好的可扩展性和对多样函数的综合能力而成为深度强化学习方法中最实用的拟合方法。神经网

络是一个可微分方法，因而可以基于梯度进行优化，这提供了在凸（Convex）函数情况下收敛到

最优的保证。然而，实践中，它可能需要极大量的数据来训练，而且可能造成其他困难。

86

2.7 策略优化

将深度学习问题扩展到强化学习带来了额外的挑战，包括非独立同分布（Not Independently

and Identically Distributed）的数据。绝大多数监督学习方法建立在这样一个假设之上，即训练数

据是从一个稳定的独立同分布 (Schmidhuber, 2015) 中采样得到的。然而，强化学习中的训练数据

通常包括高度相关的样本，它们是在智能体和环境交互中顺序得到的，而这违反了监督学习中的

独立性条件。更糟的是，强化学习中的训练数据分布通常是不稳定的，因为价值函数经常根据当

前策略来估计，或者至少受当前策略对状态的访问频率影响，而策略是随训练一直在更新的。智

能体通过对在状态空间探索不同部分来学习。所有这些情况违反了样本数据来自同分布的条件。

在强化学习中使用价值函数拟合对表征方式也有一些实际要求，而如果没有适当地考虑到这

些实际要求，将可能导致发散的情况的发生 (Achiam et al., 2019)。具体来说，不稳定性和发散带来

的危险在以下三个条件同时发生时就会产生：（1）在一个转移分布（Distribution of Transitions）上

训练，而这个分布不满足由一个过程自然产生且这个过程的期望值被估计（比如在离线学习中）

的条件；（2）可扩展的函数拟合，比如，线性半梯度（Semi-Gradient）；（3）自举（Bootstrapping），

比如 DP 和 TD 学习。这三个主要属性只有在它们被结合时会导致学习的发散，而这被称为死亡

三件套（the Deadly Triad）(Van Hasselt et al., 2018)。在使用函数拟合的方式不足够公正的情况下，

基于价值的方法使用函数拟合时可能会有过估计或欠估计（Over-/Under-Estimation）的问题。举

例来说，原始 DQN 有 Q 值过估计（Over-Estimation）的问题 (Van Hasselt et al., 2016)，这在实践

中会导致略差的学习表现，而 Double/Dueling DQN 技术被提出来缓解这个问题。总体来说，使用

策略梯度的基于策略的方法相比基于价值的方法有更好的收敛性保证。

基于梯度的价值函数拟合

考虑参数化的价值函数 V

π

(s) = V

π

(s; w) 或 Qπ

(s, a) = Qπ

(s, a; w)，我们可以基于不同的估

计方法得到相应的更新规则。优化目标被设置为估计函数 V

π

(s; w)（或 Qπ

(s, a; w)) 和真实价值

函数 vπ(s)（或 qπ(s, a)）间的均方误差（Mean-Squared Error，MSE）：

J(w) = Eπ[(V

π

(s; w) − vπ(s))2

] (2.85)

或

J(w) = Eπ[(Q

π

(s, a; w) − qπ(s, a))2

] (2.86)

因此，用随机梯度下降（Stochastic Gradient Descent）法所得到的梯度为

∆w = α(V

π

(s; w) − vπ(s))∇wV

π

(s; w) (2.87)

87

第 2 章 强化学习入门

或

∆w = α(Q

π

(s, a; w) − qπ(s, a))∇wQ

π

(s, a; w) (2.88)

其中梯度对批中的每一个样本进行计算，而权重以一种随机的方式进行更新。上述等式中的目

标价值函数 vπ 或 qπ 通常是被估计的，有时使用一个目标网络（DQN 中）或一个最大化算子

（Q-Learning 中）等。我们在这里展示价值函数的一些基本估计方式。

对 MC 估计，目标值是用采样的回报 Gt 估计的。因此，价值函数参数的更新梯度为

∆wt = α(V

π

(St; wt) − Gt)∇wtV

π

(St; wt) (2.89)

或

∆wt = α(Q

π

(St, At; wt) − Gt+1)∇wtQ

π

(St, At; wt) (2.90)

对TD(0)，根据式(2.84)表示的贝尔曼最优方程，目标值是时间差分的目标函数Rt+γVπ(St+1; wt)，

因此：

∆wt = α(V

π

(St; wt) − (Rt + γVπ(St+1; wt)))∇wtV

π

(St; wt) (2.91)

或

∆wt = α(Q

π

(St, At; wt) − (Rt+1 + γQπ(St+1, At+1; wt))∇wtQ

π

(St, At; wt)) (2.92)

对 TD(λ)，目标值是 λ-回报即 Gλ

t ，因此更新规则是

∆wt = α(V

π

(St; wt) − G

λ

t

)∇wtV

π

(St; wt) (2.93)

或

∆wt = α(Q

π

(St, At; wt) − G

λ

t

)∇wtQ

π

(St, At; wt) (2.94)

不同的估计方式对偏差和方差有不同的侧重，这在之前的小节中已经有所介绍，比如 MC 和 TD

估计方法等。

88

2.7 策略优化

例子：深度 Q 网络

深度 Q 网络（DQN）是基于价值优化的典型例子之一。它使用一个深度神经网络来对 QLearning 中的 Q 值函数进行拟合，并维护一个经验回放缓存（Experience Replay Buffer）来存储

智能体-环境交互中的转移样本。DQN 也使用了一个目标网络 QT，而它由原网络 Q 的参数副本

来参数化，并且以一种延迟更新的方式，来稳定学习过程，也即缓解深度学习中非独立同分布数

据的问题。它使用如式 (2.88) 中的 MSE 损失，以及用贪心的拟合函数 r + γ maxa′ QT(s

′

, a′

) 替

代真实价值函数 qπ。

经验回放缓存为学习提供了稳定性，因为从缓存中采样到的随机批量样本可以缓解非独立同

分布的数据问题。这使得策略更新成为一种离线的（Off-Policy）方式，由于当前策略和缓存中来

自先前策略的样本间的差异。

2.7.3 基于策略的优化

在开始介绍基于策略的优化（Policy-Based Optimization）之前，我们首先介绍在强化学习中

常见的一些策略。如之前小节中所介绍，强化学习中的策略可以被分为确定性（Deterministic）和

随机性（Stochastic）策略。在深度强化学习中，我们使用神经网络来表示这两类策略，称为参数

化策略（Parameterized Policies）。具体来说，这里的参数化指抽象的策略用神经网络（包括单层

感知机）参数来，而非其他参量来表示。使用神经网络参数 θ，确定性和随机性策略可以分别写

作 At = µθ(St) 和 At ∼ πθ(·|St)。

在深度强化学习领域，有一些常见的具体分布用来表示随机性策略中的动作分布：伯努利

分布（Bernoulli Distribution），类别分布（Categorical Distribution）和对角高斯分布（Diagonal

Gaussian Distribution）。伯努利和类别分布可以用于离散动作空间，如二值的（Binary）或多类别

的（Multi-Category），而对角高斯分布可以用于连续动作空间。

一个以 θ 为参数的单变量 x ∈ {0, 1} 的伯努利分布为 P(s; θ) = θ

x

(1 − θ)

(1−x)。因而它可以

被用于表示二值化的动作，可以是单维，也可以是多维（对一个矢量中含多个变量的情况应用），

它可以用作二值化动作策略（Binary-Action Policy）。

类别型策略（Categorical Policy）使用类别分布作为它的输出，因而可以用于离散且有限的

动作空间，它将策略视为一个分类器（Classifier），以状态为条件（Conditioned on A State）而输

出在有限动作空间中每个动作的概率，比如 π(a|s) = P [At = a|St = s]。所有概率和为 1，因此，

当将类别型策略参数化时，最后输出层（Output Layer）常用 Softmax 激活函数。这里我们具体使

用 P [·|·] 矩阵表示有限动作空间的情况，来替代概率函数 p(·|·)。智能体可以根据类别分布采样

选择一个动作。实践中，这种情况下的动作通常可以编码为一个独热编码矢量（One-Hot Vector）

ai = (0, 0, · · · , 1, · · · , 0)，这个矢量跟动作空间有相同的维度，从而 ai ⊙p(·|s) 给出 p(ai

|s)，其中

⊙ 是逐个元素的乘积（Element-Wise Product）算子，而 p(·|s) 是给定状态 s 时的矩阵中的一个矢量

（行或列，依状态动作顺序而定），而这通常也是归一化后类别型策略的输出层。耿贝尔-Softmax

89

第 2 章 强化学习入门

函数技巧（Gumbel-Softmax Trick）可以在实践中参数化类别型策略后用来保持类别分布采样过

程的可微性。在没有使用其他技巧的情况下，有采样过程或像 arg max 类操作的随机性节点往往

是不可微的（Non-Differentiable），从而在对参数化策略使用基于梯度的优化（在随后小节中介

绍）时可能是有问题的。

耿贝尔-Softmax 函数技巧（Gumbel-Softmax Trick）：首先，耿贝尔-最大化技巧（Gumbel-Max

Trick）允许我们从类别分布 π 中采样

z = one_hot[arg max

i

(zi + log πi)] (2.95)

其中“one_hot”是一个将标量转换成独热编码矢量的操作。然而，如上所述，arg max 操作通常

是不可微的。因此，在耿贝尔-Softmax 函数技巧中，一个 Softmax 操作被用来对耿贝尔-最大化技

巧中的 arg max 进行连续性近似：

ai =

exp((log πi + gi)/τ )

P

j

exp((log πj + gj )/τ

, ∀i = 0, · · · , k (2.96)

其中 k 是欲求变量 a（强化学习策略的动作选择）的维度，而 gi 是采样自耿贝尔分布（Gumbel

Distribution）的耿贝尔（Gumbel）变量。耿贝尔 (0, 1) 分布可以用逆变换（Inverse Transform）采

样实现，通过采样均匀分布 u ∼ Uniform(0, 1) 并计算 g = − log(− log(u)) 得到。

对角高斯策略（Diagonal Gaussian Policy）输出一个对角高斯分布的均值和方差用于连续动

作空间。一个普通的多变量高斯分布包括一个均值矢量 µ 和一个协方差（Covariance）矩阵 Σ，

而对角高斯分布是其特殊情况，即协方差矩阵只有对角元非零，因此我们可以用一个矢量 σ 来表

示它。当使用对角高斯分布来表示概率性动作时，它移除了不同动作维度间的协相关性。一个策

略被参数化时，如下所示的再参数化（Reparametrization）技巧（与 Kingma et al. (2014) 提出的变

分自动编码器中类似）可以被用来从均值和方差矢量表示的高斯分布中采样，同时保持操作的可

微性。

再参数化技巧：从对角高斯分布中采样动作 a ∼ N (µθ,σθ)，该分布的均值和方差矢量为 µθ

和 σθ（参数化的），而这可以通过从正态分布中采样一个隐藏矢量 z ∼ N (0, I) 来得到动作：

a = µθ + σθ ⊙ z (2.97)

其中 ⊙ 是两个相同形状矢量的逐个元素乘积。

深度强化学习中的常用策略如图 2.19 所示，便于读者理解。

基于策略的优化（Policy-Based Optimization）方法在强化学习情景下直接优化智能体的策略

而不估计或学习动作价值函数。采样得到的奖励值通常用于改进动作选择的优化过程，而优化过

程可以使用基于梯度或无梯度（Gradient-Free）的方法。其中，基于梯度的方法通常采用策略梯

90

2.7 策略优化

度（Policy Gradient），它在某种程度上代表了连续动作强化学习最受欢迎的一类算法，受益于对

高维情况的可扩展性。典型的基于梯度优化方法包括 REINFORCE 等。无梯度方法对策略搜索中

相对简单的情况通常有更快的学习过程，无须有复杂计算的求导过程。典型的无梯度类方法包括

交叉熵（Cross-Entropy，CE）方法等。

深度强化学习中的常⽤策略

确定性策略

随机性策略

⼆值化动作策略

类别型策略

⾼斯分布

其他：⾃回归策略等

图 2.19 深度强化学习中的不同策略类型

回想我们在强化学习中智能体的目标是从期望或估计的角度去最大化从一个状态开始的累

计折扣奖励（Cumulative Discounted Reward），可以将其表示为

J(π) = Eτ∼π[R(τ )] (2.98)

其中 R(τ ) = PT

t=0 γ

tRt 是有限步（适用于多数情形）的折扣期望奖励，而 τ 是采样的轨迹。

基于策略的优化方法将根据以上目标函数 J(π) 通过基于梯度的或无梯度的方法，来优化策

略 π。我们将首先介绍基于梯度的方法，并给出一个 REINFORCE 法的例子，随后介绍无梯度的

算法和 CE 方法的例子。

基于梯度的优化

基于梯度的优化方法是使用在期望回报（总的奖励）上的梯度估计来进行梯度下降（或上

升），以改进策略，而这个期望回报是从采样轨迹中得到的。这里我们把关于策略参数的梯度叫

作策略梯度（Policy Gradient），具体表达式如下：

∆θ = α∇θJ(πθ) (2.99)

其中 θ 表示策略参数，而 α 是学习率。基于策略参数的梯度计算方法叫作策略梯度法。文献 (Sutton

et al., 2000) 和文献 (Silver et al., 2014) 提出的策略梯度定理（Policy Gradient Theorem）及其证明

将在下面介绍。

注：式 (2.99) 中参数 θ 的表示方法实际上是不合适的，根据本书默认的格式，它应当是 θ 从

而表示矢量。然而，这里我们使用基本的 θ 格式作为一种可以在使用模型参数时替代的 θ 的方

式，而这种简单的写法也在文献中常见。一种考虑这种写法合理性的方式是：参数的梯度可以对

91

第 2 章 强化学习入门

每个参数分别得到，而每个参数均可单独表示为 θ，只要方程对所有参数相同，它就可以用 θ 来

表示所有参数。本书的其余章节将遵循以上声明。

定理 2.2 策略梯度定理

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ(log πθ(At|St))Q

πθ

(St, At)



 (2.100)

= ESt∼ρπ,At∼πθ

[∇θ(log πθ(At|St))Q

πθ

(St, At)] (2.101)

其中第二项需定义折扣状态分布（Discounted State Distribution）ρ

π

(s

′

) := R

S

PT

t=0 γ

t−1 ρ0(s)

p(s

′

|s, t, π)ds，而 p(s

′

|s, t, π) 是在策略 π 下第 t 个时间步从 s 到 s

′ 的转移概率（Transition Probability），参见文献 (Silver et al., 2014)。

策略梯度定理对随机性策略和确定性策略都适用。它起初由 Sutton 等人 (Sutton et al., 2000)

为随机性策略而提出，后被 Silver 等人 (Silver et al., 2014) 扩展到确定性策略。对确定性的情况，

尽管确定性策略梯度（Deterministic Policy Gradient，DPG）定理（后续介绍）与上述策略梯度定

理看起来不同，实际上可以证明确定性策略梯度只是随机性策略梯度（Stochastic Policy Gradient，

SPG）的一种特殊（极限）情况。若用一个确定性策略 µθ : S → A 和一个方差参数 σ 来参数化

随机性策略 πµθ,σ，则有 σ = 0 时随机性策略等价于确定性策略，即 πµθ,0 ≡ µ。

(1) 随机性策略梯度

首先我们对随机性策略证明策略梯度定理，因而被称为随机性策略梯度方法。为了简便，在

本小节中，我们假设有限 MDP 下的片段式（Episodic）设定，每个轨迹长度固定为 T + 1。考虑

一个参数化的随机性策略 πθ(a|s)，对以 ρ0(S0) 为初始状态分布的 MDP 过程，有轨迹的概率为

p(τ |π) = ρ0(S0)

QT

t=0 p(St+1|St, At)π(At|St)，因而可以得到基于参数化策略 πθ 的轨迹概率的对

数（Logarithm）为

log p(τ |θ) = log ρ0(S0) +X

T

t=0



log p(St+1|St, At) + log πθ(At|St)



. (2.102)

我们也需要对数-导数技巧（Log-Derivative Trick）：∇θp(τ |θ) = p(τ |θ)∇θ log p(τ |θ) 得到轨迹概率

对数（Log-Probability）的导数为

∇θ log p(τ |θ) = ∇θ log ρ0(S0) +X

T

t=0



∇θ log p(St+1|St, At) + ∇θ log πθ(At|St)



(2.103)

=

X

T

t=0

∇θ log πθ(At|St). (2.104)

92

2.7 策略优化

其中包含 ρ0(S0) 和 p(St+1|St, At) 的项被移除，因为它们不依赖于参数 θ，尽管是未知的。

回想之前介绍过，学习目标是最大化期望累计奖励（Expected Cumulative Reward）：

J(πθ) = Eτ∼πθ



R(τ )



= Eτ∼πθ





X

T

t=0

Rt



 =

X

T

t=0

Eτ∼πθ

[Rt] , (2.105)

其中 τ = (S0, A0, R0, · · · , ST , AT , RT , ST +1) 且 R(τ ) = PT

t=0 Rt。我们可以直接在策略参数 θ 上

进行梯度上升来逐渐改进策略 πθ 的表现。

注意 Rt 只依赖 τt，其中 τt = (S0, A0, R0, · · · , St, At, Rt, St+1)。

∇θEτ∼πθ

[Rt] = ∇θ

Z

τt

Rtp(τt|θ)dτt 展开期望 (2.106)

=

Z

τt

Rt∇θp(τt|θ)dτt 对换梯度和积分 (2.107)

=

Z

τt

Rtp(τt|θ)∇θ log p(τt|θ)dτt 对数-导数技巧 (2.108)

= Eτ∼πθ



Rt∇θ log p(τt|θ)



回归期望形式 (2.109)

上面第三个等式是根据之前介绍的对数-导数技巧得到的。

将上面式子代入到 J(πθ)，

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

Rt∇θ log p(τt|θ)



 .

现在我们需要计算 ∇θ log pθ(τt)，其中 pθ(τt) 依赖于策略 πθ 和模型 p(Rt, St+1|St, At) 的真

实值，而该模型对智能体是不可用的。幸运的是，为了使用策略梯度方法，我们只需要 log pθ(τt)

的梯度而不是它本身的值，而这可以简单地用 τt = τ0:t 替换式 (2.104) 中的 τ = τ0:T 而得到下式：

∇θ log p(τt|θ) = Xt

t

′=0

∇θ log πθ(At

′ |St

′ ). (2.110)

从而

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

Rt∇θ

Xt

t

′=0

log πθ(At

′ | St

′ )





93

第 2 章 强化学习入门

= Eτ∼πθ





X

T

t

′=0

∇θ log πθ(At

′ | St

′ )

X

T

t=t

′

Rt



 . (2.111)

这里最后一个等式是加法重排（Rearranging the Summation）。

注意，我们在以上推导过程中使用了加法和期望之间的置换，以及期望和加法与求导之间的

置换（都是合理的），如下：

∇θJ(πθ) = ∇θEτ∼πθ



R(τ )



= ∇θEτ∼πθ





X

T

t=0

Rt



 =

X

T

t=0

∇θEτ∼πθ

[Rt] (2.112)

其最终在式 (2.106) 中对长度为 t + 1 的部分轨迹 τt 进行积分。然而，也有其他方式来对整个轨迹

的累计奖励取期望：

∇θJ(πθ) = ∇θEτ∼πθR(τ ) (2.113)

= ∇θ

Z

τ

p(τ |θ)R(τ ) 展开期望 (2.114)

=

Z

τ

∇θp(τ |θ)R(τ ) 对换梯度和积分 (2.115)

=

Z

τ

p(τ |θ)∇θ log p(τ |θ)R(τ ) 对数-导数技巧 (2.116)

= Eτ∼πθ

[∇θ log p(τ |θ)R(τ )] 回归期望形式 (2.117)

⇒ ∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)R(τ )



 (2.118)

= Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)

X

T

t

′=0

Rt

′



 (2.119)

仔细的读者可能注意到式 (2.119) 的第二个结果与式 (2.111) 的第一个结果有一些差别。具体

来说，累计奖励的时间范围是不同的。第一个结果只使用了动作 At 之后的累计未来奖励 PT

t=t

′ Rt

来评估动作，而第二个结果使用整个轨迹上的累计奖励 PT

t=0 Rt 来评估该轨迹上的每个动作 At，

包括选择那个动作之前的奖励。直觉上，一个动作不应该用这个动作执行以前的奖励值来对其进

行估计，而这也得到数学上的证明，即这个动作之前的奖励对最终期望梯度只有零影响。因此可

以在推导策略梯度的过程中直接丢掉那些过去的奖励值来得到式 (2.111)，而这被称为“将得到的

奖励（Reward-to-Go）”策略梯度。这里我们不给出两种策略梯度公式等价性的严格证明，感兴趣

的读者可以参考相关资料。这里的两种导出方式也可以作为两个结果等价性的论证。

上述公式中的 ∇ 称“nabla”，它是一个物理和数学领域有着三重意义的算子（梯度、散度、

94

2.7 策略优化

和旋度），依据它做操作的对象而定。而在计算机领域，这个“nabla”算子 ∇ 通常用作偏微分

（Partial Derivative），其对紧跟的对象中显式（Explicitly）包含的变量进行求导，而这个变量写在

算子脚标的位置。由于上式中的 R(τ ) 不显式包含 θ，因此 ∇θ 不作用于 R(τ )，尽管 τ 可以隐式

（Implicitly）依赖于 θ（根据 MDP 的图模型）。我们也注意到式 (2.119) 的期望可以用采样均值来

估计。如果我们收集一个轨迹的集合 D = {τi}i=1,··· ,N，而其中的轨迹是通过使智能体以策略 πθ

在环境中做出动作来得到的，那么策略梯度可以用以下方式估计：

gˆ =

1

|D|

X

τ∈D

X

T

t=0

∇θ log πθ(At|St)R(τ ), (2.120)

EGLP（Expected Grad-Log-Prob）引理在策略梯度优化中经常用到，所以我们在这里介绍它。

引理 2.2 EGLP 引理：假设 pθ 是随机变量 x 的一个参数化的概率分布，那么有

Ex∼pθ

[∇θ log Pθ(x)] = 0. (2.121)

证明: 由于所有概率分布都是归一化的：

Z

x

pθ(x) = 1. (2.122)

对上面归一化条件两边取梯度：

∇θ

Z

x

pθ(x) = ∇θ1 = 0. (2.123)

使用对数-导数技巧得到：

0 = ∇θ

Z

x

pθ(x) (2.124)

=

Z

x

∇θpθ(x) (2.125)

=

Z

x

pθ(x)∇θ log pθ(x) (2.126)

∴ 0 = Ex∼pθ

[∇θ log pθ(x)]. (2.127)

从 EGLP 引理我们可以直接得出：

EAt∼πθ

[∇θ log πθ(At|St)b(St)] = 0. (2.128)

其中 b(St) 称为基准（Baseline），而它是独立于用于求期望值的未来轨迹的。基准可以是任何一

95

第 2 章 强化学习入门

个只依赖当前状态的函数，而不影响优化公式中的总期望值。

上面公式中的优化目标最终为

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)R(τ )



 (2.129)

我们也可以更改整个轨迹的奖励 R(τ ) 为在 t 时间步后将得到的奖励 Gt：

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)Gt



 (2.130)

通过以上 EGLP 引理，期望回报可以被推广为

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)Φt



 (2.131)

其中 Φt =

PT

t

′=t

(R(St

′ , at

′ , St

′+1) − b(St))。

为了便于实际使用，Φt 可以变成以下形式：

Φt = Q

πθ

(St, At) (2.132)

或

Φt = A

πθ

(St, At) = Q

πθ

(St, At) − V

πθ

(St) (2.133)

而它们都可以证明等价于期望内的原始形式，只是在实际中有不同的方差。这些证明需要重复期

望规则（the Law of Iterated Expectations）：对两个随机变量（离散或连续）有 E[X] = E[E[X|Y ]]。

而这个式子很容易证明。剩余的证明如下：

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ log πθ(At|St)R(τ )



 (2.134)

=

X

T

t=0

Eτ∼πθ

[∇θ log πθ(At|St)R(τ )] (2.135)

=

X

T

t=0

Eτ:t∼πθ

[Eτt:∼πθ

[∇θ log πθ(At|St)R(τ )|τ:t]] (2.136)

96

2.7 策略优化

=

X

T

t=0

Eτ:t∼πθ

[∇θ log πθ(At|St)Eτt:∼πθ

[R(τ )|τ:t]] (2.137)

=

X

T

t=0

Eτ:t∼πθ

[∇θ log πθ(At|St)Eτt:∼πθ

[R(τ )|St, At]] (2.138)

=

X

T

t=0

Eτ:t∼πθ

[∇θ(log πθ(At|St))Q

πθ

(St, At)] (2.139)

其中 Eτ [·] = Eτ:t

[Eτt:

[·|τ:t]] 且 τ:t = (S0, A0, · · · , St, At) 和 Qπθ (St, At) = Eτt:∼πθ

[R(τ )| St, At]。

所以，文献中有常见的形式：

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ(log πθ(At|St))Q

πθ

(St, At)



 (2.140)

或

∇θJ(πθ) = Eτ∼πθ





X

T

t=0

∇θ(log πθ(At|St))A

πθ

(St, At)



 (2.141)

换句话说，它等价于改变优化目标为 J(πθ) = Eτ∼π[Qπθ (St, At)] 或 J(πθ) = Eτ∼π[Aπθ (St, At)]

来替换原始形式 Eτ∼π[R(τ )]。对于优化策略来说，实践中常用 Aπθ (St, At) 来估计 TD-误差（TD

Error）。

根据是否使用环境模型，强化学习算法可以被分为无模型（Model-Free）和基于模型的（ModelBased）两类。对于无模型强化学习，单纯基于梯度的优化算法可以追溯至 REINFORCE 算法，或

称策略梯度算法。而基于模型的强化学习算法一类，也有一些基于策略的算法，比如使用贯穿时

间的反向传播（Backpropagation Through Time，BPTT）来根据一个片段内的奖励去更新策略。

例子：REINFORCE 算法

REINFORCE 是一个使用式 (2.131) 的随机性策略梯度方法的算法，其中 Φt = Qπ

(St, At)，

而在 REINFORCE 中，它通常用轨迹上采样的奖励值 Gt =

P∞

t

′=t Rt

′（或折扣版本 Gt =

P∞

t

′=t

γ

t

′−tRt

′）来估计。更新策略的梯度为

g = E





X∞

t=0

X∞

t

′=t

Rt

′∇θ log πθ(At|St)



 (2.142)

97

第 2 章 强化学习入门

(2) 确定性策略梯度

以上介绍的属于随机性策略梯度（Stochastic Policy Gradient, SPG），它用于优化随机性策略

π(a|s)，即用一个基于当前状态的概率分布来表示动作的情况。与随机性策略相对的是确定性策

略，其中 a = π(s) 是一个确定性动作而非概率分布。我们可以用类似于 SPG 的方法得到 DPG，

且它在数值上（作为一种极限情况）遵循策略梯度定理，尽管有不同的显式表示。

注：在本小节后续部分，我们使用 µ(s) 代替之前定义的 π(s) 来表示确定性策略，从而消除

它与随机性策略 π(a|s) 间的歧义。

对于 DGP 的更严格和广泛的定义，我们参考由文献 (Silver et al., 2014) 提出的确定性策略梯

度定理，即式 (2.151)。在此之前，我们将逐步介绍确定性策略梯度定理并证明它，先用一种在线

策略的方式而后用离线策略的方式，同时我们也将详细讨论 DPG 和 SPG 间的关系。

首先，我们定义确定性策略的表现目标，与随机性策略梯度求解过程中的期望折扣奖励采用

同样的定义：

J(µ) = ESt∼ρµ,At=µ(St)





X∞

t=1

γ

t−1R(St, At)



 (2.143)

=

Z

S

Z

S

X∞

t=1

γ

t−1

ρ0(s)p(s

′

|s, t, µ)R(s

′

, µ(s

′

)]dsds

′

(2.144)

=

Z

S

ρ

µ

(s)R(s, µ(s))ds (2.145)

其中 p(s

′

|s, t, µ) = p(St+1|St, At)p

µ(At|St)，第一个概率是转移概率，而第二个是动作选择概率。

由于它是确定性策略，我们有 p

µ(At|St) = 1，因而 p(s

′

|s, t, µ) = p(St+1|St, µ(St))。此外，上式

中的状态分布是 ρ

µ(s

′

) := R

S

P∞

t=1 γ

t−1ρ0(s)p(s

′

|s, t, µ)ds。

由于式子V

µ(s)=E

P∞

t=1 γ

t−1R(St, At)|S1 = s; µ



=

R

S

P∞

t=1 γ

t−1p(s

′

|s, t, µ)R (s

′

, µ(s

′

)]ds

′

在除使用确定性策略这一点外遵循与随机性策略梯度中相同的定义，我们可以得出

J(µ) = Z

S

ρ0(s)V

µ

(s)ds (2.146)

=

Z

S

Z

S

X∞

t=1

γ

t−1

ρ0(s)p(s

′

|s, t, µ)R(s

′

, µ(s

′

)]dsds

′

(2.147)

这与上面直接用折扣奖励的形式得到的表示式是等价的。这里的关系也对随机性策略梯度适

用，只是将确定性策略 µ(s) 替换成随机性策略 π(a|s) 即可。对于确定性策略，我们有 V

µ(s) =

Qµ(s, µ(s))，因为状态价值对于随机性策略是关于动作分布的期望，而对于确定性策略没有动作

分布而只有单个动作值。因此我们也有对于确定性策略的如下表示：

98

2.7 策略优化

J(µ) = Z

S

ρ0(s)V

µ

(s)ds (2.148)

=

Z

S

ρ0(s)Q

µ

(s, µ(s))ds (2.149)

关于表现目标的不同形式和几个条件将被用来证明 DPG 定理。我们在这里列出这些条件如

下而不给出详细的推导过程，相关内容请参考文献 (Silver et al., 2014)。

• C.1 连续导数的存在性： p(s

′

|s, a), ∇ap(s

′

|s, a), µθ(s), ∇θµθ(s), R(s, a), ∇aR(s, a), ρ0(s) 对

所有参数和变量 s, a, s′ 和 x 连续。

• C.2 有界性条件：存在 a, b 和 L 使得 sups ρ0(s) < b,supa,s,s′ p(s

′

|s, a)

< b,supa,s R(s, a) < b,supa,s,s′ ∥∇ap(s

′

|s, a)∥ < L,supa,s ∥∇aR(s, a)∥ < L。

定理 2.3 确定性策略梯度定理：假设 MDP 满足条件 C.1，即连续的 ∇θµθ(s), ∇aQµ(s, a) 和确定

性策略梯度的存在性，那么

∇θJ(µθ) = Z

S

ρ

µ

(s)∇θµθ(s)∇aQ

µ

(s, a)|a=µθ(s)ds (2.150)

= Es∼ρµ [∇θµθ(s)∇aQ

µ

(s, a)|a=µθ(s)

] (2.151)

证明: 确定性策略梯度定理的证明基本遵循与文献 (Sutton et al., 2000) 的标准随机性策略梯度定

理一样的步骤。首先，为了方便在后续证明中交换导数和积分，以及积分的顺序，我们需要使用

两个引理，它们是微积分里的基本数学公式，如下：

引理 2.3 莱布尼茨积分法则（Leibniz Integral Rule）: f(x, t) 是一个使得 f(x, t) 及其偏导数

f

′

x

(x, t) 在 (x, t)-平面的部分区域上对 t 和 x 连续的函数，包括 a(x) ⩽ t ⩽ b(x), x0 ⩽ x ⩽ x1。同

时假设函数 a(x) 和 b(x) 都是连续的且在 x0 ⩽ x ⩽ x1 上有连续导数。那么，对于 x0 ⩽ x ⩽ x1，

d

dx

Z b(x)

a(x)

f(x, t)dt = f(x, b(x)) ·

d

dx

b(x) − f(x, a(x)) ·

d

dx

a(x) + Z b(x)

a(x)

∂

∂xf(x, t)dt (2.152)

引理 2.4 富比尼定理（Fubini’s Theorem）：假设 X 和 Y 是 σ-有限测度空间（σ-Finite Measure

Space），并且假设 X × Y 由积测度（Product Measure）给出（由于 X 和 Y 是 σ-有限的，这个测

度是唯一的）。富比尼定理声明：如果 f 是 X × Y 可积的，那么 f 是一个可测函数（Measurable

Function）且有

Z

X×Y

|f(x, y)|d(x, y) < ∞ (2.153)

99

第 2 章 强化学习入门

那么

Z

X

(

Z

Y

f(x, y)dy)dx =

Z

Y

(

Z

X

f(x, y)dx)dy =

Z

X×Y

f(x, y)d(x, y) (2.154)

为了满足以上两个引理，我们需要 C.1 所提供的充分条件作为莱布尼茨积分法则的要求，即

V

µθ (s) 和 ∇θV

µθ (s) 是 θ 和 s 的连续函数。我们也遵循状态空间 S 紧致性（Compactness）的假

设，如富比尼定理所要求，即对于任何 θ，∥∇θV

µθ (s)∥，∥∇aQµθ (s, a) |a=µθ(s)∥ 和 ∥∇θµθ(s)∥ 是

s 的有界（Bounded）函数，而这在 C.2 中提供。有了以上条件，我们可以得到以下推导：

∇θV

µθ

(s) = ∇θQ

µθ

(s, µθ(s))

= ∇θ(R(s, µθ(s)) + Z

S

γp(s

′

|s, µθ(s))V

µθ

(s

′

)ds

′

)

= ∇θµθ(s)∇aR(s, a)|a=µθ(s) + ∇θ

Z

S

γp(s

′

|s, µθ(s))V

µθ

(s

′

)ds

′

= ∇θµθ(s)∇aR(s, a)|a=µθ(s) +

Z

S

γ(p(s

′

|s, µθ(s))∇θV

µθ

(s

′

)

+ ∇θµθ(s)∇ap(s

′

|s, a)V

µθ

(s

′

))ds

′

= ∇θµθ(s)∇a(R(s, a) + Z

S

γp(s

′

|s, a)V

µθ

(s

′

)ds

′

)|a=µθ(s))

+

Z

S

γp(s

′

|s, µθ(s))∇θV

µθ

(s

′

)ds

′

= ∇θµθ(s)∇aQ

µθ

(s, a)|a=µθ(s) +

Z

S

γp(s

′

|s, µθ(s))∇θV

µθ

(s

′

)ds

′

(2.155)

在上面的推导中，莱布尼茨积分法则被用于交换求导和积分的顺序，这要求满足 p(s

′

|s, a)，µθ(s)，

V

µθ (s) 和它们的导数对 θ 的连续性条件。现在我们用 ∇θV

µθ (s) 对以上公式进行迭代，得到：

∇θV

µθ

(s) = ∇θµθ(s)∇aQ

µθ

(s, a)|a=µθ(s)

+

Z

S

γp(s

′

|s, µθ(s))∇θµθ(s

′

)∇aQ

µθ

(s

′

, a)|a=µθ(s

′)ds

′

+

Z

S

γp(s

′

|s, µθ(s)) Z

S

γp(s

′′|s

′

, µθ(s

′

))∇θV

µθ

(s

′′)ds

′′ds

′

= ∇θµθ(s)∇aQ

µθ

(s, a)|a=µθ(s)

+

Z

S

γp(s → s

′

, 1, µθ(s))∇θµθ(s

′

)∇aQ

µθ

(s

′

, a)|a=µθ(s

′)ds

′

+

Z

S

γ

2

p(s → s

′

, 2, µθ(s))∇θµθ(s

′

)∇aQ

µθ

(s

′

, a)|a=µθ(s

′)ds

′

+ · · ·

100

2.7 策略优化

=

Z

S

X∞

t=0

γ

t

p(s → s

′

, t, µθ(s))∇θµθ(s

′

)∇aQ

µθ

(s

′

, a)|a=µθ(s

′)ds

′

(2.156)

其中，我们使用富比尼定理来交换积分顺序，而这要求 ∥∇θV

µθ (s)∥ 的有界性条件。上述积分中

包含一种特殊情况，对 s

′ = s 有 p(s → s

′

, 0, µθ(s)) = 1 而对其他 s

′ 为 0。现在我们对修改过的性

能目标即期望价值函数进行求导：

∇θJ(µθ) = ∇θ

Z

S

ρ0(s)V

µθ

(s)ds

=

Z

S

ρ0(s)∇θV

µθ

(s)ds

=

Z

S

Z

S

X∞

t=0

γ

t

ρ0(s)p(s → s

′

, t, µθ(s))∇θµθ(s

′

)∇aQ

µθ

(s

′

, a)|a=µθ(s

′)ds

′ds

=

Z

S

ρ

µθ

(s)∇θµθ(s)∇aQ

µθ

(s, a)|a=µθ(s)ds (2.157)

其中我们使用莱布尼茨积分法则来交换求导和积分顺序，需要满足 ρ0(s) 和 V

µθ (s) 及其导数对

θ 连续的条件，同样由富比尼定理交换积分顺序，需要满足被积函数（Integrand）的有界性条件。

证毕。

离线策略确定性策略梯度

除了上面在线策略版本的确定性策略梯度（DPG）推导，我们也可以用离线策略的方式来

导出 DPG，使用上面的 DPG 定理和 γ-折扣状态分布 ρ

µ(s

′

) := R

S

P∞

t=1 γ

t−1p(s)p (s

′

|s, t, µ)ds。

离线策略确定性策略梯度用行为策略（Behaviour Policy，即使用经验回放池时的先前策略）的样

本来估计当前策略，而这个策略可能跟当前策略不同。在离线策略的设定下，由一个独特的行

为策略 β(s) ̸= µθ(s) 所采集轨迹来对梯度进行估计，相应的状态分布为 ρ

β

(s)，这不依赖于策

略参数 θ。在离线策略情况下，性能目标被修改为目标策略的价值函数在行为策略的状态分布上

的平均 Jβ(µθ) = R

S

ρ

β

(s)V

µ(s)ds =

R

S

ρ

β

(s)Qµ(s, µθ(s))ds，而原始的目标遵循式 (2.149)，即

J(µθ) = R

S

ρ0(s)V

µ(s)ds。注意，这里是我们在导出离线策略确定性策略梯度中进行的第一个近

似，即 J(µθ) ≈ Jβ(uθ)，而我们将在后面有另外一个近似。我们可以直接对修改过的目标取微分

如下：

∇θJβ(µθ) = Z

S

ρ

β

(s)(∇θµθ(s)∇aQ

µθ

(s, a) + ∇θQ

µθ

(s, a))|a=µ(s)ds

≈

Z

S

ρ

β

(s)∇θµθ(s)∇aQ

µθ

(s, a)ds

= Es∼ρβ [∇θµθ(s)∇aQ

µθ

(s, a)|a=µ(s)

] (2.158)

上面式子中的约等于（Approximately Equivalent）符号“≈”表示了在线策略 DPG 和离线策略 DPG

101

第 2 章 强化学习入门

的不同。上式中的依赖关系需要小心处理。因为 ρ

β

(s) 是独立于 θ 的，关于 θ 的导数可以进入积

分中，并且在 ρ

β

(s) 上没有导数。Qµθ (s, µθ(a)) 实际上以两种方式依赖于 θ（其表达式中有两个

µθ）：（1）它依赖于确定性策略 µθ 基于当前状态 s 所决定的动作 a，而（2）对 Q 值的在线策略估计

也依赖于策略 µθ 来在未来状态下选择的动作，如在 Qµθ (s, a) = R(s, a)+R

S

γp(s

′

|s, a)V

µθ (s

′

)ds

′

中所示，所以这个求导需要分别进行。然而，第一个式中的第二项 ∇θQµθ (s, a)|a=µ(s) 在近似中

由于对其估计的困难而被丢掉了，这在离线策略梯度中有类似的相应操作 (Degris et al., 2012)12。

随机性策略梯度和确定性策略梯度的关系

如式 (2.140) 所示，随机性策略梯度与前文策略梯度定理中公式有相同的形式，而式 (2.151)

中的确定性策略梯度看起来却有不一致的形式。然而，可以证明对于相当广泛的随机策略，DPG

是一个 SPG 的特殊（极限）情况。在这种情况下，DPG 也在一定条件下满足策略梯度定理。为了

实现这一点，我们通过一个确定性策略 µθ : S → A 和一个方差参数 σ 来参数化随机性策略 πµθ,σ，

从而对 σ = 0 有随机性策略等价于确定性策略，即 πµθ,0 ≡ µ。为了定义 SPG 和 DPG 之间的关系，

有一个额外的条件需要满足，这是一个定义常规 Delta-近似（Regular Delta-Approximation）的复

合条件。

• C.3 常规 Delta-近似：由 σ 参数化的函数 vσ 被称为一个 R ⊆ A 上的常规 Delta-近似，如果满

足条件：（1）对于a

′ ∈ R和适当平滑的f，vσ 收敛到一个Delta分布limσ↓0

R

A

vσ(a

′

, a)f(a)da =

f(a

′

)；（2）vσ(a

′

, ·) 在紧致而有利普希茨（Lipschitz）边界的 C

′

a ⊆ A 上得到支撑，而在边

界上消失（Vanish）并且在 Ca′ 上连续可微；（3）梯度 ∇a′vσ(a

′

, a) 总是存在；（4）转移不

变性：对任何 a ∈ A, a′ ∈ R, a + δ ∈ A, a′ + δ ∈ A，有 v(a

′

, a) = v(a

′ + δ, a + δ)。

定理2.4 确定性策略梯度作为随机性策略梯度的极限：考虑一个随机性策略πµθ,σ 使得πµθ,σ(a|s) =

vσ(µθ(s), a)，其中 σ 是一个控制方差的参数且 vσ(µθ(s), a) 满足 C.3，又有 MDP 满足 C.1 和 C.2，

那么有，

lim

σ↓0

∇θJ(πµθ,σ) = ∇θJ(µθ) (2.159)

这表示 DPG 的梯度（等号右边）是标准 SPG（等号左边）的极限情况。

以上关系的证明超出了本书的范畴，我们在这里不做讨论。细节参考原文 (Silver et al., 2014)。

确定性策略梯度应用和变体

一种最著名的 DPG 算法是深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG），

它是 DPG 的一个深度学习变体。DDPG 结合了 DQN 和 Actor-Critic 算法来使用确定性策略梯度

并通过一种深度学习的方式更新策略。行动者（Actor）和批判者（Critic）各自有一个目标网络

（Target Network）来便于高样本效率（Sample-Efficient）地学习，但是众所周知，这个算法可能

1关于这个操作的细节和相关论断可以参考原文。

2论文 SILVER D, LEVER G, HEESS N, et al. 2014. Deterministic policy gradient algorithms[C]. 中式（15）在近似操作后

的 Q 项上丢掉了 ∇a，这里我们对其勘误。

102

2.7 策略优化

使用起来有一定挑战性，由于它在实践中往往很脆弱而对超参数敏感 (Duan et al., 2016)。关于

DDPG 算法的细节和实现在后续章节有详细介绍。

从以上可以看到，策略梯度可以用至少两种方式估计：SPG 和 DPG，依赖于具体策略类型。

实际上，它们使用了两种不同的估计器，用变分推断（Variational Inference，VI）的术语来说，SPG

是得分函数（Score Function）估计器，而 DPG 是路径导数（Pathwise Derivative）估计器。

再参数化技巧使得来自价值函数的策略梯度可以用于随机性策略，这被称为随机价值梯度

（Stochastic Value Gradients，SVG）(Heess et al., 2015)。在 SVG 算法中，一个 λ 值通常用于 SVG(λ)，

以表明贝尔曼递归被展开了多少步。举例来说，SVG(0) 和 SVG(1) 表示贝尔曼递归分别被展开 0

和 1 步，而 SVG(∞) 表示贝尔曼递归被沿着有限范围的整个片段轨迹展开。SVG(0) 是一个无模

型方法，它的动作价值是用当前策略估计的，因此价值梯度被反向传播到策略中；而 SVG(1) 是

一个基于模型的方法，它使用一个学得的转移模型来估计下一个状态的值，如论文 (Heess et al.,

2015) 中所述。

一个非常简单但有用的再参数化技巧（Reparameterization Trick）的例子是将一个条件高斯概

率密度 p(y|x) = N (µ(x), σ2

(x)) 写作函数 y(x) = µ(x) + σ(x)ϵ, ϵ ∼ N (0, 1)。因而我们可以按程

序生成样本，先采样 ϵ 再以一种确定性的方式得到 y，这使得对随机性策略的采样过程进行梯度

追踪。实际上根据同样的过程也可以得到从动作价值函数到策略间的反向传播梯度。为了像 DPG

那样通过价值函数来得到随机性策略的梯度，SVG 使用了这个再参数化技巧，并且对随机噪声取

了额外的期望值。柔性 Actor-Critic（Soft Actor-Critic, SAC）和原始 SVG (Heess et al., 2015) 算法

都遵循这个程序，从而可以使用随机性策略进行连续控制。

比如，在 SAC 中，随机性策略被一个均值和一个方差，以及一个从正态分布（Normal Distribution）中采样的噪声项再参数化。SAC 中的优化目标有一个额外的熵相关项：

π

∗ = arg max

π

Eτ∼π





X∞

t=0

γ

t

(R(St, At, St+1) + αH(π(·|St)))



 (2.160)

因此，价值函数和 Q 值函数间的关系变为

V

π

(s) = Ea∼π[Q

π

(s, a)] + αH(π(·|s)) (2.161)

= Ea∼π[Q

π

(s, a) − α log π(a|s)] (2.162)

SAC 中使用的策略是一个 Tanh 归一化高斯分布，这与传统设置不同。SAC 中的动作表示可以使

用如下再参数化技巧：

aθ(s, ϵ) = tanh(µθ(s) + σθ(s) · ϵ), ϵ ∼ N (0, I) (2.163)

103

第 2 章 强化学习入门

由于 SAC 中策略的随机性，策略梯度可以在最大化期望价值函数时使用再参数化技巧得到，即：

max

θ

Ea∼πθ

[Q

πθ

(s, a) − α log πθ(a|s)] (2.164)

= max

θ

Eϵ∼N [Q

πθ

(s, a(s, ϵ)) − α log πθ(a(s, ϵ)|s)] (2.165)

因而，梯度可以经过 Q 网络到策略网络，与 DPG 类似，即：

∇θ

1

|B|

X

St∈B

(Q

πθ

(St, a(St, ϵ)) − α log πθ(a(St, ϵ)|St)) (2.166)

其使用一个采样批 B 来更新策略，而 a(St, ϵ) 通过再参数化技巧来从随机性策略中采样。在这种

情况下，再参数化技巧使得随机性策略能够以一种类似于 DPG 的方式来更新，而所得到的 SVG

是介于 DPG 和 SPG 之间的方法。DPG 也可以被看作 SVG(0) 的一种确定性极限（Deterministic

Limit）。

无梯度优化

除了基于梯度（Gradient-Based）的优化方法来实现基于策略（Policy-Based）的学习，也有

非基于梯度（Non-Gradient-Based）方法，也称无梯度（Gradient-Gree）优化方法，包括交叉熵

（Cross-Entropy，CE）方法、协方差矩阵自适应（Covariance Matrix Adaptation，CMA）(Hansen

et al., 1996)、爬山法（Hill Climbing），Simplex / Amoeba / Nelder-Mead 算法 (Nelder et al., 1965) 等。

例子：交叉熵方法

除了对策略使用基于梯度的优化，CE 方法作为一种非基于梯度的方法，在强化学习中也常用

于快速的策略搜索。在 CE 方法中，策略是迭代更新的，对参数化策略 πθ 的参数 θ 的优化目标为

θ

∗ = arg max S(θ) (2.167)

其中S(θ)是整体目标函数，对于这里的情况，它可以是折扣期望回报（Discounted Expected Return）。

CE 方法中的策略可以被参数化为一个多变量线性独立高斯分布（Multi-Variate Linear Independent Gaussian Distribution），参数矢量在迭代步 t 时的分布为 θt ∼ N(µt,σ

2

t

)。在采了 n 个样本

矢量 θ1, · · · , θn 并评估了它们的值 S(θ1), · · · , S(θn) 后，我们对这些值排序并选取最好的 ⌊ρ · n⌋

个样本，其中 0 < ρ < 1 是选择比率（Selection Ratio）。所选取的样本的指标记为 I ∈ {1, 2, · · · , n}，

分布的均值可以用以下式子更新：

µt+1 =:

P

i∈I

θi

|I|

(2.168)

104

2.7 策略优化

而方差的更新为

σ

2

t+1 :=

P

i∈I

(θi − µt+1)

T(θi − µt+1)

|I|

(2.169)

交叉熵方法是一个有效且普遍的优化算法。然而，此前研究表明 CE 对强化学习问题的适用

性严重局限于一个现象，即分布会过快集中到一个点上。所以，它在强化学习的应用中虽然速度

快，但是也有其他限制，因为它经常收敛到次优策略。一个可以预防较早收敛的标准技术是引入

噪声。常用的方法包括在迭代过程中对高斯分布添加一个常数或一个自适应值到标准差上，比如：

σ

2

t+1 :=

P

i∈I

(θi − µt+1)

T(θi − µt+1)

|I|

+ Zt+1 (2.170)

如在 Szita et al. (2006) 的工作中，有 Zt = max(5 −

t

10 , 0)。

2.7.4 结合基于策略和基于价值的方法

根据以上的初版策略梯度（Vanilla Policy Gradient）方法，一些简单的强化学习任务可以被

解决。然而，如果我们选择使用蒙特卡罗或 TD(λ) 估计，那么产生的更新经常会有较大的方差。

我们可以使用一个如基于价值的优化中的批判者（Critic）来估计动作价值函数。从而，如果我们

使用参数化的价值函数近似方法，将会有两套参数：行动者（Actor）参数和批判者参数。这实际

上形成了一个非常重要的算法结构，叫作 Actor-Critic （AC），典型的算法包括 Q 值 Actor-Critic、

深度确定性策略梯度（DDPG）等。

回想之前小节中介绍的策略梯度理论，性能目标 J 关于策略参数 θ 的导数为

∇θJ(πθ) = Eτ∼πθ

X

T

t=0

∇θ log πθ(At|St)Q

π

(St, At) (2.171)

其中 Qπ

(St, At) 是真实动作价值函数，而最简单的估计 Qπ

(St, At) 的方式是使用采样得到的

累计奖励 Gt =

P∞

t=0 γ

t−1R(St, At)。在 AC 中，我们使用一个批判者来估计动作价值函数：

Qw(St, At) ≈ Qπ

(St, At)。因此 AC 中策略的更新规则为

∇θJ(πθ) = Eτ∼πθ

X

T

t=0

∇θ log πθ(At|St)Q

w(St, At) (2.172)

其中 w 为价值函数拟合中批判者的参数。批判者可以用一个恰当的策略评估算法来估计，比如时

间差分（Temporal Difference，TD）学习，像式 (2.92) 中对 TD(0) 估计的 ∆w = α(Qπ

(St, At; w) −

Rt+1 + γvπ(St+1, w))∇wQπ

(St, At; w)。

尽管 AC 结构可以帮助减小策略更新中的方差，它也会引入偏差和潜在的不稳定（Poten105

第 2 章 强化学习入门

tial Instability）因素，因为它将真实的动作价值函数替换为一个估计的，而这需要兼容函数近似

（Compatible Function Approximation）条件来保证无偏差估计，如文献 (Sutton et al., 2000) 所提

出的。

兼容函数近似

兼容函数近似条件对 SPG 和 DPG 都适用。我们将对它们分别展示。这里的“兼容”指近似

动作价值函数 Qw(s, a) 与相应策略之间是兼容的。

对于SPG：具体来说，兼容函数近似提出了两个条件来保证使用近似动作价值函数Qπ

(s, a)时

的无偏差估计（Unbiased Estimation）：（1）Qw(s, a) = ∇θ log πθ(a|s)

Tw 和（2）参数w 被选择为能够

最小化均方误差（Mean-Squared Error，MSE）MSE(w) = Es∼ρπ,a∼πθ

[(Qw(s, a)−Qπ

(s, a))2

] 的。更

直观地，条件（1）是说兼容函数拟合器对随机策略的“特征”是线性的，该“特征”为∇θ log πθ(a|s)，

而条件（2）要求参数 w 是从这些特征估计 Qπ

(s, a) 这个线性回归（Linear Regression）问题的解。

实际上，条件（2）经常被放宽以支持策略评估算法，这些算法可以用时间差分学习来更高效地

估计价值函数。

如果以上两个条件都被满足，那么 AC 整体算法等价于没有使用批判者做近似，如 REINFORCE 算法中那样。这可以通过使得条件（2）中的 MSE 为 0 并计算梯度，然后将条件（1）代

入来证明：

∇wMSE(w) = E[2(Q

w(s, a) − Q

π

(s, a))∇wQ

w(s, a)]

= E[2(Q

w(s, a) − Q

π

(s, a))∇θ log πθ(a|s)]

= 0

⇒ E[Q

w(s, a)∇θ log πθ(a|s)] = E[Q

π

(s, a)∇θ log πθ(a|s)] (2.173)

对于 DPG: 兼容函数近似中的两个条件应按照确定性策略 µθ(s) 做相应修改：（1）∇aQw(s, a)

|a=µθ(s) = ∇θµθ(s)

Tw 而（2）w 最小化均方误差，MSE(θ, w) = E[ϵ(s; θ, w)

Tϵ (s; θ, w)]，其中

ϵ(s; θ, w) = ∇aQw(s, a)|a=µθ(s) − ∇aQw(s, a)|a=µθ(s)。同样可以证明这些条件能够保证无偏差估

计，通过将拟合过程所做近似转化成一个无批判者的情况：

∇wMSE(θ, w) = 0 (2.174)

⇒ E[∇θµθ(s)ϵ(s; θ, w)] = 0 (2.175)

⇒ E[∇θµθ(s)∇aQ

w(s, a)|a=µθ(s)

] = E[∇θµθ(s)∇aQ

µ

(s, a)|a=µθ(s)

] (2.176)

它对在线策略 Es∼ρµ [·] 和离线策略 Es∼ρβ [·] 的情况都适用。

106

参考文献

其他方法

如果我们在式 (2.171) 中用优势函数（Advantage Function）替换动作价值函数 Qπ

(s, a)（由于

减掉基准值不影响梯度）：

A

πθ

(s, a) = Q

πθ

(s, a) − V

πθ

(s) (2.177)

那么我们实际可以得到一个更先进的算法叫作优势 Actor-Critic（Advantage Actor-Critic，A2C），

它可以使用 TD 误差来估计优势函数。这对前面提出的理论和推导不产生影响，但会改变梯度估

计的方差。

近来，人们提出了无行动者（actor-free）方法，比如 QT-Opt 算法 (Kalashnikov et al., 2018) 和

Q2-Opt 算法 (Bodnar et al., 2019)。这些方法也结合了基于策略和基于价值的优化，具体是无梯度

的 CE 方法和 DQN。它们使用动作价值拟合（Action Value Approximation）来学习 Qπθ (s, a)，而

不是使用采样得到的折扣回报作为高斯分布中采样动作的估计，这被证明对现实中机器人学习更

高效和有用，尤其是当有示范数据的时候。

参考文献

ACHIAM J, KNIGHT E, ABBEEL P, 2019. Towards characterizing divergence in deep q-learning[J].

arXiv preprint arXiv:1903.08894.

AUER P, CESA-BIANCHI N, FREUND Y, et al., 1995. Gambling in a rigged casino: The adversarial

multi-armed bandit problem[C]//Proceedings of IEEE 36th Annual Foundations of Computer Science.

IEEE: 322-331.

BODNAR C, LI A, HAUSMAN K, et al., 2019. Quantile QT-Opt for risk-aware vision-based robotic

grasping[J]. arXiv preprint arXiv:1910.02787.

BUBECK S, CESA-BIANCHI N, et al., 2012. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems[J]. Foundations and Trends® in Machine Learning, 5(1): 1-122.

DEGRIS T, WHITE M, SUTTON R S, 2012. Linear off-policy actor-critic[C]//In International Conference

on Machine Learning. Citeseer.

DUAN Y, CHEN X, HOUTHOOFT R, et al., 2016. Benchmarking deep reinforcement learning for

continuous control[C]//International Conference on Machine Learning. 1329-1338.

FU J, SINGH A, GHOSH D, et al., 2018. Variational inverse control with events: A general framework

for data-driven reward definition[C]//Advances in Neural Information Processing Systems. 8538-8547.

107

第 2 章 强化学习入门

HANSEN N, OSTERMEIER A, 1996. Adapting arbitrary normal mutation distributions in evolution

strategies: The covariance matrix adaptation[C]//Proceedings of IEEE international conference on

evolutionary computation. IEEE: 312-317.

HEESS N, WAYNE G, SILVER D, et al., 2015. Learning continuous control policies by stochastic value

gradients[C]//Advances in Neural Information Processing Systems. 2944-2952.

KALASHNIKOV D, IRPAN A, PASTOR P, et al., 2018. Qt-opt: Scalable deep reinforcement learning

for vision-based robotic manipulation[J]. arXiv preprint arXiv:1806.10293.

KINGMA D P, WELLING M, 2014. Auto-encoding variational bayes[C]//Proceedings of the International

Conference on Learning Representations (ICLR).

LESHNO M, LIN V Y, PINKUS A, et al., 1993. Multilayer feedforward networks with a nonpolynomial

activation function can approximate any function[J]. Neural networks, 6(6): 861-867.

LEVINE S, 2018. Reinforcement learning and control as probabilistic inference: Tutorial and review[J].

arXiv preprint arXiv:1805.00909.

NELDER J A, MEAD R, 1965. A simplex method for function minimization[J]. The computer journal,

7(4): 308-313.

PETERS J, SCHAAL S, 2008. Natural actor-critic[J]. Neurocomputing, 71(7-9): 1180-1190.

PYEATT L D, HOWE A E, et al., 2001. Decision tree function approximation in reinforcement learning[C]//Proceedings of the third international symposium on adaptive systems: evolutionary computation and probabilistic graphical models: volume 2. Cuba: 70-77.

SCHMIDHUBER J, 2015. Deep learning in neural networks: An overview[J]. Neural networks, 61:

85-117.

SILVER D, LEVER G, HEESS N, et al., 2014. Deterministic policy gradient algorithms[C].

SINGH S, JAAKKOLA T, LITTMAN M L, et al., 2000. Convergence results for single-step on-policy

reinforcement-learning algorithms[J]. Machine learning, 38(3): 287-308.

SUTTON R S, MCALLESTER D A, SINGH S P, et al., 2000. Policy gradient methods for reinforcement

learning with function approximation[C]//Advances in Neural Information Processing Systems. 1057-

1063.

SZEPESVÁRI C, 1998. The asymptotic convergence-rate of q-learning[C]//Advances in Neural Information Processing Systems. 1064-1070.

108

参考文献

SZITA I, LÖRINCZ A, 2006. Learning tetris using the noisy cross-entropy method[J]. Neural computation,

18(12): 2936-2941.

TSITSIKLIS J N, ROY B V, 1997. An analysis of temporal-difference learning with function approximation[R]. IEEE Transactions on Automatic Control.

VAN HASSELT H, GUEZ A, SILVER D, 2016. Deep reinforcement learning with double Q-learning[C]//

Thirtieth AAAI conference on artificial intelligence.

VAN HASSELT H, DORON Y, STRUB F, et al., 2018. Deep reinforcement learning and the deadly

triad[J]. arXiv preprint arXiv:1812.02648.

WATKINS C J, DAYAN P, 1992. Q-learning[J]. Machine learning, 8(3-4): 279-292.

WILLIAMS R J, BAIRD III L C, 1993. Analysis of some incremental variants of policy iteration: First

steps toward understanding actor-critic learning systems[R]. Tech. rep. NU-CCS-93-11, Northeastern

University, College of Computer Science.

109

3 强化学习算法分类

本章将介绍强化学习算法的常见分类方式和具体类别。图 3.1 总结了一些经典的强化学习

算法，并从多个角度对强化学习算法进行分类，其中包括基于模型（Model-Based）和无模型的

（Model-Free）学习方法，基于价值（Value-Based）和基于策略的（Policy-Based）学习方法（或两

者相结合的 Actor-Critic 学习方法），蒙特卡罗（Monte Carlo）和时间差分（Temporal-Difference）

图 3.1 强化学习算法分类图。加粗方框代表不同分类，其他方框代表具体算法

110

3.1 基于模型的方法和无模型的方法

学习方法，在线策略（On-Policy）和离线策略（Off-Policy）学习方法。大多数强化学习算法都可

以根据以上类别进行划分，希望在介绍具体的强化学习算法之前，这些分类能帮助读者建立强化

学习知识体系框架。其中，第 4、5 和 6 章分别具体介绍了基于价值的方法、基于策略的方法，以

及两者的结合。

3.1 基于模型的方法和无模型的方法

我们首先讨论基于模型的方法和无模型的方法，如图 3.2 所示。什么是“模型”？在深度学习

中，模型是指具有初始参数（预训练模型）或已习得参数（训练完毕的模型）的特定函数，例如

全连接网络、卷积网络等。而在强化学习算法中，“模型”特指环境，即环境的动力学模型。回

想一下，在马尔可夫决策过程（MDP）中，有五个关键元素：S, A, P, R, γ。S 和 A 表示环境的

状态空间和动作空间；P 表示状态转移函数，p(s

′

|s, a) 给出了智能体在状态 s 下执行动作 a，并

转移到状态 s

′ 的概率；R 代表奖励函数，r(s, a) 给出了智能体在状态 s 执行动作 a 时环境返回的

奖励值；γ 表示奖励的折扣因子，用来给不同时刻的奖励赋予权重。如果所有这些环境相关的元

素都是已知的，那么模型就是已知的。此时可以在环境模型上进行计算，而无须再与真实环境进

行交互，例如第 2 章中介绍的值迭代、策略迭代等规划（Planning）方法。在通常情况下，智能

体并不知道环境的奖励函数 R 和状态转移函数 p(s

′

|s, a)，所以需要通过和环境交互，不断试错

（Trials and Errors），观察环境相关信息并利用反馈的奖励信号来不断学习。这个不断学习的过程

既对基于模型的方法适用，也对无模型的方法适用。

图 3.2 基于模型的方法和无模型的方法

在这个不断试错和学习的过程中，可能有某些环境元素是未知的，如奖励函数 R 和状态转移

函数 P。此时，如果智能体尝试通过在环境中不断执行动作获取样本 (s, a, s′

, r) 来构建对 R 和 P

的估计，则 p(s

′

|s, a) 和 r 的值可以通过监督学习进行拟合。习得奖励函数 R 和状态转移函数 P

111

第 3 章 强化学习算法分类

之后，所有的环境元素都已知，则前文所述的规划方法可以直接用来求解该问题。这种方式即称

为基于模型的方法。另一种称为无模型的方法则不尝试对环境建模，而是直接寻找最优策略。例

如，Q-learning 算法对状态-动作对 (s, a) 的 Q 值进行估计，通常选择最大 Q 值对应的动作执行，

并利用环境反馈更新 Q 值函数，随着 Q 值收敛，策略随之逐渐收敛达到最优；策略梯度（Policy

Gradient）算法不对值函数进行估计，而是将策略参数化，直接在策略空间中搜索最优策略，最

大化累积奖励。这两种算法都不关注环境模型，而是直接搜索能最大化奖励的策略。这种不需要

对环境建模的方式称为无模型的方法。可以看到，基于模型和无模型的区别在于，智能体是否利

用环境模型（或称为环境的动力学模型），例如状态转移函数和奖励函数。

通过上述介绍可知，基于模型的方法可以分为两类：一类是给定（环境）模型（Given the

Model）的方法，另一类是学习（环境）模型（Learn the Model）的方法。对于给定模型的方法，

智能体可以直接利用环境模型的奖励函数和状态转移函数。例如，在 AlphaGo 算法 (Silver et al.,

2016) 中，围棋规则固定且容易用计算机语言进行描述，因此智能体可以直接利用已知的状态转

移函数和奖励函数进行策略的评估和提升。而对于另一类学习模型的方法，由于环境的复杂性或

不可知性，我们很难描述整个动力系统的规律。此时智能体无法直接获取模型，可行的替代方式

是先通过与环境交互学习环境模型，然后将模型应用到策略评估和提升的过程中。

第二类的典型例子包括 World Models 算法 (Ha et al., 2018)、I2A 算法 (Racanière et al., 2017)

等。例如在 World Models 算法中，智能体首先使用随机策略与环境交互收集数据 (St, At, St+1)，

再使用变分自编码器（Variational Autoencoder，VAE）(Baldi, 2012) 将状态编码为低维潜向量 zt。

然后利用数据 (Zt, At, Zt+1) 学习潜向量 z 的预测模型。有了预测模型之后，智能体便可以通过

习得的预测模型提升策略能力。

基于模型的方法的主要优点是，通过环境模型可以预测未来的状态和奖励，从而帮助智能体

进行更好的规划。一些典型的方法包括朴素规划方法、专家迭代 (Sutton et al., 2018) 方法等。例

如，MBMF 算法 (Nagabandi et al., 2018) 采用了朴素规划的算法；AlphaGo 算法 (Silver et al., 2016)

采用了专家迭代的算法。基于模型的方法的缺点在于，存在或构建模型的假设过强。现实问题中

环境的动力学模型可能很复杂，甚至无法显式地表示出来，导致模型通常无法获取。另一方面，

在实际应用中，学习得到的模型往往是不准确的，这给智能体训练引入了估计误差，基于带误差

模型的策略的评估和提升往往会造成策略在真实环境中失效。

相较之下，无模型的方法不需要构建环境模型。智能体直接与环境交互，并基于探索得到的

样本提升其策略性能。与基于模型的方法相比，无模型的方法由于不关心环境模型，无须学习环

境模型，也就不存在环境拟合不准确的问题，相对更易于实现和训练。然而，无模型的方法也有

其自身的问题。最常见的问题是，有时在真实环境中进行探索的代价是极高的，如巨大的时间消

耗、不可逆的设备损耗及安全风险，等等。比如在自动驾驶中，我们不能在没有任何防护措施的

情况下，让智能体用无模型的方法在现实世界中探索，因为任何交通事故的代价都将是难以承

受的。

第 4、5 和 6 章中介绍的算法都是无模型算法，包括深度 Q 网络（Deep Q-Network，DQN）

112

3.2 基于价值的方法和基于策略的方法

算法 (Mnih et al., 2015)、策略梯度（Policy Gradient）方法 (Sutton et al., 2000)、深度确定性策略

梯度（Deep Deterministic Policy Gradient，DDPG）算法 (Lillicrap et al., 2015) 等。虽然无模型方

法仍然是现在的主流方法，但由于其采样效率（Sample Efficiency）低的缺点很难克服，天然具

有高采样效率的基于模型的方法发挥着越来越重要的作用（详见第 7 章）。例如，第 15 章中介绍

的 AlphaGo (Silver et al., 2016)、AlphaZero (Silver et al., 2017, 2018) 算法，以及最新的 MuZero 算

法 (Schrittwieser et al., 2019) 都属于基于模型的方法。

3.2 基于价值的方法和基于策略的方法

回忆第 2 章，深度强化学习中的策略优化主要有两类：基于价值的方法和基于策略的方法。

两者的结合产生了 Actor-Critic 类算法和 QT-Opt (Kalashnikov et al., 2018) 等其他算法，它们利用

价值函数的估计来帮助更新策略。其分类关系如图 3.3 所示。基于价值的方法通常意味着对动作

价值函数 Qπ

(s, a) 的优化。优化后的最优值函数表示为 Qπ

∗

(s, a) = maxa Qπ

(s, a)，最优策略通

过选取最大值函数对应的动作得到 π

∗ ≈ arg maxπ Qπ（“≈”由函数近似误差导致）。

图 3.3 基于价值的方法和基于策略的方法。图片参考文献 (Li, 2017)

基于价值的方法的优点在于采样效率相对较高，值函数估计方差小，不易陷入局部最优；缺

点是它通常不能处理连续动作空间问题，且最终的策略通常为确定性策略而不是概率分布的形

式。此外，深度 Q 网络等算法中的 ϵ-贪心策略（ϵ-greedy）和 max 算子容易导致过估计的问题。

常见的基于价值的算法包括 Q-learning (Watkins et al., 1992)、深度 Q 网络（Deep Q-Network，

DQN）(Mnih et al., 2015) 及其变体：（1）优先经验回放（Prioritized Experience Replay，PER）(Schaul

et al., 2015) 基于 TD 误差对数据进行加权采样，以提高学习效率；（2）Dueling DQN (Wang et al.,

2016) 改进了网络结构，将动作价值函数 Q 分解为状态值函数 V 和优势函数 A 以提高函数近似

能力；（3）Double DQN (Van Hasselt et al., 2016) 使用不同的网络参数对动作进行选择和评估，以

113

第 3 章 强化学习算法分类

解决过估计的问题；（4）Retrace (Munos et al., 2016) 修正了 Q 值的计算方法，减少了估计的方

差；（5）Noisy DQN (Fortunato et al., 2017) 给网络参数添加噪声，增加了智能体的探索能力；（6）

Distributed DQN (Bellemare et al., 2017) 将状态-动作值估计细化为对状态-动作值分布的估计。

基于策略的方法直接对策略进行优化，通过对策略迭代更新，实现累积奖励最大化。与基于

价值的方法相比，基于策略的方法具有策略参数化简单、收敛速度快的优点，且适用于连续或高

维的动作空间。一些常见的基于策略的算法包括策略梯度算法（Policy Gradient，PG）(Sutton et al.,

2000)、信赖域策略优化算法（Trust Region Policy Optimization，TRPO）(Schulman et al., 2015)、近

端策略优化算法（Proximal Policy Optimization，PPO）(Heess et al., 2017; Schulman et al., 2017) 等，

信赖域策略优化算法和近端策略优化算法在策略梯度算法的基础上限制了更新步长，以防止策略

崩溃（Collapse），使算法更加稳定。

除了基于价值的方法和基于策略的方法，更流行的是两者的结合，这衍生出了 Actor-Critic 方

法。Actor-Critic 方法结合了两种方法的优点，利用基于价值的方法学习 Q 值函数或状态价值函数

V 来提高采样效率（Critic），并利用基于策略的方法学习策略函数（Actor），从而适用于连续或

高维的动作空间。Actor-Critic 方法可以看作是基于价值的方法在连续动作空间中的扩展，也可以

看作是基于策略的方法在减少样本方差和提升采样效率方面的改进。虽然 Actor-Critic 方法吸收

了上述两种方法的优点，但同时也继承了相应的缺点。比如，Critic 存在过估计的问题，Actor 存

在探索不足的问题等。一些常见的 Actor-Critic 类的算法包括 Actor-Critic（AC）算法 (Sutton et al.,

2018) 和一系列改进：（1）异步优势 Actor-Critic 算法（A3C）(Mnih et al., 2016) 将 Actor-Critic 方

法扩展到异步并行学习，打乱数据之间的相关性，提高了样本收集速度和训练效率；（2）深度确

定性策略梯度算法（Deep Deterministic Policy Gradient，DDPG）(Lillicrap et al., 2015) 沿用了深度

Q 网络算法的目标网络，同时 Actor 是一个确定性策略；（3）孪生延迟 DDPG 算法（Twin Delayed

Deep Deterministic Policy Gradient，TD3）(Fujimoto et al., 2018) 引入了截断的（Clipped）Double

Q-Learning 解决过估计问题，同时延迟 Actor 更新频率以优先提高 Critic 拟合准确度；（4）柔性

Actor-Critic 算法（Soft Actor-Critic，SAC）(Haarnoja et al., 2018) 在 Q 值函数估计中引入熵正则

化，以提高智能体探索能力。

3.3 蒙特卡罗方法和时间差分方法

蒙特卡罗（Monte Carlo，MC）方法和时间差分（Temporal Difference，TD）方法的区别已经

在第 2 章中讨论过，一些算法如图 3.4 所示。这里我们再次总结它们的特点以保证本章的完整性。

时间差分方法是动态规划（Dynamic Programming，DP）方法和蒙特卡罗方法的一种中间形式。首

先，时间差分方法和动态规划方法都使用自举法（Bootstrapping）进行估计，其次，时间差分方法

和蒙特卡罗方法都不需要获取环境模型。这两种方法最大的不同之处在于如何进行参数更新，蒙

特卡罗方法必须等到一条轨迹生成（真实值）后才能更新，而时间差分方法在每一步动作执行都

可以通过自举法（估计值）及时更新。这种差异将使时间差分方法方法具有更大的偏差，而使蒙

114

3.4 在线策略方法和离线策略方法

特卡罗方法方法具有更大的方差。

图 3.4 蒙特卡罗方法和时间差分方法

3.4 在线策略方法和离线策略方法

在线策略（On-Policy）方法和离线策略（Off-Policy）方法依据策略学习的方式对强化学习算

法进行划分（图 3.5）。在线策略方法试图评估并提升和环境交互生成数据的策略，而离线策略方

法评估和提升的策略与生成数据的策略是不同的。这表明在线策略方法要求智能体与环境交互的

策略和要提升的策略必须是相同的。而离线策略方法不需要遵循这个约束，它可以利用其他智能

体与环境交互得到的数据来提升自己的策略。常见的在线策略方法是 Sarsa，它根据当前策略选

择一个动作并执行，然后使用环境反馈的数据更新当前策略。因此，Sarsa 与环境交互的策略和

更新的策略是同一个策略。它的 Q 函数更新公式如下：

Q(St, At) ← Q(St, At) + α[Rt + γQ(St+1, At+1) − Q(St, At)]. (3.1)

图 3.5 在线策略方法和离线策略方法

115

第 3 章 强化学习算法分类

Q-learning 是一种典型的离线策略方法。它在选择动作时采用 max 操作和 ϵ-贪心策略，使得

与环境交互的策略和更新的策略不是同一个策略。它的 Q 函数更新公式如下：

Q(St, At) ← Q(St, At) + α[Rt + γ max

a

Q(St+1, At+1) − Q(St, At)]. (3.2)

参考文献

BALDI P, 2012. Autoencoders, Unsupervised Learning, and Deep Architectures[C]//Proceedings oast the

International Conference on Machine Learning (ICML). 37-50.

BELLEMARE M G, DABNEY W, MUNOS R, 2017. A distributional perspective on reinforcement

learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.

org: 449-458.

FORTUNATO M, AZAR M G, PIOT B, et al., 2017. Noisy networks for exploration[J]. arXiv preprint

arXiv:1706.10295.

FUJIMOTO S, VAN HOOF H, MEGER D, 2018. Addressing function approximation error in actor-critic

methods[J]. arXiv preprint arXiv:1802.09477.

HA D, SCHMIDHUBER J, 2018. Recurrent world models facilitate policy evolution[C]//Advances in

Neural Information Processing Systems. 2450-2462.

HAARNOJA T, ZHOU A, ABBEEL P, et al., 2018. Soft actor-critic: Off-policy maximum entropy deep

reinforcement learning with a stochastic actor[J]. arXiv preprint arXiv:1801.01290.

HEESS N, SRIRAM S, LEMMON J, et al., 2017. Emergence of locomotion behaviours in rich environments[J]. arXiv:1707.02286.

KALASHNIKOV D, IRPAN A, PASTOR P, et al., 2018. Qt-opt: Scalable deep reinforcement learning

for vision-based robotic manipulation[J]. arXiv preprint arXiv:1806.10293.

LI Y, 2017. Deep reinforcement learning: An overview[J]. arXiv preprint arXiv:1701.07274.

LILLICRAP T P, HUNT J J, PRITZEL A, et al., 2015. Continuous control with deep reinforcement

learning[J]. arXiv preprint arXiv:1509.02971.

MNIH V, KAVUKCUOGLU K, SILVER D, et al., 2015. Human-level control through deep reinforcement

learning[J]. Nature.

116

参考文献

MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learning[C]//International Conference on Machine Learning (ICML). 1928-1937.

MUNOS R, STEPLETON T, HARUTYUNYAN A, et al., 2016. Safe and efficient off-policy reinforcement

learning[C]//Advances in Neural Information Processing Systems. 1054-1062.

NAGABANDI A, KAHN G, FEARING R S, et al., 2018. Neural network dynamics for model-based

deep reinforcement learning with model-free fine-tuning[C]//2018 IEEE International Conference on

Robotics and Automation (ICRA). IEEE: 7559-7566.

RACANIÈRE S, WEBER T, REICHERT D, et al., 2017. Imagination-augmented agents for deep reinforcement learning[C]//Advances in Neural Information Processing Systems. 5690-5701.

SCHAUL T, QUAN J, ANTONOGLOU I, et al., 2015. Prioritized experience replay[C]//arXiv preprint

arXiv:1511.05952.

SCHRITTWIESER J, ANTONOGLOU I, HUBERT T, et al., 2019. Mastering atari, go, chess and shogi

by planning with a learned model[Z].

SCHULMAN J, LEVINE S, ABBEEL P, et al., 2015. Trust region policy optimization[C]//International

Conference on Machine Learning (ICML). 1889-1897.

SCHULMAN J, WOLSKI F, DHARIWAL P, et al., 2017. Proximal policy optimization algorithms[J].

arXiv:1707.06347.

SILVER D, HUANG A, MADDISON C J, et al., 2016. Mastering the game of go with deep neural

networks and tree search[J]. Nature.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2017. Mastering chess and shogi by self-play with

a general reinforcement learning algorithm[J]. arXiv preprint arXiv:1712.01815.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2018. A general reinforcement learning algorithm

that masters chess, shogi, and Go through self-play[J]. Science, 362(6419): 1140-1144.

SUTTON R S, BARTO A G, 2018. Reinforcement learning: An introduction[M]. MIT press.

SUTTON R S, MCALLESTER D A, SINGH S P, et al., 2000. Policy gradient methods for reinforcement

learning with function approximation[C]//Advances in Neural Information Processing Systems. 1057-

1063.

VAN HASSELT H, GUEZ A, SILVER D, 2016. Deep reinforcement learning with double Q-learning[C]//

Thirtieth AAAI conference on artificial intelligence.

117

第 3 章 强化学习算法分类

WANG Z, SCHAUL T, HESSEL M, et al., 2016. Dueling network architectures for deep reinforcement

learning[C]//International Conference on Machine Learning. 1995-2003.

WATKINS C J, DAYAN P, 1992. Q-learning[J]. Machine learning, 8(3-4): 279-292.

118

4 深度 Q 网络

本章将介绍的 DQN 算法全称为深度 Q 网络算法，是深度强化学习算法中最重要的算法之

一。我们将从基于时间差分学习的 Q-Learning 算法入手，介绍 DQN 算法及其变体。在本章的最

后，我们提供了代码示例，并对 DQN 及其变体进行实验比较。

强化学习最重要的突破之一是 Q-Learning 算法。它是一种离线策略（Off-Policy）的时间差分

（Temporal Difference）算法，此前在第 2 章中有介绍。在使用表格（Tabular）的情况下或使用线

性函数逼近 Q 函数时，Q-Learning 已被证明可以收敛于最优解。然而，当使用非线性函数逼近器

（如神经网络）来表示 Q 函数时，Q-Learning 并不稳定，甚至是发散的 (Tsitsiklis et al., 1996)。随

着深度神经网络技术的不断发展，深度 Q 网络（Deep Q-Networks，DQN）算法 (Mnih et al., 2015)

解决了这一问题，并点燃了深度强化学习的研究。在本章中，我们将先回顾 Q-Learning 的背景。

之后介绍 DQN 算法及其变体，并给出详细的理论和解释。最后，在 4.8 节，我们将通过代码展示

算法在雅达利游戏上的实现细节与实战表现，为读者提供快速上手的实战学习过程。每种算法的

完整代码可以在随书提供的代码仓库中找到1。

无模型（Model-Free）方法为解决基于 MDP 的决策问题提供了一种通用的方法。其中“模

型”是指显式地对 MDP 相关的转移概率分布和回报函数建模，而时间差分（Temporal Difference，

TD）学习就是一类无模型方法。在 2.4 节中，我们讨论过，当拥有一个完美的 MDP 模型时，通

过递归子问题的最优解，就可以得到动态规划的最优方案。TD 学习也遵循了这样一种思想，即

使对子问题的估计并非一直是最优的，我们也可以通过自举（Bootstrapping）来估计子问题的值。

1代码链接见读者服务

119

第 4 章 深度 Q 网络

子问题通过 MDP 中的状态表示。在策略 π 下，状态为 s 时的 value 值（V 值）vπ(s) 被定义

为从状态 s 开始，以策略 π 进行动作的预期回报：

vπ(s) = Eπ[Rt + γvπ(St+1)|St = s], (4.1)

此处的 γ ∈ [0, 1] 是衰减率。TD 学习用自举法分解上述估计。给定价值函数 V : S → R，

TD(0) 是一个最简单的版本，它只应用一步自举，如下所示：

V (St) ← V (St) + α[Rt + γV (St+1) − V (St)] (4.2)

此处的 Rt + γV (St+1) 和 Rt + γV (St+1) − V (St) 分别被称为 TD 目标和 TD 误差。

策略的评估值提供了一种对策略的动作质量（Quality）进行评估的方法。为了进一步了解如

何选择某一特定状态下的动作，我们将通过 Q 值来评估状态-动作组合的效果。Q 值可以这样被

估计：

qπ(s, a) = Eπ[Rt+1 + γvπ(St+1)|St = s, At = a] (4.3)

有了 Q 值对策略进行评估之后，我们只需要找到一种能提升 Q 值的方法就能提升策略的

效果。最简单的提升效果的方法就是通过贪心的方法执行动作：π

′

(s) = arg maxa′ q

π

(s, a′

)。由

qπ′ (s, a) = maxa′ qπ(s, a′

) ⩾ qπ(s, a) 我们可以知道，贪心的策略一定不会得到一个更差的解法。

考虑到探索的必要性，我们可以用一种替代方案来提升策略的效果。在该方案中，多数情况下我

们仍然选择贪心动作，但是同时会以一个小概率 ϵ，从所有动作中以相同概率随机选择一个动作。

该方法被称为 ϵ-贪心。我们可以这样计算 ϵ-贪心策略中 π

′ 的 Q 值：

qπ(s, π′

(s)) = (1 − ϵ) max

a∈A

qπ(s, a) + ϵ

|A|

X

a∈A

qπ(s, a). (4.4)

值得注意的是，π(s,a)−ϵ/|A|

1−ϵ 在 a ∈ A 上的和为 1。由于最大值不小于加权平均值，所以可以

得到：

qπ(s, π′

(s)) = (1 − ϵ) max

a∈A

qπ(s, a)

X

a∈A

π(s, a) − ϵ/|A|

1 − ϵ

+

ϵ

|A|

X

a∈A

qπ(s, a)

⩾ (1 − ϵ)

X

a∈A

π(s, a) − ϵ/|A|

1 − ϵ

qπ(s, a) + ϵ

|A|

X

a∈A

qπ(s, a) = qπ(s, π(s)), (4.5)

由此得知，通过 ϵ-贪心策略 π

′ 进行动作产生的 Q 值并不会小于原始的策略 π。也就是说，

ϵ-贪心方法能确保策略的优化。接下来，我们将在下一节中讨论如何使用 Q 函数进行策略优化。

120

4.1 Sarsa 和 Q-Learning

4.1 Sarsa 和 Q-Learning

更新 Q 函数的方式也和 TD(0) 中更新 V 函数的方式相似，直接在每次发生非终结（NonTerminal）状态下的状态转移之后，用此时的状态 St 对 Q 函数进行更新即可。

Q(St, At) ← Q(St, At) + α[Rt + γQ(St+1, At+1) − Q(St, At)] (4.6)

此处的 At 和 At+1 动作都是通过基于 Q 值的 ϵ-贪心方法来选择的。如果 St+1 是一个终结状

态（Terminal State），则 Q(St+1, At+1) 将被设置为 0。我们能不断地估计行为策略 π 产生的 Q，同

时让 π 趋近于基于 Q 的贪心策略。此算法就是 Sarsa 算法。值得注意的是，策略 π 在 Sarsa 中有

两个职责：产生经验和提升策略。通常来说，用来产生行为的策略被称为行为策略，而用来评估

和提升的策略被称为目标策略。当算法中的行为策略和目标策略是同一个策略时（例如 Sarsa），

该算法就是一种在线策略（On-Policy）方法。

在线策略方法本质上是一种试错的过程，当前策略产生的经验仅会被直接用于进行策略提

升。离线策略方法考虑一种反思的策略，使得反复使用过去的经验成为了可能。Q-Learning 就是

一种离线策略方法。其最简单的形式，即单步（One-Step）Q-Learning 遵循如下更新规则：

Q(St, At) ← Q(St, At) + α[Rt + γ max

At+1

Q(St+1, At+1) − Q(St, At)] (4.7)

此处的 At 是通过基于 Q 的 ϵ-贪心方法采样得到的。注意 At+1 是通过贪心方式选择的，此处

与 Sarsa 不同。也就是说，Q-Learning 中的行为策略也是 ϵ-贪心，但是目标策略是贪心（Greedy）

策略。单步 Q-Learning 只考虑当前的状态转移，而我们可以选择多步（Multi-Steps）Q-Learning

方法，在近似情况下，通过使用多步奖励（Multi-Steps Rewards）来获得更加精准的 Q 值。要注

意，多步 Q-Learning 中需要考虑后续奖励的不匹配问题，以保持 Q 函数对目标策略预期回报（参

考公式 (4.3)）的近似。我们将在第 4.7 节中继续对多步 Q-Learning 展开讨论。

4.2 为什么使用深度学习: 价值函数逼近

在使用表格方式表示 Q 函数的时候，Q 函数可以表示为一个大型二维表格。也就是说，每个

离散的状态和动作都有一个单独的条目。然而该方法在处理具有大规模数据空间（如原始像素输

入）的任务时将十分低效，更不用说具有连续数据的控制任务了。幸运的是，通过使用函数逼近

从不同输入进行泛化的技术已经得到了广泛的研究，我们可以将其应用于基于价值（Value-based）

的强化学习。

接下来，我们考虑 Q-Learning 中使用参数 θ 进行函数拟合。函数拟合器可以是线性模型、决

121

第 4 章 深度 Q 网络

策树或者神经网络。之后，我们通过 (4.7) 式子进行更新，它可以被重写为

θt ← arg min

θ

L(Q(St, At; θ), Rt + γQ(St+1, At+1; θ)) (4.8)

此处的 L 代表损失函数，如均方误差（Mean Squared Error）。对于上述的优化问题，可以通过批

量采样构造出拟合 Q 迭代（Fitted Q Iteration）(Riedmiller, 2005)，其过程如算法 4.15 所示，其中

S

′

i 是 Si 的后继状态。该算法的一个在线随机的变种就是如算法 4.16 所示的在线 Q 迭代（Online

Q Iteration）算法。

算法 4.15 拟合 Q 迭代

for 迭代数 i = 1, T do

收集 D 份采样 {(Si

, Ai

, Ri

, S′

i

)}

D

i=1

for t = 1, K do

设置 Yi ← Ri + γ maxa Q(S

′

i

, a; θ)

设置 θ ← arg minθ

′

1

2

PD

i=1(Q(Si

, Ai

; θ

′

) − Yi)

2

end for

end for

算法 4.16 在线 Q 迭代

for 迭代数 = 1, T do

选择动作 a 与环境交互，并得到观察数据 (s, a, r, s′

)

设置 y ← r + γ maxa′ Q(s

′

, a′

; θ)

设置 θ ← θ − α(Q(s, a; θ) − y)

dQ(s,a;θ)

dθ

end for

值得注意的是，拟合 Q 迭代和在线 Q 迭代都是离线策略算法，因此，它们可以多次重用过

去的经验。我们将在下一节对此进行深入讨论。

在 2.4.2 节中，我们通过贝尔曼最优回溯算子 T

∗ 介绍了值迭代的收敛性。我们定义一个新

的运算符 B，其函数近似为 BV = arg minV ′∈Ω L(V

′

, V )，其中 Ω 是所有可近似的值函数的集合。

值得注意的是，B 中的 arg min 可以看作是 T

∗V 到 Ω 的映射。所以函数近似的回溯算子可以表

示为 BT ∗。而 T

∗ 在无穷范式（∞-norm）下收敛，B 则是在 L2 范式下的 MSE 损失下收敛。然

而 BT ∗ 不以任何形式收敛。因此，当用神经网络等非线性函数逼近器来表示数值函数时，数值

迭代是不稳定的，甚至可能发散 (Tsitsiklis et al., 1997)。我们将在下一节讨论深度神经网络训练的

稳定性。

122

4.3 DQN

4.3 DQN

在上一节中，我们介绍了近似学习状态-动作值函数的方法及其收敛不稳定性。为了在使用

原始像素输入的复杂问题中实现端到端决策，DQN 通过两个关键技术结合 Q-Learning 和深度学

习来解决不稳定性问题，并在雅达利游戏上取得了显著进展。

第一个关键技术被称为回放缓存（Replay Buffer）。这是一种被称为经验重演的生物学启发机

制 (Lin, 1993; McClelland et al., 1995; O’Neill et al., 2010)。在每个时间步 t 中，DQN 先将智能体获得

的经验 (St, At, Rt, St+1) 存入回放缓存中，然后从该缓存中均匀采样小批量样本用于 Q-Learning

更新。回放缓存相较于拟合 Q 迭代有几个优势。首先，它可以重用每个时间步的经验来学习 Q

函数，这样可以提高数据使用效率。其次，如果像拟合 Q 迭代那样没有回放缓存，那么一个批次

中的样本将会是连续采集的，即样本高度相关。这样会增加更新的方差。最后，经验回放防止用

于训练的样本只来自上一个策略，这样能平滑学习过程并减少参数的震荡或发散。在实践中，为

了节省内存，我们往往只将最后 N 个经验存入回放缓存（FIFO 缓存）。

第二个关键技术是目标网络。它作为一个独立的网络，用来代替所需的 Q 网络来生成 QLearning 的目标，进一步提高神经网络的稳定性。此外，目标网络每 C 步将通过直接复制（硬

更新）或者指数衰减平均（软更新）的方式与主 Q 网络同步。目标网络通过使用旧参数生成 QLearning 目标，使目标值的产生不受最新参数的影响，从而大大减少发散和震荡的情况。例如，

在动作 (St, At) 上的更新使得 Q 值增加，此时 St 和 St+1 的相似性可能会导致所有动作 a 的

Q(St+1, a) 值增加，从而使得由 Q 网络产生的训练目标值被过估计。但是如果使用目标网络产生

训练目标，就能避免过估计的问题。

这两项关键技术在 5 个雅达利游戏的效果提升效果如表 4.1 所示。智能体进行了 1e7 次配备

超参数搜索功能的训练。每 250000 次训练，会对各个智能体进行 135000 帧评估，并且记录最高

的片段平均分。

表 4.1 分别使用回放缓存和目标 Q 网络的效果。数据来自文献 (Mnih et al., 2015).

游戏名称 使用回放缓存和

目标 Q 网络

使 用 回 放 缓 存，

且不使用目标 Q

网络

不 使 用 回 放 缓

存，使用目标 Q

网络

不使用回放缓存

和目标 Q 网络

Breakout 316.8 240.7 10.2 3.2

Enduro 1006.3 831.4 141.9 29.1

River Raid 7446.6 4102.8 2867.7 1453.0

Seaquest 2894.4 822.6 1003.0 275.8

Space Invaders 1088.9 826.3 373.2 302.0

由于将任意长度的历史数据作为神经网络的输入较为复杂，DQN 转而处理由函数 ϕ 生成的

123

第 4 章 深度 Q 网络

固定长度表示的历史数据。准确来说，ϕ 集合了当前帧和前三帧的数据，这对于跟踪时间相关信

息（如对象的移动）非常有用。完整的算法展示在算法 4.17 中。其中原始帧被调整为 84 × 84 的

灰度图像。函数 ϕ 堆叠了最近 4 帧的数据作为神经网络的输入。此外，神经网络的结构由三个

卷积层和两个完全连接的层组成，每个有效动作只有一个输出。我们将在 4.8 节讨论更多的训练

细节。

算法 4.17 DQN

超参数: 回放缓存容量 N，奖励折扣因子 γ，用于目标状态-动作值函数更新的延迟步长 C，

ϵ-greedy 中的 ϵ。

输入: 空回放缓存 D，初始化状态-动作值函数 Q 的参数 θ。

使用参数 ˆθ ← θ 初始化目标状态-动作值函数 Qˆ。

for 片段 = 0, 1, 2, · · · do

初始化环境并获取观测数据 O0。

初始化序列 S0 = {O0} 并对序列进行预处理 ϕ0 = ϕ(S0)。

for t = 0, 1, 2, · · · do

通过概率 ϵ 选择一个随机动作 At，否则选择动作 At = arg maxa Q(ϕ(St), a; θ)。

执行动作 At 并获得观测数据 Ot+1 和奖励数据 Rt。

如果本局结束，则设置 Dt = 1，否则 Dt = 0。

设置 St+1 = {St, At, Ot+1} 并进行预处理 ϕt+1 = ϕ(St+1)。

存储状态转移数据 (ϕt, At, Rt, Dt, ϕt+1) 到 D 中。

从 D 中随机采样小批量状态转移数据 (ϕi

, Ai

, Ri

, Di

, ϕ′

i

)。

若 Di = 0，则设置 Yi = Ri + γ maxa′ Qˆ(ϕ

′

i

, a′

;

ˆθ)，否则，设置 Yi = Ri。

在 (Yi − Q(ϕi

, Ai

; θ))2 上对 θ 执行梯度下降步骤。

每 C 步对目标网络 Qˆ 进行同步。

如果片段结束，则跳出循环。

end for

end for

4.4 Double DQN

Double DQN 是对 DQN 在减少过拟合方面的改进 (Van Hasselt et al., 2016)。在进一步讨

论算法之前，我们先在经典的 DQN 算法上说明一下过拟合问题。我们注意到 Q-Learning 目标

Rt + γ maxa Q(St+1, a) 包含一个最大化算子 max 的操作。而 Q 又由于环境、非稳态、函数近似

或者其他原因，可能带有噪声。需注意的是，最大噪声的期望值并不会小于噪声的最大期望，即

E[max(ϵ1, · · · , ϵn)] ⩾ (max(E[ϵ1], · · · , E[ϵn]))。因此，下一个 Q 值往往被过估计了。文献 (Thrun

et al., 1993) 对此提供了进一步的理论分析和实验结果。

通过增加对网络参数 θ 的关注，标准 DQN 的学习目标可以被重写为如下式子：

Rt + γQˆ(St+1, arg max

a

Qˆ(St+1, a;

ˆθ); ˆθ), (4.9)

124

4.5 Dueling DQN

在式子中可以注意到一个问题：ˆθ 既用于估计 Q 值，又用于对估计过程中的下一个动作 a 进

行选择。而 Double DQN 的核心思想是在这两个阶段使用两个不同的网络，以去除选择和评价中

噪声的相关性。因此，需要一个额外的网络完成这项工作，而 DQN 结构中的 Q 网络则是一个很

自然能想到的选择。（回顾一下 DQN 结构中有 Q 网络和目标网络这两个网络，并通过目标网络

进行评估来进一步提高稳定性。）因此，Double DQN 中使用的 Q 学习目标是

Rt + γQˆ(St+1, arg max

a

Q(St+1, a; θ); ˆθ). (4.10)

在 Wang 等人 (Wang et al., 2016) 的工作之上，我们通过如下公式计算智能体分数相对人类和

基准智能体分数的提升百分比（有正有负）：

ScoreAgent − ScoreBaseline

max(ScoreBaseline, ScoreHuman) − ScoreRandom

(4.11)

Double DQN 相比于 DQN 的效果提升情况如图 4.1 所示。

Demon Attack

Time Pilot

Asterix

Up and Down

Wizard Of Wor

Tutankham

James Bond

Kangaroo

Bank Heist

Gopher

Video Pinball

Defender

Enduro

Space Invaders

Zaxxon

Phoenix

Amidar

River Raid

Name This Game

Skiing

Beam Rider

Alien

Assault

Berzerk

Seaquest

Fishing Derby

Frostbite

Surround

Q*Bert

Kung-Fu Master

Bowling

Road Runner

Star Gunner

Double Dunk

Breakout

Freeway

Centipede

Crazy Climber

Battle Zone

Boxing

Pitfall!

Pong

Robotank

Montezuma’s Revenge

Private Eye

H.E.R.O.

Asteroids

Gravitar

Solaris

Chopper Command

Venture

Ms. Pac-Man

Ice Hockey

Krull

Yars’ Revenge

Atlantis

Tennis

0%

200%

400%

Normalized Score

图 4.1 Double DQN (Van Hasselt et al., 2016) 相比于 DQN (Mnih et al., 2015) 在雅达利基准上的效果提

升情况。计算标准参考公式 (4.11)。所有数据来自文献 (Wang et al., 2016)

。

4.5 Dueling DQN

对于某些状态来说，不同的动作与预期值无关，因此我们不需要学习各个动作对该状态的影

响。例如，假想我们在山上看日出，美丽的景色令人陶醉，这是一个很高的奖励。此时，你即使

在这里继续做不同的动作也不会对 Q 值产生影响。因此，将动作无关的状态值与 Q 值进行解耦，

可以获得更加鲁棒的学习效果。

125

第 4 章 深度 Q 网络

Dueling DQN 提出了一种新的网络结构来实现这一思想 (Wang et al., 2016)。更准确地说，Q

值可以被分为状态值和动作优势这两部分：

Q

π

(s, a) = V

π

(s) + A

π

(s, a) (4.12)

然后，Dueling DQN 通过如下方法将这两部分的表示分开：

Q(s, a; θ, θv, θa) = V (s; θ, θv) + (A(s, a; θ, θa) − max

a′

A(s, a′

; θ, θa)) (4.13)

其中 θv 和 θa 是两个全连接层的参数，θ 表示卷积层的参数。注意公式 (4.13) 中的 max 函数

确保了 Q 值能唯一地对应状态值和动作优势。否则，训练将忽略状态值项，并只会使优势函数收

敛到 Q 值。此外，文献 (Wang et al., 2016) 还提出使用取平均代替取最大值的方法，以获得更好

的稳定性：

Q(s, a; θ, θv, θa) = V (s; θ, θv) + (A(s, a; θ, θa) −

1

|A|

X

a′

A(s, a′

; θ, θa)) (4.14)

其中，优势函数只需要向平均优势方向靠近，而不必追求最大优势。

训练 Dueling 结构和训练标准 DQN 一样，它只需要更多的网络层。实验表明，Dueling 结构

在许多价值相似的动作中，能获得更好的策略评估效果。Dueling DQN 相比于 DQN 的效果提升

效果如图 4.2 所示。

图 4.2 Dueling DQN (Wang et al., 2016) 相比于 DQN (Mnih et al., 2015) 在雅达利基准上的效果提升，

计算标准参考公式 (4.11)。所有数据来自文献 (Wang et al., 2016)

126

4.6 优先经验回放

4.6 优先经验回放

标准 DQN 中还剩下的一个可改进的地方就是，使用更好经验回放采样策略。优先经验回放

（Prioritized Experience Replay，PER）是一种将经验进行优先排序的技术。通过该技术可以使重要

的状态转移经验被更加频繁地回放 (Schaul et al., 2015)。PER 的核心思想是通过 TD 误差 δ 来考虑

不同状态转移数据的重要性。TD 误差 δ 是一个令人惊喜的衡量标准。该方法之所以能有效，是

由于某些经验数据相较于其他经验数据，可能包含更多值得学习的信息，所以给予这些包含更丰

富信息量的经验更多的回放机会，有助于使得整个学习进度更为快速和高效。

当然，直接使用 TD 误差做优先排序是最直接能想到的方法。然而这种方法有一些问题。首

先，扫描整个回放缓存空间非常低效。其次，这种方法对近似误差和随机回报的噪声十分敏感。

最后，这种贪心的方法会使误差收敛缓慢，可能导致刚开始训练时有着高误差的状态转移被频繁

地回放。为了克服这些问题，文献 (Schaul et al., 2015) 提出了使用如下方法计算状态转移 i 的采

样概率：

P(i) = p

α

P

i

k

p

α

k

(4.15)

其中，pi 指状态转移 i 的优先级，它是一个正数，即 pi > 0。α 是一个指数超参数，α = 0 对

应均匀采样情况，而 k 表示对采样的状态转移进行枚举。pi 有两种变体。第一种是按比例优先：

pi = |δi

| + ϵ。其中 δi 是状态转移 i 的 TD 误差，而 ϵ 是一个用于数值稳定的小正数。第二种变体

是基于顺序的优先：pi =

1

rank(i)。其中 rank(i) 是状态转移 i 基于 |δi

| 的等级评定。

回想起在回放缓存中，正是因为随机采样而有助于消除样本之间的相关性的。然而在使用优

先采样时，又放弃了纯随机采样。因此，减少高优先级状态转移数据的训练权重也有一定的道理。

PER 使用了重要性采样（Importance-Sampling）权重来修正状态转移 i 的偏差。

wi = (NP(i))−β

(4.16)

其中，N 指回放缓存的容量大小，而 P 是按照公式 (4.15) 定义的概率。β 是训练过程中将会

退火（Anneal）2到 1 的超参数，这么设置是由于随着训练增加，更新会趋近于无偏。此权重通常

被折叠进损失函数来构造加权学习。

为了更有效地实现上述方法，我们将使用一个分段线性函数逼近采样概率的累积密度函数，

该函数具有 k 段。更准确地说，优先级存储在一个称为线段树的高效查询数据结构中。在运行

期间，首先对线段范围进行采样，然后在该线段范围内的样本进行均匀采样。对 DQN 的改进如

图 4.3 所示。

2指模拟退火法中的退火，一种简单的实现是线性退火，例如，若设置初始值 0.6，终止值 1.0，最大迭代步数为 100，

则第 0 ⩽ t < 100 步取 β = 0.6 + t(1.0 − 0.6)/99

127

第 4 章 深度 Q 网络

图 4.3 使用基于等级优先排序的优先经验回放算法 (Schaul et al., 2015) 相比于 DQN (Mnih et al.,

2015) 在雅达利基准上的效果提升，计算标准参考公式 (4.11)。所有数据来自文献 (Wang

et al., 2016)

4.7 其他改进内容：多步学习、噪声网络和值分布强化学习

Rainbow 在包含 Double Q-Learning、Dueling 结构和 PER 之外，还包含了另外 3 个 DQN 的

扩展，并在雅达利游戏上取得了显著的成果 (Hessel et al., 2018)。在本节中，我们将对此展开讨

论，并进一步讨论它们的延伸内容。

第一个扩展是多步学习（Multi-Step Learning）。使用 n 步回报将使估计更加准确，也被证明可

以通过适当调整 n 值来加快学习速度 (Sutton et al., 2018)。然而，在离线策略学习过程中，目标策

略和行为策略在多个步骤中的行为选择可能并不匹配。我们可以在文献 (Hernandez-Garcia et al.,

2019) 中找到一个系统性的研究方法来纠正此类错配问题。Rainbow 直接使用了来自给定状态 St

的截断的 n 步回报 R

(k)

t

(Castro et al., 2018; Hessel et al., 2018)，其中 R

(k)

t 由以下公式定义。

R

(k)

t =

nX−1

k=0

γ

kRt+k (4.17)

接着，Q-Learning 多步学习变体的目标通过下式定义。

R

(k)

t + γ

k max

a

Q(St+k, a) (4.18)

第二个扩展是噪声网络 (Fortunato et al., 2017)。它是另一种 ϵ 贪心的探索算法，对于像《蒙

特祖玛的复仇》这样需要大量探索的游戏十分有效。我们使用一个额外的噪声流将噪声加入线性

层 y = (W x + b) 中。

128

4.7 其他改进内容：多步学习、噪声网络和值分布强化学习

y = (W x + b) + ((Wnoisy ⊙ ϵw)x + bnoisy ⊙ ϵb) (4.19)

其中，⊙ 表示元素间的乘积，Wnoisy 和 bnoisy 都是可训练的参数，而 ϵw 和 ϵb 是将退火到 0 的随机

的标量。实验表明，噪声网络相比于许多基线算法，使得众多雅达利游戏的得分有了大幅提升。

最后一个扩展是值分布强化学习 (Bellemare et al., 2017)。该方法为值估计提供了一个新的视

角。文献 (Bellemare et al., 2017) 提出了分布式贝尔曼算子 T

π 用于估计回报 Z 的分布，以改进过

去只考虑 Z 的期望的做法:

T

πZ = R + γP πZ. (4.20)

图 4.4 展示了 T

π 的一种连续分布的情况。

图 4.4 一种分布式贝尔曼算子在连续分布上的情况。它提供了在策略 π 下，下个状态的回报分

布。它将先被折扣因子 γ 折损，然后被当前时间步中的奖励移动

Rainbow 中使用的值分布 DQN 变体被称为离散 DQN (Bellemare et al., 2017)，它通过一个离

散分布来对状态-动作值分布进行建模，该分布由一个有 N 个元素（也被称为原子）的向量 z 参

数化而来。该向量表示为 zi = Vmin + (i − 1)∆z，其中 [Vmin, Vmax] 是状态-动作值分布的范围，

并且 ∆z =

Vmax−Vmin

N−1 。在实践中，N 值通常设置为 51，因此，有时该算法也被称为 C51。C51

的参数模型 θ 输出每个原子概率 pi(s, a) = e

θi(s,a)/

P

j

e

θj (s,a) 组成分布 Zθ。其中值得注意的是，

离散化的近似会导致贝尔曼更新 T

πZ 与参数化的 Zθ 脱节。而 C51 通过将目标分布 T

πZθˆ 投影

到 Zθ 上来解决这个问题。更加准确地说，若给定一个转移数据 (St, At, Rt, St+1)，则使用 Double

Q-Learning 的投影目标 ΦT

πZθˆ(St, At) 的第 i 个分量由以下公式算出：

129

第 4 章 深度 Q 网络

X

N

j=1

pj (St+1, arg max

a

z

⊺

p(St+1, a; θ); ˆθ)[1 −

|[Rt + γzj ]

Vmax

Vmin

− zi

|

∆z

]

1

0

(4.21)

其中，[·]

b

a 将其参数限制在 [a, b] 范围内。由于 TD 误差无法度量值分布之间的差异，因此 C51

提出使用如下的 Kullbeck-Leibler 散度作为训练损失：

DKL(ΦT

πZθˆ(St, At)∥Zθ(St, At)). (4.22)

另外，用于经验回放的优先级也被 KL 散度所代替。对于 Dueling 结构，输出分布也将分为

价值数据流和优势数据流，并且总分布估计如下所示：

pi(s, a) = exp(Vi(s) + Ai(s, a) − A¯

i(s, a))

P

j

exp(Vj (s) + Aj (s, a) − A¯

j (s, a)) (4.23)

其中 A¯

j (s, a) 由 1

|A|

P

a′ Aj (s, a′

) 定义。

通过 C51 实现的值分布强化学习的主要缺点是，它只能在一个固定的离散集上估计值。文献

(Dabney et al., 2018b) 提出了分位数回归 DQN（Quantile Regression DQN，QR-DQN），通过分位

数回归估计完整分布的分位数来解决这个问题。在介绍 QR-DQN 之前，我们先来看看这个分位

数回归（Quantile Regression）。回想一下，对绝对损失函数进行经验风险最小化，能使预测符合

中值（50% 分位数）。具体来说，给定随机变量 x 及其标签 y，对于估计函数 f，经验平均绝对误

差为 Lmae = E[|f(x) − y|]。接着用如下的偏微分：

∂Lmae

∂f(x)

=

∂

∂f(x)

(P(f(x) > y)(f(x) − y) + P(f(x) ⩽ y)(y − f(x)))

= P(f(x) > y) − P(f(x) ⩽ y) = 0, (4.24)

我们能得到 F(x) = 0.5，其中 F 是 f 的原函数。通常来说，对于分位数 τ，其分位数损失定

义为 Lquantile(τ ) = E[ρτ (f(x) − y)]，其中

ρτ (α) =







τα, 若α > 0

(τ − 1)α, 其他

(4.25)

与之类似，通过 ∂Lquantile

∂f(x) ，我们能得到 F(x) = 1 − τ，即 f(x) 是随机变量 y 的 τ 分位数值。

具体来说，QR-DQN 考虑将 N 个均匀的分位数 qi =

1

N 作为值分布。对于一个 QR-DQN 模

型 θ : S → R

N×|A|，在采样期间，Q 值的状态 s 和动作 a 是 N 个估计的平均值：Q(s, a) =

PN

i=1 qiθi(s, a)。在训练过程中，基于 Q 值的贪心策略在下个状态提供 a

∗ = arg maxa′ Q(s

′

, a′

)，

并且根据公式 (4.20)，分布式贝尔曼目标为 T θj = r + γθj (s

′

, a∗

)。文献 (Dabney et al., 2018b) 中

130

4.8 DQN 代码实例

的引理 2 指出下式的和可以最小化近似值分布与真实值之间的 1-Wasserstein 距离：

X

N

i=1

Ej [ρτˆi

(T θj − θi(s, a))]. (4.26)

其中 τˆi =

i

N −

1

2N 。

图 4.5 展示了 DQN、C51 和 QR-DQN 的对比。接下来在值分布强化学习上，其参数化分布

的灵活性和鲁棒性上还有更多的工作要做。读者对这方面感兴趣的话可以从文献 (Dabney et al.,

2018a; Mavrin et al., 2019; Yang et al., 2019) 中找到相关资源。

图 4.5 对比 s 和动作 a 下的 DQN，C51 和 QR-DQN。其中箭头指向的是估计值。QR-DQN 中分位

数的数量指定为 4。DQN 的结构只输出实际 Q 值的近似值。对于值分布强化学习，C51 估

计了多个 Q 值，而 QR-DQN 提供了 Q 值的分位数

4.8 DQN 代码实例

本节中，我们将围绕 DQN 及其变体算法讨论更多训练细节。首先演示雅达利环境的设置过

程，以及如何实现一些十分有用的装饰器（Wrapper）。高效地使用装饰器能使训练更加简单和

稳定。

Gym 环境相关

OpenAI Gym 是一个用于开发和对比强化学习算法的开源工具包。它包含了如图 4.6 显示的

一系列环境。它可以直接从 PyPI 安装，默认安装包不带有雅达利组件，需要使用雅达利扩展安装：

pip install gym[atari]

也可以直接从源安装。

git clone https://github.com/openai/gym.git

cd gym

131

第 4 章 深度 Q 网络

pip install -e .

图 4.6 OpenAI Gym 的一些环境

可以通过以下代码建立环境实例 env：

import gym

env = gym.make(env_id)

其中 env_id 是环境名称的字符串。所有可用的 env_id 可以在网址（链接见读者服务）上

查到。

env 实例中有以下重要的方法：

1. env.reset() 重启环境并返回初始的观测数据。

2. env.render(mode) 根据所给的 mode 模式呈现环境图像。默认为 human 模式，它将呈

现当前显示画面或者终端窗口，并不返回任何内容。你可以指定 rgb_array 模式来使

env.render 函数返回 numpy.ndarray 对象，这些数据可用于生成视频。

3. env.step(action) 在环境中执行动作 action，并运行一个时间步。之后返回 (observation, reward, done, info) 的数据元组，其中 observation 为当前环境的观测数据，

reward 是状态转移的奖励，done 指出当前片段是否结束，info 则包含一些辅助信息。

4. env.seed(seed) 手动设置随机种子。该函数在复现效果时非常有用。

这里展示了一个经典游戏 Breakout（打砖块）的例子。我们将先运行一个 BreakoutNoFrameskip-v4 环境的实例直到本片段结束。游戏过程的一个样帧图像如图 4.7 所示。

import gym

env = gym.make(’BreakoutNoFrameskip-v4’)

132

4.8 DQN 代码实例

o = env.reset()

while True:

env.render()

# take a random action

a = env.action_space.sample()

o, r, done, _ = env.step(a)

if done:

break

env.close() # close and clean up

图 4.7 Breakout 游戏的一个样帧图像。在屏幕上方有几行需要被破坏的砖块。智能体可以控制屏

幕下方的挡板，并控制角度弹射小球到想要的位置来撞毁砖块。该游戏的观测数据是形

状为 (210, 160, 3) 的 RGB 屏幕图像

需要注意的是，游戏 id 中的 NoFrameskip 意味着没有跳帧和动作重复，而 v4 意思是当前

为第 4 个版本，也是本书写稿时的最新版本。我们将在接下来的例子中使用该环境。

OpenAI Gym 的另一个十分有用的特性是环境装饰器。它可以对环境对象进行装饰，使训练

代码更加简洁。如下代码展示了一个用于限制每个回合片段最大长度的时间限制装饰器，这也是

雅达利游戏的一个默认装饰器。

class TimeLimit(gym.Wrapper):

def __init__(self, env, max_episode_steps=None):

super(TimeLimit, self).__init__(env)

self._max_episode_steps = max_episode_steps

self._elapsed_steps = 0

133

第 4 章 深度 Q 网络

def step(self, ac):

o, r, done, info = self.env.step(ac)

self._elapsed_steps += 1

if self._elapsed_steps >= self._max_episode_steps:

done = True

info[’TimeLimit.truncated’] = True

return o, r, done, info

def reset(self, **kwargs):

self._elapsed_steps = 0

return self.env.reset(**kwargs)

为了更加高效地训练，gym.vector.AsyncVectorEnv 提供了一个用来并行运行 n 个环境的

矢量化装饰器的实现。所有的接口将统一收到并返回 n 个变量。此外，还可以实现一个带有缓存

的矢量化装饰器，其接口也接受和返回 n 个变量，但会在后台保持 m > n 个线程。这样将更为

高效地运行某些状态转移耗时较长的环境。

Gym 提供一系列雅达利 2600 游戏的标准接口。这些游戏可以以游戏内存数据或者屏幕图像

数据作为输入，使用街机学习环境 (Bellemare et al., 2013) 运行。在这 2600 款雅达利游戏中，有

些游戏最多包含 18 个不同的按键组合：

1. 移动按键：空动作、上移、右移、左移、下移、右上键组合、左上键组合、右下键组合、左

下键组合。

2. 攻击按键：开火、上移开火组合、右移开火组合、左移开火组合、下移开火组合、右上开火

组合、左上开火组合、右下开火组合、左下开火组合。

此处的空动作表示什么都不做。然后开火键可能被作为开始游戏的按键。为了方便起见，我

们后续将以按键名称称呼其对应的动作。

DQN

DQN 还有三个额外的训练技巧。首先，依次使用如下的装饰器可以让训练更加稳定高效。

1. NoopResetEnv 在重置游戏时，会随机地进行几步空动作，以确保初始化的状态更为随机。

默认的最大空动作数量为 30。这个装饰器将有助于智能体收集更多的初始状态，提供更为

鲁棒的学习。

2. MaxAndSkipEnv 重复每个动作 4 次，以提供更为高效的学习。为了进一步对观测数据降噪，

返回的图像帧是在最近 2 帧上对像素进行最大池化的结果。

3. Monitor 记录原始奖励数据。我们可以在这个装饰器中实现一些有用的函数，比如速度跟

踪器。

134

4.8 DQN 代码实例

4. EpisodicLifeEnv 使得本条命结束的时候，相当于本片段结束。这样不用等到玩家所有命

都消耗完才能结束本片段，对价值估计很有帮助 (Roderick et al., 2017)。

5. FireResetEnv 在环境重置的时候触发开火动作。很多游戏需要这个开火动作来开始游戏。

这是快速开始游戏的先验知识。

6. WarpFrame 将观测数据转换为 84 × 84 的灰度图像。

7. ClipRewardEnv 将奖励通过符号进行装饰，只根据奖励数据的符号输出 −1、0、1 三种奖励

值。这样防止任何一个单独的小批量更新而大幅改变参数，可以进一步提高稳定性。

8. FrameStack 堆叠最后 4 帧。我们回忆一下，DQN 为了捕捉运动信息，通过堆叠当前帧和前

3 帧来用函数 ϕ 对观测数据进行预处理。FrameStack 和 WarpFrame 实现了 ϕ 的功能。需要

注意的是，我们可以通过只在观测值之间存储一次公共帧来优化内存使用，这也称为延迟

帧技术（Lazy-Frame Trick）。

其次，为避免梯度爆炸，DQN (DeepMind, 2015; Mnih et al., 2015) 使用了对平方误差进行了裁

剪，这等同于将均方差替换成了 δ = 1 情况下的 Huber 损失 (Huber, 1992)。Huber 损失如下所示：

Lδ(x) =







1

2

x

2

|x| ⩽ δ

δ



|x| − 1

2

δ



其他

(4.27)

最终，回放缓存采样了大批有放回的抽样。在能够有个稳定的开始之前，最后还需要完成一

些热启动步骤。

注意到上述所说的全部三个技巧都用于本节中所有的实验。现在我们将展示如何建立一个能

玩 Breakout 游戏的智能体。首先，为了实验的可复现性，我们将手动设置相关库的随机种子。

random.seed(seed)

np.random.seed(seed)

tf.random.set_seed(seed)

接着，我们通过 tf.keras.Model 创建一个 Q 网络：

class QFunc(tf.keras.Model):

def __init__(self, name):

super(QFunc, self).__init__(name=name)

self.conv1 = tf.keras.layers.Conv2D(

32, kernel_size=(8, 8), strides=(4, 4),

padding=’valid’, activation=’relu’)

self.conv2 = tf.keras.layers.Conv2D(

64, kernel_size=(4, 4), strides=(2, 2),

135

第 4 章 深度 Q 网络

padding=’valid’, activation=’relu’)

self.conv3 = tf.keras.layers.Conv2D(

64, kernel_size=(3, 3), strides=(1, 1),

padding=’valid’, activation=’relu’)

self.flat = tf.keras.layers.Flatten()

self.fc1 = tf.keras.layers.Dense(512, activation=’relu’)

self.fc2 = tf.keras.layers.Dense(action_dim, activation=’linear’)

def call(self, pixels, **kwargs):

# scale observation

pixels = tf.divide(tf.cast(pixels, tf.float32), tf.constant(255.0))

# extract features by convolutional layers

feature = self.flat(self.conv3(self.conv2(self.conv1(pixels))))

# calculate q-value

qvalue = self.fc2(self.fc1(feature))

return qvalue

DQN 对象的定义由 Q 网络、目标 Q 网络、训练时间步数目和优化器、同步 Q 网络、目标 Q

网络这些属性组成，代码如下所示。

class DQN(object):

def __init__(self):

self.qnet = QFunc(’q’)

self.targetqnet = QFunc(’targetq’)

sync(self.qnet, self.targetqnet)

self.niter = 0

self.optimizer = tf.optimizers.Adam(lr, epsilon=1e-5, clipnorm=clipnorm)

申明一个内部方法，以装饰 Q 网络，之后再给 DQN 对象添加一个 get_action 方法来执行

ϵ-贪心的行动。

@tf.function

def _qvalues_func(self, obv):

return self.qnet(obv)

def get_action(self, obv):

eps = epsilon(self.niter)

if random.random() < eps:

136

4.8 DQN 代码实例

return int(random.random() * action_dim)

else:

obv = np.expand_dims(obv, 0).astype(’float32’)

return self._qvalues_func(obv).numpy().argmax(1)[0]

其中，这里的 epsilon 函数是一个在前 10% 训练时间步中，将 ϵ 线性地从 1.0 退火到 0.01 的

函数。为了更好地训练，我们为 DQN 及其变体提供了 3 个通用接口，即 train、_train_func、

_tderror_func。

def train(self, b_o, b_a, b_r, b_o_, b_d):

self._train_func(b_o, b_a, b_r, b_o_, b_d)

self.niter += 1

if self.niter

sync(self.qnet, self.targetqnet)

@tf.function

def _train_func(self, b_o, b_a, b_r, b_o_, b_d):

with tf.GradientTape() as tape:

td_errors = self._tderror_func(b_o, b_a, b_r, b_o_, b_d)

loss = tf.reduce_mean(huber_loss(td_errors))

grad = tape.gradient(loss, self.qnet.trainable_weights)

self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))

return td_errors

@tf.function

def _tderror_func(self, b_o, b_a, b_r, b_o_, b_d):

b_q_ = (1 - b_d) * tf.reduce_max(self.targetqnet(b_o_), 1)

b_q = tf.reduce_sum(self.qnet(b_o) * tf.one_hot(b_a, action_dim), 1)

return b_q - (b_r + reward_gamma * b_q_)

其中 train 调用了 _train_func 并每 target_q_update_freq 个时间步将目标 Q 网络与 Q

网络进行同步。

最终，我们构建主要训练步骤：

137

第 4 章 深度 Q 网络

dqn = DQN()

buffer = ReplayBuffer(buffer_size)

o = env.reset()

nepisode = 0

t = time.time()

for i in range(1, number_time steps + 1):

a = dqn.get_action(o)

# execute action and feed to replay buffer

# note that ‘_‘ tail in var name means next

o_, r, done, info = env.step(a)

buffer.add(o, a, r, o_, done)

if i >= warm_start and i

transitions = buffer.sample(batch_size)

dqn.train(*transitions)

if done:

o = env.reset()

else:

o = o_

# episode in info is real (unwrapped) message

if info.get(’episode’):

nepisode += 1

reward, length = info[’episode’][’r’], info[’episode’][’l’]

print(

’Time steps so far: {}, episode so far: {}, ’

’episode reward: {:.4f}, episode length: {}’

.format(i, nepisode, reward, length)

)

我们在 3 个随机种子上运行了 Breakout 游戏 107 个时间步（4 × 107 帧）。为了更好地可视

化，我们将训练时的片段奖励进行平滑处理。之后通过如下代码绘制均值和标准差，输出效果如

图 4.8 所示的红色区域。

138

4.8 DQN 代码实例

图 4.8 DQN 及其变体在 Breakout 游戏中的效果（见彩插）

from matplotlib import pyplot as plt

plt.plot(xs, mean, color=color)

plt.fill_between(xs, mean - std, mean + std, color=color, alpha=.4)

Double DQN

Double DQN 可以通过更新 Double Q 的估计来简单地实现。在智能体的 _tderror_func 中

使用如下 Double Q 估计的代码进行替换即可。

# double Q estimation

b_a_ = tf.one_hot(tf.argmax(qnet(b_o_), 1), out_dim)

b_q_ = (1 - b_d) * tf.reduce_sum(targetqnet(b_o_) * b_a_, 1)

我们也在 Breakout 游戏上，使用 3 个随机种子运行了 107 个时间步。输出效果显示在图 4.8

上的绿色区域。

Dueling DQN

Dueling 架构只对 Q 网络进行了修改，它可以通过如下方式实现：

class QFunc(tf.keras.Model):

def __init__(self, name):

super(QFunc, self).__init__(name=name)

139

第 4 章 深度 Q 网络

self.conv1 = tf.keras.layers.Conv2D(

32, kernel_size=(8, 8), strides=(4, 4),

padding=’valid’, activation=’relu’)

self.conv2 = tf.keras.layers.Conv2D(

64, kernel_size=(4, 4), strides=(2, 2),

padding=’valid’, activation=’relu’)

self.conv3 = tf.keras.layers.Conv2D(

64, kernel_size=(3, 3), strides=(1, 1),

padding=’valid’, activation=’relu’)

self.flat = tf.keras.layers.Flatten()

self.fc1q = tf.keras.layers.Dense(512, activation=’relu’)

self.fc2q = tf.keras.layers.Dense(action_dim, activation=’linear’)

self.fc1v = tf.keras.layers.Dense(512, activation=’relu’)

self.fc2v = tf.keras.layers.Dense(1, activation=’linear’)

def call(self, pixels, **kwargs):

# scale observation

pixels = tf.divide(tf.cast(pixels, tf.float32), tf.constant(255.0))

# extract features by convolutional layers

feature = self.flat(self.conv3(self.conv2(self.conv1(pixels))))

# calculate q-value

qvalue = self.fc2q(self.fc1q(feature))

svalue = self.fc2v(self.fc1v(feature))

return svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)

我们同样在 Breakout 游戏上，使用 3 个随机种子运行了 107 个时间步。在图 4.8 上的青色区域是

该方法的输出效果。

经验优先回放

PER 相较于标准的 DQN 有三个变化。首先，回放缓存维持了 2 个线段树进行取小和求和操

作，来高效地计算最小优先级和优先级之和。更具体地说，_it_sum 属性是具备两个接口的求和

操作线段树对象，sum 用于获得指定区间内的元素之和，而 find_prefixsum_idx 用于查找更高

的索引 i，以使最小的 i 个元素比输入值要小。

其次，为了代替原本的均匀采样，考虑比例信息的采样策略如下所示：

140

4.8 DQN 代码实例

res = []

p_total = self._it_sum.sum(0, len(self._storage) - 1)

every_range_len = p_total / batch_size

for i in range(batch_size):

mass = random.random() * every_range_len + i * every_range_len

idx = self._it_sum.find_prefixsum_idx(mass)

res.append(idx)

return res

最后，不同于普通的回放缓存，PER 必须返回采样经验的索引和标准化的权重。权重用于计

算加权 Huber 损失，而索引则用于更新优先级。采样步骤将被修改为

*transitions, idxs = buffer.sample(batch_size)

priorities = dqn.train(*transitions)

priorities = np.clip(np.abs(priorities), 1e-6, None)

buffer.update_priorities(idxs, priorities)

_train_func 可修改为

@tf.function

def _train_func(self, b_o, b_a, b_r, b_o_, b_d, b_w):

with tf.GradientTape() as tape:

td_errors = self._tderror_func(b_o, b_a, b_r, b_o_, b_d)

loss = tf.reduce_mean(huber_loss(td_errors) * b_w)

grad = tape.gradient(loss, self.qnet.trainable_weights)

self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))

return td_errors

我们还是在 Breakout 游戏上，使用 3 个随机种子运行了 107 个时间步。图 4.8 上的洋红色区

域是该方法的输出效果。

深度 Q 分布网络

值分布强化学习对 Q 值进行估计。在本节中，我们将通过演示如何实现其中的 C51 技术，来

实现一种值分布强化学习方法。在 Breakout 游戏中，奖励都是正数。因此，我们将文献 (Bellemare

et al., 2017) 中值的范围 [−10, 10] 换成 [−1, 19]，其中 −1 是为了允许一些近似误差。实现 C51 首

141

第 4 章 深度 Q 网络

先要做的是让 Q 网络给每个动作输出 51 个估计值，这点可以通过在最后的全连接层增加更多的

输出单元来实现。接着，为了替代 TD 误差，需要使用目标 Q 分布和估计分布之间的 KL 散度作

为误差：

@tf.function

def _kl_divergence_func(self, b_o, b_a, b_r, b_o_, b_d):

b_r = tf.tile(

tf.reshape(b_r, [-1, 1]),

tf.constant([1, atom_num])

) # batch_size * atom_num

b_d = tf.tile(

tf.reshape(b_d, [-1, 1]),

tf.constant([1, atom_num])

)

z = b_r + (1 - b_d) * reward_gamma * vrange # shift value distribution

z = tf.clip_by_value(z, min_value, max_value) # clip the shifted distribution

b = (z - min_value) / deltaz

index_help = tf.expand_dims(tf.tile(

tf.reshape(tf.range(batch_size), [batch_size, 1]),

tf.constant([1, atom_num])

), -1)

b_u = tf.cast(tf.math.ceil(b), tf.int32) # upper

b_uid = tf.concat([index_help, tf.expand_dims(b_u, -1)], 2) # indexes

b_l = tf.cast(tf.math.floor(b), tf.int32)

b_lid = tf.concat([index_help, tf.expand_dims(b_l, -1)], 2) # indexes

b_dist_ = self.targetqnet(b_o_) # whole distribution

b_q_ = tf.reduce_sum(b_dist_ * vrange_broadcast, axis=2)

b_a_ = tf.cast(tf.argmax(b_q_, 1), tf.int32)

b_adist_ = tf.gather_nd( # distribution of b_a_

b_dist_,

tf.concat([tf.reshape(tf.range(batch_size), [-1, 1]),

tf.reshape(b_a_, [-1, 1])], axis=1)

)

b_adist = tf.gather_nd( # distribution of b_a

self.qnet(b_o),

142

参考文献

tf.concat([tf.reshape(tf.range(batch_size), [-1, 1]),

tf.reshape(b_a, [-1, 1])], axis=1)

) + 1e-8

b_l = tf.cast(b_l, tf.float32)

mu = b_adist_ * (b - b_l) * tf.math.log(tf.gather_nd(b_adist, b_uid))

b_u = tf.cast(b_u, tf.float32)

ml = b_adist_ * (b_u - b) * tf.math.log(tf.gather_nd(b_adist, b_lid))

kl_divergence = tf.negative(tf.reduce_sum(mu + ml, axis=1))

return kl_divergence

当然我们在 Breakout 游戏上，使用 3 个随机种子运行了 107 个时间步。图 4.8 上的蓝色区域

是该方法的输出效果。

参考文献

BELLEMARE M G, NADDAF Y, VENESS J, et al., 2013. The Arcade Learning Environment: An

evaluation platform for general agents[J]. Journal of Artificial Intelligence Research, 47: 253-279.

BELLEMARE M G, DABNEY W, MUNOS R, 2017. A distributional perspective on reinforcement

learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.

org: 449-458.

CASTRO P S, MOITRA S, GELADA C, et al., 2018. Dopamine: A research framework for deep

reinforcement learning[J].

DABNEY W, OSTROVSKI G, SILVER D, et al., 2018a. Implicit quantile networks for distributional

reinforcement learning[C]//International Conference on Machine Learning. 1104-1113.

DABNEY W, ROWLAND M, BELLEMARE M G, et al., 2018b. Distributional reinforcement learning

with quantile regression[C]//Thirty-Second AAAI Conference on Artificial Intelligence.

DEEPMIND, 2015. Lua/Torch implementation of DQN[J]. GitHub repository.

FORTUNATO M, AZAR M G, PIOT B, et al., 2017. Noisy networks for exploration[J]. arXiv preprint

arXiv:1706.10295.

143

第 4 章 深度 Q 网络

HERNANDEZ-GARCIA J F, SUTTON R S, 2019. Understanding multi-step deep reinforcement learning:

A systematic study of the DQN target[C]//Proceedings of the Neural Information Processing Systems

(Advances in Neural Information Processing Systems) Workshop.

HESSEL M, MODAYIL J, VAN HASSELT H, et al., 2018. Rainbow: Combining improvements in deep

reinforcement learning[C]//Thirty-Second AAAI Conference on Artificial Intelligence.

HUBER P J, 1992. Robust estimation of a location parameter[M]//Breakthroughs in statistics. Springer:

492-518.

LIN L J, 1993. Reinforcement learning for robots using neural networks[R]. Carnegie-Mellon Univ

Pittsburgh PA School of Computer Science.

MAVRIN B, YAO H, KONG L, et al., 2019. Distributional reinforcement learning for efficient exploration[C]//International Conference on Machine Learning. 4424-4434.

MCCLELLAND J L, MCNAUGHTON B L, O’REILLY R C, 1995. Why there are complementary

learning systems in the hippocampus and neocortex: insights from the successes and failures of

connectionist models of learning and memory.[J]. Psychological review, 102(3): 419.

MNIH V, KAVUKCUOGLU K, SILVER D, et al., 2015. Human-level control through deep reinforcement

learning[J]. Nature.

O’NEILL J, PLEYDELL-BOUVERIE B, DUPRET D, et al., 2010. Play it again: reactivation of waking

experience and memory[J]. Trends in neurosciences, 33(5): 220-229.

RIEDMILLER M, 2005. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method[C]//European Conference on Machine Learning. Springer: 317-328.

RODERICK M, MACGLASHAN J, TELLEX S, 2017. Implementing the deep Q-network[J]. arXiv

preprint arXiv:1711.07478.

SCHAUL T, QUAN J, ANTONOGLOU I, et al., 2015. Prioritized experience replay[C]//arXiv preprint

arXiv:1511.05952.

SUTTON R S, BARTO A G, 2018. Reinforcement learning: An introduction[M]. MIT press.

THRUN S, SCHWARTZ A, 1993. Issues in using function approximation for reinforcement learning[C]//

Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum.

144

参考文献

TSITSIKLIS J, VAN ROY B, 1996. An analysis of temporal-difference learning with function approximationtechnical[J]. Report LIDS-P-2322). Laboratory for Information and Decision Systems,

Massachusetts Institute of Technology, Tech. Rep.

TSITSIKLIS J N, VAN ROY B, 1997. Analysis of temporal-diffference learning with function approximation[C]//Advances in Neural Information Processing Systems. 1075-1081.

VAN HASSELT H, GUEZ A, SILVER D, 2016. Deep reinforcement learning with double Q-learning[C]//

Thirtieth AAAI conference on artificial intelligence.

WANG Z, SCHAUL T, HESSEL M, et al., 2016. Dueling network architectures for deep reinforcement

learning[C]//International Conference on Machine Learning. 1995-2003.

YANG D, ZHAO L, LIN Z, et al., 2019. Fully parameterized quantile function for distributional reinforcement learning[C]//Advances in Neural Information Processing Systems. 6190-6199.

145

5 策略梯度

策略梯度方法（Policy Gradient Methods）是一类直接针对期望回报（Expected Return）通过

梯度下降（Gradient Descent）进行策略优化的增强学习方法。这一类方法避免了其他传统增强学

习方法所面临的一些困难，比如，没有一个准确的价值函数，或者由于连续的状态和动作空间，

以及状态信息的不确定性而导致的难解性（Intractability）。在这一章中，我们会学习一系列策略

梯度方法。从最基本的 REINFORCE 开始，我们会逐步介绍 Actor-Critic 方法及其分布式计算的

版本、信赖域策略优化（Trust Region Policy Optimization）及其近似算法，等等。在本章最后一

节，我们附上了本章涉及的所有方法所对应的伪代码，以及一个具体的实现例子。

5.1 简介

这一章主要介绍策略梯度方法。和上一章介绍的学习 Q 值函数的深度 Q-Learning 方法不同，

策略梯度方法直接学习参数化的策略 πθ。这样做的一个好处是不需要在动作空间中求解价值最

大化的优化问题，从而比较适合解决具有高维或者连续动作空间的问题。策略梯度方法的另一个

好处是可以很自然地对随机策略进行建模1。最后，策略梯度方法利用了梯度的信息来引导优化

的过程。一般来讲，这样的方法有更好的收敛性保证2。

顾名思义，策略梯度方法通过梯度上升的方法直接在神经网络的参数上优化智能体的策略。

在这一章中，我们会在 5.2 节中推导出策略梯度的初始版本算法。这个算法一般会有估计方差过

高的问题。我们在 5.3 节会看到 Actor-Critic 算法可以有效地减轻这个问题。有趣的是，Actor-Critic

1在价值学习的设定下，智能体需要额外构造它的探索策略，比如 ϵ-贪心，以对随机性策略进行建模。

2但一般也仅限于局部收敛性，而不是全局收敛性。近期的一些研究在策略梯度的全局收敛性上有一些进展，但本章不

讨论这一方面的工作。

146

5.2 REINFORCE：初版策略梯度

和 GAN 的设计非常相像。我们会在 5.4 节比较它们的相似之处。在 5.5节、5.6 节中，我们会接

着介绍 Actor-Critic 的分布式版本。最后，我们通过考虑在策略空间（而不是参数空间）中的梯

度上升进一步提高策略梯度方法的性能。一个被广泛使用的方法是信赖域策略优化（Trust Region

Policy Optimization，TRPO），我们会在 5.7 节和 5.8 节介绍它及其近似版本，即近端策略优化算法

（Proximal Policy Optimization，PPO），以及在 5.9 节中介绍使用 Kronecker 因子化信赖域的 Actor

Critic（Actor Critic using Kronecker-factored Trust Region，ACKTR）。

在本章的最后一节，即 5.10 节中，我们提供了所涉及算法的代码实现，以方便读者可以迅速

上手试验。每个算法的完整实现可以在本书的代码库找到3。

5.2 REINFORCE：初版策略梯度

REINFORCE 算法在策略的参数空间中直观地通过梯度上升的方法逐步提高策略 πθ 的性能。

回顾一下，由式子 (2.119) 我们有

∇θJ(πθ)=Eτ∼πθ





X

T

t=0

Rt∇θ

Xt

t

′=0

log πθ(At

′ |St

′ )



=Eτ∼πθ





X

T

t

′=0

∇θ log πθ(At

′ |St

′ )

X

T

t=t

′

Rt



 . (5.1)

注 5.1 上述式子中 PT

t=i Rt 可以看成是智能体在状态 Si 处选择动作 Ai，并在之后执行当前策略

的情况下，从第 i 步开始获得的累计奖励。事实上，PT

t=i Rt 也可以看成 Qi(Ai

, Si)，在第 i 步状

态 Si 处采取动作 Ai，并在之后执行当前策略的 Q 值。所以，一个理解 REINFORCE 的角度是：

通过给不同的动作所对应的梯度根据它们的累计奖励赋予不同的权重，鼓励智能体选择那些累计

奖励较高的动作 Ai。

只要把上述式子中的 T 替换成 ∞ 并赋予 Rt 以 γ

t 的权重，上述式子很容易可以扩展到折扣

因子为 γ 的无限范围的设定如下。

∇J(θ) = Eτ∼πθ





X∞

t

′=0

∇θ log πθ(At

′ | St

′ )γ

t

′ X∞

t=t

′

γ

t−t

′

Rt



 . (5.2)

由于折扣因子给未来的奖励赋予了较低的权重，使用折扣因子还有助于减少估计梯度时的方差大

的问题。实际使用中，γ

t

′ 经常被去掉，从而避免了过分强调轨迹早期状态的问题。

虽然 REINFORCE 简单直观，但它的一个缺点是对梯度的估计有较大的方差。对于一个长度

为 L 的轨迹，奖励 Rt 的随机性可能对 L 呈指数级增长。为了减轻估计的方差太大这个问题，一

个常用的方法是引进一个基准函数 b(Si)。这里对 b(Si) 的要求是：它只能是一个关于状态 Si 的

函数（或者更确切地说，它不能是关于 Ai 的函数）。

3链接见读者服务

147

第 5 章 策略梯度

有了基准函数 b(St) 之后，增强学习目标函数的梯度 ∇J(θ) 可以表示成

∇J(θ) = Eτ∼πθ







X∞

t

′=0

∇θ log πθ(At

′ | St

′ )





X∞

t=t

′

γ

t−t

′

Rt − b(St

′ )









 . (5.3)

这是因为

Eτ,θ

∇θ log πθ(At

′ | St

′ )b(St

′ )



= Eτ,θ h

b(St

′ )Eθ



∇ log πθ(At

′ | St

′ )| St

′



i

= 0. (5.4)

上述式子的最后一个等式可以由 EGLP 引理（引理 2.2）得到。最后如算法 5.18 所示，我们得到

带有基准函数的 REINFORCE 算法。

算法 5.18 带基准函数的 REINFORCE 算法

超参数: 步长 ηθ、奖励折扣因子 γ、总步数 L、批尺寸 B、基准函数 b。

输入: 初始策略参数 θ0

初始化 θ = θ0

for k = 1, 2, · · · , do

执行策略 πθ 得到 B 个轨迹，每一个有 L 步，并收集 {St,ℓ, At,ℓ, Rt,ℓ}。

Aˆ

t,ℓ =

PL

ℓ

′=ℓ

γ

ℓ

′−ℓRt,ℓ − b(St,ℓ)

J(θ) = 1

B

PB

t=1

PL

ℓ=0 log πθ(At,ℓ|St,ℓ)Aˆ

t,ℓ

θ = θ + ηθ∇J(θ)

用 {St,ℓ, At,ℓ, Rt,ℓ} 更新 b(St,ℓ)

end for

返回 θ

直观来讲，从奖励函数中减去一个基准函数这个方法是一个常见的降低方差的方法。假设需

要估计一个随机变量 X 的期望 E [X]。对于任意一个期望为 0 的随机变量 Y ，我们知道 X − Y

依然是 E [X] 的一个无偏估计。而且，X − Y 的方差为

V(X − Y ) = V(X) + V(Y ) − 2cov(X, Y ). (5.5)

式子中的 V 表示方差，cov(X, Y ) 表示 X 和 Y 的协方差。所以如果 Y 本身的方差较小，而且

和 X 高度正相关，那么 X − Y 会是一个方差较小的关于 E [X] 的无偏估计。在策略梯度方法

中，基准函数的常见选择是状态价值函数 V (Si)。在下一节中我们可以看到，这个算法和初版的

Actor-Critic 算法很相像。最近的一些研究工作也提出了其他不同的基准函数的选择，感兴趣的读

者可以从文献 (Li et al., 2018; Liu et al., 2017; Wu et al., 2018) 中了解更多的细节。

148

5.3 Actor-Critic

5.3 Actor-Critic

Actor-Critic 算法 (Konda et al., 2000; Sutton et al., 2000) 是一个既基于策略也基于价值的方法。

在上一节我们提到，在初版策略梯度方法中可以用状态价值函数作为基准函数来降低梯度估计的

方差。Actor-Critic 算法也沿用了相同的想法，同时学习行动者（Actor）函数（也就是智能体的策略

函数 π(·|s)）和批判者（Critic）函数（也就是状态价值函数 V

π

(s)）。此外，Actor-Critic 算法还沿用

了自举法（Bootstrapping）的思想来估计 Q 值函数。REINFORCE 中的误差项 P∞

t=i

γ

t−iRt −b(Si)

被时间差分误差取代了，即 Ri + γV π

(Si+1) − V

π

(Si)。

我们这里采用 L 步的时间差分误差，并通过最小化该误差的平方来学习批判者函数 V

πθ

ψ

(s)，

即

ψ ← ψ − ηψ∇JV

πθ

ψ

(ψ). (5.6)

式子中 ψ 表示学习批判者函数的参数，ηψ 是学习步长，并且

JV

πθ

ψ

(ψ) = 1

2





i+

X

L−1

t=i

γ

t−iRt + γ

LV

πθ

ψ

(S

′

) − V

πθ

ψ

(Si)





2

, (5.7)

S

′ 是智能体在 πθ 下 L 步之后到达的状态，所以

∇JV

πθ

ψ

(ψ) =



V

πθ

ψ

(Si) −

i+

X

L−1

t=i

γ

t−iRt − γ

LV

πθ

ψ

(S

′

)



 ∇V

πθ

ψ

(Si). (5.8)

类似地，行动者函数 πθ(·|s) 决定每个状态 s 上所采取的动作或者动作空间上的一个概率分

布。我们采用和初版策略梯度相似的方法来学习这个策略函数。

θ = θ + ηθ∇Jπθ

(θ), (5.9)

这里 θ 表示行动者函数的参数，ηθ 是学习步长，并且

∇J(θ) = Eτ,θ







X∞

i=0

∇ log πθ(Ai

| Si)





i+

X

L−1

t=i

γ

t−iRt + γ

LV

πθ

ψ

(S

′

) − V

πθ

ψ

(Si)









 . (5.10)

注意到，我们这里分别用了 θ 和 ψ 来表示策略函数和价值函数的参数。在实际应用中，当我

们选择用神经网络来表示这两个函数的时候，经常会让两个网络共享一些底层的网络层作为共同

的状态表征（State Representation）。此外，AC 算法中的 L 值经常设为 1, 也就是 TD(0) 误差。AC

149

第 5 章 策略梯度

算法的具体步骤如算法 5.19 所示。

算法 5.19 Actor-Critic 算法

超参数: 步长 ηθ 和 ηψ, 奖励折扣因子 γ。

输入: 初始策略函数参数 θ0、初始价值函数参数 ψ0。

初始化 θ = θ0 和 ψ = ψ0。

for t = 0, 1, 2, · · · do

执行一步策略 πθ, 保存 {St, At, Rt, St+1}。

估计优势函数 Aˆ

t = Rt + γV πθ

ψ

(St+1) − V

πθ

ψ

(St)。

J(θ) = P

t

log πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ) = P

t Aˆ2

t

ψ = ψ + ηψ∇JV

πθ

ψ

(ψ), θ = θ + ηθ∇J(θ)

end for

返回 (θ, ψ)

值得注意的是，AC 算法也可以使用 Q 值函数作为其批判者。在这种情况下，优势函数可以

用以下式子估计。

Q(s, a) − V (s) = Q(s, a) −

X

a

π(a|s)Q(s, a). (5.11)

用来学习 Q 值函数这个批判者的损失函数为

JQ =



Rt + γQ(St+1, At+1) − Q(St, At)

2

, (5.12)

或者

JQ =



Rt + γ

X

a

πθ(a|St+1)Q(St+1, a) − Q(St, At)

!2

. (5.13)

这里动作 At+1 由当前策略 πθ 在状态 St+1 下取样而得。

5.4 生成对抗网络和 Actor-Critic

初看上去，生成对抗网络（Generative Adversarial Networks，GAN）(Goodfellow et al., 2014) 和

Actor-Critic 应该是截然不同的算法，用于不同的机器学习领域，一个是生成模型，而另一个是强

化学习算法。但是实际上它们的结构十分类似。对于 GAN，有两个部分：用于根据某些输入生成

对象的生成网络，以及紧接生成网络的用于判断生成对象真实与否的判别网络。对于 Actor-Critic

方法，也有两部分：根据状态输入生成动作的动作网络，以及一个紧接动作网络之后用价值函数

15

5.4 生成对抗网络和 Actor-Critic

（比如下一个动作的价值或 Q 值）评估动作好坏的批判网络。

因此，GAN 和 Actor-Critic 基本遵循相同的结构。在这个结构中有两个相继的部分：一个用

于生成物体，第二个用一个分数来评估生成物体的好坏；随后选择一个优化过程来使第二部分能

够准确评估，并通过第二部分反向传播梯度到第一部分来保证它生成我们想要的内容，通过一个

定义为损失函数的标准，也就是一个来自结构第二部分的分数或价值函数来实现。

GAN 和 Actor-Critic 的结构详细比较如图 5.1 所示。

图 5.1 对比 GAN 和 Actor-Critic 的结构。在 GAN 中，z 是输入的噪声变量，它可以从如正态分布

中采样，而 x 是从真实目标中采集的数据样本。在 Actor-Critic 中，s 和 a 分别表示状态和

动作

• 对第一个生成物体的部分：GAN 中的生成器和 Actor-Critic 中的行动者基本一致，包括其前

向推理过程和反向梯度优化过程。对于前向过程，生成器采用随机变量做输入，并输出生

成的对象；对于方向优化过程，它的目标是最大化对生成对象的判别分数。行动者用状态

作为输入并输出动作，对于优化来说，它的目标是最大化状态-动作对的评估值。

• 对于第二个评估物体的部分：判别器和批判者由于其功能不同而优化公式也不同，但是遵循

相同的目标。判别器有来自真实对象额外输入。它的优化规则是最大化真实对象的判别值而

最小化生成对象的判别值，这与我们的需要相符。对于批判者，它使用时间差分（Temporal

Difference，TD）误差作为强化学习中的一种自举方法来按照最优贝尔曼方程优化价值函数。

也有一些其他模型彼此非常接近。举例来说，自动编码器（Auto-Encoder，AE）和 GAN 可

151

第 5 章 策略梯度

以是彼此的相反结构等。注意到，不同深度学习框架中的相似性可以帮助你获取关于现有不同领

域方法共性的认识，而这有助于为未解决的问题提出新的方法。

5.5 同步优势 Actor-Critic

同步优势 Actor-Critic（Synchronous Advantage Actor-Critic，A2C）(Mnih et al., 2016) 和上一

节讨论的 Actor-Critic 算法非常相似，只是在 Actor-Critic 算法的基础上增加了并行计算的设计。

如图 5.2 所示，全局行动者和全局批判者在 Master 节点维护。每个 Worker 节点的增强学习

智能体通过协调器和全局行动者、全局批判者对话。在这个设计中，协调器负责收集各个 Worker

节点上与环境交互的经验（Experience），然后根据收集到的轨迹执行一步更新。更新之后，全局

行动者被同步到各个 Worker 上继续和环境交互。在 Master 节点上，全局行动者和全局批判者的

学习方法和 Actor-Critic 算法中行动者和批判者的学习方法一致，都是使用 TD 平方误差作为批

判者的损失函数，以及 TD 误差的策略梯度来更新行动者的。

图 5.2 A2C 基本框架

在这种设计下，Worker 节点只负责和环境交互。所有的计算和更新都发生在 Master 节点。实

际应用中，如果希望降低 Master 节点的计算负担，一些计算也可以转交给 Worker 节点4，比如说，

每个 Worker 节点保存了当前全局批判者（Critic）。收集了一个轨迹之后，Worker 节点直接在本

地计算给出全局行动者（Actor）和全局批判者的梯度。这些梯度信息继而被传送回 Master 节点。

最后，协调器负责收集和汇总从各个 Worker 节点收集到的梯度信息，并更新全局模型。同样地，

更新后的全局行动者和全局批判者被同步到各个 Worker 节点。A2C 算法的基本框架如算法 5.20

所示。

4这经常取决于每个 Worker 节点的计算能力，比如是否有 GPU 计算能力，等等。

152

5.6 异步优势 Actor-Critic

算法 5.20 A2C

Master:

超参数: 步长 ηψ 和 ηθ, Worker 节点集 W。

输入: 初始策略函数参数 θ0、初始价值函数参数 ψ0。

初始化 θ = θ0 和 ψ = ψ0

for k = 0, 1, 2, · · · do

(gψ, gθ) = 0

for W 里每一个 Worker 节点 do

(gψ, gθ) = (gψ, gθ) + worker(V

πθ

ψ

, πθ)

end for

ψ = ψ − ηψgψ; θ = θ + ηθgθ。

end for

Worker:

超参数: 奖励折扣因子 γ、轨迹长度 L。

输入: 价值函数 V

πθ

ψ 、策略函数 πθ。

执行 L 步策略 πθ, 保存 {St, At, Rt, St+1}。

估计优势函数 Aˆ

t = Rt + γV πθ

ψ

(St+1) − V

πθ

ψ

(St)。

J(θ) = P

t

log πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ) = P

t Aˆ2

t

(gψ, gθ) = (∇JV

πθ

ψ

(ψ), ∇J(θ))

返回 (gψ, gθ)

5.6 异步优势 Actor-Critic

异步优势 Actor-Critic（Asynchronous Advantage Actor-Critic, A3C）(Mnih et al., 2016) 是上一

节中 A2C 的异步版本。在 A3C 的设计中，协调器被移除。每个 Worker 节点直接和全局行动者和

全局批判者进行对话。Master 节点则不再需要等待各个 Worker 节点提供的梯度信息，而是在每

次有 Worker 节点结束梯度计算的时候直接更新全局 Actor-Critic。由于不再需要等待，A3C 有比

A2C 更高的计算效率。但是同样也由于没有协调器协调各个 Worker 节点，Worker 节点提供梯度

信息和全局 Actor-Critic 的一致性不再成立，即每次 Master 节点从 Worker 节点得到的梯度信息很

可能不再是当前全局 Actor-Critic 的梯度信息。

注 5.2 虽然 A3C 为了计算效率而牺牲 Worker 节点和 Master 节点的一致性这一点看起来有些特

殊，这种异步更新的方式在神经网络的更新中其实非常常见。近期的研究 (Mitliagkas et al., 2016)

还表明，异步更新不仅加速了学习，还自动为 SGD 产生了类似于动量（Momentum）的效果。

153

第 5 章 策略梯度

算法 5.21 A3C

Master:

超参数: 步长 ηψ 和 ηθ、当前策略函数 πθ、价值函数 V

πθ

ψ 。

输入: 梯度 gψ, gθ。

ψ = ψ − ηψgψ; θ = θ + ηθgθ。

返回 (V

πθ

ψ

, πθ)

Worker:

超参数: 奖励折扣因子 γ、轨迹长度 L。

输入: 策略函数 πθ、价值函数 V

πθ

ψ 。

(gθ, gψ) = (0, 0)

for k = 1, 2, · · · , do

(θ, ψ) = Master(gθ, gψ)

执行 L 步策略 πθ, 保存 {St, At, Rt, St+1}。

估计优势函数 Aˆ

t = Rt + γV πθ

ψ

(St+1) − V

πθ

ψ

(St)

J(θ) = P

t

log πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ) = P

t Aˆ2

t

(gψ, gθ) = (∇JV

πθ

ψ

(ψ), ∇J(θ))

end for

5.7 信赖域策略优化

截至目前，我们在本章中介绍了初版策略梯度方法及其并行计算版本。在异步 Actor-Critic 的

策略梯度中，我们更新策略如下。

θ = θ + ηθ∇J(θ), (5.14)

这里

∇J(θ) = Eτ,θ





X∞

i=0

∇ log πθ(Ai

| Si)A

πθ

(Si

, Ai)



 , (5.15)

其中优势函数 Aπθ (s, a) 定义为

A

πθ

(s, a) = Q

πθ

(s, a) − V

πθ

ψ

(s). (5.16)

和标准的梯度下降算法一样，初版策略梯度方法也有步长不好确定的缺陷。梯度 ∇J(θ) 本

身只提供了在当前 θ 下局部的一阶信息而忽略了奖励函数定义的曲面的曲度。如果在高度弯曲的

区域选择了较大的步长，那么学习算法的性能可能会突然大幅下降。相反地，如果选择的步长太

154

5.7 信赖域策略优化

小，学习的过程可能会太保守，从而非常缓慢。更甚，策略梯度方法中的梯度 ∇J(θ) 需要从基于

当前策略 πθ 收集的样本中估计。策略性能的突然下降或者提升太过缓慢，会反过来影响收集到

的样本的质量，这让学习的性能对于步长的选择更敏感。

初版策略梯度方法的另一个局限是：它的更新是在参数空间，而不是策略空间中进行的。

Π = {π| π ⩾ 0,

Z

π = 1}. (5.17)

因为相同的步长 ηθ 可能使策略 πθ 在策略空间中有完全不一样幅度的更新，这使得步长 ηθ 在实

际应用中更加难以选择。举个例子，考虑当前的策略 π = (σ(θ), 1 − σ(θ)) 的两种不同情况。这里

σ(θ) 是 Sigmod 函数。假设在第一种情况下，θ 被从 θ = 6 更新到了 θ = 3。而在另一种情况中，θ

被从 θ = 1.5 更新到了 θ = −1.5。两种情况 πθ 在参数空间中的更新幅度都是 3。然而，在第一种

情况下，πθ 在策略空间中从几乎是 π ≈ (1.00, 0.00) 变成了 π ≈ (0.95, 0.05)，而在另一种情况下，

π = (0.82, 0.18) 被更新到了 π = (0.18, 0.82)。虽然两者在参数空间中的更新幅度相同，但是在策

略空间中的更新幅度却完全不同。

在本节中，我们会开发一个能更好处理步长的策略梯度算法。这个算法的思想基于信赖域的

想法，所以被称为信赖域策略优化算法（Trust Region Policy Optimization，TRPO）(Schulman et al.,

2015)。注意到，我们的目标是找到一个比原策略 πθ 更好的策略 π

′

θ。下述引理为 πθ 和 π

′

θ 的性能

提供了一个很深刻的联系：从 πθ 到 π

′

θ 在性能上的提升，可以由 πθ 的优势函数 Aπθ (s, a) 来计算。

文献 (Kakade et al., 2002) 让 θ

′ 表示 π

′

θ 的参数。

引理 5.1

J(θ

′

) = J(θ) + Eτ∼π

′

θ





X∞

t=0

γ

tA

πθ

(St, At)



 . (5.18)

这里 J(θ) = Eτ∼πθ

P∞

t=0 γ

tR(St, At)



，τ 是由 π

′

θ 产生的同状态动作轨迹。

所以，学习最优的策略 πθ 等价于最大化以下这个目标

Eτ∼π

′

θ





X∞

t=0

γ

tA

πθ

(St, At)



 . (5.19)

然而，上述式子其实难以直接优化，因为式子中的期望是在 π

′

θ 上。基于此，TRPO 优化该式子的

一个近似，我们用 Lπθ

(π

′

θ

) 表示，如下式。

Eτ∼π

′

θ





X∞

t=0

γ

tA

πθ

(St, At)



 (5.20)

155

第 5 章 策略梯度

= Es∼ρπ′

θ

(s)

h

Ea∼π

′

θ

(a| s)



A

πθ

(s, a)| s



i

(5.21)

≈ Es∼ρπθ

(s)

h

Ea∼π

′

θ

(a| s)



A

πθ

(s, a)| s



i

(5.22)

= Es∼ρπθ

(s)

"

Ea∼πθ(a| s)



π

′

θ

(a| s)

πθ(a| s)

A

πθ

(s, a)| s



#

(5.23)

= Eτ∼πθ





X∞

t=0

γ

t π

′

θ

(At| St)

πθ(At| St)

A

πθ

(St, At)



 . (5.24)

用 Lπθ

(π

′

θ

) 表示上述等式的最后一个式子，即

Lπθ

(π

′

θ

) = Eτ∼πθ





X∞

t=0

γ

t π

′

θ

(At| St)

πθ(At| St)

A

πθ

(St, At)



 .

在上面的式子中，我们直接用 ρπθ

(s) 来近似 ρπ

′

θ

(s)。这个近似虽然看似粗糙，但下面的定理在理

论上证明了，当 πθ 和 π

′

θ 相似的时候，这个近似并不差。

定理 5.1 让 Dmax

KL (πθ∥π

′

θ

) = maxs DKL(πθ(·|s)∥π

′

θ

(·|s)), 那么

|J(θ

′

) − J(θ) − Lπθ

(π

′

θ

)| ⩽ CDmax

KL (πθ∥π

′

θ

). (5.25)

这里 C 是和 π

′

θ 无关的常数。

因此，如果 Dmax

KL (πθ∥π

′

θ

) 很小，那么 Lπθ

(π

′

θ

) 可以合理地被作为一个优化目标。这便是 TRPO

的想法。实际中，TRPO 试图在平均 KL 散度的约束下优化 Lπθ

(π

′

θ

)，如下所示。

max

π

′

θ

Lπθ

(π

′

θ

) (5.26)

s.t. Es∼ρπθ



DKL(πθ∥π

′

θ

)



⩽ δ.

我们进一步讨论如何解 TRPO 中的这个优化问题。这里我们利用目标函数的一阶近似和约束

的二阶近似。事实上，Lπθ

(π

′

θ

) 在策略 πθ 处的梯度和 Actor-Critic 中一样。

g = ∇θLπθ

(π

′

θ

)|θ = Eτ∼πθ





X∞

t=0

γ

t ∇θπ

′

θ

(At| St)

πθ(At| St)

A

πθ

(St, At)















θ

(5.27)

= Eτ∼πθ





X∞

t=0

γ

t∇θ log πθ(At| St)











θ

A

πθ

(St, At)



 . (5.28)

156

5.8 近端策略优化

此外，让 H 表示 Es∼ρπθ



DKL(πθ∥π

′

θ

)



的 Hessian 矩阵，那么，TRPO 在当前的 πθ 求解如下

优化问题。

θ

′ = arg max

θ′

g

⊤(θ

′ − θ) (5.29)

s.t. (θ

′ − θ)

⊤H(θ

′ − θ) ⩽ δ.

易见这个问题的解析解存在：

θ

′ = θ +

s

2δ

g⊤H−1g

H−1

g. (5.30)

实际中，我们使用共轭梯度算法来近似 H−1g5。我们选择合适的步长来保证满足样本上的

KL 散度约束。最后，价值函数的学习通过最小化 MSE 误差达到。基于论文 (Schulman et al., 2015)

的完整的 TRPO 算法在算法 5.22 中。

注 5.3 负值 Hessian 矩阵 −H 也被称为 Fisher 信息矩阵。事实上，在批优化中，将 Fisher 信息矩

阵应用到梯度下降算法中已经有不少的研究，被称为自然梯度（Nature Gradient）下降。这个方

法的一个好处是，它对于再参数化是不变的，也即，不管函数参数化的方法是什么，该梯度保持

不变。想了解更多关于自然梯度的细节，请参考论文 (Amari, 1998)。

5.8 近端策略优化

上一节我们介绍了信赖域策略优化算法（TRPO）。TRPO 的实现较为复杂，而且计算自然

梯度的计算复杂度也较高。即使是用共轭梯度法来近似 H−1g，每一次更新参数也需要多步的

共轭梯度算法。在这一节中，我们介绍另一个策略梯度方法，即近端策略优化（Proximal Policy

Optimization, PPO）。PPO 用一个更简单有效的方法来强制 πθ 和 π

′

θ 相似 (Schulman et al., 2017)。

回顾 TRPO 中的优化问题式 (5.26)：

max

π

′

θ

Lπθ

(π

′

θ

) (5.35)

s.t. Es∼ρπθ



DKL(πθ∥π

′

θ

)



⩽ δ. (5.36)

与其优化一个带约束的优化问题，PPO 直接优化它的正则化版本。

max

π

′

θ

Lπθ

(π

′

θ

) − λEs∼ρπθ



DKL(πθ∥π

′

θ

)



. (5.37)

5一般来讲，计算 H−1 需要计算复杂度 O(N3

)。这在实际应用中一般代价十分昂贵，因为这里的 N 是模型参数的个

数。

157

第 5 章 策略梯度

算法 5.22 TRPO

超参数: KL-散度上限 δ、回溯系数 α、最大回溯步数 K。

输入: 回放缓存 Dk、初始策略函数参数 θ0、初始价值函数参数 ϕ0。

for episode = 0, 1, 2, · · · do

在环境中执行策略 πk = π(θk) 并保存轨迹集 Dk = {τi}。

计算将得到的奖励 Gˆ

t。

基于当前的价值函数 Vϕk 计算优势函数估计 Aˆ

t （使用任何估计优势的方法）。

估计策略梯度

gˆk =

1

|Dk|

X

τ∈Dk

X

T

t=0

∇θ log πθ(At|St)





θk

Aˆ

t (5.31)

使用共轭梯度算法计算

xˆk ≈ Hˆ −1

k

gˆk (5.32)

这里 Hˆ

k 是样本平均 KL 散度的 Hessian 矩阵。

通过回溯线搜索更新策略：

θk+1 = θk + α

j

s

2δ

xˆ

T

k Hˆ

kxˆk

xˆk (5.33)

这里 j 是 {0, 1, 2, · · · K} 中提高样本损失并且满足样本 KL 散度约束的最小值。

通过使用梯度下降的算法最小化均方误差来拟合价值函数：

ϕk+1 = arg min

ϕ

1

|Dk|T

X

τ∈Dk

X

T

t=0



Vϕ(St) − Gˆ

t

2

(5.34)

end for

这里 λ 是正则化系数。对于式 (5.26) 每一个 δ 值，都有一个相对应的 λ 使得两个优化问题有相同

的解。然而，λ 的值依赖于 πθ。基于此，在式 (5.37) 使用一个适应性的 λ 更合理。在 PPO 中，我们

通过检验 KL 散度的值来决定 λ 的值应该增大还是减小。这个版本的 PPO 算法称为 PPO-Penalty。

这个版本的实现如算法 5.23 所示 (Heess et al., 2017; Schulman et al., 2017)。

另一个方法是直接剪断用于策略梯度的目标函数，从而得到更保守的更新。让 ℓt(θ

′

) 表示两

个策略的比值 π

′

θ

(At|St)

πθ(At|St)。经验表明，下述目标函数可以让策略梯度方法有稳定的学习性能：

L

PPO-Clip(π

′

θ

) = Eπθ

h

min

ℓt(θ

′

)A

πθ

(St, At), clip(ℓt(θ

′

), 1 − ϵ, 1 + ϵ)A

πθ

(St, At)



i

. (5.38)

这里 clip(x, 1−ϵ, 1+ϵ) 将 x 截断在 [1−ϵ, 1+ϵ] 中。这个版本的算法被称为 PPO-Clip，如算法 5.24

所示 (Schulman et al., 2017)。更具体，PPO-Clip 先将 ℓt(θ

′

) 截断在 [1 − ϵ, 1 + ϵ] 中来保证 π

′

θ 和 πθ

15

5.9 使用 Kronecker 因子化信赖域的 Actor-Critic

算法 5.23 PPO-Penalty

超参数: 奖励折扣因子 γ，KL 散度惩罚系数 λ，适应性参数 a = 1.5, b = 2, 子迭代次数 M, B。

输入: 初始策略函数参数 θ、初始价值函数参数 ϕ。

for k = 0, 1, 2, · · · do

执行 T 步策略 πθ，保存 {St, At, Rt}。

估计优势函数 Aˆ

t =

P

t′>t γ

t

′−tRt

′ − Vϕ(St)。

πold ← πθ

for m ∈ {1, · · · , M} do

JPPO(θ) = PT

t=1

πθ(At|St)

πold(At|St)Aˆ

t − λEˆ

t



DKL(πold(·|St)∥πθ(·|St))

使用梯度算法基于 JPPO(θ) 更新策略函数参数 θ。

end for

for b ∈ {1, · · · , B} do

L(ϕ) = −

PT

t=1 P

t

′>t γ

t

′−tRt

′ − Vϕ(St)

2

使用梯度算法基于 L(ϕ) 更新价值函数参数 ϕ。

end for

计算 d = Eˆ

t



DKL(πold(·|St)∥πθ(·|St))

if d < dtarget/a then

λ ← λ/b

else if d > dtarget × a then

λ ← λ × b

end if

end for

相似。最后，取截断的目标函数和未截断的目标函数中较小的一方作为学习的最终目标函数。所

以，PPO-Clip 可以理解为在最大化目标函数的同时将从 πθ 到 π

′

θ 的更新保持在可控范围内。

5.9 使用 Kronecker 因子化信赖域的 Actor-Critic

使用 Kronecker 因子化信赖域的 Actor-Critic（Actor Critic using Kronecker-factored Trust Region，

ACKTR）(Wu et al., 2017) 是降低 TRPO 计算负担的另一个方法。ACKTR 的想法是通过 Kronecker

因子近似曲度方法（Kronecker-Factored Approximated Curvature，K-FAC）(Grosse et al., 2016; Martens

et al., 2015) 来计算自然梯度。在这一节中，我们介绍如何用 ACKTR 来学习 MLP 策略网络。

注意到

Es∼ρπold "

∂

2

∂

2θ

DKL(πold∥πθ)

#

(5.45)

= −Es∼ρπold "X

a

πold(a|s)

∂

2

∂

2θ

log πθ(a|s)

#

(5.46)

159

第 5 章 策略梯度

算法 5.24 PPO-Clip

超参数: 截断因子 ϵ，子迭代次数 M, B。

输入: 初始策略函数参数 θ、初始价值函数参数 ϕ。

for k = 0, 1, 2, · · · do

在环境中执行策略 πθk 并保存轨迹集 Dk = {τi}。

计算将得到的奖励 Gˆ

t。

基于当前的价值函数 Vϕk 计算优势函数 Aˆ

t（基于任何优势函数的估计方法）。

for m ∈ {1, · · · , M} do

ℓt(θ

′

) = πθ(At|St)

πθold (At|St)

(5.39)

采用 Adam 随机梯度上升算法最大化 PPO-Clip 的目标函数来更新策略：

θk+1 = arg max

θ

1

|Dk|T

X

τ∈Dk

X

T

t=0

min(ℓt(θ

′

)A

πθold (St, At), (5.40)

clip(ℓt(θ

′

), 1 − ϵ, 1 + ϵ)A

πθold (St, At)) (5.41)

end for

for b ∈ {1, · · · , B} do

采用梯度下降算法最小化均方误差来学习价值函数：

ϕk+1 = arg min

ϕ

1

|Dk|T

X

τ∈Dk

X

T

t=0



Vϕ(St) − Gˆ

t

2

end for

end for

= −Es∼ρπold



Ea∼πold "

∂

2

∂

2θ

log πθ(a|s)

#

 (5.47)

= Es∼ρπold 

Ea∼πold h

∇θ log πθ(a|s)

 ∇θ log πθ(a|s)

⊤

i



. (5.48)

在 TRPO 中，我们需要使用多步的共轭梯度方法来近似 H−1g。在 ACKTR 中，我们用一个分块

对角矩阵来来近似 H−1。矩阵的每一块对应神经网络每一层的 Fisher 信息矩阵。假设网络的第 ℓ

层为 xout = Wℓxin。这里 Wℓ 的维度为 dout × din。我们来介绍 ACKTR 分解的想法。注意到这一

层的梯度 ∇WℓL 是 (∇xoutL) 和 xin 的外积 (∇xoutL)x

⊤

in。所以



∇θ log πθ(a|s)

 ∇θ log πθ(a|s)

⊤

= xinx

⊤

in ⊗ (∇xoutL)(∇xoutL)

⊤, (5.49)



5.10 策略梯度代码例子

这里 ⊗ 是 Kronecker 乘积。进一步



∇θ log πθ(a|s)

 ∇θ log πθ(a|s)

⊤

−1

g (5.50)

=



xinx

⊤

in ⊗ (∇xoutL)(∇xoutL)

⊤

−1

g (5.51)

=



xinx

⊤

in−1

⊗



(∇xoutL)(∇xoutL)

⊤

−1



g (5.52)

所以，与其对一个 (dindout) × (dindout) 的矩阵求逆，从而需要 O(d

3

ind

3

out) 计算复杂度，ACKTR 只

需要对两个维度为 din × din 和 dout × dout 的矩阵求逆，从而计算复杂度只有 O(d

3

in + d

3

out)。

ACKTR 算法的实现如算法 5.25 所示。ACKTR 算法也可以被用于学习价值网络。感兴趣的

读者可以参考论文 (Wu et al., 2017) 了解更多的细节，我们这里不做详细解释。

算法 5.25 ACKTR

超参数: 步长 ηmax、KL-散度上限 δ。

输入: 空回放缓存 D、初始策略函数参数 θ0、初始价值函数参数 ϕ0

for k = 0, 1, 2, · · · 。 do

在环境中执行策略 πk = π(θk) 并保存轨迹集 Dk = {τi

|i = 0, 1, · · · }。

计算累积奖励 Gt。

基于当前的价值函数 Vϕk 计算优势函数 Aˆ

t（基于任何优势函数的估计方法）。

估计策略梯度。

gˆk =

1

|Dk|

X

τ∈Dk

X

T

t=0

∇θ log πθ(At|St)





θk

Aˆ

t (5.42)

for l = 0, 1, 2, · · · do

vec(∆θ

l

k

) = vec(A−1

l ∇θ

l

k

gˆkS

−1

l

)

这里 Al = E[ala

T

l

], Sl = E[(∇slgˆk)(∇slgˆk)

T]（Al

,Sl 通过计算片段的滚动平均值所得），

al 是第 l 层的输入激活向量，sl = Wlal，vec(·) 是把矩阵变换成一维向量的向量化变换。

end for

由 K-FAC 近似自然梯度来更新策略：

θk+1 = θk + ηk∆θk (5.43)

这里 ηk = min(ηmax,

q 2δ

θ

T

k Hˆ kθk

)，Hˆ l

k = Al ⊗ Sl。

采用 Gauss-Newton 二阶梯度下降方法（并使用 K-FAC 近似）最小化均方误差来学习价值函

数：

ϕk+1 = arg min

ϕ

1

|Dk|T

X

τ∈Dk

X

T

t=0



Vϕ(St) − Gt

2

(5.44)

end for



第 5 章 策略梯度

5.10 策略梯度代码例子

在前几节中，我们在理论角度介绍了几个基于策略梯度算法的伪代码，介绍的内容包括 REINFORCE（初版策略梯度）、Actor-Critic（AC）、同步优势 Actor-Critic（A2C）、异步优势 Actor-Critic

（A3C）、信赖域策略优化（TRPO）、近端策略优化（PPO）、使用 Kronecker 因子化信赖域的 Actor

Critic（ACKTR）。在本节中，我们将提供以上部分算法的 Python 代码例子。例子中以 OpenAI Gym

作为游戏环境。我们会先简单地介绍一下在例子中用到的环境，之后详细介绍各算法的实现。虽

然本章中介绍的多数算法都能应用于离散和连续的环境，但在实现中对于离散和连续环境的处理

有一些不同。这里我们提供的例子只是作为演示，只能应用在同一种动作空间的特定环境中。不

过读者可以通过简单地修改就能使代码应用于不同动作空间的其他环境中。完整代码在 GitHub

库中6，例子参考并改编自许多开源资料，感兴趣的读者可以参考各代码简介注释中 Reference 部

分所提及的内容进行扩展学习。

5.10.1 相关的 Gym 环境

在以下几节中提供例子的环境都基于 OpenAI Gym 环境。这些环境可以被分为离散动作空间

的环境和连续动作空间的环境。

import gym

env = gym.make(’Pong-V0’)

print(env.action_space)

上述代码建立了一个 ID 为 Pong-V0 的环境，并且打印出了它的动作空间。将 Pong-V0 这个 ID 换

成其他诸如 CartPole-V1 或者 Pendulum-V0 的 ID 可以建立相应的环境。

以下几节中的代码会用到一些开源库。这里通过如下代码引入它们。

import numpy as np

import tensorflow as tf

import tensorflow_probability as tfp

import tensorlayer as tl

...

离散动作空间环境：Pong 与 CartPole

这里将介绍两个 OpenAI Gym 中使用离散动作空间的游戏：Pong 和 CartPole。

6链接见读者服务

162

5.10 策略梯度代码例子

Pong

在 Pong 游戏中（如图 5.3 所示），我们控制绿色的板子上下移动来弹球。这里使用了 Pong-V0

版本。在这个版本中，状态空间是一个 RGB 图像向量，形状为 (210, 160, 3)。需要输入的动作是

一个在 0,1,2,3,4,5 中的整数，分别对应如下动作：0 空动作，1 开火，2 右，3 左，4 右 + 开火，5

左 + 开火。

图 5.3 Pong

CartPole

CartPole（如图 5.4 所示）是一个经典的倒立摆环境。我们通过控制小车进行左右移动，来使

杆子保持直立。在 CartPole-V0 环境中，观测空间是一个 4 维向量，分别表示小车的速度、小车

的位置、杆子的角度、杆子顶端的速度。需要输入的动作是一个为 0 或者 1 的整数，分别控制小

车左移和右移。

图 5.4 CartPole

连续动作空间环境：BipedalWalker-V2 与 Pendulum-V0

本节中，我们将介绍使用连续动作空间的环境：BipedalWalker-V2 和 Pendulum-V0。

163

第 5 章 策略梯度

BipedalWalker-V2

BipedalWalker-V2 是一个双足机器人仿真环境（如图 5.5 所示）。在环境中，我们要控制机器

人在相对平坦的地面上行走，并最终到达目的地。其状态空间是一个 24 维向量，分别表示速度、

角度信息，以及前方视野情况（详见表 5.1）。环境的动作空间是一个 4 维的连续动作空间，分别

控制机器人的 2 个膝关节、2 个臀关节，一共 4 个关节进行旋转。

图 5.5 BipedalWalker-V2

表 5.1 BipedalWalker-V2 各维度状态意义简介

索引 简介 索引 简介

0 壳体角度 8 1 号腿触地状态

1 壳体角速度 9 2 号臀关节角度

2 壳体 x 方向速度 10 2 号臀关节速度

3 壳体 y 方向速度 11 2 号膝关节角度

4 1 号臀关节角度 12 2 号膝关节速度

5 1 号臀关节速度 13 2 号腿触地状态

6 1 号膝关节角度 14–23 10 位前方雷达测距值

7 1 号膝关节速度

Pendulum-V0

Pendulum-V0 也是一个经典的倒立摆环境（如图 5.6 所示）。在环境中，我们需要控制杆子旋

转来让其直立。环境的状态空间是一个 3 维向量，分别代表 cos(θ)、sin(θ) 和 ∆(θ)。其中 θ 是杆

子和垂直向上方向的角度。环境的动作是一维的动作，来控制杆子的旋转力矩。

164

5.10 策略梯度代码例子

图 5.6 Pendulum-V0

值得注意的是，该环境中没有终止状态。这里的意思是，必须人为设置游戏的结束。在默认

情况下，环境的最大运行步长被限制为 200 步。当运行超过 200 步时，step() 函数返回的 Done

变量将为 True。由于有这个限制，当我们每个回合片段运行超过 200 步时，代码逻辑会因为收到

done 信号而退出该回合。通过如下代码可以移除这个限制。

import gym

env = gym.make(’Pendulum-V0’)

env = env.unwrapped # 解除最大步长的限制

5.10.2 REINFORCE: Atari Pong 和 CartPole-V0

Pong

开始之前，我们需要准备一下环境、模型、优化器，并初始化一些之后会用上的变量。

env = gym.make("Pong-V0") # 创建环境

observation = env.reset() # 重置环境

prev_x = None

running_reward = None

reward_sum = 0

episode_number = 0

# 准备收集数据

xs, ys, rs = [], [], []

epx, epy, epr = [], [], []

model = get_model([None, D]) # 创建模型

train_weights = model.trainable_weights

optimizer = tf.optimizers.RMSprop(lr=learning_rate, decay=decay_rate) # 创建优化器

165

第 5 章 策略梯度

model.train() # 设置模型为训练模式（防止模型被加上 DropOut）

start_time = time.time()

game_number = 0

在完成准备工作之后，就可以运行主循环了。首先，我们需要对观测数据进行预处理，并将

处理后的数据传递给变量 x。在将 x“喂”入网络之后，我们将从网络得到每个动作的执行概率。

为了简化难度，在这里只用到了 3 个动作：空动作、上、下。在 REINFORCE 算法中，使用

了 Softmax 函数输出动作概率，最后通过概率选择动作。

while True:

if render:

env.render()

cur_x = prepro(observation)

x = cur_x - prev_x if prev_x is not None else np.zeros(D, dtype=np.float32)

x = x.reshape(1, D)

prev_x = cur_x

_prob = model(x)

prob = tf.nn.softmax(_prob)

# 动作 1: 空动作 2: 上 3: 下

action = tl.rein.choice_action_by_probs(prob[0].numpy(), [1, 2, 3])

现在基于当前状态选出了一个动作。接下来要用该动作和环境进行交互。环境根据当前收到

的动作执行到下一步，并返回观测数据、奖励、结束状态和额外信息（对应代码中的变量 _）。我

们将这些数据存储起来用于之后的更新。

observation, reward, done, _ = env.step(action)

reward_sum += reward

xs.append(x) # 一个片段内的所有观测数据

ys.append(action - 1) # 一个片段内的所有伪标签（由于动作从 1 开始，所以这里减 1）

rs.append(reward) # 一个片段内的所有奖励

如果 step() 返回的结束状态为 True，说明当前片段结束。我们可以重置环境并开始一个

166

5.10 策略梯度代码例子

新的片段。但在那之前，我们需要将刚刚采集的本片段的数据进行处理，之后存入跨片段数据列

表中。

if done:

episode_number += 1

game_number = 0

epx.extend(xs)

epy.extend(ys)

disR = tl.rein.discount_episode_rewards(rs, gamma)

disR -= np.mean(disR)

disR /= np.std(disR)

epr.extend(disR)

xs, ys, rs = [], [], []

智能体在进行了很多局游戏，并收集了足够的数据之后，就可以开始更新了。我们使用交叉

熵损失和梯度下降方法来计算各参数的梯度，之后将梯度应用在相应的参数上，并结束更新。

if episode_number

print(’batch over...... updating parameters......’)

with tf.GradientTape() as tape:

_prob = model(epx)

_loss = tl.rein.cross_entropy_reward_loss(_prob, epy, disR)

grad = tape.gradient(_loss, train_weights)

optimizer.apply_gradients(zip(grad, train_weights))

epx, epy, epr = [], [], []

以上内容描述了主要工作，之后的代码主要用于显示训练相关数据，以便更好地观察训练走

势。我们可以使用滑动平均来计算每个片段的运行奖励，以降低数据抖动的程度，方便观察趋势。

最后，做完这些内容后别忘了重置环境，因为此时当前片段已经结束了。

# if episode_number

# tl.files.save_npz(network.all_params, name=model_file_name + ’.npz’)

running_reward = reward_sum if running_reward is None else running_reward * 0.99

+ reward_sum * 0.01

print(’resetting env. episode reward total was {}. running mean:

{}’.format(reward_sum, running_reward))

167

第 5 章 策略梯度

reward_sum = 0

observation = env.reset()

prev_x = None

if reward != 0:

print(

( ’episode

(episode_number, game_number, time.time() - start_time, reward)

), (’’ if reward == -1 else ’ !!!!!!!!’)

)

start_time = time.time()

game_number += 1

CartPole

这个例子中，算法和 Pong 的一样。我们可以考虑将整个算法放入一个类中，并将各部分代

码写入对应的函数。这样可以使得代码更为简洁易读。PolicyGradient 类的结构如下所示：

class PolicyGradient:

def __init__(self, state_dim, action_num, learning_rate=0.02, gamma=0.99):

# 类初始化。创建模型、优化器和需要的变量

......

def get_action(self, s, greedy=False): # 基于动作分布选择动作

......

def store_transition(self, s, a, r): # 存储从环境中采样的交互数据

......

def learn(self): # 使用存储的数据进行学习和更新

......

def _discount_and_norm_rewards(self): # 计算折扣化回报并进行标准化处理

......

def save(self): # 存储模型

......

def load(self): # 载入模型

......

初始化函数先后创建了一些变量、模型并选择 Adam 作为策略优化器。在代码中，我们可以

看出这里的策略网络只有一层隐藏层。

168

5.10 策略梯度代码例子

def __init__(self, state_dim, action_num, learning_rate=0.02, gamma=0.99):

self.gamma = gamma

self.state_buffer, self.action_buffer, self.reward_buffer = [], [], []

input_layer = tl.layers.Input([None, state_dim], tf.float32)

layer = tl.layers.Dense(

n_units=30, act=tf.nn.tanh, W_init=tf.random_normal_initializer(mean=0,

stddev=0.3),

b_init=tf.constant_initializer(0.1)

)(input_layer)

all_act = tl.layers.Dense(

n_units=action_num, act=None, W_init=tf.random_normal_initializer(mean=0,

stddev=0.3),

b_init=tf.constant_initializer(0.1)

)(layer)

self.model = tl.models.Model(inputs=input_layer, outputs=all_act)

self.model.train()

self.optimizer = tf.optimizers.Adam(learning_rate)

在初始化策略网络之后，我们可以通过 get_action() 函数计算某状态下各动作的概率。通

过设置’greedy=True’，可以直接输出概率最高的动作。

def get_action(self, s, greedy=False):

_logits = self.model(np.array([s], np.float32))

_probs = tf.nn.softmax(_logits).numpy()

if greedy:

return np.argmax(_probs.ravel())

return tl.rein.choice_action_by_probs(_probs.ravel())

但此时，我们选择的动作可能并不好。只有通过不断学习之后，网络才能做出越来越好的判

断。每次的学习过程由 learn() 函数完成，这部分函数的代码基本也和 Pong 例子中一样。我们

使用标准化后的折扣化奖励和交叉熵损失来更新模型。在每次更新后，学过的转移数据将被丢弃。

def learn(self):

# 计算标准化后的折扣化奖励

discounted_ep_rs_norm = self._discount_and_norm_rewards()

169

第 5 章 策略梯度

with tf.GradientTape() as tape:

_logits = self.model(np.vstack(self.ep_obs))

neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=_logits,

labels=np.array(self.ep_as))

loss = tf.reduce_mean(neg_log_prob * discounted_ep_rs_norm)

grad = tape.gradient(loss, self.model.trainable_weights)

self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))

self.ep_obs, self.ep_as, self.ep_rs = [], [], [] # 清空片段数据

return discounted_ep_rs_norm

learn() 函数需要使用智能体与环境交互得到的采样数据。因此我们需要使用 store_transition() 来存储交互过程中的每个状态、动作和奖励。

def store_transition(self, s, a, r):

self.ep_obs.append(np.array([s], np.float32))

self.ep_as.append(a)

self.ep_rs.append(r)

策略梯度算法使用蒙特卡罗方法。因此，我们需要计算折扣化回报，并对回报进行标准化，

也有助于学习。

def _discount_and_norm_rewards(self):

# 计算折扣化片段奖励

discounted_ep_rs = np.zeros_like(self.ep_rs)

running_add = 0

for t in reversed(range(0, len(self.ep_rs))):

running_add = running_add * self.gamma + self.ep_rs[t]

discounted_ep_rs[t] = running_add

# 标准化片段奖励

discounted_ep_rs -= np.mean(discounted_ep_rs)

discounted_ep_rs /= np.std(discounted_ep_rs)

return discounted_ep_rs

和 Pong 的代码一样，我们先准备好环境和算法。在创建好环境之后，我们产生一个名为 agent

的 PolicyGradient 类的实例。

env = gym.make(ENV_ID).unwrapped

170

5.10 策略梯度代码例子

# 通过设置随机种子，可以复现一些运行情况

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

env.seed(RANDOM_SEED)

agent = PolicyGradient(

action_num=env.action_space.n,

state_dim=env.observation_space.shape[0],

)

t0 = time.time()

在训练模式中，我们使用模型输出的动作来和环境进行交互，之后存储转移数据并在每个片

段更新策略。为了简化代码，智能体将在每局结束时直接进行更新。

if args.train:

all_episode_reward = []

for episode in range(TRAIN_EPISODES):

# 重置环境

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS): # 在一个片段中

if RENDER:

env.render()

# 选择动作

action = agent.get_action(state)

# 与环境交互

next_state, reward, done, info = env.step(action)

# 存储转移数据

agent.store_transition(state, action, reward)

state = next_state

episode_reward += reward

# 如果环境返回 done 为 True，则跳出循环

if done:

break

# 在每局游戏结束时进行更新

agent.learn()

print(

171

第 5 章 策略梯度

’Training | Episode: {}/{} | Episode Reward: {:.0f} | Running Time:

{:.4f}’.format(

episode + 1, TRAIN_EPISODES, episode_reward,

time.time() - t0))

我们可以在每局游戏结束后的部分增加一些代码，以便更好地显示训练过程。我们显示每个

回合的总奖励和通过滑动平均计算的运行奖励。之后可以绘制运行奖励以便更好地观察训练趋

势。最后，存储训练好的模型。

agent.save()

plt.plot(all_episode_reward)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’pg.png’))

如果我们使用测试模式，则过程更为简单，只需要载入预训练的模型，再用它和环境进行交

互即可。

if args.test:

# 进行测试

agent.load()

for episode in range(TEST_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS):

env.render()

state, reward, done, info = env.step(agent.get_action(state, True))

episode_reward += reward

if done:

break

print(

’Testing | Episode: {}/{} | Episode Reward: {:.0f} | Running Time:

{:.4f}’.format(

episode + 1, TEST_EPISODES, episode_reward,

time.time() - t0))

172

5.10 策略梯度代码例子

5.10.3 AC: CartPole-V0

Actor-Critic 算法通过 TD 方法计算基准，能在每次和环境交互后立刻更新策略，和 MC 非常

不同。

在 Actor-Critic 算法中，我们建立了 2 个类：Actor 和 Critic，其结构如下所示。

class Actor(object):

def __init__(self, state_dim, action_num, lr=0.001): # 类初始化。创建模型、优化器及其

# 所需变量

...

def learn(self, state, action, td_error): # 更新模型

...

def get_action(self, state, greedy=False): # 通过概率分布或者贪心方法选择动作

...

def save(self): # 存储训练模型

...

def load(self): # 载入训练模型

...

class Critic(object):

def __init__(self, state_dim, lr=0.01): # 类初始化。创建模型、优化器及其所需变量

...

def learn(self, state, reward, state_): # 更新模型

...

def save(self): # 存储训练模型

...

def load(self): # 载入训练模型

...

Actor 类的部分和策略梯度算法很像。唯一的区别是 learn() 函数使用了 TD 误差作为优

势估计值进行更新，而不是使用折扣化奖励。

def learn(self, state, action, td_error):

with tf.GradientTape() as tape:

_logits = self.model(np.array([state]))

_exp_v = tl.rein.cross_entropy_reward_loss(logits=_logits, actions=[action],

rewards=td_error[0])

grad = tape.gradient(_exp_v, self.model.trainable_weights)

self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))

173

第 5 章 策略梯度

return _exp_v

和 PG 算法不同，AC 算法有一个带有价值网络的批判者，它能估计每个状态的价值。所以

它初始化函数十分清晰，只需要创建网络和优化器即可。

class Critic(object):

def __init__(self, state_dim, lr=0.01):

input_layer = tl.layers.Input([1, state_dim], name=’state’)

layer = tl.layers.Dense(

n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01),

name=’hidden’

)(input_layer)

layer = tl.layers.Dense(n_units=1, act=None, name=’value’)(layer)

self.model = tl.models.Model(inputs=input_layer, outputs=layer, name="Critic")

self.model.train()

self.optimizer = tf.optimizers.Adam(lr)

在初始化函数之后，我们有了一个价值网络。下一步就是建立 learn() 函数。learn() 函

数任务非常简单，通过公式 δ = R + γV (s

′

) − V (s) 计算 TD 误差 δ，之后将 TD 误差作为优势估

计来计算损失。

def learn(self, state, reward, state_, done):

d = 0 if done else 1

v_ = self.model(np.array([state_]))

with tf.GradientTape() as tape:

v = self.model(np.array([state]))

# TD_error = r + d * lambda * V(newS) - V(S)

td_error = reward + d * LAM * v_ - v

loss = tf.square(td_error)

grad = tape.gradient(loss, self.model.trainable_weights)

self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))

return td_error

存储和载入函数与往常一样。我们也可以将网络参数存储为.npz 格式的文件。

def save(self): # 存储模型

if not os.path.exists(os.path.join(’model’, ’ac’)):

os.makedirs(os.path.join(’model’, ’ac’))

174

5.10 策略梯度代码例子

tl.files.save_npz(self.model.trainable_weights, name=os.path.join(’model’, ’ac’,

’model_critic.npz’))

def load(self): # 载入模型

tl.files.load_and_assign_npz(name=os.path.join(’model’, ’ac’,

’model_critic.npz’), network=self.model)

训练循环的代码和之前的代码非常相似。唯一的不同是更新的时机不同。使用 TD 误差的情

况下，我们可以在每步进行更新。

if args.train:

all_episode_reward = []

for episode in range(TRAIN_EPISODES):

# 重置环境

state = env.reset().astype(np.float32)

step = 0 # 片段中的步数

episode_reward = 0 # 整个片段的奖励

while True:

if RENDER: env.render()

# 选择动作，并与环境交互

action = actor.get_action(state)

state_new, reward, done, info = env.step(action)

state_new = state_new.astype(np.float32)

if done: reward = -20 # reward shaping trick

episode_reward += reward

# 在和环境交互后，更新模型

td_error = critic.learn(state, reward, state_new, done)

actor.learn(state, action, td_error)

state = state_new

step += 1

# 一直运行，直到环境返回 done 为 True，或者达到最大步数限制

if done or step >= MAX_STEPS:

break

175

第 5 章 策略梯度

显示信息、绘图和测试部分的代码和策略梯度的代码一样，这里就不再赘述了。

5.10.4 A3C: BipedalWalker-v2

在这里的 A3C 实现中，有个全局的 AC 和许多 Worker。全局 AC 的功能是使用 Worker 节点

采集的数据更新网络。每个 Worker 节点都有自己的 AC 网络，用来和环境交互。Worker 节点并

将采集的数据传给全局 AC，之后从全局 AC 获取最新的网络参数，再替换自己本地的参数并接

着采集数据。Worker 类的结构如下所示：

class Worker(object):

def __init__(self, name): # 初始化

...

def work(self, globalAC): # 主要的功能函数

...

如上所说，每个 Worker 节点都有自己的行动者网络和批判者网络。所以在初始化函数中，我

们通过实例化 ACNet 类来创建模型。

class Worker(object):

def __init__(self, name):

self.env = gym.make(GAME)

self.name = name

self.AC = ACNet(name)

work() 函数是 Worker 类的主要函数。它和之前代码中的主循环相似，但在更新的地方有

所不同。和往常一样，这里循环的主要内容是从智能体取得动作，并与环境交互。

def work(self, globalAC):

global GLOBAL_RUNNING_R, GLOBAL_EP

total_step = 1

buffer_s, buffer_a, buffer_r = [], [], []

while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:

# 重置环境

s = self.env.reset()

ep_r = 0

while True:

176

5.10 策略梯度代码例子

# 在训练过程中，将 Worker0 可视化

if self.name == ’Worker_0’ and total_step

self.env.render()

# 选择动作并与环境交互

s = s.astype(’float32’)

a = self.AC.choose_action(s)

s_, r, done, _info = self.env.step(a)

s_ = s_.astype(’float32’)

# 将机器人摔倒的奖励设置为 -2，代替原来的 -100

if r == -100: r = -2

ep_r += r

# 存储转移数据

buffer_s.append(s)

buffer_a.append(a)

buffer_r.append(r)

当智能体采集足够的数据时，将开始更新全局网络。在那之后，本地网络的参数将被替换为

更新后的最新全局网络参数。

if total_step

if done:

v_s_ = 0 # 终止情况下

else:

v_s_ = self.AC.critic(s_[np.newaxis, :])[0,0] # 修正数据维度

# 折扣化奖励

buffer_v_target = []

for r in buffer_r[::-1]:

v_s_ = r + GAMMA * v_s_

buffer_v_target.append(v_s_)

buffer_v_target.reverse()

buffer_s = tf.convert_to_tensor(np.vstack(buffer_s))

buffer_a = tf.convert_to_tensor(np.vstack(buffer_a))

buffer_v_target = tf.convert_to_tensor(np.vstack(buffer_v_target).astype(’float32’))

177

第 5 章 策略梯度

# 更新全局网络

self.AC.update_global(buffer_s, buffer_a, buffer_v_target.astype(’float32’),

globalAC)

buffer_s, buffer_a, buffer_r = [], [], []

# 同步本地网络

self.AC.pull_global(globalAC)

s = s_

total_step += 1

if done:

if len(GLOBAL_RUNNING_R) == 0: # 存储运行过程中的奖励

GLOBAL_RUNNING_R.append(ep_r)

else: # 使用滑动平均

GLOBAL_RUNNING_R.append(0.95 * GLOBAL_RUNNING_R[-1] + 0.05 * ep_r)

print(’Training | {}, Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’\

.format(self.name, GLOBAL_EP, MAX_GLOBAL_EP, ep_r, time.time()-T0 ))

GLOBAL_EP += 1

break

在上述代码中用到的 ACNet 类包含行动者和批判者。它的结构如下所示：

class ACNet(object):

def __init__(self, scope): # 初始化

...

def update_global(self, buffer_s, buffer_a, buffer_v_target, globalAC):

# 更新全局网络

...

def pull_global(self, globalAC): # 本地网络同步全局网络

...

def get_action(self, s, greedy=False): # 本地网络采集动作

...

def save(self): # 存储训练模型

...

def load(self): # 载入训练模型

178

5.10 策略梯度代码例子

...

update_global() 函数是其中最重要的函数之一，从如下代码可以看出，使用了采样数据来

计算梯度，但是将梯度应用到全局网络，在那之后，再从全局网络更新数据，并继续循环。在这

个模式下，可以异步更新多个 Worker 节点。

def update_global(

self, buffer_s, buffer_a, buffer_v_target, globalAC

): # 通过采样更新全局 AC 网络

# 更新全局批判者

with tf.GradientTape() as tape:

self.v = self.critic(buffer_s)

self.v_target = buffer_v_target

td = tf.subtract(self.v_target, self.v, name=’TD_error’)

self.c_loss = tf.reduce_mean(tf.square(td))

self.c_grads = tape.gradient(self.c_loss, self.critic.trainable_weights)

OPT_C.apply_gradients(zip(self.c_grads, globalAC.critic.trainable_weights))

# 将本地梯度应用在全局网络上

# 更新全局行动者

with tf.GradientTape() as tape:

self.mu, self.sigma = self.actor(buffer_s)

self.test = self.sigma[0]

self.mu, self.sigma = self.mu * A_BOUND[1], self.sigma + 1e-5

normal_dist = tfd.Normal(self.mu, self.sigma) # tf2.0 中没有 tf.contrib

self.a_his = buffer_a

log_prob = normal_dist.log_prob(self.a_his)

exp_v = log_prob * td # td 在 critic 用过了，这里没有梯度

entropy = normal_dist.entropy() # 鼓励探索

self.exp_v = ENTROPY_BETA * entropy + exp_v

self.a_loss = tf.reduce_mean(-self.exp_v)

self.a_grads = tape.gradient(self.a_loss, self.actor.trainable_weights)

OPT_A.apply_gradients(zip(self.a_grads, globalAC.actor.trainable_weights))

# 将本地梯度应用在全局网络上

return self.test # 返回测试用数据

更新本地网络的函数非常简单，只要将本地网络的参数替换为全局网络的参数即可。

def pull_global(self, globalAC): # 本地运行，从全局网络同步数据

179

第 5 章 策略梯度

for l_p, g_p in zip(self.actor.trainable_weights,

globalAC.actor.trainable_weights):

l_p.assign(g_p)

for l_p, g_p in zip(self.critic.trainable_weights,

globalAC.critic.trainable_weights):

l_p.assign(g_p)

最后，准备工作都完成后，在主函数中逐一启动各个线程即可。

env = gym.make(GAME)

N_S = env.observation_space.shape[0]

N_A = env.action_space.shape[0]

A_BOUND = [env.action_space.low, env.action_space.high]

A_BOUND[0] = A_BOUND[0].reshape(1, N_A)

A_BOUND[1] = A_BOUND[1].reshape(1, N_A)

with tf.device("/cpu:0"):

GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE) # 这里的全局网络只用来存储参数

T0 = time.time()

if args.train:

with tf.device("/cpu:0"):

OPT_A = tf.optimizers.RMSprop(LR_A, name=’RMSPropA’)

OPT_C = tf.optimizers.RMSprop(LR_C, name=’RMSPropC’)

workers = []

for i in range(N_WORKERS):

i_name = "Worker_%i" %i # worker name

workers.append(Worker(i_name, GLOBAL_AC))

COORD = tf.train.Coordinator()

# 启动 TF 线程

worker_threads = []

for worker in workers:

# t = threading.Thread(target=worker.work)

job = lambda: worker.work(GLOBAL_AC)

t = threading.Thread(target=job)

180

5.10 策略梯度代码例子

t.start()

worker_threads.append(t)

COORD.join(worker_threads)

GLOBAL_AC.save()

plt.plot(GLOBAL_RUNNING_R)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’a3c.png’))

5.10.5 TRPO: Pendulum-V0

TRPO 以信赖域方法使用在 KL 散度约束下的最大更新步长。例子中也使用了通用优势估计

器（Generalized Advantage Estimator, GAE）。我们先看一下 GAE_Buffer 如何实现。

class GAE_Buffer:

def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95): # 初始化缓存

...

def store(self, obs, act, rew, val, logp, mean, log_std): # 存储数据

...

def finish_path(self, last_val=0): # 通过 GAE-Lambda 计算优势估计

...

def _discount_cumsum(self, x, discount): # 机选折扣化累积和

...

def is_full(self): # 查看缓存是否已满

...

def get(self): # 从缓存中取出数据

...

我们在初始化函数中建立之后要用到的变量。

class GAE_Buffer:

def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):

self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)

self.act_buf = np.zeros((size, act_dim), dtype=np.float32)

self.adv_buf = np.zeros(size, dtype=np.float32)

self.rew_buf = np.zeros(size, dtype=np.float32)

self.ret_buf = np.zeros(size, dtype=np.float32)

181

第 5 章 策略梯度

self.val_buf = np.zeros(size, dtype=np.float32)

self.logp_buf = np.zeros(size, dtype=np.float32)

self.mean_buf = np.zeros(size, dtype=np.float32)

self.log_std_buf = np.zeros(size, dtype=np.float32)

self.gamma, self.lam = gamma, lam

self.ptr, self.path_start_idx, self.max_size = 0, 0, size

在 store() 函数中，我们将数据存入对应的缓存中，再移动指针。

def store(self, obs, act, rew, val, logp, mean, log_std):

assert self.ptr < self.max_size # 确保有存储空间

self.obs_buf[self.ptr] = obs

self.act_buf[self.ptr] = act

self.rew_buf[self.ptr] = rew

self.val_buf[self.ptr] = val

self.logp_buf[self.ptr] = logp

self.mean_buf[self.ptr] = mean

self.log_std_buf[self.ptr] = log_std

self.ptr += 1

finish_path() 函数在每个轨迹的结尾或者一个回合结束时会被调用。它提取当前轨迹并计

算 GAE-Lambda 优势和价值函数会用到的累积回报。

def finish_path(self, last_val=0):

path_slice = slice(self.path_start_idx, self.ptr)

rews = np.append(self.rew_buf[path_slice], last_val)

vals = np.append(self.val_buf[path_slice], last_val)

# 下面两行计算了 GAE-Lambda 优势

deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]

self.adv_buf[path_slice] = self._discount_cumsum(deltas, self.gamma * self.lam)

# 下一行计算了折扣化奖励，它将作为价值函数的目标

self.ret_buf[path_slice] = self._discount_cumsum(rews, self.gamma)[:-1]

self.path_start_idx = self.ptr

在之前代码中用到的 _discount_cumsum() 函数如下所示。这里使用了 scipy（一个开源库）

的内建函数来实现。

182

5.10 策略梯度代码例子

def _discount_cumsum(self, x, discount):

return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]

is_full() 函数只是简单确认一下指针是否移动到底。

def is_full(self):

return self.ptr == self.max_size

当缓存满了的时候，我们将取出数据并重置指针。这里使用了优势标准化技术。

def get(self):

assert self.ptr == self.max_size # 取数据之前，缓存必须是满的

self.ptr, self.path_start_idx = 0, 0

# 下两行实现的是优势标准化技术

adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)

self.adv_buf = (self.adv_buf - adv_mean) / adv_std

return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf,

self.mean_buf, self.log_std_buf]

接下来我们将介绍 TRPO，其结构如下所示：

class TRPO:

def __init__(self, state_dim, action_dim, action_bound): # 创建网络、优化器及变量

...

def get_action(self, state, greedy=False): # 获取动作和其他变量

...

def pi_loss(self, states, actions, adv, old_log_prob): # 计算策略损失

...

def gradient(self, states, actions, adv, old_log_prob): # 计算策略网络梯度

...

def train_vf(self, states, rewards_to_go): # 训练价值网络

...

def kl(self, states, old_mean, old_log_std): # 计算 KL 散度

...

def _flat_concat(self, xs): # 展平变量

...

def get_pi_params(self): # 获取策略网络的参数

...

183

第 5 章 策略梯度

def set_pi_params(self, flat_params): # 设置策略网络的参数

...

def save(self): # 存储网络参数

...

def load(self): # 载入网络参数

...

def cg(self, Ax, b): # 共轭梯度算法

...

def hvp(self, states, old_mean, old_log_std, x): # Hessian 向量积（Hessian-vector

product）

...

def update(self): # 更新全部网络

...

def finish_path(self, done, next_state): # 结束一段轨迹

...

和往常一样，我们在初始化函数中先设置网络、优化器和其他变量。这里的动作分布是由一

个均值和一个标准差描述的高斯分布。策略网络只输出了每个动作维度的均值，所有动作共用一

个变量来作为对数标准差。

class TRPO:

def __init__(self, state_dim, action_dim, action_bound):

# critic

with tf.name_scope(’critic’):

layer = input_layer = tl.layers.Input([None, state_dim], tf.float32)

for d in HIDDEN_SIZES:

layer = tl.layers.Dense(d, tf.nn.relu)(layer)

v = tl.layers.Dense(1)(layer)

self.critic = tl.models.Model(input_layer, v)

self.critic.train()

# actor

with tf.name_scope(’actor’):

layer = input_layer = tl.layers.Input([None, state_dim], tf.float32)

for d in HIDDEN_SIZES:

layer = tl.layers.Dense(d, tf.nn.relu)(layer)

mean = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)

mean = tl.layers.Lambda(lambda x: x * action_bound)(mean)

log_std = tf.Variable(np.zeros(action_dim, dtype=np.float32))

184

5.10 策略梯度代码例子

self.actor = tl.models.Model(input_layer, mean)

self.actor.trainable_weights.append(log_std)

self.actor.log_std = log_std

self.actor.train()

self.buf = GAE_Buffer(state_dim, action_dim, BATCH_SIZE, GAMMA, LAM)

self.critic_optimizer = tf.optimizers.Adam(learning_rate=VF_LR)

self.action_bound = action_bound

有了网络，我们就可以通过如下函数取得对应状态下的动作。除此之外，我们需要计算一些

额外数据存入 GAE 缓存中。

def get_action(self, state, greedy=False):

state = np.array([state], np.float32)

mean = self.actor(state)

log_std = tf.convert_to_tensor(self.actor.log_std)

std = tf.exp(log_std)

std = tf.ones_like(mean) * std

pi = tfp.distributions.Normal(mean, std)

if greedy:

action = mean

else:

action = pi.sample()

action = np.clip(action, -self.action_bound, self.action_bound)

logp_pi = pi.log_prob(action)

value = self.critic(state)

return action[0], value, logp_pi, mean, log_std

如下代码显示了如何计算策略损失。我们先计算替代优势，这是一个描述当前策略在之前策

略采样的数据中表现如何的数据。之后使用负的替代优势作为子策略损失。

def pi_loss(self, states, actions, adv, old_log_prob):

mean = self.actor(states)

pi = tfp.distributions.Normal(mean, tf.exp(self.actor.log_std))

log_prob = pi.log_prob(actions)[:, 0]

ratio = tf.exp(log_prob - old_log_prob)

185

第 5 章 策略梯度

surr = tf.reduce_mean(ratio * adv)

return -surr

通过调用之前定义的 pi_loss() 函数，我们可以很简单地计算梯度。

def gradient(self, states, actions, adv, old_log_prob):

pi_params = self.actor.trainable_weights

with tf.GradientTape() as tape:

loss = self.pi_loss(states, actions, adv, old_log_prob)

grad = tape.gradient(loss, pi_params)

gradient = self._flat_concat(grad)

return gradient, loss

训练价值网络的方法如下所示。只要通过回归减少均方差即可拟合价值函数。

def train_vf(self, states, rewards_to_go):

with tf.GradientTape() as tape:

value = self.critic(states)

loss = tf.reduce_mean((rewards_to_go - value[:, 0]) ** 2)

grad = tape.gradient(loss, self.critic.trainable_weights)

self.critic_optimizer.apply_gradients(zip(grad, self.critic.trainable_weights))

计算 KL 散度的过程如下所示。我们先基于均值和标准差产生动作分布，然后计算两个分布

的 KL 散度。

def kl(self, states, old_mean, old_log_std):

old_mean = old_mean[:, np.newaxis]

old_log_std = old_log_std[:, np.newaxis]

old_std = tf.exp(old_log_std)

old_pi = tfp.distributions.Normal(old_mean, old_std)

mean = self.actor(states)

std = tf.exp(self.actor.log_std)*tf.ones_like(mean)

pi = tfp.distributions.Normal(mean, std)

kl = tfp.distributions.kl_divergence(pi, old_pi)

all_kls = tf.reduce_sum(kl, axis=1)

return tf.reduce_mean(all_kls)

186

5.10 策略梯度代码例子

在这个代码例子中，许多参数都使用 _flat_concat() 函数展平，这样能简化很多计算过程。

def _flat_concat(self, xs):

return tf.concat([tf.reshape(x, (-1,)) for x in xs], axis=0)

如下的 get_pi_params() 和 set_pi_params() 函数用于获得和设置行动者网络的参数。在

获取和设置参数的过程中需要进行一些简单的处理。

def get_pi_params(self):

pi_params = self.actor.trainable_weights

return self._flat_concat(pi_params)

def set_pi_params(self, flat_params):

pi_params = self.actor.trainable_weights

flat_size = lambda p: int(np.prod(p.shape.as_list())) # the ’int’ is important

for scalars

splits = tf.split(flat_params, [flat_size(p) for p in pi_params])

new_params = [tf.reshape(p_new, p.shape) for p, p_new in zip(pi_params, splits)]

return tf.group([p.assign(p_new) for p, p_new in zip(pi_params, new_params)])

存储和载入函数和之前一样。

def save(self):

path = os.path.join(’model’, ’trpo’)

if not os.path.exists(path):

os.makedirs(path)

tl.files.save_weights_to_hdf5(os.path.join(path, ’actor.hdf5’), self.actor)

tl.files.save_weights_to_hdf5(os.path.join(path, ’critic.hdf5’), self.critic)

def load(self):

path = os.path.join(’model’, ’trpo’)

tl.files.load_hdf5_to_weights_in_order(os.path.join(path, ’actor.hdf5’),

self.actor)

tl.files.load_hdf5_to_weights_in_order(os.path.join(path, ’critic.hdf5’),

self.critic)

如下代码实现的是共轭梯度算法7。使用这个函数可以不通过计算和存储整个矩阵来直接计

算矩阵向量积。

7链接见读者服务

187

第 5 章 策略梯度

def cg(self, Ax, b):

x = np.zeros_like(b)

r = copy.deepcopy(b) # 注意，这里应该是’b - Ax(x)’，但 x=0 时，Ax(x)=0。如果想热启

# 动可以进行修改

p = copy.deepcopy(r)

r_dot_old = np.dot(r, r)

for _ in range(CG_ITERS):

z = Ax(p)

alpha = r_dot_old / (np.dot(p, z) + EPS)

x += alpha * p

r -= alpha * z

r_dot_new = np.dot(r, r)

p = r + (r_dot_new / r_dot_old) * p

r_dot_old = r_dot_new

return x

如下代码显示了通过使用公式 Hx = ∇θ



∇θD¯KL(θ∥θk)

T

x



计算 Hessian 向量积的过程。

这里使用阻尼系数来改变计算 Hx → (αI + H)x 的过程，可以获得更好的数值稳定性。

def hvp(self, states, old_mean, old_log_std, x):

pi_params = self.actor.trainable_weights

with tf.GradientTape() as tape1:

with tf.GradientTape() as tape0:

d_kl = self.kl(states, old_mean, old_log_std)

g = self._flat_concat(tape0.gradient(d_kl, pi_params))

l = tf.reduce_sum(g * x)

hvp = self._flat_concat(tape1.gradient(l, pi_params))

if DAMPING_COEFF > 0:

hvp += DAMPING_COEFF * x

return hvp

有了如上准备，我们最后可以开始更新了。首先，通过 GAE 采集数据并计算梯度和损失。接

着我们使用共轭梯度算法来计算变量 x，它对应公式 xˆk ≈ Hˆ −1

k

gˆk 中的 xˆk。然后，我们计算公

式 θk+1 = θk + α

j

q 2δ

xˆT

k Hˆ kxˆk

xˆk 中的 q 2δ

xˆT

k Hˆ kxˆk

部分。之后，我们使用回溯线搜索来更新策略网

络。最后，通过 MES 损失更新价值网络。

def update(self):

18

5.10 策略梯度代码例子

states, actions, adv, rewards_to_go, logp_old_ph, old_mu, old_log_std =

self.buf.get()

g, pi_l_old = self.gradient(states, actions, adv, logp_old_ph)

Hx = lambda x: self.hvp(states, old_mu, old_log_std, x)

x = self.cg(Hx, g)

alpha = np.sqrt(2 * DELTA / (np.dot(x, Hx(x)) + EPS))

old_params = self.get_pi_params()

def set_and_eval(step):

params = old_params - alpha * x * step

self.set_pi_params(params)

d_kl = self.kl(states, old_mu, old_log_std)

loss = self.pi_loss(states, actions, adv, logp_old_ph)

return [d_kl, loss]

# 回溯线搜索，固定 KL 限制

for j in range(BACKTRACK_ITERS):

kl, pi_l_new = set_and_eval(step=BACKTRACK_COEFF ** j)

if kl <= DELTA and pi_l_new <= pi_l_old:

# 接受一步线搜索中更新的新参数

break

else:

# 线搜索失败，保持旧参数

set_and_eval(step=0.)

# 价值网络更新

for _ in range(TRAIN_V_ITERS):

self.train_vf(states, rewards_to_go)

这里在轨迹要被切断或者回合结束的时候，也会需要使用 finish_path() 函数。如果轨迹

由于智能体到达终止状态而结束，那么最后的价值将被设置为 0。

def finish_path(self, done, next_state):

if not done:

next_state = np.array([next_state], np.float32)

last_val = self.critic(next_state)

189

第 5 章 策略梯度

else:

last_val = 0

self.buf.finish_path(last_val)

代码的主循环如下所示。我们先创建环境、智能体和一些后面会用上的变量。

env = gym.make(ENV_ID).unwrapped

# 设置随机种子以便复现效果

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

env.seed(RANDOM_SEED)

state_dim = env.observation_space.shape[0]

action_dim = env.action_space.shape[0]

action_bound = env.action_space.high

agent = TRPO(state_dim, action_dim, action_bound)

t0 = time.time()

在训练模式下，我们将智能体与环境产生的交互数据存入缓存，当缓存满了的时候则进行一

次更新。

if args.train: # train

all_episode_reward = []

for episode in range(TRAIN_EPISODES):

state = env.reset()

state = np.array(state, np.float32)

episode_reward = 0

for step in range(MAX_STEPS):

if RENDER:

env.render()

action, value, logp, mean, log_std = agent.get_action(state)

next_state, reward, done, _ = env.step(action)

next_state = np.array(next_state, np.float32)

agent.buf.store(state, action, reward, value, logp, mean, log_std)

episode_reward += reward

state = next_state

if agent.buf.is_full():

190

5.10 策略梯度代码例子

agent.finish_path(done, next_state)

agent.update()

if done:

break

agent.finish_path(done, next_state)

if episode == 0:

all_episode_reward.append(episode_reward)

else:

all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *

0.1)

print(

’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode+1, TRAIN_EPISODES, episode_reward,

time.time() - t0

)

)

if episode

agent.save()

agent.save()

接着我们可以增加一些绘图的代码，以便于观察训练过程。

plt.plot(all_episode_reward)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’trpo.png’))

当训练完成后，我们可以开始测试。

if args.test:

# test

agent.load()

for episode in range(TEST_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS):

env.render()

action, *_ = agent.get_action(state, greedy=True)

191

第 5 章 策略梯度

state, reward, done, info = env.step(action)

episode_reward += reward

if done:

break

print(

’Testing | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode + 1, TEST_EPISODES, episode_reward,

time.time() - t0))

5.10.6 PPO: Pendulum-V0

PPO 是一种一阶方法，与 TRPO 这样的二阶算法不同。

在 PPO-Penalty 中，是通过给目标函数增加一个 KL 散度惩罚项的，以解决像 TRPO 这样带

KL 约束的更新问题。PPO 类的结构如下所示：

class PPO(object):

def __init__(self, state_dim, action_dim, action_bound, method=’clip’): # 初始化

...

def train_actor(self, state, action, adv, old_pi): # 行动者训练函数

...

def train_critic(self, reward, state): # 批判者训练函数

...

def update(self): # 主更新函数

...

def get_action(self, s, greedy=False): # 选择动作

...

def save(self): # 存储网络

...

def load(self): # 载入网络

...

def store_transition(self, state, action, reward): # 存储每步的状态、动作、奖励

...

def finish_path(self, next_state): # 计算累积奖励

...

在 PPO 算法中，我们在初始化函数中建立行动者网络和批判者网络。PPO 有两种方法：PPOPenalty 和 PPO-Clip。我们在选用不同的方法时，要设置其相对应的参数。由于环境是一个连续运

192

5.10 策略梯度代码例子

动控制环境，我们可以使用随机策略网络输出均值和对数标准差来描述动作分布。另外，我们在

网络输出加了一个 lambda 层将均值乘以 2，这是由于’Pendulum-V0’ 环境中的动作范围是 [−2, 2]。

class PPO(object):

def __init__(self, state_dim, action_dim, action_bound, method=’clip’):

# Critic

with tf.name_scope(’critic’):

inputs = tl.layers.Input([None, state_dim], tf.float32, ’state’)

layer = tl.layers.Dense(64, tf.nn.relu)(inputs)

layer = tl.layers.Dense(64, tf.nn.relu)(layer)

v = tl.layers.Dense(1)(layer)

self.critic = tl.models.Model(inputs, v)

self.critic.train()

# Actor

with tf.name_scope(’actor’):

inputs = tl.layers.Input([None, state_dim], tf.float32, ’state’)

layer = tl.layers.Dense(64, tf.nn.relu)(inputs)

layer = tl.layers.Dense(64, tf.nn.relu)(layer)

a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)

mean = tl.layers.Lambda(lambda x: x * action_bound, name=’lambda’)(a)

logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))

self.actor = tl.models.Model(inputs, mean)

self.actor.trainable_weights.append(logstd)

self.actor.logstd = logstd

self.actor.train()

self.actor_opt = tf.optimizers.Adam(LR_A)

self.critic_opt = tf.optimizers.Adam(LR_C)

self.method = method

if method == ’penalty’:

self.kl_target = KL_TARGET

self.lam = LAM

elif method == ’clip’:

self.epsilon = EPSILON

self.state_buffer, self.action_buffer = [], []

self.reward_buffer, self.cumulative_reward_buffer = [], []

193

第 5 章 策略梯度

self.action_bound = action_bound

train_actor() 函数负责使用 PPO 方法更新行动者。PPO 使用特定的目标函数来防止新策

略远离旧策略。

def train_actor(self, state, action, adv, old_pi):

with tf.GradientTape() as tape:

mean, std = self.actor(state), tf.exp(self.actor.logstd)

pi = tfp.distributions.Normal(mean, std)

ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))

surr = ratio * adv

if self.method == ’penalty’: # ppo penalty

kl = tfp.distributions.kl_divergence(old_pi, pi)

kl_mean = tf.reduce_mean(kl)

aloss = -(tf.reduce_mean(surr - self.lam * kl))

else: # ppo clip

aloss = -tf.reduce_mean(

tf.minimum(surr,

tf.clip_by_value(ratio, 1. - self.epsilon, 1. + self.epsilon)

* adv)

)

a_gard = tape.gradient(aloss, self.actor.trainable_weights)

self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))

if self.method == ’kl_pen’:

return kl_mean

train_critic() 函数负责对批判者进行更新，代码如下所示。过程就是计算优势并最小化

损失 P

t Aˆ2

t。

def train_critic(self, reward, state):

reward = np.array(reward, dtype=np.float32)

with tf.GradientTape() as tape:

advantage = reward - self.critic(state)

loss = tf.reduce_mean(tf.square(advantage))

grad = tape.gradient(loss, self.critic.trainable_weights)

self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))

194

5.10 策略梯度代码例子

在 update() 函数中，我们先计算旧策略的分布，之后再进行更新。如果我们使用 PPO-Penalty

方法，则我们还需要在更新行动者之后，根据 KL 散度来更新 lambda 值。

def update(self):

s = np.array(self.state_buffer, np.float32)

a = np.array(self.action_buffer, np.float32)

r = np.array(self.cumulative_reward_buffer, np.float32)

mean, std = self.actor(s), tf.exp(self.actor.logstd)

pi = tfp.distributions.Normal(mean, std)

adv = r - self.critic(s)

# update actor

if self.method == ’kl_pen’:

for _ in range(A_UPDATE_STEPS):

kl = self.a_train(s, a, adv, pi)

if kl < self.kl_target / 1.5:

self.lam /= 2

elif kl > self.kl_target * 1.5:

self.lam *= 2

else:

for _ in range(A_UPDATE_STEPS):

self.a_train(s, a, adv, pi)

# update critic

for _ in range(C_UPDATE_STEPS):

self.c_train(r, s)

self.state_buffer.clear()

self.action_buffer.clear()

self.cumulative_reward_buffer.clear()

self.reward_buffer.clear()

get_action() 函数就是简单地使用均值和标准差来描述动作分布，并且从中采样动作。如

果我们想要一个没有探索的动作，就只需要输出均值即可。

def get_action(self, s, greedy=False):

state = state[np.newaxis, :].astype(np.float32)

mean, std = self.actor(state), tf.exp(self.actor.logstd)

if greedy:

195

第 5 章 策略梯度

action = mean[0]

else:

pi = tfp.distributions.Normal(mean, std)

action = tf.squeeze(pi.sample(1), axis=0)[0]

return np.clip(action, -self.action_bound, self.action_bound)

save()、load()、store_transition() 函数和之前的代码类似，这里不做展开。finish_

path() 函数负责在游戏结束或者采集好了一批数据的时候计算累计奖励。

def finish_path(self, next_state, done):

if done:

v_s_ = 0

else:

v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]

discounted_r = []

for r in self.reward_buffer[::-1]:

v_s_ = r + GAMMA * v_s_

discounted_r.append(v_s_)

discounted_r.reverse()

discounted_r = np.array(discounted_r)[:, np.newaxis]

self.cumulative_reward_buffer.extend(discounted_r)

self.reward_buffer.clear()

主函数也和之前的十分相似。首先建立环境和 PPO 智能体。

env = gym.make(ENV_ID).unwrapped

# 设置随机种子，可以更好地复现效果

env.seed(RANDOM_SEED)

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

state_dim = env.observation_space.shape[0]

action_dim = env.action_space.shape[0]

action_bound = env.action_space.high

agent = PPO(state_dim, action_dim, action_bound)

t0 = time.time()

196

5.10 策略梯度代码例子

接着使用智能体和环境进行交互，并存储数据。在游戏结束或者收集足够的数据时，执行

finish_path() 函数计算累计奖励。在采集好一批数据时更新智能体。在经历过很多次学习之

后，智能体就能取得很好的分数了。

if args.train:

all_episode_reward = []

for episode in range(TRAIN_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS): # 在单个片段中

if RENDER:

env.render()

action = agent.get_action(state)

state_, reward, done, info = env.step(action)

agent.store_transition(state, action, reward)

state = state_

episode_reward += reward

# 更新 PPO

if len(agent.state_buffer) >= BATCH_SIZE:

agent.finish_path(state_, done)

agent.update()

if done:

break

agent.finish_path(state_, done)

print(

’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0)

)

if episode == 0:

all_episode_reward.append(episode_reward)

else:

all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *

0.1)

agent.save()

plt.plot(all_episode_reward)

197

第 5 章 策略梯度

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’ppo.png’))

最后，像往常一样测试智能体。

if args.test:

agent.load()

for episode in range(TEST_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS):

env.render()

state, reward, done, info = env.step(agent.get_action(state, greedy=True))

episode_reward += reward

if done:

break

print(

’Testing | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode + 1, TEST_EPISODES, episode_reward,

time.time() - t0))

参考文献

AMARI S I, 1998. Natural gradient works efficiently in learning[J]. Neural computation, 10(2): 251-276.

GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al., 2014. Generative Adversarial Nets[C]//

Proceedings of the Neural Information Processing Systems (Advances in Neural Information Processing

Systems) Conference.

GROSSE R, MARTENS J, 2016. A kronecker-factored approximate fisher matrix for convolution layers[C]//International Conference on Machine Learning (ICML). 573-582.

HEESS N, SRIRAM S, LEMMON J, et al., 2017. Emergence of locomotion behaviours in rich environments[J]. arXiv:1707.02286.

KAKADE S, LANGFORD J, 2002. Approximately optimal approximate reinforcement learning[C]//

Proceedings of the International Conference on Machine Learning (ICML): volume 2. 267-274.

198

参考文献

KONDA V R, TSITSIKLIS J N, 2000. Actor-critic algorithms[C]//Advances in Neural Information

Processing Systems. 1008-1014.

LI J, WANG B, 2018. Policy optimization with second-order advantage information[J]. arXiv preprint

arXiv:1805.03586.

LIU H, FENG Y, MAO Y, et al., 2017. Action-depedent control variates for policy optimization via stein’s

identity[J]. arXiv preprint arXiv:1710.11198.

MARTENS J, GROSSE R, 2015. Optimizing neural networks with kronecker-factored approximate

curvature[C]//International Conference on Machine Learning (ICML). 2408-2417.

MITLIAGKAS I, ZHANG C, HADJIS S, et al., 2016. Asynchrony begets momentum, with an application to deep learning[C]//2016 54th Annual Allerton Conference on Communication, Control, and

Computing (Allerton). IEEE: 997-1004.

MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learning[C]//International Conference on Machine Learning (ICML). 1928-1937.

SCHULMAN J, LEVINE S, ABBEEL P, et al., 2015. Trust region policy optimization[C]//International

Conference on Machine Learning (ICML). 1889-1897.

SCHULMAN J, WOLSKI F, DHARIWAL P, et al., 2017. Proximal policy optimization algorithms[J].

arXiv:1707.06347.

SUTTON R S, MCALLESTER D A, SINGH S P, et al., 2000. Policy gradient methods for reinforcement

learning with function approximation[C]//Advances in Neural Information Processing Systems. 1057-

1063.

WU C, RAJESWARAN A, DUAN Y, et al., 2018. Variance reduction for policy gradient with actiondependent factorized baselines[J]. arXiv preprint arXiv:1803.07246.

WU Y, MANSIMOV E, GROSSE R B, et al., 2017. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation[C]//Advances in Neural Information Processing

Systems. 5279-5288.

199

6

深 度 Q 网 络 和 Actor-Critic

的结合

深度 Q 网络（Deep Q-Network，DQN）算法是最著名的深度强化学习算法之一，将强化学

习与深度神经网络相结合以近似最优动作价值函数，只需以像素值作为输入就在绝大部分 Atari

游戏中达到了人类水平的表现。Actor-Critic 方法将 REINFORCE 算法的蒙特卡罗更新方式转化

为时间差分更新方式，大幅度提高了采样效率。近年来，将深度 Q 网络算法与 Actor-Critic 方法

相结合的算法愈加流行，如深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）算

法。这些算法结合了深度 Q 网络和 Actor-Critic 方法的优点，在大多数环境特别是连续动作空

间的环境中表现出优越的性能。本章先简要介绍各类方法的优缺点，然后介绍一些将深度 Q 网

络和 Actor-Critic 方法相结合的经典算法，如 DDPG 算法、孪生延迟 DDPG（Twin Delayed Deep

Deterministic Policy Gradient，TD3）算法和柔性 Actor-Critic（Soft Actor-Critic，SAC）算法。

6.1 简介

深度 Q 网络（Deep Q-Network，DQN）(Mnih et al., 2015) 算法是一种经典的离线策略方法。

它将 Q-Learning 算法与深度神经网络相结合，实现了从视觉输入到决策输出的端到端学习。该

算法仅使用 Atari 游戏的原始像素作为输入，便在几十款游戏中取得了人类水平级的表现。然而，

虽然深度 Q 网络的输入可以是高维的状态空间，但是它只能处理离散的、低维的动作空间。对于

连续的、高维的动作空间，深度 Q 网络无法直接计算出每个动作对应的 Q 值。

Actor-Critic（AC）(Sutton et al., 2018) 方法是 REINFORCE (Sutton et al., 2018) 算法的扩展。通

过引入 Critic，该方法将策略梯度算法的蒙特卡罗更新转化为时间差分更新。通过这种方式，自

举法（Bootstrapping）可以灵活地运用到值估计当中，因此策略的更新不需要等得到完整的轨迹

之后再进行，即不需要等到每局游戏结束。虽然时间差分更新会引入一些估计偏差，但它可以减

200

6.2 深度确定性策略梯度算法

少估计方差从而加快学习速度。尽管如此，原始的 Actor-Critic 方法仍然是一种在线策略的算法，

而在线策略方法的采样效率远低于离线策略方法。

将深度 Q 网络与 Actor-Critic 相结合可以同时利用这两种算法的优点。由于深度 Q 网络的存

在，Actor-Critic 方法转化为离线策略方法，可以使用回放缓存的样本对网络进行训练，从而提高

采样效率。从回放缓存中随机采样也可以打乱数据的序列关系，最小化样本之间的相关性，从而

使价值函数的学习更加稳定。Actor-Critic 方法使得我们可以通过网络学习策略函数 π，便于处理

深度 Q 网络很难解决的具有高维或连续动作空间的问题（表 6.1）。

表 6.1 深度 Q 网络算法与 Actor-Critic 算法的特点

算法 在线策略/离线策略 采样效率 动作空间

深度 Q 网络 离线策略 高 离散

Actor-Critic 在线策略 低 连续

深度 Q 网络 +Actor-Critic 离线策略 高 离散和连续

6.2 深度确定性策略梯度算法

深度确定性策略梯度算法可以看作是确定性策略梯度（Deterministic Policy Gradient，DPG）

算法 (Silver et al., 2014) 和深度神经网络的结合，也可以看作是深度 Q 网络算法在连续动作空间

中的扩展。它可以解决深度 Q 网络算法无法直接应用于连续动作空间的问题。深度确定性策略

梯度算法同时建立 Q 值函数（Critic）和策略函数（Actor）。Q 值函数（Critic）与深度 Q 网络算

法相同，通过时间差分方法进行更新。策略函数（Actor）利用 Q 值函数（Critic）的估计，通过

策略梯度方法进行更新。

在深度确定性策略梯度算法中，Actor 是一个确定性策略函数，表示为 π(s)，待学习参数表

示为 θ

π。每个动作直接由 At = π(St|θ

π

t

) 计算，不需要从随机策略中采样。

这里，一个关键问题是如何平衡这种确定性策略的探索和利用（Exploration and Exploitation）。

深度确定性策略梯度算法通过在训练过程中添加随机噪声解决该问题。每个输出动作添加噪声

N，此时有动作为 At = π(St|θ

π

t

) + Nt。其中 N 可以根据具体任务进行选择，原论文 (Uhlenbeck

et al., 1930) 中使用 Ornstein-Uhlenbeck 过程（O-U 过程）添加噪声项。

O-U 过程满足以下随机微分方程：

dXt = θ(π − Xt)dt + σdWt, (6.1)

其中 Xt 是随机变量，θ > 0, x, σ > 0 为参数。Wt 是维纳过程或称布朗运动 (It et al., 1965)，

它具有以下性质：

201

第 6 章 深度 Q 网络和 Actor-Critic 的结合

• Wt 是独立增量过程，表示对于时间T0 < T1 < ... < Tn，有随机变量WT0

, WT1−WT0

, · · · , WTn −

WTn−1 都是独立的。

• 对于任意时刻 t 和增量 ∆t, 有 W(t + ∆t) − W(t) ∼ N(0, σ2

W ∆t)。

• Wt 是关于 t 的连续函数。

我们知道马尔可夫决策过程是基于马尔可夫性质的，满足p(Xt+1| Xt, · · · , X1) = p(Xt+1|Xt)，

其中 Xt 是 t 时刻的随机变量，这意味着随机变量 Xt 的时间相关性只取决于上一个时刻的随机

变量 Xt−1。而 O-U 噪声就是一个具有时间相关性的随机变量，这一点与马尔可夫决策过程的性

质相符，因此很自然地被运用到随机噪声的添加中。然而，实践表明，时间不相关的零均值高斯

噪声也能取得很好的效果。

回到深度确定性策略梯度算法，动作价值函数 Q(s, a|θ

Q) 和深度 Q 网络算法一样，通过贝尔

曼方程（Bellman Equations）进行更新。

在状态 St 下，通过策略 π 执行动作 At = π(St|θ

π

t

)，得到下一个状态 St+1 和奖励值 Rt。我

们有：

Q

π

(St, At) = E[r(St, At) + γQπ

(St+1, π(St+1))]. (6.2)

然后计算 Q 值：

Yi = Ri + γQπ

(St+1, π(St+1)). (6.3)

使用梯度下降算法最小化损失函数：

L =

1

N

X

i

(Yi − Q(Si

, Ai

|θ

Q))2

. (6.4)

通过将链式法则应用于期望回报函数 J 来更新策略函数 π。这里，J = ERi,Si∼E,Ai∼π[Rt]

（E 表示环境），Rt =

PT

i=t

γ

(i−t)

r(Si

, Ai)。我们有：

∇θπ J ≈ESt∼ρβ [∇θπ Q(s, a|θ

Q)|s=St,a=π(St|θπ)

],

=ESt∼ρβ [∇aQ(s, a|θ

Q)|s=St,a=π(St)∇θπ π(s|θ

π

)|s=St

].

(6.5)

通过批量样本（Batches）的方式更新：

∇θπ J ≈

1

N

X

i

∇aQ(s, a|θ

Q)|s=Si,a=π(Si)∇θπ π(s|θ

π

)|Si

. (6.6)

此外，深度确定性策略梯度算法采用了类似深度 Q 网络算法的目标网络，但这里通过指数平

202

6.3 孪生延迟 DDPG 算法

滑方法而不是直接替换参数来更新目标网络：

θ

Q′ ← ρθQ + (1 − ρ)θ

Q′

, (6.7)

θ

π

′ ← ρθπ + (1 − ρ)θ

π

′

. (6.8)

由于参数 ρ ≪ 1，目标网络的更新缓慢且平稳，这种方式提高了学习的稳定性。

算法伪代码详见算法 6.26。

算法 6.26 DDPG

超参数：软更新因子 ρ，奖励折扣因子 γ。

输入：回放缓存 D，初始化 critic 网络 Q(s, a|θ

Q) 参数 θ

Q、actor 网络 π(s|θ

π

) 参数 θ

π、目标网

络 Q′、π

′。

初始化目标网络参数 Q′ 和 π

′，赋值 θ

Q′ ← θ

Q, θπ

′ ← θ

π。

for episode = 1, M do

初始化随机过程 N 用于给动作添加探索。

接收初始状态 S1。

for t = 1, T do

选择动作 At = π(St|θ

π

) + Nt。

执行动作 At 得到奖励 Rt，转移到下一状态 St+1。

存储状态转移数据对 (St, At, Rt, Dt, St+1) 到 D。

令 Yi = Ri + γ(1 − Dt)Q′

(St+1, π′

(St+1|θ

π

′

)|θ

Q′

)

通过最小化损失函数更新 Critic 网络：

L =

1

N

P

i

(Yi − Q(Si

, Ai

|θ

Q))2

通过策略梯度的方式更新 Actor 网络：

∇θπ J ≈

1

N

P

i ∇aQ(s, a|θ

Q)|s=Si,a=π(Si)∇θπ π(s|θ

π

)|Si

更新目标网络：

θ

Q′ ← ρθQ + (1 − ρ)θ

Q′

θ

π

′ ← ρθπ + (1 − ρ)θ

π

′

end for

end for

6.3 孪生延迟 DDPG 算法

孪生延迟 DDPG（Twin Delayed Deep Deterministic Policy Gradient，TD3）算法是深度确定性

策略梯度算法的改进，其中运用了三个关键技术：

(1) 截断的 Double Q-Learning：通过学习两个 Q 值函数，用类似 Double Q-Learning 的方式更新

critic 网络。

(2) 延迟策略更新：更新过程中，策略网络的更新频率低于 Q 值网络。

(3) 目标策略平滑：在目标策略的输出动作中加入噪声，以此平滑 Q 值函数的估计，避免过

203

第 6 章 深度 Q 网络和 Actor-Critic 的结合

拟合。

对于第一个技术，我们知道在深度 Q 网络算法中 max 操作会导致 Q 值过估计的问题，这个

问题同样存在于深度确定性策略梯度算法中，因为深度确定性策略梯度算法中 Q(s, a) 的更新方

式与深度 Q 网络算法相同：

Q(s, a) ← R

a

s + γ max

aˆ

Q(s

′

, aˆ). (6.9)

在表格学习方法（Tabular Methods）中不存在该问题，因为 Q 值是精确存储的。而当我们使

用神经网络等工具作为函数近似器（Function Approximator）来处理更复杂的问题时，Q 值的估

计是存在误差的，也就是说：

Q

approx(s

′

, aˆ) = Q

target(s

′

, aˆ) + Y

aˆ

s

′ , (6.10)

其中，Y

aˆ

s

′ 是零均值的噪声。但使用 max 操作，会导致 Qapprox 和 Qtarget 之间存在误差。将误差表

示为 Zs，我们有：

Zs

def =R

a

s + γ max

aˆ

Q

approx(s

′

, aˆ) − (R

a

s + γ max

aˆ

Q

target(s

′

, aˆ)),

=γ(max

aˆ

Q

approx(s

′

, aˆ) − max

aˆ

Q

target(s

′

, aˆ)).

(6.11)

考虑噪声项 Y

aˆ

s

′，一些 Q 值可能偏小，而另一些可能偏大。max 操作总是为每个状态选择最大

的 Q 值，这将导致算法对高估动作的对应 Q 值异常敏感。在这种情况下，该噪声使得 E[Zs] > 0，

从而导致过估计问题。

孪生延迟 DDPG 算法在深度确定性策略梯度算法中引入了 Double Q-Learning，通过建立两

个 Q 值网络来估计下一个状态的值：

Qθ

′

1

(s

′

, a′

) = Qθ

′

1

(s

′

, πϕ1

(s

′

)), (6.12)

Qθ

′

2

(s

′

, a′

) = Qθ

′

2

(s

′

, πϕ1

(s

′

)). (6.13)

使用两个 Q 值中的最小值计算贝尔曼方程：

Y1 = r + γ min

i=1,2

Qθ

′

i

(s

′

, πϕ1

(s

′

)). (6.14)

使用截断的 Double Q-Learning，目标网络的估值不会给 Q-Learning 的目标带来过高的估计

误差。虽然此更新规则可能导致低估，但这对更新影响不大。因为与过估计的动作不同，低估的

动作的 Q 值不会被显式更新 (Fujimoto et al., 2018)。

对于第二个技术，我们知道目标网络是实现深度强化学习算法稳定更新的有力工具。因为函

204

6.4 柔性 Actor-Critic 算法

数逼近需要多次梯度更新才能收敛，目标网络在学习过程中给算法提供了一个稳定的更新目标。

因此，如果目标网络可以用来减少多步更新的误差，且错误状态估计下的策略更新会导致发散的

策略更新，那么策略网络应该以比价值网络更低的频率进行更新，以便在进行策略更新之前先最

小化价值估计的误差。因此，孪生延迟 DDPG 算法降低了策略网络的更新频率，策略网络只在价

值网络更新 d 次后才进行更新。这种策略更新方式可以使 Q 值函数的估计具有更小的方差，从

而获得质量更高的策略更新。

对于第三个技术，确定性策略的一个问题是该类方法对于值空间中的窄峰估计可能存在过拟

合。在孪生延迟 DDPG 算法原文中，作者认为相似的动作应该具有相似的值估计，因此将目标动

作周围的一小块区域的值进行模糊拟合是有道理的：

y = r + Eϵ[Qθ

′ (s

′

, πϕ′ (s

′

) + ϵ)]. (6.15)

通过在每个动作中加入截断的正态分布噪声作为正则化，可以平滑 Q 值的计算，避免过拟

合。修正后的更新如下：

y = r + γQθ

′ (s

′

, πϕ′ (s

′

) + ϵ), ϵ ∼ clip(N(0, σ), −c, c). (6.16)

算法伪代码见算法 6.27。

算法 6.27 TD3

超参数：软更新因子 ρ、回报折扣因子 γ、截断因子 c

输入：回放缓存 D，初始化 Critic 网络 Qθ1

, Qθ2 参数 θ1, θ2，初始化 Actor 网络 πϕ 参数 ϕ

初始化目标网络参数 ˆθ1 ← θ1,

ˆθ2 ← θ2, ϕˆ ← ϕ

for t = 1 to T do do

选择动作 At ∼ πϕ(St) + ϵ, ϵ ∼ N (0, σ)

接受奖励 Rt 和新状态 St+1

存储状态转移数据对 (St, At, Rt, Dt, St+1) 到 D

从 D 中采样大小为 N 的小批量样本 (St, At, Rt, Dt, St+1)

a˜t+1 ← πϕ

′ (St+1) + ϵ, ϵ ∼ clip(N (0, σ, ˜ −c, c))。

y ← Rt + γ(1 − Dt) mini=1,2 Qθi

′ (St+1, a˜t+1)

更新 Critic 网络 θi ← arg minθi N −1 P(y − Qθi

(St, At))2

if t mod d then

更新 ϕ：

∇ϕJ(ϕ) = N −1 P∇aQθ1

(St, At)|At=πϕ(St)∇ϕπϕ(St)

更新目标网络：

ˆθi ← ρθi + (1 − ρ)

ˆθi

ϕˆ ← ρϕ + (1 − ρ)ϕˆ

end if

end for

205

第 6 章 深度 Q 网络和 Actor-Critic 的结合

6.4 柔性 Actor-Critic 算法

柔性 Actor-Critic（Soft Actor-Critic，SAC）算法继续采用了上一章提到的最大化熵的想法。学

习的目标是最大化熵正则化的累积奖励而不只是累计奖励，从而鼓励更多的探索。

max

πθ

E

"X

t

γ

t



r(St, At) + αH(πθ(·|St))

#

. (6.17)

这里 α 是正则化系数。最大化熵增强学习这个想法已经被很多论文，包括 (Fox et al., 2016; Haarnoja

et al., 2017; Levine et al., 2013; Nachum et al., 2017; Ziebart et al., 2008) 提及。在本节中，我们主要

介绍柔性策略迭代（Soft Policy Iteration）算法。以这个算法为基础，我们会接着介绍 SAC。

6.4.1 柔性策略迭代

柔性策略迭代是一个有理论保证的学习最优最大化熵策略的算法。和策略迭代类似，柔性策

略迭代也分为两步：柔性策略评估和柔性策略提高。

令

V

π

(s) = E

"X

t

γ

t



r(St, At) + αH(π(·|St))

#

, (6.18)

其中 s0 = s，令

Q(s, a) = r(s, a) + γE



V (s

′

)



(6.19)

这里假设 s

′ ∼ Pr

·|s, a

是下一个状态。可以很容易地验证以下式子成立。

V

π

(s) = Ea∼π



Q(s, a) − α log(a|s)



. (6.20)

在柔性策略评估时，定义的贝尔曼回溯算子 T 为

T

πQ(s, a) = r(s, a) + γE



V

π

(s

′

)



. (6.21)

和策略评估类似，我们可以证明对于任何映射 Q0

: S × A → R, Qk = T

πQk−1 会收敛到 π

的柔性 Q 值。

在策略提高阶段，我们用当前的 Q 值求解以下最大化熵正则化奖励的优化问题。

π(·|s) = arg max

π

Ea∼π



Q(s, a) + αH(π)



. (6.22)



6.4 柔性 Actor-Critic 算法

求解以上这个优化问题后 (Fox et al., 2016; Nachum et al., 2017) 可以得到的解为

π(·|s) = exp

1

αQ(s, ·)



Z(s)

. (6.23)

这里 Z(s) 是归一化常数，也即 Z(s) = P

a

exp

1

αQ(s, a)



。如果采用的策略模型无法表达最

优的策略 π，我们可以进一步求解

π(·|s) = arg min

π∈Π

DKL

π(·|s)∥

exp

1

αQ(s, ·)



Z(s)

!

. (6.24)

我们可以证明在学习过程，上面描述的柔性策略提高阶段也有单调提高的性质。即使在使用KL-散

度投影到 Π 之后这个性质也是成立的。这一点和上一章提到的 TRPO 类似。最后，我们可以证

明柔性策略迭代和策略迭代类似收敛到最优解，如以下定理所示。

定理 6.1 让 π0 ∈ Π 为初始策略。假设在柔性策略迭代算法下，π0 会收敛到 π∗，那么对任意的

(s, a) ∈ S × A 和任意的 π ∈ Π，Qπ∗

(s, a) ⩾ Qπ

(s, a) 。

我们省略了这一章提到的各个结论的证明过程。感兴趣的读者可以参考论文 (Haarnoja et al.,

2018)。

6.4.2 SAC

SAC 进一步把柔性策略迭代拓展到更实用的函数近似设定下，它采用在价值函数和策略函数

之间进行交替优化的方式来学习，而不只是通过估计策略 π 的 Q 值来提升策略。

令 Qϕ(s, a) 表示 Q 值函数，πθ 表示策略函数。这里我们考虑连续动作的设定并假设 πθ 的

输出为一个正态分布的期望和方差。和本书前面提到的方法类似，Q 值函数可以通过最小化柔性

Bellman 残差来学习：

JQ(ϕ) = E

"

Q(St, At) − r(St, At) − γESt+1 h

Vϕ˜(St+1)

i2

#

. (6.25)

这里 Vϕ˜(s) = Eπθ

h

Qϕ˜(s, a) − α log πθ(a|s)

i

, Qϕ˜ 表示参数 ϕ˜ 由 Q 值函数的参数 ϕ 的指数移

动平均数得到的目标 Q 值网络。策略函数 πθ 可以通过最小化以下的 KL-散度得到。

Jπ(θ) = Es∼D h

Ea∼πθ



α log πθ(a|s) − Qϕ(s, a)



i

. (6.26)

实际中，SAC 也使用了两个 Q 值函数（同时还有两个 Q 值目标函数）来处理 Q 值估计的

偏差问题，也就是令 Qϕ(s, a) = min

Qϕ1

(s, a), Qϕ2

(s, a)



。注意到 Jπ(θ) 中的期望也依赖于策略



第 6 章 深度 Q 网络和 Actor-Critic 的结合

πθ，我们可以使用似然比例梯度估计的方法来优化 Jπ(θ) (Williams, 1992)。在连续动作空间的设

定下，我们也可以用策略网络的重参数化来优化。这样往往能够减少梯度估计的方差。再参数化

的做法将 πθ 表示成一个使用状态 s 和标准正态样本 ϵ 作为其输入的函数直接输出动作 a：

a = fθ(s, ϵ). (6.27)

代入 Jπ(θ) 的式子中

Jπ(θ) = Es∼D,ϵ∼N

α log πθ(fθ(s, ϵ)|s) − Qϕ(s, fθ(s, ϵ))

. (6.28)

式子中 N 表示标准正态分布，πθ 现在被表示为 fθ。

最后，SAC 还提供了自动调节正则化参数 α 方法。该方法通过最小化以下损失函数实现。

J(α) = Ea∼πθ



−α log πθ(a|s) − ακ

. (6.29)

这里 κ 是一个可以理解为目标熵的超参数。这种更新 α 的方法被称为自动熵调节方法。其背

后的原理是在给定每一步平均熵至少为 κ 的约束下，原来的策略优化问题的对偶形式。对自动熵

调节方法的严格表述感兴趣的读者，可以参考 SAC 的论文 (Haarnoja et al., 2018)。算法 6.28 给出

了 SAC 的伪代码。

算法 6.28 Soft Actor-Critic (SAC)

超参数: 目标熵 κ, 步长 λQ, λπ, λα, 指数移动平均系数 τ。

输入: 初始策略函数参数 θ, 初始 Q 值函数参数 ϕ1 和 ϕ2。

D = ∅; ϕ˜

i = ϕi

, for i = 1, 2

for k = 0, 1, 2, · · · do

for t = 0, 1, 2, · · · do

从 πθ(·|St) 中取样 At, 保存 (Rt, St+1)。

D = D ∪ {St, At, Rt, St+1}

end for

进行多步梯度更新：

ϕi = ϕi − λQ∇JQ(ϕi) for i = 1, 2

θ = θ − λπ∇θJπ(θ)

α = α − λα∇J(α)

ϕ˜

i = (1 − τ )ϕi + τϕ˜

i for i = 1, 2

end for

返回 θ, ϕ1, ϕ2。

208

6.5 代码例子

6.5 代码例子

本节将分享 DDPG、TD3、和 SAC 的代码例子。它们都是使用 Q 网络作为批判者的 ActorCritic 方法。这里的例子都基于 OpenAI Gym 环境。由于这些算法都基于连续动作空间，我们使

用了“Pendulum-V0”环境。

6.5.1 相关的 Gym 环境

之前有提到过，Pendulum-V0 是一个经典的倒立摆环境。它有 3 维观测空间和 1 维动作空间。

在每步中，环境根据当前的旋转角度、速度和加速度返回一个奖励。此任务的目标是让倒立摆尽

量直立不动，来获取最高分数。

6.5.2 DDPG: Pendulum-V0

DDPG 使用离线策略和 TD 方法。DDPG 类的结构如下所示。

class DDPG(object):

def __init__(self, action_dim, state_dim, action_range): # 初始化

...

def ema_update(self): # 指数滑动平均更新

...

def get_action(self, s, greedy=False): # 获得动作

...

def learn(self): # 学习和更行

...

def store_transition(self, s, a, r, s_): # 存储转移数据

...

def save(self): # 存储模型

...

def load(self): # 载入模型

...

在初始化函数中，建立了 4 个网络，分别是行动者网络、批判者网络、行动者目标网络和批

判者目标网络。目标网络的参数将被直接替换为对应网络的参数。

class DDPG(object):

def __init__(self, action_dim, state_dim, action_range):

self.memory = np.zeros((MEMORY_CAPACITY, state_dim * 2 + action_dim + 1),

dtype=np.float32)

209

第 6 章 深度 Q 网络和 Actor-Critic 的结合

self.pointer = 0

self.action_dim, self.state_dim, self.action_range = action_dim, state_dim,

action_range

self.var = VAR

W_init = tf.random_normal_initializer(mean=0, stddev=0.3)

b_init = tf.constant_initializer(0.1)

def get_actor(input_state_shape, name=’’):

input_layer = tl.layers.Input(input_state_shape, name=’A_input’)

layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init,

b_init=b_init, name=’A_l1’)(input_layer)

layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init,

b_init=b_init, name=’A_l2’)(layer)

layer = tl.layers.Dense(n_units=action_dim, act=tf.nn.tanh, W_init=W_init,

b_init=b_init, name=’A_a’)(layer)

layer = tl.layers.Lambda(lambda x: action_range * x)(layer)

return tl.models.Model(inputs=input_layer, outputs=layer, name=’Actor’ + name)

def get_critic(input_state_shape, input_action_shape, name=’’):

state_input = tl.layers.Input(input_state_shape, name=’C_s_input’)

action_input = tl.layers.Input(input_action_shape, name=’C_a_input’)

layer = tl.layers.Concat(1)([state_input, action_input])

layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init,

b_init=b_init, name=’C_l1’)(layer)

layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init,

b_init=b_init, name=’C_l2’)(layer)

layer = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init,

name=’C_out’)(layer)

return tl.models.Model(inputs=[state_input, action_input], outputs=layer,

name=’Critic’ + name)

# 建立网络

self.actor = get_actor([None, state_dim])

self.critic = get_critic([None, state_dim], [None, action_dim])

self.actor.train()

self.critic.train()

210

6.5 代码例子

def copy_para(from_model, to_model):

for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):

j.assign(i)

# 替换参数

self.actor_target = get_actor([None, state_dim], name=’_target’)

copy_para(self.actor, self.actor_target)

self.actor_target.eval()

self.critic_target = get_critic([None, state_dim], [None, action_dim],

name=’_target’)

copy_para(self.critic, self.critic_target)

self.critic_target.eval()

self.ema = tf.train.ExponentialMovingAverage(decay=1 - TAU) # 软替换

self.actor_opt = tf.optimizers.Adam(LR_A)

self.critic_opt = tf.optimizers.Adam(LR_C)

在训练过程中，目标网络的参数将通过滑动平均来更新。

def ema_update(self):

paras = self.actor.trainable_weights + self.critic.trainable_weights

self.ema.apply(paras)

for i, j in zip(self.actor_target.trainable_weights +

self.critic_target.trainable_weights, paras):

i.assign(self.ema.average(j))

由于策略网络是一个确定性策略网络，所以我们如果不是要贪心地选择动作，就要对动作增

加一些随机。我们这里使用了一个正态分布作为随机项，它的方差会随着更新迭代而渐渐减小。

这里的随机可以改成其他方式，如 O-U 噪声。不过 OpenAI1 推荐使用不相关的 0 均值高斯噪声，

效果很好。

def get_action(self, state, greedy=False):

a = self.actor(np.array([s], dtype=np.float32))[0]

if greedy:

return a

1链接见读者服务

211

第 6 章 深度 Q 网络和 Actor-Critic 的结合

# 增加一些随机，来让动作采样带有一些探索

return np.clip(np.random.normal(a, self.var),

-self.action_range,

self.action_range)

在 learn() 函数中，我们从回放缓存中采样离线数据，并使用贝尔曼方程来学习 Q 函数。之

后，可以通过最大化 Q 值来学习策略。最后，通过 Polyak 平均 (Polyak, 1964) 来更新目标网络，

其公式为 θ

Q′ ← ρθQ + (1 − ρ)θ

Q′

, θπ

′ ← ρθπ + (1 − ρ)θ

π

′。

def learn(self):

self.var *= .9995

indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)

bt = self.memory[indices, :]

bs = bt[:, :self.s_dim]

ba = bt[:, self.s_dim:self.s_dim + self.a_dim]

br = bt[:, -self.s_dim - 1:-self.s_dim]

bs_ = bt[:, -self.s_dim:]

with tf.GradientTape() as tape:

a_ = self.actor_target(bs_)

q_ = self.critic_target([bs_, a_])

y = br + GAMMA * q_

q = self.critic([bs, ba])

td_error = tf.losses.mean_squared_error(y, q)

c_grads = tape.gradient(td_error, self.critic.trainable_weights)

self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))

with tf.GradientTape() as tape:

a = self.actor(bs)

q = self.critic([bs, a])

a_loss = -tf.reduce_mean(q) # 最大化 Q 值

a_grads = tape.gradient(a_loss, self.actor.trainable_weights)

self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))

self.ema_update()

212

6.5 代码例子

store_transition() 函数使用了回放缓存来存储每步的转移数据。

def store_transition(self, s, a, r, s_):

s = s.astype(np.float32)

s_ = s_.astype(np.float32)

transition = np.hstack((s, a, [r], s_))

index = self.pointer

self.memory[index, :] = transition

self.pointer += 1

主函数非常直接易懂，就是在每一步中使用智能体和环境交互，将数据存入回放缓存，再从

回放缓存中随机采样数据更新网络。

env = gym.make(ENV_ID).unwrapped

# 设置随机种子，方便复现效果

env.seed(RANDOM_SEED)

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

state_dim = env.observation_space.shape[0]

action_dim = env.action_space.shape[0]

action_range = env.action_space.high # 缩放动作 [-action_range, action_range]

agent = DDPG(action_dim, state_dim, action_range)

t0 = time.time()

if args.train: # 训练

all_episode_reward = []

for episode in range(TRAIN_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS):

if RENDER:

env.render()

# 添加探索噪声

action = agent.get_action(state)

state_, reward, done, info = env.step(action)

agent.store_transition(state, action, reward, state_)

213

第 6 章 深度 Q 网络和 Actor-Critic 的结合

if agent.pointer > MEMORY_CAPACITY:

agent.learn()

state = state_

episode_reward += reward

if done:

break

if episode == 0:

all_episode_reward.append(episode_reward)

else:

all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *

0.1)

print(

’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode+1, TRAIN_EPISODES, episode_reward,

time.time() - t0

)

)

agent.save()

plt.plot(all_episode_reward)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’ddpg.png’))

在训练完成后，可以进行测试。

if args.test:

# 测试

agent.load()

for episode in range(TEST_EPISODES):

state = env.reset()

episode_reward = 0

for step in range(MAX_STEPS):

env.render()

state, reward, done, info = env.step(agent.get_action(state, greedy=True))

episode_reward += reward

214

6.5 代码例子

if done:

break

print(

’Testing | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode + 1, TEST_EPISODES, episode_reward,

time.time() - t0))

6.5.3 TD3: Pendulum-V0

TD3 代码使用了这些类：ReplayBuffer、QNetwork、PolicyNetwork 和 TD3。

ReplayBuffer 类用来建立一个回放缓存，它的主要函数是 push() 和 sample() 函数。

class ReplayBuffer:

def __init__(self, capacity): # 初始化函数

...

def push(self, state, action, reward, next_state, done): # 存入数据

...

def sample(self, batch_size): # 采样数据

...

def __len__(self): # 通过重构以实现对 len 函数的支持

...

__init__ 函数负责初始化，其中只包含指针、缓存和容量值变量。

def __init__(self, capacity):

self.capacity = capacity

self.buffer = []

self.position = 0

push() 函数负责将数据存入缓存，并且移动指针。这里的缓存是一个环形缓存。

def push(self, state, action, reward, next_state, done):

if len(self.buffer) < self.capacity:

self.buffer.append(None)

self.buffer[self.position] = (state, action, reward, next_state, done)

self.position = int((self.position + 1)

sample() 函数负责从缓存中采样数据并返回。

215

第 6 章 深度 Q 网络和 Actor-Critic 的结合

def sample(self, batch_size):

batch = random.sample(self.buffer, batch_size)

state, action, reward, next_state, done = map(np.stack, zip(*batch)) # 堆叠各元素

return state, action, reward, next_state, done

通过重构 __len__() 函数可以在 ReplayBuffer 类被 len() 函数调用的时候返回缓存的

大小。

def __len__(self):

return len(self.buffer)

QNetwork 类被用于建立批判者的 Q 网络。这里使用了另一种建立网络的方法，通过继承

Model 类并重构 forward 函数来建立网络模型。

class QNetwork(Model):

def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):

super(QNetwork, self).__init__()

input_dim = num_inputs + num_actions

w_init = tf.random_uniform_initializer(-init_w, init_w)

self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=input_dim, name=’q1’)

self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=hidden_dim, name=’q2’)

self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name=’q3’)

def forward(self, input):

x = self.linear1(input)

x = self.linear2(x)

x = self.linear3(x)

return x

PolicyNetwork类用于建立行动者的策略网络。它在建立网络模型的同时，也增加了 evaluate()、

get_action()、sample_action() 函数。

class PolicyNetwork(Model):

def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1.,

init_w=3e-3): # 初始化网络

...

216

6.5 代码例子

def forward(self, state): # 重构前向传播函数

...

def evaluate(self, state, eval_noise_scale): # 进行评估

...

def get_action(self, state, explore_noise_scale, greedy=False): # 获取动作

...

def sample_action(self): # 采样动作

...

建立网络结构的详细过程如下所示。

class PolicyNetwork(Model):

def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1.,

init_w=3e-3):

super(PolicyNetwork, self).__init__()

w_init = tf.random_uniform_initializer(-init_w, init_w)

self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=num_inputs, name=’policy1’)

self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=hidden_dim, name=’policy2’)

self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=hidden_dim, name=’policy3’)

self.output_linear = Dense(n_units=num_actions, W_init=w_init,

b_init=tf.random_uniform_initializer(-init_w, init_w),

in_channels=hidden_dim, name=’policy_output’)

self.action_range = action_range

self.num_actions = num_actions

def forward(self, state):

x = self.linear1(state)

x = self.linear2(x)

x = self.linear3(x)

output = tf.nn.tanh(self.output_linear(x)) # 这里的输出范围是 [-1, 1]

return output

evaluate() 函数通过评估状态产生用于计算梯度的动作。它利用目标策略平滑技术来产生

有噪声的动作。

217

第 6 章 深度 Q 网络和 Actor-Critic 的结合

def evaluate(self, state, eval_noise_scale):

state = state.astype(np.float32)

action = self.forward(state)

action = self.action_range * action

# 添加噪声

normal = Normal(0, 1)

eval_noise_clip = 2 * eval_noise_scale

noise = normal.sample(action.shape) * eval_noise_scale

noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)

action = action + noise

return action

get_action() 函数通过状态来产生用于和环境交互的动作。

def get_action(self, state, explore_noise_scale, greedy=False):

action = self.forward([state])

action = self.action_range * action.numpy()[0]

if greedy:

return action

# 添加噪声

normal = Normal(0, 1)

noise = normal.sample(action.shape) * explore_noise_scale

action += noise

return action.numpy()

sample_action() 函数用于在训练开始时产生随机动作。

def sample_action(self, ):

a = tf.random.uniform([self.num_actions], -1, 1)

return self.action_range * a.numpy()

接下来介绍 TD3 类，它是本例子的核心内容。

class TD3():

def __init__(self, state_dim, action_dim, replay_buffer, hidden_dim, action_range,

policy_target_update_interval=1, q_lr=3e-4, policy_lr=3e-4):

# 创建回放缓存和网络

...

def target_ini(self, net, target_net): # 初始化目标网络时用到的硬拷贝更新

218

6.5 代码例子

...

def target_soft_update(self, net, target_net, soft_tau): # 通过使用 Polyak 平均对目标

# 网络进行软更新

...

def update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9,

soft_tau=1e-2): # 更新 TD3 中的所有网络

...

def save(self): # 存储训练参数

...

def load(self): # 载入训练参数

...

初始化函数创建了 2 个 Q 网络、1 个策略网络，还建立了它们的目标网络。总共建立了

(2 + 1) × 2 = 6 个网络。

class TD3():

def __init__(self, state_dim, action_dim, replay_buffer, hidden_dim, action_range,

policy_target_update_interval=1, q_lr=3e-4, policy_lr=3e-4):

self.replay_buffer = replay_buffer

# 初始化所有网络

self.q_net1 = QNetwork(state_dim, action_dim, hidden_dim)

self.q_net2 = QNetwork(state_dim, action_dim, hidden_dim)

self.target_q_net1 = QNetwork(state_dim, action_dim, hidden_dim)

self.target_q_net2 = QNetwork(state_dim, action_dim, hidden_dim)

self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)

self.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim,

action_range)

print(’Q Network (1,2): ’, self.q_net1)

print(’Policy Network: ’, self.policy_net)

# 初始化目标网络参数

self.target_q_net1 = self.target_ini(self.q_net1, self.target_q_net1)

self.target_q_net2 = self.target_ini(self.q_net2, self.target_q_net2)

self.target_policy_net = self.target_ini(self.policy_net, self.target_policy_net)

# 设置训练模式

self.q_net1.train()

219

第 6 章 深度 Q 网络和 Actor-Critic 的结合

self.q_net2.train()

self.target_q_net1.train()

self.target_q_net2.train()

self.policy_net.train()

self.target_policy_net.train()

self.update_cnt = 0

self.policy_target_update_interval = policy_target_update_interval

self.q_optimizer1 = tf.optimizers.Adam(q_lr)

self.q_optimizer2 = tf.optimizers.Adam(q_lr)

self.policy_optimizer = tf.optimizers.Adam(policy_lr)

target_ini() 函数和 target_soft_update() 函数都用来更新目标网络。不同之处在于前

者是通过硬拷贝直接替换参数，而后者是通过 Polyak 平均进行软更新。

def target_ini(self, net, target_net):=

for target_param, param in zip(target_net.trainable_weights,

net.trainable_weights):

target_param.assign(param)

return target_net

def target_soft_update(self, net, target_net, soft_tau):=

for target_param, param in zip(target_net.trainable_weights,

net.trainable_weights):

target_param.assign(target_param * (1.0 - soft_tau) + param * soft_tau) # 软更

新

return target_net

接下来将介绍关键的 update() 函数。这部分充分体现了 TD3 算法的 3 个关键技术。

在函数的开始部分，我们先从回放缓存中采样数据。

def update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9,

soft_tau=1e-2): # 更新 TD3 中的所有网

络

self.update_cnt += 1

# 采样数据

state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)

220

6.5 代码例子

reward = reward[:, np.newaxis] # 扩展维度

done = done[:, np.newaxis]

接下来，我们通过给目标动作增加噪声实现了目标策略平滑技术。通过这样跟随动作的变化，

对 Q 值进行平滑，可以使得策略更难利用 Q 函数的拟合差错。这是 TD3 算法中的第三个技术。

# 技术三: 目标策略平滑。通过给目标动作增加噪声来实现

new_next_action = self.target_policy_net.evaluate(

next_state, eval_noise_scale=eval_noise_scale

) # 添加了截断的正态噪声

# 通过批数据的均值和标准差进行标准化

reward = reward_scale * (reward - np.mean(reward, axis=0)) / np.std(reward,

axis=0)

下一个技术是截断的 Double-Q Learning。它将同时学习两个 Q 值函数，并且选择较小的 Q 值

来作为贝尔曼误差损失函数中的目标 Q 值。通过这种方法可以减轻 Q 值的过估计。这也是 TD3

算法中的第一个技术。

# 训练 Q 函数

target_q_input = tf.concat([next_state, new_next_action], 1) # 0 维是样本数量

# 技术一: 截断的 Double-Q Learning。这里使用了更小的 Q 值作为目标 Q 值

target_q_min = tf.minimum(self.target_q_net1(target_q_input),

self.target_q_net2(target_q_input))

target_q_value = reward + (1 - done) * gamma * target_q_min # 如果 done==1，则只有

# reward 值

q_input = tf.concat([state, action], 1) # 处理 Q 网络的输入

with tf.GradientTape() as q1_tape:

predicted_q_value1 = self.q_net1(q_input)

q_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))

q1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)

self.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))

with tf.GradientTape() as q2_tape:

predicted_q_value2 = self.q_net2(q_input)

q_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))

221

第 6 章 深度 Q 网络和 Actor-Critic 的结合

q2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)

self.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))

最后一个技术是延迟策略更新技术。这里的策略网络及其目标网络的更新频率比 Q 值网络

的更新频率更小。论文 (Fujimoto et al., 2018) 中建议每 2 次 Q 值函数更新时进行 1 次策略更新。

这也是 TD3 算法中提到的第二个技术。

# 训练策略函数

# 技术二: 延迟策略更新。减少策略更新的频率

if self.update_cnt

with tf.GradientTape() as p_tape:

new_action = self.policy_net.evaluate(

state, eval_noise_scale=0.0

) # 无噪声，确定性策略梯度

new_q_input = tf.concat([state, new_action], 1)

# 实现方法一：

# predicted_new_q_value =

tf.minimum(self.q_net1(new_q_input),self.q_net2(new_q_input))

# 实现方法二：

predicted_new_q_value = self.q_net1(new_q_input)

policy_loss = -tf.reduce_mean(predicted_new_q_value)

p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)

self.policy_optimizer.apply_gradients(zip(p_grad,

self.policy_net.trainable_weights))

# 软更新目标网络

self.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1,

soft_tau)

self.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2,

soft_tau)

self.target_policy_net = self.target_soft_update(self.policy_net,

self.target_policy_net, soft_tau)

如下是主要训练代码。这里先创建环境和智能体。

# 初始化环境

env = gym.make(ENV_ID).unwrapped

state_dim = env.observation_space.shape[0]

action_dim = env.action_space.shape[0]

222

6.5 代码例子

action_range = env.action_space.high # 缩放动作 [-action_range, action_range]

# 设置随机种子，以便复现效果

env.seed(RANDOM_SEED)

random.seed(RANDOM_SEED)

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

# 初始化回放缓存

replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)

# 初始化智能体

agent = TD3(state_dim, action_dim, action_range, HIDDEN_DIM, replay_buffer,

POLICY_TARGET_UPDATE_INTERVAL, Q_LR, POLICY_LR)

t0 = time.time()

在开始片段之前，需要做一些初始化操作。这里训练时间受总运行步数的限制，而不是最大

片段迭代数。由于网络建立的方式不同，这种方式需要在使用前额外调用一次函数。

# 训练循环

if args.train:

frame_idx = 0

all_episode_reward = []

# 这里需要进行一次额外的调用，以使内部函数进行一些初始化操作，让其可以正常使用

# model.forward 函数

state = env.reset().astype(np.float32)

agent.policy_net([state])

agent.target_policy_net([state])

在训练刚开始的时候，会先由智能体进行随机采样。通过这种方式可以采集到足够多的用于

更新的数据。在那之后，智能体还是和往常一样与环境进行交互并采集数据，再进行存储和更新。

for episode in range(TRAIN_EPISODES):

state = env.reset().astype(np.float32)

episode_reward = 0

for step in range(MAX_STEPS):

if RENDER:

env.render()

if frame_idx > EXPLORE_STEPS:

223

第 6 章 深度 Q 网络和 Actor-Critic 的结合

action = agent.policy_net.get_action(state, EXPLORE_NOISE_SCALE)

else:

action = agent.policy_net.sample_action()

next_state, reward, done, _ = env.step(action)

next_state = next_state.astype(np.float32)

done = 1 if done is True else 0

replay_buffer.push(state, action, reward, next_state, done)

state = next_state

episode_reward += reward

frame_idx += 1

if len(replay_buffer) > BATCH_SIZE:

for i in range(UPDATE_ITR):

agent.update(BATCH_SIZE, EVAL_NOISE_SCALE, REWARD_SCALE)

if done:

break

最终，我们提供了一些可视化训练过程所需的函数，并将训练的模型进行存储。

if episode == 0:

all_episode_reward.append(episode_reward)

else:

all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *

0.1)

print(

’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode+1, TRAIN_EPISODES, episode_reward,

time.time() - t0

)

)

agent.save()

plt.plot(all_episode_reward)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’td3.png’))

224

6.5 代码例子

6.5.4 SAC: Pendulum-v0

SAC 使用了离线策略的方式对随机策略进行优化。它最大的特点是使用了熵正则项，但也

使用了一些 TD3 中的技术。其目标 Q 值的计算使用了两个 Q 网络中的最小值和策略 π(˜a|s) 的对

数概率。例子中的代码使用了这些类：ReplayBuffer、SoftQNetwork、PolicyNetwork 和 SAC。

其中 ReplayBuffer 和 SoftQNetwork 类与 TD3 中的 ReplayBuffer 和 QNetwork 类一样，

这里就不再赘述，直接介绍后续的代码。

class ReplayBuffer: # 一个环形回放缓存，用于存储转移数据并提供数据采样

def __init__(self, capacity):

......

def push(self, state, action, reward, next_state, done):

......

def sample(self, batch_size):

......

def __len__(self):

......

class SoftQNetwork(Model): # 用于评估状态-动作值 Q(s,a) 的网络

def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):

......

def forward(self, input):

......

PolicyNetwork 类也和 TD3 的十分相似。不同之处在于，SAC 使用了一个随机策略网络，而

不是 TD3 中的确定性策略网络。

class PolicyNetwork(Model):

def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1.,

init_w=3e-3, log_std_min=-20, log_std_max=2): # 初始化

......

def forward(self, state): # 前向传播

......

def evaluate(self, state, epsilon=1e-6): # 进行评估

......

def get_action(self, state, greedy=False): # 获取动作

......

def sample_action(self): # 采样动作

......

225

第 6 章 深度 Q 网络和 Actor-Critic 的结合

随机策略网络输出了动作和对数标准差来描述动作分布。因此网络有两层输出。

class PolicyNetwork(Model):

def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1.,

init_w=3e-3, log_std_min=-20, log_std_max=2):

super(PolicyNetwork, self).__init__()

self.log_std_min = log_std_min

self.log_std_max = log_std_max

w_init = tf.keras.initializers.glorot_normal(seed=None)

self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=num_inputs, name=’policy1’)

self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=hidden_dim, name=’policy2’)

self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init,

in_channels=hidden_dim, name=’policy3’)

self.mean_linear = Dense(n_units=num_actions, W_init=w_init,

b_init=tf.random_uniform_initializer(-init_w, init_w),

in_channels=hidden_dim, name=’policy_mean’)

self.log_std_linear = Dense(n_units=num_actions, W_init=w_init,

b_init=tf.random_uniform_initializer(-init_w, init_w),

in_channels=hidden_dim, name=’policy_logstd’)

self.action_range = action_range

self.num_actions = num_actions

这里在 forward() 函数中的对数标准差上进行截断，防止标准差过大。

def forward(self, state):

x = self.linear1(state)

x = self.linear2(x)

x = self.linear3(x)

mean = self.mean_linear(x)

log_std = self.log_std_linear(x)

log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)

return mean, log_std

evaluate() 函数使用重参数技术从动作分布上采样动作，这样可以保证梯度能够反向传播。

函数也计算了采样动作在原始动作分布上的对数概率。

226

6.5 代码例子

def evaluate(self, state, epsilon=1e-6):

state = state.astype(np.float32)

mean, log_std = self.forward(state)

std = tf.math.exp(log_std) # 评估时不进行裁剪，裁剪会影响梯度

normal = Normal(0, 1)

z = normal.sample(mean.shape)

action_0 = tf.math.tanh(mean + std * z) # 动作选用 TanhNormal 分布；这里使用了重参

# 数技术

action = self.action_range * action_0

# 根据论文原文，这里最后加了一个额外项以标准化不同动作范围

log_prob = Normal(mean, std).log_prob(mean + std * z) - tf.math.log(1. -

action_0 ** 2 + epsilon) - np.log(self.action_range)

# normal.log_prob 和 -log(1-a**2) 的维度都是 (N,dim_of_action);

# Normal.log_prob 输出了和输入特征一样的维度，而不是 1 维的概率

# 这里需要跨维度相加，来得到 1 维的概率，或者使用多元正态分布

log_prob = tf.reduce_sum(log_prob, axis=1)[:, np.newaxis]

# 由于 reduce_sum 减少了 1 个维度，这里将维度扩展回来

return action, log_prob, z, mean, log_std

get_action() 函数是前面函数的简单版。它只需要从动作分布上采样动作即可。

def get_action(self, state, greedy=False):

mean, log_std = self.forward([state])

std = tf.math.exp(log_std)

normal = Normal(0, 1)

z = normal.sample(mean.shape)

action = self.action_range * tf.math.tanh(

mean + std * z

) # 动作分布使用 TanhNormal 分布; 这里使用了重参数技术

action = self.action_range * tf.math.tanh(mean) if greedy else action

return action.numpy()[0]

sample_action() 函数更加简单。它只用在训练刚开始的时候采集第一次更新所需的数据。

def sample_action(self, ):

a = tf.random.uniform([self.num_actions], -1, 1)

return self.action_range * a.numpy()

227

第 6 章 深度 Q 网络和 Actor-Critic 的结合

SAC 的结构如下：

class SAC():

def __init__(self, state_dim, action_dim, replay_buffer, hidden_dim, action_range,

soft_q_lr=3e-4, policy_lr=3e-4, alpha_lr=3e-4): # 建立网络及变量

......

def target_ini(self, net, target_net): # 初始化目标网络时所需的硬拷贝更新

......

def target_soft_update(self, net, target_net, soft_tau): # 更新目标网络时所用到的软更

# 新，使用了 Polyak 平均

......

def update(self, batch_size, reward_scale=10., auto_entropy=True,

target_entropy=-2, gamma=0.99, soft_tau=1e-2): # 更新 SAC 中所有的网络

......

def save(self): # 存储训练参数

......

def load(self): # 载入训练参数

......

SAC 算法中有 5 个网络，分别是 2 个 soft Q 网络及其目标网络，以及一个随机策略网络。另

外还需要一个 alpha 变量来作为熵正则化的权衡系数。

class SAC():

def __init__(self, state_dim, action_dim, replay_buffer, hidden_dim, action_range,

soft_q_lr=3e-4, policy_lr=3e-4, alpha_lr=3e-4):

self.replay_buffer = replay_buffer

# 初始化所有网络

self.soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim)

self.soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim)

self.target_soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim)

self.target_soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim)

self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)

self.log_alpha = tf.Variable(0, dtype=np.float32, name=’log_alpha’)

self.alpha = tf.math.exp(self.log_alpha)

print(’Soft Q Network (1,2): ’, self.soft_q_net1)

print(’Policy Network: ’, self.policy_net)

# set mode

self.soft_q_net1.train()

228

6.5 代码例子

self.soft_q_net2.train()

self.target_soft_q_net1.eval()

self.target_soft_q_net2.eval()

self.policy_net.train()

# 初始化目标网络的参数

self.target_soft_q_net1 = self.target_ini(self.soft_q_net1,

self.target_soft_q_net1)

self.target_soft_q_net2 = self.target_ini(self.soft_q_net2,

self.target_soft_q_net2)

self.soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)

self.soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)

self.policy_optimizer = tf.optimizers.Adam(policy_lr)

self.alpha_optimizer = tf.optimizers.Adam(alpha_lr)

这里我们介绍一下 update() 函数。其他函数和之前 TD3 的代码一样，这里不做赘述。和往

常一样，在 update() 函数的开始，我们先从回放缓存中采样数据。对奖励值进行正则化，以提

高训练效果。

def update(self, batch_size, reward_scale=10., auto_entropy=True, target_entropy=-2,

gamma=0.99, soft_tau=1e-2):

state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)

reward = reward[:, np.newaxis] # 扩展维度

done = done[:, np.newaxis]

reward = reward_scale * (reward - np.mean(reward, axis=0)) / (

np.std(reward, axis=0) + 1e-6

) # 通过批数据的均值和标准差进行标准化，并增加一个极小的数防止除以 0 导致数值溢出问题

在这之后，我们将基于下一个状态值计算相应的 Q 值。SAC 使用了两个目标网络输出中较

小的值，这里和 TD3 相同。但是与之不同的是，SAC 在计算目标 Q 值的时候增加了熵正则项。这

里的 log_prob 部分是一个权衡策略随机性的熵值。

# 训练 Q 函数

new_next_action, next_log_prob, _, _, _ = self.policy_net.evaluate(next_state)

target_q_input = tf.concat([next_state, new_next_action], 1) # 第 0 维是样本数量

target_q_min = tf.minimum(

self.target_soft_q_net1(target_q_input),

229

第 6 章 深度 Q 网络和 Actor-Critic 的结合

self.target_soft_q_net2(target_q_input)

) - self.alpha * next_log_prob

target_q_value = reward + (1 - done) * gamma * target_q_min

# 如果 done==1，则只有 reward 值

在计算 Q 值之后，训练 Q 网络就很简单了。

q_input = tf.concat([state, action], 1)

with tf.GradientTape() as q1_tape:

predicted_q_value1 = self.soft_q_net1(q_input)

q_value_loss1 =

tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value1,

target_q_value))

q1_grad = q1_tape.gradient(q_value_loss1, self.soft_q_net1.trainable_weights)

self.soft_q_optimizer1.apply_gradients(zip(q1_grad,

self.soft_q_net1.trainable_weights))

with tf.GradientTape() as q2_tape:

predicted_q_value2 = self.soft_q_net2(q_input)

q_value_loss2 =

tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value2,

target_q_value))

q2_grad = q2_tape.gradient(q_value_loss2, self.soft_q_net2.trainable_weights)

self.soft_q_optimizer2.apply_gradients(zip(q2_grad,

self.soft_q_net2.trainable_weights))

这里的策略损失考虑了额外的熵项。通过最大化损失函数，可以训练策略来使预期回报和熵

之间的权衡达到最佳。

# 训练策略网络

with tf.GradientTape() as p_tape:

new_action, log_prob, z, mean, log_std = self.policy_net.evaluate(state)

new_q_input = tf.concat([state, new_action], 1) # 第 0 维是样本数量

# 实现方式一

predicted_new_q_value = tf.minimum(self.soft_q_net1(new_q_input),

self.soft_q_net2(new_q_input))

# 实现方式二

# predicted_new_q_value = self.soft_q_net1(new_q_input)

policy_loss = tf.reduce_mean(self.alpha * log_prob - predicted_new_q_value)

p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)

230

6.5 代码例子

self.policy_optimizer.apply_gradients(zip(p_grad,

self.policy_net.trainable_weights))

最后，我们要更新熵权衡系数 alpha 和目标网络。

# 更新 alpha

# alpha: 探索（最大化熵）和利用（最大化 Q 值）之间的权衡

if auto_entropy is True:

with tf.GradientTape() as alpha_tape:

alpha_loss = -tf.reduce_mean((self.log_alpha * (log_prob +

target_entropy)))

alpha_grad = alpha_tape.gradient(alpha_loss, [self.log_alpha])

self.alpha_optimizer.apply_gradients(zip(alpha_grad, [self.log_alpha]))

self.alpha = tf.math.exp(self.log_alpha)

else: # 固定 alpha 值

self.alpha = 1.

alpha_loss = 0

# 软更新目标价值网络

self.target_soft_q_net1 = self.target_soft_update(self.soft_q_net1,

self.target_soft_q_net1, soft_tau)

self.target_soft_q_net2 = self.target_soft_update(self.soft_q_net2,

self.target_soft_q_net2, soft_tau)

训练的主循环和 TD3 一样，先建立环境和智能体。

# 初始化环境

env = gym.make(ENV_ID).unwrapped

state_dim = env.observation_space.shape[0]

action_dim = env.action_space.shape[0]

action_range = env.action_space.high # 缩放动作，[-action_range, action_range]

# 设置随机种子，方便复现效果

env.seed(RANDOM_SEED)

random.seed(RANDOM_SEED)

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

# 初始化缓存

231

第 6 章 深度 Q 网络和 Actor-Critic 的结合

replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)

# 初始化智能体

agent = SAC(state_dim, action_dim, action_range, HIDDEN_DIM,

replay_buffer, SOFT_Q_LR, POLICY_LR, ALPHA_LR)

t0 = time.time()

之后，使用智能体和环境交互，并存储用于更新的采样数据。在第一次更新之前，用随机动

作来采集数据。

# 训练循环

if args.train:

frame_idx = 0

all_episode_reward = []

# 这里需要进行一次额外的调用，来使内部函数进行一些初始化操作，让其可以正常使用

# model.forward 函数

state = env.reset().astype(np.float32)

agent.policy_net([state])

for episode in range(TRAIN_EPISODES):

state = env.reset().astype(np.float32)

episode_reward = 0

for step in range(MAX_STEPS):

if RENDER:

env.render()

if frame_idx > EXPLORE_STEPS:

action = agent.policy_net.get_action(state)

else:

action = agent.policy_net.sample_action()

next_state, reward, done, _ = env.step(action)

next_state = next_state.astype(np.float32)

done = 1 if done is True else 0

replay_buffer.push(state, action, reward, next_state, done)

state = next_state

episode_reward += reward

frame_idx += 1

采集到足够的数据后，我们可以开始在每步进行更新。

232

6.5 代码例子

if len(replay_buffer) > BATCH_SIZE:

for i in range(UPDATE_ITR):

agent.update(

BATCH_SIZE, reward_scale=REWARD_SCALE,

auto_entropy=AUTO_ENTROPY,

target_entropy=-1. * action_dim

)

if done:

break

通过上述步骤，智能体就可以通过不断更新变得越来越强了。增加下面的代码可以更好地显

示训练过程。

if episode == 0:

all_episode_reward.append(episode_reward)

else:

all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward *

0.1)

print(

’Training | Episode: {}/{} | Episode Reward: {:.4f} | Running Time:

{:.4f}’.format(

episode+1, TRAIN_EPISODES, episode_reward,

time.time() - t0

)

)

最后，存储模型并且绘制学习曲线。

agent.save()

plt.plot(all_episode_reward)

if not os.path.exists(’image’):

os.makedirs(’image’)

plt.savefig(os.path.join(’image’, ’sac.png’))

233

第 6 章 深度 Q 网络和 Actor-Critic 的结合

参考文献

FOX R, PAKMAN A, TISHBY N, 2016. Taming the noise in reinforcement learning via soft updates[C]//

Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. AUAI Press:

202-211.

FUJIMOTO S, VAN HOOF H, MEGER D, 2018. Addressing function approximation error in actor-critic

methods[J]. arXiv preprint arXiv:1802.09477.

HAARNOJA T, TANG H, ABBEEL P, et al., 2017. Reinforcement learning with deep energy-based

policies[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.

org: 1352-1361.

HAARNOJA T, ZHOU A, HARTIKAINEN K, et al., 2018. Soft actor-critic algorithms and applications[J].

arXiv preprint arXiv:1812.05905.

IT K, MCKEAN H, 1965. Diffusion processes and their sample paths[J]. Die Grundlehren der math.

Wissenschaften, 125.

LEVINE S, KOLTUN V, 2013. Guided policy search[C]//International Conference on Machine Learning.

1-9.

MNIH V, KAVUKCUOGLU K, SILVER D, et al., 2015. Human-level control through deep reinforcement

learning[J]. Nature.

NACHUM O, NOROUZI M, XU K, et al., 2017. Bridging the gap between value and policy based

reinforcement learning[C]//Advances in Neural Information Processing Systems. 2775-2785.

POLYAK B T, 1964. Some methods of speeding up the convergence of iteration methods[J]. USSR

Computational Mathematics and Mathematical Physics, 4(5): 1-17.

SILVER D, LEVER G, HEESS N, et al., 2014. Deterministic policy gradient algorithms[C].

SUTTON R S, BARTO A G, 2018. Reinforcement learning: An introduction[M]. MIT press.

UHLENBECK G E, ORNSTEIN L S, 1930. On the theory of the brownian motion[J]. Physical review,

36(5): 823.

WILLIAMS R J, 1992. Simple statistical gradient-following algorithms for connectionist reinforcement

learning[J]. Machine Learning, 8(3-4): 229-256.

234

参考文献

ZIEBART B D, MAAS A L, BAGNELL J A, et al., 2008. Maximum entropy inverse reinforcement

learning.[C]//Proceedings of the AAAI Conference on Artificial Intelligence: volume 8. Chicago, IL,

USA: 1433-1438.

235

研究部分

这个部分介绍了一些深度强化学习的研究课题，这些内容对希望深入理解相关研究方向的读

者非常有用。我们首先在第 7 章中介绍了几个深度强化学习的重大挑战，包括采样效率（Sample

Efficiency）、学习稳定性（Learning Stability）、灾难性遗忘（Catastrophic Interference）、探索

（Exploration）、元学习（Meta-Learning）与表征学习（Representation Learning）、多智能体强化学

习（Multi-Agent Reinforcement Learning）、模拟到现实（Simulation-to-Reality，Sim2Real），以及

大规模强化学习（Large-Scale Reinforcement Learning）。然后我们用 6 个章节来介绍不同的前沿

研究挑战的细节，以及目前的解决方法。从研究角度来看，很多经典的方法都包含在这 7 个章节

中了，具体来说：

第 8 章较为全面地介绍了模仿学习（Imitation Learning）。模仿学习在学习过程中利用专家的

示范例子，帮助减缓强化学习中低采样效率的问题。第 9 章介绍了基于模型的强化学习（Modelbased RL），它也能用于提升学习效率，但这系列方法需要学习对环境的建模。基于模型的强化学

习是一个非常有前景的研究方向，有很多面向现实应用的前沿研究内容。第 10 章介绍了分层强

化学习（Hierarchical Reinforcement Learning），用以解决深度强化学习中灾难性遗忘和难以探索

的问题，并提高学习效率。这个章节还介绍了一些框架和封建制强化学习（Feudal Reinforcement

Learning）方法。第 11 章介绍了多智能体强化学习的概念，用以把强化学习拓展到多个智能体

上。不同智能体之间的竞争（Competitive）与协作（Collaborative）、纳什均衡（Nash Equilibrium）

和一些多智能体强化学习的内容细节会在这个章节中介绍。第 12 章介绍了深度强化学习的并行

计算（Parallel Computing），用以解决可扩展性挑战（Scalability Challenge），以提升学习的速度。

这章介绍了不同的并行训练框架，帮助大家把深度强化学习用于现实世界中的大规模问题。

236

7 深度强化学习的挑战

本章介绍了现有深度强化学习研究和应用中的挑战，包括：（1）样本效率问题；（2）训练稳

定性；（3）灾难性遗忘问题；（4）探索相关问题；（5）元学习和表示学习对于强化学习方法的

跨任务泛化性能；（6）有其他智能体作为环境一部分的多智能体强化学习；（7）通过模拟到现实

迁移来弥补模拟环境和现实世界间的差异；（8）对大规模强化学习使用分布式训练来缩短执行时

间，等等。本章提出了以上挑战，并介绍了一些可能的解决方案和研究方向，来引出本书第二个

板块的前沿主题，从第 8 章到第 12 章，给读者提供关于深度强化学习现有方法的缺陷、近来发

展和未来方向的相对全面的理解。

7.1 样本效率

强化学习中一个样本高效（Sample-Efficient，或称数据高效，Data-Efficient）的算法意味着

这个算法可以更好地利用收集到的样本，从而实现更快速的策略学习。使用同样数量的训练样本

（比如按强化学习中的时间步来统计），相比于其他样本低效的方法，一个样本效率高的方法可以

在学习曲线或最终结果上表现得更好。以 Pong 游戏为例，一个普通人可能通过几十次尝试就基

本掌握游戏规则并取得较好的分数。然而，对于现有的强化学习算法（尤其是无模型的方法）而

言，它可能需要成百上千个样本来逐渐学到一些有用的策略。这构成了强化学习中的一个关键问

题：我们如何为智能体设计更有效的强化学习算法，从而用更少的样本更快地学习？

这个问题的重要性主要是由于实时或现实世界中的智能体与环境交互往往有较大的代价，甚

至目前即使在模拟环境中的交互也需要一定的时间和能源上的消耗。多数现有强化学习算法在解

决大规模或连续空间问题时有较低的学习效率，以至于一个典型的训练过程即使有着较快的模拟

速度在当前计算能力下也需要难以忍受的等待时间。对于现实世界的交互过程情况可能更糟，一

237

第 7 章 深度强化学习的挑战

些潜在的问题，比如时间消耗、设备损耗、强化学习探索过程中的安全性和失败情况下的风险等，

都对实践中强化学习算法的学习效率提出了更高的要求。

提高数据使用效率，一方面需要包含有用信息的先验知识，另一方面需要能够从可获得数据

中更高效提取信息的方式。从这两方面出发，现有文献中有许多方式解决学习效率的问题：

• 从专家示范（Expert Demonstrations）中学习。这个想法需要一个专家来提供有高奖励值的

训练样本，实际上属于模仿学习（Imitation Learning）的范畴。它尝试不仅模仿专家的动作

选择，而且学习一个能解决未见过情况的泛化策略。模仿学习和强化学习的结合实际上是

一个很有前景的研究领域，在近几年来被广泛研究，并应用于如围棋游戏、机器人学习等，

来缓解强化学习低学习效率的问题。

从专家示范中学习的关键是从可获得的示范数据集中提取能生成好的动作的潜在规则，并

将其用于更广泛的情况。

• 基于模型（Model-Based）的强化学习而不是无模型（Model-Free）强化学习。如前面章节所

介绍的，一个基于模型的强化学习方法一般指智能体不仅学会一个预测其动作的策略，而

且学习一个环境的模型来辅助其动作规划，因此可以加速策略学习的速度。环境的模型基

本包括两个子模型：一个是状态转移模型（State Transition Model），它可以给出智能体做出

动作后的状态变化；一个是奖励模型（Reward Model），它决定了智能体能从环境中得到多

少奖励作为其动作的反馈。

学习准确的环境模型可以为更好地评估智能体的当前策略提供额外信息，而这可以使整个

学习过程更高效。然而，基于模型的方法有它自己的缺点，比如，实践中，基于模型的方法

经常会有模型偏差（Model Bias）的问题，即基于模型的方法经常固有地假设学习到的环境

模型能准确地刻画真实环境，但是对于模型只能从少量样本中学习的情况，这往往不成立，

即实际模型基本不准确。在真实环境中，当策略基于不准确或者有偏差的模型进行学习时

可能会产生问题。

举例来说，一种基于模型的高效强化学习算法叫 PILCO (Deisenroth et al., 2011)，它应用非

参数化的概率模型高斯过程来近似环境的动力学模型。它利用了高斯过程简单直接的求解

过程来有效地学习模型，而不是采用神经网络拟合。策略评估和改进是基于所学的概率模

型。对于现实世界中一个推车双钟摆上翻（Cart-Double-Pendulum Swing Up）任务，PILCO

方法用仅 20 到 30 次尝试就能学会一个控制的有效策略，而其他方法像多层感知机可能最

终需要至少几百次尝试的样本来学习一个动力学模型。然而，PILCO 方法也有它自己的问

题，比如，由于学习策略参数是一个非凸优化问题，难以保证能搜索到最优控制方式，而

且高斯过程的求解无法扩展到复杂模型的高维参数空间上。

通过解决存在的缺陷来设计更加高效的学习算法。上述两种方法尝试通过利用额外信息来

解决学习效率问题，如专家示范数据和环境建模信息。如果没有额外信息可以利用或环境

的动态模型难以准确学到，那么我们就应该改进算法本身的学习效率而不利用额外信息。

强化学习算法根据它们的更新方式一般分为两类：在线策略（On-Policy）和离线策略（Off238

7.1 样本效率

Policy），如之前章节中所介绍的。在线策略方法对策略的评估有较小的偏差（Bias）但有较

大的方差（Variance），而离线策略方法可以利用一个较大的随机采样批来实现较小的估计

方差。

近年来，更加先进和有效算法被不断提出。多数算法是针对一些传统算法中的特定缺陷。

比如，为了减小策略梯度的方差，Critic 网络被引入来估计 Actor-Critic 的动作--价值函数

（Action-Value Function）；为了将强化学习任务从小规模扩展到大规模，DQN 采用了深度

神经网络来改进基于表格（Tabular-based）的 Q-Learning 算法；为了解决 DQN 更新规则中

使用最大化算子造成的过估计问题，Double DQN 算法使用了一个额外的 Q 网络；为了促

进探索，基于参数噪声的 Noisy DQN 被提出，柔性 Actor-Critic（Soft Actor-Critic，缩写为

SAC）对策略的概率分布采用自适应熵；为了将 DQN 方法从只能解决离散任务扩展到连续

任务，深度确定性策略梯度算法（Deep Deterministic Policy Gradient，缩写为 DDPG）被提

出；为了稳定 DDPG 算法的学习过程，孪生延迟 DDPG（Twin Delayed DDPG，缩写为 TD3）

提出用额外的网络和延迟更新的方式来优化策略；为了确保在线策略强化学习策略优化的

安全更新，基于信赖域的算法像信赖域策略优化算法（Trust Region Policy Optimization，缩

写为 TRPO）被提出；为了缩减 TRPO 二阶优化方法的计算时间，近端策略优化（Proximal

Policy Optimization，缩写为 PPO）算法采用一阶近似；为了加速二阶自然梯度下降方法，使

用 Kronecker 因子化信赖域的 Actor-Critic 算法（Actor Critic Using Kronecker-Factored Trust

Region，缩写为 ACKTR）提出在二阶优化过程中使用 Kronecker 因子化（Kronecker-Factored）

方法近似逆 Fisher 信息矩阵；最大化后验策略梯度（Maximum A Posteriori Policy Optimization，MPO）(Abdolmaleki et al., 2018) 算法和它的在线策略变体 V-MPO (Song et al., 2019) 用

一种“强化学习作为推理”的观点实现策略优化。MPO 使用概率推理工具，像期望最大化

算法（Expectation Maximization，EM）来优化最大熵强化学习目标。以上的算法只是整个

强化学习算法领域发展的一小部分，我们希望读者到文献中查找更多改进算法学习效率和

其他缺陷的强化学习算法。与此同时，所提出的强化学习算法结构变得越来越复杂，有更

多灵活的参数可以被自适应地学习或人为选择，而这需要在强化学习研究中对其进行更加

细致的考虑。有时额外的超参数可以显著改进学习表现，但有时它们使得学习过程更加敏

感，而你需要对具体情况具体分析。

在上面例子中，我们假设数据样本包含丰富信息，而只是强化学习算法的学习效率较低。实

践中，经常见到样本缺乏有用信息的情况，尤其是稀疏奖励的任务。比如，对于单个二值变量

表示任务成功与否的情况来说，中间样本可能全部都是直接奖励（Immediate Reward）值为

0，从而没有任何区分度。这些样本中的信息自然就很稀疏。像这样的情况，在没有充分的奖

励函数指引的情况下，有效探索空间的方式可能就很关键。像后见之明经验回放（Hindsight

Experience Replay）(Andrychowicz et al., 2017)，分层学习结构 (Kulkarni et al., 2016)、内在奖

励（Intrinsic Reward） (Sukhbaatar et al., 2018)、好奇心驱使的探索 (Pathak et al., 2017) 和其

他有效的探索机制 (Houthooft et al., 2016) 都被用于一些工作中。强化学习中的学习效率由

239

第 7 章 深度强化学习的挑战

于强化学习的固有性质被探索过程显著地影响，而有效的探索可以通过采集到更有信息的

样本而提高从样本中学习的效率。由于探索是强化学习中的另一个巨大挑战，它将在后续

小节之一中被单独讨论。

7.2 学习稳定性

深度强化学习可能非常不稳定或有随机性。这里的“不稳定”指，在多次训练中，每次学习

表现在随时间变化的横向比较中的差异。随时间变化的不稳定，学习过程体现为有巨大的局部方

差或在单次学习曲线上的非单调增长，比如有时学习表现甚至由于某些原因会下降。在多次训练

中，不稳定的学习过程体现为在每一个阶段上的多次学习表现之间的巨大差异，而这将导致横向

对比中的巨大方差。

深度神经网络的不稳定性和不可预测性在深度强化学习领域被进一步加剧，移动的目标分

布、数据不满足独立同分布条件、对价值函数的不稳定的有偏差估计等因素导致了梯度估计器中

的噪声，而进一步造成不稳定的学习表现。不同于监督学习在固定的数据集上学习（这里不考虑

批限制的强化学习），强化学习经常是从高度相关的样本中学习的。比如，学习智能体大多采用

策略探索得到的样本，要么是用在线策略学习的当前策略，要么是离线策略学习的先前策略（有

时甚至是其他策略）。智能体和环境之间连续交互产生的样本可能是高度相关的，这打破了有效

学习神经网络的独立性条件。由于价值函数是由当前策略选择的轨迹估计的，价值函数和估计它

的策略之间也有依赖关系。由于策略随训练时间改变，参数化的价值函数的优化流形也随时间改

变。考虑到为了便于在训练中探索，策略往往具有一定的随机性，价值函数于是更加难以追寻，

而这也会导致用来学习的数据不满足独立同分布条件。不稳定的学习过程主要是由策略梯度或价

值函数估计的变化造成的。然而，有偏差估计是强化学习中不稳定表现的另一根源，尤其是当偏

差本身也不稳定的时候。举例来说，回想第 2 章，为了实现用 Qw(s, a) 对动作价值函数 Qπ

(s, a)

进行的无偏差估计，可兼容函数拟合条件（Compatible Function Approximation Condition）需要被

满足。同时，有一些其他条件来确保价值函数的无偏差估计，以及一些进一步的要求条件来保证

高级强化学习算法对策略改进有正确且准确的梯度计算。然而，实践中，这些要求或条件经常被

放宽，而导致对价值函数的不稳定有偏差估计，或者策略梯度中较大的方差。多数情况下，人们

讨论强化学习算法中估计的偏差和方差之间的权衡，而不稳定的偏差项本身也可能促成不稳定的

学习表现。也有一些其他因素会导致不稳定的学习表现，比如探索策略中的随机性、环境中的随

机性、数值计算的随机种子等。

论文 (Houthooft et al., 2016) 提出了以 Variational Information Maximizing Exploration（VIME）

作为一种应用于一般强化学习算法中的探索方式。一些学习表现展示于他们所做的算法比较中，

在三种不同的环境上使用 TRPO 或 TRPO+VIME 算法的学习结果基本上在学习曲线上都显示出

了较大的方差，如图 7.1 所示。对于环境 MountainCar 来说，TRPO 算法的学习曲线能够覆盖整

个奖励值范围 [0, 1]，而且对 TRPO+VIME 方法在 HalfCheetah 环境也是类似的情况。我们需要注

240

7.2 学习稳定性

意相比于其他一些强化学习算法，TRPO 在多数情况下已经是一个相对稳定的算法，它使用对梯

度下降的二阶优化和信赖域限制。其他算法像 DDPG 可能在训练过程中表现得更加不稳定，有噪

声的探索甚至可能在训练了较长一段时间后显著降低学习表现 (Fujimoto et al., 2018)。

(a) MountainCar (b) HalfCheetah (c) CartPoleSwingup

图 7.1 VIME 实验中的学习曲线。图片改编自文献 (Houthooft et al., 2016)（见彩插）

强化学习过程中的随机性会给准确评估算法表现带来困难，而这也显示出使用不同随机种子

获得平均结果的重要性。

先前关于强化学习的调研 (Henderson et al., 2018) 中给出了一些关于深度强化学习实验中不

稳定性和敏感性相关的结论：

• 策略网络结构可以对 TRPO 和 DDPG 算法的结果有显著影响。

• 对于策略网络或价值网络的隐藏层，ReLU 或 Leaky ReLU 激活函数往往在多个环境和多个

算法上有最好的表现。而这个效果的大小对不同算法或环境不一致。

• 奖励值缩放的效果对不同环境和不同缩放值不一致。

• 5 个随机种子（通常的报告设置）可能不足以论证显著的结果，因为如果你仔细挑选随机种

子，不同的随机种子可能得到完全不重合的置信区间，即使采用完全相同的实现方式。

• 环境动态的稳定性可能严重影响强化学习算法的学习表现。比如，一个不稳定的环境可以

迅速削弱 DDPG 算法的有效学习表现。

人们已经有很长一段时间在尝试解决强化学习中的稳定性问题。为了解决累计奖励函数在原

始 REINFORCE 算法中的较大方差，价值函数拟合被引入来估计奖励值。进一步地，动作价值函

数也被用于奖励函数近似，这降低了方差，即使它可能是有偏差的。像这样方法构成了深度强化

学习算法的主流——结合 Q-Learning 和策略梯度（Policy Gradient）方法，如之前第 6 章中所介

绍的。在原始 DQN (Mnih et al., 2013) 中，使用目标网络和延迟更新，以及经验回放池帮助缓解了

不稳定学习的问题。通常一个深度函数拟合器需要多次梯度更新而不是单次更新来达到收敛，而

目标网络给学习过程提供了一个稳定的目标，这有助于在训练数据上收敛。在某种程度上，它可

以满足同分布条件，而强化学习在没有目标网络时会将其打破。经验回放池给 DQN 提供了一种

离线策略的学习方式，而从回放池中随机采样到的训练数据更接近于独立同分布数据，这也有助

于稳定学习过程。更多关于 DQN 的细节在第 4 章中有所介绍。此外，TD3 算法（在第 6 章中介

241

第 7 章 深度强化学习的挑战

绍）在 DQN 的稳定技术上应用目标策略平滑正则化（Target Policy Smooth Regularization）方法，

基于相似动作有相似值的平滑性假设，从而在动作目标价值的估计中加入噪声，以减小方差。同

时，TD3 使用了一对 Critic 而不是像 DDPG 中的一个，而这进一步稳定了学习表现。另一方面，

对于基于策略梯度的方法来说，TRPO 使用二阶优化通过更全面的信息提供更稳定的更新，以及

使用对更新后策略的限制来保证其保守但稳定的进步。

然而，即使有了以上工作，不稳定性、随机性和对初值及超参数的敏感性都使得强化学习

研究人员在不同任务上评估算法和复现结果有一定困难，而这仍旧是强化学习社区的一个巨大

挑战。

7.3 灾难性遗忘

由于强化学习通常有动态的学习过程而非像监督学习一样在固定的数据集上学习，它可以被

看作是追逐一个移动目标的过程，而数据集在整个过程不断被更新。比如，在第 2 章中我们介绍

了在线策略价值函数 V

π

(s) 和动作价值函数 Qπ

(s, a)，它们都是用当前策略 π 来估计的。但是策

略在整个学习过程中都在更新，这会导致对价值函数的动态估计。尽管通过离线策略回放池可以

用一个相对稳定的训练集来缓解这个问题，回放池中的样本仍旧随着智能体的探索过程而不断改

变。因此，一个叫作灾难性遗忘（Catastrophic Interference 或 Catastrophic Forgetting）(Kirkpatrick

et al., 2017) 的问题可能在学习过程中发生，尤其是当策略或价值函数是基于神经网络的深度学习

方法时，这个问题描述了其在解决如上所述的增量学习过程中有较差能力的现象。新的数据经常

使得已训练过的网络改变很多来拟合它，从而忘记网络在之前训练过程中所学到的内容，即使这

些内容也是有用的。这是在强化学习方法中使用神经网络做拟合器的一种局限性。

相较于离线策略方式，自然的人类学习过程实际更接近于在线策略学习。人们每天都在实时

地学习新事物而不是一直从记忆中学习。然而，在线策略强化学习方法仍旧在努力提高学习效

率，并且企图防止灾难性遗忘的问题。基于信赖域的方法像 TRPO 和 PPO 对学习过程中更新策

略的潜在范围做了限制，来保证稳定但相对缓慢的学习表现进步。对于在线策略学习，样本通常

以相关联数据的形式被采集，这极大促使了灾难性遗忘的发生。因此，离线策略学习方法使用经

验回放池来缓解这个问题，从而在某种程度上保留旧数据来学习。像优先经验回放（Prioritized

Experience Replay）和后见之明经验回放（Hindsight Experience Replay）的技术作为更复杂和先进

的方式被提出，按照回放池中数据的重要性或者其目标来使用数据。

灾难性遗忘也发生在学习过程分为几个阶段的情况中。比如，在模拟到现实的策略迁移过程

中，策略通常需要在模拟环境中预训练而后利用现实世界数据微调。然而，实践中，两个过程可能

使用不同的损失函数，而且损失函数可能不总是与整体强化学习目标一致。如在文献 (Jeong et al.,

2019a) 中，图像观察量被嵌入潜在表示而作为策略的输入，这个嵌入网络（Embedding Network）

在模拟到现实的适应过程中通过一个自监督损失函数来微调，而非使用原来在模拟训练过程中的

强化学习损失。这种在多阶段训练过程损失函数上的不匹配也可能在实践中造成灾难性遗忘，这

242

7.4 探索

意味着策略可能遗忘预训练中获得的技能。为了解决这个问题，固定部分网络层并用之前的损失

函数继续更新网络可以在后训练（Post-Training）过程中尽可能保持预训练的网络。另一个相似

的想法是残差策略学习（Residual Policy Learning），如 8.6 节中所提到的，它也固定了预训练网

络的权重并在旁边添加了一个新的网络来学习修正项。

7.4 探索

探索是强化学习中另一个主要的挑战，它会显著影响学习效率。相比于探索和利用间的权衡

（Exploration-Exploitation Trade-Off）这个强化学习中经典且为人所知问题，这里着重于探索本身

的挑战。强化学习中探索的困难可能来自稀疏的奖励函数、较大的动作空间和不稳定的环境，以

及现实世界中探索的安全性问题等。探索意味着通过交互来获取更多关于环境的信息，通常与利

用相对。利用指通过开发已知信息来最大化奖励。强化学习的学习过程基于试错。除非那些最优

的轨迹在之前被探索过，否则最优的策略无法被学到。举例来说，雅达利游戏像 OpenAI Gym 中

的 Montezuma’s Revenge、Pitfall 由于探索的困难，对于一般强化学习算法会很难解决，这几个游

戏的场景如图 7.2 所示，其中通常包括一个复杂的迷宫，需要较复杂的一系列操作来解决。它们

像一个解迷宫的问题但是有着更复杂的结构和层次。Montezuma’s Revenge 是一个非常典型的稀

疏奖励任务，这使得强化学习的探索非常难以进行。在一个游戏场景中，Montezuma’s Revenge 的

智能体必须完成几十个连续动作来通过一个房间，而这个游戏有 23 个不同的房间场景需要智能

体指导它自己通过。相似的情况在 Pitfall 游戏中也有。这些游戏常用作评估强化学习方法在探索

能力方面的基准。OpenAI 和 Deepmind (Aytar et al., 2018) 都声称他们用高效的深度强化学习方法

解决了 Montezuma’s Revenge 游戏。然而，这些结果可能不令人满意。在他们的解决方案中，专

图 7.2 难以学习的雅达利游戏：Montezuma's Revenge（左）和 Pitfall（右）（见彩插）

243

第 7 章 深度强化学习的挑战

家示范都被用于辅助探索。比如，在 Deepmind 的解决方案中，他们让智能体观察 YouTube 视频，

而 OpenAI 使用人类示范来更好地初始化智能体位置。

这里稀疏奖励任务的瓶颈实际在于探索本身。稀疏奖励可能使价值网络和策略网络在一个

不平滑且非凸的超曲面上优化，甚至在训练的某些阶段有不连续的情况。因此，一步优化后的

策略可能无法帮助探索到更高奖励的区域。基于传统探索策略的智能体，比如随机动作或 ϵ-贪

心（ϵ-Greedy）策略，会发现很难在探索过程中遇到高奖励值的轨迹。而即便它们采样到近最优

（Near-Optimal）的轨迹，基于价值的或基于策略的优化方法可能也没有对这些样本充分重视，而

导致失败情况或者缓慢的学习过程。上面描述的问题提出了当前深度强化学习方法的缺陷。

除稀疏奖励外，较大的动作空间和不稳定的环境也对强化学习智能体的探索造成困难。一个

典型的例子是在文献 (Vinyals et al., 2019) 中解决的《星际争霸 II》（StarCraft）游戏。表 7.11 中

比较了雅达利游戏、围棋和《星际争霸》的信息类型、动作空间、游戏中的活动次数和玩家数量。

大的动作空间和长的游戏控制序列使得在《星际争霸》中探索一个好的策略十分困难。此外，多

玩家的设置使得对手在某种程度上成为游戏环境的一部分，这也增加了探索的难度。

表 7.1 对比不同的游戏

雅达利游戏 围棋 《星际争霸》

信息类型 近完美 完美 不完美

动作空间 17 361 1026

每场游戏的活动次数 100/s 100/s 1000/s

玩家数量 单个 两个 多个

为了解决探索的问题，研究人员调查了包括模仿学习、内在奖励（Intrinsic Reward）、分层学

习等概念。通过模仿学习，智能体试图模仿来自人类或其他的专家示范来改进学习效率并减少探

索到近最优样本的困难。内在奖励是基于这样的观念，即行为不仅是外在奖励的结果，而且也受

到内在欲求的驱使，比如希望获得关于未知的更多有效信息。举例来说，婴儿可以通过好奇心驱

使的探索很快地学习关于世界的知识。好奇心是一种内部驱动来改进智能体的学习，使其朝向更

有探索性的策略改进。更多的内部驱动力需要在研究中探索。分层学习将复杂且难以探索的任务

分解成小的子任务，这使其容易学习。举例来说，封建制网络（Feudal Network，FuN）作为封建

制强化学习（Feudal Reinforcement Learning）中的一个关键方法使用了有管理者和工作者的层次

性结构来解决 Montezuma’s Revenge，实现更有效的探索和学习 (Vezhnevets et al., 2017)。

近年来，一些新方法被提出来解决探索问题，其中一个称为 Go-Explore，它不是一个深度强

化学习的解决方案。Go-Explore 的主要想法是首先使用无神经网络的确定性训练来探索游戏世

界，即不使用深度强化学习的方法，随后使用一个深度神经网络来模仿学习最好的轨迹，从而使

得策略能够对环境的随机性鲁棒。为了解决大规模高度复杂游戏，比如《星际争霸 II》，DeepMind

1数据源：Oriol Vinyals, Deep Reinforcement Learning Workshop, NeurIPS 2019.

244

7.5 元学习和表征学习

的研究人员 (Vinyals et al., 2019) 使用了基于族群的训练（Population-based Training，PBT）机制来

有效探索全局最优策略，其中智能体集合成为联盟（League）。不同的智能体被初始化到策略分

布中的不同集群（Clusters）上，来保证探索过程的多样性。基于族群的训练相比于单个智能体对

策略空间有更充分的探索。

现实世界中的探索也与安全性问题相关。举例来说，当考虑一辆由智能体控制的自动驾驶车

辆时，有车祸的失败情况也是智能体应该从中进行学习的。但是现实中一辆实际的车不可能被用

来采集这些失败情况的样本，而使智能体以可接受的低损耗从中学习。现实的车辆甚至不能采用

随机动作来探索，因为它可能导致灾难性的结果。相同的问题也存在于其他现实世界应用中，比

如机器人操作、机器人手术等。为了解决这个问题，模拟到现实的转移（Sim-to-Real Transfer）的

方法可以用于将强化学习部署到现实世界，它先在模拟中进行训练，再将策略转移到现实中。

7.5 元学习和表征学习

除改善一个具体任务上的学习效率外，研究人员也在寻求能够提高在不同任务上整体学习表

现的方法，这与模型的通用性（Generality）和多面性（Versatility）相关。因此，我们会问，如何

让智能体基于它所学习的旧任务来在新任务上更快地学习？而在这里可以介绍多个概念，包括元

学习（Meta-Learning）、表征学习（Representation Learning）、迁移学习（Transfer Learning）等。

元学习的问题实际上可以追溯到 1980—1990 年 (Bengio et al., 1990)。近来深度学习和深度强

化学习重新将这个问题带入我们的视野。许多令人兴奋的想法被提出，比如那些与模型无关的元

学习（Model-Agnostic Meta-Learning）方法，以及一些更强大的跨任务学习方法在近年来都有快

速发展。元学习的最初目的是让智能体解决不同问题或掌握不同技能。然而，我们无法忍受它对

每个任务都从头学习，尤其是用深度学习来拟合的时候。元学习（Meta-Learning），也称学会学

习，是让智能体根据以往经验在新任务上更快学习的方法，而非将每个任务作为一个单独的任务。

通常一个普通的学习者学习一个具体任务的过程被看作是元学习中的内循环（Inner-Loop）学习

过程，而元学习者（Meta-Learner）可以通过一个外循环（Outer-Loop）学习过程来更新内循环学

习者。这两种学习过程可以同时优化或者以一种迭代的方式进行。三个元学习的主要类别为循环

模型（Recurrent Model）、度量学习（Metric Learning）和学习优化器（Optimizer）。结合元学习和

强化学习，可以得到元强化学习（Meta-Reinforcement Learning）方法。一种有效的元强化学习方

法像与模型无关的元学习 (Finn et al., 2017) 可以通过小样本学习（Few-Shot Learning）或者几步

更新来解决一个简单的新任务。

对于一个具体的任务领域，不同的任务之间可能有隐藏的关联性质。我们是否能让智能体

从这个域内采样到的一些任务中学习这些潜在的规律，从而将所学到的内容泛化到其他任务上

来更快地学习？这个学习潜在的关系或规律的过程与一个叫表征学习（Representation Learning）

(Bengio et al., 2013) 的概念密切相关。表征学习起初在机器学习中提出，被定义为从原始数据中

学习表示方式和提取有效信息或特征来便于分类器或预测器（比如强化学习中的策略）使用。表

245

第 7 章 深度强化学习的挑战

征学习试图学习抽象且简洁的特征来表示原始材料，并且通过这种抽象，预测器或分类器不会降

低它们的表现，而有更高的学习效率。学习隐藏的表示对于强化学习中提高学习效率十分有用，

将这些规律迁移有利于在不同任务上的学习过程。表征学习通常可以用于学习强化学习环境中复

杂状态的简单表示，这被称为状态表征学习（State Representation Learning，SRL）。这个表示包含

在一个合适的抽象空间下的不变性和独特性特征，而这是从多样化的任务域中提炼出来的。举例

来说，在一个拍摄物体运动的视频的一系列帧中，物体表面角上的关键点（或者物体表面上其他

的特殊点）集合是对物体运动的一种恒定且鲁棒的表示，尽管帧中的像素点总是随着物体运动而

改变。这些关键点有时在计算机视觉术语中称为描述器（Descriptors），它们存在一个描述器空间

中。在这种表示方式下，这些关键点的位置在物体运动中将会改变，因此可以用来表示物体的运

动。不同的物体有不同的关键点集合，因而也可以用来区分物体。强化学习中的表征学习对需要

跨域的强化学习策略很重要，包括不同的任务域、模拟到现实的域迁移等。它是一个有希望且在

探索中的方向，可以用于研究人类是如何利用知识进行规划的。

7.6 多智能体强化学习

在之前介绍的章节中，环境中只有一个智能体来寻找最优策略，这属于单智能体强化学习。

除单智能体强化学习外，我们实际可以在同一个场景中设置多个智能体，来对多智能体策略进行

同时探索，这个过程可以交替或者同时进行，称为多智能体强化学习（Multi-Agent Reinforcement

Learning，MARL）。MARL 是一个有希望且值得探索的方向，提供了一种能够研究非常规强化学

习情况的方式，包括群体智能、智能体环境的动态变化、智能体本身的创新等。

现代学习算法更多的是出色的受试者（Test-Takers），而非创新者。智能体的智能上限可能受

到其所在环境的限制。因此，创新的产生成为人工智能（Artificial Intelligence，AI）中一个较热

的话题。一种通向这个愿景的最有希望路径是通过多智能体的社会交互来学习。在多智能体学习

中，智能体如何击败对手或与他人合作不是由环境的建造者决定的。举例来说，古老的围棋游戏

的发明者从未定义什么策略能够击败对手，而对手通常也构成了动态环境的一部分。然而，在一

代又一代人类玩家或人工智能体的自我演化过程中，大量先进的策略被发明出来，每个智能体作

为其他人环境的一部分，而对自身的提高也构成他人的新挑战。

MARL 中结合传统的博弈论（Game Theory）和现代深度强化学习的方法近来在文献 (Lanctot

et al., 2017; Nowé et al., 2012) 中有所探索，以及一些新的想法如自我博弈（Self-Play）(Berner

et al., 2019; Heinrich et al., 2016; Shoham et al., 2003; Silver et al., 2018a)、优先虚拟自我博弈

（Prioritized Fictitious Self-Play）(Vinyals et al., 2019)、基于族群的训练 (Population-Based Training，

PBT）(Jaderberg et al., 2017; Vinyals et al., 2019) 和独立性强化学习（Independent Reinforcement

Learning，InRL）(Lanctot et al., 2017; Tan, 1993)。MARL 不仅使得探索多智能体环境中的分布

式智能成为可能，而且有助于在较大规模复杂环境中学习近最优或近平衡的智能体策略，比如，

Deepmind 用于掌握游戏《星际争霸 II》的 AlphaStar，如图 7.3 所示。AlphaStar 的框架中用到了

246

7.7 模拟到现实

PBT，通过使用一个联盟（League）的智能体，每一个智能体由图 7.3 中一个带索引值的色块来

表示，这种训练方式被用来保证在策略空间的充分探索。在 PBT 中，策略优化的单位不再是每

个智能体的单一策略，而是整个联盟的智能体。整体策略不仅关于一个具体策略，而更是整个联

盟中智能体的整体表现。更多关于 MARL 的内容在第 11 章中有详细介绍。

图 7.3 AlphaStar 的训练机制。每个小方块表示一个 AlphaStar 联盟中训练的智能体（见彩插）

7.7 模拟到现实

强化学习方法可以成功地解决大量模拟环境中的任务，甚至在一些具体领域可以超过最好的

人类表现，比如围棋游戏。然而，应用强化学习方法到现实任务上的挑战仍旧未被解决。除了雅

达利游戏、策略性计算机游戏、纸牌游戏，强化学习在现实世界中的潜在应用包括机器人控制、

车辆自动驾驶、无人机自动控制等。这些涉及现实世界中硬件的任务通常对安全性和准确性有较

高要求。对于这些情况，一个误操作可能导致灾难性后果。当策略是通过强化学习方法学到的时

候，这个问题就更加值得考虑，因为即便不考虑现实世界的采样效率，学习智能体的探索过程也

会有巨大影响。现代工业中的机器控制仍旧严重依赖传统控制方法，而非最先进的机器学习或强

化学习解决方案。然而，用一个聪明的智能体来控制这些物理机械仍旧是一个很好的追求，而大

量相关领域的研究人员正为之努力。

近年来，深度强化学习被逐渐应用到越来越多的控制问题中。但是由于强化学习算法较高的

样本复杂度以及其他一些物理限制，许多在模拟中展示的能力尚未在现实世界中复现。我们主要

通过机器人学习的例子来展示这些内容，而这是一个越发活跃的研究方向，吸引了来自学术界和

工业界的关注。

指导性策略搜索（Guided Policy Search，GPS）(Levine et al., 2013) 是一种能够直接用真实机器

247

第 7 章 深度强化学习的挑战

人在有限时间内训练的算法。通过所学线性动态模型进行轨迹优化，这个方法能够以较少的环境

交互学会复杂的操作技巧。研究人员也探索了用多个机器人进行并行化训练的方法 (Levine et al.,

2018)。文献 (Kalashnikov et al., 2018) 提出能同时在 7 个真实机器人上进行分布式训练的 QT-Opt

算法，但是需要持续 4 个月的 800 个小时的机器人数据采样时间作为代价。他们成功示范了直接

在现实世界部署的机器人学习，但是其时间消耗和资源上的要求一般是无法接受的。更进一步来

说，直接在物理系统上训练策略的成功例子尚且只在有限的领域得到验证。

模拟到现实迁移（Sim-to-Real Transfer）则是可以替代直接在现实中训练深度强化学习智能

体的方法，由于模拟性能的提升和一些其他原因，模拟到现实迁移的方法比之前受到更多注意。

相比于直接在现实世界中训练，模拟到现实迁移可以通过在模拟中快速学习来实现。近年来，许

多模拟到现实的方法成功将强化学习智能体部署到现实中 (Akkaya et al., 2019; Andrychowicz et al.,

2018)。然而，相比于直接在现实环境中部署训练过程，模拟到现实的方法也有它本身的缺陷，这

主要由模拟和现实环境的差异造成，称为现实鸿沟（Reality Gap）。在实践中有大量因素会导致现

实鸿沟，而这由具体系统而定。举例来说，系统动力学过程的差异将导致模拟和现实的动力学鸿

沟，如图 7.4 所示是一个例子。不同的方法被提出来解决模拟到现实迁移的问题，后续还会介绍。

图 7.4 图片展示了模拟和现实中 MDP 的差异，它是由状态采集和策略推理过程产生的时间延迟

造成的，这是造成现实鸿沟的可能因素之一（见彩插）

我们首先要理解现实鸿沟的概念。现实应用中的现实鸿沟可以在某种程度上用文献 (Jeong

et al., 2019b) 中的图 7.5 来理解，该图展示了机器人上模拟轨迹和现实轨迹的差异，以及模拟和参

考信号的差异。对于强化学习进行机器人控制任务来说，参考信号是发送给智能体的控制信号，

从而在机械臂的关节角度上获得预期的行为。由于延迟、惯性和其他动力学上的不准确性，模拟

和现实中的轨迹都会与参考信号有显著差异。此外，现实中的轨迹与模拟中的不同就是现实鸿

沟。图中的系统识别（System Identification）是一种确认系统中动力学参数值的方法，可以用在

策略或者模拟器中来缩减模拟动力学过程和现实的差异。泛化力模型（Generalized Force Model，

GFM）是一个在论文 (Jeong et al., 2019b) 中新提出的方法，可以用额外的力来校正模拟器，从而

生成与现实更接近的模拟轨迹。然而，即使使用了识别和校正的方法，现实鸿沟依然可能存在，

248

7.7 模拟到现实

从而影响策略从模拟到现实中迁移。

图 7.5 在一个简单的关节角度控制过程中，机器人控制的参考信号、模拟和现实中的差异。图

片改编自文献 (Jeong et al., 2019b)（见彩插）

除了由于不同动力学过程导致的每一个时间步上模拟现实轨迹的差异，现实鸿沟也有其他来

源。比如，在连续的现实世界控制系统中，有系统响应时间延迟或系统观察量构建过程耗时，而

这些在有离散时间步的理想模拟情况下可能都不存在。如图 7.4 所示，在模拟环境或传统强化学

习设置下，状态采集和策略推理过程都认为是始终没有时间损耗的，而在现实情况下，这两个过

程都可能需要相当的时间，这使得智能体总是根据先前动作执行时的先前状态产生的滞后观察量

来进行动作选择。

上面的问题也会使得模拟和现实的轨迹展现出不同的模式，如图 7.6 所示。考虑一个物体操

作任务，即使我们假定有很快的神经网络前向过程（Forward Process）而忽略策略推理的时间消

耗，现实世界中物体位置也可能需要一个摄像机来捕捉并用一些定位技术来追踪，而这需要相当

的时间来处理。这个观察量构建的过程会引入时间延迟，从而即使在完全相同的控制信号下，现

实轨迹和模拟轨迹的对比图上也会展示出时间间隙。这类延迟观察量使得现实世界中的强化学习

智能体只能够接受先前观察量 Ot−1 来对当前步做出动作选择 At，而非直接根据当前状态 St。因

此实践中的策略根据时间延迟 δ 通常会有形式 π(At|Ot−δ)，而这不同于模拟中根据实时观察量训

练的策略，从而会产生较差的现实表现。一种解决这个问题的方式是修改模拟器，使其有相同的

时间延迟，从而训练智能体去学习。然而，这会导致其他的问题，比如如何精确地表示和测量模

拟和现实中的时间延迟，如何保证基于延迟观察量学习的智能体的表现等。近来，文献 (Ramstedt

et al., 2019) 提出了实时强化学习方法，文献 (Xiao et al., 2020) 提出了“边运动边思考（Thinking

While Moving）”的方法，在连续时间 MDP 设置下减轻了强化学习对于实时环境中延迟观察量和

并发动作选择（Concurrent Action Choices）的问题，使得在现实世界中的控制轨迹更加平滑。

如上所述，从强化学习角度来看模拟到现实迁移的主要问题在于：在模拟中训练得到的策略

249

第 7 章 深度强化学习的挑战

图 7.6 图片展示了物体观察状态（位置）在同一控制信号下的时间延迟。由于现实中额外的观

察量构建过程，现实世界轨迹（下方）相比于模拟轨迹（上方）有一定延迟。不同的线

体现了多次测试结果，加粗的线为均值（见彩插）

由于现实鸿沟不能在现实世界中始终正常使用，这个现实鸿沟即模拟和现实的差异。由于这个模

型的差异，模拟环境中的成功策略无法很好地迁移到相应的现实中。总体来说，解决模拟到现实

迁移的方法可以分为至少两个大类：零样本（Zero-Shot）方法和自适应学习方法。将控制策略从

模拟迁移到现实的问题可以被看作是域自适应（Domain Adaption）的一个例子，即将一个在源域

（Source Domain）中训练的模型迁移到新的目标域（Target Domain）。这些方法背后的一个关键假

设是不同的域有公共的特征，从而在一个域中的表征方式和行为会对其他域有用。域自适应要求

新的域中的数据适应预训练的策略。在新的域中获取数据的复杂性或困难程度，比如在现实世界

中收集样本，这种自适应学习方法因而需要有较高的效率。像元学习 (Arndt et al., 2019; Nagabandi

et al., 2018)、残差策略学习（Residual Policy Learning）(Johannink et al., 2019; Silver et al., 2018b) 和

渐进网络（Progressive Networks）(Rusu et al., 2016a,b) 等方法被用于这些情形。零样本（Zero-Shot）

迁移是一个与域自适应互补一类技术，它尤其适用于在模拟中学习。这意味着在迁移过程中没

有任何基于现实世界数据的进一步学习过程。域随机化（Domain Randomization）是零样本迁移

中典型的一类方法。通过域随机化，源和目标域的差异被建模为源域中的随机性。通过域随机化

可以学到更普适的策略，而非过拟合到具体模拟器设置的特征策略。根据具体的应用，随机化可

以被施加到不同的特征上。举例来说，对于机器人操作任务，摩擦力和质量的大小、力矩和速度

的误差在实际机器人上都会影响到控制的精度。因此，在模拟器中这些参数可以被随机化，从

而用强化学习训练一个更鲁棒的策略 (Peng et al., 2018)，这个过程称为动力学随机化（Dynamics

Randomization）。在视觉域下的随机化可以用于直接将基于视觉的策略从模拟迁移到现实，而不

需要任何现实的图像来训练 (Sadeghi et al., 2016; Tobin et al., 2017)。可能的视觉随机化的特征变

量包括纹理、光照条件和物体位置等。

现实鸿沟通常是依赖于具体任务的，它可能由动力学参数或者动力学过程的定义不同造成。

250

7.8 大规模强化学习

除了动力学随机化 (Peng et al., 2018) 或视觉特征（观察量）随机化，还有一些其他方法来跨越现

实鸿沟。利用系统识别（System Identification）来学习一个对动力学敏感（Dynamics-Aware）的

策略 (Yu et al., 2017; Zhou et al., 2019) 是一个有希望的方向，它试图学习一个以系统特征为条件

的策略，这些系统特征包括动力学参数或者轨迹的编码。也有一些方法来最小化模拟与现实的差

异，比如之前介绍的 GFM 方法用于进行力校正，等等。模拟到现实通过模拟到模拟（Sim-to-Real

via Sim-to-Sim）(James et al., 2019) 是另一个跨过现实鸿沟的方法，它使用随机到标准自适应网

络（Randomized-to-Canonical Adaptation Networks, RCANs）来将随机的或现实世界图像转化成它

们同等的非随机的标准型，而与模拟环境中的类似。渐进网络 (Rusu et al., 2016a) 也可以用于模

拟到现实迁移 (Rusu et al., 2016b)，这是一个普适的框架，重复利用任何低级视觉特征到高级策略

中，从而迁移到新的任务上，它以一种组合式但是简单的方法来构建复杂技能。

当今的计算框架利用离散的基于二值运算的计算过程，因此在某种程度上，我们应当始终承

认模拟和现实世界的差异。这是因为后者在时间和空间上是连续的（至少在经典物理系统中）。

只要学习算法不足够高效而能够直接像人脑一样应用于现实世界（或者即便可以实现），在模拟

环境中得到一些预训练模型也总是有用的。如果模型在一定程度上有对现实环境的泛化能力就会

更好，而这是模拟到现实迁移算法的意义。换句话说，模拟到现实迁移算法提供了始终考虑到在

现实鸿沟下的学习模型方法论，而无关于模拟器本身有多精确。

7.8 大规模强化学习

如前面小节中所讨论的，强化学习在现实世界的应用目前遭遇到的如延迟观察量、域变换等

问题，通常属于现实鸿沟的问题范畴。然而，也有其他一些因素阻止了强化学习的应用，或在模

拟情况下，或在现实世界中。最有挑战性的问题之一是强化学习的可扩展性（Scalability），尽管

深度强化学习利用了深度神经网络的通用表达能力，而这提出了大规模强化学习的挑战。

我们可以首先看一些例子。在像掌握大规模实时计算机游戏的应用中，如《星际争霸 II》（StarCraft）和《刀塔 2》（Dota），DeepMind 和 OpenAI 的团队分别提出了 AlphaStar (Vinyals et al., 2019)

和 OpenAI Five (Berner et al., 2019) 方法。在 AlphaStar 中，深度强化学习和监督学习（比如，模

仿学习中的行为克隆）都被用于一个基于族群的训练（Population-Based Training，PBT）框架中，

以及用到高级网络结构如 Scatter Connections、Transformer 和 Pointer 网络，这使得深度强化学习

在整个策略中实际上只占一小部分。在 AlphaStar 中最终解决任务的关键步骤是如何高效地从存

在的示范数据中学习和使用预训练的策略，作为强化学习智能体的初始状态，以及如何有效地结

合来自联盟中不同智能体的不同次优策略。在 OpenAI Five 中，一个自我博弈（Self-Play）的框

架被用于训练，而非 PBT 框架，但它也使用了从人类示范中模仿学习的方法。上述事实说明，在

多数情况下，当前的深度强化学习算法本身对于完美地从端到端去解决一个大规模任务可能仍旧

是不足够有效且高效的。一些其他技术如模仿学习等通常需要被用来解决这些大规模问题。

此外，并行训练框架也常于解决大规模问题。举例来说，在解决现实中机器人学习的算法 QT251

第 7 章 深度强化学习的挑战

Opt (Kalashnikov et al., 2018)中，为了实现并行的机器人采样，它应用了一个包含在线和离线数据的

经验回放缓存，以及分布式训练工作者来高效地从缓存数据中学习。一个分布式或并行的采样和

训练框架对于解决这类大规模问题很关键，尤其是对高维的状态和动作空间。文献 (Espeholt et al.,

2018) 提出了重要性加权的行动者-学习者结构（Importance Weighted Actor-Learner Architecture，

IMPALA），而文献 (Espeholt et al., 2019) 提出了可扩展高效深度强化学习（Scalable, Efficient

Deep-RL，SEED）来实现大规模分布式强化学习。另外，强化学习的分布式框架通常与不同计算

设备（比如 CPU 和 GPU）间的平衡有关，如第 18 章中所讨论的。在强化学习算法方面，异步优

势 Actor-Critic（Asynchronous Advantage Actor-Critic，A3C）(Mnih et al., 2016)、分布式近端策略

优化（Distributed Proximal Policy Optimizaion，DPPO）(Heess et al., 2017)、循环缓存分布式 DQN

（Recurrent Peplay Distributed DQN，R2D2）(Kapturowski et al., 2019) 等算法在近年来被提出，来

更好地支持强化学习中的并行采样和训练。更多关于强化学习中并行计算的内容在第 12 章中有

所介绍。

7.9 其他挑战

除了上面提到的（深度）强化学习中的挑战，也有一些其他挑战，比如深度强化学习的可解

释性 (Madumal et al., 2019)、强化学习应用的安全性问题 (Berkenkamp et al., 2017; Garcıa et al.,

2015)、相关理论中复杂度证明 (Koenig et al., 1993; Lattimore et al., 2013) 中的困难、强化学习算

法的效率 (Jin et al., 2018) 和收敛性质 (Papavassiliou et al., 1999)，以及理解清楚强化学习方法在整

个人工智能中的作用和角色等。这些内容超出本书范畴，有兴趣的读者可以自行探索这些领域的

前沿。

在本章最后，我们引用 Richard Sutton2的一些话，“我们从这些痛苦的教训中应当学到的一点

是通用型（General Purpose）模型的力量，即那些能够随着计算能力提升而不断扩展的方法，它

们甚至到极其巨大的计算量时也能工作。有两个看起来能够以这种方式任意扩展的方法是搜索和

学习。”这些话基于这样的观察，即在计算机象棋或计算机围棋，以及像语音识别和计算机视觉

等领域上的以往成功，一般的统计性方法（如神经网络）胜过了基于人类知识的方法。因此，智

能系统中的嵌入式知识可能只能在较短时间内满足研究人员，而在长期阻碍了通用人工智能的整

体发展过程。“第二个从痛苦的教训中学到的东西是大脑中实际的内容是极其复杂的，且这种复

杂性是不可更改的；我们应当停止寻找简单的方式来考虑大脑中的内容，比如用简答的方式考虑

空间、物体、多个智能体或对称性。所有的这些都是任意的、本质上复杂的外在环境的部分。它

们不是我们应当嵌入的东西，因为它们的复杂度是无穷的；相反，我们应当只构建元方法来找到

和采集这种任意的复杂度。”这句话阐释了提出元方法来自然地处理世界的复杂度的重要性，而

非使用人为构建的、有具体用途的、相对简单的认知结构和决策机制。

2Richard S. Sutton. “The Bitter Lesson.”March 13, 2019.

252

参考文献

参考文献

ABDOLMALEKI A, SPRINGENBERG J T, TASSA Y, et al., 2018. Maximum a posteriori policy

optimisation[J]. arXiv preprint arXiv:1806.06920.

AKKAYA I, ANDRYCHOWICZ M, CHOCIEJ M, et al., 2019. Solving rubik’s cube with a robot hand[J].

arXiv preprint arXiv:1910.07113.

ANDRYCHOWICZ M, WOLSKI F, RAY A, et al., 2017. Hindsight experience replay[C]//Advances in

Neural Information Processing Systems. 5048-5058.

ANDRYCHOWICZ M, BAKER B, CHOCIEJ M, et al., 2018. Learning dexterous in-hand manipulation[J]. arXiv preprint arXiv:1808.00177.

ARNDT K, HAZARA M, GHADIRZADEH A, et al., 2019. Meta reinforcement learning for sim-to-real

domain adaptation[J]. arXiv preprint arXiv:1909.12906.

AYTAR Y, PFAFF T, BUDDEN D, et al., 2018. Playing hard exploration games by watching youtube[C]//

Advances in Neural Information Processing Systems. 2930-2941.

BENGIO Y, BENGIO S, CLOUTIER J, 1990. Learning a synaptic learning rule[M]. Université de

Montréal, Département d’informatique et de recherche opérationnelle.

BENGIO Y, COURVILLE A, VINCENT P, 2013. Representation learning: A review and new perspectives[J]. IEEE transactions on pattern analysis and machine intelligence, 35(8): 1798-1828.

BERKENKAMP F, TURCHETTA M, SCHOELLIG A, et al., 2017. Safe model-based reinforcement

learning with stability guarantees[C]//Advances in Neural Information Processing Systems. 908-918.

BERNER C, BROCKMAN G, CHAN B, et al., 2019. Dota 2 with large scale deep reinforcement

learning[J]. arXiv preprint arXiv:1912.06680.

DEISENROTH M, RASMUSSEN C E, 2011. Pilco: A model-based and data-efficient approach to

policy search[C]//Proceedings of the 28th International Conference on Machine Learning (ICML-11).

465-472.

ESPEHOLT L, SOYER H, MUNOS R, et al., 2018. Impala: Scalable distributed deep-rl with importance

weighted actor-learner architectures[J]. arXiv preprint arXiv:1802.01561.

ESPEHOLT L, MARINIER R, STANCZYK P, et al., 2019. Seed rl: Scalable and efficient deep-rl with

accelerated central inference[J]. arXiv preprint arXiv:1910.06591.

253

第 7 章 深度强化学习的挑战

FINN C, ABBEEL P, LEVINE S, 2017. Model-agnostic meta-learning for fast adaptation of deep networks[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.

org: 1126-1135.

FUJIMOTO S, VAN HOOF H, MEGER D, 2018. Addressing function approximation error in actor-critic

methods[J]. arXiv preprint arXiv:1802.09477.

GARCIA J, FERNÁNDEZ F, 2015. A comprehensive survey on safe reinforcement learning[J]. Journal

of Machine Learning Research, 16(1): 1437-1480.

HEESS N, SRIRAM S, LEMMON J, et al., 2017. Emergence of locomotion behaviours in rich environments[J]. arXiv:1707.02286.

HEINRICH J, SILVER D, 2016. Deep reinforcement learning from self-play in imperfect-information

games[J]. arXiv:1603.01121.

HENDERSON P, ISLAM R, BACHMAN P, et al., 2018. Deep reinforcement learning that matters[C]//

Thirty-Second AAAI Conference on Artificial Intelligence.

HOUTHOOFT R, CHEN X, DUAN Y, et al., 2016. Vime: Variational information maximizing exploration[Z].

JADERBERG M, DALIBARD V, OSINDERO S, et al., 2017. Population based training of neural

networks[J]. arXiv preprint arXiv:1711.09846.

JAMES S, WOHLHART P, KALAKRISHNAN M, et al., 2019. Sim-to-real via sim-to-sim: Dataefficient robotic grasping via randomized-to-canonical adaptation networks[C]//Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition. 12627-12637.

JEONG R, AYTAR Y, KHOSID D, et al., 2019a. Self-supervised sim-to-real adaptation for visual robotic

manipulation[J]. arXiv preprint arXiv:1910.09470.

JEONG R, KAY J, ROMANO F, et al., 2019b. Modelling generalized forces with reinforcement learning

for sim-to-real transfer[J]. arXiv preprint arXiv:1910.09471.

JIN C, ALLEN-ZHU Z, BUBECK S, et al., 2018. Is q-learning provably efficient?[C]//Advances in

Neural Information Processing Systems. 4863-4873.

JOHANNINK T, BAHL S, NAIR A, et al., 2019. Residual reinforcement learning for robot control[C]//

2019 International Conference on Robotics and Automation (ICRA). IEEE: 6023-6029.

254

参考文献

KALASHNIKOV D, IRPAN A, PASTOR P, et al., 2018. Qt-opt: Scalable deep reinforcement learning

for vision-based robotic manipulation[J]. arXiv preprint arXiv:1806.10293.

KAPTUROWSKI S, OSTROVSKI G, DABNEY W, et al., 2019. Recurrent experience replay in distributed

reinforcement learning[C]//International Conference on Learning Representations.

KIRKPATRICK J, PASCANU R, RABINOWITZ N, et al., 2017. Overcoming catastrophic forgetting in

neural networks[J]. Proceedings of the national academy of sciences, 114(13): 3521-3526.

KOENIG S, SIMMONS R G, 1993. Complexity analysis of real-time reinforcement learning[C]//

Proceedings of the AAAI Conference on Artificial Intelligence. 99-107.

KULKARNI T D, NARASIMHAN K, SAEEDI A, et al., 2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation[C]//Advances in Neural Information

Processing Systems. 3675-3683.

LANCTOT M, ZAMBALDI V, GRUSLYS A, et al., 2017. A unified game-theoretic approach to multiagent

reinforcement learning[C]//Advances in Neural Information Processing Systems. 4190-4203.

LATTIMORE T, HUTTER M, SUNEHAG P, et al., 2013. The sample-complexity of general reinforcement learning[C]//Proceedings of the 30th International Conference on Machine Learning. Journal of

Machine Learning Research.

LEVINE S, KOLTUN V, 2013. Guided policy search[C]//International Conference on Machine Learning.

1-9.

LEVINE S, PASTOR P, KRIZHEVSKY A, et al., 2018. Learning hand-eye coordination for robotic

grasping with deep learning and large-scale data collection[J]. The International Journal of Robotics

Research, 37(4-5): 421-436.

MADUMAL P, MILLER T, SONENBERG L, et al., 2019. Explainable reinforcement learning through

a causal lens[J]. arXiv preprint arXiv:1905.10958.

MNIH V, KAVUKCUOGLU K, SILVER D, et al., 2013. Playing atari with deep reinforcement learning[J].

arXiv preprint arXiv:1312.5602.

MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learning[C]//International Conference on Machine Learning (ICML). 1928-1937.

NAGABANDI A, CLAVERA I, LIU S, et al., 2018. Learning to adapt in dynamic, real-world environments

through meta-reinforcement learning[J]. arXiv preprint arXiv:1803.11347.

255

第 7 章 深度强化学习的挑战

NOWÉ A, VRANCX P, DE HAUWERE Y M, 2012. Game theory and multi-agent reinforcement

learning[M]//Reinforcement Learning. Springer: 441-470.

PAPAVASSILIOU V A, RUSSELL S, 1999. Convergence of reinforcement learning with general function

approximators[C]//International Joint Conference on Artificial Intelligence: volume 99. 748-755.

PATHAK D, AGRAWAL P, EFROS A A, et al., 2017. Curiosity-driven exploration by self-supervised

prediction[C]//Proceedings of the International Conference on Machine Learning (ICML).

PENG X B, ANDRYCHOWICZ M, ZAREMBA W, et al., 2018. Sim-to-real transfer of robotic control

with dynamics randomization[C]//2018 IEEE International Conference on Robotics and Automation

(ICRA). IEEE: 1-8.

RAMSTEDT S, PAL C, 2019. Real-time reinforcement learning[C]//Advances in Neural Information

Processing Systems. 3067-3076.

RUSU A A, RABINOWITZ N C, DESJARDINS G, et al., 2016a. Progressive neural networks[J]. arXiv

preprint arXiv:1606.04671.

RUSU A A, VECERIK M, ROTHÖRL T, et al., 2016b. Sim-to-real robot learning from pixels with

progressive nets[J]. arXiv preprint arXiv:1610.04286.

SADEGHI F, LEVINE S, 2016. Cad2rl: Real single-image flight without a single real image[J]. arXiv

preprint arXiv:1611.04201.

SHOHAM Y, POWERS R, GRENAGER T, 2003. Multi-agent reinforcement learning: a critical survey[J].

Web manuscript.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2018a. A general reinforcement learning algorithm

that masters chess, shogi, and Go through self-play[J]. Science, 362(6419): 1140-1144.

SILVER T, ALLEN K, TENENBAUM J, et al., 2018b. Residual policy learning[J]. arXiv preprint

arXiv:1812.06298.

SONG H F, ABDOLMALEKI A, SPRINGENBERG J T, et al., 2019. V-mpo: On-policy maximum a

posteriori policy optimization for discrete and continuous control[J]. arXiv preprint arXiv:1909.12238.

SUKHBAATAR S, LIN Z, KOSTRIKOV I, et al., 2018. Intrinsic motivation and automatic curricula via

asymmetric self-play[C]//International Conference on Learning Representations.

TAN M, 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents[C]//Proceedings

of the International Conference on Machine Learning (ICML).

256

参考文献

TOBIN J, FONG R, RAY A, et al., 2017. Domain randomization for transferring deep neural networks

from simulation to the real world[C]//ROS.

VEZHNEVETS A S, OSINDERO S, SCHAUL T, et al., 2017. Feudal networks for hierarchical reinforcement learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70.

JMLR. org: 3540-3549.

VINYALS O, BABUSCHKIN I, CZARNECKI W M, et al., 2019. Grandmaster level in starcraft ii using

multi-agent reinforcement learning[J]. Nature, 575(7782): 350-354.

XIAO T, JANG E, KALASHNIKOV D, et al., 2020. Thinking while moving: Deep reinforcement learning

with concurrent control[J]. arXiv preprint arXiv:2004.06089.

YU W, TAN J, LIU C K, et al., 2017. Preparing for the unknown: Learning a universal policy with online

system identification[J]. arXiv preprint arXiv:1702.02453.

ZHOU W, PINTO L, GUPTA A, 2019. Environment probing interaction policies[J]. arXiv preprint

arXiv:1907.11740.

257

8 模仿学习

为了缓解深度强化学习中的低样本效率问题，模仿学习（Imitation Learning）或称学徒学习

（Apprenticeship Learning）是一种可能的解决方式，在连续决策过程中利用专家示范来更快速地

实现策略优化。为了让读者全面理解如何高效地从示范数据中提取信息，我们将介绍模仿学习中

最重要的几类方法，包括行为克隆（Behavioral Cloning）、逆向强化学习（Inverse Reinforcement

Learning）、从观察量（Observations）进行模仿学习、概率性方法和一些其他方法。在强化学习的

范畴下，模仿学习可以用作对智能体训练的初始化或引导。在实践中，结合模仿学习和强化学习

是一种可以有效学习并进行快速策略优化的方法。

8.1 简介

如我们所知，强化学习，尤其是无模型的强化学习，有着低样本效率的问题，如第 7 章所

讨论的。通常用其解决一个不是很复杂的任务并达到人类级别的表现可能需要成百上千的样本。

然而，人类可以用少得多的时间和样本来解决这些任务。为了改进强化学习的算法效率，除了通

过更精细地设计强化学习算法本身，我们实际上可以让智能体利用一些额外的信息源，比如专家

示范（Expert Demonstrations）。这些专家示范依据先验知识而对策略选择有一定偏向性，而这些

有效的偏见可以通过一个适当的学习过程而被提取或转移到强化学习的智能体策略中。从专家

示范中学习的任务被称为模仿学习（Imitation Learning，IL），也称为学徒学习（Apprenticeship

Learning）。人类和动物天生就有模仿同类其他个体的能力，这启发了让智能体从其他个体的示范

中进行模仿学习的方法。相比于强化学习，监督学习在数据使用方面是一种更加高效的方法，因

为它可以利用有标签数据。因此，如果示范数据是以有标签的形式提供的，监督学习的方法可以

被融合到智能体的学习过程中来改进它的学习效率。

258

8.1 简介

本章中，我们将介绍不同的使用示范进行策略学习的方法。图 8.1 是对模仿学习中各个类别

方法的概览。我们将在后续小节中详细介绍各种模仿学习方法，并将它们总结成几个主要的类

别，包括（1）行为克隆（Behavioral Cloning，BC），（2）逆向强化学习（Inverse Reinforcement

Learning，IRL），（3）从观察量进行模仿学习（Imitation Learning from Observations，IfO，（一些

文献 (Sun et al., 2019b) 中称 ILFO），（4）概率推理，（5）其他方法。BC 是一种最简单和直接的通

过监督学习方式利用示范数据的方法，由于它的简便性而被广泛使用并作为其他更高级方法的基

石。IRL 对于某些应用情况是有用的，比如难以写出显式的奖励函数（Explicit Reward Function）

来实现在不同的目标之间权衡的情况。举例来说，对于一个基于视觉观察量的自动驾驶车辆，多

少注意力应当被分配到处理不同的反光镜上，这难以通过奖励函数工程的方式来定义。IRL 是一

种可以从示范数据中恢复未知奖励函数的方法，从而促进强化学习过程。IfO 实际上解决了模仿

学习的一个缺陷，即它通常要求每个状态输入都伴有动作标签，而这种方式在人类的模仿学习过

程中也是经常发生的。从概率推理的角度出发的方法包括用高斯混合模型回归（Gaussian Mixture

Model Regression）或高斯过程回归（Gaussian Process Regression）来表示示范数据并引导动作策

略，在某些情况下这是比深度神经网络更高效的替代方法。也有一些其他方法，比如对离线策略

（Off-Policy）强化学习直接将示范数据送入经验回放缓存（Replay Buffer）等。在介绍了不同模仿

学习方法的基本类别后，我们将讨论模仿学习和强化学习的关系，比如将模拟学习用作强化学习

的初始化，来提高强化学习的效率。最终，我们将介绍一些其他的伴随强化学习的具体模仿学习

方法，它们可能是之前一些概念和方法的组合，或者我们之前总结过的方法类别之外的方法，如

图 8.1 所总结的。

从观察量进行模仿学习（Imitation

 Learning from Observation ，IfO）

基于模型（Model-based ） 无模型（Model-free ）

反向动态模型

（Inverse Dynamics Model ）

正向动态模型

（Forward Dynamics Model ）

生成对抗（Generative

Adversarial ）方法

奖励函数工程

（Reward-Engineering ）

模仿学习（Imitation

Learning，IL）

逆向强化学习（Inverse

Reinforcement Learning ，IRL)

行为克隆（Behavioral

Cloning ，BC)

从观察量进行行为克隆

（Behavioral Cloning from

Observation ，BCO ）等

从观察量模仿潜在策略

（Imitating Latent Policies

from Observation ，ILPO）等

生成对抗模仿学习

（Generative Adversarial

Imitation Learning ，GAIL ）等

最大熵逆向强化学习

（Maximum Entropy IRL ）等

时间对比网络 DAGGER 等

（Time-contrastive

Networks ，TCN ）等

RL缓存中的示范 概率推理（Probabilistic

 Inference ）

图 8.1 模仿学习算法概览

模仿学习的概念可以用学徒学习的形式 (Abbeel et al., 2004) 来定义：按照一个未知的奖励函

数 r(s, a)，学习者找到一个策略 π 能够表现得和专家策略 πE 相当。我们定义一个策略 π ∈ Π 的占

用率（Occupancy）的度量 ρπ ∈ D : S ×A → R 为：ρπ(s, a) = π(a|s)

P∞

t=0 γ

tp(St = s|π)(Puterman,

259

第 8 章 模仿学习

2014)，这是一个用当前策略估计的状态和动作的联合分布。由于 Π 和 D 的一一对应关系，模仿

学习的问题等价于 ρπ(s, a) 和 ρπE

(s, a) 之间的一个匹配问题。模仿学习的一个普遍目标是学习这

样一个策略：

πˆ = arg min

π∈Π

ψ

∗

(ρπ − ρπE

) − λH(π) (8.1)

其中 ψ

∗ 是一个 ρπ 和 ρπE 之间的距离度量，而 λH(π) 是一个有权衡因子 λ 的正则化项。举例来说，

这个正则化项可以定义为策略 π 的 γ-折扣因果熵（Causal Entropy）：H(π)

def = Eπ[− log π(s, a)]。模

仿学习的整体目标就是增加从当前策略采样得到的 {(s, a)} 分布和示范数据中分布的相似度，同

时考虑到策略参数上的一些限制。

8.2 行为克隆方法

如果示范数据有相应标签的话（比如，对于给定状态的一个好的动作可以被看作一个标签），

利用示范的模仿学习可以自然地被看作是一个监督学习任务。在强化学习的情况下，有标签的示

范数据 D 通常包含配对的状态和动作：D = {(si

, ai)|i = 1, · · · , N}，其中 N 是示范数据集的大

小而指标 i 表示 si 和 ai 是在同一个时间步的。在满足 MDP 假设的情况下（即最优动作只依赖

于当前状态），状态-动作对的顺序在训练中可以被打乱。考虑强化学习设定下，有一个以 θ 参数

化和 s 为输入状态的初始策略 πθ，其输出的确定性动作为 πθ(s)，我们有专家生成的示范数据集

D = {(si

, ai)|i = 1, · · · , N}，可以用来训练这个策略，其目标如下：

min

θ

X

(si,ai)∼D

∥ai − πθ(si)∥

2

2

(8.2)

一些随机性策略 πθ(˜a|s) 的具体形式，比如高斯策略等，可以用再参数化技巧来处理：

min

θ

X

a˜i∼π(·|si),(si,ai)∼D

∥ai − a˜i∥

2

2

(8.3)

这个使用监督学习直接模仿专家示范的方法在文献中称为行为克隆（Behavioral Cloning，BC）。

8.2.1 行为克隆方法的挑战

• 协变量漂移（Covariate Shift）：尽管模仿学习可以对与示范数据集（用于训练策略）相似的

样本有较好的表现，对它在训练过程中未见过的样本可能会有较差的泛化表现，因为示范

数据集中只能包含有限的样本。举例来说，如果数据分布是多模式的，测试中的新样本可

能跟训练中的样本来自不同的群集（Cluster），比如，在实践中将一个不同猫的分类器用于

260

8.2 行为克隆方法

区分狗的种类。由于 BC 方法将决策问题归结为一个监督学习问题，机器学习中，众所周知

的协变量漂移 (Ross et al., 2010) 的问题可能使通过监督学习方法学得的策略很脆弱，而这

对 BC 方法是一个挑战。图 8.2 进一步阐释了 BC 中的协变量漂移。

训练样本

测试样本

学习的函数

真实值

图 8.2 协变量漂移：所学的函数（虚线）对训练样本可以很好地拟合（交叉符号），但是对测试

样本（点符号）有很大的预测偏差。线是真实值

• 复合误差（Compounding Errors）：BC 方法在很大程度上受复合误差的影响，这是一种小

误差可以随时间累积而最终导致显著不同的状态分布 (Ross et al., 2011) 的现象。强化学习

任务的 MDP 性质是导致复合误差的主要因素，即连续误差的放大效应。而在 BC 方法中，

实际上在每一个时间步上产生的误差主要可能是由上面所述的协变量漂移所造成的。图 8.3

展示了复合误差。

图 8.3 在一个连续决策任务中，复合误差沿着当前策略选择的轨迹逐渐增加

8.2.2 数据集聚合

数据集聚合（Dataset Aggregation，DAgger）(Ross et al., 2011) 是一种更先进的基于 BC 方法

的从示范中模仿学习的算法，它是一种无悔的（No-Regret）迭代算法。根据先前的训练迭代过程，

它主动选择策略，在随后过程中有更大几率遇到示范样本，这使得 DAgger 成为一种更有用且高

261

第 8 章 模仿学习

效的在线模仿学习方法，可以应用于像强化学习中的连续预测问题。示范数据集 D 会在每个时

间步 i 连续地聚合新的数据集 Di，这些数据集包含当前策略在整个模仿学习过程中遇到的状态

和相应的专家动作。因此，DAgger 同样有一个缺陷，即它需要不断地与专家交互，而这在现实应

用中通常是一种苛求。DAgger 的伪代码如算法 8.29 所示，其中 π

∗ 是专家策略，而 βi 是在迭代 i

时对策略软更新（Soft-Update）的参数。

算法 8.29 DAgger

1: 初始化 D ← ∅

2: 初始化策略 πˆ1 为策略集 Π 中任意策略

3: for i = 1, 2, · · · , N do

4: πi ← βiπ

∗ + (1 − βi)ˆπi

5: 用 πi 采样几个 T 步的轨迹

6: 得到由 πi 访问的策略和专家给出的动作组成的数据集 Di = {(s, π∗

(s))}

7: 聚合数据集：D ← D ∪ Di

8: 在 D 上训练策略 πˆi+1

9: end for

10: 返回策略 πˆN+1

8.2.3 Variational Dropout

一种缓解模仿学习中泛化问题的方法是预训练并使用 Variational Dropout (Blau et al., 2018)，

来替代 BC 方法中完全克隆专家示范的行为。在这个方法中，使用示范数据集预训练（模仿学习）

得到的权重被参数化为高斯分布，并用一个确定的方差阈值来进行高斯 Dropout，然后用来初始

化强化学习策略。对于模仿学习的 Variational Dropout 方法 (Molchanov et al., 2017) 可以被看作一

种相比于在预训练的权重中加入噪声来说更高级的泛化方法，它可以减少对噪声大小选择的敏感

性，因而是一种使用模仿学习来初始化强化学习的有用技巧。

8.2.4 行为克隆的其他方法

行为克隆方法也包含了其他一些概念。比如，一些方法提供了在一个任务中将示范数据泛

化到更一般情形的方法，比如动态运动基元（Dynamic Movement Primitives，DMP）(Pastor et al.,

2009) 法，它使用一系列微分方程（Differential Equations）来表示任何记录过的运动。DMP 中

的微分方程通常包含可调整的权重，以及非线性函数来生成任意复杂运动。因此在行为克隆中，

相比于“黑盒”深度学习方法，DMP 更像是一种解析形式的解决方法。此外，有一种单样本的

（One-Shot）模仿学习方法 (Duan et al., 2017) 使用对示范数据的柔性注意力（Soft Attention）来将

模型泛化到在训练数据中未见过的情景。它是一种元学习（Meta-Learning）的方法，在多个任务

中将一个任务的一个示范映射到一个有效的策略上。相关的方法不限于此，在这里不做过多介绍。

262

8.3 逆向强化学习方法

8.3 逆向强化学习方法

8.3.1 简介

另一种主要的模仿学习方法基于逆向强化学习（Inverse Reinforcement Learning，IRL）(Ng

et al., 2000; Russell, 1998)。IRL 可以归结为解决从观察到的最优行为中提取奖励函数（Reward

Function）的问题，这些最优行为也可以表示为专家策略 πE。基于 IRL 的方法反复地在两个过程

中交替：一个是使用示范来推断一个隐藏的奖励或代价（Cost）函数，另一个是使用强化学习基

于推断的奖励函数来学习一个模仿策略。IRL 选择奖励函数 R 来最优化策略，并且使得任何脱离

于 πE 的单步选择尽可能产生更大损失。对于所有满足 |R(s)| ⩽ Rmax, ∀s 的奖励函数 R，IRL 用

以下方式选择 R∗：

R

∗ = arg max

R

X

s∈S

(Q

π

(s, aE) − max

a∈A\aE

Q

π

(s, a)) (8.4)

其中 aE = πE(s) 或 aE ∼ π(·|s) 是专家（最优的）动作。基于 IRL 的技术已经被用于许多任务，

比如操控一个直升机 (Abbeel et al., 2004) 和物体控制 (Finn et al., 2016b)。IRL (Ng et al., 2000;

Russell, 1998) 企图从观察到的最优行为，比如专家示范中提取一个奖励函数，但是这个奖励函数

可能不是唯一的（在之后有所讨论）。IRL 中一个典型的方法是使用最大因果熵（Maximum Causal

Entropy）正则化，即最大熵（Maximum Entropy, MaxEnt）IRL (Ziebart et al., 2010) 方法。MaxEnt

IRL 可以表示为以下两个步骤：

IRL(πE) = arg max

R

EπE

[R(s, a)] − RL(R) (8.5)

RL(R) = max

π

H(π(·|s)) + Eπ[R(s, a)] (8.6)

这构成了 RL ◦ IRL(πE) 策略学习架构。第一个式子 IRL(πE) 学习一个奖励函数来最大化专家策略

和强化学习策略间的奖励值差异，并且由于 Q 值是对奖励的估计，它可以被式 (8.4) 替代。第二

个式子 RL(R) 是熵正则化（Entropy-Regularized）正向强化学习，而其奖励函数 R 是第一个式子

学到的。这里的熵 H(π(·|s)) 是给定状态下的策略分布的熵函数。

关于随机变量 X 的分布 p(X) 的香农信息熵度量了这个概率分布的不确定性。

定义 8.1 一个满足 p 分布的离散随机变量 X 的信息熵为

Hp(X) = Ep(X)

[− log p(X)] = −

X

X∈X

p(X)log p(X) (8.7)

对于强化学习中随机策略的情况，表示动作分布的随机变量通常排列成一个与动作空间维数

263

第 8 章 模仿学习

相同的矢量。常用的分布有对角高斯分布和类别分布，导出它们的熵是很简单的。

代价函数 c(s, a) = −R(s, a) 也很常见，它在强化学习的过程中被最小化：

RL(c) = arg min

π

−H(π) + Eπ[c(s, a)] (8.8)

其中 H(π) = Eπ[− log π(a|s)] 是策略 π 的熵。代价函数 c(s, a) 常用作当前策略 π 的分布和示范

数据间相似度的度量。熵 H(π) 可以被视作实现最优解的唯一性的正则化项。

把上式代入 IRL 公式 (8.5) 中，我们可以将 IRL 的目标表示成 max-min 的形式，它企图在最

大化熵正则化奖励值的目标下学习一个状态 s 和动作 a 的代价函数 c(s, a)，以及进行策略 π 的

学习。

max

c

(min

π

−Eπ[− log π(a|s)] + Eπ[c(s, a)]) − EπE

[c(s, a)] (8.9)

其中 πE 表示生成专家示范的专家策略，而 π 是强化学习过程训练的策略。所学的代价函数将给

专家策略分配较高的熵而给其他策略较低的熵。

8.3.2 逆向强化学习方法的挑战

• 奖励函数的非唯一性或奖励歧义（Reward Ambiguity）：IRL 的函数搜索是病态的（Ill-Posed），

因为示范行为可以由多个奖励或代价函数导致。它始于奖励塑形（Reward Shaping）(Ng et al.,

1999) 的概念，这个概念描述了一类能保持最优策略的奖励函数变换。主要的结果是，在以

下奖励变换之下：

rˆ(s, a, s′

) = r(s, a, s′

) + γϕ(s

′

) − ϕ(s), (8.10)

最优策略对任何函数 ϕ : S → R 保持不变。只用示范数据通过 IRL 方法学到的奖励函数，

是不能消除上面一类变换下奖励函数之间分歧的。

因此，我们需要对奖励或者策略施加限制来保证示范行为最优解的唯一性。举例来说，奖

励函数通常被定义为一个状态特征的线性组合 (Abbeel et al., 2004; Ng et al., 2000) 或凸的组

合（Convex Combination）(Syed et al., 2008)。所学的策略也假设其满足最大熵 (Ziebart et al.,

2008) 或者最大因果熵 (Ziebart et al., 2010) 规则。然而，这些显式的限制对所提出方法 (Ho

et al., 2016) 的通用性有一定潜在限制。

• 较大的计算代价：IRL 可以在一般强化学习过程中通过示范和交互学到一个更好的策略。然

而，在推断出的奖励函数下，使用强化学习来优化策略要求智能体与它的环境交互，这从

时间和安全性的角度考虑都可能是要付出较大代价的。此外，IRL 的步骤主要要求智能体

在迭代优化奖励函数 (Abbeel et al., 2004; Ziebart et al., 2008) 的内循环中解决一个 MDP 问

264

8.3 逆向强化学习方法

题，而这从计算的角度也可能是有极大消耗的。然而，近来有一些方法被提出，以减轻这

个要求 (Finn et al., 2016b; Ho et al., 2016)。其中一种方法称为生成对抗模仿学习（Generative

Adversarial Imitation Learning，GAIL）(Ho et al., 2016)。

8.3.3 生成对抗模仿学习

生成对抗模仿学习（Generative Adversarial Imitation Learning，GAIL）(Ho et al., 2016) 采用了

生产对抗网络（Generative Adversarial Networks，GANs）(Goodfellow et al., 2014) 中的生成对抗方

法。相关算法可以被想成是企图引入一个对模仿者的状态-动作占用率（Occupancy）的度量，使

之与示范者的相关特性类似。它使用一个 GAN 中的辨别器（Discriminator）来给出基于示范数

据的动作-价值（Action Value）函数估计。对于一般基于动作价值函数的强化学习过程来说，动

作-价值可以通过一种生成式方法来从示范中得到：

Q(s, a) = ETi

[log(Dωi+1 (s, a))], (8.11)

其中 Ti 迭代次数为 i 时探索的样本集合，而 Dωi+1 (s, a) 是来自辨别器的输出值 Dωi+1 (s, a)，辨

别器的参数为 ωi+1。ωi+1 表示 Q 值是在更新了一步辨别器的参数过后再估计的，因此迭代次数

是 i + 1。辨别器的损失函数定义为一般形式：

Loss = ETi

[∇ω log(Dω(s, a))] + ETE

[∇ω log(1 − Dω(s, a))] (8.12)

其中 Ti，TE 分别是来自探索和专家示范的样本集合，而 ω 是辨别器的参数。图 8.4 展示了 GAIL

的结构。

图 8.4 GAIL 的结构，改编自文献 (Ho et al., 2016)

265

第 8 章 模仿学习

通过 GAIL 方法，策略可以通过由示范数据泛化得到的样本进行学习，而且相比于使用 IRL

的方法有较低的计算消耗。它也不需要在训练中跟专家进行交互，而像 DAgger 等方法可能需要

这种实际上有时难以得到的交互数据。

这种方法可以进一步推广到多模态的（Multi-Modal）策略来从多任务中学习。基于 GAN 的

多模态模仿学习 (Hausman et al., 2017) 将一个更高级的目标函数（额外的潜在指标表示不同的任

务）用于生成对抗过程中，从而自动划分来自不同任务的示范，并以模仿学习的方法学习一个多

模态策略。

根据文献 (Goodfellow et al., 2014)，如果有无限的数据和无限的计算资源，在最优情况下，以

GAIL 的目标生成的状态-动作分布应当完全匹配示范数据的状态-动作对。然而，这种方法的缺

点是，我们绕过了生成奖励的中间步骤，即我们不能从辨别器中提取奖励函数，因为 Dω(s, a) 对

于所有的 (s, a) 将收敛到 0.5。

8.3.4 生成对抗网络指导性代价学习

如上所述，GAIL 方法无法从示范数据中恢复奖励函数。一个类似的工作称为生成对抗网络

指导性代价学习（Generative Adversarial Network Guided Cost Learning，GAN-GCL），它基于 GAN

的结构来优化一个指导性代价学习（Guided Cost Learning，GCL）方法，以此来从使用示范数据

训练的最优辨别器中提取一个最优的奖励函数。我们将详细介绍该方法。

GAN-GCL 方法（具体来说 GCL 部分）是基于之前介绍的最大因果熵反向强化学习方法的，

它考虑一个熵正则化马尔可夫决策过程（Markov Decision Process，MDP）。熵正则化 MDP 对

于强化学习的目标是最大化熵正则化折扣奖励的期望（Expected Entropy-Regularized Discounted

Reward）：

π

∗ = arg max

π

Eτ∼π





X

T

t=0

γ

t

(r(St, At) + H(π(·|St)))



 , (8.13)

这是源自式 (8.5) 的用于实际学习策略的一个具体形式。可以看出最优策略 π

∗

(a|s) 给出的轨

迹分布满足 π

∗

(a|s) ∝ exp(Q∗

soft(s, a)) (Ziebart et al., 2010)，其中 Q∗

soft(St, At) = r(St, At) +

Eτ∼π[

PT

t

′=t

γ

t

′−t

(r(st

′ , at

′ ) + H(π(·|st

′ )))] 表示柔性 Q 函数（Soft Q-Function），这在柔性 ActorCritic 算法中也有用到。

IRL 问题可以被理解为解决如下一个极大似然估计（Maximum Likelihood Estimation，MLE）

问题：

max

θ

Eτ∼πE

[log pθ(τ )], (8.14)

其中 πE 是提供示范的专家策略，而 pθ(τ ) ∝ p(S0)

QT

t=0 p(St+1|St, At)e

γ

t

rθ(St,At) 以奖励函数

266

8.3 逆向强化学习方法

rθ(s, a) 的参数 θ 为参数，并且依赖 MDP 的初始状态分布和动态变化（或称状态转移）。pθ(τ ) 是

示范数据以轨迹为中心的（Trajectory-Centric）分布，这些数据是从以状态为中心的（State-Centric）

πE 得来的，即 pθ(τ ) ∼ πE。根据确定性转移过程中 p(St+1|St, At) = 1，其简化为一个基于能量

的模型 pθ(τ ) ∝ e

∑T

t=0 γ

t

rθ(St,At)

(Ziebart et al., 2008) 。参数化的奖励函数可以按照上面的目标来

优化参数 θ。与之前的过程类似，我们在这里可以引入代价函数作为累积折扣奖励（Cumulative

Discounted Rewards）cθ = −

PT

t=0 γ

t

rθ(St, At) 的负值，它也由 θ 参数化。那么 MaxEnt IRL 可以

看作是使用玻尔兹曼分布（Boltzmann Distribution）在以轨迹为中心的形式下对示范数据建模的

结果，其中由代价函数 cθ 给出的能量为

pθ(τ ) = 1

Z

exp(−cθ(τ )), (8.15)

其中 τ 是状态-动作轨迹，而 cθ(τ ) = P

t

cθ(St, At) 总的代价函数，配分函数（Partition Function）

Z 是 exp(−cθ(τ )) 对所有符合环境动态变化的轨迹的积分，用以归一化概率。对于大规模或连

续空间的情况，准确估计配分函数 Z 会很困难，因为通过动态规划（Dynamic Programming）对

Z 的精确估计只适用于小规模离散情况。否则我们需要使用近似估计的方法，比如基于采样的

（Sampling-Based）GCL 方法。

GCL 使用重要性采样（Importance Sampling）来以一个新的分布 q(τ )（原来的示范数据分布

为 p(τ )）估计 Z，并采用 MaxEnt IRL 的形式：

θ

∗ = arg min

θ

Eτ∼p[− log pθ(τ )] (8.16)

= arg min

θ

Eτ∼p[cθ(τ )] + logZ (8.17)

= arg min

θ

Eτ∼p[cθ(τ )] + log

Eτ

′∼q



exp(−cθ(τ

′

))

q(τ

′)

!

. (8.18)

其中 τ

′ 是从分布 q 采样得到的，而 q(τ

′

) 是其概率。因此 q 可以通过最小化 q(τ

′

) 和 1

Z

exp(−cθ(τ

′

))

间的 KL 散度来优化，从而更新 θ 以学习 q(τ

′

)，其等价表示如下：

q

∗ = min Eτ∼q[cθ(τ )] + Eτ∼q[log q(τ )] (8.19)

文献 (Finn et al., 2016a) 提出使用 GAN 的形式来解决上述优化问题，它使用 GAN 的结构优

化 GCL，与 GAIL 方法类似但是有不同的具体形式。

注意，GAN 中的辨别器也可以实现用一个分布去拟合另一个的功能：

D∗

(τ ) = p(τ )

p(τ ) + q(τ )

(8.20)

267

第 8 章 模仿学习

我们可以在这里将它用于 MaxEnt IRL 形式的 GCL。

Dθ(τ ) =

1

Z

exp(−cθ(τ ))

1

Z

exp(−cθ(τ )) + q(τ )

(8.21)

这产生了 GAN-GCL 方法。策略 π 被训练以最大化 Rθ(τ ) = log(1 − Dθ(τ )) − log Dθ(τ )，从而奖

励函数可以通过优化辨别器来学习。策略通过更新采样分布 q(τ ) 来学习，这个采样分布是用来

估计配分函数的。如果达到了最优情况，那么我们可以用所学的最优的代价函数 c

∗

θ = −R∗

θ

(τ ) =

−

PT

t=0 γ

t

r

∗

θ

(St, At) 来得到最优奖励函数，而最优策略可以通过 π

∗ = q

∗ 得到。GAN-GCL 为解

决 MaxEnt IRL 问题提供了一种除直接最大化似然（Maximum Likelihood）方法外的方法。

8.3.5 对抗性逆向强化学习

由于上面介绍的 GAN-GCL 是以轨迹为中心（Trajectory-Centric）的，这意味着完整的轨迹需

要被估计，相比于估计单个状态动作对会有较大的估计方差。对抗性逆向强化学习（Adversarial

Inverse Reinforcement Learning，AIRL）(Fu et al., 2017) 直接对单个状态和动作进行估计：

Dθ(s, a) = exp(fθ(s, a))

exp(fθ(s, a)) + π(a|s)

(8.22)

其中 π(a|s) 是待更新的采样分布而 fθ(s, a) 是所学的函数。配分函数在上面式子中被忽略了，而

概率值的归一性在实践中可以由 Softmax 函数或者 Sigmoid 输出激活函数来保证。经证明，在最

优情况下，f

∗

(s, a) = log π

∗

(a|s) = A∗

(s, a) 给出了最优策略的优势函数（Advantage Function）。

然而，优势函数是一个高度纠缠的奖励函数减去一个基线值的结果。文献 (Fu et al., 2017) 论证说

奖励函数从环境动态的变化中不能被鲁棒地恢复出来。因此，他们提出通过 AIRL 来从优势函数

中解纠缠（Disentangle）以得到奖励函数：

Dθ,ϕ(s, a, s′

) = exp(fθ,ϕ(s, a, s′

))

exp(fθ,ϕ(s, a, s′)) + π(a|s)

(8.23)

其中，fθ,ϕ 被限制为一个奖励拟合器 gθ 和一个塑形（Shaping）项 hϕ：

fθ,ϕ(s, a, s′

) = gθ(s, a) + γhϕ(s

′

) − hϕ(s) (8.24)

其中还需要对 hϕ 进行额外拟合。

268

8.4 从观察量进行模仿学习

8.4 从观察量进行模仿学习

首先，从观察量进行模仿学习（Imitation Learning from Observation，IfO）是在没有完整可观

察的动作的情况下进行的模仿学习。IfO 的一个例子是从视频中学习，其中物体的真实动作值是

无法单纯地通过一些帧中的信息得到的，但人类仍旧能够从视频中学习，比如模仿动作，因此，

在 IfO 相关文献中经常见到从视频中学习的例子。相比于其他前面介绍过的方法，IfO 从另一个

角度来看待模仿学习。因而，这一小节所介绍的具体方法和之前介绍的方法有不可避免的重叠之

处，但是，要注意这一小节的方法是在 IfO 的范畴之下的。当你阅读这一小节时，应当记得，这

里的 IfO 方法与其他类别的方法大多是正交的关系，因为它是从另一个角度来处理模仿学习的，

并且着重于解决不可观测动作的问题。

之前提到的算法，几乎都不能用于解决只包含部分可观测或不可观测动作的示范数据的情

况。一个对于学习这种类型的示范数据的想法是先从状态中恢复动作，再采用标准的模仿学习

算法从恢复出来的状态-动作对（State-Action Pairs）中进行策略学习。比如，文献 (Torabi et al.,

2018a) 通过学习一个状态转移（State Transition）的动态模型来恢复动作，并使用 BC 算法来找到

最优策略。然而，这种方法的性能极大地依赖于所学动态模型的好坏，对于状态转移中有噪声的

情况则很可能失败。相反，文献 (Merel et al., 2017) 提出只通过状态（或状态的特征值）轨迹来学

习。他们拓展了 GAIL 框架，并只通过采集运动示范数据的状态来学习控制策略，展示了只需要

部分状态特征而不需要示范者的具体动作对对抗式模仿（Adversarial Imitation）也是足够的。相

似地，文献 (Eysenbach et al., 2018) 指出策略应该可以控制智能体到达哪些状态，因而可通过最大

化策略和状态轨迹间的互信息（Mutual Information）来仅仅通过状态训练策略。也有一些其他研

究尝试只从观察量而不是真实状态中学习。比如，文献 (Stadie et al., 2017) 通过域自适应（Domain

Adaption）方法从观察量中提取特征来保证专家（Experts）和新手（Novices）在同一个特征空间

下。然而，只使用示范状态或状态特征在训练中可能需要大量的环境交互，因为任何来自动作的

信息都被忽略了。

为了提供 IfO 方法的一个清楚的框架，我们把文献中的 IfO 方法总结为两大类：（1）基于模

型（Model-Based）方法；（2）无模型（Model-Free）方法。这也与强化学习中的一种主要的分类

方法吻合。随后，我们讨论每一类方法的特点，并提出相关文献中的算法作为例子。

8.4.1 基于模型方法

类似于基于模型的强化学习（如第 9 章），如果环境模型可以用较低的消耗来精确学习，这

个模型可能对学习过程有利，因为通过它可以高效地做出规划。由于模仿学习在与环境交互的过

程中模仿的是一系列的动作而非单个动作，所以它难以避免地涉及环境的动态变化，而这可以通

过基于模型方法学习。根据不同的动态模型类型，基于模型的 IfO 方法可以被分类为：（1）逆向

动态模型（Inverse Dynamics Models）和（2）正向动态模型（Forward Dynamics Models）。

269

第 8 章 模仿学习

逆向动态模型：一个逆向动态模型是从状态转移 {(St, St+1)} 到动作 {At} 的映射 (Hanna

et al., 2017)。在这一类中的一个工作如文献 (Nair et al., 2017) 提出的方法，它通过人类操作绳子

从一个初始状态到目标状态的一系列图像，来学习预测绳结操作中的一系列动作，这需要学习如

下的一个像素级（Pixel-Level）的逆向动态模型：

At = Mθ(It, It+1) (8.25)

以上面的任务为例，其中 At 是通过逆向动态模型 M 以输入的一对图片 It, It+1 所预测的动作，

模型由 θ 参数化，卷积神经网络被用于学习逆向动态模型。机器人通过探索策略自动地收集绳结

操作的样本，收集到的样本被用于学习逆向动态模型，随后机器人使用所学的模型和来自人类示

范的期望状态进行规划。学到的逆向动态模型 M∗

θ 实际可以作为策略来根据期望帧 I

e 选择与示

范相似的动作：

At = M∗

θ

(It, Ie

t+1) (8.26)

另一个工作叫作增强逆向动态建模（Reinforced Inverse Dynamics Modeling，RIDM）(Pavse et al.,

2019)，它在使用预定义的探索策略所收集的样本进行训练的基础上，使用一个增强的后训练

（Post-Training）过程来微调所学的逆向动态模型。如上所述，预训练的逆向动态模型被看作是强

化学习设置下的一个智能体策略，这时可以用一个稀疏奖励函数 R 来基于强化学习对这个策略

进行微调：

θ

∗ = arg max

θ

X

t

R(St, Mpre

θ

(St, Se

t+1)) (8.27)

其中 M

pre

θ 是预训练模型，在这里通过强化学习的方式来进行微调，微调目标是最大化奖励函数

R。

协方差矩阵自适应进化策略（Covariance Matrix Adaptation Evolution Strategy，CMA-ES）或

者贝叶斯优化（Bayesian Optimization，BO）方法可以用于在低维的情况下优化模型。然而，作

者假设每个观察量转移（Observation Transition）都可以通过单个动作实现。为了消除这个不需要

的假设，文献 (Pathak et al., 2018) 允许智能体执行多个动作直到它与下一个示范帧足够接近。

上面介绍的算法试图对每个示范状态使用逆向动态模型从而实现对策略的恢复。从观察量进

行行为克隆（Behavioral Cloning from Observation，BCO）算法由文献 (Torabi et al., 2018a) 提出，

这个算法则试图使用完整的观察量-动作对（Observation-Action Pair）和所学的逆向动态模型来恢

复示范数据集，然后用常规模仿学习的形式使用这个增强后的示范数据集来学习策略，如图 8.5

所示。

270

8.4 从观察量进行模仿学习

图 8.5 从观察量进行行为克隆（Behavioral Cloning from Observation，BCO）的学习框架，改编自

文献 (Torabi et al., 2018a)

文献 (Guo et al., 2019) 提出使用一个基于张量的（Tensor-Based）模型来推理专家状态序列相

应的未观测动作（即一个 IfO 问题），如图 8.6 所示。智能体的策略通过一个结合了强化学习和模

仿学习的混合目标来优化：

θ

∗ = arg min

θ

LRL(π(a|s; θ)) − E(S

e

t

,Se

t+1)∼D[log πθ(M(S

e

t

, Se

t+1)|S

e

t

)] (8.28)

其中 LRL 是常规强化学习的损失项，其策略 π 由 θ 参数化。D 是示范数据集，而第二项是行为克隆

损失函数，用于最大化基于专家状态 s

e 和逆向动态模型 M 预测专家动作的可能性（Likelihood）。

文献 (Guo et al., 2019) 提出一种结合 RIDM 和 BCO 的方法。这里的逆向动态模型 M 是一个低秩

的（Low-Rank）张量模型，而非像上面介绍的其他方法中的参数化（Parameterized）模型，它在

某些情况下比深度神经网络有优势。类似于 RIDM，这个方法需要提供奖励信号（Reward Signals）

来得到强化学习损失函数。

图 8.6 混合强化学习和专家状态序列的学习框架，改编自文献 (Guo et al., 2019)

正向动态模型：正向动态模型是从状态-动作对 {(St, At)} 到下一个状态 {St+1} 的映射。一

271

第 8 章 模仿学习

个典型的在 IfO 中使用正向动态模型的方法叫作从观察量模仿潜在策略（Imitating Latent Policies

from Observation，ILPO）(Edwards et al., 2018)。ILPO 在其学习过程中使用两个网络：潜在策略

（Latent Policy）网络和动作重映射（Action Remapping）网络。潜在策略网络包括一个动作推理

（Action Inference）模块，它将状态 St 映射到一个潜在动作（Latent Action）z，而一个正向动态

模块根据当前状态 St 和潜在动作 z 预测下一个状态 St+1。这两个模块的更新规则如下：

ω

∗ = arg min E(S

e

t

,Se

t+1)∼D[∥Gω(S

e

t

, z) − S

e

t+1∥

2

2

] (8.29)

这是对于潜在动态模块 Gω 的，而

θ

∗ = arg max E(S

e

t

,Se

t+1)∼D

























X

z

πθ(z|S

e

t

)Gω(S

e

t

, z) − S

e

t+1





















2

2



 (8.30)

是对于潜在策略 πθ(·|z) 而言的，其中 D 是专家示范数据集。

然而，由于潜在策略网络产生的潜在动作可能并不是真正的环境动态中的真实动作，动作重

映射网络被用来将潜在动作关联到真实动作。使用潜在动作不需要在学习潜在模型和潜在策略的

过程中与环境进行交互，而动作网络重映射只需要跟环境交互有限的次数，这使得整个算法在学

习过程中很高效（Efficient）。

8.4.2 无模型方法

除了使用所学动态模型进行基于模型的 IfO 方法，也有一些无模型 IfO 方法，这属于另一个

主要的方法类别，即不使用模型进行学习。对于高度复杂的动态变化，模型可能很难学习，这与在

常规强化学习设置中的情况一样。对于无模型 IfO 有两个主要的方法：（1）生成对抗（Generative

Adversarial）方法和（2）奖励函数工程（Reward Engineering）方法。其中生成对抗方法类似于常

规模仿学习中的，但是只有状态作为示范。

生成对抗方法：一种基本的生成对抗 IfO 的框架是由之前介绍的在常规模仿学习设置下 IRL

中的 GAIL 方法改进的。辨别器（Discriminator）只判别和比较当前策略探索到的样本的状态或

专家示范数据中的状态，而非对状态-动作对进行判别，于是给出以下损失函数：

Loss = Es∼D[∇ω log(Dω(s))] + Es∼De [∇ω log(1 − Dω(s))] (8.31)

其中 D 是用当前策略探索到的样本集，而 De 是示范数据集。不同的具体算法基于以上有不同的

具体形式和修正方式。

举例来说，文献 (Merel et al., 2017) 发展了一个 GAIL 的变体，它只使用部分可观测的状态特

征而不使用动作来给人类提供类似人的（Human-Like）运动轨迹，通过 GAN 的结构。它类似于

272

8.4 从观察量进行模仿学习

基于模型的 IfO 中的 RIDM 方法和混合（Hybrid）强化学习方法，也使用了一个强化学习模块和

一个模仿学习模块，但是以一种层次化的结构使用的。强化学习模块是一个高阶的（High-Level）

控制器，它基于一个低阶的（Low-Level）控制器，这个低阶控制器使用 BC 方法来采集人类的运

动特征。状态和动作的轨迹在一个随机性策略 π 和环境的交互中被采集，这对应于 GAN 结构中

的生成器（Generator）。状态-动作对随后被转化成特征 z，其中动作可能被除去。根据原文所述，

示范数据和采集到的数据被假设在同一个特征空间（Feature Space）下。示范或生成数据由辨别

器评估来得到这个数据属于示范数据的概率。辨别器的输出值随后被用作奖励来通过强化学习更

新模仿策略，类似于 GAIL 中的式 (8.12)。如果学习多种行为的（Multi-Behavior）策略，那么可

以添加一个额外的背景变量（Context Variable）。这个辨别器的损失函数可以写作：

Loss = Ez∼s,s∼D[∇ω log(Dω(z, c))] + Ez

e∼s

e,se∼De [∇ω log(1 − Dω(z

e

, ce

))] (8.32)

其中 z, ze 是 s, se 的编码特征，而 s, se 分别来自强化学习探索得到的数据集 D 和专家示范数据

集 De，而 c, ce 是表示不同行为的背景变量。

由文献 (Henderson et al., 2018) 提出的 OptionGAN 使用分层强化学习中的选项框架（Options

Framework），从而基于只使用可观测状态的生成对抗式结构（Generative Adversarial Architecture）

来恢复奖励-策略的联合选项（Joint Reward-Policy Options），如图 8.7 所示。经过策略分解（Decomposition），它不仅可以在简单的任务上学习得好，而且对于复杂的连续控制任务也能学得一

个基于选项的一般策略（A General Policy over Options）。

图 8.7 OptionGAN 的结构，改编自文献 (Henderson et al., 2018)

273

第 8 章 模仿学习

图 8.7 中 IfO 方法的一个潜在问题是，即使所学的最优策略能够生成一个与专家策略非常类

似的状态分布，不意味着对于模仿策略和专家策略的所有状态，它相应的动作都是完全相同的。

由文献 (Torabi et al., 2019d) 提出的一个简单例子是，在一个环状的（Ring-Like）环境中，两个智

能体以相同的速度但是不同的方向移动（即一个为顺时针、另一个为逆时针），这将导致相同的

状态分布，即使它们的行为与彼此相反（即在给定状态下有不同的动作分布）。

一种解决上述动作分布不匹配问题的方法是，给辨别器输入一系列状态而非单个状态，如

文献 (Torabi et al., 2018b, 2019b) 所提出的一个相似算法，它只是将辨别器的输入改为状态转移

{(St, St+1)} 而非单个状态。这时辨别器的损失函数将变为

ED[∇ω log(Dω(St, St+1))] + EDe [∇ω log(1 − Dω(St, St+1))] (8.33)

其中状态序列在实践中也可以选择长度大于 2 的。

另一个由文献 (Torabi et al., 2019c) 提出的工作使用本体感觉（Proprioceptive）特征而非观察

到的图像作为策略的状态输入，来在强化学习智能体中构建类似于人和动物的基于本体感觉控制

（Proprioception-Based Control）的模型。由于本体感觉特征的低维性质，策略可以用一个简单的多

层感知机（Multi-Layer Perceptron，MLP），而非一个卷积神经网络（Convolutional Neural Network，

CNN）来表示，而辨别器仍旧以来自探索样本和专家示范的序列观测图像为输入，如图 8.8 所示。

低维本体感觉特征也使得整个学习过程更高效。

图 8.8 使用本体感觉状态，只从观察量进行模仿学习。图片改编自文献 (Torabi et al., 2019c)

如第 7 章中所提及的，较低的样本效率（Sample Efficiency）是当前强化学习算法的一个主

要问题，这在模仿学习和 IfO 领域中也存在。由于生成对抗的方法属于 IRL 的范畴，上面介绍

的这些方法可能有 8.3 节所提到的较大计算消耗的问题。这些对抗式模仿学习算法通常需要大

量的示范样本和迭代学习来成功学会模仿示范者的行为。为了进一步提高上述方法的样本效率，

文献 (Torabi et al., 2019a) 提出在策略学习中使用线性二次型调节器（Linear Quadratic Regulators，

LQR）(Tassa et al., 2012) 作为一种基于轨迹的（Trajectory-Centric）强化学习方法，而这有可能使

得真实机器人的模仿学习成为现实。

上述方法主要基于示范数据空间和模仿者学习的空间有一致性的基本假设。然而，当这两个

空间不匹配时，比如在三维空间中由于提供观察量的摄像机位置不同而造成的视角变化，一般的

模仿学习方法可能会有性能上的下降。示范和模仿的空间差异可能在动作空间，也可能在状态空

274

8.4 从观察量进行模仿学习

间。对于动作空间的差异，文献 (Zołna et al., 2018) 提出使用成对有任意时间间隔（Time Gaps）的

状态替代连续不断的状态（Consecutive States）来作为辨别器的输入，这可以看作是用噪声进行

数据集增强（Dataset Augmentation），从而有更鲁棒和通用的表现。在他们的实验中，这个方法

确实展示出了在模仿者策略与示范数据有不同动作空间的情况下的性能提升。而对状态空间的差

异，比如上面提及的视角变化，文献 (Stadie et al., 2017) 提出使用一个分类器（Classifier）来区分

来自不同视角的样本，将辨别器最初的几个神经网络层的输出作为分类器的输入。这个方法使用

了域混淆（Domain Confusion）的想法来学习域无关的（Domain Agnostic）特征，其中域在这种

情况下指不同的视角。在辨别器的最初神经网络层（作为一个特征提取器）混淆被最大化，但对

分类器混淆被最小化，因而这也利用了对抗式训练的框架。在训练之后，提取器（辨别器的最初

几个神经网络层）所学特征对视角变化有了不变性。

这领域也有一些其他方法。Sun et al. (2019b) 提出 IfO 中第一个可证明高效的算法，叫作正向

对抗式模仿学习（Forward Adversarial Imitation Learning，FAIL），它可以用跟所有相关参数有多

项式（Polynomial）数量关系的样本量来学习一个近最优的策略，而不依赖于单一观察量（Unique

Observations）的数量。FAIL 中的极小化极大（Minimax）方法学习一个策略，这个策略能够根

据之前时间步的策略匹配下一个状态的概率分布。近来，一个称为动作指导性对抗式模仿学习

（Action-Guided Adversarial Imitation Learning，AGAIL）由文献 (Sun et al., 2019a) 提出，它试图利

用示范中的状态和不完整动作信息，因而是 IfO 跟传统 IL 的一个结合方法。辨别器被用来区分

单个状态，类似于之前介绍的文献 (Merel et al., 2017) 的方法。此外，它还用一个指导性 Q 网络

（Guided Q-Network）来以一种监督学习的方式学习 p(a

e

|a ∼ π(s

e

)) 的真实后验（Posterior），其

中 (s

e

, ae

) 表示专家示范样本。

奖励函数工程方法：生成对抗方法自然地提供了可以让模仿策略以强化学习方式训练的奖励

信号。除了生成对抗方法，也有像奖励函数工程（Reward Engineering）的方法来解决无模型 IfO。

事实上，之前小节中提到的基于模型的 IfO 中的 RIDM 方法是一种奖励函数工程方法。这里的奖

励函数工程指需要人为设计奖励函数来以强化学习的方式从专家示范中学习模仿策略的方法。奖

励函数工程将模仿学习的监督学习方式转化为一个强化学习问题，通过给强化学习智能体构建一

个奖励函数。需要注意的是，人为设计的奖励函数不需要是真实的产生专家策略的奖励函数，而

更像是一个基于示范数据集或任务先验知识（Prior Knowledge）的估计。比如，文献 (Kimura et al.,

2018) 提出使用预测的下一个状态和示范者的下一个真实状态间的欧氏距离（Euclidean Distance）

作为奖励函数，随后根据这个奖励函数可以用一般强化学习的方式来学习一个模仿策略。

另一种奖励函数工程方法称为时间对比网络（Time-Contrastive Networks，TCN），由文献 (Sermanet et al., 2018) 提出，如图 8.9 所示。为了解决前面提及的多视角问题，而这个问题对于学习人

的行为很重要，TCN 方法通过学习一个视角不变的表示来获取物体之间的关系，它通过 TCN 网

络处理从不同视角获得的几个（原文中是两个）同步的相机视野。对抗式训练因此可以用在嵌入

式表示空间（Embedded Representation Space），而非原来的状态空间（如其他方法中所用的）。这

个表示是通过一个三重（Triplet）损失函数和 TCN 嵌入网络（Embedding Network）来学到的。这

275

第 8 章 模仿学习

视⾓1

视⾓2

视⾓（和模态）

随机的或来⾃时间 时间 近邻的极负面样本

正的 负的 ⾃监督模仿

深度⽹络

锚点（Anchor）

三重（triplet）损失

图 8.9 使用三重损失函数的时间对比网络（TCN）的学习框架，它以一种自监督式的学习，用于

只从观察量进行的模仿学习（IfO）中的观察量嵌入（Observation Embedding）。图片来自

文献 (Sermanet et al., 2018)（见彩插）

个三重损失被设定为在视频示范数据中驱散（Disperse）连续帧的短时近邻（Temporal Neighbors），

而这些近邻满足有相似的视觉特征但是不同的实际动态状态，同时吸引（Attract）那些不同视角

下同时发生的帧，这些帧在嵌入空间中有相同的动态状态。因此，模仿策略能够用无标签的人类

示范视频以自监督（Self-Supervised Learning）的方式进行学习。类似文献 (Kimura et al., 2018)

中描述的工作，奖励函数定义为同一时间步下示范状态和智能体实际状态的欧氏距离，但它是

在嵌入空间而不是状态空间。TCN 被设计成用于单帧状态嵌入（Single Frame State Embedding）。

Dwibedi et al. (2018) 扩展了 TCN 的工作，使其可以对多个帧进行嵌入，从而更好地表示轨迹的

模式（Patterns in Trajectory）。文献 (Aytar et al., 2018) 也采用了一个相似的方法，从 YouTube 视

频帧中基于示范数据来学习嵌入函数，从而解决难以探索的任务，比如 Montezuma’s Revenge 和

Pitfall，这些任务在第 7 章的探索挑战中有所提及。它可以解决较小的变化，比如视频的失真和

颜色变化。模仿者嵌入状态和示范者嵌入状态的距离测度（Measurement）也被用作奖励函数。

如之前所介绍的，可以用一个分类器来区分来自不同视角的观察量。文献 (Goo et al., 2019)

提出，分类器也可以用于预测示范数据中帧的顺序，通过一种打乱学习（Shuffle-and-Learn）的训

练方式 (Misra et al., 2016)。奖励函数可以根据所学的分类器来定义，并用于训练模拟者策略。同

时，在之前生成对抗方法的描述中，状态空间的不匹配，比如由视角不同造成，可以通过不变的

特征表示（Invariant Feature Representation）来解决。然而，它也可以用一个定义为示范状态和模

仿者状态在表征空间下的欧氏距离作为奖励函数，来训练模仿策略，而非使用辨别器并以示范状

态和模仿者状态作为输入时的输出值为奖励，这在文献 (Gupta et al., 2017; Liu et al., 2018) 中都有

提到。

276

8.5 概率性方法

8.4.3 从观察量模仿学习的挑战

根据以上所提及的 IfO 中的方法，智能体能够只从观察到的状态来学习策略，但是仍旧存在

文献 (Torabi et al., 2019d) 所提到的问题。

• 具象不匹配（Embodiment Mismatch）：具象不匹配通常用来描述外观（对于基于视觉的控

制）、动态过程和其他特征在模仿者域和示范者域间的差异。一个典型的例子是让机械臂

模仿人的手臂执行动作。由于控制动力学和观察智能体的视角会有显著的差别，所以这样

的模仿学习过程可能很难实现。即使是确认机器人和人的手臂是否在同一个状态都会有困

难。一个解决这个问题的方法是学习隐藏对应关系（Correspondences）或潜在表示（Latent

Representations），这个关系或表示能够对两个域的差异产生不变性，然后基于这个关系或者

在所学的表征空间内进行模仿学习。一个用来解决这个问题的 IfO 方法 (Gupta et al., 2017)

用自动编码器（Autoencoder）来学习不同的具象之间的对应关系以一种监督学习的方式。自

动编码器被训练使得编码后的表示对具象特征有不变性。另一个方法 (Sermanet et al., 2018)

使用少量人类监督和无监督的学习方式来学习对应关系。

• 视角差异：在上面提到的几个方法中，比如 TCN 和一些其他基于模型的 IfO 方法，对于基

于视觉的控制，由于示范数据由相机采集的图像或视频给出，视角的差异可能导致模仿策

略表现显著下降。通常来讲，需要有一个在对视角不变的（Viewpoint Invariant）空间中表

征状态的编码模型（Encoding Model），如文献 (Sieb et al., 2019) 中提到的，或者一个能够根

据某一帧预测具体视角的分类器，如文献 (Stadie et al., 2017) 所提到。另一种试图解决这个

问题的 IfO 方法是去学习一个背景转化（Context Translation）模型，从而根据一个观察量预

测它在目标背景中的表示 (Liu et al., 2018)。这个转化是通过包含源背景和目标背景下的图

像数据来学习的，而任务是将源背景转化到目标背景。这需要收集源背景和目标背景下相

似的样本来实现。

8.5 概率性方法

除了使用神经网络的参数化方法，许多概率推理方法也可以被用于模仿学习，尤其是在机器

人运动领域，这些方法包括高斯混合回归（Gaussian Mixture Regression，GMR）(Calinon, 2016)、动态

运动基元（Dynamic Movement Primitives，DMP）(Pastor et al., 2009)、概率性运动基元（Probabilistic

Movement Primitives，ProMP）(Paraschos et al., 2013)、核运动基元（Kernelized Movement Primitives，

KMP）(Huang et al., 2019)、高斯过程回归（Gaussian Process Regression，GPR）(Schneider et al.,

2010)、基于 GMR 的高斯过程 (Jaquier et al., 2019) 等。由于本书主要是介绍使用深度神经网络参

数化的深度强化学习，所以我们将仅简单介绍这些概率性方法，而将概率性方法和深度强化学习

结合起来本身就不是平庸的（Non-Trivial），不像在本章中介绍的其他方法那样直接。

然而，即使将概率性方法用于深度强化学习任务可能是不容易实现的，概率性方法由于其一

277

第 8 章 模仿学习

些优点还是很值得研究的，具体表现讨论如下。

不同于深度神经网络给出确定性的预测结果，由 GRM、ProMP 和 KMP 计算得到预测分布

的协方差矩阵（Covariance Matrices）编码了预测轨迹的变化性。而这在使用所学模型来预测或

做决策且其决策的置信度同样重要时会很有用，比如在机器人操作或车辆驾驶的情形中为了保

证安全，每个指令的可行性和风险都需要以概率模型的方式来分析。除此之外，概率性方法根据

概率论的支持通常有解析解，这与基于深度神经网络的“黑盒”优化过程不同。而这也使得概率

性方法能够在数据量较小时用较短时间求解。此外，像基于 GMR 的高斯过程类的概率性方法对

未见过的输入数据点有快速的适应能力，这在下面小节中将会讨论。对于模仿学习中的概率性方

法，数据集被默认为是以有标签数据类型来提供的，即输入和输出的配对，对于一般强化学习，

它通常是状态-动作对 {(si

, ai)|i = 0, · · · , N}，而对按时间排列的示范数据，它可以是时间-状态

对 {(t, St)|t = 0, · · · , N} (Jaquier et al., 2019)。

基于高斯混合回归（GMR）的高斯回归（GPR）是一种结合了高斯混合回归和高斯过程回

归的方法。GMR 利用了高斯条件定理（Gaussian Conditioning Theorem）来估计给定输入数据的

输出分布。高斯混合模型（Gaussian Mixture Model，GMM）通过期望最大化算法（Expectation

Maximization，EM）来拟合输入输出数据点的联合分布（Joint Distribution）。给定观察输入，基

于条件的（Conditional）均值和方差可以有封闭解，其输出结果因而可以通过基于条件的期望

的线性组合来得到，使用测试数据点作为输入。GP 如同深度神经网络一样，是针对学习确定性

（Deterministic）输入-输出关系问题的方法，它基于可能的目标函数的高斯先验（Prior）来计算。

基于 GMR 的 GP（GMR-Based GP）是种结合的方法，它的 GP 先验均值等于 GMR 模型基于条件

的均值，而 GP 的核（Kernel）是相应 GMM 各个组分单独的核的叠加。这种结合使得基于 GMR

的 GP 方法有 GP 通过均值和核来编码多种先验置信（Prior Beliefs）的能力，并且允许 GMR 估

计的多样化信息被封装到 GP 的不确定性（Uncertainty）估计中。当给出新的未见过的输入观察

数据点时，基于 GMR 的 GP 能够快速适应它们并给出合理预测输出，如图 8.10 所示。对于一个

图 8.10 模仿学习中基于 GMR 的 GP 方法。左边图中，先验均值为蓝色，采样轨迹为紫色。右边

图中，先验均值（与左图相同）为蓝色，采样轨迹为粉色，预测轨迹为红色，有三个黑

色的点为新观察量。图片来自文献 (Jaquier et al., 2019)（见彩插）

278

8.6 模仿学习作为强化学习的初始化

二维轨迹的估计过程，图 8.10 中的左边用紫色线展示了所给的样本，而蓝色线展示了先验均值。

右边的图是基于 GMR 的 GP 方法，其中有 3 个新的观察数据点被标为黑色，粉色线展示了采样

轨迹，而红色线是预测轨迹。这个方法经证实对使用示范数据进行学习并快速适应到新的数据点

的情况有很好的表现，而这可以用于操作机器人基于示范规避障碍物。

8.6 模仿学习作为强化学习的初始化

使用模仿学习的基本设定是在不使用任何强化信号而只有示范数据的情况下学习一个策略，

这意味着通过模仿学习所学策略是来自示范数据的最终策略。然而，在实际中，来自模仿学习的

策略通常没有足够的泛化能力，尤其是对于未见过的情况。因此，我们可以在强化学习的过程中

使用模仿学习，以此来提高强化学习的效率。举例来说，使用示范数据的预训练策略可以用来初

始化强化学习的策略。关于这些方法的细节将在随后讨论。因此，我们并不需要模仿学习给出的

策略是最优的，而是通过一个相对简单的学习过程得到一个足够好的策略，比如使用监督学习的

模仿学习方法。所以，我们在下面只选择一些简单直接的方法来作为后续强化学习过程的初始化

方法。模仿学习中更精致的方法毫无疑问会成为更好的初始化策略，但是也会相应带来如较长的

预训练时间等缺点。

总体来说，通过监督学习方式模仿示范数据而学到的策略，可以使用包括 BC、DAgger、Variational Dropout 等方法，它们被看作是对强化学习策略较好的初始化，具体地，通过下面小节中

描述的策略替换（Policy Replacement）或者残差策略学习（Residual Policy Learning）方法。

除了用策略替换来初始化强化学习（模拟学习策略在强化学习初始时替换其策略），残差策

略学习 (Johannink et al., 2019; Silver et al., 2018) 是另一种实现初始化的方法。比如对于机器人控

制任务，它通常基于一个较好但是不完美的控制器，并以这个初始控制器为基础学习一个残差策

略。对于现实世界的机器人控制，初始控制器可以是一个模拟器中预训练的策略；对于模拟的机

器人控制，初始控制器可以用监督学习的方式基于专家轨迹预训练得到，如 8.2 节中的方法。

残差策略学习中的动作遵循结合式策略，即由初始策略（Initial Policy）πini 和残差策略 πres

求和得到：

a = πini(s) + πres(s). (8.34)

通过这种方式，残差策略学习能够尽可能地保持初始策略的表现。

例子：使用 DDPG 的残差策略学习

这里我们使用深度确定性策略梯度（DDPG）算法来实现基于示范的残差策略学习。根据残

差策略学习方法，DDPG 中的行动者（Actor）策略将包含两部分：一个是预训练得到的初始策略，

在初始化后将被固定；另一个是后面学习过程中将训练的残差策略。初始策略通过模仿学习根据

279

第 8 章 模仿学习

示范数据训练得到。这个预训练的初始策略只用于 DDPG 的行动者部分。基于 DDPG 算法使用

残差策略学习的过程如下：

（1）以残差学习的方式初始化 DDPG 中的所有网络，包括对批判者（Critic）、目标批判者

（Target Critic）的一般初始化，以及对残差策略（Residual Policy）和目标残差策略（Target Residual

Policy）的最后网络层（Final Layers）进行零值初始化，还有将通过模仿学习得到的策略作为初

始策略和目标初始策略（Target Initial Policy），一共是六个网络。这时固定住初始策略和目标策

略，开始训练过程。

（2）让智能体与环境交互，动作值是初始化策略和残差策略的和值：a = aini + ares；将样本

以 (s, ares, s′

, r, done) 的形式存储。

（3）从经验回放缓存中采样 (s, ares, s′

, r, done)，有

Qtarget(s, ares) = r + γQT(s, πT

res(s)) (8.35)

其中QT, πT

res 分别表示目标批判者和目标残差策略。批判损失函数是MSE(Qtarget(s, ares), Q(s, ares))。

行动者的目标是最大化状态 s 和动作 ares 的动作价值函数，如下：

max

θ

Q(s, ares) = max

θ

Q(s, πres(s|θ)) (8.36)

这可以通过确定性策略梯度（Deterministic Policy Gradient）来优化。

（4）重复上面的第（2）（3）步，直到策略收敛到接近最优。

对比一般的 DDPG 算法，使用残差策略学习的不同只是对残差策略的动作 ares，而非智能体

的整个动作 a 来学习动作价值函数和策略。

8.7 强化学习中利用示范数据的其他方法

8.7.1 将示范数据导入经验回放缓存

基于示范的深度 Q-Learning（Deep Q-Learning from Demonstrations，DQfD）(Hester et al., 2018)

通过直接将专家轨迹导入离线（Off-Policy）强化学习的记忆缓存（Memory Buffer）中来利用示范

数据，而非预训练一个策略来初始化强化学习策略。它使用 DQN 来解决只有离散动作空间的应

用。DQfD 使用一个由所有专家示范初始化的经验回放缓存（Experience Replay Buffer），并不断

向其添加采集到的新样本。DQfD 使用优先经验回放（Prioritized Experience Replay）(Schaul et al.,

2015) 来从回放缓存中采样训练批，且它使用一个监督式折页损失函数（Hinge Loss）来模仿示范

数据和一个一般的 TD 损失函数的结合来训练策略。

基于示范的深度确定性策略梯度（Deep Deterministic Policy Gradient from Demonstrations，

DDPGfD）(Večerík et al., 2017) 是一种与上面 DQfD 类似的方法，但是使用 DDPG 来处理连续动作

280

8.7 强化学习中利用示范数据的其他方法

空间的应用。DDPGfD 通过直接将专家策略输入离线强化学习（即 DDPG）的缓存来利用示范数

据，从而通过示范和探索数据一同训练策略。优先经验回放被用来平衡两种训练数据。DDPGfD

可以用于强化学习中的简单、易解决的任务，而对从稀疏奖励学习等较难任务需要在训练中进行

更积极的探索。

文献 (Nair et al., 2018) 提出一个基于 DQfD 和 DDPGfD 的方法，对较难的任务有更好的学习

效率，这些任务需要基于示范数据进一步探索去解决。它的策略损失函数是策略梯度损失（Policy

Gradient Loss）和行为克隆损失（Behavioral Cloning Loss）的结合，其梯度如下：

λ1∇θJ − λ2∇θLBC (8.37)

其中 J 是一般的强化学习目标（最大化的），而 LBC（最小化的）是本章开始时定义的行为克隆

损失。

此外，这个方法也使用了 Q-Filter 技术，它要求行为克隆损失函数只用于部分状态，在这些

状态下所学的批判者 Q(s, a) 判定示范者动作比行动者动作更好：

LBC =

X

ND

i=1

∥π(si

|θπ) − ai∥

21Q(si,ai)>Q(si,π(si)) (8.38)

其中 ND 是示范数据集中样本的数量，而 (si

, ai) 是从示范数据集中采样得到的。这保证了策略

能够探索到更好的动作，而不是被示范数据所限制。

以同样的方式，QT-Opt (Kalashnikov et al., 2018) 和分位数 QT-Opt（Quantile QT-Opt）(Bodnar

et al., 2019) 算法也使用在线缓存和离线示范缓存混合的方式来实现离线学习，通过一种无行动者

（Actor-Free）的交叉熵方法和 DQN，可以在现实世界中基于图像的机器人学习任务上达到当时最

先进的（State-of-the-Art）表现。

8.7.2 标准化 Actor-Critic

标准化 Actor-Critic（Normalized Actor-Critic，NAC）(Gao et al., 2018) 是另一个利用示范数据

来进行高效强化学习的方法，它先预训练一个策略作为改进强化学习过程的初始化。NAC 与其

他方法的差异是它在使用示范数据预训练初始化策略和改进强化学习的过程中使用完全相同的

目标函数，这使得 NAC 对包含次优样本的示范数据也表现得很鲁棒。

另一方面，NAC 方法类似于 DDPGfD 和 DQfD 方法，但是它依次使用示范数据和交互样本

进行训练，而不是同时使用这两类样本数据。

281

第 8 章 模仿学习

8.7.3 用示范数据进行奖励塑形

用示范数据进行奖励塑形（Reward Shaping with Demonstrations）(Brys et al., 2015) 是一个专

注于初始化强化学习中价值函数而非动作策略的方法。它给智能体提供了一个中间的奖励来丰富

稀疏奖励信号：

RF (s, a, s′

) = R(s, a, s′

) + F

D(s, a, s′

) (8.39)

其中基于示范数据 D 的塑形奖励 F

D 通过势函数 ϕ 来定义并保证其收敛性，其形式如下：

F

D(s, a, s′

, a′

) = γϕD(s

′

, a′

) − ϕ

D(s, a) (8.40)

而 ϕ

D 定义为

ϕ

D(s, a) = max

(s

d,a)

e

− 1

2

(s−s

d

)

TΣ−1

(s−s

d

)

(8.41)

它被用来最大化最接近示范状态 s

d 的状态 s 的势值。优化后的势函数被用来初始化强化学习中

的动作价值函数 Q：

Q0(s, a) = ϕ

D(s, a) (8.42)

奖励塑形的直观理解是使探索到的样本倾向于那些等于或接近示范数据的状态-动作对，从而加速

强化学习的训练过程。奖励塑形提供了一种在强化学习过程中初始化价值估计函数的较好方式。

其他方法像无监督感知奖励（Unsupervised Perceptual Rewards）(Sermanet et al., 2016) 也用

于通过示范数据学习一个密集且平滑的奖励函数，使用的是一个预训练的深度学习模型得出的

特征。

8.8 总结

由于第 7 章中提到的强化学习低学习效率的挑战，我们介绍模仿学习来作为一种可能的解决

方案，它需要使用专家示范。本章整体可以总结为几个主要类别。8.2 节中介绍的行为克隆方法

是以监督学习方式进行模仿学习的最直接方法，它可以进一步与强化学习结合，比如 8.6 节中介

绍的将其作为强化学习的初始化。一个更先进的结合模仿学习和强化学习的方式是通过 IRL 来

显式或隐式地从示范中恢复奖励函数，如 8.3 节所介绍的。像 MaxEnt IRL 方法可以显式地学习

奖励函数，但是可能有较大计算消耗。其他的生成对抗式方法，如 GAIL、GAN-GCL、AIRL 则

能更高效地学习奖励函数和策略。另一个问题是如果示范数据集中的动作是缺失的，比如只从视

频中学习，那么怎样合理地进行模仿学习？这实际是 IfO 的研究范畴，如 8.4 节所介绍。由于 IfO

282

参考文献

问题是从另一个角度来看模仿学习的，之前介绍的方法像 BC、IRL 同样可以经过适当修改用于

IfO。IfO 中的方法基本可以总结为基于模型和无模型两类。基于模型的方法从样本中学习动态模

型，而且它可以通过模型中状态-动作关系从只有观察量的示范数据中恢复动作，以显式或者隐

式的方法。随后，如果动作被显式地恢复了，就可以使用常规的模仿学习方法。像 RIDM、BCO、

ILPO 等方法属于这个基于模型的 IfO 范畴。对于 IfO 中的无模型方法，奖励函数工程或者生成对

抗式方法可以用来提供奖励函数从而进行强化学习。像 OptionGAN、FAIL、AGAIL 等方法属于

生成对抗式 IfO，而 TCN 和一些其他方法属于 IfO 的奖励函数工程一类。这里对 IfO 的两个类别

实际对一般的模仿学习也适用，比如 GAIL 是一种生成对抗式方法，而最近提出的对比正向动态

（Contrastive Forward Dynamics，CFD）(Jeong et al., 2019) 是模仿学习的一种从观察量和动作示范

中学习的奖励函数工程方法。概率性方法包括 GMR、GPR 和基于 GMR 的 GP 方法作为一般的模

仿学习方法而在本章中有所介绍，它们对于相对低维的情况有较高的学习效率，如 8.5 节所讨论

的。最终，一些其他方法像 DDPGfD 和 DQfD 将示范数据直接导入离线强化学习的回放缓存中，

等等，都在 8.7 节中介绍。模仿学习作为一种解决学习问题的高效方式，可以与强化学习有机结

合，相关研究领域依然十分活跃，

参考文献

ABBEEL P, NG A Y, 2004. Apprenticeship learning via inverse reinforcement learning[C]//Proceedings

of the twenty-first international conference on Machine learning. ACM: 1.

AYTAR Y, PFAFF T, BUDDEN D, et al., 2018. Playing hard exploration games by watching youtube[C]//

Advances in Neural Information Processing Systems. 2930-2941.

BLAU T, OTT L, RAMOS F, 2018. Improving reinforcement learning pre-training with variational

dropout[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE:

4115-4122.

BODNAR C, LI A, HAUSMAN K, et al., 2019. Quantile QT-Opt for risk-aware vision-based robotic

grasping[J]. arXiv preprint arXiv:1910.02787.

BRYS T, HARUTYUNYAN A, SUAY H B, et al., 2015. Reinforcement learning from demonstration

through shaping[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence.

CALINON S, 2016. A tutorial on task-parameterized movement learning and retrieval[J]. Intelligent

Service Robotics, 9(1): 1-29.

DUAN Y, ANDRYCHOWICZ M, STADIE B, et al., 2017. One-shot imitation learning[C]//Advances in

Neural Information Processing Systems. 1087-1098.

283

第 8 章 模仿学习

DWIBEDI D, TOMPSON J, LYNCH C, et al., 2018. Learning actionable representations from visual

observations[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).

IEEE: 1577-1584.

EDWARDS A D, SAHNI H, SCHROECKER Y, et al., 2018. Imitating latent policies from observation[J].

arXiv preprint arXiv:1805.07914.

EYSENBACH B, GUPTA A, IBARZ J, et al., 2018. Diversity is all you need: Learning skills without a

reward function[J]. arXiv preprint arXiv:1802.06070.

FINN C, CHRISTIANO P, ABBEEL P, et al., 2016a. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models[J]. arXiv preprint arXiv:1611.03852.

FINN C, LEVINE S, ABBEEL P, 2016b. Guided cost learning: Deep inverse optimal control via policy

optimization[C]//International Conference on Machine Learning. 49-58.

FU J, LUO K, LEVINE S, 2017. Learning robust rewards with adversarial inverse reinforcement learning[J]. arXiv preprint arXiv:1710.11248.

GAO Y, LIN J, YU F, et al., 2018. Reinforcement learning from imperfect demonstrations[J]. arXiv

preprint arXiv:1802.05313.

GOOW, NIEKUM S, 2019. One-shot learning of multi-step tasks from observation via activity localization

in auxiliary video[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE:

7755-7761.

GOODFELLOW I, POUGET-ABADIE J, MIRZA M, et al., 2014. Generative Adversarial Nets[C]//

Proceedings of the Neural Information Processing Systems (Advances in Neural Information Processing

Systems) Conference.

GUO X, CHANG S, YU M, et al., 2019. Hybrid reinforcement learning with expert state sequences[J].

arXiv preprint arXiv:1903.04110.

GUPTA A, DEVIN C, LIU Y, et al., 2017. Learning invariant feature spaces to transfer skills with

reinforcement learning[J]. arXiv preprint arXiv:1703.02949.

HANNA J P, STONE P, 2017. Grounded action transformation for robot learning in simulation[C]//

Thirty-First AAAI Conference on Artificial Intelligence.

HAUSMAN K, CHEBOTAR Y, SCHAAL S, et al., 2017. Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets[C]//Advances in Neural Information Processing

Systems. 1235-1245.

284

参考文献

HENDERSON P, CHANG W D, BACON P L, et al., 2018. OptionGAN: Learning joint reward-policy options using generative adversarial inverse reinforcement learning[C]//Thirty-Second AAAI Conference

on Artificial Intelligence.

HESTER T, VECERIK M, PIETQUIN O, et al., 2018. Deep Q-learning from demonstrations[C]//

Thirty-Second AAAI Conference on Artificial Intelligence.

HO J, ERMON S, 2016. Generative adversarial imitation learning[C]//Advances in Neural Information

Processing Systems. 4565-4573.

HUANG Y, ROZO L, SILVÉRIO J, et al., 2019. Kernelized movement primitives[J]. The International

Journal of Robotics Research, 38(7): 833-852.

JAQUIER N, GINSBOURGER D, CALINON S, 2019. Learning from demonstration with model-based

gaussian process[J]. arXiv preprint arXiv:1910.05005.

JEONG R, AYTAR Y, KHOSID D, et al., 2019. Self-supervised sim-to-real adaptation for visual robotic

manipulation[J]. arXiv preprint arXiv:1910.09470.

JOHANNINK T, BAHL S, NAIR A, et al., 2019. Residual reinforcement learning for robot control[C]//

2019 International Conference on Robotics and Automation (ICRA). IEEE: 6023-6029.

KALASHNIKOV D, IRPAN A, PASTOR P, et al., 2018. Qt-opt: Scalable deep reinforcement learning

for vision-based robotic manipulation[J]. arXiv preprint arXiv:1806.10293.

KIMURA D, CHAUDHURY S, TACHIBANA R, et al., 2018. Internal model from observations for

reward shaping[J]. arXiv preprint arXiv:1806.01267.

LIU Y, GUPTA A, ABBEEL P, et al., 2018. Imitation from observation: Learning to imitate behaviors from

raw video via context translation[C]//2018 IEEE International Conference on Robotics and Automation

(ICRA). IEEE: 1118-1125.

MEREL J, TASSA Y, SRINIVASAN S, et al., 2017. Learning human behaviors from motion capture by

adversarial imitation[J]. arXiv preprint arXiv:1707.02201.

MISRA I, ZITNICK C L, HEBERT M, 2016. Shuffle and learn: unsupervised learning using temporal

order verification[C]//European Conference on Computer Vision. Springer: 527-544.

MOLCHANOV D, ASHUKHA A, VETROV D, 2017. Variational dropout sparsifies deep neural networks[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.

org: 2498-2507.

285

第 8 章 模仿学习

NAIR A, CHEN D, AGRAWAL P, et al., 2017. Combining self-supervised learning and imitation for

vision-based rope manipulation[C]//2017 IEEE International Conference on Robotics and Automation

(ICRA). IEEE: 2146-2153.

NAIR A, MCGREW B, ANDRYCHOWICZ M, et al., 2018. Overcoming exploration in reinforcement

learning with demonstrations[C]//2018 IEEE International Conference on Robotics and Automation

(ICRA). IEEE: 6292-6299.

NG A Y, HARADA D, RUSSELL S, 1999. Policy invariance under reward transformations: Theory and

application to reward shaping[C]//Proceedings of the International Conference on Machine Learning

(ICML): volume 99. 278-287.

NG A Y, RUSSELL S J, et al., 2000. Algorithms for inverse reinforcement learning.[C]//Proceedings of

the International Conference on Machine Learning (ICML): volume 1. 2.

PARASCHOS A, DANIEL C, PETERS J R, et al., 2013. Probabilistic movement primitives[C]//Advances

in Neural Information Processing Systems. 2616-2624.

PASTOR P, HOFFMANN H, ASFOUR T, et al., 2009. Learning and generalization of motor skills by

learning from demonstration[C]//2009 IEEE International Conference on Robotics and Automation.

IEEE: 763-768.

PATHAK D, MAHMOUDIEH P, LUO G, et al., 2018. Zero-shot visual imitation[C]//Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2050-2053.

PAVSE B S, TORABI F, HANNA J P, et al., 2019. Ridm: Reinforced inverse dynamics modeling for

learning from a single observed demonstration[J]. arXiv preprint arXiv:1906.07372.

PUTERMAN M L, 2014. Markov decision processes: Discrete stochastic dynamic programming[M].

John Wiley & Sons.

ROSS S, BAGNELL D, 2010. Efficient reductions for imitation learning[C]//Proceedings of the thirteenth

international conference on artificial intelligence and statistics. 661-668.

ROSS S, GORDON G, BAGNELL D, 2011. A reduction of imitation learning and structured prediction

to no-regret online learning[C]//Proceedings of the fourteenth international conference on artificial

intelligence and statistics. 627-635.

RUSSELL S J, 1998. Learning agents for uncertain environments[C]//COLT: volume 98. 101-103.

SCHAUL T, QUAN J, ANTONOGLOU I, et al., 2015. Prioritized experience replay[C]//arXiv preprint

arXiv:1511.05952.

286

参考文献

SCHNEIDER M, ERTEL W, 2010. Robot learning by demonstration with local gaussian process regression[C]//2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE: 255-260.

SERMANET P, XU K, LEVINE S, 2016. Unsupervised perceptual rewards for imitation learning[J].

arXiv preprint arXiv:1612.06699.

SERMANET P, LYNCH C, CHEBOTAR Y, et al., 2018. Time-contrastive networks: Self-supervised

learning from video[C]//2018 IEEE International Conference on Robotics and Automation (ICRA).

IEEE: 1134-1141.

SIEB M, XIAN Z, HUANG A, et al., 2019. Graph-structured visual imitation[J]. arXiv preprint

arXiv:1907.05518.

SILVER T, ALLEN K, TENENBAUM J, et al., 2018. Residual policy learning[J]. arXiv preprint

arXiv:1812.06298.

STADIE B C, ABBEEL P, SUTSKEVER I, 2017. Third-person imitation learning[J]. arXiv preprint

arXiv:1703.01703.

SUN M, MA X, 2019a. Adversarial imitation learning from incomplete demonstrations[J]. arXiv preprint

arXiv:1905.12310.

SUN W, VEMULA A, BOOTS B, et al., 2019b. Provably efficient imitation learning from observation

alone[J]. arXiv preprint arXiv:1905.10948.

SYED U, BOWLING M, SCHAPIRE R E, 2008. Apprenticeship learning using linear programming[C]//

Proceedings of the 25th international conference on Machine learning. ACM: 1032-1039.

TASSA Y, EREZ T, TODOROV E, 2012. Synthesis and stabilization of complex behaviors through

online trajectory optimization[C]//2012 IEEE/RSJ International Conference on Intelligent Robots and

Systems. IEEE: 4906-4913.

TORABI F, WARNELL G, STONE P, 2018a. Behavioral cloning from observation[J]. arXiv preprint

arXiv:1805.01954.

TORABI F, WARNELL G, STONE P, 2018b. Generative adversarial imitation from observation[J]. arXiv

preprint arXiv:1807.06158.

TORABI F, GEIGER S, WARNELL G, et al., 2019a. Sample-efficient adversarial imitation learning from

observation[J]. arXiv preprint arXiv:1906.07374.

287

第 8 章 模仿学习

TORABI F, WARNELL G, STONE P, 2019b. Adversarial imitation learning from state-only demonstrations[C]//Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent

Systems. International Foundation for Autonomous Agents and Multiagent Systems: 2229-2231.

TORABI F, WARNELL G, STONE P, 2019c. Imitation learning from video by leveraging proprioception[J]. arXiv preprint arXiv:1905.09335.

TORABI F, WARNELL G, STONE P, 2019d. Recent advances in imitation learning from observation[J].

arXiv preprint arXiv:1905.13566.

VEČERÍK M, HESTER T, SCHOLZ J, et al., 2017. Leveraging demonstrations for deep reinforcement

learning on robotics problems with sparse rewards[J]. arXiv preprint arXiv:1707.08817.

ZIEBART B D, MAAS A L, BAGNELL J A, et al., 2008. Maximum entropy inverse reinforcement

learning.[C]//Proceedings of the AAAI Conference on Artificial Intelligence: volume 8. Chicago, IL,

USA: 1433-1438.

ZIEBART B D, BAGNELL J A, DEY A K, 2010. Modeling interaction via the principle of maximum

causal entropy[J].

ZOŁNA K, ROSTAMZADEH N, BENGIO Y, et al., 2018. Reinforced imitation learning from observations[J].

288

9 集成学习与规划

在本章中，我们将从学习和规划的角度进一步分析强化学习。我们首先将介绍基于模型和无

模型强化学习的概念，并着重介绍模型规划的优势。为了在强化学习中充分利用基于模型和无模

型方法，我们将介绍集成学习和规划的架构，并详细阐述应用其架构的 Dyna-Q 算法。最终，将

进一步详细分析集成学习和规划的基于模拟的搜索应用。

9.1 简介

在强化学习中，智能体可以和环境进行交互。智能体在每一轮交互中收集到的信息可以称为

智能体的经验，这能帮助智能体提升自身的决策策略。一般来说，学习指代智能体决策策略基于

实际和环境的交互逐渐提升的过程。直接策略学习是最为基本的学习方式，如图 9.1 所示，其中，

智能体首先根据当前的决策策略在环境中制定动作，环境会基于智能体当前的状态和动作反馈给

图 9.1 直接策略学习

289

第 9 章 集成学习与规划

智能体所得到的收益，使其能够评估当前策略的表现并帮助智能体探索如何进一步提升策略。然

而，直接策略学习是基于智能体在环境中每一个单步动作所产生的经验，由于环境的随机性和不

确定性，基于单步动作的经验会使学习结果存在很大方差，大大影响了学习的速度和质量。

为了提高学习效率，在策略学习的每一个学习周期中，积累多轮和环境的交互作为智能体的

经验是很有帮助的。通过在环境中进行演算（Roll-out）收集多轮交互信息，即在环境中根据当前

的状态和决策策略形成一条具体的包含一系列状态、动作和奖励信息的探索轨迹。在一般的无模

型学习中，智能体将在真实的环境中在线演算，并将获得的多轮交互信息用于策略学习。

然而，在环境中通过在线演算产生经验的成本很高。例如，在工业界的应用中，一些状态可

以指代系统崩溃或者设备爆炸，这些状态在策略学习的探索过程中是十分危险的。另外，在实际

环境中只能顺序演算，不能并行计算，这导致其采样效率和学习速度都很低。因而，在一些场景

下，我们希望能够使用模拟环境来取代实际环境进行探索和经验积累。在模拟环境中的演算被称

为规划（Planning），可通过并行计算高效地为策略学习产生大量模拟经验。为了在规划中使用有

效的模拟环境，基于模型的方法得以提出。

9.2 基于模型的方法

为了能够实行规划，模型的概念将在智能体和环境之间产生 (Kaiser et al., 2019)，如图 9.2 所

示，当智能体在状态 St 采取决策动作 At 时，环境会为模型给予反馈奖励 Rt+1 并使智能体进入

下一状态 St+1。根据智能体和环境之间收集到的经验信息，我们将 St+1 和 (St, At) 之间的映射

关系称为转移模型，并将 Rt+1 和 (St, At) 之间的映射关系称为奖励模型。当状态不能完全被观察

信息表示时，还将设定观察模型 M(Ot|St) 和表示模型 M(St+1|St, At, Ot+1) (Hafner et al., 2019)，

其中 Ot 表示在状态 St 下第 t 步所对应的观察信息。例如，捕捉到的关乎物体运动的图片属于观

察信息，可以体现该物体蕴含的所处状态信息。后面，为了集中分析其中的转移模型和奖励模型，

我们假设状态是完全可观测的。我们将转移模型和奖励模型分别由方程 Fs 和 Fr 表示：

图 9.2 基于模型的强化学习方法

290

9.2 基于模型的方法

St+1 ∼ Fs (St, At), (9.1)

Rt+1 = Fr (St, At). (9.2)

模型学习是一个监督式的拟合学习过程，目标是建立一个虚拟的环境，其中的转移关系和奖

励关系和真实环境保持一致。因而，基于对真实环境的了解，我们可以使用一个环境模型使智能

体在其中进行规划，然后将收集到的经验信息用于帮助其策略学习。

在不同的应用场景中，模型学习和策略学习的关系是多样的，具体如下所述。

• 直接学习：如果智能体已经基于规则或专家信息和环境交互过多次，那么之前收集到的经

验信息可以直接用来进行模型学习。当模型学习完成时，智能体可以将训练后的模型当作

模拟的环境，并与其交互帮助其进行策略学习。

• 迭代学习：如果模型在初始时并没有足够的数据进行学习，那么模型学习和策略学习可以

迭代交替进行。基于当前智能体和环境交互产生的有限信息，模型可以学习真实环境中部

分且有限的信息。智能体在基于有限学习产生的模拟环境进行规划并以此训练参数，且其

策略表现得到了少许提升后，将用更新的策略在真实环境中交互，并将收集到的经验信息

进一步用于对模型的学习。随着迭代次数的增加，模型学习和策略学习将逐步收敛到最优

结果。因此，模型学习和策略学习可以相互辅助而进行有效的学习。

因此，基于模型的强化学习将通过对真实环境的学习建立一个模拟环境的模型，并在其中进

行规划，使智能体更好地进行策略学习。模型学习的优势可列举如下：

• 由于规划可以在智能体和模型之间完成，智能体不需要在真实环境中采取大量的决策动作

进行探索和策略学习。因而，和成本高并且需要在线采取动作的真实环境相比，基于模型

的方法能够有效地降低训练时间并且保障在策略学习过程中的安全性。例如，在真实环境

中，机器人完成任务需要实际操作，在 QT-Opt (Kalashnikov et al., 2018) 方法中，为了完成

抓取的任务，7 个机器人需要昼夜不停地在实际环境中收集采样数据。然而一个模拟的环

境（通过学习或人工建立）可以用来节约大量的时间并且降低机器人的磨损。

• 当策略学习在智能体和模拟模型之间进行时，学习过程可以采用并行计算。在分布式系统

中可以存在多个学习者合作同时进行策略学习，其中每个学习者可以和一个根据真实环境

模拟的模型进行交互，从而所有学习者都可以在其对应的模型进行规划。模型之间是相互

独立的，并且不会影响到真实环境中所处的状态信息。因此，具有并行性的策略学习大大

提高了学习效率，且增大了可学习问题的规模。

然而，基于模型的强化学习的结构同样也存在缺点和不足：

• 在基于模型的强化学习中，模型学习的表现将会影响策略学习的结果。对于复杂且动态的

环境场景，如果学习到的模型不能很好地模拟出真实环境，智能体在规划中会和一个错误

且不准确的模型进行交互，从而将增大策略学习的误差。

• 如果真实环境有更新或者调整，模型需要通过多次迭代之后才会学到环境的变化，然后还需

要耗费大量训练时间使智能体学习并调整其策略。因此，对于在线学习中真实环境的变化，

291

第 9 章 集成学习与规划

智能体对其策略的相应调整有着很高的延迟，这并不适用于那些对实时性有要求的应用。

9.3 集成模式架构

综合无模型和基于模型的强化学习方法的优劣，集成学习和规划的过程可以很好地将无模型

和基于模型的方法结合在一起。对于不同的应用场景，集成学习和规划的方法和架构是不同的。

一般来说，在无模型的方法中，智能体仅在与真实环境的交互中得到真实的经验，没有采用

规划辅助其策略的学习和提升。在基本的基于模型的方法中，首先将通过智能体和真实环境的交

互进行模型学习，然后基于学到的模型，智能体将迭代式采取规划并用收集到的经验进行策略

学习。

由于模型处于智能体和环境之间，在智能体策略学习中，经验来源可以分为如下两类：

• 真实经验：真实经验是从智能体和真实环境中直接采样获得的。一般来说，真实经验体现

了环境正确的特征和属性，但获得成本较高，并且在真实环境中的探索不可逆且难以人工

干预。

• 模拟经验：模拟经验是从模型规划过程中获得的，可能不能准确地表现真实环境的真实特

征，但模型很容易人工操纵，并且可以通过模型学习减小模型和真实环境的误差。

对于策略学习，如果我们能够同时考虑真实经验和模拟经验，那么就能结合无模型和基于模

型的方法的优势，提高学习的效率和准确性。Dyna 架构在 (Sutton, 1991) 中提出。如图 9.3 所示，

根据基础的基于模型的方法，在策略学习中，智能体不仅从已经学到的模型所提供的模拟经验中

更新策略，并且考虑了与真实环境交互所收集到的真实经验。因此，在策略学习中，模拟经验能

够保证学习过程中有足够多的训练数据来降低学习方差，另外，真实经验能够更准确地体现环境

的动态变化和正确特征，从而降低由于环境而产生的学习偏差。

图 9.3 Dyna 架构

基于此架构，Dyna-Q 算法得以进一步提出，如算法 9.30 所述。Dyna-Q 算法将建立并维护一

个 Q 表格，据此指导智能体做出动作决策。在每个学习周期中，Q 表格通过智能体和真实环境的

292

9.4 基于模拟的搜索

交互中学习更新，模拟的模型同时也会从真实经验中学习，并且通过规划获得 n 组模拟经验用于

进一步的 Q 表格学习。因此，随着学习周期的增加，Q 表格能够学习并收敛到最佳的结果。

算法 9.30 Dyna-Q

初始化 Q(s, a) 和 Model(s, a)，其中 s ∈ S，a ∈ A。

while(true):

(a) s ← 当前（非终止）状态

(b) a ← ϵ-greedy(s, Q)

(c) 执行决策动作 a; 观测奖励 r, 获得下一个状态 s

′

(d) Q(s, a) ← Q(s, a) + α



r + γ maxa′ Q(s

′

, a′

) − Q(s, a)



(e) Model(s, a) ← r, s′

(f) 重复 n 次:

s ← 随机历史观测状态

a ← 在状态 s 下历史随机决策动作

r, s′ ← Model(s, a)

Q(s, a) ← Q(s, a) + α



r + γ maxa′ Q(s

′

, a′

) − Q(s, a)



9.4 基于模拟的搜索

在本节中，我们侧重于规划部分，并介绍一些基于模拟的搜索算法，使其在当前的状态通过

演算形成探索轨迹。因此，基于模拟的搜索算法一般是使用基于样本规划的前向搜索范式。前向

搜索和采样具体的阐述如下。

• 前向搜索：在规划过程中，智能体当前所处马尔可夫过程中的状态比其他的状态更值得关

注。因而从另一角度，我们将具有有限选择的 MDP 看作一个树形的结构，其中树的根部代

表当前状态，如图 9.4 所示，前向搜索算法从当前的状态选择最佳的决策动作，并且通过树

形结构的枝干来考虑未来的选择。

• 采样：当基于 MDP 采用规划过程时，从当前的状态到下一个状态可能有多种选择，因而在

规划中需要采样的操作，即智能体随机选定下一个状态并继续前向搜索的演算过程。因而

下一个状态的选取具有随机性，并且有可能服从某种概率或分布，具体是由模拟中智能体

采取的决策策略决定的。

在基于模拟的搜索中，模拟策略被用来指导规划过程中探索的方向。模拟策略与智能体学习

的策略相结合，有助于规划过程能够准确地反映智能体当前的决策策略。

下面将进一步介绍几种不同的基于模拟的搜索方法并结合策略学习来解决问题。

293

第 9 章 集成学习与规划

图 9.4 前向搜索

9.4.1 朴素蒙特卡罗搜索

如果一开始提供了固定的模型 M 和固定的模拟策略 π，朴素蒙特卡罗搜索可以依据模拟得

到的经验来评估对应动作的性能好坏并更新学习到的策略。如算法 9.31 所示，对每一个作用于

当前状态 St 的动作 a, a ∈ A，执行模拟策略 π 并用 Gk

t 表示第 k 个轨迹的全部奖励。根据保存

的轨迹，我们利用 Q(St, At) 来评估选择动作 At 的性能，最后根据当前状态下所有动作各自的 Q

值选择最优的动作。

算法 9.31 朴素蒙特卡罗搜索

固定模型 M 和模拟策略 π

for 每个动作 a ∈ A do

for 每个片段 k ∈ {1, 2, · · · , K} do

根据模型 M 和模拟策略 π, 从当前状态 St 开始在环境中展开

记录轨迹 {St, a, Rk

t+1, Sk

t+1, Ak

t+1, Rk

t+2, · · · , Sk

T

}

计算从每个 St 开始的累积奖励 Gk

t =

PT

j=t+1 Rk

j

end for

Q(St, a) = 1

K

P

K

k=1

Gk

t

end for

返回当前最大 Q 值的动作 At = arg maxa∈A Q(St, a)。

9.4.2 蒙特卡罗树搜索

朴素蒙特卡罗搜索的一个明显不足是，它的模拟策略是固定的，从而没有办法利用在规划过

程中学习到的信息。蒙特卡罗树搜索（Monte Carlo Tree Search，MCTS）(Browne et al., 2012) 正

是针对这个不足所设计的。具体地说，MCTS 维护了一棵搜索树来保存收集到的信息并逐步优化

模拟策略。

如算法 9.32 所示，在从当前的状态 St 开始采样到一个轨迹之后，对于轨迹中所有访问过的

294

9.4 基于模拟的搜索

(s, a)，MCTS 类似地使用平均回报更新了 Q 值，进而根据树中新的 Q 值更新每个节点处的模拟

策略 π。一个更新模拟策略 π 的方法是根据当前 Q 值的 ϵ 贪心策略。当模拟策略到达一个新的

当前并不在搜索树中的状态的时候，π 转换成默认的策略，比如均匀探索策略。第一个被探索的

新状态会接着被加入搜索树中。1 MCTS 重复这个节点评估和策略提升的过程直到到达模拟的预

算。最后，智能体选择在当前状态 St 上有最大 Q 值的动作。

算法 9.32 蒙特卡罗树搜索

固定模型 M

初始化模拟策略 π

for 每个动作 a ∈ A do

for 每个片段 k ∈ {1, 2, · · · , K} do

根据模型 M 和模拟策略 π 从当前状态 St 在环境中展开

记录轨迹 {St, a, Rt+1, St+1, At+1, Rt+2, · · · ST }

用从 (St, At)，At = a 开始的平均回报更新每个 (Si

, Ai), i = t, · · · , T 的 Q 值

由当前的 Q 值更新模拟策略 π

end for

end for

返回当前最大 Q 值的动作 At = arg maxa∈A Q(St, a)

9.4.3 时间差分搜索

除了 MCTS 的方法，时间差分（Temporal Difference，TD）搜索同样受到关注 (Silver et al.,

2012)。和 MCTS 的方法相比，TD 搜索不需要演算一个扩展轨迹并用其来评估和更新当前策略。

在模拟的每一步中，策略都将被更新并用更新的策略指导智能体在下一个状态中做出决策动作。

Dyna-2 算法就是采用 TD 搜索的方式 (Silver et al., 2008)，如算法 9.33 所述，智能体将存储两

组网络参数，分别存储于长期存储空间和短期存储空间。在下层中通过采用 TD 学习的方法，短

期存储空间中的网络参数将会根据收集到的模拟经验进行更新，并在策略 Q 的指导下将学到的

网络参数 θ 用于帮助智能体在真实环境中做出决策动作，而在长期存储空间的网络参数将在真实

环境的探索中通过在上层的 TD 学习得到更新。在上层中学习到的基于网络参数 θ 的策略 Q 将是

最终智能体学习到的最佳策略。

和 MCTS 的方法相比，由于每一步策略都会更新，TD 搜索会更有效率。然而，由于频繁的

更新，TD 搜索倾向于降低结果的方差但是有可能增大偏差。

1另一个方法是将轨迹上所有新的节点都加入搜索树中。

295

第 9 章 集成学习与规划

算法 9.33 Dyna-2

function LEARNING

初始化 Fs 和 Fr

θ ← 0 # 初始化长期存储空间中网络参数

loop

s ← S0

θ ← 0 # 初始化短期存储空间中网络参数

z ← 0 # 初始化资格迹

SEARCH(s)

a ← π(s; Q) # 基于和 Q 相关的策略选择决策动作

while s 不是终结状态 do

执行 a, 观测奖励 r 和下一个状态 s

′

(Fs, Fr) ← UpdateModel(s, a, r, s′

)

SEARCH(s

′

)

a

′ ← π(s

′

; Q) # 选择决策动作使其用于下一个状态 s

′

δ ← r + Q(s

′

, a′

) − Q(s, a) # 计算 TD-error

θ ← θ + α(s, a)δz # 更新长期存储空间中网络参数

z ← λz + ϕ # 更新资格迹

s ← s

′

, a ← a

′

end while

end loop

end function

function SEARCH(s)

while 时间周期内 do

z ← 0 # 清除短期存储的资格迹

a ← π(s; Q) # 基于和 Q 相关的策略决定决策动作

while s 不是终结状态 do

s

′ ← Fs(s, a) # 获得下一个状态

r ← Fr(s, a) # 获得奖励

a

′ ← π(s

′

; Q)

δ ← R + Q(s

′

, a′

) − Q(s, a) # 计算 TD-error

θ ← θ + α(s, a)δz # 更新短期存储空间中网络参数

z ← λz + ϕ # 更新短期存储的资格迹

s ← s

′

, a ← a

′

end while

end while

end function

参考文献

BROWNE C B, POWLEY E, WHITEHOUSE D, et al., 2012. A survey of monte carlo tree search

methods[J]. IEEE Transactions on Computational Intelligence and AI in games, 4(1): 1-43.

296

参考文献

HAFNER D, LILLICRAP T, BA J, et al., 2019. Dream to control: Learning behaviors by latent

imagination[J]. arXiv preprint arXiv:1912.01603.

KAISER L, BABAEIZADEH M, MILOS P, et al., 2019. Model-based reinforcement learning for atari[Z].

KALASHNIKOV D, IRPAN A, PASTOR P, et al., 2018. Qt-opt: Scalable deep reinforcement learning

for vision-based robotic manipulation[J]. arXiv preprint arXiv:1806.10293.

SILVER D, SUTTON R S, MÜLLER M, 2008. Sample-based learning and search with permanent and

transient memories[C]//Proceedings of the 25th international conference on Machine learning. ACM:

968-975.

SILVER D, SUTTON R S, MÜLLER M, 2012. Temporal-difference search in computer go[J]. Machine

learning, 87(2): 183-219.

SUTTON R S, 1991. Dyna, an integrated architecture for learning, planning, and reacting[J]. ACM Sigart

Bulletin, 2(4): 160-163.

297

10 分层强化学习

在本章中，我们将介绍分层强化学习。它是一种通过构建并利用认知和决策过程的底层结构

来提高学习效果的方法。具体来说，首先我们将介绍了分层强化学习的背景和两个主要类别：选

项框架（Options Framework）和封建制强化学习（Feudal Reinforcement Learning）。然后我们将详

细介绍这些类别中的一些典型算法，包括战略专注作家（Strategic Attentive Writer）、选项批判者

（Option-critic）和封建制网络（Feudal Networks）等。在本章的最后，我们对近年来关于分层强化

学习的研究成果进行了总结。

10.1 简介

近年来，深度强化学习在许多领域取得了显著的成功 (Levine et al., 2018; Mnih et al., 2015;

Schulman et al., 2015; Silver et al., 2016, 2017)。然而，长期规划对智能体来说仍然是一个挑战，特别

是在一些奖励稀疏、大时间跨度的环境，例如 Dota (OpenAI, 2018) 和《星际争霸》 (Vinyals et al.,

2019)。分层强化学习（Hierarchical Reinforcement Learning，HRL）提供了一种方法来寻找这种

复杂控制问题中的时空抽象和行为模式 (Bacon et al., 2017; Barto et al., 2003; Dayan, 1993b; Dayan

et al., 1993a; Dietterich, 1998, 2000; Hausknecht, 2000; Kaelbling, 1993; Nachum et al., 2018; Parr et al.,

1998a; Sutton et al., 1999; Vezhnevets et al., 2016, 2017)。与人类认知的层次结构类似，HRL 具备抽

象多层次控制的潜力，其中高层次的长期规划和元学习指导低层次的控制器。层次结构的模块化

也提供了可移植性和可解释性，例如，理解地图和达到有利状态的技术通常在像 grid-world (Tamar

et al., 2016) 或者 Doom (Bhatti et al., 2016; Kempka et al., 2016) 这样的游戏中十分有用。

以往对 HRL 的研究大多从 4 个主要方面展开：选项框架（Options Framework）(Sutton et al.,

1999)、封建制强化学习（Feudal Reinforcement Learning，FRL）(Dayan et al., 1993a)、MAXQ 分解

298

10.2 选项框架

（MAXQ Decomposition）(Dietterich, 2000) 和 层次抽象机（Hierarchical Abstract Machines，HAMs）

(Parr et al., 1998a,b) 在选项框架中，高层策略会在特定的时间步上切换低层策略，以便在时间域

上分解问题。在 FRL 智能体中，高层控制器负责为下层控制器提出明确的目标（如某些特定的

状态），来实现状态空间的层次分解。MAXQ 分解也提出了一种将子任务的解与 Q 值函数相结合

的状态抽象方法。HAMs 则考虑了一个学习过程来减少大型复杂问题中的搜索空间，其学习过程

中智能体能执行的动作受限于有限状态机的层次。在本章中，我们将重点介绍在 HRL 中应用深

度学习的最新研究成果。具体来说，我们讨论了分别属于选项框架和 FRL 的两种算法，并在本

章结尾对深度 HRL 进行了简要的总结。

10.2 选项框架

选项框架 (Hausknecht, 2000; Sutton et al., 1999) 将动作在时间层面扩展。选项（Options），也

被称为技能 (Da Silva et al., 2012) 或者宏操作 (Hauskrecht et al., 1998; Vezhnevets et al., 2016)，是

一种具有终止条件的子策略。它观察环境并输出动作，直到满足终止条件为止。终止条件是一类

时序上的隐式分割点，来表示相应的子策略已经完成了自己的工作，且顶层的选项策略（PolicyOver-Action）需要切换至另一个选项。给定一个状态集为 S、动作集为 A 的 MDP，选项 ω ∈ Ω

被定义为三元组 (Iω, πω, βω)，其中 Iω ⊆ S 为一组初始状态集，πω : S × A → [0, 1] 是一个选项

内置策略，而 βω : S → [0, 1] 是一个通过伯努利分布提供随机终止条件的终止函数。一个选项 ω

只有在 s ∈ Iω 时，才能用于状态 s。一个智能体通过其选项策略选择一个选项，并继续保持该策

略直到终止条件满足，然后再次查询选项策略并重复该步骤。注意，若选项 ω 被执行，则动作将

由相应的策略 πω 进行选择，直到选项根据 βω 被随机终止。比如说，一个名为“开门”的选项可

能包含一个用于靠近、抓取和转动门把手的策略，以及一个确定门被打开概率的终止条件。

尤其特别的是，一个选项框架由两层结构组成：底层的每个元素是一个选项，而顶层则是一

个选项策略，用来在片段开始或上个选项终结时候选择一个选项。选项策略从环境给出的奖励信

息学习，而选项可通过明确的子目标来学习。例如，在表格情况下，每个状态可以被看作子目标

的候选 (Schaul et al., 2015; Wiering et al., 1997)。一旦给出了选项，则顶层可以将其作为动作，通

过标准技术来进行学习。

在选项框架中，顶层模块学习的是一个选项策略，而底层模块学习能完成各个选项目标的策

略。这可以看成马尔可夫过程在时间层（几个时间步）上的分解。半马尔可夫决策过程（SemiMarkov Decision Process，SMDP）为动作间持续时间具备不确定性的选项框架提供了一个理论观

点 (Sutton et al., 1999)，如图 10.1 所示。SMDP 是一个具备额外元素 F: (S, A,P, R, F) 的标准

MDP。其中 F(t|s, a) 给出在状态 s 下执行动作 a 时，转移时间为 t 的概率。不严谨地说，选项框

架中的顶层控制可以被看成一个 SMDP 上的策略。对于多级选项的情况，更高层的选项可以看成

低层选项在时间上进一步扩展的 SMDP (Riemer et al., 2018)。

299

第 10 章 分层强化学习

图 10.1 在 SMDP 视角下的选项，改编自文献 (Sutton et al., 1999). 顶部：一个马尔可夫决策过程

（MDP）的状态轨迹。中部：一个半马尔可夫决策过程（SMDP）的状态轨迹。底部：一

个两层结构上 MDP 的状态轨迹。实心圆表示 SMDP 的决策，而空心圆则是相应选项包含

的原始动作

研究表明，人工定义的选项通过和深度学习的结合，即使在像《我的世界》和雅达利游戏这

样很有挑战性的环境中，也可以取得显著的效果 (Kulkarni et al., 2016; Tessler et al., 2017)。然而，

初始集和终结条件是选项框架的一个制约因素。例如，一个人工定义的策略 πω 是让移动机器人

插上它的充电器，而它很有可能是只为充电器在视野范围内的状态而定制的。终结条件表明当机

器人成功插上充电器或者状态在 Iω 之外时，终结的概率为 1。因此，如何自动地发掘选项也曾是

HRL 的一个研究主题。我们将介绍两种算法，它们将选项发掘表示为优化问题，并用函数逼近的

方式解决这类问题。第一个是一种深度递归神经网络，被称为战略专注作家（Strategic Attentive

Writer，STRAW），它通过开环选项内置策略1（Open-Loop Intra-Option Policies）学习选项。第二

个则是考虑闭环选项内置策略2（Close-Loop Intra-Option Policies）的选项-批判者（Option-Critic）

结构。

10.2.1 战略专注作家

战略专注作家 (Vezhnevets et al., 2016) 是一种新奇的深度递归神经网络结构。它对常见的动

作序列（宏动作）进行时域抽象，并通过这些动作进行端到端的学习。值得注意的是，宏动作是

一个在神经网络中隐式表示的特定选项。其动作序列（或者在此之上的分布）是在宏动作被初始

化的时候决定的。STRAW 分别包含短期动作分布和长期计划这两个模块。

第一个模块将环境的观测数据转化为一个动作-计划（Action-Plan），它是一个显式（Explicit）

1开环意即不将控制的结果反馈，进而影响当前控制的系统。

2闭环意即将控制的结果进行完全反馈，进而影响当前控制的系统。

300

10.2 选项框架

的随机变量，用于表示接下来一段时间内计划执行的动作。当时间步为 t 时，动作-计划表示为矩

阵 A ∈ R

|A|×T，其中 T 是计划的最大时间跨度，而在 A 中的第 τ 列对应动作在时间步 t + τ 的

分对数。

第二个模块通过单行矩阵 c

t ∈ R

1×T 维护承诺-计划（Commitment-Plan），即一个决定在哪

一步网络结束一个宏动作并更新动作-计划的状态变量。在时间步为 t 时，c

t−1 的第一个元素提

供了终止条件的伯努利分布的参数。在落实计划的期间，行动-计划 At 和承诺-计划 c

t 都被一个

时间移位运算符 ρ 直接滑动至下一步，其中 ρ 通过移除矩阵的第一列并在末尾添 0 的形式来移动

矩阵。

图 10.2 显示了一个包含动作-计划和承诺-计划的 STRAW 工作流示例。为了更新这两类计划，

STRAW 在时间维度上使用了专注写作技术 (Gregor et al., 2015)，它使网络能够聚焦在当前部分。

该技术将一个高斯滤波器矩阵沿着时间维度应用于计划。更准确地说，对于时间大小 K，一个

|A| × K 一维高斯滤波器的网格通过指定网格中心的坐标和相邻过滤器之间的步幅来在计划中定

位。注意，这里的步幅（Stride）和 CNN 中的相同术语相似。让 ψ

A 作为动作-计划的注意力参数，

即高斯滤波器的网格位置、步幅和标准差。STRAW 将注意力操作定义如下：

D = write(p, ψA

t

); βt = read(At

, ψA

t

), (10.1)

图 10.2 STRAW 在一个迷宫导航游戏中的工作流程，改编自文献 (Vezhnevets et al., 2016)。观测数

据是原始像素，其中像素的颜色可以是蓝色、黑色、红色和绿色，分别代表墙、走廊、智

能体和最终目的地。动作空间为上、下、左、右四个方向的移动。当 t = 1 时，帧的特

征被一个卷积神经网络提取后输入进 STRAW。STRAW 立刻产生两个计划。在紧接着的 2

个时间步中，这两个计划被 ρ 滑动。之后，智能体来到角落并由承诺-计划 c

t 给出一个

重新计划的信号（见彩插）

301

第 10 章 分层强化学习

其中，p ∈ R

A×K 是一个时间窗口为 K 的计划补丁。write 操作生成了一个与 At 相同大小

的平滑计划 D，而 read 操作生成了一个读取补丁 βt ∈ R

A×K。此外，将 zt 作为在时间步 t 下的

观测数据的特征表示，并将相似的注意力技术应用于承诺-计划，计划的更新算法如算法 10.34 所

示。其中 f

ψ、f

A 和 f

c 都是线性函数，h 是一个多层感知器，b ∈ R

1×T 是一个具有相同标量参

数 b 的偏差，而 e 是 固定为 40 的标量 (Vezhnevets et al., 2016)，以便经常重新做计划。

算法 10.34 STRAW 中的计划更新

if gt = 1 then

计算动作-计划的注意力参数 ψ

A

t = f

ψ(zt)

应用专注阅读：βt = read(At−1

, ψA

t

)

计算中间表示 ϵt = h(concat(βt, zt))

计算承诺-计划的注意力参数 ψ

c

t = f

c

(concat(ψ

A

t

, ϵt))

更新 At = ρ(At−1

) + write(f

A(ϵt), ψA

t

)

更新 ct = Sigmoid(b + write(e, ψc

t

))

else

更新 At = ρ(At−1

)

更新 ct = ρ(ct−1)

end if

对于进一步的结构化搜索，STRAW 在对角高斯分布上使用了重参数技术 Q(zt|ζt) = N (µ(ζt),

σ(ζt))，其中 ζt 是特征提取器的输出。STRAW 的训练 loss 被定义如下：

L =

X

T

t=1

(L(At

) + αgtKL(Q(zt|ζt)|P(zt)) + λc

t

1

), (10.2)

其中，L 是一个领域特定损失函数（例如回报的负对数似然），P(zt) 是一个先决条件，而最

后一项惩罚了重新计划并鼓励承诺。

要特别注意的是，STRAW 是一个网络结构。对于强化学习的任务，可以使用一系列的强化学

习算法。文献 (Vezhnevets et al., 2016) 展示了在《2D 迷宫》和雅达利游戏上使用 A3C (Mnih et al.,

2016) 算法的效果。《2D 迷宫》是由许多格子组成的一个 2D 网格世界，其中每个格子只可能是

墙壁或者通道，而其中，某个通道会随机选择为目的地。智能体将完全观测到迷宫的状态，并需

要通过结构化探索来到达目标。在本任务中，文献 (Vezhnevets et al., 2016) 展示了 STRAW 在策

略上的表现优于 LSTM，并且很接近由 Dijkstra 算法给出的最优策略。在雅达利游戏领域中，文

献 (Vezhnevets et al., 2016) 选出了 8 个需要一些规划和探索的游戏，其中 STRAW 及其变体在 8 个

游戏中的 6 个里，比 LSTM 和简单前馈网络在游戏中获得了更高的分数。

302

10.2 选项框架

10.2.2 选项-批判者结构

选项-批判者结构（Option-Critic Architecture）(Bacon et al., 2017) 将策略梯度定理扩展至选

项，它提供一种端到端的对选项和选项策略的联合学习。它直接优化了折扣化回报。我们先考虑

将选项-价值函数定义如下：

QΩ(s, ω) = X

a

πω(a|s)QU (s, ω, a), (10.3)

其中 QU : S × Ω × A → R 是在确定状态-选项对 (s, ω) 后执行某个动作的价值：

QU (s, ω, a) = R(s, a) + γ

X

s

′

p(s

′

|s, a)U(ω, s′

), (10.4)

其中 U : Ω × S → R 是进入一个状态 s

′ 时，执行 ω 的价值：

U(ω, s′

) = (1 − βω(s

′

))QΩ(s

′

, ω) + βω(s

′

)VΩ(s

′

), (10.5)

其中 VΩ : S → R 是选项的最优价值函数：

VΩ(s

′

) = max

ω∈Ω

Eω[

k

X−1

n=0

γ

nRt+n + γ

kVΩ(St+k)|St = s

′

], (10.6)

其中 k 是 ω 在状态 s

′ 中的预计持续时间。因此，我们可以定义 AΩ : S × Ω → R 为选项的

优势函数：

AΩ(s, ω) = QΩ(s, ω) − VΩ(s). (10.7)

如果选项 ωt 曾被初始化或者已经在状态 St 中执行了 t 个时间步，通过将状态-选项对视为马

尔可夫链中的常规状态，那么在一步中状态转移至 (St+1, ωt+1) 的概率为

X

a

πωt

(a|St)p(St+1|St, a)[(1 − βωt

(St+1))1ωt=ωt+1 + βωt

(St+1)πΩ(ωt+1|St+1)]. (10.8)

通过假设所有选项在任何地方都可用，上述转移是一个在状态-选项对的唯一稳态。

用于学习选项的随机梯度下降算法的结构如图 10.3 所示，其中梯度由定理 10.1 和定理 10.2

给出。然而，文献 (Bacon et al., 2017) 提出了通过一种基于两种时间尺度结构来学习价值，在更新

选项内置策略使用更快的时间尺度，而更新终止函数时使用比例更小的时限 (Konda et al., 2000)。

我们可以从行动者-批判者结构中看出，选项内置策略、终止函数和选项策略都属于行动者的部

303

第 10 章 分层强化学习

分，而批判者则包括 QU 和 AΩ。

图 10.3 选项-评判家结构，改编自文献 (Bacon et al., 2017)

定理 10.1 选项内置策略梯度理论（Intra-Option Policy Gradient Theorem）(Bacon et al., 2017) 给

定一组马尔可夫选项，随机选项内置策略对它们的参数 θ 可微。折扣化回报期望关于 θ 和初始条

件 (ˆs, ωˆ) 的梯度为

X

s,ω

µΩ(s, ω|s, ˆ ωˆ)

X

a

∂πω,θ(a|s)

∂θ QU (s, ω, a), (10.9)

其中 µΩ(s, ω|s, ˆ ωˆ) = P∞

t=0 γ

tp(St = s, ωt = ω|S0 = ˆs, ω0 = ˆω) 是一个沿着从 (ˆs, ωˆ) 开始的轨

迹的状态-选项对的折扣化权重。

定理 10.2 终止梯度定理（Termination Gradient Theorem）(Bacon et al., 2017) 给定一组马尔可夫

选项，选项的随机终止函数对其参数 φ 可微。折扣化回报目标期望对于 φ 和初始条件 (ˆs, ωˆ) 的

梯度为

−

X

s

′

,ω

µΩ(s

′

, ω|s, ˆ ωˆ)

∂βω,φ(s

′

)

∂φ AΩ(s

′

, ω), (10.10)

其中 µΩ(s, ω|s, ˆ ωˆ) = P∞

t=0 γ

tp(St = s, ωt = ω|S0 = ˆs, ω0 = ˆω) 是一个沿着从 (ˆs, ωˆ) 开始的轨迹的

状态-选项对的折扣化权重。

304

10.3 封建制强化学习

文献 (Bacon et al., 2017) 提供了离散和连续环境下的实验。在离散环境中，文献 (Bacon et al.,

2017) 在雅达利学习环境（Arcade Learning Environment，ALE）(Bellemare et al., 2013) 中训练了 4

个雅达利游戏，这些训练与文献 (Mnih et al., 2015) 采取了相同的设置。结果表明，选项-批判者能

够在这全部 4 个游戏中学到结构选项。在连续环境中，文献 (Bacon et al., 2017) 选择了 Pinball 游

戏 (Konidaris et al., 2009)，游戏中智能体控制一个小球在随机形状的多边形 2D 迷宫中进行移动，

其目的地也随机生成。通过选项-批判者学习到的轨迹表明，智能体可以实现时域抽象。

10.3 封建制强化学习

封建制强化学习（Feudal Reinforcement Learning，FRL）(Dayan et al., 1993a) 提出了一种封建

制等级结构。其中，管理者有着为他们工作的下级管理者和他们自己的上级管理者。它反映了封

建等级制度，其中每层的各个管理者可以为他们的下级设置任务、奖励和惩罚。有两个保证封建

制规则的关键原则需要被重视：奖励隐藏（Reward Hiding）和 信息隐藏（Information Hiding）。奖

励隐藏指的是，无论某管理者做出的指令是否能使其上级满意，该管理者的下级都必须服从。而

信息隐藏是指管理者的下级不知道该管理者被派予的任务，而管理者的上级也不知道该管理者给

其下级安排了什么任务。顶层的封建智能体并非像选项框架那样学习一个选项的时间分解，而是

通过为底层策略制定明确目标来分解状态空间的问题。这样的结构允许强化学习扩展到管理层之

间具有明确分工到大型领域中。

在这种解耦学习的启发下，文献 (Vezhnevets et al., 2017) 引入了一种新的神经网络结构，称为

封建制网络（Feudal Networks，FuNs）。它可以自动发现子目标，并且具备奖励隐藏和信息隐藏的

软条件。它跨越多层解耦了端到端学习，这使得它可以处理不同的时间分辨率。此外，使用离线

策略修正的分层强化学习（Hierarchical Reinforcement Learning with Off-policy Correction，HIRO）

可进一步提高了离线策略经验的样本效率 (Nachum et al., 2018)。实验显示，HIRO 取得了显著的

进展，并且能解决非常复杂的结合运动和基本物体交互的问题。

10.3.1 封建制网络

封建制网络（Feudal Networks，FuNs）是一个完全可微模块化的 FRL 神经网络，它有两个

模块：管理者和工作者。管理者在一个潜在状态空间中更低的时间分辨率上设定目标，而工作者

则学习如何通过内在奖励达到目标。图 10.4 展示了 FuN 的结构，其中前向过程可以描述成以下

等式：

zt = f

Percept(St) (10.11)

mt = f

Mspace(zt) (10.12)

h

M

t

, gˆt = f

Mrnn(mt, h

M

t−1

); gt = gˆt/∥gˆt∥; (10.13)

305

第 10 章 分层强化学习

wr = ϕ





Xt

i=t−c

gi



 (10.14)

h

W, Ut = f

Wrnn(zt, hW

t−1

) (10.15)

πt = SoftMax(Utwt) (10.16)

图 10.4 FuN 的结构，改编自文献 (Vezhnevets et al., 2017)。在文献 (Vezhnevets et al., 2017) 中，超参

数 k 和 d 被定为 k = 16 ≪ d = 256

其中 zt 是 St 的表示，f

Mspace 向管理者提供状态 mt，而 gt 表示管理者输出的目标。在 FRL

中需要注意以下两个原则：管理者和工作者之间没有梯度传播；但接收观测数据的感知机模块

f

Percept 共享。管理者的 f

Mrnn 和工作者的 f

Wrnn 都是循环模块，f

Mspace 是全连接的。h

M 和 hW 分

别对应管理者和工作者各自的内部状态。ϕ 是一个无偏线性变换，将目标 gt 映射成一个嵌入向

量 wt。Ut 表示动作的嵌入矩阵，它通过矩阵与 wt 的积输出工作者动作策略的分对数。

考虑到标准强化学习的设置是最大化折扣化回报 Gt =

P∞

k=0 γ

kRt+k。一个自然而然的学习

整个结构的方法就是通过策略梯度算法进行端到端训练，因为 FuNs 全部可微。然而这样会导致

梯度会被工作者通过任务目标传播给管理者，这可能导致目标会变成一个内部潜在变量，而不是

分层标志。因此，FuN 分别训练管理者和工作者。对于管理者，更新规则遵循预测优势方向：

∇gt = (Gt − V

M

t

(St, θ))∇θdcos(mt+c − mt, gt(θ)) (10.17)

306

10.3 封建制强化学习

其中，V

M

t 是管理者的值函数，而 dcos(α, β) = αTβ/(|α∥β|) 是余弦相似度。另一方面，工

作者可以通过任意现成的深度强化学习方式训练，其内在奖励定义如下：

R

I

t =

1

c

Xc

i=1

dcos(mt − mt−i

, gt−i) (10.18)

其中，状态空间中的方向偏移为目标提供了结构不变性。在实践中，FuN 通过使用 Rt + αRI

t

训练工作者，软化了原始 FRL 中的奖励隐藏条件，其中 α 是一个正则化内在奖励影响的超参数。

文献 (Vezhnevets et al., 2017) 也提供了一个关于管理者训练规则的理论分析。考虑到有高层

跨策略的策略 o(St, θ)，它在固定时长 c 下，在几个子策略中进行选择。对每个子策略来说，转移

分布 p(St+c|St, o) 可以被看作一个转移策略 π

T(St+c|St, θ)。和选项框架的 SMDP 视角类似，我

们可以在高层 MDP 对 π

T(St+c|St, θ) 应用策略梯度理论。

∇θπ

T(St+c|St, θ) = E[(Gt − V

M

t

(St, θ))∇θ log p(St+c|St, o)] (10.19)

这也被称为转移策略梯度（Transition Policy Gradients）。假设方向 St+c −St 遵循 Mises-Fisher

分布，我们可以得到 log p(St+c|St, o) ∝ dcos(St+c − St, gt)。

此外，文献 (Vezhnevets et al., 2017) 提出了用于管理者的 Dilated LSTM，与空洞卷积一样，可

以在分辨率无损的情况下获取更大的感受野。Dilated LSTM 维持了几个内部 LSTM 单元的状态。

在任意时间步中，只有一个单元状态被更新，而输出的是最近 c 个被更新的状态进行池化后的

结果。

需要注意的是，与 STRAW 相类似，FuN 也是一个用于 HRL 的神经网络结构。文献 (Vezhnevets

et al., 2017) 选择了 A3C 作为强化学习算法，并设计了一系列的实验来显示 FuN 相对于 LSTM 的

有效性。首先，它展示了对 FuN 应用在 Montezuma’s Revenge 游戏的分析。Montezuma’s Revenge

是一个雅达利游戏，它在强化学习领域是个难题。它需要通过许多技巧来控制角色躲开致命的陷

阱，并且从稀疏奖励中进行学习。实验结果显示，FuN 在采样效率上有着显著的提高。此外，它

在另外 10 款雅达利游戏中也有效果提升，其中 FuN 的分数明显高于选项-批判者结构。同样，文

献 (Vezhnevets et al., 2017) 使用了 4 个不同等级的 DeepMind 实验室 3D 游戏平台 (Beattie et al.,

2016) 来验证 FuN。它证明 FuN 学习了更加有意义的子策略，之后将这些子策略在内存中高效地

结合起来能产生更有价值的行为。

10.3.2 离线策略修正

HRL 方法提出训练多层策略来对时间和行为进行抽象。在前几节中，我们讨论了 STRAW 和

FuN 使用神经网络结构来学习一个分层策略，而选项-批判者结构则端到端地同时学习内部策略

和选项的终止条件。HRL 还存在许多问题，例如通用性、可迁移性和采样效率等。在本节中，我们

307

第 10 章 分层强化学习

将介绍离线策略修正分层强化学习（Hierarchical Reinforcement Learning with Off-policy Correction，

HIRO）(Nachum et al., 2018)。它为训练 HRL 智能体提供了一种普遍适用且数据效率很高的方法。

一般来说，HIRO 考虑了高层控制器通过自动提出一些目标来监督低层控制器的方案。更准

确地说，在每个时间步 t 中，HIRO 通过一个目标 gt 来驱动智能体。给定一个用户指定的参数 c，

若 t 是 c 的倍数，则目标 gt 由高层策略 µ

h 产生，否则 gt 由目标转移函数 h: gt = h(St−1, gt−1,

St) 通过之前的目标 gt−1 提供。和 FuN 类似，目标是指包含所需位置和方向信息在内的高层决

策。实验发现，与在嵌入空间中表示目标不同，HIRO 直接使用原始观测数据更为有效。需要注

意的是，我们可以根据特定任务的领域知识设计内在奖励和目标转移函数。具体来说，在最简单

的情况下，内在奖励被定义如下：

R

I

t = −∥St + gt − St+1∥2, (10.20)

目标转移函数被定义为

h(St−1, gt−1, St) = St−1 + gt−1 − St (10.21)

来维持目标方向。

为了提高数据效率，HIRO 将离线策略技术扩展到高层和低层训练。HIRO 让低层策略 µ

l

存储经验 (St, gt, At, RI

t

, St+1, h(St, gt, St+1))，并将 gt 视为模型的额外输入，以支持任意离线算

法训练这些策略。对于高层策略，转移元组 (St:t+c, gt:t+c, At:t+c, Rt:t+c, St+c)（‘:’ 在 Python 中

表示切片操作。这里的切片不包括最后的元素）也可以通过任意的离线策略算法进行训练，这

里只需将 gt 视为一个动作并累加 Rt:t+c 作为奖励。然而过去的低层控制器观测的转移数据并

不能反映动作。为了解决这个问题，HIRO 提出使用重标记（Re-label）技术来纠正高层转移数

据。旧的转移数据 (St, gt,

PRt:t+c, St+c) 将被重新标记一个不同的目标 gˆt 使得 gˆt 能最大化

µ

l

(At:t+c|St:t+c, gˆt:t+c) 概率，其中 gˆt+1:t+c 通过目标转移函数 h 计算。对于随机行为策略，其对

数概率 log µ

l

(At:t+c|St:t+c, gˆt:t+c) 可以通如下方式计算：

log µ

l

(At:t+c|St:t+c, gˆt:t+c) ∝ −

1

2

t+

Xc−1

i=t

∥At − µ

l

(Si

, gˆi)∥

2

2 + const. (10.22)

在实践中，HIRO 从一个包括原始目标的目标候选集中选择能最大化对数概率的目标。该目标

对应 St+c −St 的差，并来自一个对角高斯分布的采样。分布中每个平均项随机对应向量 St+c −St

中的元素，其中减号表示一个元素运算符。

HIRO 的结构如图 10.5 所示。Nachum 等人 (Nachum et al., 2018) 在文献 (Duan et al., 2016) 中

通过 4 个挑战性的任务验证了 HIRO。实验表明，离线策略修正具有显著的优势，并且对低层控

制器的重标记可以对初始训练进行加速。

308

10.4 其他工作

图 10.5 HIRO 的结构，改编自文献 (Nachum et al., 2018)。低层策略接收高层目标，并直接与环境

交互。其中目标是由高层策略或者目标转移函数产生的

10.4 其他工作

在本节中，我们对近年来 HRL 方面的工作进行了简要的总结。图 10.6 显示了两个视角。先

从低层策略奖励信号这个视角看，通常有两种观点，第一种观点是提出直接用端到端的通过环

境学习低层策略，例如前文介绍的 STRAW (Vezhnevets et al., 2016) 和选项-批判者结构 (Bacon

et al., 2017)。第二种观点认为通过辅助奖励进行学习可以获得更好的分层效果，例如前文提到过

的 FuN (Vezhnevets et al., 2017) 和 HIRO (Nachum et al., 2018)。

图 10.6 HRL 算法的两个视角

一般来说，第一种观点可以从端到端学习中获得更为有效的效果。这个分支下的主要工作聚

焦在选项上。对于选项的发现方法，STRAW (Vezhnevets et al., 2016) 和选项-评判者结构 (Bacon

309

第 10 章 分层强化学习

et al., 2017) 都可以被视为自上而下的方法，这种方法先通过探索获得一些奖励信号，随后对动作

进行拆解，从而组成选项。与之不同的是，文献 (Machado et al., 2017) 介绍了一种自下而上的方法，

该方法使用了在一个 Laplacian 图框架下的原始值函数（Proto-Value Functions，PVFs）来对环境进

行表示学习，为任务无关的选项提供了理论基础。文献 (Riemer et al., 2018) 扩展了选项-批判者结

构，并得出了一个深度分层选项的策略梯度定理。实验结果表明，分层选项-批判者在离散和连续

的环境中都十分有效。文献 (Harutyunyan et al., 2018) 仿照离线策略学习中的做法，将终止条件解

耦为行为终止和目标终止。该方法在文中的实验里表现出了更快的收敛速度。文献 (Sharma et al.,

2017) 受到 SMDP 视角下的选项的启发，提出了细粒度动作重复（Fine Grained Action Repetition，

FiGAR）。它能通过学习来预测选择出的动作要被重复执行的时间步数。

此外，另外一种直观的方法是将元学习与这种端到端的方法结合起来形成一个层次结构。文

献 (Frans et al., 2017) 开发了一个能提升未知任务采样效率的元学习算法，该算法共享了分层结构

中的基础策略，并在 3D 仿人机器人上取得了显著的成果。然而，由于对最终任务具有唯一依赖

性，如何将该方法扩展到复杂领域仍然是一个问题 (Bacon et al., 2017; Frans et al., 2017; Nachum

et al., 2018)。

第二种观点是使用辅助奖励。FuN (Vezhnevets et al., 2017) 和 HIRO (Nachum et al., 2018) 都

为低层策略建立了目标导向的内在奖励。有许多其他的工作聚焦于能在一系列领域上有效的目标

导向奖励。通用价值函数逼近器（Universal Value Function Approximators, UVFAs）(Schaul et al.,

2015) 在目标上泛化价值函数。文献 (Levy et al., 2018) 进一步引入了后见之明目标转移，扩展了

后见之明经验回放（Hindsight Experience Replay，HER）(Andrychowicz et al., 2017) 的思想，并取

得了显著的稳定性。文献 (Kulkarni et al., 2016) 介绍了 h-DQN 算法，它学习不同时间尺度下的分

层动作价值函数。其中顶层的动作价值函数学习选项策略，低层的动作价值函数学习如何达到给

定的子目标。

另外也可以利用领域知识来构建手工辅助奖励。文献 (Heess et al., 2016) 介绍了一个用于移动

任务的结构，它会先在相关简单任务上进行预训练。文献 (Tessler et al., 2017) 提出了应用在《我

的世界》游戏领域的终生学习系统。它会有选择地将学到的技能转移到新任务上。文献 (Florensa

et al., 2017) 引入了随机神经网络结构，它通过预训练的技能来学习高层策略，需要最少的下游任

务领域知识，并可以很好利用学到的技能的可迁移性。然而，无论是目标导向奖励和手工奖励都

很难简单地将任务扩展到其他领域，比如像素级观测的领域。

我们也可以从抽象目标的视角来理解 HRL 算法。选项框架通常学习时域抽象，而 FuN 则考

虑状态抽象。HIRO 可以被认为既考虑了状态抽象又考虑了时域抽象。其目标提供了状态方向和

目标转移函数模型的时间信息。对于时域抽象，与选项框架相比，文献 (Haarnoja et al., 2018) 使

用了图模型来实现另一个分层思想。在该分层中，若当前任务没有完全成功，则每一层解决自己

当前的任务。这会使上层的工作更为简单。在状态抽象和时域抽象之外，文献 (Mnih et al., 2014)

提供了一个利用注意力机制对状态空间进行分解的方法。更准确地说，这项工作在选择动作前，

在状态空间增加了一个视觉注意力机制。此处的注意力完成了在状态空间上的高层规划 (Sahni

310

参考文献

et al., 2017; Schulman, 2016)。对于选择抽象对象，其核心是回答高层策略如何指导低层策略的问

题。对于有足够先验知识的领域，通过元学习进行技能组合学习的方式可以取得更好的效果。对

于长期规划，顶层的时域抽象是十分必要的。

我们可以看出，HRL 仍然是强化学习的一个高级课题，还有许多问题需要解决。回想起 HRL

的动机是通过分层抽象来提高采样效率和通过重用学到的技巧来处理大时间跨度的问题。实验结

果表明，分层架构带来了一些效果提升，但并没有足够的证据表明，它是确实实现了分层抽象，

或者只是进行了更有效的探索 (Nachum et al., 2018)。未来，在概率规划、相关理论研究，以及在

其他强化学习领域进行分层等方向上，可能会带来新的突破。

参考文献

ANDRYCHOWICZ M, WOLSKI F, RAY A, et al., 2017. Hindsight experience replay[C]//Advances in

Neural Information Processing Systems. 5048-5058.

BACON P L, HARB J, PRECUP D, 2017. The option-critic architecture[C]//Thirty-First AAAI Conference on Artificial Intelligence.

BARTO A G, MAHADEVAN S, 2003. Recent advances in hierarchical reinforcement learning[J]. Discrete

event dynamic systems, 13(1-2): 41-77.

BEATTIE C, LEIBO J Z, TEPLYASHIN D, et al., 2016. DeepMind Lab[J]. arXiv:1612.03801.

BELLEMARE M G, NADDAF Y, VENESS J, et al., 2013. The Arcade Learning Environment: An

evaluation platform for general agents[J]. Journal of Artificial Intelligence Research, 47: 253-279.

BHATTI S, DESMAISON A, MIKSIK O, et al., 2016. Playing Doom with slam-augmented deep

reinforcement learning[J]. arXiv preprint arXiv:1612.00380.

DA SILVA B, KONIDARIS G, BARTO A, 2012. Learning parameterized skills[J]. arXiv preprint

arXiv:1206.6398.

DAYAN P, 1993b. Improving generalization for temporal difference learning: The successor representation[J]. Neural Computation, 5(4): 613-624.

DAYAN P, HINTON G E, 1993a. Feudal reinforcement learning[C]//Advances in Neural Information

Processing Systems. 271-278.

DIETTERICH T G, 1998. The MAXQ method for hierarchical reinforcement learning.[C]//Proceedings

of the International Conference on Machine Learning (ICML): volume 98. Citeseer: 118-126.

311

第 10 章 分层强化学习

DIETTERICH T G, 2000. Hierarchical reinforcement learning with the MAXQ value function decomposition[J]. Journal of Artificial Intelligence Research, 13: 227-303.

DUAN Y, CHEN X, HOUTHOOFT R, et al., 2016. Benchmarking deep reinforcement learning for

continuous control[C]//International Conference on Machine Learning. 1329-1338.

FLORENSA C, DUAN Y, ABBEEL P, 2017. Stochastic neural networks for hierarchical reinforcement

learning[J]. arXiv preprint arXiv:1704.03012.

FRANS K, HO J, CHEN X, et al., 2017. Meta learning shared hierarchies[J]. arXiv preprint

arXiv:1710.09767.

GREGOR K, DANIHELKA I, GRAVES A, et al., 2015. Stochastic backpropagation and approximate

inference in deep generative models[C]//Proceedings of the International Conference on Machine

Learning (ICML).

HAARNOJA T, HARTIKAINEN K, ABBEEL P, et al., 2018. Latent space policies for hierarchical

reinforcement learning[J]. arXiv preprint arXiv:1804.02808.

HARUTYUNYAN A, VRANCX P, BACON P L, et al., 2018. Learning with options that terminate

off-policy[C]//Thirty-Second AAAI Conference on Artificial Intelligence.

HAUSKNECHT M J, 2000. Temporal abstraction in reinforcement learning[D]. University of Massachusetts, Amherst.

HAUSKRECHT M, MEULEAU N, KAELBLING L P, et al., 1998. Hierarchical solution of Markov

decision processes using macro-actions[C]//Proceedings of the Fourteenth conference on Uncertainty

in artificial intelligence. Morgan Kaufmann Publishers Inc.: 220-229.

HEESS N,WAYNE G, TASSA Y, et al., 2016. Learning and transfer of modulated locomotor controllers[J].

arXiv preprint arXiv:1610.05182.

KAELBLING L P, 1993. Hierarchical learning in stochastic domains: Preliminary results[C]//Proceedings

of the tenth International Conference on Machine Learning (ICML): volume 951. 167-173.

KEMPKA M, WYDMUCH M, RUNC G, et al., 2016. ViZDoom: A Doom-based AI research platform

for visual reinforcement learning[C]//2016 IEEE Conference on Computational Intelligence and Games

(CIG). IEEE: 1-8.

KONDA V R, TSITSIKLIS J N, 2000. Actor-critic algorithms[C]//Advances in Neural Information

Processing Systems. 1008-1014.

312

参考文献

KONIDARIS G, BARTO A G, 2009. Skill discovery in continuous reinforcement learning domains using

skill chaining[C]//Advances in Neural Information Processing Systems. 1015-1023.

KULKARNI T D, NARASIMHAN K, SAEEDI A, et al., 2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation[C]//Advances in Neural Information

Processing Systems. 3675-3683.

LEVINE S, PASTOR P, KRIZHEVSKY A, et al., 2018. Learning hand-eye coordination for robotic

grasping with deep learning and large-scale data collection[J]. The International Journal of Robotics

Research, 37(4-5): 421-436.

LEVY A, PLATT R, SAENKO K, 2018. Hierarchical reinforcement learning with hindsight[J]. arXiv

preprint arXiv:1805.08180.

MACHADO M C, BELLEMARE M G, BOWLING M, 2017. A Laplacian framework for option discovery

in reinforcement learning[C]//Proceedings of the 34th International Conference on Machine LearningVolume 70. JMLR. org: 2295-2304.

MNIH V, HEESS N, GRAVES A, et al., 2014. Recurrent models of visual attention[C]//Advances in

Neural Information Processing Systems. 2204-2212.

MNIH V, KAVUKCUOGLU K, SILVER D, et al., 2015. Human-level control through deep reinforcement

learning[J]. Nature.

MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learning[C]//International Conference on Machine Learning (ICML). 1928-1937.

NACHUM O, GU S S, LEE H, et al., 2018. Data-efficient hierarchical reinforcement learning[C]//

Advances in Neural Information Processing Systems. 3303-3313.

OPENAI, 2018. Openai five[Z].

PARR R, RUSSELL S J, 1998a. Reinforcement learning with hierarchies of machines[C]//Advances in

Neural Information Processing Systems. 1043-1049.

PARR R E, RUSSELL S, 1998b. Hierarchical control and learning for Markov decision processes[M].

University of California, Berkeley Berkeley, CA.

RIEMER M, LIU M, TESAURO G, 2018. Learning abstract options[C]//Advances in Neural Information

Processing Systems. 10424-10434.

313

第 10 章 分层强化学习

SAHNI H, KUMAR S, TEJANI F, et al., 2017. State space decomposition and subgoal creation for

transfer in deep reinforcement learning[J]. arXiv preprint arXiv:1705.08997.

SCHAUL T, HORGAN D, GREGOR K, et al., 2015. Universal value function approximators[C]//

International Conference on Machine Learning. 1312-1320.

SCHULMAN J, 2016. Optimizing expectations: From deep reinforcement learning to stochastic computation graphs[D]. UC Berkeley.

SCHULMAN J, LEVINE S, ABBEEL P, et al., 2015. Trust region policy optimization[C]//International

Conference on Machine Learning (ICML). 1889-1897.

SHARMA S, LAKSHMINARAYANAN A S, RAVINDRAN B, 2017. Learning to repeat: Fine grained

action repetition for deep reinforcement learning[J]. arXiv preprint arXiv:1702.06054.

SILVER D, HUANG A, MADDISON C J, et al., 2016. Mastering the game of go with deep neural

networks and tree search[J]. Nature.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2017. Mastering chess and shogi by self-play with

a general reinforcement learning algorithm[J]. arXiv preprint arXiv:1712.01815.

SUTTON R S, PRECUP D, SINGH S, 1999. Between MDPs and semi-MDPs: A framework for temporal

abstraction in reinforcement learning[J]. Artificial intelligence, 112(1-2): 181-211.

TAMAR A, WU Y, THOMAS G, et al., 2016. Value iteration networks[C]//Advances in Neural Information Processing Systems. 2154-2162.

TESSLER C, GIVONY S, ZAHAVY T, et al., 2017. A deep hierarchical approach to lifelong learning in

Minecraft[C]//Thirty-First AAAI Conference on Artificial Intelligence.

VEZHNEVETS A, MNIH V, OSINDERO S, et al., 2016. Strategic attentive writer for learning macroactions[C]//Advances in Neural Information Processing Systems. 3486-3494.

VEZHNEVETS A S, OSINDERO S, SCHAUL T, et al., 2017. Feudal networks for hierarchical reinforcement learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70.

JMLR. org: 3540-3549.

VINYALS O, BABUSCHKIN I, CZARNECKI W M, et al., 2019. Grandmaster level in starcraft ii using

multi-agent reinforcement learning[J]. Nature, 575(7782): 350-354.

WIERING M, SCHMIDHUBER J, 1997. HQ-learning[J]. Adaptive Behavior, 6(2): 219-246.

314

11 多智能体强化学习

在强化学习中，复杂的应用需要多个智能体的介入来同时学习并处理不同的任务。然而，智

能体数目的增加会对管理其之间的交互带来挑战。根据每个智能体的优化问题，均衡的概念被提

出并用于规范多智能体的分布式动作。结合典型的多智能体强化学习算法，我们进一步分析了在

多种场景下智能体之间合作与竞争的关系，以及一般性的博弈架构如何用于建模多智能体多种类

型的交互场景。通过对博弈架构中每一部分优化和均衡的分析，每一个智能体最优的多智能体强

化学习策略将得到指引和进一步探索。

11.1 简介

基于规则和环境反馈，一个智能体可以通过强化学习学到动作策略并且表现优异。然而，人

工智能中有很多应用具有大规模的环境背景和复杂的学习任务，这不仅要求一个智能体做出明智

的动作，而且我们希望有多个智能体可以通过有限的通信共同做出明智的决策。因此，我们需要

在多个智能体的情况下为每一个智能体制定有效的强化学习策略。考虑到多个智能体之间的相互

交流和影响，多智能体强化学习的概念被提出并受到广泛的关注和探索。

为了方便分析和理解，在多智能体强化学习中，我们列出三个基本元素，分别是智能体、策

略和效用函数。

• 智能体: 智能体是一群具有自主决策意识的个体，它们中每一个个体都可以独立地和环境

进行交互。为了能使自己获得最大的收益和最小的损失，每一个智能体会基于对其他智能

体动作的观察、学习并制定自己的动作策略。在本章我们要考虑的情况中，会有多个智能

体同时存在。智能体的数目为 1 时即普通强化学习的场景。

• 策略: 在多智能体强化学习中，每一个智能体会制定策略来最大化自身的收益并且最小化

315

第 11 章 多智能体强化学习

损失。其制定的策略基于智能体对环境的感知，并且会被其他智能体的策略影响。

• 效用函数: 考虑到每个智能体自身的需求和对环境及其他智能体的依赖关系，每一个智能

体都会有独自的效用函数。一般来说，效用函数定义为智能体在实现各种目标时获得的总

收益和总成本之差。在多智能体的场景下，在对周围环境和其他智能体的学习过程中，每

一个智能体会以最大化自身的效用函数为最终目标。

在多智能体强化学习中，每一个智能体会有自身的效用函数，并以最大化其效用价值为目标，

基于对环境的观察和交互自主地学习并制定策略。由于每一个智能体在自主学习时不会考虑到其

策略对其他智能体效用函数的影响，因此，在多个智能体相互交互影响下会存在竞争或合作的情

况。考虑到智能体之间相互交互的多种复杂情况，博弈论普遍被用来对智能体的决策进行具体分

析 (Fudenberg et al., 1991)。针对不同的多智能体强化学习的场景，可以采用不同的博弈框架来模

拟交互的场景，整体上可以分为如下三种类别。

• 静态博弈: 静态博弈是模拟智能体间交互的最基本形式。在静态博弈中，所有智能体同时做

出决策，并且每一个智能体只做出一个决策动作。由于每个智能体只行动一次，所以其可

以做出一些出乎常规的欺骗和背叛策略来使自己在博弈中获益。因此，在静态博弈中，每

一个智能体在制定策略时需要考虑并防范其他智能体的欺骗和背叛来降低自身的损失。

• 重复博弈: 重复博弈是多个智能体在相同的状态下采取重复多次的决策动作。因此，每个

智能体的总效益函数是其在每次决策动作所带来的效益价值的总和。由于所有智能体会做

出多次动作，当某个智能体在某一次动作时采取了欺骗或背叛的决策时，在未来的动作中，

该智能体可能会收到其他智能体的惩罚和报复。因此，相比于静态博弈，重复博弈大大地

避免了多智能体之间恶意的动作决策，从而整体上提高了所有智能体总效益价值之和。

• 随机博弈: 随机博弈（或马尔可夫博弈）可以看作是一个马尔可夫过程，其中存在多个智能

体在多个状态下多次做出动作决策。随机博弈模拟出了多个智能体做多次决策的一般情况，

每个智能体会根据自身所处的状态，通过对环境的观察和对其他智能体动作的预测，做出

提升自身效用函数的最佳动作决策。

在本章中，在单智能体强化学习的基础上，我们更多地关注智能体之间的交互和关联，寻求

在多智能体强化学习中所有智能体之间达到均衡状态，并且每个智能体都能获得相对较高和稳定

的效用函数。

11.2 优化和均衡

由于每个智能体以提高自身的效用函数为目标，多智能体强化学习可以看成一个求解多个优

化问题的数学问题，其中每个智能体对应一个优化问题。为了分析智能体之间的关系，设有 m 个

智能体，用 X = X1 × X2×, · · · , ×Xm 表示所有智能体的决策空间，用 u = (u1(x), · · · , um(x))

表示所有智能体在采取决策 x, x ∈ X 时的效用空间。因此，每个智能体 i，∀i ∈ {1, 2, · · · , m}，

需要在和其他智能体的交互情况下，最大化其自身的效用函数。在多智能体强化学习下，一般来

316

11.2 优化和均衡

说，就是同时或者顺序求解多个优化问题，来保证每个智能体都能获得最优的效用函数。

因为每个智能体的收益函数和所有智能体的决策动作相关，在求解多智能体的优化问题中，

我们希望所有智能体最终都能有稳定的决策策略，在其状态下，每一个智能体都不能通过只改变

自身的决策策略而使自己获得更高的收益。因而，在多智能体强化学习中，我们提出了均衡的概

念。为了更好地理解和分析，在不失一般性的前提下，我们通过胆小鬼博弈（Chicken Dare Game，

或被称为斗鸡博弈）及其延伸来介绍多种均衡概念。经典的胆小鬼博弈是一种静态博弈模型，其

中涉及两个智能体之间的交互关系。两个智能体可以相互独立地选择怯懦（简称为“C”）或者勇

敢（简称为“D”）作为自身的动作决策。基于两个智能体所有可能的动作决策，两个智能体获得

的效用价值由图 11.1 所示。当两个智能体选择“D”即勇敢时，两者各自都会获得最低的效用价

值 0；当其中一个智能体选择“D”即勇敢，另一个智能体选择“C”即怯懦时，选择勇敢的智能

体获得其最佳的效用价值 6，选择怯懦的智能体获得相对较低的效用价值 3。当两个智能体都选

择“C”即怯懦时，两者都会获得相对较高的收益 5。

图 11.1 胆小鬼博弈

11.2.1 纳什均衡

根据图 11.1 所示的胆小鬼博弈 (Rapoport et al., 1966) 的场景，我们设定规则要求两个智能体

同时做出决策。因而，当两个智能体同时选择“C”时，假设对方在保持当前决策动作，每个智

能体都想要选择“D”而使自己获得更高的效用价值。当两个智能体同时选择“D”时，两者都只

能获得最低的效用价值 0，因而希望改变策略“D”而获得更高的收益。然而，当一个智能体选

择“C”，而另一个智能体选择“D”时，在假设对方不会改变当前决策动作的前提下，两个智能

体都不能只单独改变自己的决策而提高自己的效用价值。因此，我们称一个智能体选择“C”，而

另一个智能体选择“D”这种情况在当前场景下达到了纳什均衡 (Nash et al., 1950)，其定义如下：

定义 11.1 令 (X ,u) 表示 m 个智能体下的静态场景，其中 X = X1 × X2×, · · · , ×Xm 表示智能体

的策略空间。当所有智能体采取策略 x，其中 x ∈ X 时，u = (u1(x), · · · , um(x)) 表示智能体的

效用空间。我们同时设 xi 为智能体 i 的策略，设 x−i 为除智能体 i 外其他所有智能体的策略集

合。当 ∀i, xi ∈ Xi 时，

ui(x

∗

i

, x

∗

−i

) ⩾ ui(xi

, x

∗

−i

). (11.1)

策略 x

∗ ∈ X 使当前场景达到纳什均衡。

317

第 11 章 多智能体强化学习

纯策略纳什均衡

根据定义所示，在多智能体强化学习的静态场景下，所有智能体同时采取一次决策动作。在

其他智能体的决策动作不改变的前提下，每个智能体不能通过改变当前的决策动作而获得更高的

收益，我们称所有的智能体达到纯策略纳什均衡。在胆小鬼博弈的例子中存在两个纯策略纳什均

衡，其中一个智能体选择怯懦动作，另一个智能体选择勇敢动作。一般来说，纯策略纳什均衡不

一定存在，因为智能体的纯策略动作不能保证其他智能体通过改变当前的动作来获得更高的效用

价值。

混合策略纳什均衡

在纯决策动作之外，每个智能体还可以制定并采取决策的策略，并根据策略基于不同的概率

随机选择不同的决策动作。因而，智能体制定策略可以在其相互交互的过程中带来随机性和不可

确定性，并可以考虑其他智能体的策略调整改变自己的策略组合而达到混合策略纳什均衡。一般

来说，混合策略纳什均衡总是存在。以胆小鬼博弈为例子，我们设智能体 1 采取怯懦的概率是 p，

相对应地，其采取勇敢的概率是 1 − p。为了保证智能体 1 策略的制定没有使其对手智能体 2 的

动作有偏见，从而使智能体 2 产生最佳的纯策略动作，需要满足如下关系：

5p + 3(1 − p) = 6p + 0(1 − p). (11.2)

我们得到 p = 0.75。从智能体 2 的角度来说，依此类推，即当两个智能体选择“C”的概率

均为 0.75，并且选择"D" 的概率为 0.25 时，两个智能体达到了混合策略纳什均衡，其中每个智能

体获得的期望效益价值为 4.5。

综上所述，我们将胆小鬼博弈的结果用图 11.2 表示，其中 X 轴表示智能体 1 的效用函数，Y

轴表示智能体 2 的效用函数。基于图 11.1 表示的智能体之间的关系，点 A 对应两个智能体同时

选择“C”的情况，点 B 表示智能体 1 采用动作“C”智能体 2 采用动作“D”的结果，点 C 表

示智能体 1 采用动作“D”智能体 2 采用动作“C”的结果，点 D 对应两个智能体同时选择“D”

的情况。因此，两个智能体采取所有可能的决策策略结果落在在四边形 ABDC 区域中，其中点

B 和点 C 为纯策略纳什均衡的结果，线段 BC 的中点 E 即为混合策略纳什均衡的结果。对于所

有纳什均衡的结果，两个智能体效用函数之和相同，等于 9。

11.2.2 关联性均衡

在胆小鬼博弈的纳什均衡中，两个智能体总效益之和为 9，小于所有两个智能体总效益之和

的最大可能值 10。然而，两个智能体需要都选择策略“C”使得总效益之和达到 10，在绝对分布

式的方式下是不稳定的。因此，为了更好地提高所有智能体的总效益价值并同时保证每个智能体

能够拥有稳定的收益，关联性均衡被进一步提出。

318

11.2 优化和均衡

图 11.2 胆小鬼博弈中的纳什均衡

在胆小鬼博弈的例子中，我们设定两个智能体选择“CC”（第一个“C”对应智能体 1 的决

策动作，第二个“C”对应智能体 2 的决策动作），“CD”、“DC”和“DD”的可能性为 v。当两个

智能体相关联并且设定每种情况的可能性为 v =



1/3, 1/3, 1/3, 0



时，两个智能体的总效用价值

为 9.3333，比纳什均衡的结果要高。不仅如此，假设当智能体 1 宣布将选择“C”时，为了满足每

种情况的可能性保持为 v，其对手智能体 2 需要采取混合策略，其选择“C”和“D”的可能性分

别均为 0.5。那么当智能体 1 真实选择“C”的时候，能获得的效益价值为 0.5 × 5 + 0.5 × 3 = 4。

但如果智能体 1 私自改变了决策动作“D”，在智能体 2 策略不发生改变的情况下，智能体 1 能够

收到的效益价值为 0.5 × 6 + 0.5 × 0 = 3，低于选择“C”情况下的效益价值 4。相对应地，当智

能体 1 宣布将选择“D”时，为了满足每种情况的可能性保持为 v，其对手智能体 2 需要以 100%

的概率做出决策动作“C”，那么智能体 1 依然不能将宣布的动作私自改变到“C”而获得更高的

效用价值。因此，其相关联的概率分布 v 让两个智能体达到了关联性均衡，具体定义如下：

定义 11.2 关联性均衡 (Aumann, 1987) 定义为智能体之间能够相关联实现概率分布 v，并且满足

如下关系

X

x−i∈X−i

v(x

∗

i

, x−i)[ui(x

∗

i

, x−i) − ui(xi

, x−i)] ⩾ 0, ∀xi ∈ Xi

, (11.3)

其中 Xi 表示智能体 i 的策略空间，X−i 表示除智能体 i 外所有其他智能体的策略空间。

因此，在假设两个智能体服从相关联分布的前提下，每个智能体不能改变当前相关联的策略

而获得更高的效用价值。为了更直观地表现出关联性均衡的优势，我们在图 11.3 中用点 F 标注

出本例中关联性均衡的结果。一般来说，在图中 ABC 区域中，只要满足公式 (11.3) 所示的关系，

其结果均可达到关联性均衡。

319

第 11 章 多智能体强化学习

图 11.3 胆小鬼博弈中的关联性均衡

11.2.3 斯塔克尔伯格博弈

除了同时做出决策的情况，智能体之间还可能会顺序做出决策。在顺序做出决策的情况下，

智能体会分别被定义为领导者和追随者，其中领导者会先做出决策，追随者随后做出决策 (Bjorn

et al., 1985)。因而，领导者在决策时会有先发优势（First-Mover Advantage），可以通过预测追随

者对其决策的反应来决定能够给自身带来最大收益的最佳决策。在胆小鬼博弈的例子中，如果我

们扩展场景使两个智能体的决策是顺序决定的，并令智能体 1 为领导者，智能体 2 为追随者，那

么智能体 1 会选择策略动作“D”，因为智能体 1 可以预测到，当其选择“D”时，为了获得更高

的收益，智能体 2 一定会选择动作“C”，从而使自己的效用价值为所有可能结果中的最大值 6，

并且在顺序执行的前提假设下，两个智能体能够达到斯塔克尔伯格均衡。斯塔克尔伯格均衡的定

义如下：

定义 11.3 设



(X , Π),(g, f)



为顺序执行的场景，其中有 m 个领导者同时先做出策略动作，n

个追随者同时后做出策略动作。X = X1 × X2×, · · · , ×Xm 和 Π = Π1 × Π2×, · · · , ×Πn 分别

表示领导者和追随者的策略空间，g = (g1(x), · · · , gm(x)) 为领导者 x ∈ X 的效用函数。f =

(f1(π), · · · , fn(π)) 为追随者 π ∈ Π 的效用函数。设 xi 为领导者 i 的决策策略，x−i 为除领导者

i 外其他领导者的决策策略集合。同样地，设 πj 为追随者 j 的决策策略，π−j 为除追随者 j 外其

他追随者的决策策略集合。那么对于 ∀i, ∀j xi ∈ Xi

, πj ∈ Πj，策略集合 x

∗ ∈ X，π

∗ ∈ Π 可以达

到多领导者多追随者的斯塔克尔伯格均衡，并且满足如下关系：

gi(x

∗

i

, x

∗

−i

,π

∗

) ⩾ gi(xi

, x

∗

−i

,π

∗

) ⩾ gi(xi

, x−i

,π

∗

), (11.4)

fj (x,π

∗

j

,π

∗

−j

) ⩾ fj (x,πj ,π

∗

−j

). (11.5)

32

11.3 竞争与合作

11.3 竞争与合作

在上一节中，我们以胆小鬼博弈为例子介绍了多智能体强化学习中优化和均衡的概念。除此

之外，在不同的应用中，多智能体之间的关系会多种多样，在本节，我们会更多分析在分布式的

场景下，多智能体之间竞争和合作的关系。在没有特殊说明的情况下，我们设所考虑的场景中存

在 m 个智能体，X = X1 × X2×, · · · , ×Xm 表示所有智能体的策略空间，u = (u1(x), · · · , um(x))

表示所有智能体在采用策略集合 x 的情况下的效用集合，其中 x ∈ X。

11.3.1 合作

当多个智能体相互合作的时候，一般来说，所有智能体的效用价值之和会期望高于不合作的

情况下的效用价值之和，并且在分布式的场景下，每个智能体会更多地考虑自身的效用价值。因

此，为了使智能体能够加入合作联盟，每个智能体自身需要在合作的情况下获得比不在合作的时

候更高的效用价值。因而，其对智能体 i, ∀i ∈ {1, 2, · · · , m} 的优化问题可以归纳为

max

xi

k

X=m

k=1

uk(xk|x−k),

s.t. ui(x

∗

i

|x

∗

−i

) ⩾ ui(xi

|x

∗

−i

), (11.6)

11.3.2 零和博弈

零和博弈 (VINCENT, 1974) 在许多应用中被频繁使用。为了简化问题但不失一般性，我们设

有两个智能体，每个智能体可以采取策略“A”或者“B”，因而，在博弈中不同情况下的效用函

数如图 11.4 所示，其中，每种情况下智能体收益价值之和总是为零。在一般性的零和博弈中，每

个智能体需要基于对其他智能体的动作预测最大化其自身的效用价值并且最小化其他智能体的

效用价值之和。因而，其对智能体 i, ∀i ∈ {1, 2, · · · , m} 优化问题可以总结如下

max

xi

min

x−i

ui

. (11.7)

图 11.4 零和博弈

321

第 11 章 多智能体强化学习

基于此优化问题，在文献 (Littman, 1994) 对一个简化的踢足球问题进行分析并建模为零和博

弈。在足球游戏中，存在两个智能体，每个智能体都努力地把球踢进来提高自身的效用价值并且

防守对方智能体来最小化其对手的效用价值。因此，在该问题中，对于智能体 i，其优化问题具

体表示为

max

πi

min

a−i

X

ai

Q(s, ai

, a−i)πi

, (11.8)

其中 πi 表示智能体 i 的策略，ai 代表智能体基于策略 πi 实际的动作。在足球游戏中，智能体 i

努力提高自己的价值函数，然而其对手采取动作 a−i 努力降低该价值函数。

11.3.3 同时决策下的竞争

除了零和博弈，一般来说，还有很多应用在多种智能体同时做出决策时存在竞争的关系。在

同时决策下的竞争，所有智能体需要在相同的时间下同时做出决策动作，因而其优化问题可以总

结如下：

max

xi

ui(xi|x−i). (11.9)

在文献 (Hu et al., 1998) 中，Q 学习被提出来解决一般情况下多智能体之间的竞争问题。其具

体算法如算法 11.35 所示，基于交互过程中经历的积累，每个智能体 i 都会维护一个 Q 列表，用

于指导指定策略 πi。随着更多经历的积累，Q 列表更新方程如下：

Qi(s, ai

, a−i) = (1 − αi)Qi(s, ai

, a−i) + αi

[ri + γπi(s

′

)Qi(s

′

, a′

i

, a

′

−i

)π−i(s

′

)]. (11.10)

算法 11.35 多智能体一般性 Q-learning

设定 Q 表格中初始值 Qi(s, ai

, a−i) = 1, ∀i ∈ {1, 2, · · · , m}。

for episode = 1 to M do

设定初始状态 s = S0

for step = 1 to T do

每个智能体 i 基于 πi(s) 选择决策行为 ai

, 其行为是根据当前 Q 中所有智能体混合纳什均

衡决策策略

观测经验 (s, ai

, a−i

, ri

, s′

) 并将其用于更新 Qi

更新状态 s = s

′

end for

end for

在多智能体的场景下，由于 Q 列表的更新和其他智能体 π−i 的策略相关，因而，智能体 i 需

要同时建立并估计所有其他智能体的 Q 列表。根据由这些 Q 列表推导出对其他智能体策略 π−i

322

11.3 竞争与合作

的预测，智能体 i 才可以更好制定策略 πi，以使所有智能体的策略集合 (πi

,π−i) 最终达到混合

策略纳什均衡的结果。

除了基本的 Q 学习，其他深度强化学习的方法也在尝试探索在多智能体强化学习中的应用。

基于单智能体深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）算法，多智能体深

度确定性策略梯度（Multi-Agent Deep Deterministic Policy Gradient，MADDPG）(Lowe et al., 2017)

在所有智能体同时做出决策的场景下，为每一个智能体提供策略。MADDPG 算法如算法 11.36 所

示，每个智能体对应一个分布式的行动者（Actor），为其决策提供建议。另一方面，批判者（Critic）

是集中控制的，并整体维护一个和所有智能体动作集合相关的 Q 列表。

算法 11.36 多智能体深度确定性策略梯度

for episode = 1 to M do

设定初始状态 s = S0

for step = 1 to T do

每个智能体 i 基于当前决策策略 πθi 选择决策行为 ai

同时执行所有智能体的决策行为 a = (a1, a2, · · · , am)

将 (s, a, r, s′

) 存在重放缓冲区 M

更新状态 s = s

′

for 智能体 i = 1 to m do

从回访缓冲区 M 中采样批量历史经验数据

对于行动者和批判者网络，计算网络参数梯度并根据梯度更新参数

end for

end for

end for

特别来说，对于每个行动者 i，其期望回报的梯度表示为

▽θ

π

i

J(πi) = E[▽θ

π

i

πi(oi

|θ

π

i

)▽aiQ

π

i

(o1, · · · , om, a1, · · · , am|θ

Q

i

)], (11.11)

其中，设 o1, · · · , om 分别为 m 个智能体的观察样本。πi 为智能体 i 的确定性策略，因而其决策

动作满足 ai = πi(oi)。

相对应地，批判者对于智能体 i 的损失函数是 Q 值的 TD-error，表示为

Li = E[(Q

π

i

(o1, · · · , om, a1, · · · , am|θ

Q

i

) − ri − γQπ

′

i

(o

′

1

, · · · , o′

m, a′

1

, · · · , a′

m|θ

Q′

i

))2

], (11.12)

其中 θ

Q′

i 指 Q 预测的延迟参数，π

′ 表示在延迟参数 θ

π

′

i 下的目标决策策略。

11.3.4 顺序决策下的竞争

在某些应用中，不同类型的智能体可能在做出决策时会有时间先后之分。因而，在竞争

中，多个智能体之间可能会顺序做出决策动作，并且先做出决策的智能体会有先发优势。设

323

第 11 章 多智能体强化学习



(X , Π),(g, f)



为一般情况下 m 个领导者和 n 个追随者的顺序决策场景。其中 X = X1 ×

X2×, · · · , ×Xm 和 Π = Π1 × Π2×, · · · , ×Πn 分别表示领导者和追随者的决策策略空间。设

g = (g1(x), · · · , gm(x)) 为领导者 x ∈ X 的效用函数，f = (f1(π), · · · , fn(π)) 为追随者 π ∈ Π

的效用函数。那么追随者 j, ∀j ∈ {1, 2, · · · , n} 的优化问题可以表示为

max fj (πj |π−j , x). (11.13)

领导者 i, ∀i ∈ {1, 2, · · · , m} 的优化问题为

max gi(xi

|x−i

,π),

s.t. πj = arg max fj (πj |π−j , x), ∀j ∈ {1, 2, · · · , n}. (11.14)

11.4 博弈分析架构

基于对多智能体之间关系的分析，我们总结出一个满足一般性多智能体博弈分析架构，如

图 11.5 所示。在此架构中，我们设定一个循环迭代的场景，其中所有的智能体能够在不同时间

段中多次做出决策。在同一个时间段中，我们将所有智能体进一步分为多个层级，在最高层级的

智能体先做出动作，基于对高层级智能体动作的观察，低层级的智能体相对应地做出利于自身的

决策，并且在每一个层级中，可以存在多个智能体同时做出决策。因而，在不同层级之间，所有

智能体期望达到斯塔克尔伯格均衡，如果多个智能体存在相同层级中，根据这些智能体可否相关

联，期望能够达到纳什均衡或者关联性均衡的结果而使所有智能体获得稳定效用价值。

图 11.5 一般性多智能体博弈分析架构

博弈分析架构一般可以用来建模并处理所有多智能体强化学习的问题。为了更好地测试并且

评估，各种多智能体强化学习平台目前已经建立并广受关注。比如 AlphaStar 可以很好模拟《星际

争霸》游戏中多智能体之间的关系和动作。多智能体互联自动驾驶（MACAD）平台 (Palanisamy,

2019) 很好地学习并且模拟在公路上驾驶汽车的环境场景。谷歌研究足球 (Kurach et al., 2019) 则是

一个模拟多个有自主意识的智能体一起踢足球的平台等等。基于适用于多种不同场景的多智能体

32

参考文献

学习平台，我们期待在博弈分析架构下的多智能体强化学习策略可以得到更具体的分析和研究。

参考文献

AUMANN R J, 1987. Correlated equilibrium as an expression of bayesian rationality[J]. Econometrica:

Journal of the Econometric Society: 1-18.

BJORN P A, VUONG Q H, 1985. Econometric modeling of a stackelberg game with an application to

labor force participation[J].

FUDENBERG D, TIROLE J, 1991. Game theory, 1991[J]. Cambridge, Massachusetts, 393(12): 80.

HU J, WELLMAN M P, 1998. Multiagent reinforcement learning: Theoretical framework and an

algorithm[C]//International Conference on Robotics and Automation (ICRA).

KURACH K, RAICHUK A, STACZYK P, et al., 2019. Google research football: A novel reinforcement

learning environment[Z].

LITTMAN M L, 1994. Markov games as a framework for multi-agent reinforcement learning[C]//

Proceedings of the International Conference on Machine Learning (ICML). 157-163.

LOWE R, WU Y, TAMAR A, et al., 2017. Multi-agent actor-critic for mixed cooperative-competitive

environments[C]//Advances in Neural Information Processing Systems.

NASH J F, et al., 1950. Equilibrium points in n-person games[J]. Proceedings of the national academy of

sciences, 36(1): 48-49.

PALANISAMY P, 2019. Multi-agent connected autonomous driving using deep reinforcement learning[Z].

RAPOPORT A, CHAMMAH A M, 1966. The game of chicken[J]. American Behavioral Scientist, 10

(3): 10-28.

VINCENT P, 1974. Learning the optimal strategy in a zero-sum game[J]. Econometrica, 42(5): 885-891.

325

12 并行计算

基于强化学习低采样效率的问题，并行计算作为解决方案可以高效地加速模型训练过程并提

高学习效果。在本章中，我们将具体介绍强化学习中采用并行计算的系统架构。对应不同的应用

场景，我们分别分析同步通信和异步通信，并详细阐述并行计算在多种网络拓扑结构中的不同运

算方式。通过并行计算，经典的分布式强化学习算法和架构将被逐一介绍并互相比较。最终，我

们将总结一般性的分布式计算架构的基本构成和组成元素。

12.1 简介

在深度强化学习中，针对模型的训练需要大量数据。以 OpenAI Five (OpenAI et al., 2019) 为

例，为了使智能体能够通过学习在 Dota 游戏中做出明智的决策，每两秒钟就大概有 2 百万组数

据被用来训练模型。不仅如此，从优化的角度，特别在基于策略梯度的方法中，大批量的训练数

据能够有效地降低结果的方差。然而，由于在强化学习中，智能体和环境的交互限制于在时间上

顺序执行，强化学习的算法在采集数据上往往存在低效率的问题，从而带来不理想的训练结果和

缓慢的收敛速度。并行计算，即对相互分离独立的任务以同时计算的方式，为解决强化学习问题

带来有效的解决方案。一般来说，并行计算中的并行性可以体现在以下两个方面：

• 计算的并行性: 数据计算是包括特征工程、模型学习，以及结果评估等任务在内的核心过

程，是由每一个计算单元具体操作执行的，不同的操作单元可以根据任务的大小和种类灵

活地结合并扩展到不同的规模。在同等级规模的任务中，计算的性能和表现取决于以下两

类策略：一类是合并多个计算单元共同计算一个任务，另一类是分别使用多个计算单元同

时并行计算多个任务。基于第一类计算策略，随着越来越多的计算单元用于一个计算任务，

完成任务的效率会先上升，然后会因为一些瓶颈的环节而逐渐收敛。因而在深度强化学习

326

12.2 同步和异步

中，在有计算资源充足的前提下，将一个计算任务拆分成多个相互独立的子任务，并且将

每个子任务分配适当的计算资源进行并行计算，是寻求计算效率提升的重要方向。

• 数据传输的并行性: 在拥有充足的计算资源时，计算资源之间的数据传输会成为解决问题

效率的瓶颈。一般来说，为了避免传输的过多冗余，平衡网络中传输的数据量并且降低传

输延时，基于不同的应用提出了不同的数据传输网络拓扑模型。在并行计算中，由于多个

进程或线程可能同时需要完成不同的任务。管理数据的传输并且在有限传输带宽的网络中，

保证传输效率是极具挑战的。

在监督学习的设定中，一种简单的提升学习速度的方法是同时训练多种不同的训练数据。然

而，在深度强化学习中，智能体和环境需要在时间上顺序多次交互来逐步获得有效信息，因而不

可能把所有数据集合在一起让模型同时学习。在深度强化学习中提高并行计算的能力可以通过让

智能体在训练中同时并行学习多个训练轨迹，或者可以积累批量的数据来训练深度强化学习模型

中的参数。在本章中，我们即从计算的并行性和数据传输的并行性的角度分析深度强化学习中可

以采取并行计算的方面，并且在解决大规模深度强化学习问题的同时，我们将介绍当前重要的分

布式计算的算法和架构。

12.2 同步和异步

在并行计算中，最普及的数据计算和传输方法采用类似星形的拓扑结构，是由一个主节点和

多个奴隶节点组成的。主节点整体上管理数据信息，完成从奴隶节点的数据分发和收集。基于从

奴隶节点收集到的数据，总体的网络参数将得到学习并更新。每一个奴隶节点，相应地，会从主

节点收到分配的数据，进行具体的数据计算，并将计算的结果提交给主节点。由于在主节点的管

理下，同时可以有多个奴隶节点进行数据计算。这样的并行计算可以合作高效地完成大规模模型

参数训练问题。

星形结构在解决深度强化学习的问题中得到了广泛的应用，例如，在 Actor-Critic 方法的并

行计算版本，通常会采用一个主节点，以及多个奴隶节点。每个奴隶节点会维护一个深度策略网

络，该策略网络的结构和其他奴隶节点和主节点一样。因此，奴隶节点在初始化的时候会从主节

点，同步策略网络的参数，然后其将独立与环境交互学习。在与环境交互之后，奴隶节点将再次

和主节点通信，将学习到的信息提交给主节点，其中学习到的信息基于不同的架构可以是单步探

索所得到的经验、连续探索的轨迹的经验、存储的带有权重的探索经验、网络参数的梯度信息，

等等。在收集到每个奴隶节点探索并学习的经验和反馈之后，主节点将更新其网络的参数，并同

步给所有奴隶节点，用于奴隶节点下一轮的探索。

星形拓扑结构清晰地将任务细分，通过多个奴隶节点的并行计算加速了智能体对策略的学

习。然而基于不同的计算能力，不同奴隶节点完成探索并收集经验的时间可能不会完全相同，因

而制定数据传输的模式会因所解决的问题和系统架构的不同而有所变化。一般来说，其分为同步

通信和异步通信两种模式。

327

第 12 章 并行计算

同步通信模式如图 12.1 所示，其中红色的区间代表数据通信所使用的时间，蓝色的区间表示

与环境交互和数据计算所使用的的时间。值得注意的是，在同步通信模式中，所有奴隶节点将使

用完全相同的时间区间进行信息交互。主节点将用相同固定的时间段同时与所有奴隶节点进行通

信。然而，有更强算力的奴隶节点不得不等待其他所有弱算力的节点完成本轮计算任务之后才能

继续和主节点通信，因而同步通信模式虽然对主节点来说在收集分析奴隶节点计算结果的时间分

配上更为清晰固定，但是在奴隶节点中会有大量计算资源因为等待同步而造成浪费。

图 12.1 同步通信（见彩插）

为了减少奴隶节点的等待时间，提升计算资源的使用效率，异步通信模式相应被提出。如

图 12.2 所示，只要奴隶节点完成了本轮的计算或探索任务，可以立刻将信息提交给主节点。只有

奴隶节点提交信息，主节点才会用收集到的信息更新网络参数并与该奴隶节点进行同步，使其奴

隶节点能够继续开始下一轮的计算或探索任务。基于此种方式，主节点和众多奴隶节点的数据通

信将会在多段不同的时间区间内完成，主节点需要不定时地与不同的奴隶节点进行通信，确保奴

隶节点中的计算资源得到了充分的利用。

图 12.2 异步通信（见彩插）

328

12.3 并行计算网络

12.3 并行计算网络

星形拓扑结构采用了集中控制式并行计算的方式，其中存在一个主节点管理并维护整个系

统，使其确保所有分布式计算的任务能够进行得井井有条。然而，从另一个方面来说，主节点同

样也是整个系统最薄弱的部分，为了确保计算的高性能，主节点需要在处理数据上比奴隶节点更

具效率。另外，由于所有奴隶节点可能需要将信息同时传输给主节点，为了避免过大的传输延时，

主节点的数据传输带宽需要足够大。不仅如此，系统的稳定性将大大取决于主节点，如果主节点

有任何的停机事件，整个系统将停止工作，即使所有的奴隶节点存在大量可用的计算资源。

综上所述，对于很多对稳定性和大规模并行计算有需求的应用来说，拥有一个分布式数据计

算和通信的结构十分必要。我们假设有多个相互独立的进程，其中每个进程会建立并维护一个结

构相同的深度学习网络，并且进程与进程之间会保持频繁的通信，以使网络参数保持同步。由于

每个进程需要与所有其他进程通信，当进程的数量增加时，进程间通信的成本将指数级上升。为

了降低冗余的通信并高效地实现信息同步，具有消息传递接口（Message Passing Interfaces，MPI）

的进程间通信（Inter-Process Communication，IPC）方式将被采纳。一般来说，MPI 提供基本的进

程进行数据发送，广播和接收的接口标准，基于这些标准，不同的通信结构得以进一步提出并由

此提高信息交流效率。一些经典的通信结构举例如下。

• 树形结构通信: 设系统中有 N 个进程，当其中一个进程希望将其自身的网络参数信息广播

给其他 N − 1 个进程时，可以通过如图 12.3 所示的树形结构通信。在树形通信结构中，该

进程首先会将信息发送给其周围 m − 1 个进程，然后在下一步迭代中，其周围的 m − 1 个

进程均会将信息并行发送新的 m − 1 个不同的进程。因此，随着并行通信数目的增加，该

进程仅需要 ⌈logm N⌉ 次迭代即可将信息发送给所有其他 N − 1 个进程，其中每个进程只需

要完成 (m − 1)⌈logm N⌉ 次通信。相比于星形结构中每个进程都需要把信息分别发送给所

有其他的进程，树形结构通信通过提高迭代次数使用并行通信的方式大大降低了所有进程

发送信息的总数。

图 12.3 树形结构通信

329

第 12 章 并行计算

• 蝴蝶形结构通信: 当所有 N 个进程需要同时将自身的信息广播给所有其他进程时，每个进

程将可以通过树形结构的形式进行通信，从而叠加为蝴蝶形通信结构。如图 12.4 所示，蝴

蝶形通信结构中，每个进程首先将信息发送给其周围 m − 1 个进程，并同时收集并处理其

他进程发来的信息用于下一次迭代中信息的发送。由于每个进程会在每次迭代中收集并处

理其他进程发来的信息，所以其分布式的信息发送和处理效率得到了进一步的提升。一般

来说，所有进程需要 (m − 1)⌈logm N⌉ 次迭代完成所有信息的通信，并且每个进程一共只

需要完成 (m − 1)⌈logm N⌉ 次通信。另外，在这个系统中无论其中哪一个进程出现故障中

断，其他进程之间仍可以继续完成信息的同步而不受到影响。

图 12.4 蝴蝶形结构通信

基于不同结构的通信，深度强化学习算法中并行数据计算和数据传输的方式是灵活且多样

的。对于不同的应用，为了提高并行性和处理效率，深度强化学习算法的架构也会相应地有所调

整，在下一节中，我们将进一步分析并总结深度强化学习中一般性的分布式计算架构。

12.4 分布式强化学习算法

12.4.1 异步优势 Actor-Critic

异步优势 Actor-Critic（Asynchronous Advantage Actor-Critic，A3C）(Mnih et al., 2016) 是基于

优势 Actor-Critic（Advantage Actor-Critic，A2C）算法的分布式版本，如图 12.5 所示，多个行动-学

习者（Actor-Learner）将与多个独立且完全相同的环境交互，并采用 A2C 算法学习并更新网络参

数，因而每个行动-学习者都需要维护一个策略网络和一个价值网络来指导其在与环境的交互中

采取明智的决策动作。为了使所有的行动-学习者的网络参数初始化相同并保持同步，在所有行

动-学习者之外将建立参数服务器，并使其支持对所有行动-学习者的异步通信。

从每一个行动-学习者的角度，其具体的学习算法如算法 12.37 所述，在每个学习周期中，通

过异步通信，每个行动-学习者首先会从参数服务器中同步其网络参数，基于更新的策略网络，行

动-学习者会做出决策动作并与环境最多交互 tmax 次，与环境交互探索的经验会被收集并用来训

练其自身的策略网络和价值网络，分别得到两个网络参数的更新梯度 dθ 和 dθv。在行动-学习者

330

12.4 分布式强化学习算法

图 12.5 A3C 架构

算法 12.37 异步优势 Actor-Critic （Actor-Learner）

超参数: 总探索步数 Tmax，每个周期内最多探索步数 tmax。

初始化步数 t = 1

while T ⩽ Tmax do

初始化网络参数梯度: dθ = 0 和 dθv = 0

和参数服务器保持同步并获得网络参数 θ

′ = θ 和 θ

′

v = θv

tstart = t

设定每个探索周期初始状态 St

while 达到终结状态 or t − tstart == tmax do

基于决策策略 π(St|θ

′

) 选择决策行为 at

在环境中采取决策行为，获得奖励 Rt 和下一个状态 St+1。

t = t + 1, T = T + 1。

end while

if 达到终结状态 then

R = 0

else

R = V (St|θ

′

v

)

end if

for i = t − 1, t − 2, · · · , tstart do

更新折扣化奖励 R = Ri + γR。

积累参数梯度 θ

′

, dθ = dθ + ∇θ

′ log π(Si

|θ

′

)(R − V (Si

|θ

′

v

))。

积累参数梯度 θ

′

v

, dθv = dθv + ∂(R − V (Si

|θ

′

v

))2/∂θ′

v 。

end for

基于梯度 dθ 和 dθv 异步更新 θ 和 θv

end while

331

第 12 章 并行计算

和环境交互 Tmax 次后，行动-学习者会将两个网络所有累积的梯度之和提交给参数服务器，使其

能够分别异步更新网络服务器中的网络参数 θ 和 θv。

12.4.2 GPU/CPU 混合式异步优势 Actor-Critic

为了更好地利用 GPU 的计算资源从而提高整体计算效率，A3C 进一步优化提升为 GPU/CPU

混合式异步优势 Actor-Critic（Hybrid GPU/CPU A3C，GA3C）(Babaeizadeh et al., 2017)，如图 12.6

所示，在学习模型与环境的交互过程中，GA3C 算法主要由智能体、预测者（Predictor）和训练

者（Trainer）三部分组成，每一部分的功能具体如下。

图 12.6 GA3C 架构

• 智能体: 在 GA3C 算法中，多个智能体分别与模拟的环境进行交互，然而每个智能体自身不

需要维护一个策略网络来指导其做出决策动作，而是基于当前的状态 St，将如何决策的请

求发送给预测序列，预测者则会根据整体策略网络顺序为预测序列中的请求提供决策建议。

当智能体采取 At 决策动作，通过与环境的交互获得奖励 Rt 并进入状态 St+1 时，智能体会

将其探索经验的信息 (St, At, Rt, St+1) 发送到训练队列，用于学习网络参数的训练提升。

• 预测者: 当预测者从智能体中收集决策请求并存储在预测队列中，再进行模型推论时，将从

预测队列中批量获取决策请求，并将其输入决策网络中，从而为每一个请求得到建议的决

策动作。由于批量的数据输入使得模型在推论时可以利用 GPU 的并行计算能力，因而提升

了学习模型的计算效率。基于不同的请求数量，预测者和其预测队列的数目可以随之调整

用于控制信息的处理速度，降低计算延迟，从而进一步提升计算效率。

332

12.4 分布式强化学习算法

• 训练者: 在收到多个智能体的交互经验信息后，训练者会将信息存储在训练队列中，并从中

批量选取数据，用于整体策略网络和价值网络的模型训练。同样，在模型训练中，批量的

信息输入利用 GPU 的并行计算能力提升了计算效率，同时也降低了训练结果的方差。

12.4.3 分布式近端策略优化

分布式近端策略优化（Distributed Proximal Policy Optimization，DPPO）是 PPO 算法的分布

式版本，如图 12.7 所示，其中领导者和工人分别与 A3C 算法中的参数服务器和行动-学习者的功

能相对应。DPPO 算法将数据的采集和梯度计算分布在多个工人中执行，从而大大降低了学习的

时间。周期性地接收每一个工人提交的平均梯度值，领导者会更新其自身的网络参数并将最新的

网络参数同步更新给所有工人。

图 12.7 DPPO 架构

DPPO 算法的伪代码从领导者（Chief）和工人（Worker）的角度分别由算法 12.38、算法 12.39

和算法 12.40 所述，由于工人可以基于 PPO 算法两种版本 PPO-Penalty 和 PPO-Clip 中的一个，因

而本节相对应地提出两种 DPPO 算法，分别为 DPPO-Penalty 和 DPPO-Clip。这两种算法在领导者

的部分是相同的，唯一的区别是工人计算梯度的方法，具体可见如下伪代码。

算法 12.38 DPPO (Chief)

超参数: workers 数目 W，可获得梯度的 worker 数目门限值 D, 次迭代数目 M, B。

输入: 初始全局策略网络参数 θ，初始全局价值网络参数 ϕ。

for k = 0, 1, 2, · · · do

for m ∈ {1, · · · , M} do

333

第 12 章 并行计算

等待至少可获得 W − D 个 worker 计算出来梯度 θ，去梯度的均值并更新全局梯度 θ。

end for

for b ∈ {1, · · · , B} do

等待至少可获得 W − D 个 worker 计算出来梯度 ϕ，去梯度的均值并更新全局梯度 ϕ。

end for

end for

领导者从工人中收集网络参数的梯度信息并用于更新其自身的网络参数。由算法 12.38 所示，

在每一次迭代中，策略网络和价值网络分别将执行 M 和 B 次子迭代。在每一次子迭代中，领导

者至少等待所有工人提交 (W − D) 组梯度数据，然后用所有这些梯度的均值来更新网络参数。更

新的网络参数将会和所有工人同步，用于其之后的采样和梯度计算。

从自身的角度会收集数据样本并计算梯度，然后将梯度传递给领导者。算法 12.39 和算

法 12.40 除在计算策略梯度的部分外大致相同，在每次迭代中，工人首先会收集一组数据 Dk，

并根据收集的数据计算算法中的 Gˆ

t 或 Aˆ

t，将当前探索的策略 πθ 存储为 πold，在策略网络和价

值网络中分别重复 M 和 B 次子迭代过程。

算法 12.39 DPPO (PPO-Penalty worker)

超参数: KL 惩罚系数 λ, 自适应参数 a = 1.5, b = 2, 次迭代数目 M, B。

输入: 初始局部策略网络参数 θ, 初始局部价值网络参数 ϕ。

for k = 0, 1, 2, · · · do

通过在环境中采用策略 πθ 收集探索轨迹 Dk = {τi}

计算 rewards-to-go Gˆ

t

基于当前价值函数 Vϕk 计算对 advantage 的估计，Aˆ

t（可选择使用任何一种 advantage 估计方

法）。

存储部分轨迹信息

πold ← πθ

for m ∈ {1, · · · , M} do

JPPO(θ) = X

T

t=1

πθ(At|St)

πold(At|St)

Aˆ

t − λKL[πold|πθ] − ξ max(0, KL[πold|πθ] − 2KLtarget)

2

if KL[πold|πθ] > 4KLtarget then

break 并继续开始 k + 1 次迭代

end if

计算 ∇θJPPO

发送梯度数据 θ 到 chief

等待梯度被接受或被舍弃，更新网络参数。

334

12.4 分布式强化学习算法

end for

for b ∈ {1, · · · , B} do

L(ϕ) = −

PT

t=1(Gˆ

t − Vϕ(St))2

计算 ∇ϕL

发送梯度数据 ϕ 到 chief

等待梯度被接受或被舍弃，更新网络参数。

end for

计算 d = Eˆ

t



KL[πold(·|St), πθ(·|St)]

if d < dtarget/a then

λ ← λ/b

else if d > dtarget × a then

λ ← λ × b

end if

end for

算法 12.40 DPPO (PPO-Clip worker)

超参数: clip 因子 ϵ, 次迭代数目 M, B。

输入: 初始局部策略网络参数 θ, 初始局部价值网络参数 ϕ。

for k = 0, 1, 2, · · · do

通过在环境中采用策略 πθ 收集探索轨迹 Dk = {τi}

计算 rewards-to-go Gˆ

t

基于当前价值函数 Vϕk 计算对 advantage 的估计，Aˆ

t（可选择使用任何一种 advantage 估计方

法）。

存储部分轨迹信息

πold ← πθ

for m ∈ {1, · · · , M} do

通过最大化 PPO-Clip 目标更新策略：

JPPO(θ) = 1

|Dk|T

X

τ∈Dk

X

T

t=0

min 

πθ(At|St)

πold(At|St)

Aˆ

t(

π(At|St)

πold(At|St)

, 1 − ϵ, 1 + ϵ)Aˆ

t



计算 ∇θJPPO

发送梯度数据 θ 到 chief

等待梯度被接受或被舍弃，更新网络参数。

end for

for b ∈ {1, · · · , B} do

335

第 12 章 并行计算

通过回归均方误差拟合价值方程：

L(ϕ) = −

1

|Dk|T

X

τ∈Dk

X

T

t=0



Vϕ(St) − Gˆ

t

2

计算 ∇ϕL

发送梯度数据 ϕ 到 chief

等待梯度被接受或被舍弃，更新网络参数。

end for

end for

在 DPPO-Clip 中，网络参数 λ 会在所有工人中共享，但是否更新取决于每一个工人计算的平

均 KL 散度。另外，在工人们共享的数据中建议使用统计值，例如，对观察到的数据，奖励和优

势函数通过计算均值和标准差，使其具有归一化。另外，在 DPPO-Clip 算法中，当 KL 散度超过

一定数值后会添加额外的处罚项。对于策略网络，在每次子迭代过程中会采用早停法来进一步提

高算法稳定性。

12.4.4 重要性加权的行动者-学习者结构和可扩展高效深度强化学习

基于 A2C 学习算法，重要性加权的行动者-学习者结构（Importance Weighted Actor-Learner

Architecture，IMPALA）在分布式计算中使用智能体探索轨迹的所有经验作为通信信息。如图 12.8

所示，IMPALA 架构由行动者和学习者组成，具体介绍如下：

图 12.8 重要性加权的行动者-学习者结构

• 行动者：每个行动者中会有一个复制的策略网络，用于在和模拟的环境交互时做出决策，在

交互时收集到的经验将会存储到缓冲区中，在与环境交互固定次数后，每个行动者会将其

336

12.4 分布式强化学习算法

存储的探索轨迹经验发送给学习者，并和其他行动者以同步通信的方式从学习者中收到更

新的策略网络参数信息。

• 学习者：通过和行动者通信，学习者收到所有行动者收集的轨迹经验信息并用其训练模型，

设在状态 ST 下的价值估计为 n 步 V 轨迹 Target，定义如下

Target = V (ST ) +

T +

Xn−1

t=T

γ

t−T

(Πt−1

i=T

ci)δtV, (12.1)

其中 δtV = ρt(Rt+γV (St+1)−V (St)) 表示时间差分。ρt = min(¯ρ,

π(St)

µ(St)

)。ci = min(¯c,

π(Si)

µ(Si)

)。

π 为学习者的决策策略，为上一轮同步时所有行动者的策略 µ 的均值。

大规模模型训练算法可以存在多个学习者，细分为工人学习者和主学习者，每个学习者会和

不同的行动者通信并独立完成模型训练，但周期性地，所有工人学习者需要和主学习者通信，每

个工人学习者会将学习到的网络参数梯度发送给主学习者，然后主学习者会更新其自身的网络参

数并同步更新到所有的工人学习中。

可扩展高效深度强化学习（Scalable，Efficient Deep-RL，SEED）架构 (Espeholt et al., 2019) 和

IMPALA 十分类似，主要的区别在于策略网络的推断过程会从行动者部分转移到学习者中，从而

降低了行动者的算力要求和通信延时。具体的 SEED 架构如图 12.9 所示，由于每个行动者中只需

要完成和环境的交互，很多弱算力的计算资源可以加入架构中并成为独立的行动者。根据学习者

指导的决策动作，每一个行动者将一步的经验反馈给学习者，其反馈的经验信息将首先存储在学

习者的经验缓冲器中。在多次迭代之后，批量的轨迹经验数据将提供给学习者进行模型训练，其

中方程 (12.1) 中 V 轨迹目标也被使用作为状态的价值估计。

图 12.9 可扩展高效深度强化学习架构

337

第 12 章 并行计算

12.4.5 Ape-X、回溯-行动者和分布式深度循环回放 Q 网络

在分布式网络中，考虑到智能体和环境的频繁交互，将带有优先级的经验回放加入架构中对

规模化场景很有助益。Ape-X (Horgan et al., 2018) 是典型的包含带有优先级的经验回放部分的分

布式架构，如图 12.10 所示。设有多个相互独立的行动者，在每个行动者中会有一个智能体在策

略网络的指导下与环境进行交互。基于从多个行动者中收集的经验信息，学习者将训练其网络参

数，从而学习最优的动作策略。不仅如此，除了行动者和学习者，算法中还有回放缓冲区收集所

有行动者采集的信息，维护并更新每一个存储经验的优先级，并且根据优先程度从中批量选取数

据发送给学习者进行模型训练。经过回放缓冲区的处理，批量且标有优先级的训练数据能够有效

地提升计算效率及模型训练结果。

图 12.10 Ape-X 架构

在行动者中的算法如算法 12.41 所示，其中每一个行动者首先和学习者在策略网络参数上保

持同步，更新的网络参数信息将指导智能体和环境发生交互。在收到环境带来的反馈后，行动者

会计算其中探索经验数据的优先级，并将带有优先级信息的数据发送给回放缓冲区。

当回放缓冲区从行动者中收集到确定数目的经验信息后，学习者将开始从回放缓冲区获取批

量信息进行学习。从学习者的角度如算法 12.42 所示，在每个模型训练周期中，学习者首先将从

回放缓冲区中获得带有优先级的批量经验数据，每个数据信息用 (i, d) 表示，其中 i 表示数据的

索引编号，d 为具体的经验数据信息，其中包括初始状态、决策动作、奖励和采取动作后到达的

状态四个部分。批量的经验数据将用来训练学习者的网络参数，并周期性地将更新的网络参数与

所有行动者中的策略网络参数保持同步。另外，在模型训练之后，采样数据的优先级将会被调整

并在回放缓冲区中更新。由于容量大小的限制，在回放缓冲区中会周期性地将具有较低优先级的

数据删除。

当训练模型分别使用 DQN 或 DPG 算法的时候，基于如上架构，Ape-X 深度 Q 网络（Ape-X

DQN）和 Ape-X 深度策略梯度（Ape-X DPG）相对应被提出。在 Ape-X DQN 中，Q 网络存在学

习者和所有行动者中，在行动者中智能体的决策动作受网络产生的 Q 值指导；在 Ape-X DPG 中，

338

12.4 分布式强化学习算法

算法 12.41 Ape-X (Actor)

超参数: 单次批量发送到回放缓冲区的数据大小 B、迭代数目 T。

与学习者同步并获得最新的网络参数 θ0

从环境中获得初始状态 S0

for t = 0, 1, 2, · · · , T − 1 do

基于决策策略 π(St|θt) 选择决策行为 At

将经验 (St, At, Rt, St+1) 加入当地缓冲区

if 当地缓冲区存储数据达到数目门限值 B then

批量获得缓冲数据 B

计算获得缓冲数据的优先级 p

将批量缓冲数据和其更新的优先级发送回放缓冲区

end if

周期性同步并更新最新的网络参数 θt

end for

算法 12.42 Ape-X (Learner)

超参数: 学习周期数目 T

初始化网络参数 θ0

for t = 1, 2, 3, · · · , T do

从回放缓冲区中批量采样带有优先级的数据 (i, d)

通过批数据进行模型训练

更新网络参数 θt

对于批数据 d 计算优先级 p

更新回放缓冲区中索引 i 数据的优先级 p

周期性地从回放缓冲区中删除低优先级的数据

end for

学习者将构建策略网络和价值网络，而在行动者里只有相同结构的策略网络，用于指导其智能体

制定策略动作。

同样设立带有优先级的分布式回放缓冲区，回溯-行动者（Retrace-Actor, Reactor）(Gruslys

et al., 2017) 基于 Actor-Critic 架构被提出，取代之前的单个经验信息，一个序列的经验信息将被

同时输入缓冲区中，并采用 Retrace(λ) 算法来更新对 Q 值的估计。在神经网络中，LSTM 网络将

在策略和价值网络中使用，并获得很好的模型训练结果。

类似地，分布式深度循环回放 Q 网络（Recurrent Replay Distributed DQN，R2D2）(Kapturowski

et al., 2019) 在带有优先级的分布式回放缓冲区中采用具有固定长度序列的经验格式，基于深度 Q

网络（DQN）算法，R2D2 同样在策略网络中使用 LSTM 层，并且用存在回放缓存区中的状态数

据训练网络。

339

第 12 章 并行计算

12.4.6 Gorila

基于深度 Q 网络算法（General Reinforcement Learning Architecture，Gorila）(Nair et al., 2015)

如图 12.11 所示。在此架构中，当和参数服务器中深度 Q 网络的参数保持同步之后，行动者将在

深度 Q 网络的指导下和环境进行交互，并将通过交互收集到的经验直接发送到回放缓冲区。回

放缓冲区将存储并管理所有从行动者中收集经验信息。当从回放缓冲区中获取批量数据后，学习

者将会进行模型学习并计算 Q 网络中参数的梯度。在学习者中会用一个学习 Q 网络和一个目标

Q 网络来计算 TD 误差，其中学习 Q 网络将会在学习的每一步和参数服务器中的网络参数保持同

步，然而目标 Q 网络只在每过 N 步之后和参数服务器同步。参数服务器将周期性地从学习者中

接收网络参数的梯度信息，并更新自身的网络参数，以使之后的探索更具效率。

梯度信息

⽹络参数

⽹络参数 相同结构

策略⽹络/

Q⽹络

训练

收集经验

图 12.11 Gorila 架构

12.5 分布式计算架构

基于并行计算的基本模式和结构，在分布式强化学习中，大规模并行计算架构能够得以进一

步探索和研究。一般来说，其系统一般会有如下基本组成元素：

• 环境（Environments）: 环境是智能体需要与其交互的场景。在深度学习的大规模并行计算

340

12.5 分布式计算架构

中，环境可能会存在多个复制版本，并分别对应到多个行动者中，使其能够相互独立地并

行探索，获取经验，并且，在基于模型的强化学习中，通常会用多个模拟的环境来使其在

探索和学习中具备并行性。

• 行动者（Actors）: 系统中行动者通常指直接和环境进行交互的部分。其中，可能会有多个

行动者和一个或多个真实或模拟的环境进行交互，并且其中每一个行动者都能在所给出的

环境状态下独立地做出决策动作。其决策动作可以由行动者自身的策略网络或者 Q 网络推

断产生，或者是由参数服务器或者其周围的学习者中共享的策略网络或 Q 网络产生的。当

行动者在环境中进行了连续多步的探索之后，探索轨迹将形成。形成的探索轨迹将会被提

交到回放存储缓冲器或者直接提交给学习者。由于行动者和环境的交互需要花费很多时间，

所以，行动者探索和数据收集的并行性能够提升获得经验数据的速度，从而可以将批量数

据发送给学习者训练模型，训练效果得到很大提升。

• 回放存储缓冲区（Replay Memory Buffers）：回放存储缓冲区将会从所有行动者中收集探索

轨迹，并将其整理后提供给学习者用于策略学习或者 Q 学习。由于存储缓冲区需要快速的

数据读写和数据打乱重排，数据存储的结构同样需要支持动态且并行的方式。并且，由于大

多数学习者依赖回放存储缓冲区中的数据进行模型训练。为了保证模型训练的高效率，建

议在回放存储缓冲区分配在学习者的周围并高效连通。

• 学习者（Learners）：学习者是深度强化学习的关键组成部分，基于不同的深度强化学习方

法，学习者的结构也将各有不同。通常来说，每个学习者会维护一个策略网络或 Q 网络，并

且用从回放存储缓冲区中得到的行动者与环境交互的经验信息来训练深度网络参数。在训

练前后，学习者均会和参数服务器进行通信，使其在训练前同步深度网络参数信息，并在

训练后提交训练得到的参数梯度信息。多个学习者和参数服务器的通信方式可以是同步或

异步的。

• 参数服务器（Parameter Servers）：参数服务器是从学习者中收集所有信息并维护管理策略

网络或者 Q 网络中的参数信息。参数服务器将周期性地和所有学习者保持同步，使每个学

习者获得其他学习者学习得到的信息，并且参数服务器能够在行动者和环境交互时帮助其

制定决策策略。在大规模深度强化学习系统中，为了保证参数服务器和学习者及行动者通

信时的稳定性和高效率，参数服务器自身也可以具有多种不同的架构，其内部也可采用集

中式或分布式的数据通信方式。

一般性的分布式计算架构可以采纳其中元素组合形成。由于其中的每一部分可独立且并行进

行数据的存储，传输或计算，其架构可以根据要解决的问题适应性调整并灵活改变，从而充分满

足应用中多样的学习任务需求。

341

第 12 章 并行计算

参考文献

BABAEIZADEH M, FROSIO I, TYREE S, et al., 2017. Reinforcement learning thorugh asynchronous

advantage actor-critic on a gpu[C]//ICLR.

ESPEHOLT L, MARINIER R, STANCZYK P, et al., 2019. Seed rl: Scalable and efficient deep-rl with

accelerated central inference[J]. arXiv preprint arXiv:1910.06591.

GRUSLYS A, DABNEY W, AZAR M G, et al., 2017. The reactor: A fast and sample-efficient actor-critic

agent for reinforcement learning[Z].

HORGAN D, QUAN J, BUDDEN D, et al., 2018. Distributed prioritized experience replay[Z].

KAPTUROWSKI S, OSTROVSKI G, DABNEY W, et al., 2019. Recurrent experience replay in distributed

reinforcement learning[C]//International Conference on Learning Representations.

MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learning[C]//International Conference on Machine Learning (ICML). 1928-1937.

NAIR A, SRINIVASAN P, BLACKWELL S, et al., 2015. Massively parallel methods for deep reinforcement learning[Z].

OPENAI, :, BERNER C, et al., 2019. Dota 2 with large scale deep reinforcement learning[Z].

342

应用部分

为了帮助读者更加深入地理解深度强化学习，并能很快地把相关技术用到实践中，下面的章

节将会介绍五个精选的应用，包括 Learning to Run、图像增强、AlphaZero、机器人学习和基于

Arena 平台的多智能体强化学习。这些应用覆盖了尽可能多的细节，帮助读者理解不同场景下的

实现技巧。表 1 列出了该部分的应用及其算法名称、策略、动作空间和观测的形式。我们相信这

些内容能帮助读者根据具体应用来选择对应类似的项目。

表 1 本书应用部分的总结

应用 算法 动作空间 观测

Learning to Run SAC 连续 连续

图像增强 PPO 离散 图片特征

AlphaZero MCTS 离散 二值棋盘矩阵

（Binary Chessboard Matrix）

机器人学习 SAC 连续 连续

基于 Arena 平台的多智能体强化学习 MADDPG, etc 任意 任意

343

13 Learning to Run

在这一章，我们提供了一个实践项目，方便读者获得一些深度强化学习应用的经验，而这个

项目是一个由 CrowdAI 和 NeurIPS1 2017 主办的挑战：Learning to Run。这个环境有 41 维的状

态空间和 18 维的动作空间，二者都是连续的，因此，对于初学者获取经验而言是一个较大规模

的环境。我们为这个任务提供了一个柔性 Actor-Critic（Soft Actor-Critic，SAC）算法的解决方案，

同时也有一些辅助技巧来提高其表现。环境和代码链接见读者服务。

13.1 NeurIPS 2017 挑战：Learning to Run

13.1.1 环境介绍

Learning to Run 是一个由 CrowdAI 和 NeurIPS 2017 举办的竞赛，吸引了许多强化学习研

究人员的参与。在这个任务中，参与者被要求开发一个控制器，使得一个生理学人体模型可以尽

可能快地通过一条复杂而有障碍物的路线。任务中提供了一个肌肉骨骼模型和一个基于物理过程

模拟环境。为了模拟这个物理和生物力学过程并用强化学习智能体对其导航，任务提供了一个基

于 OpenSim 库的 osim-rl 环境，而 OpenSim 是一个对肌肉骨骼建模的标准物理和生物力学环境。

如图 13.1 所示是一个包括主体在内的环境场景。

这个环境结合了一个包括两条腿、一个骨盆、一个代表上半身的部分（躯干、头部、手臂）

的肌肉骨骼模型。不同部分之间由关节连接（比如膝关节和髋），而这些关节处的活动由肌肉激

发来控制。模型中的这些肌肉有很复杂的路径（比如，肌肉可以经过不止一个关节，并且模型中

有冗余肌肉），而肌肉激励器本身也是高度非线性的。这个智能体在 3 维世界中进行 2 维运动。为

1该会议名称当时缩写为 NIPS。

344

13.1 NeurIPS 2017 挑战：Learning to Run

了便于理解和操作，我们在这个项目中使用的环境相比挑战赛中使用的有所简化，因此可能有些

地方跟竞赛官方文档略微不同。如下是骨骼模型的所有组成部分。

图 13.1 NeurIPS 2017 挑战赛：Learning to Run 环境（见彩插）

• 观察量包括 41 个值：

* 骨盆位置（角度，x 坐标，y 坐标）

* 骨盆速度（角速度，x 速度，y 速度）

* 每个踝关节、膝关节和髋的角度（6 个值）

* 每个踝关节、膝关节和髋的角速度（6 个值）

* 质心位置（2 个值）

* 质心速度（2 个值）

* 头部、骨盆、躯干、左右脚趾、左右踝的位置（共 14 个值）

* 左右腰肌强度：对难度级别低于 2（难度值是一个默认环境参数）的环境，其值为 1.0，否

则它是一个随机变量，在整个模拟周期中采样于均值为 1.0、标准差为 0.1 的固定正态分

布。（注意：在我们简化的环境中，这些腰肌强度值被设为 0.0。）

* 下一个障碍物：到骨盆的 x 轴距离，以及其中心相对地面的 y 坐标。（注意：在我们的简

化环境中，所有这些值被设为 0.0，无障碍物出现。）

• 动作包括 18 个标量值，分别表示 18 块肌肉的激发程度（每条腿 9 个）：

* 腘绳肌腱

* 股二头肌

* 臀大肌

* 髂腰肌

345

第 13 章 Learning to Run

* 股直肌

* 股肌

* 腓肠肌

* 比目鱼肌

* 胫骨前肌

• 奖励函数：

* 奖励函数由骨盆沿 x 轴运动距离减去由于使用韧带的惩罚计算得到。

• 其他细节：

* “done”信号表示这一步是环境模拟的最后一步。这会在 1000 次迭代到达或者骨盆高度

低于 0.65 米时发生。

从以上对环境的描述中我们可以看出，相比于其他 OpenAI Gym 或 DeepMind Control Suite 中的

游戏，这个竞赛的环境相对复杂，有着高维观察量空间和动作空间。因此，以较好的表现和较短

的训练时间解决这个任务需要一些特殊的技巧。我们将介绍这些具体方法及用一个并行训练框架

来解决这个任务。我们在随书的代码库中提供了这个环境的副本和解决方案的代码，因此，我们

推荐读者用这个项目进行上手练习。

13.1.2 安装

根据官方库，这个环境可以用以下命令行安装：

1. 创建一个包含 OpenSim 软件包的 Conda 环境（命名为 opensim-rl）。

conda create -n opensim-rl -c kidzik opensim python=3.6.1

2. 激活我们刚创建的 Conda 环境。

在 Windows 上，运行：

activate opensim-rl

在 Linux/OS X 上，运行：

source activate opensim-rl

你需要在每次打开一个新的终端时输入上面的命令。

3. 安装我们的 Python 强化学习环境。

conda install -c conda-forge lapack git

pip install osim-rl

346

13.2 训练智能体

自从 2017 年以后，这个挑战已连续举办了三年（至 2019 年）。因而，最初的 Learning to

Run 环境由于版本更新已经被废弃。虽然如此，我们仍旧选择用这个原始的 2017 版本环境来做

示范，因为它相对简单。于是，在我们的项目中提供了一个仓库存放 2017 版本的环境：

git clone

https://github.com/deep-reinforcement-learning-book/Chapter13-Learning-to-Run.git

我们所用的强化学习算法代码和环境的封装也都在上述仓库中提供。

通过以上几步，我们已经完成了环境的安装，可以通过以下命令检验安装是否成功：

python -c "import opensim"

如果它能正常运行，说明安装已成功；否则，可以在这个网站找到解决方案。

要用随机采样执行 200 次模拟迭代，我们可以用 Python 解释器运行以下命令（在 Linux 环

境）：

from osim.env import RunEnv # 导入软件包

env = RunEnv(visualize=True) # 初始化环境

observation = env.reset(difficulty = 0) # 重置环境

for i in range(200): # 采集样本

observation, reward, done, info = env.step(env.action_space.sample())

这个环境由于已被写成 OpenAI Gym 游戏的格式，对用户十分友好，而且有一个定义好的奖

励函数。我们的任务是得到一个从当前观察量（一个 41 维矢量）到肌肉激活动作（18 维矢量）

的映射函数，使得它能够最大化奖励值。如前所述，奖励函数被定义为一个迭代步中骨盆沿 x 轴

的位移减去韧带受力大小，从而尽可能鼓励智能体在最小身体损耗的情况下向前移动。

13.2 训练智能体

为了更好地解决这个任务，在训练框架中需要实现一系列技巧，包括：

• 一个可以平衡 CPU 和 GPU 资源的并行训练框架；

• 奖励值缩放；

• 指数线性单元 (Exponential Linear Unit，ELU) 激活函数；

• 层标准化（Layer Normalization）；

• 动作重复；

• 更新重复；

• 观察量标准化和动作离散化可能是有用的，但我们未在提供的解决方案中使用；

347

第 13 章 Learning to Run

• 根据智能体双腿的对称性所做的数据增强可能是用的，但我们未在提供的解决方案中使用。

注意，根据竞赛参与团体的实验和报告，后两个技巧也可能是有用的，但由于它们更多基于

该具体任务的方法而不对其他任务广泛适用，我们未在这里的解决方案中使用。然而，要知道观

察量标准化、动作值离散化和数据增强是可以根据一些任务的具体情况应用来加速学习过程的。

这个环境一个典型的缺陷是模拟速度太慢，在一个普通 CPU 上完成单个片段至少需要几十

秒时间。为了更高效地学习策略，我们需要将采样和训练过程并行化。

13.2.1 并行训练

至少有两个原因需要我们对这个任务进行并行训练。第一个是由于上面所述 Learning to

Run 环境较慢的模拟速度，至少耗时几十秒完成一个模拟片段。第二个是由于该环境有较高的内

在复杂度。基于作者经验，这个环境用普通的无模型（Model-Free）强化学习算法，如深度决定

性策略梯度（Deep Deterministic Policy Gradient，DDPG）或柔性 Actor-Critic（Soft Actor-Critic，

SAC），需要至少上百个 CPU/CPU 计算小时来获得一个较好的策略。因此，这里需要一个多进程

跨 GPU 的训练框架。

由于 Learning to Run 环境的高复杂度，训练过程需要用多个 CPU 和 GPU 来并行分布实

现。此外，CPU 和 GPU 之间的平衡对这个任务也很关键，因为与环境交互采样的过程一般是在

CPU 上，而反向传播训练过程一般是在 GPU 上。整个过程的训练效率在实践中满足短板效应。

关于并行训练中如何均衡 CPU 和 GPU 计算的内容在第 12 章和第 18 章中也有讨论。这里有一种

解决这个任务的方案。

如图 13.2 所示，在一般的单进程深度强化学习中，训练过程由一个进程来处理，而这通常无

法充分发挥计算资源的潜力，尤其在有多个 CPU 核和多个 GPU 的情况下。

智能体 环境

经验回放缓存

a

s r

推送 更新

训练（进程）

图 13.2 在离线策略深度强化学习中进行单进程训练：只有一个进程来采样和训练策略

图 13.3 展示了在多个 CPU 和多个 GPU 上部署离线策略（Off-Policy）深度强化学习的并行

训练架构，其中，一个智能体和一个环境被封装进一个“工作者”来运行一个进程。多个工作者

可以共享同一个 GPU，因为有时单个工作者无法完全占用整个 GPU 内存。在这种设置下，使用

同一个 GPU 的进程数量和工作者数量可以被手动设置，从而在学习过程中最大化所有计算资源

的利用率。

348

13.2 训练智能体

图 13.3 一个离线策略深度强化学习的并行训练架构。每个工作者包含一个与环境交互的智能

体，策略被分布在多个 GPU 上训练

我们的项目提供了一个高度并行化的 SAC 算法，它使用上述架构来解决这个需要多进程和

多 GPU 计算的任务。由于多进程的内存之间互相不共享，需要用特殊的模块来处理信息交流和

参数共享。在代码中，回放缓冲区通过 Python 内的 multiprocessing 模块共享，训练过程中的

网络和参数更新由 PyTorch 的 multiprocessing 模块共享（在 Linux 系统上）。

实践中，尽管每个工作者包含一个智能体，但是智能体内的网络实际在多个工作者间共享，

因此实际上只保留了一套网络（用于一个智能体的）。PyTorch 的 nn.Module 模块可以处理使用

多个进程更新共享内存中网络参数的情况。由于 Adam 优化器在训练中也有一些统计量，我们使

用以下 ShareParameters() 函数来在多进程中共享这些值：

def ShareParameters(adamoptim):

# 共享 Adam 优化器的参数便于实现多进程

for group in adamoptim.param_groups:

for p in group[’params’]:

state = adamoptim.state[p]

# 初始化：需要在这里初始化，否则无法找到相应量

state[’step’] = 0

state[’exp_avg’] = torch.zeros_like(p.data)

state[’exp_avg_sq’] = torch.zeros_like(p.data)

# 在内存中共享

state[’exp_avg’].share_memory_()

state[’exp_avg_sq’].share_memory_()

349

第 13 章 Learning to Run

在训练函数中，我们用以下方式设置 SAC 算法中的共享模块，包括网络和优化器：

# 共享网络

sac_trainer.soft_q_net1.share_memory()

sac_trainer.soft_q_net2.share_memory()

sac_trainer.target_soft_q_net1.share_memory()

sac_trainer.target_soft_q_net2.share_memory()

sac_trainer.policy_net.share_memory()

# 共享优化器参数

ShareParameters(sac_trainer.soft_q_optimizer1)

ShareParameters(sac_trainer.soft_q_optimizer2)

ShareParameters(sac_trainer.policy_optimizer)

ShareParameters(sac_trainer.alpha_optimizer)

share_memory() 是一个继承自 PyTorch 的 nn.Module 模块的函数，可用于共享神经网络。

我们也可以共享熵因子，但是在这个代码里没有实现它。“forkserver”启动方法是在 Python 3 中

使用 CUDA 子进程所需的，如代码中所示：

torch.multiprocessing.set_start_method(’forkserver’, force=True)

回放缓冲区可以用 Python 的 multiprocessing 模块共享：

from multiprocessing.managers import BaseManager

replay_buffer_size = 1e6

BaseManager.register(’ReplayBuffer’, ReplayBuffer)

manager = BaseManager()

manager.start()

replay_buffer = manager.ReplayBuffer(replay_buffer_size)

# 通过 manager 来共享经验回放缓存

在克隆下来的文件夹中运行以下命令来开始训练（注意，由于使用“forkserver”启动方法，

所以在 Windows 10 上无法进行这样的并行训练）：

python sac_learn.py --train

我们也可用以下命令测试训练的模型：

python sac_learn.py --test

350

13.2 训练智能体

13.2.2 小技巧

然而，即便使用了上面的并行架构，我们仍旧不能在这个任务上取得很好的表现。由于任务

的复杂性和深度学习模型的非线性，损失函数上的局部最优和非平滑甚至不可微的曲面都容易使

优化过程陷入困境（对于策略或价值函数）。在使用深度强化学习方法的过程中经常需要一些微

调策略，尤其是对像 Learning to Run 这样的复杂任务。所以，下面介绍我们使用的一些小技

巧，来更高效和稳定地解决这个任务。

• 奖励值缩放：奖励值缩放遵循一般的值缩放规则，即将奖励值除以训练过程中所采批样本

的标准差。奖励值缩放，或叫标准化和归一化，是强化学习中使训练过程稳定而加速收敛

速度的常用技术手段。如 SAC 算法后续的一篇文章 (Haarnoja et al., 2018) 所报道的，最大

熵强化学习算法可能对奖励函数的缩放敏感，这不同于其他传统强化学习算法。因此，SAC

算法的作者添加了一个基于梯度的温度调校模块用作熵正则化项，这显著缓解了实践中超

参数微调过程的困难。

• 指数线性单元（Exponential Linear Unit，ELU）(Clevert et al., 2015) 激活函数被用以替代

整流线性单元（Rectified Linear Unit，ReLU）(Agarap, 2018)：为了得到更快的学习过程和

更好的泛化表现，我们使用 ELU 作为策略网络隐藏层的激活函数。ELU 函数定义如下：

f(x) =







x, if x > 0

α exp(x − 1), if x ⩽ 0

(13.1)

ELU 和 ReLU 的对比如图 13.4 所示。相比于 ReLU，ELU 有负数值，这使得它能够将神经单

元激活的平均值拉至更接近 0 的位置，如同批标准化，但是却有着更低的计算复杂度。均

值移动到趋于 0 可以加速学习，因为它通过减少神经单元激发造成的移动偏差，使得一般

的梯度更加接近于神经网络单元的自然梯度。

• 层标准化：我们也对价值网络和策略网络的每个隐藏层使用层标准化 (Ba et al., 2016)。相比

于批标准化（Batch Normalization），层标准化对单个训练样本在某神经网络层上的神经元

的累加输入计算均值和方差来进行标准化。每个神经元有其与众不同的适应性偏差（Bias）

和增益（Gain），这些值在标准化之后和非线性激活之前被添加到神经元的值上。这种方法

在实际中可以帮助加速训练过程。

• 动作重复：我们在训练过程中使用一个常见的技巧叫动作重复（或叫跳帧），来加速训练的

执行时间（Wall-Clock Time）。DQN 原文中使用跳帧和像素级的最大化（Max）算子来实

现在 Atari 2600 游戏上基于图像的学习。如果我们定义单个帧的原始观察量是 oi，其中 i

表示帧指标，原始 DQN 文章中的输入是 4 个堆叠帧，其中每个是两个连续帧中的最大值，

即 [max(oi−1, oi), max(oi+3, oi+4), max(oi+7, oi+8), max(oi+11, oi+12)]，对应的跳帧率就是

4（实际上，对于不同游戏，该跳帧率可以是 2,3 或 4）。在这些跳过的帧中，动作被重复执

351

第 13 章 Learning to Run

图 13.4 对比 ReLU 和 ELU 激活函数。ELU 在零点可微

行。最大化算子在图像观察量上按像素计算，奖励函数对所有跳过和不跳过的帧累加。原

始 DQN 中的跳帧机制增加了随机性，同时加速了采样率。然而，在我们的任务中，我们使

用一种不同的设置，不使用最大化算子和堆叠帧：每个动作在跳过的帧上进行简单的重复

执行，包括跳过帧和未跳过帧在内的所有样本被存入回放缓冲区。实践中，我们使用 3 作

为动作重复率，减少了策略与环境交互所需的正向推理时间。

• 更新重复：我们也在训练中使用一个小的学习率并重复更新策略的技巧，从而策略以重复

率 3 在同一个批样本上进行学习。

13.2.3 学习结果

通过以上设置和 SAC 算法上的这些小技巧，智能体能够在 3 天的训练时长下学会用人类的

方式奔跑很长的一段距离，训练是在一个 4GPU 和 56CPU 的服务器上进行的，结果如图 13.5 所

示。图 13.6 展示了学习曲线，包括原始的奖励函数值和移动平均的平滑曲线，呈现了上升的学习

表现。纵轴是一个片段内的累计奖励值，显示了智能体奔跑的距离和姿势状况。

352

参考文献

图 13.5 Learning to Run 任务中奔跑智能体的最终表现（场景）（见彩插）

0 5000 10000 15000 20000

Episode

0

2

4

6

Episode Reward

Learning Process

Original

Smoothed

学习过程

⽚段

⽚段奖励

原始的

平滑的

图 13.6 Learning to Run 任务的学习过程（见彩插）

参考文献

AGARAP A F, 2018. Deep learning using rectified linear units (relu)[Z].

BA J L, KIROS J R, HINTON G E, 2016. Layer normalization[Z].

CLEVERT D A, UNTERTHINER T, HOCHREITER S, 2015. Fast and accurate deep network learning

by exponential linear units (elus)[Z].

HAARNOJA T, ZHOU A, HARTIKAINEN K, et al., 2018. Soft actor-critic algorithms and applications[J].

arXiv preprint arXiv:1812.05905.

353

14 鲁棒的图像增强

深度生成模型相较于经典的算法，在超分辨率、图像分割等计算机视觉任务中，取得了显著

的进展。然而，这种基于学习的方法缺乏鲁棒性和可解释性，限制了它们在现实世界中的应用。

本章将讨论一种鲁棒的图像增强方法，它可以通过深度强化学习与许多可解释的技术进行结合。

我们将先从一些图像增强的背景知识进行介绍，接着将图像增强的过程看作一个由马尔可夫决策

过程（Markov Decision Process，MDP）建模的处理流程。最后，我们将展示如何通过近端策略优

化（Proximal Policy Optimization，PPO）算法构建智能体来处理这个 MDP 过程。实验环境由一

个真实世界的数据集构建，包含 5000 张照片，其中包括原始图像和专家调整后的版本。项目代

码链接见读者服务。

14.1 图像增强

图像增强技术属于图像处理技术。它的主要目标是使处理后的图像更适合各种应用的需要。

典型的图像增强技术包括去噪、去模糊和亮度改善。现实世界中的图像总是需要多种图像增强技

术。图 14.1 显示了一个包括亮度改善和去噪的图像增强流程。专业的照片编辑软件，如 Adobe

Photoshop，提供强大的修图能力，但效率不高，需要用户在照片编辑方面具备专业知识。在诸

如推荐系统这样的大规模场景中，图像的主观质量对用户体验至关重要，因此需要一种满足有效

性、鲁棒性和效率的自动图像增强方法。其中鲁棒性是最重要的条件，尤其是在用户生成内容的

平台上，比如 Facebook 和 Twitter，即使 1% 的较差情况（Bad Case）也会伤害数百万用户的使用

体验。

与图像分类或分割有着其独特的真实值（Ground Truth）不同，图像增强的训练数据依赖于

人类专家。因此，图像增强并没有大规模的公共图像增强数据集。经典的图像增强方法主要基于

354

14.1 图像增强

图 14.1 一个图像增强流程的案例。左侧的原始图像存在 JPEG 压缩噪声并且曝光不足（见彩插）

的是伽马校正和直方图均衡化，以及先验的专家知识。这些方法也不需要大量的数据。伽马校正

利用了人类感知的非线性，比如，我们感知光和颜色的能力。直方图均衡化实现了允许局部对比

度较低的区域获得更高的对比度，以更好地分布在像素直方图上的思想，这在背景和前景为全亮

或全暗（如 X 射线图像）时非常有用。这些方法虽然快速、简单，但是缺乏对上下文信息的考虑，

限制了它们的应用。

最近，有学者使用基于学习的方法，试图用 CNN 拟合从输入图像到所需像素值的映射，并取

得了很大的成功 (Bychkovsky et al., 2011; Kupyn et al., 2018; Ulyanov et al., 2018; Wang et al., 2019)。

然而，这种方法也存在问题。首先，很难训练出一个能处理多种增强情况的综合神经网络。此外，

像素到像素的映射缺乏鲁棒性，例如，在处理诸如头发和字符等细节信息时，它的表现不是很

好 (Nataraj et al., 2019; Zhang et al., 2019)。一些研究者提出，将深度强化学习应用于图像增强，将

增强过程描述为一系列策略迭代问题，以解决上述问题。在本章中，我们遵循这些方法，并提出

一种新的 MDP 公式来进行图像增强。我们在一个包含 5000 对图像的数据集上用代码示例演示

了我们的方法，以提供快速的实际学习过程。

在讨论算法之前，我们先介绍两个 Python 库：Pillow (Clark, 2015) 和 scikit-image (Van der Walt

et al., 2014)。它们提供了许多友好的接口来实现图像增强。可以使用如下代码直接从 PyPI 安装

它们：

pip install Pillow

pip install scikit-image

下面是 Pillow 的子模块 ImageEnhance 调整对比度的示例代码。

from PIL import ImageEnhance

def adjust_contrast(image_rgb, contrast_factor):

# 调整对比度

# 参数:

# image_rgb (PIL.Image): RGB 图像

355

第 14 章 鲁棒的图像增强

# contrast_factor (float): 颜色平衡因子范围从 0 到 1.

# 返回:

# PIL.Image 对象

#

enhancer = ImageEnhance.Contrast(image_rgb)

return enhancer.enhance(contrast_factor)

14.2 用于鲁棒处理的强化学习

在将强化学习应用于图像增强时，首先需要考虑如何构造该领域的马尔可夫决策过程。一个

自然出现的想法是将像素处理为状态，将不同的图像增强技术视为强化学习的动作。该构想提供

了几种可控的初级增强算法的组合方法，以获得稳健、有效的结果。在本节中，我们将讨论这种

基于强化学习的颜色增强方法。为了简单起见，我们只采取全局增强操作。值得一提的是，通过

区域候选模块 (Ren et al., 2015) 来适应一般的增强算法也是很自然的想法。

假设训练集包含 N 对 RGB 图像 {(li

, hi)}

N

i=1，其中 li 为低质量原始图像，hi 是高质量修复

图像。为了保持数据分布，初始状态 S0 应从 {li}

N

i=1 中均匀采样。在每步中，智能体会执行一个

预定义对动作，如调整对比度，再将它应用于当前状态。需要注意的是，当前状态和选择动作完

全决定了状态转移。也就是说，环境没有不确定性。我们在之前工作的基础上 (Furuta et al., 2019;

Park et al., 2018) 上继续研究，并使用了 CIELAB 颜色空间作为转移奖励函数。

∥L(h) − L(St)∥

2

2 − ∥L(h) − L(St+1)∥

2

2

(14.1)

其中 h 是对应的高质量图像 S0，L 是 RGB 颜色空间到 CIELAB 颜色空间到映射。

另一个重点是定义学习和评估时的终结状态。在游戏的强化学习应用中，终结状态可以由环

境决定。而与此不同的是，在图像增强中的智能体需要由自己决定退出时机。文献 (Park et al.,

2018) 提出了一个基于 DQN 的智能体，它在所有动作预测的 Q 值都为负数时会退出。然而，QLearning 中由函数近似引起的过估计问题可能会导致推理过程的鲁棒性降低。我们通过训练一个

明确的策略并增加一个用于退出选择的“无操作”动作来处理这个问题。表 14.1 列出了所有预

定义的动作，其中索引为 0 的动作表示“无操作”动作。

从零开始训练一个卷积神经网络需要大量的原始-修复图像对。因此，我们不使用原始图像

状态作为观测值的方案，而是考虑使用在 ILSVRC 分类数据集 (Russakovsky et al., 2015) 上预训练

的 ResNet50 网络中的最后一层卷积层的激活值作为深层特征输入。这样的深层特征十分重要，它

可以提升许多其他视觉识别任务的效果 (Redmon et al., 2016; Ren et al., 2016)。受到前人工作 (Lee

et al., 2005; Park et al., 2018) 的启发，我们在构造观测信息时进一步考虑使用直方图信息。具体来

说，我们计算了 RGB 颜色空间在 (0, 255), (0, 255), (0, 255) 范围内和 CIELab 颜色空间在 (0, 100),

356

14.2 用于鲁棒处理的强化学习

表 14.1 全局图像增强动作集

索引 简介 索引 简介

0 无操作 7 红、绿色调整 ×0.95

1 对比度 ×0.95 8 红、绿色调整 ×1.05

2 对比度 ×1.05 9 蓝、绿色调整 ×0.95

3 饱和度 ×0.95 10 蓝、绿色调整 ×1.05

4 饱和度 ×1.05 11 红、蓝色调整 ×0.95

5 亮度 ×0.95 12 红、蓝色调整 ×1.05

6 亮度 ×1.05

(−60, 60), (−60, 60) 范围内的统计信息。这三个特征连成 2048 + 2000 维的观测信息。接着，我

们选择 PPO (Schulman et al., 2017) 作为策略优化算法。PPO 是一种 Actor-Critic 算法，它在一系

列任务上已经取得了显著的成果。它的网络由 3 部分组成：3 层特征抽取作为主干网络、1 层行

动者（Actor）网络和 1 层批判者（Critic）网络。所有层都是全连接的，其中特征抽取器中各层

的输出分别为 2048、512 和 128 个单元，并都使用了 ReLU 作为激活函数。

我们在 MIT-Adobe FiveK (Bychkovsky et al., 2011) 数据集上对我们的方法进行了评估。其中

包括 5000 张原始图像，而每张原始图像又有 5 个不同专家（A/B/C/D/E）修复后的图像。继之前

的工作 (Park et al., 2018; Wang et al., 2019) 之后，我们只使用专家 C 修复的图像，并随机选择 4500

张图像进行训练，剩下的 500 张图像用于测试。原始图像是 DNG 格式的，而修复图像是 TIFF 格

式的。我们使用 Adobe Lightroom 将它们都转换为质量为 100、颜色空间为 sRGB 的 JPEG 格式。

为了更有效地训练，我们也调整了图像大小，使得每张图像的最大边为 512 像素。具体的超参数

在表 14.2 中列出。

表 14.2 用于图像增强的 PPO 超参数

超参数 值 超参数 值

优化器 Adam 每次迭代的优化数 2

学习率 1e-5 最大迭代数 10000

梯度范数裁剪 1.0 熵因子 1e-2

GAE λ 0.95 奖励缩放 0.1

每次迭代的片段数 4 γ 0.95

接下来开始，我们将演示如何实现上述算法，首先需要构建一个环境对象。

class Env(object):

# 训练环境

357

第 14 章 鲁棒的图像增强

def __init__(self, src, max_episode_length=20, reward_scale=0.1):

# 参数:

# src (list[str, str]): 原始图像和处理图像路径的列表，初始状态将从中均匀采样

# max_episode_length (int): 最大可执行动作数量

self._src = src

self._backbone = backbone

self._preprocess = preprocess

self._rgb_state = None

self._lab_state = None

self._target_lab = None

self._current_diff = None

self._count = 0

self._max_episode_length = max_episode_length

self._reward_scale = reward_scale

self._info = dict()

通过使用 TensorFlow 的 ResNet API，我们可以通过 _state_feature 函数构建观测数据。过

程如下所示：

backbone = tf.keras.applications.ResNet50(include_top=False, pooling=’avg’)

preprocess = tf.keras.applications.resnet50.preprocess_input

def get_lab_hist(lab):

# 获取 Lab 图像的直方图

lab = lab.reshape(-1, 3)

hist, _ = np.histogramdd(lab, bins=(10, 10, 10),

range=((0, 100), (-60, 60), (-60, 60)))

return hist.reshape(1, 1000) / 1000.0

def get_rgb_hist(rgb):

# 获取 RGB 图像的直方图

rgb = rgb.reshape(-1, 3)

hist, _ = np.histogramdd(rgb, bins=(10, 10, 10),

range=((0, 255), (0, 255), (0, 255)))

return hist.reshape(1, 1000) / 1000.0

def _state_feature(self):

s = self._preprocess(self._rgb_state)

358

14.2 用于鲁棒处理的强化学习

s = tf.expand_dims(s, axis=0)

context = self._backbone(s).numpy().astype(’float32’)

hist_rgb = get_rgb_hist(self._rgb_state).astype(’float32’)

hist_lab = get_lab_hist(self._lab_state).astype(’float32’)

return np.concatenate([context, hist_rgb, hist_lab], 1)

接着，我们构建和 OpenAI Gym (Brockman et al., 2016) 相同的接口。其中，我们按照表 14.1

定义转移函数 _transit，并依据公式 (14.1) 构建奖励函数 _reward。

def step(self, action):

# 执行单步

self._count += 1

self._rgb_state = self._transit(action)

self._lab_state = rgb2lab(self._rgb_state)

reward = self._reward()

done = self._count >= self._max_episode_length or action == 0

return self._state_feature(), reward, done, self._info

def reset(self):

# 重置环境

self._count = 0

raw, retouched = map(Image.open, random.choice(self._src))

self._rgb_state = np.asarray(raw)

self._lab_state = rgb2lab(self._rgb_state)

self._target_lab = rgb2lab(np.asarray(retouched))

self._current_diff = self._diff(self._lab_state)

self._info[’max_reward’] = self._current_diff

return self._state_feature()

这里的 PPO 与 5.10.6 节实现有所不同。我们将 PPO (Schulman et al., 2017) 算法用于离散动

作情况。需要注意的是，我们将 LogSoftmax 作为行动者网络的激活函数，这样能在计算替代目

标时提供更好的数值稳定性。对 PPO 智能体，我们先定义它的初始化函数和行为函数：

class Agent(object):

# PPO 智能体

def __init__(self, feature, actor, critic, optimizer,

epsilon=0.1, gamma=0.95, c1=1.0, c2=1e-4, gae_lambda=0.95):

# 参数:

# feature (tf.keras.Model): 行动者和批判者的基础网络

359

第 14 章 鲁棒的图像增强

# actor (tf.keras.Model): 行动者网络

# critic (tf.keras.Model): 批判者网络

# optimizer (tf.keras.optimizers.Optimizer): 优化器

# epsilon (float): 裁剪操作中的 epsilon

# gamma (float): 奖励折扣

# c1 (float): 价值损失系数

# c2 (float): 熵系数

self.feature, self.actor, self.critic = feature, actor, critic

self.optimizer = optimizer

self._epsilon = epsilon

self.gamma = gamma

self._c1 = c1

self._c2 = c2

self.gae_lambda = gae_lambda

def act(self, state, greedy=False):

# 参数:

# state (numpy.array): 1 * 4048 维的状态

# greedy (bool): 是否要选取贪心动作

# Returns:

# action (int): 所选择的动作

# logprob (float): 所选动作的概率对数

# value (float): 当前状态的价值

feature = self.feature(state)

logprob = self.actor(feature)

if greedy:

action = tf.argmax(logprob[0]).numpy()

return action, 0, 0

else:

value = self.critic(feature)

logprob = logprob[0].numpy()

action = np.random.choice(range(len(logprob)), p=np.exp(logprob))

return action, logprob[action], value.numpy()[0, 0]

360

14.2 用于鲁棒处理的强化学习

在采样过程中，我们通过 GAE (Schulman et al., 2015) 算法记录轨迹。

def sample(self, env, sample_episodes, greedy=False):

# 从给定环境中采样一个轨迹

# 参数:

# env: 给定的环境

# sample_episodes (int): 要采样多少片段

# greedy (bool): 是否选取贪心动作

trajectories = [] # s, a, r, logp

e_reward = 0

e_reward_max = 0

for _ in range(sample_episodes):

s = env.reset()

values = []

while True:

a, logp, v = self.act(s, greedy)

s_, r, done, info = env.step(a)

e_reward += r

values.append(v)

trajectories.append([s, a, r, logp, v])

s = s_

if done:

e_reward_max += info[’max_reward’]

break

episode_len = len(values)

gae = np.empty(episode_len)

reward = trajectories[-1][2]

gae[-1] = last_gae = reward - values[-1]

for i in range(1, episode_len):

reward = trajectories[-i - 1][2]

delta = reward + self.gamma * values[-i] - values[-i - 1]

gae[-i - 1] = last_gae = \

delta + self.gamma * self.gae_lambda * last_gae

for i in range(episode_len):

trajectories[-(episode_len - i)][2] = gae[i] + values[i]

e_reward /= sample_episodes

e_reward_max /= sample_episodes

return trajectories, e_reward, e_reward_max

361

第 14 章 鲁棒的图像增强

最后，策略优化的部分如下所示，其中价值损失裁剪和优势标准化遵循文献 (Dhariwal et al.,

2017) 的描述。

def _train_func(self, b_s, b_a, b_r, b_logp_old, b_v_old):

# 训练函数

all_params = self.feature.trainable_weights + \

self.actor.trainable_weights + \

self.critic.trainable_weights

with tf.GradientTape() as tape:

b_feature = self.feature(b_s)

b_logp, b_v = self.actor(b_feature), self.critic(b_feature)

entropy = -tf.reduce_mean(

tf.reduce_sum(b_logp * tf.exp(b_logp), axis=-1))

b_logp = tf.gather(b_logp, b_a, axis=-1, batch_dims=1)

adv = b_r - b_v_old

adv = (adv - tf.reduce_mean(adv)) / (tf.math.reduce_std(adv) + 1e-8)

c_b_v = b_v_old + tf.clip_by_value(b_v - b_v_old,

-self._epsilon, self._epsilon)

vloss = 0.5 * tf.reduce_max(tf.stack(

[tf.pow(b_v - b_r, 2), tf.pow(c_b_v - b_r, 2)], axis=1), axis=1)

vloss = tf.reduce_mean(vloss)

ratio = tf.exp(b_logp - b_logp_old)

clipped_ratio = tf.clip_by_value(

ratio, 1 - self._epsilon, 1 + self._epsilon)

pgloss = -tf.reduce_mean(tf.reduce_min(tf.stack(

[clipped_ratio * adv, ratio * adv], axis=1), axis=1))

total_loss = pgloss + self._c1 * vloss - self._c2 * entropy

grad = tape.gradient(total_loss, all_params)

self.optimizer.apply_gradients(zip(grad, all_params))

return entropy

def optimize(self, trajectories, opt_iter):

# 基于给定轨迹数据进行优化

b_s, b_a, b_r, b_logp_old, b_v_old = zip(*trajectories)

362

参考文献

b_s = np.concatenate(b_s, 0)

b_a = np.expand_dims(np.array(b_a, np.int64), 1)

b_r = np.expand_dims(np.array(b_r, np.float32), 1)

b_logp_old = np.expand_dims(np.array(b_logp_old, np.float32), 1)

b_v_old = np.expand_dims(np.array(b_v_old, np.float32), 1)

b_s, b_a, b_r, b_logp_old, b_v_old = map(

tf.convert_to_tensor, [b_s, b_a, b_r, b_logp_old, b_v_old])

for _ in range(opt_iter):

entropy = self._train_func(b_s, b_a, b_r, b_logp_old, b_v_old)

return entropy.numpy()

最终经过训练后，智能体学到了图像增强的策略。图 14.2 展示了一个训练效果样例。

图 14.2 一个在 MIT-Adobe FiveK 数据集上使用全局增强的效果样例。当右上角的天空等区域需要

局部增强时，全局亮度会增加（见彩插）

参考文献

BROCKMAN G, CHEUNG V, PETTERSSON L, et al., 2016. OpenAI gym[J]. arXiv:1606.01540.

BYCHKOVSKY V, PARIS S, CHAN E, et al., 2011. Learning photographic global tonal adjustment with

a database of input/output image pairs[C]//CVPR 2011. IEEE: 97-104.

CLARK A, 2015. Pillow (pil fork) documentation[Z].

DHARIWAL P, HESSE C, KLIMOV O, et al., 2017. OpenAI baselines[J]. GitHub, GitHub repository.

FURUTA R, INOUE N, YAMASAKI T, 2019. Fully convolutional network with multi-step reinforcement

learning for image processing[C]//Proceedings of the AAAI Conference on Artificial Intelligence:

volume 33. 3598-3605.

363

第 14 章 鲁棒的图像增强

KUPYN O, BUDZAN V, MYKHAILYCH M, et al., 2018. DeblurGAN: Blind motion deblurring using

conditional adversarial networks[C]//Proceedings of the IEEE Conference on Computer Vision and

Pattern Recognition. 8183-8192.

LEE S, XIN J, WESTLAND S, 2005. Evaluation of image similarity by histogram intersection[J]. Color

Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain),

Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color,

The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Français de la Couleur,

30(4): 265-274.

NATARAJ L, MOHAMMED T M, MANJUNATH B, et al., 2019. Detecting GAN generated fake images

using co-occurrence matrices[J]. Journal of Electronic Imaging.

PARK J, LEE J Y, YOO D, et al., 2018. Distort-and-recover: Color enhancement using deep reinforcement

learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 5928-

5936.

REDMON J, DIVVALA S, GIRSHICK R, et al., 2016. You only look once: Unified, real-time object

detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 779-

788.

REN S, HE K, GIRSHICK R, et al., 2015. Faster R-CNN: Towards real-time object detection with region

proposal networks[C]//Advances in Neural Information Processing Systems. 91-99.

REN S, HE K, GIRSHICK R, et al., 2016. Object detection networks on convolutional feature maps[J].

IEEE transactions on pattern analysis and machine intelligence, 39(7): 1476-1481.

RUSSAKOVSKY O, DENG J, SU H, et al., 2015. Imagenet Large Scale Visual Recognition Challenge[J].

International Journal of Computer Vision (IJCV), 115(3): 211-252.

SCHULMAN J, MORITZ P, LEVINE S, et al., 2015. High-dimensional continuous control using

generalized advantage estimation[J]. arXiv preprint arXiv:1506.02438.

SCHULMAN J, WOLSKI F, DHARIWAL P, et al., 2017. Proximal policy optimization algorithms[J].

arXiv:1707.06347.

ULYANOV D, VEDALDI A, LEMPITSKY V, 2018. Deep image prior[C]//Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition. 9446-9454.

VAN DER WALT S, SCHÖNBERGER J L, NUNEZ-IGLESIAS J, et al., 2014. scikit-image: image

processing in python[J]. PeerJ, 2: e453.

364

参考文献

WANG R, ZHANG Q, FU C W, et al., 2019. Underexposed photo enhancement using deep illumination

estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.

6849-6857.

ZHANG S, ZHEN A, STEVENSON R L, 2019. GAN based image deblurring using dark channel prior[J].

arXiv preprint arXiv:1903.00107.

365

15 AlphaZero

本章首先介绍组合博弈问题（如象棋、围棋等）的概念，然后以五子棋为例介绍 AlphaZero

算法。AlphaZero 算法作为棋类问题的通用算法，在许多挑战巨大的棋类游戏中都取得了超越人

类的表现，例如围棋、国际象棋、日本将棋等。该算法结合蒙特卡罗树搜索和深度强化学习自博

弈，是人工智能史上的标志性算法。本章分为三个部分：第一部分介绍组合博弈的概念；第二部

分介绍蒙特卡罗树搜索算法；第三部分以五子棋为例，详细介绍 AlphaZero 算法。

15.1 简介

AlphaGo Zero (Silver et al., 2017b) 算法在围棋中取得了超越人类冠军的表现，AlphaZero (Silver

et al., 2017a, 2018) 算法是 AlphaGo Zero 的通用版本。相比最初击败人类选手的 AlphaGo (Silver

et al., 2016) 系列算法 AlphaGo Fan（击败 Fan Hui）、AlphaGo Lee（击败 Lee Sedol）和 AlphaGo

Master（击败柯洁），AlphaZero 算法完全基于自博弈（Self-Play）的强化学习从零开始提升。它

没有利用人类专家数据进行监督学习，而是直接从随机动作选择开始探索。AlphaZero 有两个关

键部分：（1）在自博弈中使用蒙特卡罗树搜索来收集数据；（2）使用深度神经网络拟合数据，并

在树搜索过程中用于动作概率和状态价值估计。该算法不仅适用于围棋，还在国际象棋和日本将

棋中击败了世界冠军程序，证明了该算法的通用性。本章首先介绍组合博弈（包括围棋、国际象

棋、五子棋等）的概念，并给出无禁手五子棋的代码；然后介绍蒙特卡罗树搜索的具体步骤；最

后以五子棋为游戏环境演示 AlphaZero 算法的具体细节。为了帮助读者理解，我们提供了五子棋

游戏和 AlphaZero 算法的代码链接见读者服务。

366

15.2 组合博弈

15.2 组合博弈

组合博弈理论（CGT）(Albert et al., 2007) 是数学和理论计算机科学的一个分支，通常研究具

有完美信息（Perfect Information）的序列化游戏。这类游戏通常具有以下特点：

• 游戏通常包含两个玩家（如围棋、象棋）。有时只包含一个玩家的游戏（如数独、纸牌）也

可以看成游戏设计者和玩家之间的组合博弈。包含两个以上玩家的游戏不被视为组合博弈

问题，因为游戏中会出现合作等更加复杂的博弈问题 (Browne et al., 2012)。

• 游戏不包含任何影响游戏结果的随机性因素（Chance Factor），如骰子等。

• 游戏给玩家提供完美信息 (Muthoo et al., 1996)，这意味着每个玩家都完全了解之前发生的

所有事件。

• 玩家以回合制的方式执行动作，且动作空间和状态空间都是有限的。

• 游戏会在有限步内结束，结果通常为输赢，有些游戏有平局情况。

许多组合博弈问题 (Albert et al., 2007)，包括数独和纸牌等单人游戏，以及如六连棋（Hex）、

围棋（Go）和象棋（Chess）等双人游戏，都是计算机科学家需要解决的经典问题。自从 IBM 公

司的深蓝系统 (Campbell et al., 2002; Hsu, 1999) 击败了国际象棋大师 Gary Kasparov 之后，围棋成

为了人工智能的下一个桥头堡。除此之外，还有很多其他的游戏，如黑白棋（Othello）、亚马逊

棋（Amazons）、日本将棋（Shogi）、跳棋（Chinese Checkers）、四子棋（Connect Four）、五子棋

（Gomoku）等，吸引了一大批人用计算机来寻找解决方案。

介绍完组合博弈的特点之后，我们以五子棋为例给出一些代码细节。首先从一个空棋盘开

始，当有五个相同颜色的棋子连成一条线（水平、垂直或斜线）时，即代表一名玩家获胜，否则

为平局。五子棋有各种各样的规则，最常见的规则是无禁手（Freestyle Gomoku）规则或长连禁手

（Standard Gomoku）规则。无禁手五子棋只需要有至少五个子连成一条线即可赢得比赛。而长连

禁手五子棋需要恰好五个子才代表获胜，任何多于五个棋子都不算获胜。这里我们以无禁手五子

棋作为示例。

这里我们进一步简化棋盘大小，使用 3 × 3 的棋盘作为示例。三个棋子连成一线即表示获胜

（我们可以称之为“三子棋”或井字棋），图 15.1 展示了在该棋盘上的动作序列样例。

图 15.1 3 × 3 棋盘上的落子序列示例。“b”代表“黑方玩家”，“w”代表“白方玩家”。(b, 5) 表

示黑方玩家在位置 5 处落子。最终黑方玩家获得了游戏胜利（见彩插）

棋盘上的红色数字表示不同的位置，可用于表示每次动作的选择。白色和黑色圆圈是两个玩

家的棋子。游戏过程可以表示为一个序列：((b, 5),(w, 4),(b, 1),(w, 7),(b, 9))，其中“b”代表“黑

367

第 15 章 AlphaZero

方玩家”，“w”代表“白方玩家”。如图 15.1 最后一个棋盘状态所示，黑方玩家有三个棋子连成

一线，这表明黑方赢得了比赛。回忆我们之前提到的定义，这个简化的五子棋（或“三子棋”）满

足组合博弈问题的所有特征：游戏包含两个玩家；游戏不包含任何随机因素；游戏提供完美信息；

玩家以回合制的方式执行动作；游戏在有限时间步内结束。

这里我们给出无禁手五子棋的代码示例。

定义游戏为 Board 类，并将游戏规则实现成一些函数。我们之前用简化的版本介绍了五子

棋的规则，这里通过给变量 n_in_row 赋值为 5 来定义一个标准的五子棋。

class Board(object):

# 定义游戏的类

def __init__(self, width, height, n_in_row): ... # 初始化函数

def move_to_location(self, move): ... # 位置表示转换函数

def location_to_move(self, location): ... # 位置表示转换函数

def do_move(self, move): ... # 更新每一步走子，并交换对手

def has_a_winner(self): ... # 判断是否有玩家胜利

def current_state(self): ... # 生成网络的状态输入

...

如图 15.1 所示，棋盘上的每个走子位置都用一个数字表示，这样方便在蒙特卡罗树搜索过程

中建立树节点。但这种方式不便于辨认是否有五个棋子连成一线。所以我们定义了坐标和数字之

间的转换函数，坐标用来判断玩家是否有五个棋子连成一线，数字用来在树搜索中建立树节点。

def move_to_location(self, move):

# 从数字转换到坐标表示

# 例如 3 x 3 棋盘：

# 6 7 8

# 3 4 5

# 0 1 2

# 数字 5 的坐标表示为 (1,2)

h = move // self.width

w = move

return [h, w]

def location_to_move(self, location):

# 从坐标转换到数字表示

if len(location) != 2:

return -1

h = location[0]

w = location[1]

368

15.2 组合博弈

move = h * self.width + w

if move not in range(self.width * self.height):

return -1

return move

为了判断是否有玩家获胜，需要函数来判断一行或一列或对角线中是否有五个棋子连成一

线。函数 has_a_winner() 如下所示：

def has_a_winner(self):

# 判断是否有玩家获胜，如果有，返回是哪个玩家

width = self.width

height = self.height

states = self.states

n = self.n_in_row

# 棋盘上所有棋子的位置

moved = list(set(range(width * height)) - set(self.availables))

# 当前所有棋子数量不足以获胜

if len(moved) < self.n_in_row + 2:

return False, -1

for m in moved:

h, w = self.move_to_location(m)

player = states[m]

# 判断是否有水平线

if (w in range(width - n + 1) and

len(set(states.get(i, -1) for i in range(m, m + n))) == 1):

return True, player

# 判断是否有竖线

if (h in range(height - n + 1) and

len(set(states.get(i, -1) for i in range(m, m + n * width, width))) ==

1):

return True, player

# 判断是否有斜线

if (w in range(width - n + 1) and h in range(height - n + 1) and

len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width

+ 1))) == 1):

return True, player

if (w in range(n - 1, width) and h in range(height - n + 1) and

369

第 15 章 AlphaZero

len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width

- 1))) == 1):

return True, player

return False, -1

15.3 蒙特卡罗树搜索

蒙特卡罗树搜索（MCTS）(Browne et al., 2012) 是一种通过动作采样，并根据结果建立搜索

树，寻找在给定空间中最优决策的方法。这种方法在组合博弈和规划问题方面产生了革命性的影

响，并将围棋等 AI 算法的性能推向了前所未有的高度。

蒙特卡罗树搜索主要包括两部分：树结构和搜索算法。树是一种数据结构（图 15.2），它包

含由边连接的节点。一些重要的概念包括根节点、父节点与子节点、叶节点等。树结构最上方的

节点称为根节点；一个节点对应的上一级节点称为其父节点，一个节点对应的下一级节点称为其

子节点；没有子节点的节点称为叶节点。通常，除状态和动作外，搜索树中的节点还存有被访问

次数的统计和奖励的估值。在 AlphaZero 算法中，节点还包含该状态对应的动作概率分布。

图 15.2 树结构示意图

综上，如图 15.3 所示，AlphaZero 算法的搜索树中，每个节点包含以下信息：

• A：到达该节点所需执行的上一个动作（用以索引其父节点）。

• N：节点被访问次数。初始值为 0，表示该节点未被访问过。

• W：节点的奖励值之和，用以计算平均奖励。初始值设为 0。

• Q：节点的平均奖励值，通过 W

N 计算得到，代表该节点的值函数估计。初始值设为 0。

370

15.3 蒙特卡罗树搜索

• P：动作 A 的选取概率。这个值由神经网络输入其父节点的状态得到，存储到该子节点便

于索引和计算。

图 15.3 节点包含信息示例。其中位置 5 处有黑方玩家落子，这里动作可表示为 (b, 5)，代表黑方

玩家（“b”）执行动作 5 并到达该状态。N = 0 表示当前节点访问次数为 0，W 表示该

状态的奖励值之和，Q 表示平均奖励，P 表示选择动作 A = 5 的概率。由于当前节点还

未被访问过，所有的初始值都设为 0

在继续介绍之前，我们先强调一个关键点。由于游戏中存在两个玩家，所以在建立搜索树时，

一棵树里存在两个玩家的视角。节点上的信息要么从黑方玩家的视角进行更新，要么从白方玩家

的视角进行更新。例如，在图 15.3 中，此节点表示的棋盘状态只有一个黑方的棋子，所以此时应

该轮到白方玩家执行下一步。但是，需要注意的是，这个节点上的信息是从其父节点（即黑方玩

家）的角度来存储的。由于该节点是父节点在扩展其子节点的过程中新产生的，因此该节点上的

A、N、W、Q、P 都是由黑方玩家初始化的，并用于黑方视角下的后续更新和使用。所以，只有

黑方玩家选择动作 A = 5 才到达该节点，同时初始化当前信息为 N = 0、W = 0、Q = 0、P = 0。

对每个节点的视角有一个清晰的理解是非常重要的，否则在随后树搜索的过程中执行backup 步

时，不易理解整个更新过程。

建立搜索树后，蒙特卡罗树搜索通过启发式的方法探索决策空间，用以估计根节点的动作价

值函数 Qπ

(s, a)。整个过程可以描述为，从根节点开始一直探索到叶节点，多次重复该过程使得

每个动作的奖励估计逐渐精确，从而在搜索树中找到最优动作。不带折扣因子（Discount Factor）

的动作价值函数可以表示为 (Couetoux et al., 2011)：

Q

π

(s, a) = Eπ





T

X−1

h=0

P(Sh+1|Sh, Ah)R(Sh+1|Sh, Ah)|S0 = s, A0 = a, Ah = π(Sh)



 . (15.1)

其中 Qπ

(s, a) 表示动作价值函数，即在状态 s 执行动作 a 并依策略 π 选择动作，直到终止状

态时获得的期望奖励。

通常，树搜索方法有四个步骤：选择（Select），扩展（Expand），模拟（Simulate），回溯

（Backup），所有这些步骤都是在搜索树中执行的，真正的棋盘上没有落子。

• 选择：根据某个策略，从根节点开始选择动作，直到到达某个叶节点。

• 扩展：在当前叶节点之后添加子节点。

371

第 15 章 AlphaZero

• 模拟：从当前节点开始，通过某种策略（如随机策略）模拟下棋直到游戏结束，得到结果：

胜、负或平局。根据结果获得奖励，通常 +1 代表胜，−1 代表负，0 代表平局。

• 回溯：回溯更新模拟得到的结果，依次回访本轮树搜索中经过的节点，并更新每个节点上

的信息。

最常用的树搜索算法是 UCT（Upper Confidence Bound in Tree，树置信上界）算法 (Kocsis

et al., 2006)，它很好地解决了树搜索过程中探索与利用（Exploration versus Exploitation）之间的

平衡。UCT 算法是 UCB（Upper Confidence Bound，置信上界）算法 (Auer et al., 2002) 在树结构

中的扩展。UCB 算法（详见 2.2.2 节）是解决多臂赌博机（Multi-Armed Bandit）问题的经典算法。

在多臂赌博机问题中，智能体需要在每个时刻选择一个赌博机并得到对应奖励，其目标为最大化

期望奖励。UCB 算法根据以下策略在 t 时刻选择动作：

At = arg max

a



Qt(a) + c

s

ln t

Nt(a)



 . (15.2)

其中 Qt(a) 是动作值估计，该项增加了优势动作被选到的可能性，即估值越大，动作越倾向

被选取（即利用，Exploitation）。后一项平方根式中 Nt(a) 表示动作 a 在前 t 次时间步内被选中

的次数，该项增加了动作的探索度，即动作被选中的次数越少，该动作越倾向被选取（即探索，

Exploration）。c 是一个正的实数，用来调节探索与利用之间的权重。UCB 算法还有一系列变体，

如 UCB1、UCB1-NORMAL、UCB1-TUNED 和 UCB2 等 (Auer et al., 2002)。

UCT 算法是 UCB1 算法在树结构中的实现，该算法选择搜索树中最大 UCT 值对应的动作，

UCT 值定义如下：

UCT = Xj + Cp

s

2 ln n

nj

. (15.3)

这里，n 是当前节点的访问次数，nj 是其子节点 j 的访问次数，Cp > 0 是控制探索的权重

参数，可以根据具体问题具体设置。平均奖励项

q

Xj 鼓励利用高奖励对应的动作，而平方根项

2 ln n

nj 鼓励探索访问次数少的动作。

UCT 算法解决了树搜索中每个状态下对应动作的探索与利用的平衡，并颠覆了许多大规模

的强化学习问题，例如六连棋（Hex）、围棋（Go）和雅达利游戏（Atari）等。Levente Kocsis 和

Csaba Szepesvari (Kocsis et al., 2006) ´ 证明了：考虑一个有限状态马尔可夫决策过程（Finite-Horizon

MDP），其中奖励在 [0, 1] 之间，状态数为 D，每个状态的动作数为 K。考虑 UCT 算法，令 UCT

的根号项乘以 D，那么期望奖励 Xn 的估计偏差与 O(

log n

n

) 同阶。此外，随着搜索次数的增加，

根节点估计错误的概率以多项式速率收敛到零。这表明，随着搜索次数的增加，UCT 算法能够保

证树搜索收敛到最优解。

AlphaZero 算法舍弃了模拟步骤，直接用深度神经网络预测结果。因此，AlphaZero 算法包含

372

15.3 蒙特卡罗树搜索

三个关键步骤，如图 15.4 所示。

图 15.4 AlphaZero 中的蒙特卡罗树搜索。在每次真正落子之前，树搜索过程都会重复多次。它首

先从根节点选择动作直到到达叶节点，然后扩展叶节点并对其估值，最后执行回溯步骤

更新节点信息

• 选择: 根据某个策略，从根节点开始选择动作，直到到达某个叶节点。

• 扩展和评估：在当前叶节点之后添加子节点。同时每个动作的选取概率和状态值的估计直

接通过策略网络和价值网络预测得到。为了节约资源，通常在不损失算法效力的前提下，会

设置一个阈值来判断该节点是否需要扩展。我们的实现省略了这个阈值，每次到达叶节点

都进行扩展和评估。

• 回溯：扩展和评估完成之后，回溯更新结果，依次回访本轮树搜索中经过的节点，并更新

每个节点上的信息。如果叶节点不是游戏的终止状态，那么游戏无法返回胜负结果，转而

由神经网络预测得到。如果叶节点已经到达游戏的终止状态，那么结果直接由游戏给出。

在选择步骤中，动作由公式 a = arg maxa(Q(s, a) + U(s, a)) 给出。其中 Q(s, a) = W

N 鼓励利

用高奖励值对应的动作，U(s, a) = cpuctP(s, a)

√∑

b N(s,b)

1+N(s,a) 鼓励探索访问次数较少的动作，cpuct 平

衡探索和利用的权重，在 AlphaZero 算法中该值设为 5。

在扩展和评估步骤中，策略网络输出当前状态下每个动作被选择的概率p(s, a)，价值网络输出

当前状态s的估值v。p(s, a)用于在select步骤中计算U(s, a)，其中U(s, a) = cpuctP(s, a)

√∑

b N(s,b)

1+N(s,a) 。

v 用于在回溯步骤中计算 W，其中 W(s, a) = W(s, a) + v。神经网络输出的动作概率和状态值估

计开始时可能不准确，但在训练过程中会逐渐变准。

373

第 15 章 AlphaZero

在回溯步骤中，每个节点上的信息被依次更新，其中 N(s, a) = N(s, a) + 1, W(s, a) =

W(s, a) + v, Q(s, a) = W(s,a)

N(s,a) 。

部分核心代码如下：

蒙特卡罗树搜索过程定义为类 MCTS，它包含整个树结构和树搜索函数 _playout()：

class MCTS(object):

# 蒙特卡罗树搜索类

def__init__(self, policy_value_fn,action_fc,evaluation_fc, is_selfplay,c_puct,

n_playout): ... # 初始化函数

def _playout(self, state): ... # 树搜索过程

树中的节点定义为类 TreeNode，其中包括前述的三个关键步骤：选择，扩展和评估，回溯。

class TreeNode(object):

# 树节点类

# 每个节点保存值估计，动作选择概率等相关参数

def __init__(self, parent, prior_p): ... # 初始化函数

def select(self, c_puct): ... # 选择动作

def expand(self, action_priors, add_noise): ...# 扩展节点并评估当前状态和每个动作

def update(self, move): ... # 回溯更新节点

...

函数 select() 对应选择步骤：

def select(self, c_puct):

# 选择最大 UCT 值对应的动作，返回动作和下一个节点

return max(self._children.items(),

key=lambda act_node: act_node[1].get_value(c_puct))

函数 expand() 对应扩展和评估步骤。我们在每个节点都添加了狄利克雷噪声，增加随机

探索：

def expand(self, action_priors, add_noise):

# 扩展新节点

# action_priors 是策略网络输出的动作及其对应的概率值

if add_noise:

action_priors = list(action_priors)

length = len(action_priors)

dirichlet_noise = np.random.dirichlet(0.3 * np.ones(length))

374

15.3 蒙特卡罗树搜索

for i in range(length):

if action_priors[i][0] not in self._children:

self._children[action_priors[i][0]] = TreeNode(self,

0.75 * action_priors[i][1] + 0.25 * dirichlet_noise[i])

else:

for action, prob in action_priors:

if action not in self._children:

self._children[action] = TreeNode(self, prob)

函数 update_recursive() 对应回溯步骤：

def update_recursive(self, leaf_value):

# 递归更新所有节点

# 若该节点不是根节点，则递归更新

if self._parent:

# 通过传递取反后的值来改变玩家的视角

self._parent.update_recursive(-leaf_value)

self.update(leaf_value)

def update(self, leaf_value):

# 更新节点信息

self._n_visits += 1

# 更新访问次数

self._Q += 1.0 * (leaf_value - self._Q) / self._n_visits

# 更新值估计：(v-Q)/(n+1)+Q = (v-Q+(n+1)*Q)/(n+1)=(v+n*Q)/(n+1)

蒙特卡罗树搜索类 MCTS 调用树搜索函数 _playout() 依次执行三个步骤：选择，扩展和评

估，回溯。

def _playout(self, state):

# 执行一次树搜索过程

node = self._root

# 选择

while(1):

if node.is_leaf():

break

action, node = node.select(self._c_puct)

state.do_move(action)

# 扩展和评估

375

第 15 章 AlphaZero

action_probs, leaf_value =

self._policy_value_fn(state,self._action_fc,self._evaluation_fc)

end, winner = state.game_end()

if not end:

node.expand(action_probs,add_noise=self._is_selfplay)

else:

if winner == -1: # draw

leaf_value = 0.0

else:

leaf_value = (

1.0 if winner == state.get_current_player() else -1.0

)

# 回溯

node.update_recursive(-leaf_value)

15.4 AlphaZero：棋类游戏的通用算法

一般来说，AlphaZero 算法适用于各种组合博弈游戏，如围棋、国际象棋、日本将棋等。这

里，我们以 15.2 节中提到的无禁手五子棋作为例子，介绍 AlphaZero 算法的细节。因为游戏本身

不是重点，五子棋这样一个规则简单的回合制游戏非常适合作为例子。进一步，我们简化棋盘大

小为 3 × 3，如前所述，三个棋子连成一线表示获胜。另外，由于 AlphaZero 算法是 AlphaGo Zero

算法的加强版，这两种算法非常相似。我们的实现同时参考了这两种算法。

为了让读者更好理解该算法，本节将演示 AlphaZero 算法的详细流程。整个算法可分为两部

分：（1）采用蒙特卡罗树搜索的自博弈强化方法收集数据；（2）利用深度神经网络拟合数据并用

于蒙特卡罗树搜索中。整个过程如图 15.5 所示。

首先，我们演示蒙特卡罗树搜索收集数据。为了用相对较短的篇幅演示树搜索过程直到游戏

的终止状态，我们假定游戏从图 15.6 所示的状态开始（通常游戏是从一个空棋盘开始的）。此时，

轮到白方玩家执行动作。

我们从这个状态开始构建树结构，依次执行前述三个树搜索步骤：选择，扩展和评估，回溯。

此时树中只有一个节点，由于它在树的顶部，所以是根节点，又因为它没有子节点，所以它也是

叶节点。这意味着我们已经到达了一个叶节点，相当于已经完成了选择步骤。因此，接下来执行

第二个步骤：扩展和评估。图 15.7 展示了节点扩展的过程，该节点的所有子节点被展开，同时策

略网络以该节点状态作为输入，给出了每个动作被选择的概率。

最后一步是回溯。由于当前节点是根节点，我们不需要回溯 W 和 Q（用于判断树搜索是否

应该到达该节点），只需更新访问次数 N。将 N = 0 更新为 N = 1，本次树搜索过程完成。

376

15.4 AlphaZero：棋类游戏的通用算法

图 15.5 算法流程。在 AlphaZero 算法中，蒙特卡罗树搜索、数据及神经网络形成了一个循环。蒙

特卡罗树搜索结合神经网络用于生成数据，生成的数据用于提升网络预测精度。网络预

测越精确，蒙特卡罗树搜索生成的数据质量越高；数据质量越高，训练的网络预测越精

确；整个过程形成良性循环

图 15.6 棋盘状态。棋盘大小为 3 × 3。在该状态下，轮到白方玩家执行动作

每次重新执行树搜索，我们都将从根节点开始。如图 15.8 所示，第二次树搜索过程也将

从根节点开始。这一次，根节点下存在子节点，这意味该节点不是叶节点。动作由公式 a =

arg maxa(Q(s, a) + U(s, a)), Q(s, a) = W

N

, U(s, a) = cpuctP(s, a)

√∑

b N(s,b)

1+N(s,a) 给出。这里白方玩家

选择动作 A = 2(w, 2)，并到达新节点。这个新节点为叶节点，且此时，轮到黑方玩家选择动作。

我们对这个叶节点进行扩展和评估。与第一次相同：所有可行的动作都被扩展，每个动作的

概率由策略网络给出。

现在轮到回溯操作了。此时树里有两个节点，我们首先更新当前节点，然后更新前一个节点。

这两个节点的更新遵循相同的方式：N(s, a) = N(s, a) + 1, W(s, a) = W(s, a) + v(s), Q(s, a) =

W(s,a)

N(s,a) 。值得注意的是，树中有两个视角：黑方视角和白方视角。我们需要注意更新的视角，并

且总是以当前玩家的视角更新值。例如，在图 15.10 中，价值网络的估值 v(s) = −0.1，这是从黑

方玩家的角度来看的。当更新属于白方玩家的信息时需要取反，即 v(s) = −0.1。所以，我们得

到 N = 1, W = 0.1, Q = 0.1。

377

第 15 章 AlphaZero

图 15.7 根节点处的扩展和评估。所有可行动作的节点都被扩展，神经网络给出相应的概率值

π(a|s)

puct

argmax

图 15.8 根节点处的选择。白方玩家选择 A = 2 (w, 2) 并到达叶节点。此时轮到黑方玩家选择动

作

然后我们返回到它的父节点。和之前一样，由于当前状态的节点是根节点，我们不需要回溯

更新 W 和 Q，只需要更新访问次数 N。所以，令 N = 2，第二次树搜索过程完成。如图 15.11

所示。

378

15.4 AlphaZero：棋类游戏的通用算法

图 15.9 新节点处的扩展和评估。所有可行的动作都被扩展，神经网络给出相应的概率值 π(a|s)

图 15.10 新节点处的回溯。当前节点的信息被更新，Q 值从白方视角进行更新：N = 1, W =

0.1, Q = 0.1

第三次树搜索过程也从根节点开始。根据公式 a = arg maxa(Q(s, a) + U(s, a)) 和当前树中

的信息，白方玩家选择动作 2 (w, 2)，黑方玩家选择动作 9 (b, 9)。如图 15.12 所示，经过选择步骤

后，游戏到达了终止状态。这次对于扩展和评估步骤，节点将不会被扩展，同时可以直接从游戏

中获取价值 v。因此，价值网络不会用来估计状态价值，策略网络也不会输出动作概率。

379

第 15 章 AlphaZero

图 15.11 根节点处的回溯。N 更新为 2，W 和 Q 不需要更新

图 15.12 终节点处的扩展和评估。由于游戏在该节点结束，因此不会扩展任何节点，并且可以

直接从游戏中获得奖励。所以，这里不会使用策略网络和价值网络

接下来是回溯，且轨迹上有三个节点。如前所述，节点从叶节点递归更新直到根节点，其中

N(s, a) = N(s, a) + 1, W(s, a) = W(s, a) + v(s), Q(s, a) = W(s,a)

N(s,a) 。此外，还应该切换每个节点的

380

15.4 AlphaZero：棋类游戏的通用算法

视角，这意味着 vwhite = −vblack。在这个游戏中，黑方玩家选择动作 9 (b, 9) 并到达一个新节点。

现在本该轮到白方玩家选择动作，但遗憾的是游戏在此结束了，白方玩家输掉了游戏。所以奖励

值 reward = −1 是从白方玩家的视角来看的，也就是说 vwhite = −1。当我们更新这个节点上的信

息时，如前所述，这些信息是被黑方玩家用来选择动作 A = 9 并到达这个节点，所以这个节点的

值应该是 vblack = −vwhite = 1，其他信息同理，有 N = 1, W = 1, Q = 1。剩余两个节点的信息也

以同样的方式更新。

在完成回溯步骤之后，树结构如图 15.13 所示。根节点已被访问三次，并且每个被访问过的

节点信息都已更新。

图 15.13 回溯步骤之后的树结构。在第三次树搜索过程中，回溯步骤递归地更新三个被访问节

点的信息。由于两个玩家在同一树结构中，且 vwhite = −vblack，所以需要注意从正确的

视角更新信息

我们已经演示了三次蒙特卡罗树搜索的迭代过程。经过 400 次搜索后（在 AlphaGo Zero 算

法中，搜索次数是 1600；在 AlphaZero 算法中，搜索次数是 800，如图 15.14 所示），树结构变得

更大，且估值更加精确。

经过树搜索过程之后，可以在真正的棋盘上走子了。动作的选取通过计算每个动作的访问

次数并归一化为概率进行选择，而不是直接通过策略网络输出动作概率：π(a|s) = N(s,a)

1/τ

N(s)

1/τ −1 =

N(s,a)

1/τ

∑

b N(s,b)

1/τ ，其中 τ → 0 是温度参数，b ∈ A 表示状态 s 下的可行动作。这里选择的动作是 9

381

第 15 章 AlphaZero

(w, 9)，如图 15.15 所示。

图 15.14 经过 400 次搜索的树结构。由于第一次搜索是从扩展和评估步骤开始的，并没有选择

子节点，因此其子节点的访问次数之和为 400，根节点的访问次数为 401。这个细节并

不影响算法思想

τ

τ

τ

图 15.15 在真正的棋盘上走子。经过 400 次搜索后，根据 π(a|s) = N(s,a)

1/τ

∑

b N(s,b)

1/τ 选择动作。这里

白方玩家选择动作 9 (w, 9)

382

15.4 AlphaZero：棋类游戏的通用算法

温度参数用来控制探索度。若 τ = 1，动作的选择概率和访问次数成正比，则这种方式探索

度高，可以确保数据收集的多样性。若 τ → 0，则探索度低，此时倾向于选择访问次数最大的动

作。在 AlphaZero 和 AlphaGo Zero 算法中，当执行自博弈过程收集数据时，前 30 步（在我们的

实现中为 12 步）的温度参数设为 τ = 1，其余部分设置为 τ → 0。当与真正对手下棋时，温度参

数始终设置为 τ → 0，即每次都选择最优动作。

至此，位置 9 处已经放置了白方棋子，因此树中的根节点将被更改。如图 15.16 所示，蒙特

卡罗树搜索将从新的根节点继续。其他兄弟节点及其父节点将被剪枝丢弃以节省内存。

图 15.16 新的根节点。新根节点下的节点将被保留，其他节点将被丢弃

整个过程一直重复下去，直到一局游戏结束。我们得到数据和结果如图 15.17 所示。

每个动作的概率计算方式为：π(a|s) = N(s,a)

1/τ

∑

b N(s,b)

1/τ

,τ = 1。需要注意，这里的概率通过访问

次数计算，这是蒙特卡罗树搜索自博弈过程和神经网络训练相结合的关键点。由于该局游戏的结

果是平局，这里所有数据的标签都是 0（图 15.18）。

现在我们已经有了蒙特卡罗树搜索生成的数据，下一步就是利用深度神经网络进行训练。在

训练过程中，首先将数据转换成堆叠的特征层。每个特征层只包含 0-1 值用以表示玩家的落子，

其中一组特征层表示当前玩家的落子，另一组特征层表示对手的落子。这些特征层按照历史动作

序列顺序堆叠。然后，我们用与 AlphaGo Zero 算法相同的数据增强方法对数据进行扩展：由于围

棋和五子棋的规则都不受旋转和镜像翻转的影响，因此在训练之前，我们将数据做旋转和镜像翻

转增强。在蒙特卡罗树搜索过程中，棋盘状态被随机旋转或镜像翻转，然后再用神经网络进行预

测，从而可以在一定程度上减小方差。然而在 AlphaZero 算法中，由于某些游戏规则不具有旋转

和镜像翻转不变性，因此 AlphaZero 没有使用该技巧。言归正传，随着收集的数据越来越多，不

断训练的网络会得到更加精确的估计。

383

第 15 章 AlphaZero

图 15.17 棋谱数据。该局游戏的所有状态都将被保存，并赋予动作概率 π(a|s) 和状态值 v(s)

图 15.18 带标签的数据。动作的概率根据 π(a|s) = N(s,a)

1/τ

∑

b N(s,b)

1/τ 计算，标签 v(s) 来自游戏的结果：

1 表示胜利，-1 表示失败，0 表示平局

我们使用 ResNet (He et al., 2016) 作为网络结构（图 15.19），这和 AlphaGo Zero 算法相同。

网络的输入是前述构造的状态特征，输出是动作概率和状态值。网络可以表示为 (p, v) = fθ(s)，

数据为 (s, π, r)，其中 p, π 为列向量。损失函数 l 由动作分布的交叉熵损失、状态值的均方误差和

参数的 L2 正则化组成。具体公式为 l = (r − v)

2 −π

T log p + c∥θ∥

2，其中参数 c 调节正则化权重。

此外，我们介绍一些关于模型更新的细节。在 AlphaGo Zero 算法中，新模型将和当前的最优

模型对打 400 局，如果新模型胜率超过 55%，那么它将替换掉之前的模型成为当前的最优模型，

即对模型有一个评估的过程。相比之下，在 AlphaZero 算法的版本中，它不与之前的模型进行对

打，而是直接不断更新模型参数，这些都是可行的方法。我们的版本和 AlphaGo Zero 的方式相

384

15.4 AlphaZero：棋类游戏的通用算法

同，以使训练过程更加稳定。此外，如果想更快地训练模型，可以使用多进程并行收集数据，甚

至采用原论文异步树搜索的方式。图 15.20 展示了并行的训练方式，多个进程同时从最优模型中

源源不断生成自博弈数据，收集到最新的自博弈数据用来训练神经网络，最新训练的模型和最优

模型进行不断评估（AlphaGo Zero 的方式），所有这些进程都并行执行。

图 15.19 网络结构。结构与 AlphaGo Zero 算法相同。ResNet 作为主干，两个头分别输出概率分

布和状态估值

图 15.20 并行训练框架

随着新数据源源不断地生成，神经网络不断迭代训练，得到更准确的估值，一个强大的五子

棋 AI 就生成了。

我们最终在 11 × 11 的棋盘上通过多进程并行的方式训练了无禁手规则的五子棋 AI，表 15.1

列出了一些具体参数。随后我们在15× 15的棋盘上同样成功训练了一个模型，这表明了AlphaZero

385

第 15 章 AlphaZero

算法的通用性和稳定性。

表 15.1 参数对比

参数 五子棋 AlphaGo Zero AlphaZero

cpuct 5 5 5

MCTS times 400 1600 800

residual blocks 19 19/39 19/39

batch size 512 2048 4096

learning rate 0.001 annealed annealed

optimizer Adam SGD with momentum SGD with momentum

Dirichlet noise 0.3 0.03 0.03

weight of noise 0.25 0.25 0.25

τ = 1 for the first n moves 12 30 30

参考文献

ALBERT M, NOWAKOWSKI R, WOLFE D, 2007. Lessons in play: an introduction to combinatorial

game theory[M]. CRC Press.

AUER P, CESA-BIANCHI N, FISCHER P, 2002. Finite-time analysis of the multiarmed bandit problem[J].

Machine learning, 47(2-3): 235-256.

BROWNE C B, POWLEY E, WHITEHOUSE D, et al., 2012. A survey of monte carlo tree search

methods[J]. IEEE Transactions on Computational Intelligence & Ai in Games, 4(1): 1-43.

CAMPBELL M, HOANE JR A J, HSU F H, 2002. Deep blue[J]. Artificial intelligence.

COUETOUX A, MILONE M, BRENDEL M, et al., 2011. Continuous rapid action value estimates[C]//

Asian Conference on Machine Learning. 19-31.

HE K, ZHANG X, REN S, et al., 2016. Deep residual learning for image recognition[C]//Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition. 770-778.

HSU F H, 1999. Ibm’s deep blue chess grandmaster chips[J]. IEEE Micro, 19(2): 70-81.

KOCSIS L, SZEPESVÁRI C, 2006. Bandit based monte-carlo planning[C]//European conference on

machine learning. Springer: 282-293.

386

参考文献

MUTHOO A, OSBORNE M J, RUBINSTEIN A, 1996. A course in game theory.[J]. Economica, 63

(249): 164-165.

SILVER D, HUANG A, MADDISON C J, et al., 2016. Mastering the game of go with deep neural

networks and tree search[J]. Nature.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2017a. Mastering chess and shogi by self-play with

a general reinforcement learning algorithm[J]. arXiv preprint arXiv:1712.01815.

SILVER D, SCHRITTWIESER J, SIMONYAN K, et al., 2017b. Mastering the game of go without human

knowledge[J]. Nature, 550(7676): 354.

SILVER D, HUBERT T, SCHRITTWIESER J, et al., 2018. A general reinforcement learning algorithm

that masters chess, shogi, and Go through self-play[J]. Science, 362(6419): 1140-1144.

387

16 模拟环境中机器人学习

本章主要介绍模拟环境中机器人学习的一个上手项目，包括在 CoppeliaSim 中设置一个机械

臂抓取物体的任务，并用深度强化学习算法柔性 Actor-Critic（Soft Actor-Critic，SAC）去解决它。

实验部分展示不同奖励函数的效果，用以验证辅助密集奖励对于解决类似机器人抓取任务的重要

性。在本章末尾，我们也对机器人学习应用、模拟到现实的迁移和其他机器人学习项目及模拟器

进行简单的讨论。

深度强化学习算法有很多潜在的现实世界应用场景，机器人控制是其中最令人振奋的领域

之一。尽管深度强化学习算法已经能够很好地解决绝大多数简单的游戏，像之前介绍的 OpenAI

Gym 环境等，我们目前还不能期望深度强化学习方法在机器人控制领域能完全替代传统控制方

法，比如反向运动学（Inverse Kinematics）或比例-积分-微分（Proportional–Integral–Derivative,

PID）控制等。然而，深度强化学习能够应用于某些具体情形，作为与传统控制相辅相成的方法，

尤其是对于高度复杂的系统或者灵活操控任务 (Akkaya et al., 2019; Andrychowicz et al., 2018)。

在绝大多数情况下，机器人控制的动态过程可以用马尔可夫（Markov）过程很好地近似，这

使得它成为深度强化学习在模拟和现实中的一个理想的试验场。另外，深度强化学习对于现实世

界中机器人控制的巨大潜力也吸引了许多像 DeepMind 和 OpenAI 等高科技公司来投入这个研究

领域。近来，OpenAI 甚至通过自动域随机化（Automatic Domain Randomization）技术来解决模

拟到现实的迁移（Sim-to-Real Transfer）问题，从而用一个单手五指机械臂解决了 Rubik 魔方，如

图 16.1 所示。其他公司也开始研究使用比如在仓储物流中的货物分发任务上使用机械臂，甚至

直接让机器人在现实世界中训练 (Korenkevych et al., 2019)。

然而，由于将强化学习算法直接应用于现实世界中有采样效率低和安全性的问题，人们发

现直接在现实世界中训练强化学习策略来解决复杂的机器人系统控制或灵活操控任务 (Akkaya

et al., 2019) 是有困难的。在模拟环境中训练并在随后将策略迁移到现实世界，或者利用人类专家

388

16.1 机器人模拟

图 16.1 用一个机器手解决 Rubik 魔方的场景。图片改编自文献 (Akkaya et al., 2019)（见彩插）

的示范（Human Expert Demonstrations）来学习，都是更有潜力满足机器人学习的计算性能和安

全要求的方式。机器人的模拟器已经发展了数十年，包括 DART、CoppeliaSim（在 3.6.2 版本之

前叫作 V-REP）(Rohmer et al., 2013)、MuJoCo、Gazebo 等。在本章最后一小节会有相关讨论。为

了便于人们使用深度强化学习控制策略和其他数值操作，这些模拟器多数都有 Python 对应版本。

在模拟环境中学习至少在两个方面有意义。第一，模拟环境可以用作新提出算法或框架的试

验地（包括但不限于强化学习领域），尤其是大规模的现实世界应用，比如机器人学习任务。在

模拟环境中学习可以作为新方法在应用到现实情景前的验证过程。第二，对于通过模拟到现实迁

移的方式解决现实世界问题来说，在模拟中学习是不可或缺的一步，可以减少时间消耗和物理设

备磨损。

在这一章，我们将介绍把深度强化学习算法应用到一个模拟环境中简单的机器人物体抓取任

务的过程，使用 CoppeliaSim（V-REP）模拟器和它的 Python 封装：PyRep (James et al., 2019a)。我

们开源了这个项目的任务描述和深度强化学习算法相关代码1，便于读者学习和理解。

由于之前已经介绍了一个将强化学习应用于大规模高维度连续空间的应用，本章的机器人学

习任务将更加着重实践中强化学习的其他方面，包括如何构建一个能通过强化学习实现特定任务

的模拟环境，如何设计奖励函数来辅助强化学习实现最终的任务目标等，以给读者提供对强化学

习更好的理解，不仅限于训练过程，更在于如何设计学习环境。

16.1 机器人模拟

我们第一步要做的是设置一个模拟环境，包括：一个机械臂、与机械臂交互的一个物块。这

个模拟环境应当符合现实物理动态规律。然而，这里我们要强调一点，一个真实的模拟不意味着

在这个模拟环境中学习到的策略就可以直接在现实世界中取得好的表现。一个“真实的”模拟环

1链接见读者服务

389

第 16 章 模拟环境中机器人学习

境可以通过不同的具体形式实现，而其中只有一种形式可以与实际的现实世界相匹配。举例来

说，不同光照条件可以在物体上产生不同的阴影效果，而这些可能看起来都很“真实”，但是只

有其中一种是跟现实相同的，而且由于深度神经网络的敏感性，这些外观上的细微差异可能导致

现实中做出截然不同的动作。为了解决这类模拟到现实迁移过程的问题，如域随机化（Domain

Randomization）、动力学随机化（Dynamics Randomization）等许多方法被提出和应用，我们也将

在本章进行相关讨论。

现在有许多机器人的模拟器，包括 CoppeliaSim（V-REP）、MuJoCo、Unity 等。原版 CoppeliaSim

（V-REP）软件使用 C++ 和 Lua 语言支持的通用接口，而只有部分函数功能可以通过 Python 实现。

然而，对于应用深度强化学习而言，最好使用 Python 接口。幸运的是，我们有 PyRep 软件包来将

CoppeliaSim（V-REP）用于深度机器人学习。在本项目中，我们使用 CoppeliaSim（V-REP）并搭

配它的软件包 PyRep 来调用 Python 接口。

我们将在本节展示设置一个机器人学习任务的基本过程。

安装 CoppeliaSim 和 PyRep

CoppeliaSim（V-REP）软件可以在官网2下载到，而在本书的写作过程中，我们需要CoppeliaSim

（V-REP）的 3.6.2 版本（可以在网站3上找到）来跟 PyRep 兼容。它可以直接通过解压下载的文件

来安装。注意高于 CoppeliaSim（V-REP）3.6.2 的版本可能跟这个项目的其他模块不兼容。

安装完 CoppeliaSim（V-REP）之后，我们可以通过以下几步安装我们仓库网站（链接见读者

服务）上的一个 PyRep 的分支稳定版本：

git clone https://github.com/deep-reinforcement-learning-book/PyRep.git

pip3 install -r requirements.txt

python3 setup.py install --user

# 注意：在以下指令中需要将路径改为用户本机的 VREP 安装位置

export VREP_ROOT=EDIT/ME/PATH/TO/V-REP/INSTALL/DIR

export LD_LIBRARY_PATH=$



LD_LIBRARY_PATH:



$VREP_ROOT

export QT_QPA_PLATFORM_PLUGIN_PATH=$



VREP_ROOT

source~/.bashrc

记得通过上面脚本中的

VREP_ROOT 更改 V-REP 的路径。

Git 克隆本项目

本章的深度强化学习算

法应用于机器人学习任

务项目可以通过以下命

令下载：

2链接见读者服务

3链接见读者服务

390

16.1 机器人

模拟

git clone https://github.com/deep-reinforcement-learning-book/Chapter16-Robot-Learning

-in-Simulation.git

这个项目包含机器

人的部分（机械臂，夹具）和

其他我们需要的物体、构

建的机器人抓取任务情

景、用来训练智能体控制

策略的深度强化学习算

法等。本项目中的机器人

抓取任务情景见图 16.2。

我们

将在以下几小节中展示

如何构建这个包含基本

组成部分的场景。

图 16.2 CoppeliaSim (V-REP) 中的

抓取（Grasping）任务场景（见彩插）

组

装机器人

我们使用名为

Rethink Sawyer 的机械臂和一个 BraxterGripper 终端夹

具。官方

PyRep 软件包

提供了多

种机械臂和夹具，可以用

来组装和构建你想要的

任务场景。我们这里提供

一个例子，将

一个夹具安

装到机械臂，如图 16.3 所示。

在

我们的 Git 文件下，将./hands/BaxterGripper.ttm 和./arms/Sawyer.ttm 拖入

在 CoppeliaSim（V-REP）中打开的一个新场景

。我们选择夹具并同时按

Ctrl

加鼠标左键单击 Sawyer 的

终端

关节（即 Sawyer_wrist_connector，它是 CoppeliaSim（V-REP）中的一个力

传感器，可以

用于连接不

同物体），然后单击“组装”按

钮，如图 16.4 所示。CoppeliaSim（V-REP）提供了不同

种类的连接器，这里的力

传感器只是其中一种，且

这种连接器在关节受到

的真实力大于一个阈值

的时候有破碎的可能。另

一方面，我们不应该在这

里用“组合/合并”（group/merge）选项，这是

为

了能够独立控制夹具

和机械臂。更多关于如何

连接和组合不同物体的

细节可以查阅 CoppeliaSim

（V-REP）的网站。在

我们完成以上过程后，所

构建场景的层级（Scene

hierarchy）应当如

图 16.5

所示。

391

第 16

章 模拟环境中

机器人学习

图 16.3 Sawyer 机械臂末

端（左）和组装的夹具

BaxterGripper（右）（见

彩插）

图 16.4 CoppeliaSim (V-REP) 中的“组装”（assemble）按钮

构

建学习环境

图 16.2 展示了 CoppeliaSim（V-REP）中

一个构建好的场景，相应

文件为./scenes/sawyer_

reacher_rl.ttt。为了构建这个最

终场景，我们需要把其他

物体添加到当前只包含

机械臂和夹

具的场景中

。

首先，我们通过添加（Add）-> 简单

形状（Primitive shape）-> 长方体（Cuboid）添加一个

目

标物体，调整它成为我们

想要的尺寸并重命名为

“目标”。我们需要双击“目标

”前面的图标

并选择公共

（Common）-> 可渲染的（Renderable）来使得物体对

视觉传感器可见。

在以上

步骤之后，我们需要添加

一个可以给我们提供定

制场景视野的视觉传感

器。这个视觉

传感器可以

在模拟过程中一直拍摄

视野的图像，如果我们使

用基于图像的控制，那么

这个视觉传

感器是必需

的（如果不是基于图像的

控制，我们可能不需要它

）。如果我们在场景中启用

这个视

觉传感器，我们可

以在模拟的每一步返回

图像。为了设置这样的场

景，单击添加（Add）->

视觉

传感器

（Vision sensor）-> 视角类型（perspective type），然后右键单击

场景，选择添加（Add）

->

浮动视野

（Floating view）。这时先单击我们刚刚创

建的视觉传感器，然后右

键单击打开的浮动

视野

，选择视图（View）-> 关联视图和已

选择的视觉传感器（associate view with

selected vision

sensor）。随后

，我们手动设置添加的视

觉传感器的位置和旋转

角度，得到如图 16.6 所示的场

景。

下面，我们从项目文件

夹./objects

中拖入物体文件 table.ttt。通过

单击物体（Object）/

物品移动（item shift）按键

，我们手动设置这个带夹

具机械臂的位置和目标

长方体的位置，使得

它们

位于桌子上方，如图 16.7

所示

。

392

16.1 机器人模拟

图 16.5

CoppeliaSim (V-REP) 中任务场

景的层级，包括 Sawyer 机械臂在

内的所有物理模型。红色

箭头表示用于端点控制

模式的反向运动学链。黑

色字体表示场景中可见

的物体，而灰色

字体表示

不可见的虚拟物体

图 16.6 在

CoppeliaSim (V-REP) 中设置视觉传感器。左面

的图片设置相机位置；右

面图片中右上

角的小窗

口是由所放置的相机得

到的。如果采用基于图像

的控制策略并调用相机

，那么

它可以给每个时间

步提供图像观察量（见彩

插）

393

第 16 章 模拟环境中机器

人学习

图 16.7 手动改变 CoppeliaSim (V-REP) 中物

体的位置（见彩插）

以上是

设置环境场景的过程，这

个场景给我们提供了任

务中可以看到的实体。这

些实体的动

力学过程将

遵从物理模拟器的模拟

规则。除此之外，我们还需

要给在环境中定义控制

流程和奖励

函数（Reward Functions），通常包

括物体移动的限制条件

（主要是运动类的任务）、一

个训练片段

（Episode）的开启和结

束步骤、初始化条件、观察

量的形式等。在我们的 Git

文

件中，我们提供

了一个脚

本 sawyer_grasp_env_boundingbox.py 用来在场景中实现这

些功能。为了便于之后应

用强化学习算法进行控

制，这个脚本我们采用与

OpenAI Gym 环境相似的应用程序接

口（APIs）。

我们上面构建的场景

本身是静态的，而这个控

制脚本可以为它提供控

制动力学过程的功能（除

了

模拟器中实现的物理

过程）。对于这个机器人抓

取任务，我们使用正向运

动学（直接控制关节运

动

速度）的控制机制来控制

机械臂。我们也使用不同

配置方式实现了一个通

过反向运动学实现控

制

的（控制机械臂终端位置

）的场景。反向运动学控制

通常需要求一个描述关

节角度和机械臂端

点位

置关系的雅可比（Jacobian）矩阵的

逆，这个功能在 PyRep

中也有支

持。更多关于反向运动

学

设置的细节超出本书范

围。我们提供的例子程序

中的脚本定义的动力学

过程和机器人控制可以

支持以上两种控制机制

。

注意：实践中，当你尝试构

建自己的机器人模型或

用不同的组件组装定制

机械臂的时候，你

需要小

心机械臂上不同模块的

组装顺序和依赖关系。这

与 CoppeliaSim（V-REP）软件对动态和

静态组

件（比如反向运动学中的

Sawyer_tip

是一个静态组件）的一些

要求有关。细节参考官方

网站4。

在 CoppeliaSim（V-REP）中设置好环境场

景之后，我们需要用 PyRep 软件

包写一个定义环

境中动

力学过程和奖励函数的

控制脚本。我们的仓库中

提供了定义环境的代码

。下面几小节中我

们将介

绍项目中用到的函数和

模块。

环境脚本中的模块

导入所需软件包并设置

下面需要的全局变量。

4链

接见读者服务

394

16.1 机器人模

拟

from os.path import dirname, join, abspath

from pyrep import PyRep

from pyrep.robots.arms.sawyer

import Sawyer

from pyrep.robots.end_effectors.baxter_gripper import BaxterGripper

from pyrep.objects.proximity_sensor import ProximitySensor

from pyrep.objects.vision_sensor

import VisionSensor

from pyrep.objects.shape import Shape

from pyrep.objects.dummy import Dummy

from pyrep.const

import JointType, JointMode

import numpy as

np

import matplotlib.pyplot as plt

import

math

POS_MIN, POS_MAX = [0.1, -0.3,

1.], [0.45, 0.3, 1.] # 目标物体有效位置范

围

所定义机器人抓取任

务环境类的整体结构显

示如下。这里所有的函数

都在类中简写，我们将

在

后文中展开介绍。

class GraspEnv(object):

# Sawyer

机器人

抓取物块

def __init__(self, headless, control_mode=’joint_velocity’):

#

参数：

# :headless: bool，如果为 True，没

有可视化；否则有可视化

# :control_mode:

str，’end_position’ 或’joint_velocity’

......

def _get_state(self):

#

返回包括关节角度或

速度和目标位置的状态

......

def _is_holding(self):

# 返回抓取目标与否的状

态，为 bool

......

def _move(self, action, bounding_offset=0.15, step_factor=0.2,

max_itr=20,

max_error=0.05, rotation_norm =5.):

# 对于’end_position’

模式，用反向运

动学根据动作移动末端

。反向运动学模式控制是

通过设

# 置末端目标来实

现的，而非使用 solve_ik() 函数，因为

有时 solve_ik()

函数不能正确工作

395

第 16 章 模拟环境中机器人

学习

#

模式：闭环比例控制

，使用反向运动学

# 参数：

# :bounding_offset: 有

效目标位置范围外的边

界方框所用的偏移量，作

为有效且安全的动作

# 范

围

# :step_factor: 小步长因子，用来乘以

当前位置和位置的偏差

，即作为控制的比例因子

# :max_itr:

最大移动迭代次数

# :max_error: 每次

调用时移动距离误差的

上边界

# :rotation_norm:

用来归一化旋转

角度值的因子，由于动作

对每个维度有相同的值

范围，角

# 度需要额外处理

......

def reinit(self):

#

重新初始化环境，比如可

当夹具在探索中破损时

调用

......

def reset(self, random_target=False):

#

重置夹具位置和目

标位置

......

def step(self, action):

#

根据动作移动机

械臂：如果控制模式为’joint_velocity’，则

动作是 7 维的关节速度值

+1

# 维的夹具旋转值；如果控

制模式为’end_position’，则动作是 3

维末

端（机械臂端点）位置

# +1 维夹

具旋转值

......

def

shutdown(self):

# 关闭模拟器

......

第

一步是初始化环境，包括

设置共用变量，如 __init__()

函数所

定义的一样：

def __init__(self, headless, control_mode=’joint_velocity’):

#

参数：

# :headless: bool，若为 True，则

没有可视化；否则有可视

化

#

:control mode: str，’end_position’ 或’joint_velocity’

# 设置公共变量

396

16.1 机器

人模拟

self.headless = headless

# 若 headless 为 True，则无可视化

self.reward_offset =

10.0 # 抓到物体的奖励值

self.reward_range = self.reward_offset

# 奖励

值域

self.penalty_offset = 1. #

对不希望发生情形

的惩罚值

self.fall_down_offset = 0.1 # 用于判断物体

掉落桌面的距离值

self.metadata=[] # gym 环境

参数

self.control_mode =

control_mode

# 机械臂控制模式：’end_position’ 或

’joint_velocity’

函数 __init__()

的第二部分是设定

和启动场景，并设置场景

中物体相应的代理变量

：

self.pr = PyRep() # 调用

PyRep

if control_mode == ’end_position’: #

所有关节都以反向

运动学的方式进行的位

置控制模式

SCENE_FILE = join(dirname(abspath(__file__)),

’./scenes/sawyer_reacher_rl_new_ik.ttt’) #

使用反向运

动学控制的场景

elif control_mode == ’joint_velocity’: #

所有关

节都以正向运动学的力

或力矩方式进行的

# 速度

控制模式

SCENE_FILE = join(dirname(abspath(__file__)),

’./scenes/sawyer_reacher_rl_new.ttt’) # 使用正向运动

学控制的场景

self.pr.launch(SCENE_FILE, headless=headless) #

启动场景

，headless 意味着无可视化

self.pr.start() # 启动场

景

self.agent

= Sawyer() # 得到场景中的机械臂

self.gripper = BaxterGripper()

# 得到场景中的夹具

self.gripper_left_pad = Shape(’BaxterGripper_leftPad’) #

夹具

手指上的左护垫

self.proximity_sensor = ProximitySensor(’BaxterGripper_attachProxSensor’)

# 传感器

名称

self.vision_sensor = VisionSensor(’Vision_sensor’) # 传感器名称

self.table

= Shape(’diningTable’) # 场景中

的桌子，用来检查碰撞

if control_mode

== ’end_position’: # 通

过机械臂端点位置来用

反向运动学控制机械臂

self.agent.set_control_loop_enabled(True) # 若为

False，则反向运动学无法

工作

self.action_space = np.zeros(4)

# 3

自由度的端点位置

控制和 1 自由度的夹具旋

转控制

elif control_mode ==

’joint_velocity’:

# 通过直接设置每

个关节速度来用正向运

动学控制机械臂

self.agent.set_control_loop_enabled(False)

self.action_space =

np.zeros(7)

# 7 自由度

速度控制，无须额外控制

端点旋转，第 7 个关节控制

它

else:

397

第 16 章 模拟环境中机器

人学习

raise NotImplementedError

self.observation_space = np.zeros(17) #

7 个关节的标量位

置和标量速度 +3 维目标位

置

self.agent.set_motor_locked_at_zero_velocity(True)

self.target

= Shape(’target’) # 得到目标物体

self.agent_ee_tip =

self.agent.get_tip()

# 机械臂

末端的一个部分，作为反

向运动学控制链的末端

来进行控制

self.tip_target = Dummy(’Sawyer_target’)

# 末端（机械臂

的端点）运动的目标位置

self.tip_pos = self.agent_ee_tip.get_position() # 末端

x，y，z 位置

函数 __init__() 的第三部

分是设置合适的初始机

器人姿势和末端位置：

if

control_mode == ’end_position’:

initial_pos = [0.3,

0.1, 0.9]

self.tip_target.set_position(initial_pos) # 设

置目标位置

#

对旋转来说

单步控制足以通过设置

reset_dynamics=True 就可以立即设置旋转角

self.tip_target.set_orientation([0,np.pi,np.pi/2], reset_dynamics=True)

# 前两个沿着 x

和 y 轴的维度

使夹具向下

self.initial_tip_positions = self.initial_target_positions

= initial_pos

elif control_mode == ’joint_velocity’:

self.initial_joint_positions = [0.0, -1.4, 0.7, 2.5,

3.0, -0.5, 4.1]

# 一个合适的

初始姿态

self.agent.set_joint_positions(self.initial_joint_positions)

self.pr.step()

如下所示是一

个获得观察状态的函数

，包括关节位置和速度，以

及目标物体的三维空间

位置，

总共 17 维。

def

_get_state(self):

# 返回包括关

节角度或速度和目标位

置的状态

return np.array(self.agent.get_joint_positions() +

# list，维数为 7

self.agent.get_joint_velocities() + #

list，维数为

7

self.target.get_position()) # list，维数为 3

一个决定夹具是

否抓到物体的函数被定

义为

_is_holding()，通过夹具护垫上的

碰撞检测

和近距离传感

器来决定物体是否在夹

具内。

def _is_holding(self):

# 返回抓取目标与否

的状态，为

bool

398

16.1 机器人模拟

# 注

意碰撞检测不总是准确

的，对于连续碰撞帧，可能

只有开始的

4～5 帧碰撞可以

被检测到

pad_collide_object = self.gripper_left_pad.check_collision(self.target)

if

pad_collide_object and self.proximity_sensor.is_detected(self.target)==True:

return True

else:

return False

函数 _move() 可以在有

效范围内通过反向运动

学模式操控移动机械臂

末端执行器。PyRep

中可以通过

在机械臂末端放置一个

部件来实现以反向运动

学控制末端执行器，具体

做法是设置这

个末端部

件的位置和旋转角。如果

调用 pr.step() 函数，那么在 PyRep 中机械

臂关节的反向运

动学控

制可以自动求解。由于单

个较大步长的控制可能

是不精确的，这里我们将

整个动作产生的

位移运

动分解为一系列小步长

运动，并采用一个有最大

迭代次数和最大容错值

的反馈控制闭环来

执行

这些小步长动作。

def _move(self, action, bounding_offset=0.15,

step_factor=0.2, max_itr=20,

max_error=0.05, rotation_norm =5.):

#

对于’end_position’ 模

式，用反向运动学根据动

作移动末端。反向运动学

模式控制是通过设

# 置末

端目标来实现的，而非使

用 solve_ik() 函数，因为有时

solve_ik() 函数不

能正确

# 工作。

# 模式：闭环比

例控制，使用反向运动学

#

参数：

# :bounding_offset: 有效目标位置范围

外的边界方框所用的偏

移量，作为有效且安全的

动作

# 范围

# :step_factor: 小步长因子，用

来乘以当前位置和目标

位置的偏差，即作为控制

的比例因子

# :max_itr: 最大移动迭

代次数

# :max_error: 每次调用时移动

距离误差的上界

# :rotation_norm: 用来归

一化旋转角度值的因子

，由于动作对每个维度有

相同的值范围，

# 角度需要

额外处理

pos=self.gripper.get_position()

# 检查状态加动

作是否在边界方框内，若

在，则正常运动；否则动作

不会被执行。该范围为

#

x_min < x < x_max 且

y_min

< y < y_max 且 z

> z_min

if pos[0]+action[0]>POS_MIN[0]-bounding_offset and

pos[0]+action[0]<POS_MAX[0]+bounding_offset

\

and pos[1]+action[1] > POS_MIN[1]-bounding_offset and

pos[1]+action[1] <

399

第 16 章

模拟环境中机器

人学习

POS_MAX[1]+2*bounding_offset \

and pos[2]+action[2] >

POS_MIN[2]-2*bounding_offset: # z 轴有较大偏移量

# 物体的 set_orientation()

和 get_orientation() 之间有一个错

配情况，

# set_orientation() 中的（x，y，z）对应

get_orientation() 中的（y，x，-z）

ori_z=-self.agent_ee_tip.get_orientation()[2]

# 减

号是因为 set_orientation()

和 get_orientation() 之间的错配

target_pos = np.array(self.agent_ee_tip.get_position())+np.array(action[:3])

diff=1

# 初始化

itr=0

while np.sum(np.abs(diff))>max_error and

itr<max_itr:

itr+=1

# 通过小步来到达

位置

cur_pos =

self.agent_ee_tip.get_position()

diff=target_pos-cur_pos # 当前位置和目标位

置差异，进行闭环控制

pos =

cur_pos+step_factor*diff

# 根

据当前差异迈一小步，防

止反向运动学无法求解

self.tip_target.set_position(pos.tolist())

self.pr.step() # 每次设置末端目标位置

，需调用模拟步来实现

# 对

z 轴旋转单步即可，但是由

于反向运动学求解器的

问题，所以还是存在小误

差

ori_z+=rotation_norm*action[3]

# 归一化旋转值，因为通

常在策略中对旋转和位

移的动作范围是一样的

self.tip_target.set_orientation([0,

np.pi, ori_z])

# 使夹具向下并沿 z 轴旋转

ori_z

self.pr.step() # 模拟步

else:

print("Potential Movement

Out of the Bounding Box!")

pass

# 如果潜在运动超

出了边界方框，动作不会

执行

这里提供了一个可

以重新初始化场景的函

数。

def reinit(self):

#

重新初始化环境，比如

当夹具在探索中破损时

可调用

self.shutdown() # 首先关掉当前环

境

self.__init__(self.headless) #

以相同的 headless 模式进行初

始化

400

16.1 机器人模拟

如下是

一个能够重置场景中目

标物体和机械臂的函数

。

def reset(self, random_target=False):

# 重置夹具位置和目标位

置

# 设置目标物体

if random_target: # 随机化

pos

= list(np.random.uniform(POS_MIN, POS_MAX)) # 从合理范围的均匀分布

中采样

self.target.set_position(pos)

# 随机位置

else: # 无随机

化

self.target.set_position(self.initial_target_positions)

# 固定位置

self.target.set_orientation([0,0,0])

self.pr.step()

# 把末端位置

设置到初始位置

if self.control_mode == ’end_position’: # JointMode.IK

self.agent.set_control_loop_enabled(True) # 反向运

动学模式

self.tip_target.set_position(self.initial_tip_positions)

# 由于反向运动

学模式或力/力矩模式开

启，所以无法直接设置关

节位置

self.pr.step()

# 避免卡住的情况

：由于使用反向运动学来

移动，所以机械臂卡住会

使得反向运动学难以求

解，

# 从而无法正常重置，因

此在预期位置无法到达

时需要采用一些随机动

作

itr=0

max_itr=10

while np.sum(np.abs(np.array(self.agent_ee_tip.get_position()-

np.array(self.initial_tip_positions))))>0.1 and itr<max_itr:

itr+=1

self.step(np.random.uniform(-0.2,0.2,4)) # 采取随机动作来防止

卡住的情况

self.pr.step()

elif

self.control_mode == ’joint_velocity’: # JointMode.FORCE

self.agent.set_joint_positions(self.initial_joint_positions)

self.pr.step()

# 设置可碰撞

（collicable）模式，用于碰撞检测

self.gripper_left_pad.set_collidable(True)

# 设置

夹具护垫为可碰撞的，从

而可以检测碰撞

self.target.set_collidable(True)

401

第 16 章 模

拟环境中机器人学习

# 如

果夹具没有完全打开，将

其完全打开

if np.sum(self.gripper.get_open_amount())<1.5:

self.gripper.actuate(1, velocity=0.5)

self.pr.step()

return self._get_state() # 返回环境当

前状态

如其他环境（OpenAI

Gym 等）中

经常使用的 step() 函数，在我们

这里的环境中也会用到

。

这个函数需要相应的动

作值作为输入。如果机器

人是由 end_position

模式使用反向运

动学控制

的，它需要调用

之前定义的 _move() 函数来执行

动作；如果机器人是由 joint_velocity 模

式

通过正向运动学控制

的，那么机械臂上的关节

位置可以直接被设定。

def step(self, action):

# 根

据动作移动机械臂：如果

控制模式为’joint_velocity’，那么动作是

7

维的关节速度值 +1

# 维的夹

具旋转值；如果控制模式

为’end_position’，那么动作是 3 维末端（机

械臂端点）位置

# +1 维夹具旋

转值

# 初始化

done=False

# 片段结束

reward=0

hold_flag=False # 是

否抓住物体的标签

if self.control_mode == ’end_position’:

if action

is None or action.shape[0]!=4: # 检查

动作是否合理

print(’No actions or wrong action dimensions!’)

action = list(np.random.uniform(-0.1, 0.1, 4)) #

随机

self._move(action)

elif self.control_mode == ’joint_velocity’:

if action is None or action.shape[0]!=7:

# 检查

动作是否合理

print(’No actions or wrong

action dimensions!’)

action = list(np.random.uniform(-0.1, 0.1,

7)) # 随机

self.agent.set_joint_target_velocities(action) # 机械

臂执行动作

self.pr.step()

else:

raise NotImplementedError

402

16.1

机器人模拟

除了移动机械臂，奖励函

数（Reward Function）、吸收状态（Absorbing State）、结束信号（done）

和

其他像标记物体被持有

状态的信息等也是通过

step() 函数实现的，如下所示。成

功抓取物体

的奖励是一

个正数，而物体掉落桌面

的惩罚是一个同样数值

大小的负数。这构成了一

种稀疏奖励

机制，而可能

对智能体来说很难学习

。所以我们添加了距离上

的惩罚项来辅助学习。这

个惩罚项

的值是末端执

行器到目标物体的距离

，同时我们也惩罚夹具与

桌面的碰撞来避免夹具

损坏。这构

成了一个密集

奖励函数。然而，我们要知

道密集奖励函数可能跟

最终的任务目标有出入

，而我们

的目标是让机器

人抓取目标物体。由于距

离惩罚项正比于夹具和

物体中心的距离，它会促

使夹具

尽可能地接近物

体中心，而这可能导致不

合适的抓取姿态。更多关

于这种奖励函数与强化

学习任

务目标之间的分

歧可以参考第

18 章中的讨

论。由于这个原因，我们需

要对奖励函数进行修正

，比

如设定一个位于目标

物体上方的位置偏移量

（虚拟目标点）来取代目标

物体的中心，我们将在随

后的几小节中进行相关

讨论。

ax, ay, az

= self.gripper.get_position()

if math.isnan(ax): # 捕捉探索中夹具破

损的情况

print(’Gripper position is nan.’)

self.reinit()

done=True

tx, ty, tz = self.target.get_position()

sqr_distance

= (ax - tx) ** 2

+ (ay - ty) ** 2

+ (az - tz) ** 2

# 夹具和目标物

体的距离的平方

# 在夹具

与物体足够近且物体被

近距离传感器检测到时

关闭夹具

if sqr_distance<0.1

and self.proximity_sensor.is_detected(self.target)== True:

# 确保抓取之前

夹具是打开的

self.gripper.actuate(1,

velocity=0.5)

self.pr.step()

self.gripper.actuate(0, velocity=0.5)

# 如果结束

了，关闭夹具，0

是关闭，1 是打

开；速度 0.5 可以确保夹具在

一帧内关闭

self.pr.step() #

物理模拟器

前进一步

if self._is_holding():

reward += self.reward_offset

# 抓到物体的额

外奖励

done=True

hold_flag = True

else:

self.gripper.actuate(1, velocity=0.5)

self.pr.step()

403

第

16 章 模拟环境中

机器人学习

elif np.sum(self.gripper.get_open_amount())<1.5: #

如果夹具由

于碰撞或其他原因是

# 关

闭的（或未完全打开），打开

它；get_open_amount() 返回夹具关节的一些

数据值

self.gripper.actuate(1, velocity=0.5)

self.pr.step()

else:

pass

# 基本奖励是距离

目标的负值

reward

-= np.sqrt(sqr_distance)

# 物体掉落桌

面的情况

if tz

< self.initial_target_positions[2]-self.fall_down_offset:

done = True

reward

= -self.reward_offset

# 机械臂与桌面

碰撞的惩罚

if self.gripper_left_pad.check_collision(self.table):

reward -= self.penalty_offset

if math.isnan(reward): #

捕捉数值问

题

reward = 0.

return self._get_state(),

reward, done, {’finished’: hold_flag}

用于关闭环境的函数

相对简单：

def

shutdown(self):

# 关闭模拟器

self.pr.stop()

self.pr.shutdown()

在

以下实验中，我们采用如

下关于上面抓取任务的

基本设定：目标物体的初

始位置是固定的；机

器人

关节位置的初始化选取

了可以避免较复杂机器

人姿态的方式；机器人是

通过正向运动学模式

来

控制关节速度的；机器人

的控制是基于数值状态

的，包括关节位置、关节速

度和目标位置作为

观察

量。但是读者可以随意更

改这些设置使其更复杂

，比如使用反向运动学模

式来控制机器人末

端位

置，使用原始图像进行基

于视觉的控制或用它与

部分的数值状态相结合

，使用更少的信息作

为观

察量，或者设置任务使其

更加困难和复杂，等等。

404

16.2 强

化学习用于机器人学习

任务

在项目文件中，Sawyer 抓取

任务的环境可以用以下

命令来测试：

python sawyer_grasp_env_boundingbox.py

16.2 强化学习用

于机器人学习任务

上述

基于正向运动学控制的

机器人学习环境有一个

控制关节速度的 7 维连续

动作空间，以及

一个

17 维连

续状态空间，因此相比于

之前第 5 章和第 6 章中的例

子而言，这是一个相对复

杂的

环境。并且，机器人模

拟系统的复杂性使得采

样过程需要耗费相当长

的时间。这使得通过单线

程

或单进程框架在较短

时间内训练一个相对好

的策略很困难。实践中，我

们发现策略学习速度的

瓶

颈主要在于 CoppeliaSim（V-REP）的模拟过

程，如果只用单进程采样

，就会使得整个学习过程

非

常低效。我们需要并行

的离线训练框架来改善

这个任务的采样速度。

在

这个项目中，我们使用并

行的柔性

Actor-Critic（Soft Actor-Critic，SAC）算法，使用的是

第

13 章的项目中的并行框

架。SAC 算法的详细介绍在第

6 章，包括理论和实现方法

，所以这里只简

短地描述

选择 SAC 算法的原因和优点

所在。作为一种离线策略

（Off-Policy）学习算法，SAC 使

用对角高斯

（Gaussian）策略来应对高维连续动

作空间，并且它在训练中

比其他像深度确定性策

略梯度（Deep Deterministic

Policy Gradient）算法更加稳定并

且对参数鲁棒，尤其是采

用了对熵因

子进行适应

性学习的方法 (Haarnoja et al.,

2018) 后。它也采

用柔性 Q-Learning（Soft Q-Learning）

来进行熵正则化

，这对像机器人抓取这类

难以训练的任务来说可

以促进探索。还有，由于 SAC

算

法采用离线策略的学习

方式，所以它在实践中可

以较方便地改成并行版

本。

即使采用了并行的采

样进程，让机器人基于上

面定义的密集奖励探索

一个好的物体抓取姿势

也很困难，如果只使用稀

疏奖励则会难上加难。为

了进一步促进学习过程

，我们对奖励函数进行

启

发式增强。首先，目标物体

是一个长方体，由于它的

长边要长于夹具的开合

宽度，所以夹具只

能调整

方向使其垂直于物体长

边来夹取，并且朝向需要

向下。因此，我们在奖励函

数上添加了一

个额外的

惩罚项如下：

#

对角度的增

强奖励：如果夹具与目标

物体方向相垂直，这是一

个更好的抓取姿态

# 注意

夹具的坐标系与目标坐

标系有\pi/2的z方向角度差异

desired_orientation = np.concatenate(([np.pi, 0],

[self.target.get_orientation()[2]]))

# 夹具与目标垂直，并且朝

下

rotation_penalty = -np.sum(np.abs(np.array(self.agent_ee_tip.get_orientation())-

desired_orientation))

rotation_norm = 0.02

reward +=

rotation_norm*rotation_penalty

其次，如上所述，将夹具

和物体之间距离的负数

作为奖励函数的一部分

可能会导致非最优的夹

取

405

第 16 章

模拟环境中机器

人学习

姿势。所以第二个

奖励函数上的修正是通

过设置目标物体中心上

方一个偏移量的位置作

为零惩罚

点的，这个对距

离项的修改如下所示：

# 对

目标位置的增强奖励：目

标位置相对目标物体有

垂直方向上的相对偏移

offset=0.08 #

偏移量

sqr_distance = (ax - tx)

** 2 + (ay - ty)

** 2 + (az - (tz+offset))

** 2

# 夹具和目标位置

距离的平方

通过以上两

种对奖励函数的增强，学

习效果相比于原始的密

集或稀疏奖励的情况得

到进一步提

升，如图

16.8 所示

。

0 250 500 750

1000 1250 1500 1750 2000

10

0

Reward

Learning Curve (Sparse Reward)

0 250 500 750 1000 1250

1500 1750 2000

7.5

5.0

2.5

Smoothed Reward

Learning Curve (Dense Reward)

0 250 500 750 1000 1250

1500 1750 2000

7.5

5.0

2.5

Smoothed Reward

Learning Curve (Augmented Reward)

0 250 500 750 1000 1250

1500 1750 2000

Training Episode

0.00

0.05

Success Rate

Success

Sparse Reward

Dense Reward

(Augmented) Dense Reward

学习曲线（稀疏奖励）

学习

曲线（增强密集奖励）

学习

曲线（密集奖励）

成功

训练

⽚段

成功率 奖励

平滑奖励

平滑奖励

稀疏奖励

密集

奖励

增强密集奖励

图 16.8

使

用 SAC 算法并行训练的Sawyer机器

人抓取任务的学习表现

，使用不同奖励函数的

比

较

奖励函数工程是实践

中一种有效结合人类先

验知识来辅助学习的方

式，尽管这可能与科学研

究本身的诉求相斥，因为

从科研的角度讲，人们往

往更加专注于减少奖励

函数工程的工作量，以

及

其他对智能体学习的人

为辅助，同时希望实现更

加智能和自动化的学习

过程。其实，在实践中

解决

一个任务，类似以上的一

些人为辅助设计可能会

很有帮助。除了奖励函数

工程，从专家示范

中学习

也是实践中一种有效改

善学习效果的方式，如第

8 章所述。

406

16.2 强化学习用于机

器人学习任务

16.2.1 并行训练

CoppeliaSim（V-REP）软件需要每个模拟环境

有一个独立的进程。因此

，为了加速采样过程，

我们

设置了多进程而非多线

程的方式来并行收集样

本。我们的代码库中提供

了一个通过 PyTorch

实现的多进

程版本的 SAC

算法。其训练和

测试过程可以简单地运

行如下：

# 训练

python sac_learn.py --train

# 测试

python sac_learn.py --test

在这个

代码中，环境的交互是通

过多个进程实现的，每个

进程包含一个模拟环境

。

16.2.2 学习效果

我们测试了算

法在 Sawyer 抓取任务上的表现

，并在表 16.1

中给出了训练所

需的超参数。学

习效果如

图 16.8 所示，包括三种不同类

型的奖励函数。图 16.8 中稀疏

奖励函数下的值

−10 是由

物

体掉落桌面的惩罚造成

的。不同的奖励函数给出

了不同范围的奖励值，直

接对这些奖励值曲线

进

行比较可能是不公平的

。除给出（平滑的）片段奖励

外，我们还展示了整个学

习过程中的抓取

成功率

。随着训练进行，我们可以

清楚地看到成功事件发

生得越来越频繁，这显示

出机器人抓取

技能的进

步。增强的奖励函数对比

原始密集奖励函数体现

了显著的加速学习的效

果，而对于稀疏

奖励来说

，探索和学习抓取物体几

乎是不可能的。

表 16.1 SAC 的超参

数

参数

值

优化器 Adam (Kingma et al.,

2014)

学习率

3×10−4

奖励折扣（γ） 0.99

工作者（workers）数量 6

隐

藏层数（策略） 4

隐藏单元数

（策略） 512

隐藏层数（Q 网络）

3

隐藏

单元数（Q 网络） 512

批尺寸 128

目标

熵值 动作维度的负值

缓

存尺寸 1 × 106

407

第 16 章 模拟环境中

机器人学习

如图

16.9 所示，在

几千个片段的训练过后

，机器人已经能够从一个

固定的目标物体位置将

其

抓到，尽管抓取的姿势

不是很完美且成功率还

不是很高。在这个例子中

，整个训练过程是从头开

始的，没有任何的示范或

预训练。

图 16.9 经过训练，Sawyer

在模

拟环境中用深度强化学

习的策略抓取物体（见彩

插）

16.2.3 域随机化

当我们将模

拟环境中训练得到的策

略用于现实中时，由于现

实世界动力学过程和模

拟环境中

的差异，这个策

略往往不能成功。域随机

化是改善策略泛化能力

的一种方法，尤其是当我

们将模

拟环境中学习的

策略迁移到现实情景中

时。

域随机化可以通过随

机化环境中的物理参数

来实现，包括决定机械臂

动力学及其与场景中其

他物体动态交互过程的

参数。具体来说，随机化物

理参数叫作动力学随机

化 (Peng et al., 2018)，比

如，物体的质量、机械

臂上关节的摩擦力、物体

和桌面之间的摩擦力等

。并且，在基于视觉的控

制

中，物体颜色、光照条件和

物体材质也可以被随机

化，这些会影响通过观察

机器人图像来对其

进行

控制的智能体。比如，我们

可以用以下命令在 PyRep 中设

置物体的颜色：

self.target.set_color(np.random.uniform(low=0, high=1,

size=3).tolist())

# 为目标物

体颜色设置 [r，g，b]3 通道值

其他

模拟环境中的物理参数

也可以进行相应设置，这

里超出了本章范畴。在训

练智能体时，我们

可以在

整个训练过程中对每个

片段或者几十个片段进

行一次参数重设置。同时

，重要的是，要保

证对模拟

环境中动力学参数和其

他特征进行随机化的范

围要能够覆盖现实中真

实的动力学过程，

从而缓

解模拟现实间隙。

域随机

化只是在模拟到现实迁

移中缓解现实间隙的一

种可能的方式，以上使用

PyRep 在

CoppeliaSim

中进行视觉特征随机

化的步骤也只是一个很

简单的例子。关于模拟到

现实迁移的详

细描述在

第 7 章中。

408

16.2

强化学习用于机

器人学习任务

16.2.4 机器人学

习基准

在以上小节中，我

们展示了如何构建一个

机器人抓取任务，并用一

个强化学习算法去解决

它。

近来，文献 (James

et al., 2019b) 提出了 RLBench 软件

包

(链接见读者服务)，作为

一个覆盖 100 个

独立的人为

设计任务的大规模基准

和学习环境。这个软件包

专门用于促进基于视觉

的机器人操控

领域的研

究，不仅限于强化学习，而

且可以应用于模仿学习

、多任务学习、几何计算机

视觉和小

样本学习。如图

16.10

5 所示，RLBench 基于前几节所用的

PyRep，它包含了 100 个基本的机器

人操控任务，包括抓取、移

动、堆积和其他多样的现

实世界中常见的操作，也

支持通过简单的设

置步

骤实现任务定制化，并使

得包括强化学习在内的

不同的学习方法可以用

来解决这个环境中的

任

务。

图 16.10 RLBench 中定义的机器人学

习任务（见彩插）

前几小节

中介绍的机器人抓取任

务提供了一个用

CoppeliaSim（V-REP）实现模

拟环境中机

器人学习的

标准框架，这也适用于 RLBench 软

件包。它们都包括至少三

个基本要素：（1）在

CoppeliaSim（V-REP）中进行任

务场景的构建，（2）通过脚本

定义环境的模拟过程，包

括 reset()

和 step() 函数，（3）通过脚本提供

一个能够学习的智能体

，比如用强化学习。RLBench 遵循这

种构建流程，但以一种层

次化的结构来搭建全体

任务。

RLBench 软件包可以通过以

下命令来安装（如果你已

经安装了

PyRep）：

git clone https://github.com/stepjam/RLBench.git

pip3 install

-r requirements.txt

python3 setup.py install --user

16.2.5 其他模拟器

如

图 16.11 所示，有许多不同的机

器人学习模拟软件，包括

OpenAI Gym、CoppeliaSim（VREP/PyRep）(James

et al., 2019a; Rohmer et al.,

2013)、MuJoCo (Todorov et al., 2012)、Gazebo，

5图像来自

RLBench。

409

第 16 章 模拟环境

中机器人学习

Bullet/PyBullet (Coumans et al., 2016, 2013)、Webots

(Michel, 2004)、Unity 3D、NVIDIA Isaac SDK

等。实践中

，这些软件包或者平台对

于不同的应用有不同的

特征。举例来说，OpenAI

Gym robotics

环境是一

个相对简单的环境，可以

快速验证提出的方法；CoppeliaSim 和

Unity 3D 都是基于物理

模拟器的

，且有着相对较好的渲染

效果；MoJoCo 有较为现实和准确

的物理引擎，可以用于模

拟

到现实迁移；Isaac SDK 是一个相

对较新的软件（于 2019

年发布

），对深度学习算法和应用

有较

强的支持，以及基于

Unity 3D 的照片级真实的渲染，等

等。

图 16.11

机器人学习任务：（1）OpenAI Gym 中

的 FetchPush（左）；（2）使用 PyRep 实现的目标

到

达任务（中）；RoboSuite 中的 SawyerLift 任务（右）（见

彩插）

参考文献

AKKAYA

I, ANDRYCHOWICZ M, CHOCIEJ M, et

al., 2019. Solving rubik’s cube with

a robot hand[J].

arXiv preprint arXiv:1910.07113.

ANDRYCHOWICZ M, BAKER B, CHOCIEJ M,

et al., 2018. Learning dexterous in-hand

manipulation[J]. arXiv preprint arXiv:1808.00177.

COUMANS E,

BAI Y, 2016. Pybullet, a python

module for physics simulation for games,

robotics and

machine learning[J]. GitHub repository.

COUMANS E, et al., 2013. Bullet

physics library[J]. Open source: bulletphysics. org,

15(49): 5.

HAARNOJA T, ZHOU A,

HARTIKAINEN K, et al., 2018. Soft

actor-critic algorithms and applications[J].

arXiv preprint

arXiv:1812.05905.

JAMES S, FREESE M, DAVISON

A J, 2019a. Pyrep: Bringing v-rep

to deep robot learning[J]. arXiv

preprint

arXiv:1906.11176.

JAMES S, MA Z, ARROJO

D R, et al., 2019b. Rlbench:

The robot learning benchmark & learning

environment[J]. arXiv preprint arXiv:1909.12271.

410

参考文献

KINGMA

D, BA J, 2014. Adam: A

method for stochastic optimization[C]//Proceedings of the

International Conference on Learning Representations (ICLR).

KORENKEVYCH D, MAHMOOD A R, VASAN

G, et al., 2019. Autoregressive policies

for continuous

control deep reinforcement learning[J].

arXiv preprint arXiv:1903.11524.

MICHEL O, 2004.

Cyberbotics ltd. webots: professional mobile robot

simulation[J]. International Journal

of Advanced Robotic

Systems, 1(1): 5.

PENG X B,

ANDRYCHOWICZ M, ZAREMBA W, et al.,

2018. Sim-to-real transfer of robotic control

with dynamics randomization[C]//2018 IEEE International Conference

on Robotics and Automation

(ICRA). IEEE:

1-8.

ROHMER E, SINGH S P,

FREESE M, 2013. V-rep: A versatile

and scalable robot simulation framework[C]//2013 IEEE/RSJ

International Conference on Intelligent Robots and

Systems. IEEE: 1321-

1326.

TODOROV E,

EREZ T, TASSA Y, 2012. Mujoco:

A physics engine for model-based control[C]//IROS.

411

17 Arena：多智能体强化学习平台

在这一章节，我们将介绍

一个名为 Arena (Song et

al., 2019) 的用于研究多

智能体强化学习

（Multi-Agent Reinforcement Learning，MARL）的项目

。我们提供了一些上手经

验来使用

Arena

工具包构建游

戏，包括一个单智能体游

戏和一个简单的双智能

体游戏，并采用不同的奖

励机制。

Arena 中的奖励机制是

一种定义多智能体间社

会结构的方式，包括不可

学习的（Non-Learnable）、

独立的（Isolated）、竞争的（Competitive）、合

作的（Collaborative）和混合型的（Mixed）社会关

系。不同的奖励机制可以

在一个游戏场景中用于

同一个层次性结构上，不

同的层次性结构上也可

以用不同的奖励机制，配

合对物理单元的从个体

到群体的结构性表示，可

以用来全面地描述多智

能体系统的复杂关系。此

外，我们也展示了在 Arena

中使

用基准库的过程，它提供

了许多已实现

的多智能

体强化学习算法来作为

基准。通过这个项目，我们

希望给读者提供一个有

用的工具，来

研究在定制

化游戏场景中使用多智

能体强化学习算法的表

现。

Arena 是一个在 Unity

上对多体智

能学习进行评估的通用

平台。它使用多样化逻辑

和表示方

式来构建学习

环境，并对多智能体复杂

的社会关系进行简单的

配置。Arena 也包含对最先进深

度

多智能体强化学习算

法基准的实现，可以帮助

读者快速验证所建立的

环境。总体来说，Arena 是

一个帮

助读者快速创造和构建

包含多智能体社会关系

的定制化游戏环境的工

具，用以探索多智

能体问

题。Arena 注重于第一人称或第

三人称动作类游戏，借助

于 Unity 极好的渲染效果来实

现

3D 仿真环境。而其他像最

近由

DeepMind 发布的开源项目 OpenSpiel，则

专注于多智能体棋牌类

游戏。

Arena 中有两个主要的模

块：（1）开发工具包（the Building

Toolkit），可以用来

快速构建有定

制特征的

多智能体环境；（2）基准库（the Baselines），可

以用 MARL 算法来测试所搭建

的环境。

我们将从构建

Arena 中

的环境开始。

412

17.1 安装

17.1

安装

Unity ML-agents 工

具包是使用 Arena 的前提，需要

在使用

Arena 之前将其安装。Arena 完

整

的安装过程遵循开发

工具包和基准库各自的

官方网站。

注意，如果你想

在没有图形用户界面（比

如 X-Server）的远程服务器上运行

或者你无法访问

X-Server，那么你

就需要根据 17.3.1 节中或 Arena 官网

上的指示来设置虚拟显

示。

安装好之后，我们可以

发现在

Arena 文件夹下的 Arena-BuildingToolkit/Assets/Arena

SDK/GameSet/文件

中，有几十个已构建好的

或连续或离散动作空间

的游戏场景。它们是预先

制

作好的，作为使用 Arena

的例

子，你可以阅读所有这些

游戏的脚本来更好地理

解 Arena 是如何工

作的。所有的

游戏和抽象层共用同一

个 Unity 项目。每一个游戏都在

一个独立的文件夹中，游

戏

名即文件夹命名。ArenaSDK 文件

夹存放了所有的抽象层

和共享代码、实体和功能

。整体代码风

格与 Unity ML-agents 工具包

尽可能一致。

17.2 用 Arena 开发游戏

我们将使用 Arena 开发工具包

内提供的许多现成的实

体和多智能体功能，来展

示一个多智能

体游戏的

构建过程，它不需要很多

代码。在你开始之前，我们

希望你已经有了关于 Unity 使

用的

基本知识。因此，推荐

你先完成官网上的 roll-a-ball 教程

来学习关于

Unity 的一些基本

概念。

为了使用 Arena，运行 Unity，选择

打开项目，选择克隆或下

载的“Arena-BuildingToolkit”文

件。第一次打开的

过程可能会花费一定时

间。

我们可以看到 Arena 文件夹

下几十个建好的游戏。它

们是预先设计作为 Arena 用例

的，你可

以阅读这些游戏

的脚本来进一步理解

Arena 是

如何工作的。我们将在下

面小节中提供搭建这些

游

戏的基本指导。

图 17.1 Arena

文件

中提供已构建的游戏

413

第

17 章 Arena：多智能体强化学习平

台

17.2.1

简单的单玩家游戏

我

们从构建一个基本的单

玩家游戏开始：

• 创建一个

文件夹来存放游戏。在这

部分中，我们为单玩家游

戏创建名为“1P”的文件夹。

• 在

左边的“Hierarchy”窗口中，我们删除

原来的

Main Camera 和 Directional Light。将 Arena

文件夹 Assets/ArenaSDK/SharedPrefabs 下预

制的 GlobalManager（如图 17.2 所示）拖

入左边

的“Hierarchy”窗口，如图 17.3 所示。注意到

，这些预制物体是 Unity 中公用

的模块，

可以通过简单的

拖曳操作来使用任何提

前定制好的物体。Arena

中的 GlobalManager 管

理

整个游戏，因此其他组

成部分需要依附在它下

面。

图 17.2

Arena 中的预制模块（见彩

插）

图 17.3 将 Arena

预制模块中的 GlobalManager 拖

到当前游戏的“Hierarchy”窗口中

• 下

面我们需要放置一个智

能体的运动场所，我们找

到 Arena

中名为 PlayGroundWithDeadWalls 的预制模块，然

后将它附于 GlobalManager 的子节点 World

上

。GlobalManager 也

有另一个子节点 TopDownCamera 用于

提供一个游戏的全局视

野。这一步在图 17.4

中有所

展

示。

• 与上面类似，我们需要

从 Arena 预制模块中选择一个

BasicAgent

并将其附于 GlobalManager，如图 17.5 所示。从

而我们现在得到一个在

场地上的智能体，我们可

以手动拖拽智能

体到一

个合适的位置，如图 17.6

所示

。x 轴、y 轴和 z 轴的位置和旋转

角将在智能体的

Transform

属性中

展示。

• 为了让智能体正常

工作，我们需要设定游戏

参数，如图 17.7 所示。这里我们

只需要改变

GlobalManager

中的 Living Condition Based On Child

Nodes。Living Condition 被选择

为

At Least Specific

Number Living 而 At Least Specific

Number Living 值被设为 1。由于我们

在这个游戏中只有一个

智能体，上面的设置保证

了在智能体数量小于 1 的

任何情况下，这

414

17.2 用 Arena 开发游

戏

图

17.4 选择 Arena 预制模块中的

一个运动场（Playground），并将它附于

GlobalManager 的子节

点上（见彩插）

图 17.5 选

择并将一个 Arena 预制模块中

的 BasicAgent

附于 GlobalManager 的子节点上（见彩

插）

图 17.6 单个智能体在场地

上的场景（见彩插）

个游戏

片段（Episode）就会结束并重启。现

在我们需要按下 Play 按钮来

开始游戏并用键盘

上的

“W，A，S，D”操作智能体移动。由于场

地的一边是“Dead Walls”，智能体无论

何

415

第 17 章 Arena：多智能体强化学

习平台

图 17.7

进行单玩家游

戏设置

时碰触它都会结

束生命并且重新开始游

戏。使用 BasicAgent 时也会有很多其

他性质，包括

不同的 Actions

Settings、Reward Functions（用于

强化学习）等。你可以调试

它们（在当前

这个游戏中

只有 Actions Settings 是有效的）来熟悉

Arena 开

发工具包。

17.2.2 简单的使用奖

励机制的双玩家游戏

在

这一小节，我们将介绍如

何在游戏场景中按照社

交树（Social Tree）来部署多于一个智

能

体的游戏。

• 首先，让我们

从上面的单玩家游戏开

始。如果我们选择 GlobalManager 或 BasicAgent，那么

我们就会发现这些对象

都有一个叫作

Arena Node (Script) 的脚本，分

别如图 17.8 和图

17.9

所示，这个基

本概念可以用来帮助理

解 Arena 游戏中定义的社会关

系。关于 Arena Node

的

描述将在本小

节中提供。

图 17.8 存在于 GlobalManager

中的

Arena Node (Script)

图 17.9 存在于

BasicAgent 中的 Arena Node (Script)

•

我们选择

之前构建的 BasicAgent 并将它在左

边“Hierarchy”窗口中通过 Ctrl+C 和 Ctrl+V

复制，如

图 17.10 所示。现在 Global Manager 之下有两个

Arena

Nodes，因此我们需要将

两个 BasicAgents 中

的任一个设置 Node ID

为 1 而非 0 来

辨别它们（见图 17.11）。智能体

416

17.2 用

Arena 开发游戏

图 17.10

在 GlobalManager 下复制 BasicAgent

在

场景中的位置可以被移

动到一个合适的位置，从

而将它们区分开，这是因

为复制后的两

个智能体

会有相同的位置。

图 17.11 当 GlobalManager 下

有多个节点时需要改变

节点 ID，使得它们各不相同

•

下面我们选择 GlobalManager 、Arena Node (Script) 来设置游

戏的奖励函数，如图

17.12

所示

。我们单击 Is Reward Ranking，这是一个在 GlobalManager

下

用于设置智能体竞争

性

奖励函数的属性。我们也

将 Ranking Win Type 选择为

Survive，这意味着最后

结束生命

（存活到最后）的

智能体会得到一个正的

奖励。如果你选择 Depart，奖励将

会给首先结束生

命的智

能体。我们也需要取消 Is Reward

Distance（这

是根据智能体到目标的

距离给出密集

奖励的函

数）。上面是 Arena 中内置的不同

奖励机制，通过或竞争或

合作（有时二者都有）

的形

式。不同的游戏中会有不

同的奖励设置来表示不

同的社会关系结构。你可

以对不同的

游戏使用不

同的奖励设置。举例来说

，如果你想用智能体到目

标的距离来解决一个类

似到

达目标类的任务，那

么可以设置一个基于距

离的密集奖励来实现，通

过单击勾选 Is Reward

Distance，以及将一个

目标物体拖到 Target 空格中来

设置。

• 我们需要在 GlobalManager 的 Living Condition

Based On Child Nodes 下设

置 At

Least

Specific Number Living 为 2，如图

17.13 所示，从而只有

当至少两个智能体存活

时，游戏

才会继续；否则游

戏将终止并重新开始。现

在我们单击 Play 按钮，游戏应

该正常运行，只

要一个智

能体结束了生命，游戏就

会结束，而且奖励或惩罚

就会施加给智能体，如

Console

中

所显示的奖惩记录，见图

17.14。

• 下面我们会使得游戏更

加复杂，我们想要两队各

自有两个智能体来互相

竞争。首先，我们

在“Hierarchy”窗口创

建一个空的对象并将其

命名为“2 Player

Team”。随后我们将 Arena

Node 脚本

附加到它上面，如图 17.15 所示

。

417

第 17 章 Arena：多智能体强化学习

平台

图

17.12 在 GlobalManager 下设置奖励函

数

图 17.13

在 GlobalManager 中设置最小存活

智能体数量

图 17.14 给每个智

能体的奖励显示在

Console 中

418

17.2 用

Arena 开发游戏

图 17.15 将 Arena Node 脚本附加

到队伍（Team）对象上

• 现在我们

将两个之前的 BasicAgent 拖到新创

建的队伍对象 2 Player

Team 中。随后我

们复

制 2 Player Team，将第二组对象的

Node

ID 从 0 改为 1。现在我们有队伍

和智能体的结构

如图

17.16 所

示。如果我们现在单击 Play 按

钮，我们将会看到两个各

有两个智能体的队伍

在

场景中，如图 17.17

所示。

图 17.16 Arena 的 GlobalManager

下

两个队伍（Teams）各有两个智能

体（Agents）的

层次性结构

• 由于 GlobalManager 的

At

Least Specific Number Living 被设为 2，任何队伍生命的

结束都

会造成游戏结束

。由于 2 Player Team 的 At

Least Specific Number Living 默认为 1，只有

419

第 17 章

Arena：多智能体强化学习平台

当同一个队伍中两个智

能体都结束生命时才会

造成队伍的生命结束。我

们也可以设置不同

的游

戏逻辑，如果我们设置 2

Player Team 的

Living Condition 为 All

Living，那么一个队

伍中任何

智能体结束生命都会导

致队伍的生命结束，从而

结束整个游戏。从以上来

看，通

过 GlobalManager->Team->Agent 的社交树结构，Arena 基

本可以通过定义生存和

奖励机

制，使用 Arena Node 支持任意

类型的社会关系。

图 17.17

两个

各有两个智能体的队伍

在游戏中

17.2.3 高级设置

奖励

机制

为了构建复杂的社

会关系，在 Arena

中有 5 个基本的

多智能体奖励机制（Basic Multi-Agent

Reward Schemes，BMaRSs）来定

义社交树上每个节点的

不同社交范式，包括：不可

学习的（NonLearnable，NL）、独立的（Isolated，IS）、竞争的（Competitive，CP）、合

作的（Collaborative，CL）、

竞争和合作混合型

的（Competitive and Collaborative Mixed，CC）。具体来说，每个 BMaRS 是一个

对奖励函数的限制，因此

它与能产生某种具体社

交范式的一批奖励函数

相关。对于每个

BMaRS，

Arena 提供了多

个可以立即使用的奖励

函数（稀疏或密集），简化了

有复杂社会关系的游戏

构建过

程。除提供奖励函

数外，Arena 也提供了对定制化

奖励函数的验证选项，从

而可以将编写的奖励

函

数置于一种

BMaRS 下而产生相

应的具体社交范式。我们

将详细讨论这五种不同

的奖励机制。

首先我们需

要给出一些预备知识。我

们考虑基本强化学习中

定义的一个马尔可夫（Markov）

游

戏，它包括多个智能体 x ∈

X、一

个有限的全局状态空间

st ∈ S、一个对每个智能体 x 的有

限

的动作空间

ax,t ∈ Ax 和一个对

每个智能体 x 的有限步奖

励空间

rx,t ∈ R。至于环境，它包括

一

个转移函数 g :

S × 

Ax : x

∈ X 	

→ S，这是一个

有随机性的（由于

Unity 模拟器

的随机性）函数

420

17.2 用 Arena

开发游

戏

st+1 ∼ g



st+1|(st,

{ax,t : x ∈ X )}



和一个对每个智能体

的奖励函数 fx : S ×



Ax : x ∈ X

→ R。

这是一个确

定性函数 rx,t+1 =

fx



st, {ax,t : x

∈ X }

，以及一个联合

奖励函数 f =

{fx : x ∈ X }

和

对每个智能

体 x 在联合奖励函数 f 下的

片段奖励

Rf

x =

PT

t=1 rx,t。对于智能体来

说，Arena

考虑以

下情况，即它观

察 sx,t ∈ Sx，其中 Sx

包括全局状态空

间 S 的部分信息。因此，策略

πx : Sx →

Ax

是一个随机性函数 ax,t ∼ πx(sx,t)。除此

之外，Arena 考虑智能体

x 能够从

一个策略集合 Πx 中采

取一

个策略 πx。Arena

假定所有采样操

作的随机种子是 k，这是从

整个种子空间 K 中采样得

到的。

不同的 BMaRSs

定义使用的

基本概念包括，智能体 {x : x ∈ X

}、策

略 {πx : Πx}、智能体

奖励 {Rf

x

: x ∈ X }

和联合奖

励函数 F = {f : ·}，其中智能体总体

为

X。Arena 中五种不同的

BMaRSs 通过以

下方式定义：

1. 不可学习的

BMaRSs（F

NL）是一个联合奖励函数集

合 f，如下：

F

NL =



f : ∀k ∈ K, ∀x

∈ X , ∀πx ∈ Πx,

∂Rf

x/∂πx = 0

, (17.1)

其中 0 是与定义 πx

的

参数空间同样大小和形

状的零矩阵。直观上，F

NL 意味

着对于任何智能体

x ∈ X

改进

其策略 πx 都是无法优化 Rf

x 的

。

2. 独立的 BMaRSs（F

IS）是一个联合奖励

函数的集合如下：

F

IS

=



f : f ∈

F/

NL and ∀k ∈ K,

∀x ∈ X , ∀x

′

∈ X \ {x}, ∀πx ∈

Πx, ∀πx′ ∈ Πx′ ,

∂Rf

x

∂πx′

= 0

,

(17.2)

其中“\”是

集合的差。直观上，F

IS 意味着

智能体 x

∈ X 接受的片段内奖

励 Rf

x 与任何其他智

能体 x

′ ∈ X \

{x} 采

取的策略 πx′ 无关。F

IS 的

f 中的奖

励函数 fx 在其他多智能体

方法 (Bansal

et

al., 2018; Hendtlass, 2004; Jaderberg et

al., 2018) 中通常被称为内部

奖励函数（Internal Reward

Functions），意味着除了施

加到群体层面的奖励函

数（比如赢输），还有指引学

习过程去取得群

体层面

奖励的奖励函数。群体层

面奖励可能很稀疏而难

以学习，但这些内部奖励

可以更频繁地获

取，即更

加密集 (Heess et al., 2017; Singh

et al., 2009, 2010) 。F

IS

在比如当智能体

是一个机器人需

要连续

控制施加在关节上的力

的时候变得更加切实可

行，这意味着基本的动作

技巧（比如运动）

需要在生

成群体智能前被学习到

。因此，Arena 在 F

IS

中提供了 f 来应对

：能量损耗、施加较大

力的

惩罚、保持稳定速度的激

励和朝向目标的移动距

离等。

3. 竞争的

BMaRSs（F

CP）是受文献 (Cai et al., 2011)

启

发的方式，定义为

F

CP =



f

: f ∈ F/

NL ∪

FIS and ∀k ∈ K, ∀x

∈ X , ∀πx ∈ Πx,

∀πx′ ∈ Πx′ ,

∂

R

x′∈X R

f

x′dx

′

∂πx

= 0



, (17.3)

4

第 17 章 Arena：多

智能体强化学习平台

上

式直观上意味着对于任

何智能体 x

∈ X，采用任何可能

的策略 πx ∈ Πx，所有智能体在片

段内

奖励的求和是不变

的。如果片段长度为

1，它表

示典型的多玩家零和游

戏 (Cai et al., 2011)。

关于

F

CP 中 f 的有用例子为

（1）智能体需要为有限的资

源斗争，而这些资源在片

段结束后

通常会耗尽，智

能体会为它所得到的资

源受到奖励；（2）斗争一直到

结束生命，奖励根据生命

结束的顺序来给出（奖励

也可以基于相反的顺序

，从而离开游戏的一方首

先接受最高的奖励，比

如

在一些扑克游戏中，首先

打出所有牌的一方获胜

）。标准形式（Normal-Form）游戏 (Myerson,

2013) 中的剪刀

石头布（Rock, Paper, and

Scissors）和 (Balduzzi et al., 2019) 中循环游戏（Cyclic

Game）都

是 F

CP 的特殊情况。

4. 合作的

BMaRSs（F

CL）是

由文献 (Cai et al., 2011)

启发的方式，定义

为

F

CL =



f

: f ∈ F/

NL ∪

FIS and ∀k ∈ K, ∀x

∈ X , ∀x

′ ∈

X \ {x}, ∀πx ∈ Πx,

∀πx′ ∈ Πx′ ,

∂Rf

x′

∂Rf

x

⩾ 0



,

(17.4)

上式直观上意味着对

任何一对智能体 (x

′

, x)

都没有

利益冲突（∂Rf

x′/∂Rf

x < 0）。除此之外，由于

f ∈

F/

NL ∪ FIS，至少有一对智能体 (x, x′

) 使得

∂Rf

x′/∂Rf

x > 0。该式意味着这对智能体

有共同利

益，从而对智能

体 x 其 Rf

x 的提高也会造成智

能体

x

′ 的 R

f

x′

提高。最常见的关

于 F

CL 中 f 的例

子是对于所有

x ∈ X 的 fx 都是相等的，比如，一个

物体的移动距离可以由

多个智能体的共同努

力

来推动，或者一个群体的

存活时长（只要群体内有

一个个体是存活的，群体

就是存活的）。因

此，Arena 在 F

CL 中提

供了

f 来应对：队伍存活时

间（正值或负值，因为一些

游戏需要队伍尽可

能久

地存活，而其他一些游戏

需要队伍尽可能早地消

失，比如扑克中的纸牌）等

。

5. 竞争和合作混合型的 BMaRSs（F

CC）定

义为任何以上四种之外

的情况。

F

CC =



f

: f ∈ F/

NL ∪

FIS ∪ FCP ∪ F CL

, (17.5)

首先，式 (17.3) 中的 ∂

R

x′∈X R

f

x′dx

′/∂πx

= 0 可以

写为 R

x′∈X ∂Rf

x′/∂Rf

xdx

′ = 0（证明在这里不提

供

，可以参考原文），这是式

(17.3) 的

另一种表示。考虑式 (17.3) 中的

F

CP 和式

(17.5) 中的 F

CL，

对 F

CC 的一个直观

的解释是，存在 ∂Rf

x′/∂Rf

x <

0 的情形，即

智能体在这时是竞争的

。但是对整

体利益的导数

R

x′∈X ∂Rf

x′/∂Rf

xdx

′ 不总是为 0。因此，整体利益

可以用具体的策略来最

大化，即

智能体在这时是

合作的。

除了在每个

BMaRS 提供

了几个实际的 f，Arena 也对每个

BMaRS 提供了一个验证选项，即

可定制 f

并使用这个验证

选项来确保编写的 f 属于

一个具体的 BMaRS。

上面的内容

提供了关于如何使用不

同类别奖励函数来定义

社会关系的理论。此外，奖

励函

数应当根据上面定

义的类别来实现预期的

群体中的社会关系。实践

中，奖励函数有一些具体

形

式，如我们在之前小节

中提到的。Arena 框架通常在 GlobalManager 的

Arena Node 中定义了

422

17.2 用 Arena 开发游戏

Collaborative

和

Competitive 的奖励函数，而 Isolated 奖励函数

定义在像 BasicAgent 的智能体的

Arena Node 中

。

这里是一个便于理解社

会树关系的例子，这个树

中每个 Arena Node

使用了不同的 BMaRs，

如

图 17.181 所示。奖励机制被指定

到各个 Arena

Node 来定义它的子节

点的社会关系。图 17.18(a)

中的图

形用户界面（Graphical User Interface，GUI）定义了图

17.18(b) 中

的树结构，用来表示一

个

有四个智能体的群体。这

个树结构可以通过在图

17.18(a) 的 GUI 中拖曳、复制或删除来

进行简

单设置。在这个例

子中，每个智能体有个体

层面的 BMaRS。智能体是一个机

器蚂蚁，而其个体

级别的

BMaRSs 是 F

IS，具体来说，ant-motion

的选项使得

学习朝向基本的运动技

巧，比如向前

移等进行，如

图 17.18(c) 所示。每两个智能体构

成一个队伍（一个智能体

或队伍的集合），而这

两个

智能体有队伍层面的 BMaRSs。在

这个例子中，两个机器蚂

蚁互相合作来推动盒子

前进，如

图 17.18(d) 所示。因此，队伍

层面的 BMaRSs 是 F

CL，具体来说，是推

动盒子的距离。在两个队

伍之上，Arena 有全局的 BMaRSs。在这个

例子中，两个队伍被设定

为有一场关于哪个队伍

先将

盒子推向目标点的

竞赛，如图 17.18(e) 所示。因此，全局

的

BMaRSs 是 F

CP，具体来说，是将盒

子

推到目标的先后次序。应

用到每个智能体的最终

奖励函数是以上三个层

次的 BMaRSs

的加权

求和。我们也

可以想象如何来定义一

个超过三个层次的社会

树，其中小的队伍组成大

的队伍，在

每个节点定义

的 BMaRSs 给出更加复杂和结构

化的社会关系。在定义了

社会树并在每个节点使

用了 BMaRSs

之后，环境便可以使

用了。其抽象层可以解决

其他问题，比如，为窗口中

的每个智

能体分配视图

、添加队伍颜色、展示智能

体 ID 并生成一个从上到下

的视野等。

Team-level

(b)

(c) (d) (e)

Agent-level

Global-level

(a)

图 17.18 在 Arena 对每个 Arena

Node 使

用不同 BMaRs 定义的社会树（见

彩插）

此外，我们可以简单

拓展上述框架到其他常

见社会关系，如图 17.192

所示。

更

多预制智能体

除了之前

的 BasicAgent，Arena 也有其他更高级的预

制智能体可以直接使用

，如图 17.20

所

示。其他智能体的

使用基本与 BasicAgent 类似，通过拖

曳并将它附于 GlobalManager 之下。唯

一

的不同在于动作空间，你

需要改变相应的控制大

脑（Brain）来控制不同的智能体

。举例来说，

1图片来源：Song, Yuhang, et al. "Arena:

A General Evaluation Platform and Building

Toolkit for Multi-Agent Intelligence."

arXiv preprint

arXiv:1905.08085 (2019)。

2图片

来源：Song, Yuhang, et al.

"Arena: A General Evaluation Platform and

Building Toolkit for Multi-Agent Intelligence."

arXiv

preprint arXiv:1905.08085 (2019)。

423

第 17

章 Arena：多智能体强化

学习平台

图 17.19 Arena 框架下定义

的常见社会关系

图 17.20 Arena 中不

同的预制智能体

图 17.21

场景

中的 ArenaCrawlerAgent（见彩插）

预制智能体

中的 ArenaCrawlerAgent 如图 17.21

所示，它有连续

的动作空间来控制关节

的动

作值。为了恰当地使

用这个智能体，我们需要

改变 ArenaCrawlerAgent 大脑为图 17.22 所示的

ArenaCrawlerPlayerContinuous (PlayerBrain)。随

后这个游戏可以导出并

用作一般的游戏来使用

。

17.2.4 导出二进制游戏

当你在

Unity 中的玩家模式下测试了

游戏之后，确保游戏设置

没有任何问题，就可以将

游戏

导出为一个独立的

二进制文件，并用它与 Python 脚

本训练 MARL 算法。这一小节展

示了如何导

424

17.2 用 Arena 开发游戏

出游戏。

• 首先，我们需要将

大脑的类型从

PlayerBrain 改为一个

相应的 LearningBrain（同样类型），

PlayerBrain 被用于

通过用户键盘操作来控

制游戏智能体，而 LearningBrain

可以用

学习算

法来直接控制。如

图 17.23 所示，对于这个游戏，我

们改变 GeneralPlayerDiscrete (PlayerBrain)

为图 17.24 中的 GeneralLearnerDiscrete (LearningBrain)。我们也

需要取消勾选 Debugging

来减少训

练中的输出信息。

图 17.22 为 ArenaCrawlerAgent 更

改大脑

图 17.23 玩家模式下原

先的控制大脑类型

图 17.24 更

改控制大脑类型为

LearningBrain 来导

出训练游戏

425

第 17 章

Arena：多智能

体强化学习平台

• 为了导

出游戏，我们选择 File->Build Settings，相应得

到一个如图 17.25

所示的窗口

。通过

这个窗口，我们可以

设置 Target Platform 和 Architecture。

• 我们也需要单击

Player Settings 来检查其他设置，如图 17.26 所

示。一个需要注意的点是

：

Display Resolution Dialog 需要设为 Disabled 来正常工作。随

后我们回到之前的窗口

并单击

Build，这样就可以在创

建游戏之后得到其二进

制文件。

图 17.25 检查创建游戏

的设置

图 17.26

设置游戏导出

的窗口

426

17.3 MARL 训练

17.3

MARL 训练

有了用

Arena 构建并导出的独立（Standalone）游戏

，我们可以设置训练过程

来研究多智能

体强化学

习（Multi-Agent Reinforcement

Learning，MARL）中的各种问题。

在开始

训练之前，我们需要先配

置系统。由于 MARL 一般需要大

量的计算，我们通常需要

用一个服务器来应对训

练过程。Arena 环境的基本设置

遵循 17.1

节中的内容。如果你

在服务器上

不能正常使

用 X-Server，那么可以遵循以下部

分内容来设置虚拟显示

，否则可以直接跳过该部

分

到训练小节。

17.3.1 设置

X-Server

使用

虚拟显示的基本设置如

下：

# 安装 Xorg

sudo

apt-get update

sudo apt-get install -y

xserver-xorg mesa-utils

sudo nvidia-xconfig -a --use-display-device=None

--virtual=1280x1024

# 获得 BusID 信息

nvidia-xconfig

--query-gpu-info

# 添加 BusID 信

息到你的/etc/X11/xorg.conf 文件

sudo sed -i ’s/ BoardName "GeForce

GTX TITAN X"/ BoardName "GeForce GTX

TITAN X"\n

BusID "0:30:0"/g’ /etc/X11/xorg.conf

#

从/etc/X11/xorg.conf 文件中

移除小节"Files"

# 并且移除包含

小节"Files" 和 EndSection

的两行

sudo vim /etc/X11/xorg.conf

# 为

Ubuntu 下载和

安装最新的 Nvidia 驱动器

wget

http://download.nvidia.com/XFree86/Linux-x86_64/390.87/NVIDIA-Linux-x86_64-390.87.run

sudo /bin/bash ./NVIDIA-Linux-x86_64-390.87.run --accept-license --no-questions

--ui=none

# 禁用

Nouveau，因为它会使 Nvidia 驱动器崩溃

sudo echo ’blacklist

nouveau’ | sudo tee -a /etc/modprobe.d/blacklist.conf

sudo echo ’options nouveau modeset=0’ |

sudo tee -a /etc/modprobe.d/blacklist.conf

427

第

17 章 Arena：多智能体强化学习

平台

sudo echo options

nouveau modeset=0 | sudo tee -a

/etc/modprobe.d/nouveau-kms.conf

sudo update-initramfs -u

sudo reboot

now

用以下三种方式（不

同的方式可能在不同的

Linux 版本上运行）之一关闭 Xorg：

# 方

式

1：运行以下命令并运行

这个命令的输出

ps aux | grep -ie

Xorg | awk ’{print "sudo kill

-9 "



2}’

# 方式 2：运行以下命令

sudo killall

Xorg

# 方式

3：运行以下命令

sudo init 3

用该命令

开启虚拟显示：

sudo ls

sudo /usr/bin/X :0

&

你应当可

以看到虚拟显示正常启

动，并输出以下内容：

X.Org X Server 1.19.5

Release Date: 2017-10-12

X Protocol Version

11, Revision 0

Build Operating System:

Linux 4.4.0-97-generic x86_64 Ubuntu

Current Operating

System: Linux W5 4.13.0-46-generic 51-Ubuntu SMP

Tue Jun 12 12:36:29

UTC 2018

x86_64

Kernel command line: BOOT_IMAGE=/boot/vmlinuz-4.13.0-46-generic.efi.signed

root=UUID=5fdb5e18-f8ee-4762-a53b-e58d2b663df1

ro quiet splash nomodeset acpi=noirq

thermal.off=1

vt.handoff=7

Build Date: 15 October 2017

05:51:19PM

xorg-server 2:1.19.5-0ubuntu2 (For technical support

please see

http://www.ubuntu.com/support)

Current version of

pixman: 0.34.0

Before reporting problems, check

http://wiki.x.org

to make sure that you

have the latest version.

Markers: (--)

probed, (**) from config file, (==)

default setting,

(++) from command line,

(!!) notice, (II) informational,

(WW) warning,

(EE) error, (NI) not implemented, (??)

unknown.

428

17.3 MARL 训练

(==) Log

file: "/var/log/Xorg.0.log", Time: Fri Jun 14

01:18:40 2019

(==) Using config file:

"/etc/X11/xorg.conf"

(==) Using system config directory

"/usr/share/X11/xorg.conf.d"

如果你看到报错，回到“用

以下三种方式之一关闭

Xorg”并尝试用另一种方法。

在

新窗口中运行“Arena-Baselines”之前，运行

以下命令来将一个虚拟

显示端口附于窗口：

export DISPLAY=:0

17.3.2

进行

训练

创建 TMUX 会话（如果你用

的机器是一个可以用 SSH 连

接的服务器）并进入虚拟

环境：

tmux new-session -s Arena

source activate

Arena

连续动作空间

Arena 中连

续动作空间的游戏列表

：

* ArenaCrawler-Example-v2-Continuous

* ArenaCrawlerMove-2T1P-v1-Continuous

* ArenaCrawlerRush-2T1P-v1-Continuous

* ArenaCrawlerPush-2T1P-v1-Continuous

* ArenaWalkerMove-2T1P-v1-Continuous

* Crossroads-2T1P-v1-Continuous

* Crossroads-2T2P-v1-Continuous

* ArenaCrawlerPush-2T2P-v1-Continuous

* RunToGoal-2T1P-v1-Continuous

* Sumo-2T1P-v1-Continuous

* YouShallNotPass-Dense-2T1P-v1-Continuous

运行训练命令，将 GAME_NAME 用上面

的游戏名替换并根据你

所用计算机选择合适的

num-processes（num-mini-batch 需等于

num-processes）：

tmux new-session -s Arena

CUDA_VISIBLE_DEVICES=0

python main.py --mode train --env-name GAME_NAME

--obs-type

visual --num-frame-stack 4 --recurrent-brain --normalize-obs

--trainer ppo

--use-gae --lr 3e-4 --value-loss-coef

0.5 --ppo-epoch 10 --num-processes 16

429

第 17 章 Arena：多智能体强

化学习平台

--num-steps 2048

--num-mini-batch 16 --use-linear-lr-decay --entropy-coef 0 --gamma

0.995 --tau 0.95 --num-env-steps 100000000 --reload-playing-agents-principle

OpenAIFive --vis --vis-interval 1 --log-interval 1

--num-eval-episodes 10

--arena-start-index 31969 --aux 0

离散动作空

间

Arena 中离散动作空间游戏

列表：

* Crossroads-2T1P-v1-Discrete

*

FighterNoTurn-2T1P-v1-Discrete

* FighterFull-2T1P-v1-Discrete

* Soccer-2T1P-v1-Discrete

*

BlowBlow-2T1P-v1-Discrete

* Boomer-2T1P-v1-Discrete

* Gunner-2T1P-v1-Discrete

*

Maze2x2Gunner-2T1P-v1-Discrete

* Maze3x3Gunner-2T1P-v1-Discrete

* Maze3x3Gunner-PenalizeTie-2T1P-v1-Discrete

*

Barrier4x4Gunner-2T1P-v1-Discrete

* Soccer-2T2P-v1-Discrete

* BlowBlow-2T2P-v1-Discrete

*

BlowBlow-Dense-2T2P-v1-Discrete

* Tennis-2T1P-v1-Discrete

* Tank-FP-2T1P-v1-Discrete

*

BlowBlow-Dense-2T1P-v1-Discrete

运行训练命令，将 GAME_NAME 用

上面的游戏名替换并根

据你所用计算机选择合

适的

num-processes（num-mini-batch 需等于

num-processes）：

CUDA_VISIBLE_DEVICES=0 python main.py --mode train

--env-name GAME_NAME --obs-type

visual --num-frame-stack 4

--recurrent-brain --normalize-obs --trainer ppo

--use-gae --lr

2.5e-4 --value-loss-coef 0.5 --ppo-epoch 4 --num-processes

16

--num-steps 1024 --num-mini-batch 16 --use-linear-lr-decay

--entropy-coef 0.01

--clip-param 0.1 --num-env-steps 100000000

--reload-playing-agents-principle

OpenAIFive --vis --vis-interval 1 --log-interval

1 --num-eval-episodes 10

--arena-start-index 31569 --aux

0

430

参考文献

你

也可以改用其他 MARL 算法来

替代上面的

PPO 去测试你所

创建的游戏。

17.3.3 可视化

为了

用 Tensorboard

可视化分析训练过程

的学习曲线，运行：

source activate Arena && tensorboard

--logdir ../results/ --port 8888

并访问

相应端口打开 Tensorboard

可视化。

17.3.4 致

谢

我们特别感谢 Yuhang Song、教授

Zhenghua Xu、教

授 Thomas Lukasiewicz 等人对 Arena

项目

的巨大贡

献。

参考文献

BALDUZZI D, GARNELO

M, BACHRACH Y, et al., 2019.

Open-ended learning in symmetric zero-sum

games[J].

arXiv:1901.08106.

BANSAL T, PACHOCKI J, SIDOR

S, et al., 2018. Emergent complexity

via multi-agent competition[C]//

International Conference on

Learning Representations.

CAI Y, DASKALAKIS C,

2011. On minmax theorems for multiplayer

games[C]//Proceedings of the

twenty-second annual ACM-SIAM

symposium on Discrete Algorithms. Society for

Industrial and

Applied Mathematics.

HEESS N,

SRIRAM S, LEMMON J, et al.,

2017. Emergence of locomotion behaviours in

rich environments[J]. arXiv:1707.02286.

HENDTLASS T, 2004.

An introduction to collective intelligence[M]//Applied Intelligent

Systems.

JADERBERG M, CZARNECKI W M,

DUNNING I, et al., 2018. Human-level

performance in first-person

multiplayer games with

population-based deep reinforcement learning[C]//CoRR.

MYERSON R

B, 2013. Game theory[M]. Harvard university

press.

SINGH S, LEWIS R L,

BARTO A G, 2009. Where do

rewards come from[C]//Proceedings of the annual

conference of the cognitive science society.

431

第 17 章 Arena：多智能

体强化学习平台

SINGH

S, LEWIS R L, BARTO A

G, et al., 2010. Intrinsically motivated

reinforcement learning: An

evolutionary perspective[J]. IEEE

Transactions on Autonomous Mental Development.

SONG

Y, WANG J, LUKASIEWICZ T, et

al., 2019. Arena: A general evaluation

platform and building

toolkit for multi-agent

intelligence[J]. arXiv preprint arXiv:1905.08085.

432

18

深度强

化学习应用实践技巧

之

前的章节向读者展现了

深度强化学习的主要知

识点、强化学习算法的主

要类别和算法实

现，以及

为了便于理解深度强化

学习应用而讲解的几个

实践项目。然而，由于如之

前强化学习的

挑战一章

中提到的低样本效率、不

稳定性等问题，初学者想

要较好地部署这些算法

到自己的应用

中还是有

一定困难的。因此，在这一

章，从数学分析和实践经

验的角度，我们细致地总

结了一些

在深度强化学

习应用实践中常用的技

巧和方法。这些方法或小

窍门涉及了算法实现阶

段和训练调

试阶段，用来

帮助读者避免陷入一些

实践上的困境。这些经验

上的技巧有时可以产生

显著效果，

但不总是这样

。这是由于深度强化学习

模型的复杂性和敏感性

造成的，而有时需要同时

使用多个

技巧。如果在某

一个项目上卡住时，大家

也可以从这一章中得到

一些解决方案上的启发

。

18.1 概览：如何应用深度强化

学习

深度学习通常被认

为是“黑盒”方法。尽管它实

际上并不是“黑盒”，但是它

有时会表现得

不稳定且

会产生不可预测的结果

。在深度强化学习中，由于

强化学习的基本过程需

要智能体从与

环境交互

的动态过程中的奖励信

号而不是标签中学习，这

个问题变得更加严重。这

是与有监督学

习的情况

不同的。强化学习中的奖

励函数可能只包含不完

整或者局部的信息，而智

能体使用自举

（Bootstrapping）学习方法

时往往在追逐一个变化

的目标。此外，深度强化学

习中经常用到不止一

个

深度神经网络，尤其是在

那些较为高等或者最近

提出的方法中。这都使得

深度强化学习算法可

能

表现得不稳定且对超参

数敏感。以上问题使得深

度强化学习的研究和应

用困难重重。由于这个

原

因，我们在这里介绍一些

实现深度强化学习中常

用的技巧和建议。

首先，你

需要知道一个强化学习

算法是否可以用于解决

某一个特定的问题，而且

显然不是每

个算法都对

所有任务适用。我们经常

需要仔细考虑强化学习

本身是否可以用于解决

某个任务。总

433

第 18

章 深度强

化学习应用实践技巧

体

来说，强化学习可以用于

连续决策制定问题，而这

类问题通常可以用马尔

可夫（Markov）过程

来描述或近似

。一个有标签数据的预测

任务通常不需要强化学

习算法，而监督学习方法

可能更直

接和有效。强化

学习任务通常包括至少

两个关键要素：（1）环境，用来

提供动态过程和奖励信

号；（2）智能体，由一个策略控

制，而这个策略是通过强

化学习训练得到的。在之

前的几个章节

中强化学

习算法被用来解决像

OpenAI Gym 这

类环境中的任务。在这些

实验中，你不需要过多关

心环境，因为它们已经被

设计好且经过标准化和

正则化。然而，在应用章节

中介绍的几个项目则

需

要人为定义环境，并运用

强化学习算法去使智能

体正常工作。

总的来说，应

用深度强化学习算法有

以下几个阶段。

1.

简单测试

阶段：你需要使用对其正

确性和准确性有高置信

度的模型，包括强化学习

算法，

如果是一个新的任

务，用它来探索环境（甚至

使用一个随机策略）或者

逐步验证你将在最终模

型

上做的延伸，而不是直

接使用一个复杂的模型

。你需要快速进行实验来

检测环境和模型基本设

置

中可能的问题，或者至

少让你自己熟悉这个要

解决的任务，这会给你在

之后的过程中提供一些

启

发，有时也会暴露出一

些需要考虑的极端情况

。

2.

快速配置阶段：你应该对

模型设置做快速测试，来

评估其成功的可能性。如

果有错误，尽

可能多地可

视化学习过程，并在你无

法直接从数字上得到潜

在关系的时候使用一些

统计变量（方

差、均值、平均

差值、极大极小值等）。这一

步应当在简单测试阶段

后开始，然后逐步增加新

模

型的复杂度。如果你无

法百分之百确定更改的

有效性，你应当每一次都

进行测试。

3. 部署训练阶段

：在你仔细确认过模型的

正确性后，你可以开始大

规模部署训练了。由于深

度强化学习往往需要较

大量的样本去训练较长

时间，我们鼓励你使用并

行训练方式、使用云服务

器（如果你自己没有服务

器的话）等，来加速对最终

模型的大规模训练。有时

这一阶段是和第二

阶段

交替进行的，因而这一步

在实践中可能会花费较

长时间。

在下面几个小节

中，我们将分几部分介绍

应用深度强化学习的技

巧。

18.2 实现阶段

• 从头实现一

些基本的强化学习算法

。对于深度强化学习领域

的初学者而言，从头实现

一些基

本的强化学习算

法并调试这些算法直到

它们最终正确运行，是很

好的练习。Deep Q-Networks

作为一种基于

价值函数的算法，是值得

去自己实现的。连续动作

空间、策略梯度和 ActorCritic 算法也

是刚开始学习强化学习

算法实现时很好的选择

。这个过程会需要你理解

强化学

习算法实现中的

每一行代码，给你一个强

化学习过程的整体感觉

。刚开始，你不需要一个

复

杂的大规模任务，而是一

个相对简单的可以快速

验证的任务，比如那些 OpenAI Gym 环

境。在实现这些基本算法

的时候，你应当基于一种

公用的结构并且使用一

种深度学习框架

（比如 TensorFlow、PyTorch

等

），并逐步扩展到更加复杂

的任务上，同时使用更加

高级的技

术（比如优先经

验回放等）。这会显著地加

速你随后将不同的深度

强化学习算法应用于其

他

项目的进程。如果你在

实现过程中遇到一些问

题，你可以参考其他人的

实现方法（比如本

434

18.2 实现阶

段

书提供的强化学习算

法实现指南）或者通过网

络查找你遇到的问题。绝

大多数问题都已经

被他

人所解决。

• 适当地实现论

文细节。在你熟悉了这些

基本的强化学习算法后

，就可以开始实现和测试

一

些在文献中的方法。通

常强化学习算法的研究

论文中包含很多实现细

节，而有时这些细节

在不

同论文中不是一致的。所

以，当你实现这些方法的

时候，不要过拟合到论文

细节上，而

是去理解论文

作者为何在这些特定情

形下选择使用这些技巧

。举一个典型的例子，在多

数

文章中，实验部分中神

经网络的结构细节包括

隐藏层的维数和层数、各

个超参数的数值等。

这些

都或在论文主体、或在补

充材料中提到。你不需要

在自己的实现版本中严

格遵循这些

实现细节，而

且你很可能甚至跟原文

用不一样的环境来对方

法进行测试。比如，在深度

决

定性策略梯度（Deep Deterministic

Policy Gradient）算法的

论文中，作者建议使用 OrnsteinUhlenbeck（OU）噪

声来进行探索。然而，实践

中，有时很难说 OU 噪声是否

比高斯噪声更

好，而这往

往在很大程度上依赖于

具体任务。另一个例子是

，在

Vinyals et al. (2019) 关于

AlphaStar

的工作中，Vanilla TD(λ) 方法被

证实比其他更高级的离

线策略（Off-Policy）修正

方法 V-Trace (Espeholt

et al., 2018) 更有效

。因此，如果这些技巧不足

够通用，那么它们可

能不

值得你花精力去实现。相

比而言，一些微调方法可

能对具体任务有更好的

效果。然而，

如上所述，理解

作者在这些情况下为何

使用这些技巧则是更关

键和有意义的。当你采取

论

文中的某些想法并将

其应用到你自己的方法

中时，这些建议可能更有

意义，因为有时对于

你自

己的具体情况，可能不是

论文中的主要想法，而是

某些具体技巧或操作对

你帮助最大。

• 如果你在解

决一个具体任务，先探索

一下环境。你应当检查一

下环境的细节，包括观察

量

和动作的性质，如维度

、值域、连续或离散值类型

等。如果环境观察量的值

在一个很大的

有效范围

内或者是未知范围的，你

就应该把它的值归一化

。比如，如果你使用

Tanh 或者

Sigmoid 作

为激活函数，较大的输入

值将可能使第一个隐藏

层的节点饱和，而这训练

开始时

将导致较小的梯

度值和较慢的学习速度

。此外，你应当为强化学习

选择好的输入特征，这

些

特征应当包含环境的有

用信息。你也可以用能进

行随机动作选取的智能

体来探索环境并

可视化

这个过程，以找到一些极

端情况。如果环境是你自

己搭建的，这一步可能很

重要。

• 给每一个网络选取

一个合适的输出激活函

数。你应当根据环境来对

动作网络选择一个合适

的输出激活函数。比如，常

用的像 ReLU 可能从计算时间

和收敛表现上都对隐藏

层来说可以

很好地工作

，但是它对有负值的动作

输出范围来说可能是不

合适的。最好将策略输出

值的

范围跟环境的动作

值域匹配起来，比如对于

动作值域 (−1, 1) 在输出层使用

Tanh 激活函数。

•

从简单例子开

始逐渐增加复杂度。你应

当从比较清晰的模型或

环境开始测试，然后逐步

增

加新的部分，而不是一

次将所有的模块组合起

来测试和调试。在实现过

程中不断进行测试，

除非

你是这个领域的专家并

且很幸运，否则你不应当

期望一个复杂的模型可

以一次实现成

功并得到

很好的结果。

• 从密集奖励

函数开始。奖励函数的设

计可以影响学习过程中

优化问题的凸性，因此你

应当

从一个平滑的密集

奖励函数开始尝试。比如

，在第 16 章中定义的机器人

抓取任务中，我们

435

第 18

章 深

度强化学习应用实践技

巧

用一个密集奖励函数

来开始机器人学习，这个

函数是从机器人夹具到

目标物体之间距离的

负

数。这可以保证值函数网

络和策略网络能够在一

个较为光滑的超平面上

优化，从而显著

地加速学

习过程。一个稀疏奖励可

以被定义为一个简单的

二值变量，用来表示机器

人是否

抓取了目标物体

，而在没有额外信息的情

况下这对机器人来说可

能很难进行探索和学习

。

• 选择合适的网络结构。尽

管在深度学习中经常见

到一个有几十层网络和

数十亿参数的网络，

尤其

在像计算机视觉 (He et al.,

2016) 和自然

语言处理 (Jaderberg et al., 2015)

领域。对于深度

强化学习而言，神经网络

深度通常不会太深，超过

5 层的神经网络在强化学

习应用中不是

特别常见

。这是由于强化学习算法

本身的计算复杂度造成

的。因此，除非环境有很大

的规

模而且你有几十上

百个 GPU 或者

TPU 可以使用，否则

你一般不会在深度强化

学习中用一

个 10 层及以上

的网络，它的训练将会非

常困难。这不仅是计算资

源上的限制，而且这也与

深度强化学习由于缺失

监督信号而导致的不稳

定性和非单调表现增长

有关。在监督学习中，

如果

网络相比于数据而言足

够大，它可以过拟合到数

据集上，而在深度强化学

习中，它可

能只是缓慢地

收敛甚至是发散，这是因

为探索和利用之间的强

关联作用。网络大小的选

择

经常是依据环境状态

空间和动作空间而定的

。一个有几十个状态动作

组合的离散环境可能

可

以用一个表格方法，或者

一个单层或两层的神经

网络解决。更复杂的例子

如第 13 章和第

16

章中介绍的

应用，通常有几十维的连

续状态和动作空间，这就

需要可能大于 3 层的网络

，

但是相比于其他深度学

习领域中的巨型网络而

言，这仍旧是很小的规模

。

对于网络的结构而言，文

献中很常见的有多层感

知机（Multi-Layer Perceptrons，MLPs）、卷

积神经网络（CNNs）和循

环神经网络（RNNs）。更为高级和

复杂的网络结构很少用

到，除非

对模型微调方面

有具体要求或者一些其

他特殊情况。一个低维的

矢量输入可以用一个多

层

感知机处理，而基于视

觉的策略经常需要一个

卷积神经网络主干来提

前提取信息，要么与

强化

学习算法一起训练，要么

用其他计算机视觉的方

法进行预训练。也有其他

情况，比如将

低维的矢量

输入和高维的图像输入

一起使用，实践中通常先

采用从高维输入中提取

特征的

主干再与其余低

维输入并联的方法。循环

神经网络可以用于不是

完全可观测的环境或者

非

马尔可夫过程，最优的

动作选择不仅依赖当前

状态，而且依赖之前状态

。以上是实践中对策

略和

价值网络都有效的经验

指导。有时策略和价值网

络可能构成一种非对称

的 Actor-Critic

结构，因而它们的状态

输入是不同的，这可以用

于价值网络只用作训练

中策略网络的指导，

而在

动作预测时不再可以使

用价值网络的情况。

•

熟悉

你所用的强化学习算法

的性质。举例来说，像 PPO 或 TRPO 类

的基于信赖域的方法可

能需要较大的批尺寸来

保证安全的策略进步。对

于这些信赖域方法，我们

通常期待策略表

现稳定

的进步，而非在学习曲线

上某些位置突然有较大

下降。TRPO

等信赖域方法需要

用

一个较大批尺寸的原

因是，它需要用共轭梯度

来近似 Fisher 信息矩阵，这是基

于当前采样

到的批量样

本计算的。如果批尺寸太

小或者是有偏差的，可能

对这个近似造成问题，并

且

导致对

Fisher 信息矩阵（或逆

Hessian 乘积）的近似不准确而使

学习表现下降。因此，实践

中，算法 TRPO 和 PPO

中的批尺寸需

要被增大，直到智能体有

稳定进步的学习表现为

止。

436

18.2 实现阶段

所以，TRPO 有时也

无法较好地扩展到大规

模的网络或较深的卷积

神经网络和循环神经网

络上。DDPG

算法则通常被认为

对超参数敏感，尽管它被

证明对许多连续动作空

间的任务

很有效。当把它

应用到大规模或现实任

务 (Mahmood et al., 2018)

上时，这个敏感性会更

加

显著。比如，尽管在一个

简单的模拟测试环境中

通过彻底的超参数搜索

可以最终找到一个

最优

的表现效果，但是在现实

世界中的学习过程由于

时间和资源上的限制可

能不允许这种

超参数搜

索，因此 DDPG 相比与其他

TRPO 或 SAC 算

法可能不会有很好的效

果。另一方

面，尽管 DDPG

算法起

初是设计用来解决有连

续值动作的任务，这并不

意味着它不能在离

散值

动作的情况下工作。如果

你尝试将它应用到有离

散值动作的任务上，那么

需要使用一

些额外的技

巧，比如用一个有较大 t 值

的 Sigmoid(tx)

输出激活函数并且将

其修剪成二值化

的输出

，还得保证这个截断误差

比较小，或者你可以直接

使用 Gumbel-Softmax 技巧来更改

确定性

输出为一个类别的输出

分布。其他算法也可以有

相似处理。

•

归一化值处理

。总体来说，你需要通过缩

放而不是改变均值来归

一化奖励函数值，并且用

同样的方式标准化值函

数的预测目标值。奖励函

数的缩放基于训练中采

样的批样本。只做

值缩放

（即除以标准差）而不做均

值平移（为得到零均值而

减去统计均值）的原因是

，均

值平移可能会影响到

智能体的存活意愿。这实

际上与整个奖励函数的

正负号有关，而且这

个结

论只适用于你使用“Done”信号

的情况。其实，如果你事先

没有用“Done”信号来终

止片段

，那么，你可以使用均值平

移。考虑以下一种情况，如

果智能体经历了一个片

段，而

“Done=True”信号在最大片段长

度以内发生，那么假如我

们认为智能体仍旧存活

，则这个

“Done”信号之后的奖励

值实际为 0。如果这些为 0 的

奖励值总体上比之前的

奖励值高（即

之前的奖励

值基本是负数），那么智能

体会倾向于尽可能早地

结束片段，以最大化整个

片段

内的奖励。相反，如果

之前的奖励函数基本是

正值，智能体会选择“活”得

更久一些。如果

我们对奖

励值采取均值平移方式

，它会打破以上情形中智

能体的存活意愿，从而使

得智能

体即使在奖励值

基本为正时不会选择存

活得更久，而这会影响训

练中的表现。归一化值函

数的目标也是相似的情

况。举例来说，一些基于 DQN 的

算法的平均 Q 值会在学习

过程中

意外地不断增大

，而这是由最大化优化公

式中对 Q 值的过估计造成

的。归一化目标 Q 值可

以缓

解这个问题，或者使用其

他的技巧如

Double Q-Learning。

• 一个关于折

扣因子的小提示。你可以

根据折扣因子 γ 对单步动

作选择的有效时间范围

有一

个大致感觉：1 + γ + γ

2

+ · · · = 1/(1

− γ)。根据该

式，对于 γ = 0.99，我们经常可以忽

略

100

个时间步后的奖励。用

这个小技巧可以加速你

设置参数时的过程。

• Done 信号

只在终止状态时为真。对

于初学者来说，深度强化

学习中有一些很容易忽

略的细

微差别，而片段式

强化学习中的“Done”信号就是

其中一个。这些细微的差

异可能使得实

践中即使

是相同算法的不同实现

也会有截然不同的表现

。在片段式强化学习中，“Done”信

号被广泛用于结束一个

片段，而它是环境状态的

一个函数，只要智能体到

达终止状态，它

就被设置

为真。注意，这里终止状态

被定义为指示智能体已

经完成片段的情况，要么

成功

要么失败，而不是任

意一个到达时间限度或

最大片段长度的状态。将

“Done”信号的值只

437

第 18 章

深度强

化学习应用实践技巧

在

状态为终止状态时设为

真不是一个平庸的问题

。举例来说，如果一个任务

是操控机械臂

到达空间

中某个具体的位置，这个

“Done”信号只应当在机械臂确

实到达这个位置时为真

，

而不是到达默认片段最

大长度等情况下。为了理

解这个差异，我们需要知

道在强化学习中

有些环

境，时间长度是无穷的，有

些是有限的，而在采样过

程中，算法经常是对有限

长度

的轨迹做处理的。有

两种常用的实现方式，一

是设置最大片段长度，二

是使用“Done”信

号作为环境的

反馈来通过跳开循环以

终止片段。当使用“Done”信号作

为采样过程中的中

断点

时，它不应当在片段由于

到达最大长度的时候设

为真，而只应在终止状态

到达时为真。

还是前面的

例子，若一个机械臂在非

目标点的任意其他点由

于到达了片段最大长度

而结束

了这个轨迹，同时

设置了“Done”信号为真，则会对

学习过程产生消极影响

。具体来说，以

PPO 算法为例，从

状态

St 累计的奖励值被用

来估计该状态的价值 V (St)，而

一个终止状态

的价值为

0。如果在非终止状态时“Done”信

号的值为真，那么该状态

的值被强制设为 0

了，而实

际上它可能不应该为 0。这

会在价值网络估计之前

状态值的时候让其产生

混淆，从

而阻碍学习过程

。

• 避免数值问题。对于编程

实践中的除法，如果使用

不当可能会产生无穷大

的数值。两个技

巧可以解

决这类问题：一个是对正

数值的情况使用指数缩

放

a/b = exp(log(a) − log(b))；另

一个方法是对于非

负分母加上一个小量，如

a/b

≈ a/(b + 10−6

)。

•

注意奖励函数和最终目

标之间的分歧。强化学习

经常被用于一个有最终

目标的具体任务，

而通常

需要人为设计一个与最

终目标一致的奖励函数

来便于智能体学习。在这

个意义上说，

奖励函数是

目标的一种量化形式，这

也意味着它们可能是两

个不同的东西。在某些情

况下

它们之间会有分歧

。因为一个强化学习智能

体能够过拟合到你为任

务所设置的奖励函数上

，

而你可能发现训练最终

策略在达成最终目标上

与你所期望的不同。这其

中一个最可能的原

因是

奖励函数和最终目标之

间的分歧。在多数情况下

，奖励函数倾向于最终的

任务目标是

容易的，但是

设计一个奖励函数与最

终目标在所有极端情况

下都始终一致，是不平庸

的。你

应该做的是尽可能

减少这种分歧，来保证你

设计的奖励函数能够平

滑地帮助智能体达到最

终真实目标。

• 奖励函数可

能不总是对学习表现的

最好展示。人们通常在学

习过程中展示奖励函数

值（有

时用移动平均，有时

不用）来表示一个算法的

能力。然而，如同上面所说

，最终目标和你所

定义的

奖励函数之间可能有分

歧，这使得一个较高奖励

的状态可能对应一个在

达成最终目

标方面较差

的情况，或者至少没有显

式地表现出该状态与最

优状态之间的关系。由于

这个

原因，我们总需要在

使用强化学习和展示结

果时考虑这种分歧的可

能性。所以，在文献 (Fu

et al., 2018)

中很常

见到，有的学习表现不是

用平滑后的片段内奖励

（这也依赖奖励函数的

设

计）来评估和展示的，而是

用一个对这个任务更具

体的度量方式，比如图 18.1 所

示的机

器人学习任务中

，用机器夹具跟目标点的

距离来实现位置到达或

用物块跟目标的距离来

实

现物块推动。夹具与物

体间的距离，或者是否物

块被抓取，这些都是对任

务目标的真实度

量方式

。所以，这些度量可以用来

展示任务学习效果，从而

更好地体现任务最终目

标是否

438

18.2 实现阶段

到达。这

对于最终目标跟人为设

计的奖励函数有偏差的

情况很有用，如果你想比

较多个不

同个奖励函数

，那么这些额外的度量也

很关键。

图 18.1 OpenAI Gym 中的 FetchPush

环境。对这

个环境而言，使用物体到

目标位置的最终距离比

奖励函数值能更好地衡

量对所学策略的表现，因

为它是对任务整体目标

的最直接表示。

然而奖励

函数可能被设置为包含

一些其他因素，如夹具到

物体的距离等（见彩插）

• 非

马尔可夫情况。如之前章

节所述，这本书所介绍的

绝大多数理论结果都基

于马尔可夫过

程的假设

或者状态的马尔可夫性

质。马尔可夫性质不仅简

化了问题和推导，更重要

的是它

使得连续决策问

题可以描述，而且可以用

迭代的方式解决它，还能

得到简洁的解决方法。然

而，实践中，马尔可夫过程

的假设不总是成立。举例

来说，如图

18.2 所示，Gym 环境中的

Pong 游戏就不满足马尔可夫

过程在智能体选取最优

动作时对状态所做的假

设。我们需要记

住马尔可

夫性质是状态或环境的

性质，因此它是由状态的

定义决定的。非马尔可夫

决策过

程和部分可观测

马尔可夫过程（POMDP）的差异有

时是细微的。比如，如果一

个在上述游

戏中状态被

定义为同时包含小球的

位置和速度信息（假设小

球运动没有加速度），而观

察量

只有位置，那么这个

环境是 POMDP 而不是非马尔可

夫过程。然而，Pong 游戏的状态

通常

被认为是每一个时

间步静态帧，那么当前状

态只包含小球的位置而

没有智能体能够做出最

优动作选择的所有信息

，比如，小球速度和小球运

动方向也会影响最优动

作。所以这种情

况下它是

一个非马尔可夫环境。一

种提供速度和运动方向

信息的方法是使用历史

状态，而

这违背了马尔可

夫过程下的处理方法。所

以，如 DQN (Mnih et al.,

2015) 原文，堆叠帧可以

以

一种近似的 MDP 来解决 Pong

任务

。如果我们把所有的堆叠

帧看作一个单一状态，并

且

假设堆叠帧可以包含

做出最优动作选择的所

有信息，那么这个任务实

际上仍旧遵从马尔可

夫

过程假设。毕竟在所有的

模拟环境中，过程都是离

散的，而不像现实世界中

，有时间尺

度上的连续性

，我们经常可以用这种转

化方式来把一个非马尔

可夫过程看作一个马尔

可夫

439

第

18 章 深度强化学习

应用实践技巧

过程。除了

像 DQN 原文中使用堆叠帧，循

环神经网络（RNN）(Heess

et al., 2015) 或更高级

的

长短期记忆（LSTM）方法也可以

用于以历史记忆进行决

策的情况，来解决非马尔

可夫过

程的问题。

图 18.2 Gym 中的

Pong-v0 游戏：由于小球的速度无

法在单一帧中捕捉，这个

任务是非马尔可夫

的，用

堆叠帧作为观察量可以

解决它

18.3 训练和调试阶段

• 初始化很重要。深度强化

学习方法通常要么以在

线策略（On-Policy）方式用每个片段

内的

样本更新策略，要么

使用离线策略（Off-Policy）中动态的

回放缓存（Replay Buffer），这个

缓存包含

随时间变化的多样性样

本。这使得深度强化学习

不同于监督学习，监督学

习是从

一个固定的数据

集中学习，因而学习样本

的顺序不是特别重要。然

而，在深度强化学习中，

策

略的初始化可以影响随

后可能的探索范围，并决

定存入缓存的后续样本

或直接用于更新

的样本

，因此它会影响整个学习

表现。从一个随机策略开

始会导致较大的概率有

更多样的

样本，这对于训

练开始阶段是很好的。但

随着策略的收敛和进步

，探索的范围逐渐收窄，而

近趋于当前策略所生成

的轨迹。对于权重参数的

初始化而言，总体上来说

使用较高级的方

法如 Xavier

初

始化 (Glorot et al., 2010) 或正交初始化

(Saxe et al., 2013) 会较

好，这样可以避

免梯度消

失或梯度爆炸，并且对多

数深度学习情况都有较

稳定的学习表现。

• 向程序

中添加有用的探针。深度

学习往往要处理大量的

数据，而这其中有一些隐

藏的操作

是我们可能不

总清楚，尤其是当我们对

模型不熟悉的时候。通常

的报错系统可能不是针

对

其中一些错误的，尤其

是逻辑错误。模型中类似

的潜在问题将会是很危

险和难以察觉的。比

如，有

时在深度强化学习中，你

只关注奖励函数，但是也

应当可视化损失函数值

的变化来了

解其他函数

的拟合情况，比如价值函

数，或者随机分布策略的

熵来了解当前的探索状

态。如

果策略输出分布熵

过早下降，那么基本上表

明智能体不能通过当前

策略探索到更有用的样

440

18.3 训练和调试阶段

本。这可

以通过使用熵奖励或 KL 散

度惩罚项来缓解，如柔性

Actor-Critic（Soft

Actor-Critic，

SAC）等算法使用了适应性熵

类自动解决这类问题。对

于基于信赖域的方法，你

需要新旧

策略间 KL 散度值

的指标来告诉你模型是

否正常工作。有时你需要

输出网络的梯度值来检

查它的工作情况。正常网

络层的梯度值不应该过

大或全为０，否则它表明要

么有异常梯度

值，要么是

没有梯度流。其他有用的

指标有像在输出空间和

参数空间的更新步长，对

于以

上情况，Tensorboard 模块是一个

强大的工具，它起初是为

TensorFlow 开发设计的但是后来

也

支持 PyTorch 框架。它可以简化变

量的可视化过程、神经网

络计算图等，对实践中使

用这

些探针很有帮助。

• 使

用多个随机种子并计算

平均值来减少随机性。深

度强化学习方法很典型

地有不稳定的训

练过程

，随机种子甚至都会很大

地影响学习表现，有 NumPy 的随

机种子，以及

TensorFlow

或者 PyTorch 的，还有

环境的种子等。在随机化

这些种子的时候，作为一

种默认设置，所有

这些种

子都需要被合适地随机

化。刚开始，你可以固定这

些种子，然后观察采样轨

迹上是

否有任何差异，如

果仍有随机性，可能表现

系统内还有其他随机因

素。固定随机种子可以

用

来再现学习过程。使用随

机种子并得到学习曲线

的平均值，可以减少实验

对比中深度强

化学习随

机性造成的得到错误结

论的可能性。通常使用越

多的随机种子，实验结果

就越可

靠，但同时也增加

了实验耗时。根据经验，我

们采用不同的随机种子

进行 3 到 5

次试验便

可以得

到一个相对可信的结果

，但是越多越好。

• 平衡 CPU 和

GPU 计

算资源以加速训练。这个

提示实际上是关于找到

和解决训练速度上的

瓶

颈问题。在有限的计算机

上更好地使用计算资源

，对于强化学习要比监督

学习复杂。在

监督学习中

，CPU 经常用于数据读写和预

处理，而 GPU

用于进行前向推

理和反向传播过

程。然而

，由于强化学习中推理过

程总是涉及与环境的交

互，计算梯度的设备需要

与处理

环境交互的设备

匹配计算能力，否则会是

对探索或利用的浪费。在

强化学习中，CPU 经常

被用于

与环境交互采样的过程

，而这对某些复杂的模拟

系统可能涉及大量运算

。GPU 被用

来进行前向推理和

反向传播来更新网络。你

在部署大规模训练的过

程中，应当检查 CPU 和

GPU 计算资

源的利用率，避免线程或

进程沉睡。这对于将程序

分配到大规模并行计算

系统

中尤为重要。对于

GPU 过

度利用的情况，可以采用

更多的采样线程或进程

来与环境交互。

对于 CPU 过度

利用的情况，你可以减少

分布式采样线程的数量

，或者增加分布式更新线

程

的数量，增大算法内更

新迭代次数，对于离线更

新增大批尺寸等，这些都

依赖于你管理并

行线程

和进程的方式。注意上面

所述只是关于如何最大

化利用你的计算资源，你

也应当考

虑探索和利用

之间的取舍，以及对于多

种多样的强化学习任务

在不同层次上的采样效

率等。

为了解决 CPU 和 GPU

资源间

的平衡问题，你经常需要

在采样和训练过程中使

用多线程

或多进程并行

计算，来充分利用你的计

算机。需要仔细考虑如何

设计能够同时运行采样

线

程/进程和网络更新线

程/进程的并行训练框架

。锁和管道被经常用于这

种框架来支持其顺利

运

行。创建冗余进程有时可

以节省等待时间。在线策

略和离线策略的处理可

能不同，相比

于在线策略

，离线策略训练的并行设

置经常更加灵活，因为你

可以在任何时刻更新策

略而

441

第 18 章 深度强化学习

应用实践技巧

非仅在片

段的最后一步。一个典型

的在 PyTorch

框架下使用多 GPU 分布

式训练的使用方式

如图

18.3 所示。使用 PyTorch

处理多 GPU 过程在

前向推理中采用了一个

模型复制过程和一

个推

理结果采集过程，而在反

向更新过程中，梯度缩减

被用于并行的梯度反向

传播。更多

相关细节在强

化学习应用的章节中有

所讨论，我们也在代码库

中提供了一些示例程序

。

图

18.3 使用 torch.nn.DataParallel 的前向和反向过

程（见彩插）

• 可视化。如果你

不能直接从数值中看清

潜在关系，你应当尽可能

对其可视化。比如，有时

由

于强化学习过程不稳定

的特性，奖励函数可能有

很大抖动，这种情况下你

可能需要画出

奖励值的

滑动平均曲线来了解智

能体在训练中是否有进

步。

• 平滑学习曲线。强化学

习的过程可能非常不稳

定。直接从未经处理的学

习曲线中得出结论

经常

是不可靠的，像图 18.4

中未经

平滑的学习曲线那样。我

们通常要用滑动平均、卷

积核

等来平滑学习曲线

，并且选用一个合适的窗

口长度。通过这种方式，学

习表现的上升/下降

趋势

可以更清楚地展示出来

，当你在解决一个有着很

长训练周期和较慢表现

进步的复杂强

化学习任

务时，这么做可能很关键

。

图 18.4

强化学习中平滑的和

未平滑的学习曲线（见彩

插）

442

参考文献

• 理解探索和

利用。从图 18.4

中我们可以看

出，学习曲线在早期训练

阶段有一个平台期。实

际

上，这在强化学习过程中

不是一个罕见的，而是一

个十分常见的情况。这是

因为在强化

学习中，学习

样本不是像在监督学习

中那样提前准备好的，而

是通过所应用的智能体

策略

探索得到的。因此，当

前策略能否探索到较高

奖励值的轨迹在强化学

习训练中可能是很关

键

的。而这会引出探索相关

的问题，即需要保证我们

的策略能逐渐探索到接

近最优的轨迹。

当强化学

习算法不能对一个具体

任务工作时，你需要研究

这个智能体是否已经探

索到那些

更好的轨迹。如

果没有，至少说明当前的

探索方式可能有问题。然

而，如果当前策略能够

探

索到好的轨迹，但它仍不

能收敛到好的动作选择

，那么可能是利用问题。这

意味着策略

不能够较好

地从好的轨迹中学习。利

用问题可能是由较低的

样本效率、较差的价值函

数拟

合、价值函数较低的

学习率、较差的策略网络

学习效果等造成的。图 18.4 中

所示的学习曲

线展示了

一个健康的学习进步：一

旦好的样本被探索到（在

平台期中），策略更新会使

学习

表现会显著提升（在

平台期后）。

• 首先质疑你的

算法实现。当你刚完成代

码实现以后，它不会工作

，是很常见的，而这时，耐

心

地调试代码就很重要。算

法实现的正确性总是要

先于微调一个相对好的

结果，因此，应

当在保证实

现正确性的前提下再考

虑微调超参数。而这也正

是本章开头提到的强化

学习应

用的过程：先用小

规模例子测试来保证算

法实现的正确性，然后逐

步扩展到大规模环境并

微调分布式的训练过程

。一个糟糕的学习表现可

能由很多因素导致，如不

充足的训练时间、

对超参

数的糟糕选择、未经归一

化的输入数据等，而最常

见的原因是代码实现中

的错误。

为了给读者提供

更全面的关于强化学习

算法在具体项目应用上

的指导，我们在写本章的

过程

中也参考了一些外

部资源，包括 OpenAI Spinning

Up1、John Schulman 的幻灯片讲

稿2、William

Falcon 的相关博客3等。我们也

建议读者参考这些总结

得较好的建议和来自研

究人员的经验，来

帮助实

现自己的强化学习算法

和应用。查阅与你所做内

容相似的他人之前的工

作，并从中吸取经

验，总是

很有帮助的。

此外，读者需

要知道只是阅读上面段

落中经验性的指示而不

实践，几乎没有用。所以，我

们

强烈推荐读者自己手

动实现一些代码来获取

实践经验，只用通过这种

方式才能发挥这些技巧

的最

大作用。

参考文献

ESPEHOLT

L, SOYER H, MUNOS R, et

al., 2018. Impala: Scalable distributed deep-rl

with importance

weighted actor-learner architectures[J]. arXiv

preprint arXiv:1802.01561.

1OpenAI Spinning Up: https://spinningup.openai.com/en/latest/index.html

2The Nuts and Bolts of Deep

RL Research. John Schulman: http://joschu.net/docs/nuts-and-bolts.pdf

3Deep

RL Hacks: https://github.com/williamFalcon/DeepRLHacks

443

第

18 章

深度强化学习应用实

践技巧

FU J, SINGH A, GHOSH

D, et al., 2018. Variational inverse

control with events: A general framework

for data-driven reward definition[C]//Advances in Neural

Information Processing Systems. 8538-8547.

GLOROT X,

BENGIO Y, 2010. Understanding the difficulty

of training deep feedforward neural networks[C]//Proceedings

of the thirteenth international conference on

artificial intelligence and statistics.

249-256.

HE

K, ZHANG X, REN S, et

al., 2016. Deep Residual Learning for

Image Recognition[C]//Proceedings

of the IEEE Conference

on Computer Vision and Pattern Recognition

(CVPR).

HEESS N, HUNT J J,

LILLICRAP T P, et al., 2015.

Memory-based control with recurrent neural

networks[J].

arXiv preprint arXiv:1512.04455.

JADERBERG M, SIMONYAN

K, ZISSERMAN A, et al., 2015.

Spatial transformer networks[C]//

Proceedings of the

Neural Information Processing Systems (Advances in

Neural Information Processing

Systems) Conference. 2017-2025.

MAHMOOD A R, KORENKEVYCH D, VASAN

G, et al., 2018. Benchmarking reinforcement

learning

algorithms on real-world robots[J]. arXiv

preprint arXiv:1809.07731.

MNIH V, KAVUKCUOGLU K,

SILVER D, et al., 2015. Human-level

control through deep reinforcement

learning[J]. Nature.

SAXE A M, MCCLELLAND J L,

GANGULI S, 2013. Exact solutions to

the nonlinear dynamics of

learning in

deep linear neural networks[J]. arXiv preprint

arXiv:1312.6120.

VINYALS O, BABUSCHKIN I, CZARNECKI

W M, et al., 2019. Grandmaster

level in starcraft ii using

multi-agent

reinforcement learning[J]. Nature, 575(7782): 350-354.

444

总结部分

为了帮

助读者快速查阅与比较

不同的算法，我们在附录

A 总结了介绍过的算法及

其对应论

文，在附录 B 提供

了各个算法的伪代码，附

录

C 提供英文首字母缩写

和中英文对照表。

445

附录A 算

法总结表

在附录

A 中，我们

将那些常见的强化学习

算法总结成一张表格，尤

其是那些在本书中介绍

过

的。我们希望这样能为

读者寻找相关文献提供

参考。

446

强化学习算法

强化

学习算法

策略 动作空间

年份 文献 作者

Q-Learning 离线策略

（Off-Policy）

离散 1992 Q-Learning (Watkins et al.,

1992) Cristopher J.C.H Watkins and Peter

Dayan

SARSA 在线策略 （On-Policy） 离散 1994

Online Q-Learning using Connectionist Systems (Rummery

et al., 1994) G.A.Rummery and M.

Niranjan

DQN 离线

策略 离散 2015 Human-level

Control Through Deep Reinforcement Learning (Mnih

et al., 2015) Volodymyr Mnih, et

al.

Dueling DQN 离线策略 离散

2015 Dueling

Network Architectures for Deep Reinforcement Learning

(Wang et al., 2015) Ziyu Wang,

et al.

Double DQN 离线策略 离散

2016 Deep Reinforcement Learning with Double

Q-Learning (Van Hasselt et al., 2016)

Hado van Hasselt, et al.

Noisy

DQN 离线策略

离散 2017 Noisy Networks for

Exploration (Fortunato et al., 2017) Meire

Fortunato, et al.

Distributed DQN 离线策略

离散 2017 A Distributional Perspective on

Reinforcement Learning (Bellemare et al., 2017)

Marc G. Bellemare, et al.

Actor-Critic

(QAC) 在线

策略 离散或连续 2000 Actor-Critic Algorithms

(Konda et al., 2000) Vijay R.

Konda and John N. Tsitsiklis

A3C

在线策

略 离散或连续 2016 Asynchronous Methods for

Deep Reinforcement Learning (Mnih et al.,

2016) Volodymyr Mnih, et al.

DDPG

离线策略

连续 2016 Continuous Control With Deep

Reinforcement Learning (Lillicrap et al., 2015)

Timothy P. Lillicrap, et al.

447

附录

A 算法总结表 强

化学习算法 （续表一） 强化

学习算法

策略 动作空间

年份 文献 作者

REINFORCE 在线策略

离散或连续

1988 On the Use of Backpropagation

in Associative Reinforcement Learning (Williams, 1988)

Ronald J. Williams

TD3 离线策略 连

续

2018 Addressing function approximation error in

actor-critic methods (Fujimoto et al., 2018)

Scott Fujimoto, et al.

SAC 离线策略

离散或连续

2018 Soft actor-critic algorithms and applications

(Haarnoja et al., 2018) Tuomas Haarnoja,

et al.

TRPO 在线策略 离散或连续 2015

Trust region policy optimization (Schulman et

al., 2015) John Schulman, et al.

PPO 在

线策略 离散或连续 2017 Proximal policy

optimization algorithms (Schulman et al., 2017)

John Schulman, et al.

DPPO 在线

策略

离散或连续 2017 Emergence of locomotion behaviours

in rich environments (Heess et al.,

2017) Nicolas Heess, et al.

ACKTR

在线策

略 离散或连续 2017

Scalable trust-region method

for deep

reinforcement learning using

Kronecker-factored

approximation (Wu et al.,

2017)

Yuhuai

Wu, et al.

CE Method 在线策略

离散或连续

2004

The cross-entropy method: A unified

approach

to Monte Carlo simulation, randomized

optimization and machine learning (Rubinstein

et

al., 2004)

R. Rubinstein and D.

Kroese

448

参考文献

参

考文献

BELLEMARE M

G, DABNEY W, MUNOS R, 2017.

A distributional perspective on reinforcement

learning[C]//Proceedings

of the 34th International Conference on

Machine Learning-Volume 70. JMLR.

org: 449-458.

FORTUNATO M, AZAR M G, PIOT

B, et al., 2017. Noisy networks

for exploration[J]. arXiv preprint

arXiv:1706.10295.

FUJIMOTO

S, VAN HOOF H, MEGER D,

2018. Addressing function approximation error in

actor-critic

methods[J]. arXiv preprint arXiv:1802.09477.

HAARNOJA

T, ZHOU A, HARTIKAINEN K, et

al., 2018. Soft actor-critic algorithms and

applications[J].

arXiv preprint arXiv:1812.05905.

HEESS N,

SRIRAM S, LEMMON J, et al.,

2017. Emergence of locomotion behaviours in

rich environments[J]. arXiv:1707.02286.

KONDA V R,

TSITSIKLIS J N, 2000. Actor-critic algorithms[C]//Advances

in Neural Information

Processing Systems. 1008-1014.

LILLICRAP T P, HUNT J J,

PRITZEL A, et al., 2015. Continuous

control with deep reinforcement

learning[J]. arXiv

preprint arXiv:1509.02971.

MNIH V, KAVUKCUOGLU K,

SILVER D, et al., 2015. Human-level

control through deep reinforcement

learning[J]. Nature.

MNIH V, BADIA A P, MIRZA

M, et al., 2016. Asynchronous methods

for deep reinforcement learning[C]//International Conference on

Machine Learning (ICML). 1928-1937.

RUBINSTEIN R

Y, KROESE D P, 2004. The

cross-entropy method: A unified approach to

monte

carlo simulation, randomized optimization and

machine learning[J]. Information Science & Statistics,

Springer Verlag, NY.

RUMMERY G A,

NIRANJAN M, 1994. On-line q-learning using

connectionist systems: volume 37[M].

University of

Cambridge, Department of Engineering Cambridge, England.

SCHULMAN J, LEVINE S, ABBEEL P,

et al., 2015. Trust region policy

optimization[C]//International

Conference on Machine Learning (ICML).

1889-1897.

SCHULMAN J, WOLSKI F, DHARIWAL

P, et al., 2017. Proximal policy

optimization algorithms[J].

arXiv:1707.06347.

449

附录 A

算法总结表

VAN HASSELT H, GUEZ A, SILVER

D, 2016. Deep reinforcement learning with

double Q-learning[C]//

Thirtieth AAAI conference on

artificial intelligence.

WANG Z, SCHAUL T,

HESSEL M, et al., 2015. Dueling

network architectures for deep reinforcement

learning[J].

arXiv preprint arXiv:1511.06581.

WATKINS C J,

DAYAN P, 1992. Q-learning[J]. Machine learning,

8(3-4): 279-292.

WILLIAMS R J, 1988.

On the use of backpropagation in

associative reinforcement learning[C]//

Proceedings of the

IEEE International Conference on Neural Networks:

volume 1. San Diego, CA.:

263-270.

WU Y, MANSIMOV E, GROSSE R

B, et al., 2017. Scalable trust-region

method for deep reinforcement learning using

kronecker-factored approximation[C]//Advances in Neural Information Processing

Systems. 5279-5288.

450

附录B 算法速查表

附录

B 总

结了（深度）强化学习的算

法和关键概念。这些算法

被分为四个部分：深度学

习、

强化学习、深度强化学

习和高等深度强化学习

。为了便于读者学习，我们

为每个算法提供了伪代

码。我们尽量在行文中保

持数学符号、变量记号和

术语与整本书一致。

B.1 深度

学习

B.1.1

随机梯度下降

算法

B.43 随机梯度下降的训练过

程

Input: 参数 θ,

学习率 α, 训练步数

/迭代次数 S

for i

= 0 to S do

计算一个小批

量的

L

通过反向传播计算

∂L

∂θ

▽θ ← −α

·

∂L

∂θ

;

θ ←

θ + ▽θ 更新参数

end for

return θ; 返回训练好的

参数

451

附录 B

算法速查表

B.1.2 Adam 优

化器

算法 B.44

Adam 优化器的训练

过程

Input: 参数 θ, 学习率

α, 训练步

数/迭代次数 S, β1 = 0.9,

β2 = 0.999, ϵ = 10−8

m0 ← 0; 初始化一阶

动量

v0 ←

0; 初始化二阶动量

for t = 1

to S do

∂L

∂θ

;

用

一个随机的小批次计算

梯度

mt ← β1 ∗ mt−1

+ (1 − β1) ∗

∂L

∂θ

; 更新一阶动量

vt ← β2

∗ vt−1 + (1 − β2)

∗ (

∂L

∂θ

)

2

; 更新

二阶动量

mˆt ← mt

1−β

t

1

; 计算一阶动量

的滑动平均

vˆt ←

vt

1−β

t

2

; 计算二阶动

量的滑动平均

▽θ ← −α ∗ √mˆt

vˆt+ϵ

θ ← θ + ▽θ; 更新参数

end

for

return θ; 返回训练好的参数

B.2 强化

学习

B.2.1 赌博机

随机多臂赌

博机（Stochastic Multi-armed Bandit）

算法

B.45 多臂赌博机学

习

初始化 K 个手臂

定义总

时长为

T

每一个手臂都有

一个对应的 vi ∈ [0, 1].

每一个奖励

都是独立同分布地从 vi 中

采样得到的

for t =

1, 2, · · · ,

T do

智能体从 K 个

手臂中选择 At

= i

环境返回奖

励值向量 Rt = (R1

t

, R2

t

, ·

· · , RK

t

)

智能体观测到

Ri

t

end for

对抗多臂赌博机（Adversarial Multi-armed

Bandit）

算法 B.46 对

抗多臂赌博机

初始化 K

个

机器手臂

452

B.2 强化学习

for t

= 1, 2, · · ·

, T do

智能

体在 K 个手臂当中选中

It

对

抗者选择一个奖励值向

量 Rt = (R1

t

, R2

t

, · ·

· , RK

t

) ∈

[0, 1]K

智能体观察到奖励 R

It

t

（根

据具体的情况也有可能

看到整个奖励值向量）

end for

算

法 B.47 针对对抗多臂赌博机

的

Hedge 算法

初始化 K 个手臂

Gi(0)

for i = 1, 2, ·

· · , K

for t

= 1, 2, · · ·

, T do

智

能体从 p(t) 分布中选择

At = it，其中

pi(t) = exp(ηGi(t −

1))

PK

j

exp(ηGj (t −

1))

智能体观测到奖励 gt

让 Gi(t) =

G(t − 1) + g

i

t

, ∀i ∈ [1, K]

end for

B.2.2 动

态规划

策略迭代（Policy Iteration）

算法 B.48 策

略迭代

对于所有的状态

初始化 V 和

π

repeat

//执行策略评估

repeat

δ ← 0

for s ∈ S do

v

← V (s)

V (s) ←

P

r,s′ (r + γV (s

′

))P(r, s′

|s, π(s))

δ

← max(δ, |v − V (s)|)

end for

until δ 小于一个正阈值

//执行策

略提升

stable ← true

453

附录 B

算法速查表

for s ∈ S do

a

← π(s)

π(s) ← arg maxa

P

r,s′ (r + γV (s

′

))P(r, s′

|s, a)

if

a ̸= π(s) then

stable ←

false

end if

end for

until

stable = true

return 策略 π

价值迭代（Value Iteration）

算法 B.49 价值

迭代

为所有状态初始化

V

repeat

δ ← 0

for s

∈ S do

u ← V

(s)

V (s) ← maxa

P

r,s′ P(r, s′

|s, a)(r +

γV (s

′

))

δ ←

max(δ, |u − V (s)|)

end

for

until δ 小于一个正阈值

输出贪

心策略 π(s)

= arg maxa

P

r,s′ P(r,

s′

|s, a)(r + γV (s

′

))

B.2.3 蒙特卡罗

蒙特卡

罗预测

算法

B.50 首次蒙特卡

罗预测

输入：初始化策略

π

初始化所有状态的 V (s)

初始

化一列回报：Returns(s) 对所有状态

repeat

通过 π: S0, A0,

R0, S1, · · · ,

ST −1, AT −1, Rt 生成一个回合

G ← 0

t ← T

− 1

for t >= 0

do

G ← γG + Rt+1

454

B.2 强化

学习

if S0, S1,

· · · , St−1 没有

St then

Returns(St).append(G)

V (St) ←

mean(Returns(St))

end if

t ← t

− 1

end for

until 收敛

蒙特卡罗

控制

算法 B.51 蒙特卡罗探索

开始

初始化所有状态的

π(s)

对于所有的状态-动作对

，初始化

Q(s, a) 和 Returns(s, a)

repeat

随机选择 S0 和 A0 直

到所有状态-动作对的概

率为非零

根据

π: S0, A0, R0, S1, ·

· · , ST −1, AT

−1, Rt 来生成 S0, A0

G

← 0

t ← T −

1

for t >= 0 do

G ← γG + Rt+1

if

S0, A0, S1, A1 · ·

· , St−1, At−1 没

有 St,

At then

Returns(St, At).append(G)

Q(St, At)

← mean(Returns(St, At))

π(St) ← arg

maxa Q(St, a)

end if

t

← t − 1

end for

until 收敛

时间差分（Temporal Difference，TD）

算法 B.52

TD(0) 对

状态值的估算

输入策略

π

初始化 V (s)

和步长 α ∈ (0, 1]

for

每一个回

合 do

初始化 S0

for 每一个在现有

的回合的

St do

At ← π(St)

455

附录 B 算法速查

表

Rt+1, St+1 ←

Env(St, At)

V (St) ← V

(St) + α[Rt+1 + γV (St+1)

− V (St)]

end for

end

for

TD(λ)

算法 B.53 状态值半梯度 TD(λ)

输

入策略 π

初始化一个可求

导的状态值函数 v、步长 α 和

状态值函数权重

w

for 对每一

个回合 do

初始化 S0

z ← 0

for 每一个本

回合的步骤 St

do

使用 π 来选择

At

Rt+1, St+1

← Env(St, At)

z ← γλz

+ ∇V (St, wt)

δ ←

Rt+1 + γV (St+1, wt) −

V (St, wt)

w ← w

+ αδz

end for

end for

Sarsa：在线策略 TD 控制

算法 B.54 Sarsa

（在线

策略 TD 控制）

对所有的状态

-动作对初始化 Q(s, a)

for 每一个回

合 do

初始化 S0

用一个基于

Q 的

策略来选择 A0

for 每一个在当

前回合的 St

do

用一个基于 Q 的

策略从 St 选择

At

Rt+1, St+1 ← Env(St, At)

从 St+1 中用一个

基于 Q 的策略来选择 At+1

Q(St, At) ← Q(St, At) +

α[Rt+1 + γQ(St+1, At+1) − Q(St,

At)]

end for

end for

456

B.2 强化

学习

N 步 Sarsa

算法

B.55 N 步 Sarsa

对所有的

状态动作对初始化 Q(s,

a)

初始

化步长 α ∈ (0, 1]

决定一个固定的

策略 π 或者使用 ϵ-贪心

for 每一

个回合

do

初始化 S0

使用 π(S0, A)

来选

择 A0

T ← INTMAX （一个回合的长度）

γ ← 0

for t ←

0, 1, 2, · · ·

until γ − T − 1

do

if t < T then

Rt+1, St+1 ← Env(St, At)

if

St+1 是终

止状态 then

T ← t

+ 1

else

使用 π(St, A)

来选择 At+1

end if

end if

τ ← t − n +

1 （更新

的时间点。这个是 n 步 Sarsa，所以

只需要更新那个 n

+ 1 前的一

步，

就会持续这样下去，直

到所有状态都被更新。）

if τ

⩾ 0 then

G ←

Pmin(r+n,T)

i=τ+1 γ

i−γ−1Ri

if γ +

n < T then

G ←

G + γ

nQ(St+n, Aγ+n)

end

if

Q(Sγ, Aγ) ← Q(Sγ, Aγ)

+ α[G − Q(Sγ, Aγ)]

end

if

end for

end for

Q-learning：离

线策略

TD 控制

算法 B.56 Q-learning （离线策

略

TD 控制）

初始化所有的状

态-动作对的 Q(s, a) 以及步长

α ∈ (0, 1]

for 每

一个回合

do

457

附录 B 算法速查

表

初始化

S0

for 每一个在当前

回合的 St do

使用基于

Q 的策略

来选择 At

Rt+1, St+1 ←

Env(St, At)

Q(St, At) ← Q(St,

At) + α[Rt+1 + γ maxa

Q(St+1, a) − Q(St, At)]

end

for

end for

B.3 深度强化学习

深

度

Q 网络（Deep Q-Networks，DQN）是一个将 Q-learning 通过深

度神经网络来拟合价值

函数，从而延伸到高维情

况的方法，它使用一个目

标动作价值网络和一个

经验回放缓存来更新。

主

要思想：

• 用神经网络进行

Q 值函数拟合；

• 用经验回放

缓存进行离线更新；

•

目标

网络和延迟更新；

• 用均方

误差或 Huber 损失来最小化时

间差分（Temporal Difference，TD）误差。

算法 B.57 DQN

超参数

: 回放缓存容量 N、奖励折扣

因子

γ、用于目标状态-动作

值函数更新的延迟步长

C、

ϵ-greedy 中的 ϵ

输入: 空回放缓存

D，初

始化状态-动作值函数 Q 的

参数 θ

使用参数 ˆθ

← θ 初始化目

标状态-动作值函数 Qˆ

for 片段

=

0, 1, 2, · · ·

do

初始化环境并获取观测

数据 O0

初始化序列 S0 =

{O0} 并对序

列进行预处理 ϕ0 = ϕ(S0)

for

t = 0, 1, 2, ·

· · do

通过概率

ϵ 选择一个随机动作 At，否则

选择动作

At = arg maxa Q(ϕ(St), a;

θ)

执行动作 At 并获

得观测数据 Ot+1 和奖励数据

Rt

如果本局结束，则设置 Dt = 1，否

则 Dt =

0

设置 St+1 = {St, At,

Ot+1} 并进行预处理 ϕt+1 = ϕ(St+1)

存

储状态转移数据

(ϕt, At, Rt, Dt, ϕt+1) 到

D 中

从

D 中随机采样小批量状态

转移数据 (ϕi

,

Ai

, Ri

, Di

,

ϕ′

i

)

如果 Di =

0，设置 Yi = Ri + γ

maxa′ Qˆ(ϕ

′

i

, a′

;

ˆθ)，否则

设置 Yi = Ri

在

(Yi − Q(ϕi

, Ai

;

θ))2 上对 θ 执行梯度下

降步骤

458

B.3

深度强化学习

每

C 步对目标网络 Qˆ 进行同步

如果片段结束，则跳出循

环

end

for

end for

Double DQN 是一个

DQN 的改进版本，用

来解决过估计（Overestimation）问题。

主要

思想：

• 双 Q

网络是一种对目

标价值估计的嵌入式方

法，一个 Q 估计值被嵌入另

一个 Q 估计值中。

更改上面

DQN

算法的第 14 行为令 Yj = Rj

+γ(1− Dj )Qˆ(ϕj+1, arg maxa′ Q(ϕj+1,

a′

; θj ); ˆθ)。

Dueling

DQN 是对 DQN 的

一个改进版本，它将动作

价值函数分解为一个状

态价值函数和一

个依赖

状态的动作优势函数。

主

要思想：

• 将动作价值函数

Q 分解为值函数 V 和优势函

数 A。

更改DQN中动作价值函数

Q（及它的目标Qˆ）的参数化方

式为Q(s, a; θ, θv, θa) =

V (s; θ, θv)+

(A(s, a;

θ, θa)−maxa′ A(s, a′

; θ,

θa))或，Q(s, a; θ, θv, θa) =

V (s; θ, θv)+(A(s, a; θ,

θa)−

1

|A|A(s, a′

; θ,

θa))。

REINFORCE 是一个使用基于

策略优化和在线策略更

新的算法。

算法 B.58 REINFORCE

输入: 初始

策略参数 θ

for k =

0, 1, 2, · · ·

do

初始化环境

通

过在环境中运行策略πk = π(θk)收

集轨迹数据集Dk =

{τi = {(St, At, Rt)|t =

0, 1, · · · ,

T}}

计算累计

奖励 Gt

估计策略梯度 gk =

1

|Dk|

P

τ∈Dk

PT

t=0

∇θ log πθ(At|St)|θkGt

通过

梯度上升更新策略 θk+1 =

θk + αkgk

end for

带基

准函数的REINFORCE算法或称初版

策略梯度（REINFORCE

with Baseline/Vanilla Policy

Gradient）是 REINFORCE 的另一个版

本，它使用动作优势函数

而不是累计奖励来估计

策略梯度。

算法 B.59 带基准函

数的 REINFORCE 算法

超参数:

步长 ηθ、奖

励折扣因子 γ、总步数 L、批尺

寸 B、基准函数 b。

输入: 初始策

略参数 θ0

初始化 θ =

θ0

for k = 1, 2,

· · · , do

执行策略

πθ

得到 B 个轨迹，每一个有 L 步

，并收集 {St,ℓ,

At,ℓ, Rt,ℓ}。

Aˆ

t,ℓ =

PL

ℓ

′=ℓ

γ

ℓ

′−ℓRt,ℓ −

b(St,ℓ)

459

附录 B 算法速查表

J(θ) =

1

B

PB

t=1

PL

ℓ=0

log πθ(At,ℓ|St,ℓ)Aˆ

t,ℓ

θ = θ

+ ηθ∇J(θ)

用 {St,ℓ, At,ℓ, Rt,ℓ}

更新 b(St,ℓ)

end for

返回 θ

Actor-Critic 是一个改自

REINFORCE 的算法，它使用价值函数

拟合。

算法 B.60 Actor-Critic

算法

超参数: 步

长 ηθ 和 ηψ、奖励折扣因子

γ

输入

: 初始策略函数参数 θ0, 初始

价值函数参数 ψ0

初始化 θ = θ0 和

ψ =

ψ0

for t = 0, 1,

2, · · · do

执行一步策略

πθ, 保存 {St, At, Rt, St+1}

估计

优势函数 Aˆ

t = Rt +

γV πθ

ψ

(St+1) − V

πθ

ψ

(St)

J(θ) = P

t

log πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ) = P

t Aˆ2

t

ψ = ψ + ηψ∇JV

πθ

ψ

(ψ), θ = θ

+ ηθ∇J(θ)

end for

返回 (θ,

ψ)

Q 值 Actor-Critic（Q-value Actor-Critic，QAC）是另一

个版本的 Actor-Critic

算法，作为基于

价值（比如 Q-Learning）和基于策略（比

如 REINFORCE）优化方法的结合，使用

在线策略更新

的方式。

主

要思想：

•

结合 DQN 和 REINFORCE。

算法 B.61

QAC

输入

: 初始策略参数 θ、初始动作

价值函数 Q 的参数

ω、折扣因

子 γ

for k = 0,

1, 2, · · · do

初始化环境

通过在环

境中运行策略 πk = π(θk)，收集轨迹

数据集 Dk

= {τi = {(St, At, Rt,

Dt)|t =

0, 1, · ·

· , T}}。

计算 TD 误差

δt = Rt + γ maxa′

Qω(St+1, a′

) − Qω(St, At)

估计策

略梯度如 gk =

1

|Dk|

P

τ∈Dk

PT

t=0 ∇θ log πθ(At|St)|θkQω(St,

At)

通过梯度上升

更新策略 θk+1 = θk +

αkgk

使用均方误差

更新动作价值函数 ϕk+1 = arg minϕ

1

|Dk|T

P

τ∈Dk

PT

t=0

δ

2

t 通过

梯度下降算法

460

B.3

深度强化

学习

end for

优势 Actor-Critic（Advantage Actor-Critic，A2C）是

Actor-Critic 算法的改进

版本，它使用有

基准的 REINFORCE 而

非初版 REINFORCE

来进行策略优化

，并且使用在线策略更新

。

主要思想：

• 结合 DQN 和有基准

的

REINFORCE。

算法 B.62 A2C

Master:

超参数:

步长 ηψ 和 ηθ, worker 节

点集

W

输入: 初始策略函数

参数 θ0, 初始价值函数参数

ψ0

初始化

θ = θ0 和 ψ =

ψ0

for k = 0, 1,

2, · · · do

(gψ,

gθ) = 0

for W 里每一个

worker 节点

do

(gψ, gθ) = (gψ,

gθ) + worker(V

πθ

ψ

,

πθ)

end for

ψ = ψ

− ηψgψ; θ = θ +

ηθgθ。

end for

Worker:

超参数: 奖励折扣因子

γ, 轨

迹长度 L

输入: 价值函数 V

πθ

ψ

, 策

略函数 πθ

执行

L 步策略 πθ, 保存

{St, At, Rt,

St+1}

估计优势函数 Aˆ

t = Rt

+ γV πθ

ψ

(St+1) −

V

πθ

ψ

(St)

J(θ) =

P

t

log πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ) = P

t

Aˆ2

t

(gψ, gθ) = (∇JV

πθ

ψ

(ψ), ∇J(θ))

返回 (gψ,

gθ)

异步

优势 Actor-Critic（Asynchronous Advantage Actor-Critic，A3C）是一个 A2C

的修改版本

，

它使用异步梯度更新来

实现大规模并行计算。

主

要思想：

• 异步更新策略。

算

法

B.63 A3C

Master:

超参数: 步长 ηψ

和 ηθ, 当前策

略函数 πθ, 价值函数 V

πθ

ψ

461

附录 B 算

法速查表

输入: 梯度 gψ, gθ

ψ =

ψ − ηψgψ; θ = θ

+ ηθgθ。

返回

(V

πθ

ψ

,

πθ)

Worker:

超参数: 奖励折扣因子 γ、轨

迹长度 L

输入: 策略函数 πθ、价

值函数 V

πθ

ψ

(gθ, gψ) = (0, 0)

for

k = 1, 2, · ·

· , do

(θ, ψ) =

Master(gθ, gψ)

执行 L 步策略 πθ,

保存

{St, At, Rt, St+1}。

估计优势函数 Aˆ

t = Rt + γV πθ

ψ

(St+1) − V

πθ

ψ

(St)

J(θ) = P

t

log

πθ(At|St)Aˆ

t

JV

πθ

ψ

(ψ)

= P

t Aˆ2

t

(gψ,

gθ) = (∇JV

πθ

ψ

(ψ),

∇J(θ))

end for

深度确定

性策略梯度（Deep Deterministic Policy

Gradient，DDPG）是 DQN 和 QAC 的结合

，

它使用确定性策略，并采

用经验回放缓存和离线

策略更新的方式。

主要思

想：

• 确定性策略作为动作

空间上 Q 值的最大化算子

的拟合；

•

用 Ornstein-Uhlenbeck 或高斯噪声进

行随机动作的探索；

• 目标

网络和延迟更新。

算法

B.64 DDPG

超

参数：软更新因子 ρ，奖励折

扣因子 γ

输入：回放缓存

D，初

始化 Critic 网络 Q(s, a|θ

Q)

参数 θ

Q、Actor 网络 π(s|θ

π

) 参数

θ

π、目标

网络 Q′、π

′

初始化目标网

络参数 Q′ 和 π

′，赋值 θ

Q′ ← θ

Q, θπ

′

← θ

π

for episode =

1, M do

初始化随

机过程 N 用于给动作添加

探索

接收初始状态 S1

for t = 1,

T do

选择

动作 At = π(St|θ

π

) + Nt

执行动作 At

得到奖励

Rt，转移到下一状态 St+1

存储状

态转移数据对 (St, At, Rt,

Dt, St+1) 到 D

462

B.3

深度强

化学习

令 Yi = Ri +

γ(1 − Dt)Q′

(St+1, π′

(St+1|θ

π

′

)|θ

Q′

)

通过最小化损

失函数更新

Critic 网络：

L =

1

N

P

i

(Yi − Q(Si

,

Ai

|θ

Q))2

通过策

略梯度的方式更新 Actor 网络

：

∇θπ J ≈

1

N

P

i ∇aQ(s, a|θ

Q)|s=Si,a=π(Si)∇θπ π(s|θ

π

)|Si

更新目标网络：

θ

Q′ ← ρθQ

+ (1 − ρ)θ

Q′

θ

π

′ ← ρθπ + (1

− ρ)θ

π

′

end for

end for

孪生延迟

DDPG（Twin Delayed DDPG，TD3）是一个更先进的基于 DDPG

的

算法，它使用孪生

动作价

值网络，并对策略和目标

网络采用延迟更新。

主要

思想：

• Double Q-learning；

• 对目标网络和策略

的延迟更新；

• 对目标策略

的平滑正则化。

算法 B.65

TD3

超参

数：软更新因子 ρ，回报折扣

因子 γ，截断因子 c

输入：回放

缓存

D，初始化 Critic 网络 Qθ1

, Qθ2

参数 θ1, θ2，初

始化 Actor 网络 πϕ

参数 ϕ

初始化目

标网络参数 ˆθ1 ← θ1,

ˆθ2 ← θ2, ϕˆ ← ϕ

for t = 1 to T

do do

选择动作 At ∼ πϕ(St)

+ ϵ, ϵ ∼ N (0,

σ)

接

受奖励 Rt 和新状态 St+1

存储状

态转移数据对

(St, At, Rt, Dt, St+1) 到

D

从 D 中采

样大小为 N 的小批量样本

(St,

At, Rt, Dt, St+1)

a˜t+1 ←

πϕ

′ (St+1) + ϵ, ϵ

∼ clip(N (0, σ, ˜ −c,

c))。

y ← Rt + γ(1

− Dt) mini=1,2 Qθi

′ (St+1,

a˜t+1)

更新 Critic 网络 θi ←

arg minθi N −1 P(y −

Qθi

(St, At))2

if t mod

d then

更新 ϕ：

∇ϕJ(ϕ) =

N −1 P∇aQθ1

(St, At)|At=πϕ(St)∇ϕπϕ(St)

更新目标

网络：

ˆθi ← ρθi + (1 −

ρ)

ˆθi

ϕˆ ← ρϕ +

(1 − ρ)ϕˆ

463

附录 B

算法速查表

end if

end for

柔

性

Actor-Critic（Soft Actor-Critic，SAC）是一个更先进的基于

DDPG 的算法，使用额外的

柔性

熵（Soft Entropy）项来促进探索。

主要思

想：

• 熵正则化来促进探索

；

• Double Q-learning；

•

再参数化技巧使得随机

性策略可微并用确定性

策略梯度更新；

• Tanh 高斯型动

作分布。

算法 B.66

SAC

超参数: 目标

熵 κ, 步长 λQ,

λπ, λα, 指数移动平均系

数 τ

输入: 初始策略函数参

数

θ, 初始 Q 值函数参数 ϕ1 及

ϕ2

D = ∅; ϕ˜

i

= ϕi

, for i =

1, 2

for k = 0,

1, 2, · · · 。

do

for t = 0, 1,

2, · · · do

从

πθ(·|St)

中取样 At, 保存 (Rt, St+1)。

D

= D ∪ {St, At, Rt,

St+1}

end for

进行多步梯

度更新：

ϕi =

ϕi − λQ∇JQ(ϕi) for i =

1, 2

θ = θ −

λπ∇θJπ(θ)

α = α − λα∇J(α)

ϕ˜

i = (1 − τ

)ϕi + τϕ˜

i for i

= 1, 2

end for

返回

θ, ϕ1, ϕ2。

信赖域策略

优化（Trust Region Policy

Optimization，TRPO）是一个使用二阶梯

度下降和在线

策略更新

的信赖域算法。

主要思想

：

• 用 KL

散度（KL-divergence）来使得新旧策略

在策略空间中接近；

• 有限

制的二阶优化方法；

• 使用

共轭梯度（Conjugate Gradient）来避免计算逆

矩阵（Inverse

Matrix）。

算法 B.67 TRPO

超参数: KL-散度上

限

δ, 回溯系数 α, 最大回溯步

数 K

464

B.3 深度强化学习

输入: 回

放缓存 Dk, 初始策略函数参

数

θ0, 初始价值函数参数 ϕ0

for episode =

0, 1, 2, · · ·

do

在

环境中执行策略 πk = π(θk) 并保存

轨迹集

Dk = {τi}

计算将得到的奖

励 Gˆ

t

基于当前的价值函数

Vϕk 计算优势函数估计 Aˆ

t （使用

任何估计优势的方法）

估

计策略梯度

gˆk =

1

|Dk|

P

τ∈Dk

PT

t=0 ∇θ log πθ(At|St)

θk

Aˆ′

t

使用共轭梯

度算法计算

xˆk ≈ Hˆ −1

k

gˆk

这里 Hˆ

k 是样本

平均 KL 散度的

Hessian 矩阵

通过回

溯线搜索更新策略 θk+1 = θk

+ α

j

q 2δ

xˆT

k Hˆ kxˆk

xˆk 这里

j 是

{0, 1, 2, · · ·

K} 中提高样本

损失并且

满足样本 KL 散度约束的最

小值

通过使用梯度下降

的算法最小化均方误差

来拟合价值函数：ϕk+1

= arg minϕ

1

|Dk|T

P

τ∈Dk

PT

t=0 

Vϕ(St) −

Gˆ

t

2

end for

近端策

略优化（惩罚型）（Proximal

Policy Optimization，PPO-Penalty）是一个基

于 TRPO 的

信赖域算法，它使用

一阶梯度和以一个自适

应惩罚项实现的信赖域

限制。

主要思想：

• 用 KL 散度来

使得新旧策略在策略空

间中接近；

• 将受限优化问

题转化为一个不受限的

问题；

• 用一阶方法来避免

计算 Hessian 矩阵；

• 自适应地调整

惩罚系数。

算法 B.68 PPO-Penalty

超参数: 奖

励折扣因子 γ，KL

散度惩罚系

数 λ，适应性参数 a = 1.5, b

= 2, 子迭代次

数 M, B。

输入:

初始策略函数参

数 θ、初始价值函数参数 ϕ。

for k =

0, 1, 2, · · ·

do

执

行 T 步策略 πθ，保存 {St,

At, Rt}。

估计优势

函数 Aˆ

t =

P

t′>t γ

t

′−tRt

′

− Vϕ(St)。

πold ← πθ

for

m ∈ {1, · · ·

, M} do

JPPO(θ) = PT

t=1

πθ(At|St)

πold(At|St)Aˆ

t − λEˆ

t



DKL(πold(·|St)∥πθ(·|St))

使用梯度算法基于

JPPO(θ) 更新策略函数参数 θ。

end for

for b ∈ {1,

· · · , B} do

L(ϕ) = −

PT

t=1 P

t

′>t γ

t

′−tRt

′

− Vϕ(St)

2

使用

梯度算法基于 L(ϕ) 更新价值

函数参数

ϕ。

465

附录 B 算法速查

表

end

for

计算 d = Eˆ

t

DKL(πold(·|St)∥πθ(·|St))

if d < dtarget/a then

λ ← λ/b

else if d

> dtarget × a then

λ

← λ × b

end if

end for

近端策略优化（截

断型）是一个基于 TRPO 的信赖

域算法，它使用一阶梯度

和以一个对梯度

的截断

方法实现的信赖域限制

。

主要思想：

• 在目标函数中

用截断方法替换 KL-散度的

限制。

算法 B.69

PPO-Clip

超参数: 截断因

子 ϵ, 子迭代次数 M,

B。

输入: 初始

策略函数参数 θ, 初始价值

函数参数 ϕ

for k = 0, 1, 2,

· · · do

在环境中执行

策略 πθk

并保存轨迹集 Dk = {τi}

计算

将得到的奖励 Gˆ

t

基于当前

的价值函数 Vϕk 计算优势函

数 Aˆ

t（基于任何优势函数的

估计方法）

for m ∈ {1, · ·

· , M} do

ℓt(θ

′

) = πθ(At|St)

πθold (At|St) 采用

Adam 随机梯度

上升算法最大化 PPO-Clip 的目标

函数来更新策略：

θk+1 =

arg max

θ

1

|Dk|T

X

τ∈Dk

X

T

t=0

min(ℓt(θ

′

)A

πθold (St, At),

clip(ℓt(θ

′

), 1 − ϵ, 1 +

ϵ)A

πθold (St, At))

end for

for b ∈ {1, · ·

· , B} do

采用梯

度下降方法最小化均方

误差来学习价值函数:

ϕk+1

= arg minϕ

1

|Dk|T

P

τ∈Dk

PT

t=0 

Vϕ(St) −

Gˆ

t

2

end for

end

for

使

用 Kronecker 因子化信赖域的 Actor-Critic（Actor Critic

using Kronecker-Factored Trust

Region，ACKTR）是一

种信赖域在线策略算法

，对二阶自然梯度计算使

用 Kronecker 因子化近似。

主要思想

：

466

B.4 高等深度强化学习

• 使用

自然梯度的二阶优化；

• 对

自然梯度进行 K-FAC 近似。

算法

B.70 ACKTR

超参数: 步长 ηmax, KL-散度上限 δ

输

入:

空回放缓存 D, 初始策略

函数参数 θ0, 初始价值函数

参数 ϕ0

for k = 0, 1, 2,

· · · do

在环境中执行策略

πk =

π(θk) 并保存轨迹集 Dk = {τi

|i

= 0, 1, · · ·

}

计算累积

奖励 Gt

基于当前的价值函

数 Vϕk 计算优势函数

Aˆ

t（基于任

何优势函数的估计方法

）

估计策略梯度 gˆk =

1

|Dk|

P

τ∈Dk

PT

t=0 ∇θ

log πθ(At|St)

θk

Aˆ

t

for l =

0, 1, 2, · · ·

do

vec(∆θ

l

k

) =

vec(A−1

l ∇θ

l

k

gˆkS

−1

l

) 这里 Al =

E[ala

T

l

], Sl =

E[(∇sl

gˆk)(∇slgˆk)

T]（Al

,Sl 通

过

计算片段的滚动平均值

所得），al

是第 l 层的输入激活

向量，sl = Wlal，vec(·) 是把矩

阵变换成一

维向量的向量化变换

end for

由

K-FAC 近似自然梯度来更新策

略：θk+1 =

θk + ηk∆θk 这里 ηk =

min 

ηmax,

q 2δ

θ

T

k Hˆ kθk



，

Hˆ l

k = Al ⊗

Sl

采用 Gauss-Newton 二阶梯度下

降方法（并使用 K-FAC 近似）最小

化均方误差来学习价值

函

数: ϕk+1 = arg minϕ

1

|Dk|T

P

τ∈Dk

PT

t=0

Vϕ(St)

− Gt

2

end for

B.4

高等深度强化学习

B.4.1 模仿学习

DAgger

算法 B.71 DAgger

初始化 D ← ∅

初

始化策略 πˆ1

为策略集 Π 中任

意策略

for i =

1, 2, · · · ,

N do

πi ← βiπ

∗

+ (1 − βi)ˆπi

用 πi

采样几个 T 步的

轨迹

得到由 πi 访问的策略

和专家给出的动作组成

的数据集

Di = {(s, π∗

(s))}

聚合数据集：D

← D ∪ Di

46

附

录

B 算法速查表

在 D 上训练

策略 πˆi+1

end for

返回策略 πˆN+1

B.4.2 基于模型

的强化学习

Dyna-Q

算法 B.72 Dyna-Q

初始化

Q(s, a)

和 Model(s, a)，其中 s ∈ S，a

∈ A

while(true):

(a) s ←

当前（非终止）状态

(b) a ← ϵ-greedy(s, Q)

(c)

执行决策行为 a; 观测奖励

r, 获得下一个状态 s

′

(d) Q(s, a) ← Q(s, a)

+ α



r + γ

maxa′ Q(s

′

, a′

)

− Q(s, a)



(e) Model(s,

a) ← r, s′

(f) 重复

n 次

:

s ← 随机历史观测状态

a

← 在状

态 s 下历史随机决策行为

r, s′ ←

Model(s, a)

Q(s, a) ← Q(s,

a) + α



r +

γ maxa′ Q(s

′

, a′

) − Q(s, a)



朴素蒙特卡罗搜索（Simple

Monte Carlo Search）

算法

B.73 朴素蒙特卡罗搜索

固定

模型

M 和模拟策略 π

for 每个动

作 a

∈ A do

for 每个片段 k

∈ {1, 2, · · ·

, K} do

根据模型 M 和

模拟策略

π, 从当前状态 St 开

始在环境中展开

记录轨

迹 {St,

a, Rk

t+1, Sk

t+1, Ak

t+1, Rk

t+2, · · ·

S

k

T

}

计算从每个 St

开始的累

积奖励 Gk

t =

PT

j=t+1

Rk

j

end for

Q(St, a)

= 1

K

P

K

k=1

Gk

t

end for

返回当前最大 Q

值

的动作 At = arg maxa∈A Q(St,

a)

468

B.4 高等深度强化学

习

蒙特卡罗树搜索（Monte Carlo

Tree Search）

算法

B.74 蒙特卡罗树搜索

固定模

型 M

初始化模拟策略 π

for 每个

动作 a ∈

A do

for 每个片段 k ∈

{1, 2, · · · ,

K} do

根据模型

M 和模拟策略 π 从当前状态

St

在环境中展开

记录轨迹

{St, a, Rt+1, St+1, At+1,

Rt+2, · · · ST }

用从 (St, At)，At = a 开始的平均回报更

新每个

(Si

, Ai), i = t,

· · · , T 的

Q 值

由当前的 Q 值

更新模拟策略 π

end for

end for

返回当前

最大 Q

值的动作 At = arg maxa∈A Q(St,

a)

Dyna-2

算法 B.75 Dyna-2

function

LEARNING

初始

化 Fs 和 Fr

θ

← 0 # 初始化长期存储空

间中网络参数

loop

s

← S0

θ ← 0 #

初始化短

期存储空间中网络参数

z ← 0 # 初始化资格迹

SEARCH(s)

a ← π(s; Q) # 基于和

Q 相

关的策略选择决策动作

while s 不是终结状态 do

执行

a, 观测

奖励 r 和下一个状态 s

′

(Fs, Fr) ← UpdateModel(s, a, r,

s′

)

SEARCH(s

′

)

a

′ ← π(s

′

; Q)

# 选择

决策动作使其用于下一

个状态 s

′

δ ←

r + Q(s

′

, a′

) − Q(s, a) # 计算

TD-error

θ ← θ + α(s,

a)δz # 更新长期存

储空间中网络参数

z ← λz

+ ϕ # 更新

资格迹

s ←

s

′

, a ← a

′

end while

469

附录 B

算法速查表

end loop

end function

function SEARCH(s)

while 时间周期内 do

z ← 0

# 清除短期存

储的资格迹

a ← π(s; Q)

# 基于和 Q 相关

的策略决定决策动作

while s

不

是终结状态 do

s

′ ← Fs(s,

a) # 获得下一个

状态

r ← Fr(s,

a) # 获得奖励

a

′ ←

π(s

′

; Q)

δ ←

R + Q(s

′

, a′

) − Q(s, a) # 计算

TD-error

θ ← θ + α(s,

a)δz # 更新

短期存储空间中网络参

数

z ← λz

+ ϕ # 更新短期存储的资格

迹

s ←

s

′

, a ← a

′

end while

end while

end

function

B.4.3 分层强化学习

战略专

注作家（STRategic Attentive Writer，STRAW）

算法 B.76 STRAW 中的计划更

新

if gt

= 1 then

计算动作-计划的注意

力参数 ψ

A

t = f

ψ(zt)

应用专注阅读：βt =

read(At−1

, ψA

t

)

计

算中间表示

ϵt = h(concat(βt, zt))

计算承诺-计

划的注意力参数 ψ

c

t = f

c

(concat(ψ

A

t

, ϵt))

更新 At

= ρ(At−1

) + write(f

A(ϵt),

ψA

t

)

更

新 ct =

Sigmoid(b + write(e, ψc

t

))

else

更新 At = ρ(At−1

)

更新 ct = ρ(ct−1)

end if

470

B.4 高等深度强

化学习

B.4.4 多智能体强化学

习

多智能体

Q-Learning（Multi-Agent Q-Learning）

算法 B.77 多智能

体一般性 Q-learning

设定 Q 表格中初

始值 Qi(s, ai

,

a−i) = 1, ∀i ∈ {1,

2, · · · , m}

for episode = 1 to M

do

设定初始状态 s = S0

for

step = 1 to T do

每个

智能体 i 基于 πi(s) 选择决策行

为 ai

, 其行为是根据当前 Q 中

所有智能体混合纳什均

衡决策策略

观测经验 (s,

ai

, a−i

, ri

,

s′

) 并

将其用于更新 Qi

更新状态

s =

s

′

end for

end for

多智能体深度确定性策

略梯度（Multi-Agent Deep Deterministic Policy Gradient，MADDPG）

算法

B.78 多智能体深

度确定性策略梯度

for episode = 1

to M do

设定

初始状态 s =

S0

for step = 1 to

T do

每个智能体 i 基

于当前决策策略 πθi

选择决

策行为 ai

同时执行所有智

能体的决策行为 a = (a1,

a2, · · · , am)

将 (s, a, r, s′

)

存在

回放缓冲区 M

更新状态 s = s

′

for 智

能体 i = 1

to m do

从回访缓冲区 M 中采

样批量历史经验数据

对

于行动者和批判者网络

，计算网络参数梯度并根

据梯度更新参数

end for

end for

end

for

471

附录 B 算

法速查表

B.4.5

并行计算

异步

优势 Actor-Critic（Asynchronous Advantage Actor-Critic，A3C）

算法

B.79 异步优势 Actor-Critic (Actor-Learner)

超参

数: 总探索步数

Tmax，每个周期

内最多探索步数 tmax

初始化

步数 t = 1

while T ⩽ Tmax do

初始化网络参数梯

度:

dθ = 0 和 dθv =

0

和参数服务器保持

同步并获得网络参数 θ

′ = θ

和

θ

′

v = θv

tstart

= t

设定每个探索周期初始

状态 St

while 达到终结状态

or t − tstart == tmax

do

基于

决策策略 π(St|θ

′

) 选择决策行为

at

在环境中采取决策行为

，获得奖励 Rt 和下一个状态

St+1

t = t

+ 1, T = T +

1。

end while

if 达到终结状态 then

R = 0

else

R =

V (St|θ

′

v

)

end

if

for i = t −

1, t − 2, · ·

· , tstart do

更新折扣

化奖励 R

= Ri + γR

积累参数梯度 θ

′

, dθ = dθ +

∇θ

′ log π(Si

|θ

′

)(R − V (Si

|θ

′

v

))

积

累参数梯度 θ

′

v

, dθv = dθv + ∂(R

− V (Si

|θ

′

v

))2/∂θ′

v

end for

基于梯度 dθ

和

dθv 异步更新 θ 和 θv

end

while

分布式近端

策略优化（Distributed Proximal Policy Optimization，DPPO）

算法

B.80 DPPO (chief)

超参数: workers 数

目

W, 可获得梯度的 worker 数目门

限值 D, 次迭代数目

M, B

输入: 初

始全局策略网络参数 θ, 初

始全局价值网络参数

ϕ

for k = 0, 1,

2, · · · do

for

m ∈ {1, · · ·

, M} do

472

B.4 高

等深度强化学习

等待至

少可获得 W − D 个 worker

计算出来梯

度 θ，去梯度的均值并更新

全局梯度 θ

end for

for

b ∈ {1, · · ·

, B} do

等待至少可获

得 W −

D 个 worker 计算出来梯度 ϕ，去梯

度的均值并更新全局梯

度 ϕ

end for

end for

算法 B.81

DPPO (PPO-Penalty worker)

超参数: KL 惩罚系数

λ,

自适应参数 a = 1.5, b =

2, 次迭代数目

M, B

输入: 初始局部策略网络

参数 θ,

初始局部价值网络

参数 ϕ

for k = 0,

1, 2, · · · do

通过在环境中采用

策略 πθ 收集探索轨迹 Dk = {τi}

计算

rewards-to-go Gˆ

t

基于当前价值函数 Vϕk 计算

对

advantage 的估计，Aˆ

t（可选择使用任

何一种 advantage 估计方

法）

存储部

分轨迹信息

πold ← πθ

for m

∈ {1, · · · ,

M} do

JPPO(θ) = PT

t=1

πθ(At|St)

πold(At|St)Aˆ

t − λKL[πold|πθ] −

ξ max(0, KL[πold|πθ] − 2KLtarget)

2

if KL[πold|πθ] > 4KLtarget then

break

并继续开始

k + 1 次迭代

end if

计算 ∇θJPPO

发送梯度数

据 θ 到 chief

等待梯度被接受或

被舍弃，更新网络参数

end for

for b ∈

{1, · · · , B}

do

L(ϕ) = −

PT

t=1(Gˆ

t − Vϕ(St))2

计

算 ∇ϕL

发送梯度数据

ϕ 到 chief

等待

梯度被接受或被舍弃，更

新网络参数

end for

计算 d = Eˆ

t

KL[πold(·|St), πθ(·|St)]

if d < dtarget/a

then

λ ← λ/b

473

附录

B 算

法速查表

else if d >

dtarget × a then

λ ←

λ × b

end if

end

for

算法 B.82 DPPO (PPO-Clip worker)

超参数: clip 因

子 ϵ, 次迭代数目 M,

B

输入: 初始

局部策略网络参数 θ, 初始

局部价值网络参数 ϕ

for k = 0, 1, 2,

· · · do

通过

在环境中采用策略 πθ

收集

探索轨迹 Dk = {τi}

计算 rewards-to-go

Gˆ

t

基于当前

价值函数 Vϕk 计算对 advantage

的估计

，Aˆ

t（可选择使用任何一种 advantage 估

计方

法）

存储部分轨迹信

息

πold ← πθ

for m ∈

{1, · · · , M}

do

通过最大化 PPO-Clip 目标更新

策略:

JPPO(θ) =

1

|Dk|T

P

τ∈Dk

PT

t=0

min 

πθ(At|St)

πold(At|St)Aˆ

t(

π(At|St)

πold(At|St)

, 1 − ϵ, 1

+ ϵ)Aˆ

t



计算 ∇θJPPO

发送梯度数据

θ 到 chief

等待梯度被接受或被

舍弃，更新网络参数

end for

for b ∈ {1, · ·

· , B} do

通过

回归均方误差拟合价值

方程:

L(ϕ)

= −

1

|Dk|T

P

τ∈Dk

PT

t=0 

Vϕ(St) − Gˆ

t

2

计算 ∇ϕL

发送梯度数据

ϕ 到

chief

等待梯度被接受或被

舍弃，更新网络参数

end for

end for

Ape-X

算法

B.83 Ape-X (Actor)

超参数: 单次批量发送到

回放缓冲区的数据大小

B，迭代数目

T

与学习者同步

并获得最新的网络参数

θ0

从环境中获得初始状态

S0

474

B.4 高等深度强化学习

for t = 0, 1, 2,

· · · , T −

1 do

基于

决策策略 π(St|θt) 选择决策行为

At

将经验

(St, At, Rt, St+1) 加入当地缓冲区

if 当地缓冲区存储数据达

到数目门限值

B then

批量获得

缓冲数据 B

计算获得缓冲

数据的优先级 p

将批量缓

冲数据和其更新的优先

级发送回放缓冲区

end if

周期

性同步并更新最新的网

络参数 θt

end

for

算法 B.84 Ape-X (Learner)

超参数:

学习

周期数目 T

初始化网络参

数 θ0

for t

= 1, 2, 3, · ·

· , T do

从回放缓冲区中批量

采样带有优先级的数据

(i, d)

通过批数据进行模型训

练

更新网络参数 θt

对于批

数据 d 计算优先级

p

更新回

放缓冲区中索引 i 数据的

优先级 p

周期性地从回放

缓冲区中删除低优先级

的数据

end for

475

附录C 中英文对照

表

中文

英文 缩写

机器学

习基础

人工智能 Artificial Intelligence

AI

机器学

习 Machine Learning ML

深度学校

Deep Learning DL

多层感知器

Multilayer Perceptron MLP

深度神经网络 Deep Neural Networks DNN

卷积神经

网络

Convolutional Neural Network CNN

循环神经网络 Recurrent

Neural Network RNN

人工

神经网络 Artificial Neural

Network ANN

长短期记忆 Long Short-Term Memory

LSTM

单

元 Cell

偏差 Bias

隐藏状态

Hidden State

单元状

态 Cell State

隐藏层

Hidden Layer

批大小 Batch Size

小批量

Mini-Batch

整流线性单元 Rectified Linear Unit ReLU

476

附录 C 中英

文对照表

中文 英文 缩写

指数线性单元

Exponential Linear Unit ELU

梯度下降

Gradient Descent

随机梯度下降 Stochastic Gradient Descent SGD

输出层

Output Layer

权

重 Weight

引理 Lemma

步长 Step Size

步幅 Stride

超参数

Hyperparameter

输入 Input

输出 Output

初始化 Initialize/Initialization

更新 Update

协

方差 Covariance

交叉验证 Cross-Validation

过度拟合

Overfitting

欠拟合 Underfitting

权重衰减 Weight Decay

集成学

习 Ensemble Learning

自动编码器 Autoencoder AE

变分自动

编码器 Variational Autoencoder VAE

生成对抗网络 Generative

Adversarial Networks GANs

全

连接 Fully-Connected FC

密集层，亦称全连接

层 Dense Layer

朴素贝叶斯 Naive Bayes

线性回归

Linear Regression

折页损失函数 Hinge Loss

KL

散度 Kullback-Leibler Divergence KL Divergence

多类

别

Multinomial

独热码 One-Hot

学习率 Learning Rate

前向传

播 Forward Propagation

反向传播 Backward Propagation

477

附录 C 中英文

对照表

中文 英文

缩写

批

标准化 Batch Normalization

分对数 Logit

对数概率

Log Probability

线段树 Segment Tree

张量

Tensor

早停法 Early Stopping

数据

增强 Data

Augmentation

强化学习基础

状态

State

状态集 State Set

动作 Action

动作集合 Action Set

观

测

Observation

轨迹 Trajectory

智能体 Agent

奖励

Reward

环境

Environment

回报 Return

转移 Transition

长期回报 Long-Term Return

短期

回报 Short-Term Return

探索-利用的权衡 Exploration-Exploitation Trade-Off

确

定性转移过程 Deterministic Transition

Process

随机性转

移过程 Stochastic Transition Process

状态转移矩阵

State Transition Matrix

基

准 Baseline

部分可观测的

Partially Observable

完全可

观测的 Fully Observable

立即奖励

Immediate Reward

累积奖

励 Cumulative Reward

非折扣化的回报

Undiscounted Return

折扣

化回报 Discounted Return

期望回报

Expected Return

478

附录 C 中

英文对照表

中文 英文 缩

写

起始状态分布 Start-State Distribution

行动者

Actor

批判者 Critic

基于模型的 Model-Based

无模

型的

Model-Free

基于价值的 Value-Based

基于策

略的 Policy-Based

既定策略

On-Policy

新定策略

Off-Policy

在线策略 On-Policy

离线策略 Off-Policy

规划

Planning

试错过程 Trial-and-Error Process

自省法 Introspection

时间差

分 Temporal Difference TD

正向运动学 Forward

Kimematics

反向运动

学 Inverse Kinematics

马尔可夫 Markov

马尔可夫链

Markov Chain

马尔可夫性质 Markov Property

时间同质

性

Time-Homogeneous

时间不同质 Time-Inhomogeneous

折扣因子

Discount Factor

赌博机

Bandit

单臂赌博机 Single-Armed Bandit

多臂

赌博机 Multi-Armed

Bandit MAB

健忘对抗者 Oblivious Adversary

非健

忘对抗者

Non-Oblivious Adversary

全信息博弈 Full-Information Game

部

分信息博弈

Partial-Information Game

概率图模型

Probabilistic Graphical Model

观察变量

Observed Variable

蒙特卡罗 Monte Carlo MC

479

附录

C 中英文对照表

中文 英文

缩写

首次蒙特卡罗

First-Visit Monte Carlo

每次

蒙特卡罗 Every-Visit Monte

Carlo

动态规划 Dynamic Programming DP

逆矩

阵方法

Inverse Matrix Method

探索和利用 Exploration and

Exploitation

回放

缓存 Replay Buffer

自举 Bootstrap

穷举法 Exhaustive Method

非终结

Non-Terminal

强化学习 Reinforcement

Learning RL

高等强化学习

Advanced Reinforcement Learning

深度强化学习

Deep Reinforcement Learning DRL

回合/片段

Episode

回溯

Backup

崩溃 Collapse

截断 Clipped

贝尔曼方

程

Bellman Equation

贝尔曼期望方程 Bellman Expectation Equation

贝尔

曼最优方程 Bellman Optimality Equation

贝尔曼最优

回溯算子 Bellman

Optimality Backup Operator

批量 Batch

函数拟合

器

Function Approximator

马尔可夫过程 Markov Process MP

马尔可

夫奖励过程 Markov Reward Process MRP

奖励函数

Reward Function

奖

励折扣因子 Reward Discount Factor

马尔可夫决

策过程 Markov Decision Process MDP

有限范围马尔可

夫决策过程

Finite-Horizon Markov Decision Process

部分可观测

的马尔可夫决策

过程

Partially Observed Markov Decision Process POMDP

贪

心策略 Greedy Policy

ϵ-贪心 ϵ-Greedy

后悔值

Regret

480

附录

C 中英文对照表

中文 英文

缩写

置信上界 Upper Confidence Bound UCB

树置信上

界

Upper Confidence Bound in Tree UCT

雅达利游戏 Atari Game

价值函数

Value Function

Q

值函数 Q-Value Function

动作价值函数 Action-Value Function

在

线价值函数 On-Policy Value Function

最优价值函

数 Optimal

Value Function

在线动作价值函数 On-Policy Action-Value Function

最

优动作价值函数 Optimal Action-Value Function

查找表

Lookup Table

多项式族 Polynomial Family

多项式基 Polynomial Basis

傅立

叶基 Fourier Basis

傅立叶变换 Fourier Transformation

粗略编

码 Coarse Coding

瓦式编码 Tile Coding

感知域 Receptive Field

径向

基函数 Radial Basis

Function RBF

决策树 Decision Tree

最近邻

Nearest Neighbor

半

梯度 Semi-Gradient

死亡三件套 the

Deadly Triad

过估计

Over-Estimation/Over-Estimate

欠估计 Under-Estimation/Under-Estimate

均方误差

Mean Squared Error MSE

平均绝

对误差 Mean

Absolute Error MAE

策略梯度 Policy Gradient

PG

确定性

策略 Deterministic Policy

随机性策略分布 Stochastic

Policy Distribution

确

定性策略梯度 Deterministic Policy Gradient

DPG

随机性策

略梯度 Stochastic Policy Gradient SPG

条件概率分布 Conditional Probability Distribution

481

附

录

C 中英文对照表

中文 英

文 缩写

初版策略梯度

Vanilla Policy Gradient VPG

参

数化策略 Parameterized

Policy

伯努利分布 Bernoulli Distribution

类

别分布 Categorical

Distribution

对角高斯分布 Diagonal Gaussian Distribution

二

值化动作策略

Binary-Action Policy

类别型策

略 Categorical Policy

逐个元素的乘积

Element-Wise Product

耿贝

尔分布 Gumbel Distribution

耿贝尔-Softmax

函数 Gumbel-Softmax

耿贝

尔-最大化函数 Gumbel-Max

不可微的

Non-Differentiable

逆变换

Inverse Transform

对角高斯策略 Diagonal Gaussian Policy

累

计折扣奖励 Cumulative Discounted Reward

折扣状态分

布 Discounted

State Distribution

转移概率 Transition Distribution

对数-导数技

巧

Log-Derivative Trick

对数 Logarithm

将得到的奖励 Reward-to-Go

偏

微分 Partial Derivative

贯穿时间的反向传

播 Backpropagation Through

Time BPTT

莱布尼茨积分法则 Leibniz Integral Rule

富

比尼定理 Fubini’s Theorem

积测度 Product Measure

可测函

数 Measurable Function

紧致性 Compactness

被积函数

Integrand

行为

策略 Behaviour Policy

约等于 Approximately

Equivalent

常规 delta-近似 Regular delta-Approximation

利

普希茨

Lipschitz

目标网络 Target Network

482

附录

C 中

英文对照表

中文 英文 缩

写

得分函数

Score Function

路径导数 Pathwise Derivative

再

参数化

Reparametrization

随机价值梯度 Stochastic Value Gradient SVG

协

方差矩阵自适应 Covariance Matrix Adaptation CMA

协方差

矩阵自适应进化策略

Covariance Matrix Adaptation Evolution

Strategy

CMA-ES

爬

山法 Hill Climbing

选择比率 Selection Ratio

兼容函数

近似 Compatible Function Approximation

优势函数 Advantage

Function

中央处理

器 Central Processing Unit CPU

图形处理器 Graphics Processing Unit GPU

样本效率

Sample

Efficiency

高样本效率的 Sample-Efficient

灾难性遗

忘 Catastrophic Interference/Forgetting

元学习 Meta-Learning

表征学习 Representation Learning

多智

能体强化学习

Multi-Agent Reinforcement Learning MARL

模拟到现

实 Simulation-to-Reality

Sim2Real,

Sim-to-Real

信赖域 Trust Region

共轭梯度

Conjugate Gradient

自然

梯度 Nature Gradient

变分推断

Variational Inference VI

专家示范

Expert Demonstrations

模仿学习

Imitation Learning IL

交叉熵 Cross Entropy

CE

分层强

化学习 Hierarchical Reinforcement Learning HRL

封建制强化学习

Feudal Reinforcement Learning

无行动者 Actor-Free

逆向强化学习

Inverse

Reinforcement Learning IRL

行为克隆 Behavioral Cloning

BC

483

附录 C 中英文对

照表

中文

英文 缩写

学徒

学习 Apprenticeship Learning

从观察量进行模仿

学习

Imitation Learning from Observations IfO/ILFO

高斯混合模型回归

Gaussian

Mixture (Model) Regression GMR

高斯过程回归 Gaussian

Process Regression

因果熵 Causal Entropy

协

变量漂移

Covariate Shift

复合误差 Compounding Errors

数据

集聚合

Dataset Aggregation DAgger

无悔的 No-Regret

动态运动

基元

Dynamic Movement Primitives DMP

单样本的 One-Shot

最大熵逆

向强化学习 Maximum Entropy Inverse Reinforcement

Learning

MaxEnt IRL

奖励塑形 Reward Shaping

生

成对抗模仿学习

Generative Adversarial Imitation Learning GAIL

辨别器

Discriminator

多模态的 Multi-Modal

指导性代价学

习 Guided Cost Learning

GCL

生成对抗网络指导性

代价学

习

Generative Adversarial Network

Guided

Cost Learning

GAN-GCL

极大似然估计

Maximum Likelihood

Estimation MLE

以轨迹为中心的 Trajectory-Centric

以状态

为中心的 State-Centric

玻尔兹曼分布

Boltzmann Distribution

配分函数 Partition Function

重要性采样

Importance Sampling

对

抗性逆向强化学习 Adversarial Inverse Reinforcement

Learning AIRL

互信

息 Mutual Information

时间步

Time Step

逆向动态模型

Inverse Dynamics Models

正向动态模型

Forward Dynamics Models

贝叶斯优

化 Bayesian Optimization

BO

从观察量模仿潜在策

略 Imitating Latent Policies from

Observation ILPO

484

附录 C 中英文对照表

中

文 英文 缩写

选项框架 Options Framework

本

体感觉 Proprioceptive

线性二次型调节

器 Linear Quadratic Regulator

LQR

极小化极大 Minimax

从观察量

进行行为克隆 Behavioral Cloning

from Observation BCO

正向对抗

式模仿学习 Forward Adversarial

Imitation Learning FAIL

动作指导性

对抗式模仿学习 Action-Guided Adversarial

Imitation

Learning

AGAIL

增强逆

向动态建模 Reinforced Inverse

Dynamics Modeling RIDM

奖励函数工

程 Reward Engineering

欧氏距离 Euclidean Distance

时间对比网

络 Time-Contrastive Networks

TCN

具象不匹配 Embodiment Mismatch

概率性运

动基元 Probabilistic

Movement Primitives ProMP

核运动基元 Kernelized Movement

Primitives KMP

高斯

过程回归 Gaussian Process Regression

GPR

高斯混合模型

Gaussian Mixture Model GMM

策略替换

Policy Replacement

残差策略学习

Residual Policy Learning

基于示范的深度

Q-learning Deep Q-learning from Demonstrations DQfD

基于示

范的深度确定性策略

梯

度

Deep Deterministic Policy Gradient

from

Demonstrations

DDPGfD

标准化 Actor-Critic Normalized

Actor-Critic NAC

最先进的 State-of-the-Art SOTA

用示

范数据进行奖励塑形

Reward Shaping with Demonstrations

对

比正向动态 Contrastive

Forward Dynamics CFD

内在奖励 Intrinsic Reward

封

建制网络 Feudal Network FuN

基于族群的训

练 Population-Based

Training PBT

通用性 Generality

多面性 Versatility

与模型

无关的元学习 Model-Agnostic Meta-Learning

学会学习

Learning to Learn

485

附录 C 中英文对照表

中文

英文 缩写

内循环 Inner-Loop

外循环

Outer-Loop

元学习者 Meta-Learner

度量学习

Metric Learning

元强

化学习 Meta-Reinforcement Learning

小样本学习

Few-Shot Learning

状态

表征学习 State Representation Learning

SRL

描述器 Descriptor

博弈论

Game Theory

自我博弈

Self-Play SP

优先虚拟自我

博弈 Prioritized Fictitious Self-Play

PFSP

指导性策略搜搜 Guided Policy Search GPS

比

例-积分-微分 Proportional-Integral-Derivative PID

现实鸿沟 Reality Gap

系

统识别 System Identification SI

泛化力模型 Generalized

Force Model GFM

零样

本 Zero-Shot

域自适应

Domain Adaption DA

渐进网络 Progressive Networks

动

力学随机化 Dynamics Randomization DR

随机到标准

自适应网络 Randomized-to-Canonical

Adaptation

Networks

RCANs

可扩展性 Scalability

重

要性加权的行动者-学习

者

结构

Importance Weighted Actor-Learner

Architecture

IMPALA

可扩展高效深度

强化学习 Scalable, Efficient Deep-RL SEED

社交树

Social Tree

多步学

习 Multi-Step Learning

噪声网络

Noisy Nets

值分布强化

学习 Distributional Reinforcement Learning

分布式贝尔曼算子

Distributional Bellman Operator

自适应的 Adaptive

层标准化

Layer Normalization

486

附录

C 中英文对照表

中文

英文

缩写

子迭代 Sub-Iteration

分块对角矩

阵 Block Diagonal

Matrix

无穷范式 ∞-Norm

L2 范式 L2-Norm

模拟 Simulation

评

估/估计 Evaluate

策略迭代 Policy

Iteration

策略评

估 Policy Evaluation

策略提升 Policy

Improvement

泛化策略迭

代 Generalized Policy Iteration GPI

柔性策略迭代 Soft Policy Iteration

价值迭

代 Value

Iteration

最优性原则 Principle of Optimality

优先扫描

Prioritized

Sweeping

梯度赌博机 Gradient Bandit

直接策略搜

索 Direct

Policy Search

资格迹 Eligibility Trace

延迟帧

Lazy-Frame

选项策

略 Policy-over-Action

选项内置策略 Intra-Option Policy

时域抽

象 Temporal Abstraction

专注写作 Attentive Writing

选项内置策

略梯度理论 Intra-Option Policy Gradient Theorem

奖励隐藏

Reward Hiding

信

息隐藏 Information Hiding

半马尔可夫决策

过程

Semi-Markov Decision Process SMDP

转移策略梯度 Transition

Policy Gradients

重标

记 Re-Label

原始值函数 Proto-Value

Functions PVFs

后见之明

目标转移 Hindsight Goal Transitions

终生学习 Lifelong Learning

– Ornstein-Uhlenbeck OU

斯塔

克尔伯格博弈 Stackelberg Game

487

附录 C

中英

文对照表

中文 英文 缩写

先发优势 First-Mover Advantage

演算 Roll-Out

消息传递

接口 Message Passing Interfaces

MPI

进程间通信 Inter-Process Communication IPC

预测者

Predictor

训练者 Trainer

强化学习算法

探

索和利用的指数加权算

法 Exponential-Weight Algorithm

for

Exploration and Exploitation

Exp3

单步

Q-learning One-Step Q-learning

多步 Q-learning Multi-Steps

Q-learning

深度 Q 网络 Deep Q-Networks

DQN

– Categorical 51 C51

深

度确定性策略梯度

Deep Deterministic Policy Gradient DDPG

优先

经验回放

Prioritized Experience Replay PER

后见之明经验

回放 Hindsight

Experience Replay HER

信赖域策略优化 Trust Region

Policy Optimization TRPO

近

端策略优化 Proximal Policy

Optimization PPO

分布式近端

策略优化 Distributed Proximal Policy

Optimizaion DPPO

– Actor-Critic AC

归一化

Actor-Critic Normalized Actor-Critic NAC

使用 Kronecker

因

子化信赖

域的 Actor Critic

Actor Critic

Using Kronecker-Factored Trust

Region

ACKTR

（同步）优势

Actor-Critic

Synchronous Advantage Actor-Critic A2C

异步优势 Actor-Critic

Asynchronous Advantage Actor-Critic A3C

最大化后验策

略梯度 Maximum

a Posteriori Policy Optimization MPO

期望最大化算法

Expectation

Maximization EM

拟合 Q 迭代 Fitted

Q Iteration

在线 Q 迭代 Online

Q Iteration

分位

数 QT-Opt Quantile QT-Opt

Q2-Opt

有基准的 REINFORCE REINFORCE with Baseline

孪生延迟 DDPG Twin Delayed DDPG TD3

柔

性 Actor-Critic Soft Actor-Critic SAC

488

附录 C 中英文对照表

中

文 英文 缩写

变分信息量

最大化探索 Variational Information Maximizing

Exploration

VIME

朴素蒙特卡

罗搜索 Simple Monte Carlo Search

蒙特卡罗树搜索

Monte

Carlo Tree Search MCTS

多智能体 Q-learning

Multi-Agent Q-learning

多智能体深度

确定性策略梯

度

Multi-Agent Deep

Deterministic Policy

Gradient

MADDPG

截断 Double-Q

Learning Clipped Double-Q learning

分

布式深度循环回放 DQN

Recurrent Replay Distributed DQN R2D2

回溯

-行动者

Retrace-Actor

分位数回归 DQN Quantile Regression DQN

QR-DQN

战略

专注作家 Strategic Attentive Writer STRAW

选项批判者 Option-Critic

MAXQ 分

解 MAXQ Decomposition

层次抽象机 Hierarchical Abstract Machines HAMs

使用离线

策略修正的分层强

化学

习

Hierarchical Reinforcement Learning with

Off-Policy

Correction

HIRO

细粒度动作重复 Fine Grained Action

Repetition FiGAR

通用

价值函数逼近器 Universal Value Function

Approximators UVFAs

GPU/CPU 混合式

异步优势

Actor-Critic

Hybrid

GPU/CPU Asynchronous Advantage

Actor-Critic

GA3C

其他

个人主页

Homepage

章节 Chapter

小节 Section

简介

Introduction

代码库 Repository

489

反

侵权盗版声明

电子工业

出版社依法对本作品享

有专有出版权。任何未经

权利

人书面许可，复制、销

售或通过信息网络传播

本作品的行为；歪曲、

篡改

、剽窃本作品的行为，均违

反《中华人民共和国著作

权法》，其

行为人应承担相

应的民事责任和行政责

任，构成犯罪的，将被依法

追究刑事责任。

为了维护

市场秩序，保护权利人的

合法权益，我社将依法查

处

和打击侵权盗版的单

位和个人。欢迎社会各界

人士积极举报侵权盗

版

行为，本社将奖励举报有

功人员，并保证举报人的

信息不被泄露。

举报电话

：（010）88254396；（010）88258888

传　　真：（010）88254397

E-mail： dbqq@phei.com.cn

通信地址：北京市万

寿路 173 信箱

电子工业出版

社总编办公室

邮

编：100036
