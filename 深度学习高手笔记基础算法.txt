深度学习

高

手笔记

卷 1

基

础算法

刘岩

（@ 大师兄） 著

59631-深

度学习高手

笔记 卷1：基础

算法-改邮箱

.indd 6 2022/9/19

22:04:37

59631-深度学习高

手笔记 卷1：基

础算法-改邮

箱.indd 1-2,4-5

2022/10/19 9:35:23

内 容

提 要

本书通过扎

实、详细的内

容和清晰的

结构，从算法

理论、算法源

码、实验结果

等方面对深

度学习

算法

进行分析和

介绍。本书共

三篇，第一篇

主要介绍深

度学习在计

算机视觉方

向的一些卷

积神经网络

，

从基础骨干

网络、轻量级

CNN、模型架构搜

索 3 个方向展

开，介绍计算

机视觉方向

的里程碑算

法；第

二篇主

要介绍深度

学习在自然

语言处理方

向的重要突

破，包括基础

序列模型和

模型预训练

；第三篇主

要

介绍深度学

习在模型优

化上的进展

，包括模型优

化方法。

通过

阅读本书，读

者可以深入

理解主流的

深度学习基

础算法，搭建

起自己的知

识体系，领会

算法

的本质

，学习模型优

化方法。无论

是从事深度

学习科研的

教师及学生

，还是从事算

法落地实践

的工作

人员

，都能从本书

中获益。

♦ 著 刘

岩（@大师兄）

责

任编辑 孙喆

思

责任印制

王 郁 胡 南

♦ 人

民邮电出版

社出版发行

北京市丰台

区成寿寺路

11 号

邮编 100164 电子

邮件 315@ptpress.com.cn

网址 https://www.ptpress.com.cn

 涿

州市京南印

刷厂印刷

♦ 开

本：787×1092 1/16

印张：17

2022 年 11 月

第

1 版

字数：455 千

字

2022 年 11 月河北

第

1 次印刷

定

价：109.80 元

读者服

务热线：(010)81055410 印装

质量热线：(010)81055316

反

盗版热线：(010)81055315

广

告经营许可

证：京东市监

广登字 20170147 号

谨

以此书献给

生命中的亲

人和挚友

序

1

假如问近 10 年

来计算机技

术领域最热

门的方向是

什么，人工智

能一定是候

选之一。从

1969 年

马文 • 明斯基

成为第一位

人工智能方

向的图灵奖

获得者，到

2018 年

3 位学者因在

深度学习方

向的

贡献共

同获得图灵

奖，人工智能

方向已经七

获图灵奖。近

年来，在人工

智能上升为

国家战略，并

被

广泛使用

的大背景下

，众多高校开

设了诸如大

数据、深度学

习、数据挖掘

等人工智能

学科；诸多企

业也开始使

用人工智能

赋能企业运

营，为企业提

供智能化支

撑，助力企业

实现降本增

效，人工智能

技术能力俨

然成为衡量

一个企业的

核心实力的

重要指标之

一。

我们正处

在一个信息

化和智能化

交互的时代

，人工智能、物

联网、区块链

、元宇宙等技

术创

新，既是

技术发展的

阶段性成果

，也是开启智

能化时代的

重要助推器

。更重要的是

，它们正在相

互

促进，共同

发展。人工智

能的发展经

历了机器定

理、专家系统

的两次热潮

和低谷。如今

，我们正处

在

以深度学习

为代表的第

三次人工智

能热潮中，并

且人工智能

正深刻改变

着我们的生

活。创新工场

CEO 李开复先生

曾提出过著

名的“五秒钟

原则”：一项本

来由人从事

的工作，如果

人可以在 5 秒

以

内对工作

中需要思考

和决策的问

题作出相应

决定，那么这

项工作就有

非常大的可

能被人工智

能技术

全部

或部分取代

。人工智能为

经济生活带

来了颠覆性

改变，这可能

会造成部分

岗位的消失

，但它更

多的

是引发了工

作性质的变

革，所以能否

掌握这门技

术，在第三次

人工智能浪

潮中占得先

机，决定

了一

个企业和个

人的实力与

前景。在人工

智能的步步

紧逼下，你究

竟是在焦虑

还是已经看

到了其中

潜

在的机遇，并

积极地接受

变革呢？

如果

你已经准备

好迎接到来

的第三次人

工智能浪潮

，那么本书是

你不能错过

的一本读物

。本书

全面且

系统地梳理

了近 10

年来的

深度学习算

法，并集结成

册。本书结构

清晰，内容丰

富，包含了

作

者对深度学

习深刻且独

到的见解。在

本书中，作者

将深度学习

的几十篇具

有里程碑意

义的论文整

理成卷积神

经网络、自然

语言处理和

模型优化 3 个

主要方向，又

对每个方向

的重要算法

做了深入浅

出的讲解和

分析。对比业

内同类书籍

，本书将深度

学习算法的

讲解提升到

了一个新的

高度，是你深

入了解深度

学习的不二

之选。总之，本

书极具价值

，值得每一位

深度学习方

向的从业者

、研究者和

在

校学生阅读

和学习。

颜伟

鹏

京东集团

副总裁、京东

零售技术委

员会主席

序

2

在古希腊时

期，人类就梦

想着创造能

自主思考的

机器。如今，人

工智能已经

成为一个活

跃的研

究课

题和一门在

诸多场景落

地的技术。在

人工智能发

展的早期阶

段，它更擅长

解决可具象

化为数学

规

则的问题，而

人工智能的

真正挑战在

于解决那些

对人来说很

容易执行但

非常难以描

述为具体规

则

的问题，这

就是深度学

习的诞生动

机。深度学习

是人工智能

的一个重要

分支，它以大

数据为基础

，

以数理统计

为理论框架

，涵盖了计算

机视觉、自然

语言处理、语

音识别、图深

度学习、强化

学习等

不同

方向。于 2012 年提

出的

AlexNet 开启了

深度学习蓬

勃发展的 10 年

。2022

年的 1 月 3

日，著

名论文预印

本平台 arXiv 的论

文数量突破

了 200

万篇，其中

不乏残差网

络、Transformer、GAN 等

引用量

达到数万乃

至数十万的

经典算法论

文。深度学习

崛起近 10

年，我

们有必要对

深度学习近

10

年的发展做

一些梳理和

总结。

深度学

习的发展日

新月异，从使

用基础算法

的人脸识别

、机器翻译、语

音识别、AlphaGo 等，

再

到综合各类

算法的智能

客服、推荐搜

索、虚拟现实

等，这些基于

深度学习的

技术和产品

正在以惊

人

的速度改变

着我们的工

作与生活。除

此之外，深度

学习在智联

网、无人驾驶

、智能医疗等

诸多领

域的

发展中也起

到了中流砥

柱的作用。即

使你是一个

和深度学习

无关的其他

行业从业者

，你一定也

在

不知不觉中

被深度学习

影响着，而且

你也可以借

助简单、易用

的深度学习

框架快速使

用这一前沿

技术。

本书有

别于以卷积

神经网络、循

环神经网络

等基础概念

为核心的同

类书籍，主要

以近 10 年来

深

度学习方向

诞生的经典

算法为基础

，重点讨论深

度学习在卷

积神经网络

、自然语言处

理、模型归

一

化等方向上

的发展历程

以及各个算

法的优缺点

，介绍各个算

法是如何分

析先前算法

的若干问题

并

提出解决

方案的。本书

包含作者对

深度学习的

独特见解和

全新思考，知

识丰富、架构

清晰、重点突

出、可读性好

。此外，作者借

助代码、图示

、公式等手段

，对晦涩难懂

的算法进行

深入浅出的

剖

析。相信每

位读者都能

够从本书中

汲取相应的

知识并得到

启发。

包勇军

京东集团副

总裁，京东零

售技术委员

会数据算法

通道会长

前

言

目前人工

智能（artificial intelligence，AI）在计算

机界非常火

热，而其中深

度学习（deep

learning，DL）无疑

是更为火热

的一个领域

，它在计算机

视觉、自然语

言处理、语音

识别、跨模态

分析、风控建

模等领域均

取得了突破

性的进展。而

且近年来该

领域的优秀

论文、落地项

目也层出不

穷。密切关注

深度学习领

域的进展是

每个深度学

习工作者必

不可少的工

作内容之一

，不仅为了找

工

作、升职加

薪，还为了更

好地跟随前

沿科技，汲取

算法奥妙。

2014 年

是深度学习

蓬勃发展的

一年，这一年

计算机视觉

方向诞生的

算法有

VGG、GoogLeNet、

R-CNN、DeepLab，自然

语言处理方

向诞生的有

注意力机制

、神经图灵机

、编码器 - 解码

器架构。

也就

是在这一年

，我开始了自

己的研究生

生涯，由此与

人工智能和

深度学习结

下了不解之

缘。度过

了 3 年

的求学生涯

和

4 年的工作

生涯，时间很

快来到了 2022 年

，我也有了

8 年

的人工智能

相关的

科研

与工作经历

。在这 8

年的科

研及工作中

，我既见证了

SVM、决策树、ELM 等传

统机器学习

方

法的没落

，也了解了深

度学习在各

个方向的突

破性进展。我

既发表过使

用传统机器

学习方法解

决

神经机器

翻译或者细

胞检测问题

的论文，也使

用深度学习

技术在

OCR、公式

识别、人像抠

图、文

本分类

等方向实现

了业务落地

。在这 8 年的时

间里，我读了

很多论文和

源码，也做了

很多项目和

实验。

在机缘

巧合下，我听

从朋友的建

议将几篇学

习笔记上传

到了知乎，没

想到得到了

大量的收藏

和

关注，因此

开通了本书

同名专栏。截

稿时，我在知

乎上已更新

了一百多篇

文章，也有了

几百万的阅

读量和过万

的粉丝数。为

了能帮助更

多的读者，我

将知乎专栏

下的文章经

过整理、修改

、精校、勘

误之

后完成了本

套图书。

本套

图书共两卷

，分别是卷

1 基

础算法和卷

2 经典应用。卷

1 由三篇组成

，第一篇介绍

深度学

习在

计算机视觉

方向的一些

卷积神经网

络，从基础骨

干网络（第 1 章

）、轻量级 CNN（第

2 章

）、

模型架构搜

索（第 3

章）3 个方

向展开，介绍

计算机视觉

方向的 30 余个

里程碑算法

。第二篇主要

介绍深度学

习在自然语

言处理方向

的重要突破

，主要介绍几

个基础序列

模型，如

LSTM、注意

力机

制、Transformer 等（第

4 章），以及近年

来以

BERT 为代表

的 10 余个预训

练语言模型

（第

5 章）。

第三篇

（第 6

章）将介绍

模型优化的

经典策略，分

为两个方向

，一个方向是

Dropout 及其衍生算

法，

另一个方

向是以批归

一化、层归一

化为代表的

归一化算法

。

卷

2 会对专栏

中的经典或

者前沿应用

进行总结，同

样由三篇组

成。第一篇介

绍的应用是

目

标检测与

分割，其中会

介绍双阶段

的 R-CNN

系列、单阶

段的 YOLO 系列，以

及 Anchor-Free

的

CornerNet 系列这

3 个方向的目

标检测算法

，也会介绍目

标检测在特

征融合和损

失函数方向

的迭代

优化

，最后会介绍

与目标检测

非常类似的

分割算法。第

二篇介绍深

度学习中的

OCR 系列算法，用

于场景文字

检测、文字识

别两个方向

。第三篇会介

绍其他深度

学习经典或

者前沿的应

用，例如生成

模型、图神经

网络、二维信

息识别、图像

描述、人像抠

图等。

阅读本

书时有以下

两点注意事

项：本书的内

容以经典和

前沿的深度

学习算法为

主，并没有过

多

资源与支

持

2

容的非授

权传播，请您

将怀疑有侵

权行为的链

接通过邮件

发给我们。您

的这一举动

是对作者权

益的

保护，也

是我们持续

为您提供有

价值的内容

的动力之源

。

关于异步社

区和异步图

书

“异步社区

”是人民邮电

出版社旗下

IT 专业图书社

区，致力于出

版精品 IT 专业

图书和相关

学

习产品，为

作译者提供

优质出版服

务。异步社区

创办于 2015 年 8

月

，提供大量精

品 IT 专业图书

和

电子书，以

及高品质技

术文章和视

频课程。更多

详情请访问

异步社区官

网

https://www.epubit.com。

“异步图书

”是由异步社

区编辑团队

策划出版的

精品 IT 专业图

书的品牌，依

托于人民邮

电出版

社的

计算机图书

出版积累和

专业编辑团

队，相关图书

在封面上印

有异步图书

的 LOGO。异步图书

的

出版领域

包括软件开

发、大数据、AI、测

试、前端、网络

技术等。

异步

社区 微信服

务号

资源与

支持

本书由

异步社区出

品，社区（https://www.epubit.com）为您

提供相关资

源和后续服

务。

配套资源

本书提供源

代码、知识拓

扑图等免费

配套资源。

要

获得相关配

套资源，请在

异步社区本

书页面中单

击 ，跳转到下

载界面，按提

示进行

操作

即可。

提交勘

误

作者和编

辑尽最大努

力来确保书

中内容的准

确性，但难免

会存在疏漏

。欢迎您将发

现的问题反

馈给我们，帮

助我们提升

图书的质量

。

当您发现错

误时，请登录

异步社区，按

书名搜索，进

入本书页面

，单击“提交勘

误”，输入勘

误

信息，单击“提

交”按钮即可

。本书的作者

和编辑会对

您提交的勘

误进行审核

，确认并接受

后，

您将获赠

异步社区的

100 积分。积分可

用于在异步

社区兑换优

惠券、样书或

奖品。

扫码关

注本书

扫描

下方二维码

，您将会在异

步社区微信

服务号中看

到本书信息

及相关的服

务提示。

与我

们联系

我们

的联系邮箱

是 contact@epubit.com.cn。

如果您对

本书有任何

疑问或建议

，请您发邮件

给我们，并请

在邮件标题

中注明本书

书名，以便

我

们更高效地

做出反馈。

如

果您有兴趣

出版图书、录

制教学视频

，或者参与图

书技术审校

等工作，可以

发邮件给本

书的

责任编

辑（sunzhesi@ptpress.com.cn）。

如果您来

自学校、培训

机构或企业

，想批量购买

本书或异步

社区出版的

其他图书，也

可以发邮

件

给我们。

如果

您在网上发

现有针对异

步社区出品

图书的各种

形式的盗版

行为，包括对

图书全部或

部分内

前言

地介绍深度

学习的基础

知识，如果你

在阅读本书

时发现一些

概念晦涩难

懂，请移步其

他基础类图

书

查阅相关

知识点；本书

源于一系列

算法或者论

文的读书笔

记，不同章节

的知识点存

在相互依赖

的关

系，因此

知识点并不

是顺序展开

的。为了帮助

读者提前感

知先验知识

，本书会在每

一节的开始

给出

相关算

法依赖的重

要章节，并在

配套资源中

给出两卷书

整体的知识

拓扑图。

我对

本书有以下

3

个阅读建议

。

z 如果你的深

度学习基础

较为薄弱，那

么可以结合

本书提供的

知识拓扑图

和章节先验

知识，

优先阅

读拓扑图中

无先验知识

的章节，读懂

该章节后便

可以将这个

章节在拓扑

图中划掉，

然

后逐步将拓

扑图清空。

z 如

果你有一定

的深度学习

基础，对一些

经典的算法

（如 VGG、残差网络

、LSTM、

Transformer、Dropout、BN 等）都比较熟

悉，那么你可

以按顺序阅

读本书，并在

遇到陌生

的

概念时根据

每一节提供

的先验知识

去阅读相关

章节。

z

如果你

只想了解某

些特定的算

法，你可以直

接跳到相关

章节，因为本

书章节的内

容都比较

独

立，而且会对

重要的先验

知识进行复

盘，所以单独

地阅读任何

特定章节也

不会有任何

障碍。

本书是

我编写的第

一本图书，这

是一个开始

，但远不是一

个结束。首先

，由于个人的

精力和能

力

有限，图书覆

盖的知识点

难免有所欠

缺，甚至可能

因为我的理

解偏差导致

编写错误，在

此欢迎各

位

读者前去知

乎专栏对应

的文章下积

极地指正，我

也将在后续

的版本中对

本书进行修

正和维护。随

着深度学习

的发展，无疑

会有更多的

算法被提出

，甚至会有其

他经典的算

法被再次使

用，我会在个

人的知乎专

栏继续对这

些算法进行

总结和分析

。

本书的完成

离不开我在

求学、工作和

生活中遇到

的诸多“贵人

”。首先感谢我

在求学的时

候遇

到的诸

位导师，他们

带领我打开

了人工智能

的“大门”。其次

感谢我在工

作中遇到的

诸位领导和

同

事，他们对

我的工作给

予了巨大的

帮助和支持

。最后感谢我

的亲人和朋

友，没有他们

的支持和鼓

励，本书是不

可能完成的

。

刘岩（@ 大师兄

）

2022 年

5 月 28 日

2

目录

2

第 2

章  轻量级

CNN................68

2.1

SqueezeNet ......................................68

2.1.1　SqueezeNet

的压缩策略

...............69

2.1.2　点火模块.......................................69

2.1.3

SqueezeNet 的

网络结构...............70

2.1.4　SqueezeNet

的

性能.........................72

2.1.5　小结...............................................72

2.2

MobileNet v1 和

MobileNet v2

..........73

2.2.1　MobileNet v1.................................73

2.2.2　MobileNet v2.................................77

2.2.3

小结...............................................79

2.3 Xception ............................................80

2.3.1　Inception 回顾...............................80

2.3.2

Xception 详

解...............................81

2.3.3　小结...............................................82

2.4 ResNeXt ...........................................82

2.4.1

从全

连接讲起................................83

2.4.2　简

化 Inception...............................83

2.4.3　ResNeXt 详解................................84

2.4.4

分组

卷积.......................................84

2.4.5　小结...............................................85

2.5

ShuffleNet v1 和

ShuffleNet v2

.........85

2.5.1　ShuffleNet v1................................85

2.5.2　ShuffleNet v2................................88

2.5.3

小结...............................................92

2.6 CondenseNet ....................................92

2.6.1　分组卷

积的问题.............................93

2.6.2　可

学习分组卷

积.............................93

2.6.3　架构设计

.......................................96

2.6.4　小结...............................................96

第 3 章

模

型架构搜索

...............97

3.1 PolyNet .............................................97

3.1.1　结构多样性

....................................98

3.1.2　多项式模型

....................................98

3.1.3　对照实验.....................................100

3.1.4　Very

Deep PolyNet.....................101

3.1.5　小

结.............................................102

3.2 NAS ................................................103

3.2.1

NAS-CNN....................................103

3.2.2　NAS-RNN....................................106

3.2.3

小结.............................................108

3.3 NASNet ...........................................108

3.3.1　NASNet 控制

器...........................109

3.3.2

NASNet 的强化学

习....................110

3.3.3　计划

DropPath............................110

3.3.4　其他

超参数..................................111

3.3.5

小结

.............................................111

3.4 PNASNet ........................................112

3.4.1　更小的搜索

空间...........................112

3.4.2　SMBO.........................................113

3.4.3　代理函

数.....................................114

3.4.4　PNASNet

的实验结

果..................115

3.4.5　小结.............................................116

3.5

AmoebaNet .....................................116

3.5.1　搜索

空间.....................................117

3.5.2　年龄进

化.....................................118

3.5.3　AmoebaNet

的网络结

构..............120

3.5.4　小结.............................................121

3.6

MnasNet .........................................121

3.6.1　优化

目标.....................................122

3.6.2　搜索空

间.....................................124

3.6.3　优化策略

.....................................125

3.6.4　小结.............................................126

3.7 MobileNet

v3 ...................................126

3.7.1　参考结

构.....................................127

3.7.2　网络搜索

.....................................127

3.7.3　人工设计.....................................129

3.7.4　修

改 SE 块...................................131

3.7.5　Lite R-ASPP................................132

目录

第

一篇  卷积神

经网络

第

1 章

基础骨干网

络.................3

1.1

起源：LeNet-5 和 AlexNet ...................4

1.1.1　从

LeNet-5 开始..............................4

1.1.2

觉醒：AlexNet.................................6

1.2　更

深：VGG ........................................11

1.2.1　VGG 介绍......................................11

1.2.2

VGG 的训

练和测试.......................13

1.3　更

宽：GoogLeNet

..............................14

1.3.1　背景知识

.......................................14

1.3.2

Inception v1..................................17

1.3.3　GoogLeNet...................................19

1.3.4　Inception v2..................................19

1.3.5

Inception v3..................................20

1.3.6　Inception

v4..................................21

1.3.7　Inception-ResNet..........................23

1.4

跳跃连接：ResNet .............................26

1.4.1　残

差网络.......................................26

1.4.2　残差

网络背后的

原理.....................28

1.4.3　残差网

络与模型集

成.....................33

1.5　注意力：SENet ..................................33

1.5.1

SE 块

.............................................33

1.5.2　SE-Inception

和 SE-ResNet..........34

1.5.3　SENet

的复杂性

分析.....................35

1.5.4　小结...............................................35

1.6

更

密：DenseNet ................................36

1.6.1　DenseNet

算法解析

及源码实现

....37

1.6.2　压缩层...........................................38

1.6.3

小结

...............................................38

1.7　模型集成：DPN .................................39

1.7.1　高

阶 RNN、DenseNet 和残差网

络....39

1.7.2　DPN 详解......................................41

1.7.3

小结

...............................................42

1.8　像素向量：iGPT .................................43

1.8.1　iGPT 详

解......................................44

1.8.2

实验结果

分析................................48

1.8.3　小结...............................................49

1.9

Visual Transformer 之

Swin

Transformer ......................................49

1.9.1　网络结构详

解................................50

1.9.2　Swin Transformer 家族..................59

1.9.3　小结

...............................................60

1.10 Vision

Transformer 之 CSWin

Transformer ....................................60

1.10.1　CSWin

Transformer 概述............61

1.10.2　十字

形窗口自注

意力机制............61

1.10.3　局

部加强位置

编码.......................62

1.10.4　CSWin

Transformer 块................63

1.10.5　CSWin

Transformer 的复

杂度.....63

1.10.6　小结.............................................64

1.11 MLP? :MLP-Mixer ...........................64

1.11.1　网

络结构.....................................64

1.11.2　讨论

.............................................67

目录

3

3.7.6　小结.............................................133

3.8 EfficientNet v1 .................................133

3.8.1　背

景知识.....................................133

3.8.2　EfficientNet

v1 详解

....................135

3.8.3　小结.............................................137

3.9 EfficientNet v2 .................................137

3.9.1　算法动

机.....................................137

3.9.2　EfficientNet

v2 详解....................139

3.9.3　小结

.............................................141

3.10 RegNet ..........................................141

3.10.1

设计空间...................................141

3.10.2　RegNet 详

解.............................145

3.10.3　小结...........................................151

第二

篇

自然语言

处理

第 4 章

基

础序列模型

............. 155

4.1 LSTM

和 GRU .................................155

4.1.1

序列模型

的背景...........................155

4.1.2　LSTM...........................................157

4.1.3

GRU............................................159

4.1.4　其他

LSTM..................................159

4.2

注意力机制

......................................160

4.2.1　机器翻译的

注意力机制

................160

4.2.2

图解注意力

机制...........................161

4.2.3　经典注

意力模型...........................166

4.2.4

小

结.............................................170

4.3 Transformer ....................................170

4.3.1　Transformer 详解........................171

4.3.2

位置

嵌入.....................................177

4.3.3　小结.............................................178

4.4

Transformer-XL ...............................179

4.4.1　Transformer

的

缺点.....................179

4.4.2　相对位

置编码..............................181

4.4.3

Transformer-XL 详解

...................183

4.4.4　小结.............................................185

第 5 章

模

型预训练................ 186

5.1 RNN

语

言模型 .................................187

5.1.1　语言

模型中的

RNN......................187

5.1.2　训

练数据.....................................188

5.1.3

训练

细节.....................................188

5.2 ELMo ..............................................189

5.2.1　双向语

言模型..............................189

5.2.2　ELMo

详解

..................................191

5.2.3　应用 ELMo

到下游

任务................192

5.2.4　小结.............................................192

5.3

GPT-1、GPT-2 和

GPT-3 ................192

5.3.1

GPT-1：无监督学习

....................193

5.3.2　GPT-2：多任务学习

....................196

5.3.3

GPT-3：海量参数........................197

5.3.4　小

结.............................................200

5.4

BERT ..............................................200

5.4.1　BERT

详解...................................201

5.4.2　小结

.............................................205

5.5

BERT“魔改”之 RoBERTa、

ALBERT、MT-DNN 和

XLM............205

5.5.1　成

熟版 BERT

：RoBERTa............206

5.5.2　更快的

BERT ：ALBERT..............207

5.5.3　多任务 BERT ：MT-DNN.............207

5.5.4　多语

言 BERT ：XLM....................209

5.5.5　小结.............................................211

5.6 XLNet

..............................................211

5.6.1　背景

知识.....................................212

5.6.2

XLNet 详解..................................213

5.6.3　小

结.............................................216

5.7 ERNIE（清华大学

） .........................216

目录

4

5.7.1　加入知

识图谱的动

机...................217

5.7.2

异构信息

融合..............................217

5.7.3　DAE.............................................220

5.7.4

ERNIE-T 的微调

..........................220

5.7.5　小结.............................................221

5.8 ERNIE（百度）和

ERNIE 2.0 .........221

5.8.1　ERNIE-B......................................222

5.8.2　ERNIE

2.0...................................223

5.8.3　小结.............................................226

第三篇



模型优化

第

6 章

模型优化

方法.............229

6.1 Dropout ...........................................230

6.1.1　什么是

Dropout...........................230

6.1.2　Dropout

的数学原理

....................231

6.1.3　Dropout 是一个正则

网络.............232

6.1.4　CNN 的 Dropout..........................232

6.1.5　RNN 的 Dropout..........................233

6.1.6　Dropout 的

变体...........................234

6.1.7

小结.............................................236

6.2 BN ...................................................237

6.2.1　BN 详

解.......................................237

6.2.2

BN 的背后原

理............................240

6.2.3　小结.............................................243

6.3 LN ...................................................243

6.3.1

BN 的问

题...................................244

6.3.2　LN

详解.......................................244

6.3.3　对照

实验.....................................245

6.3.4

小结.............................................247

6.4 WN ..................................................247

6.4.1　WN 的

计算..................................247

6.4.2

WN 的原理

..................................248

6.4.3　BN

和 WN 的关系........................249

6.4.4

WN 的

参数初始化

.......................249

6.4.5　均值

BN.......................................249

6.4.6　小结.............................................249

6.5

IN ....................................................250

6.5.1　IST

中

的 IN..................................250

6.5.2　IN

与 BN 对比..............................250

6.5.3

TensorFlow 中

的 IN.....................251

6.5.4

小结.............................................252

6.6 GN ..................................................252

6.6.1　GN 算法

......................................252

6.6.2

GN 的源码...................................253

6.6.3　GN

的原

理...................................253

6.6.4　小结.............................................253

6.7

SN ...................................................254

6.7.1　SN

详解

.......................................254

6.7.2　SN 的优点...................................256

6.7.3　小结

.............................................256

第一篇 卷积

神经网络

“虽

然没有人这

样说，但我认

为人工智能

几

乎是一门

人文学科，是

一种试图理

解人类智力

和人类认知

的尝试。”

—Sebastian Thrun

第 1 章

基础骨干网

络

物体分类

是计算机视

觉（computer vision，CV）中最经典

的、也是目前

研究得最为

透彻的一

个

领域，该领域

的开创者也

是深度学习

领域的“名人

”级别的人物

，例如 Geoffrey

Hinton、Yoshua 

Bengio 等。物体

分类常用的

数据集有手

写数字识别

数据集

MNIST、物体

识别数据集

CIFAR-10

（10 类）和类别更

多的 CIFAR-100（100

类），以及

超大数据集

ImageNet。ImageNet 是由李飞飞

教

授主导的

ILSVRC（ImageNet Large

Scale Visual Recognition Challenge）中使用的数

据集，每年的

ILSVRC（此处指

ILSVRC 的物

体分类任务

）中产生的网

络也指引了

分类网络的

发展方向。

2012 年

，第三届

ILSVRC 的冠

军作品 Hinton 团队

的

AlexNet，将 2011 年的 top-5

错

误率从

25.8% 降低

到 16.4%。他们的最

大贡献在于

验证了卷积

操作在大数

据集上的有

效性，从此物

体分类

进入

了深度学习

时代。

2013 年，ILSVRC 已被

深度学习算

法“霸榜”，冠军

作品

ZFNet 使用了

更深的深度

，并且其论

文

给出了卷积

神经网络（CNN）的

有效性的初

步解释。

2014

年是

深度学习领

域分类算法

“井喷式”发展

的一年，在物

体检测方向

也是如此。这

一届

ILSVRC 物体分

类任务的冠

军作品是 Google

团

队提出的 GoogLeNet（top-5 错

误率：7.3%），亚军作

品则是牛津

大学的 VGG（top-5

错误

率：8.0%），但是在物

体检测任务

中 VGG 击败了 GoogLeNet。

VGG 利

用的搭建 CNN 的

思想现在来

看依旧具有

指导性，例如

按照降采样

的分布对网

络进行分块

，

使用小卷积

核，每次降采

样之后特征

图（feature map）的数量加

倍，等等。另外

VGG 使用了当初

贾扬清提出

的 Caffe

作为深度

学习框架并

开源了其模

型，凭借比 GoogLeNet 更

快的特性，VGG 很

快占有了大

量的市场，尤

其是在物体

检测领域。VGG

也

凭借增加深

度来提升精

度的思想将

CNN 推

上了“最高

峰”。GoogLeNet 则从特征

多样性的角

度研究了

CNN 结

构，GoogLeNet 的特征多

样性是

基于

一种并行的

、使用了多个

不同尺寸的

卷积核的

Inception 单

元来实现的

。GoogLeNet 的最大贡

献

在于指出

CNN 精

度的增加不

仅仅可以依

靠深度实现

，增加网络的

复杂性也是

一种有效的

策略。

2015 年的

ILSVRC 的

冠军作品是

何恺明等人

提出的残差

网络（top-5 错误率

：3.57%）。他们指

出

CNN 的

精度并不会

随着深度的

增加而增加

，导致此问题

的原因是网

络的退化问

题。残差网络

的核心思想

是通过向网

络中添加直

接映射（跳跃

连接）的方式

解决退化问

题，进而使构

建更深的

CNN 成

为可能。残差

网络的简单

易用的特征

使其成为目

前使用最为

广泛的网络

结构之一。

2016 年

ILSVRC 的前几名作

品都是通过

模型集成实

现的，CNN 的结构

创新陷入了

短暂的停滞

。

当年的冠军

作品是商汤

公司和香港

中文大学联

合推出的 CUImage，它

是 6 个模型的

集成，并无创

新性，此处不

赘述。2017

年是 ILSVRC 的

最后一届，这

一届的冠军

是 Momenta

团队，他们

提出了

基于

注意力机制

的 SENet（top-5 错误率：2.21%），其

通过自注意

力（self-attention）机制为每

个特

征图计

算出一个权

重。另外一个

非常重要的

网络是黄高

团队于 CVPR 2017 提出

的

DenseNet，本质

第 1 章



基础骨干网

络

4

上是各个

单元相互连

接的密集连

接结构。

除了

ILSVRC

中各个冠军

作品，在提升

网络精度方

面还有一些

值得我们学

习的算法，例

如

Inception 的几个变

种、结合了 DenseNet

和

残差网络的

DPN。

由于 Transformer 在自然

语言处理（natural

language processing，NLP）任

务上取得的

突破性进

展

，将 Transformer

应用到分

类网络成为

近年来非常

火热的研究

方向，比较有

代表性的包

括 iGPT、

ViT、Swin Transformer，以及混合

使用

CNN 和 Transformer 的

CSWin Transformer。

1.1　起

源：LeNet-5

和 AlexNet

在本节

中，先验知识

包括：



BN（6.2 节）；  Dropout（6.1

节）。

1.1.1 从

LeNet-5 开始

使用 CNN 解

决图像分类

问题可以往

前追溯到 1998

年

LeCun 发表的论文

1

，其中提出了

用于解

决手

写数字识别

问题的

LeNet。LeNet 又名

LeNet-5，是因为在 LeNet 中

使用的均是

5

× 5 的卷积核。

LeNet-5

的

网络结构如

图 1.1 所示。

图

1.1 LeNet-5 的

网络结构

LeNet-5

中

使用的结构

直接影响了

其后的几乎

所有 CNN，卷积层

+ 降采样层 +

全

连接层至今

仍

然是最主

流的结构。卷

积操作使网

络可以响应

和卷积核形

状类似的特

征，而降采样

操作则使网

络拥

有了一

定程度的不

变性。下面我

们简单分析

一下 LeNet-5

的网络

结构。

输入：32 × 32

的

手写数字（数

据集中共 10 类

）的黑白图片

。

C1：C1

层使用了 6 个

卷积核，每个

卷积核的大

小均是 5

× 5，pad = 0，stride

= 1（有效

卷积，与

有效

卷积对应的

是 same

卷积），激活

函数使用的

是 tanh（双曲正切

），表达式为式

（1.1），tanh 激

活函数的

值域是

( − 1,1)。所以

在第一次卷

积之后，特征

图的大小变

为 28

× 28（(32 − 5

+ 1)/1 = 28），

该层共有

28 × 28 ×

1 × 6 =

4 704 个神经元。加

上偏置，该层

共有 (5

× 5 + 1)

× 6 = 156

个参数

。

（1.1）

1　参见

Yann LeCun、Léon Bottou、Yoshua Bengio

等人的

论文“Gradient-based learning applied to

document recognition”。

1.1　起源：LeNet-5

和

AlexNet

5

S2：S2 层是

CNN 常使用

的降采样层

。在 LeNet-5 中，降采样

的过程是将

窗口内的

3 个

输入相

加，乘

一个可训练

参数再加上

一个偏置。经

过 S2

层，特征图

的大小缩小

，变成 14 × 14。该层共

有

14 × 14 ×

6 = 1 176

个神经元

，参数数量是

(1 + 1) ×

6 = 12。

C3：C3

层跟 S2 层并不

是密集连接

的，具体连接

方式是，C3 层的

前

6 个特征图

以 S2 层中

3

个相

邻的特征图

子集为输入

，接下来 6 个特

征图以

S2 层中

4 个相邻特征

图子集为输

入，然后的

3

个

特征图以不

相邻的 4 个特

征图子集为

输入，最后一

个特征图以

S2 层中所有特

征图为输入

，如

图 1.2 所示。这

两个层采用

的稀疏连接

的方式已被

抛弃，目前普

遍使用的是

密集连接，或

轻量级网

络

中使用的深

度可分离卷

积、分组卷积

。

图 1.2 LeNet-5 中

C3 层和 S2 层

的连接方式

C3

层包括 16 个大

小为 5

× 5、通道数

为 6 的

same 卷积，pad = 0，stride

= 1，激

活函数同样

为

tanh。一次卷积

后，特征图的

大小是 10

× 10（(14 − 5

+ 1)/1 = 10），神经

元数量为

10 × 10 ×

16 = 1 600，

可

训练参数数

量为 (3 × 25

+ 1) × 6

+ (4 × 25

+ 1) × 6

+ (4 × 25

+ 1) × 3

+ (6 × 25

+ 1) × 1

= 1 516。

S4：与

S2 层的

计算方法类

似，该层使特

征图的大小

变成 5 ×

5，共有 5 × 5

× 16 = 400

个

神经元，

可训

练参数数量

是 (1 +

1) × 16 =

32。

C5：节点数为

120 的全连接层

，激活函数是

tanh，参数数量是

(400 +

1) × 120 =

48 120。

F6：节点数为 84

的

全连接层，激

活函数是 tanh，参

数数量是 (120 +

1) × 84 =

10 164。

输

出：10个分类的

输出层，使用

的是softmax激活函

数，如式（1.2）所示

，参数数量是

(84 +

1) ×

10 =

850。softmax 用于分类有

如下优点：

z e

x 使

所有样本的

值均大于 0，且

指数的性质

使样本的区

分度尽量高

；

z

softmax 所有可能值

的和为 1，反映

出分类为该

类别的概率

，输出概率最

高的类别即

可。

（1.2）

使用 Keras 搭建

LeNet-5 网络的核心

代码如下，其

是基于

LeNet-5 网络

，在 MNIST 手写数字

识

别数据集

上的实现。完

整的 LeNet-5 在 MNIST

上的

训练过程见

随书资料。

注

意，这里使用

的都是密集

连接，没有复

现 C3 层和

S2 层之

间的稀疏连

接。

# 构建LeNet-5网络

model

= Sequential()

model.add(Conv2D(input_shape =

(28,28,1), filters=6, kernel_size=(5,5),

padding='valid', activation='tanh'))

model.add(MaxPool2D(pool_size=(2,2), strides=2))

model.add(Conv2D(input_shape=(14,14,6), filters=16, kernel_size=(5,5),

padding='valid', activation='tanh'))

model.add(MaxPool2D(pool_size=(2,2), strides=2))

model.add(Flatten())

model.add(Dense(120, activation='tanh'))

model.add(Dense(84,

activation='tanh'))

model.add(Dense(10, activation='softmax'))

第

1 章  基础骨

干网络

如图

1.3 所示，经过 10 个

epoch

后，LeNet-5 基本收敛

。

图 1.3

LeNet-5 在 MNIST 数据集

上的收敛情

况

1.1.2 觉醒：AlexNet

LeNet-5 之后

，CNN

沉寂了约 14 年

。直到 2012

年，AlexNet 在 ILSVRC 中

一举夺魁，直

接把

在 ImageNet 数据

集上的精度

提升了约 10

个

百分点，它将

CNN 的深度和宽

度都提升到

了传统算法

无法企及的

新高度。从此

，深度学习开

始在 CV 的各个

领域“披荆斩

棘”，至今深度

学习仍是人

工

智能最热

门的话题。AlexNet 作

为教科书式

的网络，值得

每个学习深

度学习的人

深入研究。

AlexNet 的

名字取自该

模型的第一

作者

Alex Krizhevsky。AlexNet 在 ImageNet

中的

120 万张

图片的

1 000

类分类任务

上的 top-1 错误率

是 37.5%，top-5

错误率则

是 15.3%（直接比第

二名的

26.2% 低了

约

10 个百分点

）。AlexNet 如此成功的

原因是其使

网络的宽度

和深度达到

了前所未有

的

高度，而该

模型也使网

络的可学习

参数达到了

58

322 314 个。为了学习

这些参数，AlexNet 并

行使

用了两

块 GTX 580，大幅提升

了训练速度

。

笔记

AlexNet 当初使

用分组卷积

是因为硬件

资源有限，不

得不将模型

分到两块 GPU 上

运行。相关研

究者并没有

给出分组卷

积的概念，而

且没有对分

组卷积的性

能进行深入

探

讨。ResNeXt 的相关

研究者则明

确给出了分

组卷积的定

义，并证明和

验证了分组

卷积有

接近

普通卷积的

精度。

当想要

使用机器学

习解决非常

复杂的问题

时，我们必须

使用容量足

够大的模型

。在深度学习

中，增加网络

的宽度和深

度会提升网

络的容量，但

是提升容量

的同时也会

带来两个问

题：

z 计算资源

的消耗；

z 模型

容易过拟合

。

计算资源是

当时限制深

度学习发展

的瓶颈，2011 年 Ciresan 等

人提出了使

用

GPU 部署 CNN 的

技

术框架 1

，由此

深度学习得

到了可以解

决其计算瓶

颈问题的硬

件支持。

下面

来详细分析

一下

AlexNet。AlexNet 的网络

结构如图 1.4 所

示。

1　参见 Dan C.

Ciresan、Ueli Meier、Jonathan Masci 等人

的论文“Flexible,

High Performance Convolutional Neural

Networks for 

Image

Classification”。

6

1.1　起源

：LeNet-5

和 AlexNet

图 1.4

AlexNet 的网络

结构

AlexNet 基于

Keras 的

实现代码如

下。

# 构建AlexNet网络

model

= Sequential()

model.add(Conv2D(input_shape =

(227,227,3), strides = 4,

filters=96, kernel_size=(11,11),

 padding='valid',

activation='relu'))

model.add(BatchNormalization())

model.add(MaxPool2D(pool_size=(3,3), strides=2))

model.add(Conv2D(filters=256, kernel_size=(5,5), padding='same', activation='relu'))

model.add(BatchNormalization())

model.add(MaxPool2D(pool_size=(3,3), strides=2))

model.add(Conv2D(filters=384,

kernel_size=(3,3), padding='same', activation='relu'))

model.add(BatchNormalization())

model.add(Conv2D(filters=384, kernel_size=(3,3), padding='same', activation='relu'))

model.add(BatchNormalization())

model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same',

activation='relu'))

model.add(BatchNormalization())

model.add(MaxPool2D(pool_size=(2,2), strides=2))

model.add(Flatten())

model.add(Dense(4096, activation='tanh'))

model.add(Dropout(0.5))

model.add(Dense(4096, activation='tanh'))

model.add(Dropout(0.5))

model.add(Dense(10,

activation='softmax'))

model.summary()

根据 Keras

提供的

summary() 函数，可以得

到图 1.5 所示的

AlexNet

的参数数量

的统计结果

1

，

计算方法参

照 LeNet-5，不赘述。

1．多

GPU 训练

首先对

比图 1.1

和图 1.4，我

们发现 AlexNet 将网

络分成了两

个部分。由于

当时显卡的

显存大小

有

限，因此作者

使用了两块

GPU 并行训练模

型，例如第二

个卷积（图 1.4 中

通道数为

128 的

卷积）

只使用

一个 GPU

自身显

存中的特征

图，而第三个

卷积需要使

用另外一个

GPU 显存中的特

征图。不过

得

益于 TensorFlow

等开源

框架对多机

多卡的支持

和显卡显存

的提升，AlexNet 部署

在单块 GPU 上已

毫无压力，所

以这一部分

就不赘述。

2．ReLU

在

LeNet-5 中，使用了 tanh

作

为激活函数

，tanh 的函数曲线

如图 1.6 所示。tanh

是

一个以原

点

为中心点、值

域为 ( −

1,1) 的激活

函数。在反向

传播过程中

，局部梯度会

与整个损失

函数关于该

1　这里参数数

量不同是因

为代码没有

将模型部署

在两块显卡

上。

7

第 1 章

基础

骨干网络

8

局

部输出的梯

度相乘。当 tanh(x)

中

的 x 的绝对值

比较大的时

候，该局部的

梯度会非常

接近于 0，

在深

度学习中，该

现象叫作“饱

和”。同样，另一

个常用的 sigmoid 激

活函数也存

在饱和的现

象。

sigmoid

的函数如

式（1.3）所示，函数

曲线如图 1.7 所

示。

（1.3）

图 1.5　通过 Keras

的

summary() 函数得到的

AlexNet 参数数量

图

1.6

tanh 的函数曲线

1.1　起源：LeNet-5 和

AlexNet

9

图 1.7

sigmoid 的

函数曲线

饱

和现象带来

了一个深度

学习中非常

严重的问题

，那便是梯度

消失。梯度消

失是由反向

传播中

链式

法则的乘法

特性导致的

，反映在深度

学习的训练

过程中便是

越接近损失

函数的参数

梯度越大，

从

而使得这一

部分参数成

为主要学习

的参数，而远

离损失函数

的参数的梯

度则非常接

近 0，导致几

乎

没有梯度传

到这一部分

参数，从而使

得这一部分

参数很难学

习到。

为了解

决这个问题

，AlexNet

引入了 ReLU 激活

函数，如式（1.4）所

示。

f(x)

= max(0,x) （1.4）

ReLU

的函数曲

线如图 1.8 所示

。

在

ReLU 中，无论 x 的

取值有多大

，f(x)

的导数都是

1，也就不存在

导数小于 1 导

致的梯度消

失的现象了

。图 1.9

所示的是

我们在 MNIST 数据

集上，根据 LeNet-5

使

用 tanh 和 ReLU

两个激

活

函数得到

的不同模型

的收敛情况

，旨在对比两

个不同的激

活函数的模

型效果。

图 1.8

ReLU 的

函数曲线 图

1.9 LeNet-5

使用不同激

活函数的收

敛情况

此外

，由于 ReLU 将小于

0

的部分全部

置 0，因此 ReLU 的另

外一个特点

就是具有稀

疏性，不

仅可

以优化网络

的性能，还可

以缓解过拟

合现象。

虽然

使用 ReLU 的节点

不会有饱和

问题，但是会

“死掉”，即大部

分甚至所有

的值为负值

，从

而导致该

层的梯度都

为 0。“死神经元

”是由进入网

络的负值引

起的（例如在

大规模的梯

度更新之

后

可能出现），减

小学习率能

缓解该现象

。

3．LRN

局部响应归

一化（local response normalization，LRN）模拟的

是动物神经

中的横向抑

制效应，是

第

1

章  基础骨干

网络

10

一个已

经被淘汰的

算法。在 VGG1 的相

关论文中已

经指出，LRN 并没

有什么效果

。在现在的网

络中，

LRN 已经被

其他归一化

方法所替代

，例如在上面

代码中使用

的批归一化

（batch normalization，BN）2

。

LRN 是使用同一

位置临近的

特征图来归

一化当前特

征图的值的

一种方法，其

表达式如式

（1.5）所示：

（1.5）

其中，N

表

示特征图的

数量，a 是输入

特征图，b 是输

出特征图，(x, y)

是

特征图上的

坐标，n = 5，

k

= 2，α = 0.5，β

= 0.75，这些值

均由验证集

得出。

另外，AlexNet 把

LRN

放在池化层

之前，这在计

算上是非常

不经济的，一

种更好的做

法是把

LRN 放在

池化层之后

。

4．覆盖池化

当

进行池化的

时候，如果步

长（stride）小于池化

核的尺寸，相

邻的池化核

会相互覆盖

，这种

方式叫

作覆盖池化

（overlap pooling）。AlexNet 的论文中指

出这种方式

可以缓解过

拟合。

5．Dropout

在 AlexNet 的前

两层，作者使

用了

Dropout3 来缓解

容量高的模

型容易发生

过拟合的现

象。Dropout

的使用方

法是在训练

过程中随机

将一定比例

的隐层节点

置 0。Dropout

能够缓解

过拟合的原

因是每

次训

练都会采样

一个不同的

网络结构，但

是这些架构

是共享权值

的。这种技术

减轻了节点

之间的耦

合

性，因为一个

节点不能依

赖网络的其

他节点。因此

，节点能够学

习到更健壮

的特征。只有

这样，

节点才

能适应每次

采样得到的

不同的网络

结构。注意在

测试时，我们

是不对节点

进行丢弃的

。

虽然 Dropout 会减慢

收敛速度，但

其在缓解过

拟合方面的

优异表现仍

旧使其在当

前的网络中

得

到广泛的

使用。

图 1.10 所示

的是 LeNet-5

中加入

Dropout 之后模型的

训练损失曲

线。从图 1.10 中我

们可以看

出

，加入 Dropout 之后，训

练速度放缓

了一些。20 个

epoch 之

后，训练集的

损失函数曲

线仍高于没

有 Dropout 的。加入

Dropout 之

后，虽然损失

值为 0.073 5，远高于

没有

Dropout 的 0.015 5，但是

测

试集的准

确率从 0.982 6 上升

到

0.984 1。具体的实

验数据见随

书代码。可见

Dropout 对于缓解过

拟

合还是非

常有帮助的

。

图 1.10　有 Dropout

与没有

Dropout 对比

1　参见

Karen Simonyan、Andrew Zisserman 的

论文“Very

Deep Convolutional Networks for

Large-Scale Image Recognition”。

2

参见 Sergey Ioffe、Christian Szegedy

的

论文“Batch Normalization: Accelerating Deep

Network Training by Reducing

Internal Covariate Shift”。

3

参见 Nitish Srivastava、Geoffrey Hinton、Alex

Krizhevsky 等

人的论文“Dropout: A Simple

Way to Prevent Neural

Networks 

from Overfitting”。

1.2　更

深：VGG

11

1.2

更深：VGG

在本

节中，先验知

识包括：

 AlexNet（1.1

节）。

2014 年

，随着 AlexNet

在 ImageNet 数据

集上大放异

彩，使用深度

学习探寻针

对 ImageNet

数据集

的

最优网络成

为提升在该

数据集上精

度的优先级

最高的做法

。牛津大学视

觉几何组（Visual Geometry

Group）的

这篇论文 1 便

提出了对 CNN

的

深度和其性

能进行探索

的网络，该网

络被命名为

VGG。

VGG 的结构非常

清晰：

z

按照 2 × 2

的

池化层，网络

可以分成若

干个块（block）；

z 每个

块包含若干

个 same

卷积，块内

的特征图数

量固定不变

；

z 特征图的通

道数按块以

2 倍的速度逐

渐递增，第四

块和第五块

内特征图的

通道数都是

512

（即 64、128、256、512、512）。

VGG 非常容易

应用到其他

数据集。在

VGG 中

，块数每增加

1，特征图的尺

寸缩小一半

，这么

做是为

了保证每一

块的参数数

量不会剧烈

变化。通过减

少块的数目

也可以将网

络应用到如

MNIST、

CIFAR

等图像尺寸

更小的数据

集。块内的卷

积数量是可

变的，因为卷

积的数量并

不会影响特

征图的

尺寸

，我们可以根

据任务的复

杂度自行调

整块内的卷

积数量。

VGG 的表

现效果也非

常好，在

2014 年的

ILSVRC 物体分类任

务中排名第

二（第一名是

GoogLeNet2

），在物体检测

任务中排名

第一。

VGG 的模型

开源在其官

方网站上，为

其他任务提

供了非常好

的迁移学习

的材料，这使

得 VGG

占有了大

量商业市场

。关于不同框

架的

VGG 开源模

型，读者可自

行在网上搜

索。

1.2.1 VGG

介绍

“VGG 家族

”等各个类型

的实现见随

书资料，关于

VGG 家族的参数

的具体设置

可以总结为

图

1.11，图中包含

大量信息，接

下来我们一

一进行分析

。

1．家族的特征

我们来看看

VGG 家族的共同

特征：

z

输入图

像均是 224 × 224

× 3 的 RGB

彩

色图像；

z 均采

用 5

层最大池

化，使用的均

是 same 卷积，表示

最终均会产

生大小为 7

× 7 的

特征图，

这是

一个比较合

适的大小；

z 特

征层之后是

两个隐层节

点数目为 4096 的

全连接层，最

后是一个

1000 类

softmax 分类器；

z

所有

VGG 模型均可以

表示为 m ×

(n × conv3 +

max_pooling)。

VGG 在卷

积核方向的

最大改进是

将卷积核全

部换成更小

的 3

× 3 或者 1

× 1 的卷

积核，而性能

最好的 VGG-16

和 VGG-19 由

且仅由 3

× 3 的卷

积核构成，原

因有如下 3

点

。

z 根据感受野

的计算式 rfsize

= (out − 1)

× stride + ksize，其

中

stride 为模型的

步长，ksize 为卷

积

核的大小。我

们知道一层

7

× 7 的卷积核和

3 层

3 × 3 的卷积核

具有相同的

感受野，但是

由于

3 层感受

野具有更深

的深度，因此

可以构建出

更具判别性

的决策函数

。

1　参见

Karen Simonyan、Andrew Zisserman 的论文

“Very

Deep Convolutional Networks for

Large-Scale Image Recognition”。

2

参见 Christian Szegedy、Wei Liu

、Yangqing Jia 等人的

论文“Going Deeper

with Convolutions”。

第 1

章  基

础骨干网络

图 1.11

VGG 家族

z 假设

特征图的数

量都是

C，3 层 3 ×

3 卷

积核的参数

数量是 3 ×

(3 × 3 +

1) × C2

= 30C2

，1 层

7 × 7 卷

积核的参数

数量是

1 × (7 ×

7 + 1) ×

C2

 = 50C2

，3 层 3 ×

3 卷

积核具有更

少的参数。

z 由

于神经元数

量和层数的

增多，训练速

度会变得更

慢。

图 1.12 反映了

VGG 家族的各个

模型的性能

。

图 1.12 VGG 家族的各

个模型的性

能对比

图 1.13 展

示了把 LeNet-5

的单

层 5 × 5

卷积换成

两层 3 × 3

卷积在

MNIST 上的收敛表

现。论文中

的

实验表明两

层 3

× 3 卷积的网

络确实比单

层 5

× 5 卷积的网

络表现好，但

是训练速度

慢了二分之

一。

另外，作者

在前两层的

全连接处使

用丢失率为

0.5

的 Dropout，然而并没

有在图 1.11 中反

映

出来。

2．VGG-A vs VGG-A-LRN

VGG-A-LRN 比 VGG-A 多

了一个

AlexNet 介绍

的 LRN 层，但是实

验数据表明

加入了

LRN 的

VGG-A-LRN 的

错误率反而

更高了，而且

LRN

的加入会更

加占用内存

，增加训练时

间。

12

1.2　更深：VGG

图 1.13　单

层 5

× 5 卷积的 LeNet

与

两层 3 × 3

卷积的

LeNet 对比

3．VGG-A、VGG-B、VGG-D 和

VGG-E

对比

VGG-A（11 层）、VGG-B（13 层）、VGG-D（16

层）、VGG-E（19 层）的

错误率，我们

发现随着网

络深度的增

加，分类的错

误率逐渐降

低，当然深度

越深表示需

要的训练时

间越长。但是

当模型（VGG-D 和 VGG-E）到

达一定深度

时，网络的错

误率趋于收

敛，甚至偶尔

会发生深层

网络

的错误

率高于浅层

网络的情况

，这就是后面

我们要介绍

的退化问题

。同时考虑网

络的训练时

间，我

们需要

折中考虑选

择合适的网

络深度。我相

信作者一定

探索了比 VGG-E 更

深的网络，但

是由于表

现

不理想并没

有将其列在

论文中。后面

介绍的残差

网络则通过

残差机制将

网络的深度

从理论上扩

展

到了无限

大。在后面的

应用中，VGG-D 和 VGG-E

得

到了最为广

泛的应用，它

们更多的时

候被叫作

VGG-16 和

VGG-19。

4．VGG-B

和 VGG-C

VGG-C 在

VGG-B 的基础

上添加了 3 个

1

× 1 的卷积。1 ×

1 的卷

积是在 NIN1 中率

先使用的。由

于

1 × 1 卷积在不

影响感受野

的前提下提

升了决策函

数的容量，并

且有通道融

合的作用，因

此实现

了错

误率的下降

。

5．VGG-C 和 VGG-D

VGG-D 将 VGG-C 中的

1 × 1 卷

积换成了

3 × 3 卷

积，该组对比

表明

3 × 3 卷积的

提升效果要

优

于 1 × 1

卷积。

6．VGG-D 和

VGG-E

当网络层数

增加到

16 层时

，网络的损失

函数趋于收

敛。当网络提

升到 19 层时，虽

然精度有了

些许的提升

，但需要的训

练时间也大

幅增加。

1.2.2 VGG 的训

练和测试

1．训

练

VGG 的训练分

为单尺度训

练（single-scale training）和多尺度

训练（multi-scale training）。在单

尺

度训练中，原

图的短边长

度为一个固

定值 S（实验中

S 被固定为 256

或

384），然后等比例

缩放图

片，再

从缩放的图

片中裁剪 224 ×

224 的

子图用于训

练模型。在多

尺度训练中

，每张图的短

边长度

为 256

到

512 之间的一个

随机值，然后

从缩放的图

片中裁剪 224 ×

224 的

子图。

1　参见

Min Lin 、Qiang Chen、Shwicheng

Yan 的

论文“Network In Network”。

13

第 1 章

基

础骨干网络

14

2．测试

测试时

可以使用和

训练时相同

的图片裁剪

方法，然后通

过若干不同

裁剪的图片

投票的方式

选择

最后的

分类。

但测试

的时候图片

是单张输入

的，使用裁剪

的方式可能

会漏掉图片

的重要信息

。在 OverFeat1

的论文中

，提出了将整

幅图作为输

入的方式，过

程如下。

（1）将测

试图片的短

边长度固定

为

Q，Q 可以不等

于 S。

（2）将

Q 输入 VGG，在

卷积网络的

最后一个卷

积，得到 W

× H × 512

的特

征向量，W 和 H 一

般不等于

7。

（3）将

第一个全连

接层看成 7 ×

7 × 512 ×

4 096 的

卷积层（原本

需要先进行

Flattern() 操作，再进行

全

连接操作

），对比随书资

料中的VGG-E和使

用全卷积的

VGG-E-test，可以发现两

者具有相同

的参数数量

。

（4）将第二个、第

三个全连接

层看成 1 ×

1 × 4 096

× 4 096 与

1 × 1 ×

4 096 × numClasses（numClasses

指

的是类别数

）的卷积层。

（5）如

果输入图片

大小为 224 ×

224，则输

出的大小为

1 × 1 ×

numClasses，因为图片大

小可以不

一

致，所以可以

将输出看作

某张图片多

个切片的预

测结果。最终

经过加和池

化，对每个通

道求和，

将得

到 1

× 1 × numClasses

的结果作

为最终输出

，即取所有切

片的平均数

作为最终输

出。

1.3　更宽：GoogLeNet

在本

节中，先验知

识包括：

 AlexNet（1.1 节）。

2012

年

之后，CNN 的研究

分成了两大

流派，并且两

大流派都在

2014 年有重要的

研究成果发

表。一个流派

的研究方向

是增加 CNN

的深

度和宽度，经

典的网络有

2013 年 ILSVRC 的冠军作

品

ZFNet 和我们在

1.2 节中介绍的

VGG 系列。另外一

个流派的研

究方向是增

加卷积核的

拟合能力，

或

者说是增加

网络的多样

性，典型的网

络有可以拟

合任意凸函

数的 Maxout 网络 2

、可

以拟合任意

函

数的 NIN，以及

本节要解析

的基于 Inception

的 GoogLeNet。为

了能更透彻

地了解 GoogLeNet 的思

想，我们首先

需要了解

Maxout 和

NIN 两种结构。

1.3.1

背

景知识

1．Maxout 网络

在之前介绍

的 AlexNet

中，它引入

了 Dropout 来减轻模

型的过拟合

问题。Dropout 可以看

作一

种集成

模型，在训练

的每步中，Dropout 会

将网络的隐

层节点以概

率 P 置

0。Dropout 和传统

的装袋

（bagging）方法

主要有以下

两个方面不

同：

z

Dropout 的每个子

模型的权值

是共享的；

z 在

训练的每步

中，Dropout

会使用不

同的样本子

集训练不同

的子网络。

这

样在训练的

每步中都会

有不同的节

点参与训练

，可以减轻节

点之间的耦

合性。在测试

时，

1　参见

Pierre Sermanet、David Eigen、Xiang Zhang

等人

的论文“OverFeat: Integrated Recognition, Localization

and Detection using

Convolutional Networks”。

2　参见

Ian

J. Goodfellow、David Warde-Farley、Mehdi Mirza

等人的论文

“Maxout Networks”。

1.3　更宽：GoogLeNet

15

Dropout 使用的

是整个网络

的所有节点

，只是节点的

输出值要乘

p。因为在测试

时，我们不会

进

行

Dropout 操作。为

了避免 Dropout 丢失

节点带来的

缩放问题，我

们会将该层

节点值乘

p 来

达到

Dropout 引起的

缩放效果。

作

者认为，与其

像 Dropout 这样平均

地选择，不如

有条件地选

择节点来生

成网络。在传

统的神

经网

络中，第

i 个隐

层的计算方

式（暂时不考

虑激活函数

）如式（1.6）所示：

（1.6）

假

设第

i − 1 个隐层

和第

i 个隐层

的节点数分

别是 d 和

m，那么

W 是一个 d ×

m 的二

维矩阵。而

在

Maxout 网络中，W

是一

个三维矩阵

，矩阵的维度

是 d × m

× k，其中 k 表示

Maxout

网络的通道

数，

是 Maxout 网络唯

一的参数。Maxout

网

络的数学表

达式如式（1.7）所

示：

（1.7）

其中 。

下面

我们通过一

个简单的例

子来说明 Maxout 网

络的工作方

式。对于一个

传统的网络

，假设第

i

个隐

层有两个节

点，第 i + 1

个隐层

有 1 个节点，那

么多层感知

机（multi-layer perceptron，MLP）

的计算方

式如式（1.8）所示

：

（1.8）

其中 g(·)

是激活

函数，如 tanh、ReLU 等，X 是

输入数据的

集合。从图

1.14 可

以看出，传统

神经

网络的

输出节点是

由两个输入

节点计算得

到的。

如果我

们将

Maxout 的参数

k 设置为 5，Maxout

网络

可以展开成

图 1.15 所示的形

式：

图

1.14　传统神

经网络 图 1.15

Maxout 网

络

其中 z

= max(z1,z2,z3,z4,z5)。z1 ～ z5

为线

性函数，所以

z 可以看作分

段线性的激

活函数。Maxout 网络

的论文中给

出了证明，当

k 足够大时，Maxout

单

元可以以任

意小的精度

逼近任何凸

函数，如图 1.16

所

示，图中每条

直线代表一

个输出节点

zi

。

图 1.16 Maxout 单元的凸

函数无限逼

近性

在 Keras 2.0 之前

的版本中，我

们可以找到

Maxout

网络的实现

，其核心代码

只有一行。

第

1 章

基础骨干

网络

16

output =

K.max(K.dot(X, self.W) + self.b,

axis=1)

Maxout 网络存

在的最大的

一个问题是

网络的参数

数量是传统

神经网络的

k 倍，而

k 倍的参

数

数量并没

有带来等价

的精度提升

，所以现在 Maxout

网

络基本已被

工业界淘汰

。

2．NIN

Maxout 单元可以逼

近任何凸函

数，而

NIN 的节点

理论上可以

逼近任何函

数。在 NIN 中，作者

也采用整图

滑窗的形式

，只是将

CNN 的卷

积核替换成

了一个小型

MLP 网络，如图 1.17

所

示。

图 1.17 NIN

网络结

构

在卷积操

作中，一次卷

积操作仅相

当于卷积核

和滑窗的一

次矩阵乘法

，其拟合能力

有限。而

MLP 替代

卷积操作增

加了每次滑

窗的拟合能

力。图

1.18 展示了

将 LeNet-5 改造成

NIN 在

MNIST

上的训练过

程收敛曲线

。通过实验，我

们根据实验

结果得到了

3 个重要信息

：

z NIN 的参数数量

远大于同类

型的 CNN

；

z NIN 的收敛

速度快于经

典网络；

z NIN 的训

练速度慢于

经典网络。

图

1.18

NIN 与 LeNet-5 对比

通过

Keras 实现 NIN 的代码

片段如下，全

部实验内容

见随书资料

。

NIN = Sequential()

NIN.add(Conv2D(input_shape=(28,28,1),

filters= 8, kernel_size =

(5,5),

 padding =

'same',activation = 'relu'))

NIN.add(Conv2D(input_shape=(28,28,1),

filters= 8, kernel_size =

(1,1),

 padding =

'same',activation = 'relu'))

1.3

更宽：GoogLeNet

NIN.add(Flatten())

NIN.add(Dense(196,activation =

'relu'))

NIN.add(Reshape((14,14,1),input_shape = (196,1)))

NIN.add(Conv2D(16,(5,5),padding = 'same',activation =

'relu'))

NIN.add(Conv2D(16,(1,1),padding = 'same',activation

= 'relu'))

NIN.add(Flatten())

NIN.add(Dense(120,activation

= 'relu'))

NIN.add(Dense(84,activation =

'relu'))

NIN.add(Dense(10))

NIN.add(Activation('softmax'))

NIN.summary()

对比全

连接，NIN 中的 1 ×

1 卷

积操作保存

了网络隐层

节点和输入

图像的位置

关系，1 × 1

卷积

的

这个特点使

其在物体检

测和分割任

务上得到了

更广泛的应

用。除了保存

特征图的位

置关系，1 × 1

卷积

还有两个用

途：

z 实现特征

图的升维和

降维；

z

实现跨

特征图的交

互。

另外，NIN 提出

了使用全局

平均池化（global average

pooling）来

减轻全连接

层的过拟合

问题，

即在卷

积的最后一

层直接对每

个特征图求

均值，然后执

行 softmax 操作。

1.3.2 Inception v1

GoogLeNet

的核

心部件叫作

Inception。根据感受野

的递推公式

，不同大小的

卷积核对应

不同

大小的

感受野。例如

在 VGG 的最后一

层，1

× 1、3 × 3

和 5 × 5

卷积核

的感受野分

别是 196、228、

260。我们根

据感受野的

计算公式也

可以知道，网

络的层数越

多，不同大小

的卷积核对

应在原图

的

感受野的大

小差距越大

，这也就是

Inception 通

常在越深的

层次中效果

越明显的原

因。在每个

Inception 模

块中，作者并

行使用了

1 × 1、3 ×

3 和

5 × 5

这 3 个不同大

小的卷积核

。同时，考虑到

池

化一直在

CNN

中扮演着积

极的作用，所

以作者建议

Inception 中也要加入

一个并行的

步长为 1 的最

大池化。至此

，一个朴素版

本的

Inception 便诞生

了，如图 1.19 所示

。

图 1.19　朴素版本

的 Inception

但是这个

朴素版本的

Inception 会使网络的

特征图的数

量乘 4。随着 Inception

数

量的增长，特

征图的数量

会呈指数级

增长，这意味

着大量计算

资源被消耗

。为了提升运

算速度，Inception 使用

了

NIN 中介绍的

1

× 1 卷积在卷积

操作之前进

行降采样，由

此便诞生了

Inception v1，如图

1.20 所示。

Inception 的

代码也比较

容易实现，建

立

4 个并行的

分支并在最

后将其合并

到一起即可

。为了在

MNIST 数据

集上使用

Inception，我

使用了更窄

的网络（特征

图的数量均

为 4，官方特征

图的数量

已

注释在代码

中）。

17

第 1 章

基础

骨干网络

18

图

1.20 Inception

v1 结构

def inception(x):

inception_1x1 = Conv2D(4,(1,1), padding='same',

activation='relu')(x) #64

 inception_3x3_reduce

= Conv2D(4,(1,1), padding='same', activation='relu')(x)

#96

 inception_3x3 =

Conv2D(4,(3,3), padding='same', activation='relu')

(inception_3x3_reduce) #128

 inception_5x5_reduce

= Conv2D(4,(1,1), padding='same', activation='relu')(x)

#16

 inception_5x5 =

Conv2D(4,(5,5), padding='same', activation='relu')

(inception_5x5_reduce) #32

 inception_pool

= MaxPool2D(pool_size=(3,3), strides=(1,1), padding='same')(x)

#192

 inception_pool_proj =

Conv2D(4,(1,1), padding='same', activation='relu')

(inception_pool) #32

 inception_output

= merge([inception_1x1, inception_3x3, inception_5x5,

inception_pool_proj], mode='concat', concat_axis=3)

return inception_output

图 1.21

展示

了使用相同

通道数的卷

积核的 Inception 在 MNIST

数

据集上收敛

速度的对比

。从

实验结果

可以看出，对

于比较小的

数据集，Inception 的提

升非常有限

。对比两个网

络的容量，我

们

发现

Inception 和采

用相同特征

图的 3 ×

3 卷积拥

有相同数量

的参数，实验

内容见随书

资料。

图 1.21

Inception 与 CNN 对

比

1.3　更宽：GoogLeNet

19

1.3.3

GoogLeNet

GoogLeNet 的命

名方式是为

了致敬第一

代深度卷积

网络 LeNet-5，作者通

过堆叠

Inception 的方

法构造了一

个包含 9 个

Inception 模

块、共 22 层的网

络，并一举拿

下了

2014 年 ILSVRC 的物

体分

类任务

的冠军。GoogLeNet 的网

络结构如图

1.22 所示，高清大

图参考其论

文。

图

1.22 GoogLeNet 的网络

结构

对比其

他网络，GoogLeNet

的一

个最大的不

同是在中间

多了两个 softmax 分

支作为辅助

损失

（auxiliary

loss）函数。在

训练时，这两

个 softmax 分支的损

失会以 0.3

的比

例添加到损

失函数上。

根

据论文的解

释，该分支有

如下两个作

用：

z 保证较低

层提取的特

征也有分类

的能力；

z 具有

提供正则化

并解决梯度

消失问题的

能力。

需要注

意的是，在测

试的时候，这

两个 softmax

分支会

被移除。

辅助

损失函数的

提出，是为了

遵循信息论

中的数据处

理不等式（data processing inequality，

DPI）原

则。所谓数据

处理不等式

，是指数据处

理的步骤越

多，则丢失的

信息也会越

多，其数学建

模方式如式

（1.9）所示。

X → Y

→ Z; I(X;Z) ≤

I(X;Y) （1.9）

式（1.9）表明

，在数据传输

的过程中，信

息有可能消

失，但绝对不

会凭空增加

。反映到反向

传播中，也就

是在计算梯

度的时候，梯

度包含信息

的损失会逐

层减少，所以

GoogLeNet 的中间层

添

加了两组损

失函数以防

止信息的过

度丢失。

1.3.4 Inception v2

我们

知道，一个 5 × 5

的

卷积核与两

个 3 × 3

的卷积核

拥有相同大

小的感受野

，但是两个 3 × 3

的

卷积核拥有

更强的拟合

能力，所以在

Inception v21 的版本中，作

者将 5

× 5 的卷积

核替换为两

个

3

× 3 的卷积核

，如下面代码

所示。Inception v2

如图 1.23 所

示。

def

inception_v2(x):

 inception_1x1 =

Conv2D(4,(1,1), padding='same', activation='relu')(x)

inception_3x3_reduce = Conv2D(4,(1,1), padding='same',

activation='relu')(x)

1　参见 G.

E. Hinton、N. Srivastava、A. Krizhevsky

等人

的论文“Improving neural networks by

preventing co-adaptation of feature

detectors”。

第 1 章



基础骨干网

络

20

 inception_3x3

= Conv2D(4,(3,3), padding='same', activation='relu')

(inception_3x3_reduce)

 inception_5x5_reduce =

Conv2D(4,(1,1), padding='same', activation='relu')(x)

inception_5x5_1 = Conv2D(4,(3,3), padding='same',

activation='relu')

 (inception_5x5_reduce)

inception_5x5_2 = Conv2D(4,(3,3), padding='same',

activation='relu')

 (inception_5x5_1)

inception_pool = MaxPool2D(pool_size=(3,3), strides=(1,1),

padding='same')(x)

 inception_pool_proj =

Conv2D(4,(1,1), padding='same', activation='relu')

(inception_pool)

 inception_output =

merge([inception_1x1, inception_3x3, inception_5x5_2,

inception_pool_proj], mode='concat', concat_axis=3)

return inception_output

图 1.23

Inception v2

1.3.5 Inception

v3

Inception v31 将

Inception v1 和 Inception

v2 中

的 n ×

n 卷积换成

一个 n ×

1 和一个

1 × n

的卷积，

如图

1.24 所示。这样做

带来的好处

有如下几点

：

（1）节约了大量

参数，提升了

训练速度，减

轻了过拟合

的问题；

（2）多层

卷积增加了

模型的拟合

能力；

（3）非对称

卷积核的使

用增加了特

征的多样性

。

def inception_v3(x):

inception_1x1 = Conv2D(4,(1,1), padding='same',

activation='relu')(x)

 inception_3x3_reduce =

Conv2D(4,(1,1), padding='same', activation='relu')(x)

inception_3x1 = Conv2D(4,(3,1), padding='same',

activation='relu')

 (inception_3x3_reduce)

inception_1x3 = Conv2D(4,(1,3), padding='same',

activation='relu')(inception_3x1)

 inception_5x5_reduce =

Conv2D(4,(1,1), padding='same', activation='relu')(x)

inception_5x1 = Conv2D(4,(5,1), padding='same',

activation='relu')

 (inception_5x5_reduce)

inception_1x5 = Conv2D(4,(1,5), padding='same',

activation='relu')(inception_5x1)

 inception_pool =

MaxPool2D(pool_size=(3,3), strides=(1,1), padding='same')(x)

inception_pool_proj = Conv2D(4,(1,1), padding='same',

activation='relu')

 (inception_pool)

1

参见 Christian Szegedy、Vincent Vanhoucke、Sergey

Ioffe 等人的

论文“Rethinking the Inception

Architecture for Computer Vision”。

1.3　更宽：GoogLeNet

21

inception_output = merge([inception_1x1, inception_1x3,

inception_1x5,

 inception_pool_proj], mode='concat',

concat_axis=3)

 return inception_output

图

1.24 Inception v3

1.3.6

Inception v4

Inception v41

的论文中提

出了 Inception v4、Inception-ResNet v1

和 Inception-ResNet v2 共

3 个

模型架构。其

中 Inception v4

延续了 Inception v2 和

Inception

v3 的思想，而 Inception-ResNet v1

和

Inception-ResNet v2 则将 Inception

和残差

网络进行了

结合。

Inception v4 的整体

结构如图

1.25 所

示，它的核心

模块是一个

骨干（Stem）模块、3 个

不同的

Inception

和两

个不同的缩

减（Reduction）模块。

图 1.25 Inception

v4 的

整体结构

图

1.26 所示的是

Inception v4 和

Inception-ResNet v2

的骨干模块

，它由线性结

构和包含两

路分

支的 Inception 结

构组成。特征

图的降采样

通过步长为

2

的卷积来完

成。图 1.26 中带有

“V”符号

的表示

padding

= 0 的有效卷积

。

图

1.27 所示的是

Inception v4 的

3 个 Inception 模块，这

3

个 Inception 模块并不

会改变输入

特

征图的尺

寸。从

3 个 Inception 模块

的结构中我

们可以看出

它基本沿用

了

Inception v2 和 Inception

v3

的思想

，即使用多层

小卷积核代

替单层大卷

积核和变形

卷积核。

1　参见

Christian

Szegedy、Sergey Ioffe、Vincent Vanhoucke 等人的论文

“Inception-v4,

Inception-ResNet and the Impact

of Residual Connections on

Learning”。

第 1 章

基础骨

干网络

图 1.26 Inception

v4 和

Inception-ResNet v2 的骨干模块

图

1.27 Inception v4 的

3 个 Inception 模块

22

1.3　更宽：GoogLeNet

图 1.28

所示

的是 Inception v4 的缩减

模块，它也沿

用了

Inception v2 和 Inception

v3 的思

想，并

使用步

长为 2

的卷积

或者池化来

进行降采样

，其中 Reduction-A 也复用

到了 1.3.7

节介绍

的 Inception￾ResNet 模块中。

图

1.28

Inception v4 的缩减模块

1.3.7 Inception-ResNet

Inception-ResNet 共有 v1 和

v2 两个

版本，它们都

将 Inception 和残差网

络的思想进

行了整合，

并

且拥有相同

的流程框架

，如图 1.29 所示。

图

1.29

Inception-ResNet v1 和 Inception-ResNet

v2 的流程框

架

1．Inception-ResNet v1

Inception-ResNet v1 的骨干模

块并没有使

用并行结构

，仅仅由不同

类型（填充、步

长和卷积核

尺寸）的卷积

操作组成，如

图 1.30

所示。

Inception-ResNet v1 在

Inception 模

块中插入了

一条捷径，也

就是将 Inception 和残

差网络的思

想

进行了结

合。它的 3 个 Inception

模

块的结构如

图 1.31 所示。

Inception-ResNet

v1 的 Reduction-A 复

用了

Inception v4 的 Reduction-A，它的

Reduction-B

的结

构如图

1.32 所示。

23

第 1 章

基

础骨干网络

图 1.30 Inception-ResNet v1

的骨干模

块

图 1.31 Inception-ResNet

v1 的 3 个

Inception 模

块的结构

图

1.32 Inception-ResNet

v1 的 Reduction-B 的结构

24

1.3　更

宽：GoogLeNet

25

2．Inception-ResNet v2

Inception-ResNet v2

采用了 Inception v4 的

骨干模块，它

的

Reduction-A 则和其他

两个模块保

持

相同，剩下

的 3

个 Inception 模块和

Reduction-B 的结构分别

如图

1.33 和图 1.34 所

示。

图 1.33 Inception-ResNet v2

的 3 个 Inception

模

块的结构

从

图 1.33 中可以看

出，Inception-ResNet

v2 的 3 个

Inception 模块

分别和 Inception-ResNet v1

的 3 个

Inception 模块保持了

相同的网络

结构，不同的

仅有通道数

。图

1.34 体现了 Inception-ResNet v2

的

Reduction-B 和图 1.32 的

Inception-ResNet v1 的 Reduction-B

一

样具有相同

架构、不同通

道数的特点

。

3．残差的缩放

作者重新研

究了残差连

接的作用，指

出残差连接

并不会明显

提升模型精

度，而是会加

快训练收敛

速

度。另外，引

入残差连接

以后，网络太

深了，不稳定

，不太好训练

，到后面模型

的参数可能

全变为0了，

可

通过引入尺

度变量（scale）来使

得网络更加

稳定，如图

1.35 所

示，其中 Inception 可以

用任意其他

子网

络替代

，将其输出乘

一个很小的

缩放系数（通

常在0.1到0.3内），激

活缩放之后

执行单位加

和ReLU激活。

图 1.34 Inception-ResNet

v2 的

Reduction-B 的结构 图

1.35 Inception v4 中

提出的残差

缩放

第 1 章

基

础骨干网络

26

1.4　跳跃连接：ResNet

在

本节中，先验

知识包括：

 VGG（1.2 节

）； 

GoogLeNet（1.3 节）；

 BN（6.2

节）；  LSTM（4.1 节）；

 Dropout（6.1 节）。

在

VGG

中，网络深度

达到了 19 层。在

GoogLeNet 中，网络史无

前例地达到

了

22 层。那么，网

络的精度会

随着网络的

层数增多而

提高吗？在深

度学习中，网

络层数增多

一般会伴随

下面几个问

题：

（1）计算资源

过度消耗；

（2）模

型容易过拟

合；

（3）产生梯度

消失或梯度

爆炸问题。

问

题（1）可以通过

GPU 集群来解决

，对一个企业

来说，资源并

不是很大的

问题；问题（2）可

以通过采集

海量数据，并

配合 Dropout

正则化

等方法有效

避免；问题（3）可

以通过 BN 避免

。

貌似我们只

要“无脑”地增

加网络的层

数，就能从中

获益，但实验

数据给了我

们当头一棒

。作

者发现，随

着网络层数

的增加，网络

发生了退化

（degradation）的现象：随着

网络层数的

增多，训

练集

损失值逐渐

下降，然后趋

于饱和，当我

们再增加网

络深度时，训

练集损失值

反而会增大

。注意

这并不

是过拟合，因

为在过拟合

中训练集损

失值是一直

减小的。

当网

络退化时，浅

层网络能够

实现比深层

网络更好的

训练效果，这

时如果我们

把低层的特

征

传到高层

，那么效果应

该至少不比

浅层网络的

效果差，或者

说如果一个

VGG-100 网络在第 98 层

使

用的是和

VGG-16 第 14 层一模一

样的特征，那

么

VGG-100 的效果应

该会和 VGG-16 的效

果相同。

所以

，我们可以通

过在 VGG-100 的第 98

层

和 VGG-16 的第 14

层之

间添加一个

直接映射（identity 

mapping）来

实现此效果

。

从信息论的

角度讲，由于

DPI（数据处理不

等式，见

1.3.3 节）的

存在，在前向

传输的过程

中，

随着层数

的加深，特征

图包含的图

像信息会逐

层减少，而残

差网络加入

直接映射，保

证了 l

+ 1 层

的网

络一定比

l 层

的网络包含

更多的图像

信息。基于这

种使用直接

映射来连接

网络不同层

的思想，

ResNet（残差

网络）1 应运而

生。

1.4.1 残差网络

1．残差块

残差

网络是由一

系列残差块

组成的，残差

块的结构如

图 1.36

所示，其中

，weight 在 CNN 中是

指卷

积操作，addition 是指

单位加操作

。一个残差块

可以用式（1.10）表

示：

xl+1 =

xl

 + (xl

,Wl

) （1.10）

残差块分

成两部分：直

接映射部分

和残差部分

。h(xl

) 是直接映射

部分，即图 1.36 的

左侧；

F(xl

,Wl

) 是残差

部分，一般由

2

个或者 3 个卷

积操作构成

，即图 1.36

中右侧

包含卷积的

部分。在

CNN 中，xl 可

能和

xl+1 的特征

图的数量不

一样，这时候

就需要使用

1 × 1

卷积进行升

维或者降维

，

如图 1.37 所示。这

时，残差块表

示为式（1.11）：

（1.11）

1　参见

Kaiming He、Xiangyu

Zhang、Shaoqing Ren 等人的论文

“Deep Residual

Learning for Image Recognition”。

1.4　跳跃连接：ResNet

27

其

中，

，Wl

′ 是 1

× 1 的卷积

核，实验结果

表明 1

× 1 卷积对

模型性能提

升作用有限

，所

以一般在

升维或者降

维时才会使

用。

图 1.36　残差块

的结构 图

1.37　加

入 1 ×

1 卷积的残

差块

一般这

种版本的残

差块叫作 resnet_v1，Keras

代

码实现如下

：

def res_block_v1(x, input_filter,

output_filter):

 res_x =

Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(x)

res_x = BatchNormalization()(res_x)

res_x = Activation('relu')(res_x)

res_x = Conv2D(kernel_size=(3,3), filters=output_filter,

strides=1, padding='same')(res_x)

 res_x

= BatchNormalization()(res_x)

 if

input_filter == output_filter:

identity = x

else: #需要升维或

者降维

 identity

= Conv2D(kernel_size=(1,1), filters=output_filter, strides=1,

padding='same')(x)

 x =

keras.layers.add([identity, res_x])

 output

= Activation('relu')(x)

 return

output

2．残差

网络

残差网

络的搭建分

为两步：

（1）按照

VGG

的架构搭建

一个普通的

VGG 网络；

（2）在普通

的 VGG

的 CNN 之间插

入单位映射

，注意需要升

维或者降维

的时候可加

入 1

× 1 卷积。

在实

现过程中，一

般采用直接

堆叠残差块

的方式。

def resnet_v1(x):

 x

= Conv2D(kernel_size=(3,3), filters=16, strides=1,

padding='same',

 activation='relu')(x)

x = res_block_v1(x, 16,

16)

 x =

res_block_v1(x, 16, 32)

x = Flatten()(x)

outputs = Dense(10, activation='softmax',

kernel_initializer='he_normal')(x)

 return outputs

第 1 章

基础骨干网

络

28

3．为什么叫

残差网络

在

统计学中，残

差和误差是

非常容易混

淆的两个概

念。误差衡量

的是观测值

和真实值之

间的差

距，残

差是指预测

值和观测值

之间的差距

。对于残差网

络的命名，作

者给出的解

释是，网络的

一层

通常可

以看作 y = H(x)，而残

差网络的一

个残差块可

以表示为

H(x) = F(x) +

x，也

就是 F(x) = H(x)

− x，

在单位

映射中，y =

x 便是

观测值，而 H(x) 是

预测值，所以

F(x)

便对应着残

差，因此叫作

残差网络。

笔

记　比如水位

线的高度，模

型预测为 10m，你

测量的是

10.4m，但

真实值为 10.5m。通

常你认为 10.4m 为

真实值，其实

它并不是。

1.4.2 残

差网络背后

的原理

残差

块一个更通

用的表示方

式如式（1.12）所示

：

（1.12）

现在我们先

不考虑升维

或者降维的

情况。在式（1.12）中

，h(·) 是直接映射

，f(·) 是激活函数

，

一般使用

ReLU。我

们首先给出

如下两个假

设。

z 假设 1：h(·)

是直

接映射。

z 假设

2：f(·) 是直接眏射

。

那么这时候

残差块可以

表示为式（1.13）：

（1.13）

对

于一个更深

的层 L，其与

l 层

的关系可以

表示为式（1.14）：

（1.14）

式

（1.14）反映了残差

网络的两个

属性：

z L 层可以

表示为任意

一个比它浅

的 l

层和它们

之间的残差

部分之和；

z ，L 层

是各个残差

块特征的单

位累加，而残

差网络中通

常使用单位

加

组合残差

块。

根据反向

传播中使用

的导数的链

式法则，损失

函数 ε 关于

xl 的

梯度可以表

示为式（1.15）：

（1.15）

式（1.15）反

映了残差网

络的两个属

性：

z 在整个训

练过程中， 不

可能一直为

− 1，也就是说在

残差网络中

不会出现梯

度消失的问

题；

z 表示 L 层的

梯度可以直

接传递到任

何一个比它

浅的

l 层。

1.4　跳跃

连接：ResNet

29

通过分

析残差网络

的正向和反

向两个过程

，我们发现当

残差块满足

上面两个假

设时，信息可

以

非常畅通

地在高层和

低层之间传

导，说明这两

个假设是让

残差网络可

以训练深度

模型的充分

条件。

那么这

两个假设是

必要条件吗

？

1．直接映射是

最好的选择

对于假设 1，我

们采用反证

法，假设 h(xl

)

= λlxl

，那么

这时候残差

块（见图 1.38（b））表示

为式（1.16）。

（1.16）

图 1.38　直接

映射的变异

模型

对于更

深的 L 层，残差

块表示为式

（1.17）。

（1.17）

为了简化问

题，我们只考

虑式（1.17）的左半

部分 ，损失函

数 ε 对

xl 求偏微

分得

式（1.18）：

（1.18）

式（1.18）反

映了两个属

性：

z 当 λ

＞ 1 时，很有

可能发生梯

度爆炸；

第

1 章

基础骨干网

络

30

z 当 λ ＜

1 时，梯度

变成 0，会阻碍

残差网络信

息的反向传

递，从而影响

残差网络的

训练。

所以

λ 必

须等于 1。同理

，其他常见的

激活函数都

会产生和上

面的例子类

似的阻碍信

息反向传

播

的问题。

对于

其他不影响

梯度的 h(·)，例如

LSTM 中的门机制

（见图 1.38（c）、图

1.38（d））或者

Dropout（见图 1.38（f））以及图

1.37 中用于降维

的 1

× 1 卷积（见图

1.38（e）），也许会有效

果。

作者采用

了实验的方

法进行验证

，实验结果如

表

1.1 所示。

表 1.1

残

差网络的变

异模型（均为

110 层）在 CIFAR-10 数据集

上的表现

变

异情况 图 捷

径类型 F

错误

率（%） 备注

原始

的 图

1.38（a） 1 1 6.61

常数缩

放 图 1.38（b） 0

1 失败 普

通网格

0.5

1 失败

0.5 0.5 12.35

冻结门

排他

门 图 1.38（c）

1−g(x) g(x) 失败 初

始化

bg = − 0

到 − 5

1−g(x)

g(x) 8.79 初始

化 bg

= − 6

1−g(x)

g(x) 9.81 初始化 bg

= − 7

捷

径门

图 1.38（d） 1−g(x) 1

12.86 初始

化 bg =

0

1−g(x) 1 6.91

初始化 bg = −

6

1 × 1

卷

积捷径 图 1.38（e） 1

× 1 卷

积 1

12.22

Dropout 捷径 图

1.38（f） Dropout 0.5 1

失

败

从表 1.1 的实

验结果中我

们可以看出

，在所有的变

异模型中，直

接映射依旧

是效果最好

的策略。

下面

是我们对图

1.38 中的各种变

异模型的分

析。

z 排他门（exclusive

gating）：在

LSTM 的门机制中

，绝大多数门

的值为 0 或者

1，很难落

到 0.5 附

近。当 g(x)

→ 0 时，残差

块只由直接

映射组成，阻

碍卷积部分

特征的传播

；当

g(x)

→ 1 时，直接映

射失效，残差

块退化为普

通 CNN。

z 捷径门（short-only gating）：当

g(x) →

0 时，网络便是

图 1.38（a）展示的由

直接映射组

成

的残差网

络；当

g(x) → 1 时，残差

块退化为普

通

CNN。

z Dropout 捷径：类似

于将直接映

射乘

1 − p，所以会

影响梯度的

反向传播。

z

1 × 1 卷

积捷径：1

× 1 卷积

比直接映射

拥有更强的

表示能力，但

是实验效果

不如直接映

射，

这更可能

是优化问题

而非模型容

量问题。

所以

我们可以得

出结论：假设

1 成立，即式（1.19）成

立。

（1.19）

2．激活函数

的位置

原始

的残差网络

中提出的残

差块可以扩

展为更多形

式，如图 1.39（a）所示

，即在卷积之

后使

用了 BN，然

后在和直接

映射单位加

之后使用了

ReLU

作为激活函

数。

在前文中

，我们得出假

设“直接映射

是最好的选

择”，所以我们

希望构造一

种结构能够

满足直

接映

射要求，即定

义一个新的

残差结构 ，如

式（1.20）所示。

（1.20）

式（1.20）反

映到网络里

就是将激活

函数移到残

差部分使用

，即图 1.39（c）所示的

网络，这

种在

卷积之后使

用激活函数

的方法叫作

后激活（post-activation）。然后

，作者通过调

整

ReLU 和 BN

1.4

跳跃连

接：ResNet

31

的使用位

置得到了几

个变异模型

，即图 1.39（d）中的只

有

ReLU 的预激活

和图 1.39（e）中的全

部

预激活。作

者通过对照

实验对比了

这几种变异

模型，结果如

表

1.2 所示。

图 1.39

激

活函数在残

差网络中的

使用

表 1.2　基于

激活函数位

置的变异模

型在

CIFAR-10 上的实

验结果

情况

图 ResNet-110

ResNet-164

传统残差

单元 图 1.39（a）

6.61 5.93

BN 在单

位加之后

图

1.39（b） 8.17 6.50

ReLU

在单位加之

前 图 1.39（c） 7.84

6.14

只有 ReLU 的

预激活

图 1.39（d） 6.71 5.91

全

部预激活 图

1.39（e） 6.37 5.46

实验结果表

明将激活函

数移动到残

差部分可以

提高模型的

精度。该网络

一般叫作残

差网络 v2，

Keras 实现

如下：

def res_block_v2(x, input_filter, output_filter):

res_x = BatchNormalization()(x)

res_x = Activation('relu')(res_x)

res_x = Conv2D(kernel_size=(3,3), filters=output_filter,

strides=1, 

 padding='same')(res_x)

res_x = BatchNormalization()(res_x)

res_x = Activation('relu')(res_x)

res_x = Conv2D(kernel_size=(3,3), filters=output_filter,

strides=1, 

 padding='same')(res_x)

if input_filter == output_filter:

identity = x

else: #需要升

维或者降维

identity =

Conv2D(kernel_size=(1,1), filters=output_filter, strides=1,

padding='same')(x)

 output= keras.layers.add([identity,

res_x])

 return output

def resnet_v2(x):

 x

= Conv2D(kernel_size=(3,3), filters=16 ,

strides=1, padding='same',

activation='relu')(x)

 x =

res_block_v2(x, 16, 16)

x = res_block_v2(x, 16,

32)

第 1 章

基础骨

干网络

32

 x

= BatchNormalization()(x)

 y

= Flatten()(x)

 outputs

= Dense(10, activation='softmax', kernel_initializer='he_normal')(y)

return outputs

一个

残差网络的

搭建也采用

堆叠残差块

的方式，在选

择是否降维

的时候可选

择不同的残

差块。

残差网

络

v1 的网络结

构如图 1.40 所示

。

图 1.40　残差网络

v1 的网络结构

1.5

注意力：SENet

33

1.4.3 残差

网络与模型

集成

Andreas Veit 等人的

论文 1

指出残

差网络可以

从模型集成

的角度理解

。如图 1.41 所示，一

个 3

层

的残差

网络可以展

开成一棵含

有 8 个节点的

二叉树，而最

终的输出便

是这

8 个节点

的集成。而他

们

的实验也

验证了这一

点：随机删除

残差网络的

一些节点，网

络的性能变

化较为平缓

，而对 VGG

等

堆叠

到一起的网

络来说，随机

删除一些节

点后，网络的

输出将完全

随机。

图 1.41

残差

网络展开成

二叉树

1.5　注意

力：SENet

在本节中

，先验知识包

括：

 注意力机

制（4.2 节）； 

残差网

络（1.4 节）。

SENet2

的提出

动机非常简

单。传统的方

法是将网络

的特征图的

值直接传到

下一层，而

SENet 的

核心思想在

于建模通道

之间的依赖

关系，通过网

络的全局损

失函数自适

应地重新矫

正通道之间

特征

的相应

强度。简单地

讲，SENet 通过注意

力机制为每

一个通道学

习一个权值

。

SENet 由一系列连

续的 SE 块组成

，一个

SE 块包括

压缩（squeeze）和激发

（excitation）两个

步骤。其

中，压缩通过

在特征图上

执行全局平

均池化得到

当前特征图

的全局压缩

特征向量，特

征图

通过两

层全连接得

到特征图中

每个通道的

权值，并将加

权后的特征

图作为下一

层网络的输

入。从上

面的

分析中我们

可以看出，SE 块

只依赖于当

前的一组特

征图，因此可

以非常容易

地嵌入几乎

现在

所有的

CNN 中。论文中给

出了在当时

最优的

Inception 插入

SE 块后的实验

结果，提升效

果显著。

SENet

虽然

引入了更多

的操作，但是

其带来的性

能下降尚在

可以接受的

范围之内，从

十亿次浮

点

运算数每秒

（giga floating-point operations

per second，GFLOPS）、参数数量以

及运行时间

的实验

结果

上来看，SENet 带来

的时间损失

并不是非常

显著。

1.5.1 SE 块

SE

块的

结构如图 1.42 所

示。

网络的左

半部分包含

一个传统的

卷积变换，忽

略这一部分

并不会影响

我们对

SENet 的理

解。我

们直接

看一下右半

部分，其中 U

是

一个 W × H

× C 的特征

图，（W,H）是图像的

尺寸，C 是图像

的通

道数。

1　参

见 Andreas

Veit、Michael Wilber、Serge Belongie 的论文“Residual

Networks Behave Like Ensembles

of Relatively Shallow Networks”。

2　参

见 Jie Hu、Li

Shen、Gang Sun 的论文“Squeeze-and-Excitation Networks”。

第

1 章  基础骨干

网络

34

图 1.42 SE

块的

结构

经过 fsq(·)（压

缩操作）后，图

像变成了一

个 1

× 1 × C

的特征向

量，特征向量

的值由 U 确定

。

经过

fex(·，W) 后，特征

向量的维度

没有变，但是

向量值变成

了新的值。这

些值会通过

和 U 的

fscale(·,·) 得到加

权后的 。 和

U 的

维度是相同

的。

1．压缩模块

压缩模块的

作用是获得

特征图 U

的每

个通道的全

局信息嵌入

（特征向量）。在

SE 块中，这一

步

通过 VGG

中引入

的全局平均

池化实现。也

就是通过求

每个通道 c ∈ {1,C}

的

特征图的平

均值 zc

实现，如

式（1.21）所示。

（1.21）

通过

全局平均池

化得到的特

征值是全局

的（虽然比较

粗糙）。另外，zc 也

可以通过其

他方法得

到

，要求只有一

个，得到的特

征向量具有

全局性。

2．激发

模块

激发模

块的部分作

用是通过 zc 学

习 C

中每个通

道的特征权

值，要求有两

点：

z 要足够适

配，这样能保

证学习到的

权值比较具

有代表性；

z

要

足够简单，这

样不至于添

加 SE 块之后网

络的训练速

度大幅降低

。

通道之间的

关系是非排

他（non-exclusive）的，也就是

说学习到的

特征能够激

励重要的特

征，

抑制不重

要的特征。

根

据上面的要

求，SE 块使用了

两层全连接

构成的门机

制（gate mechanism）。门控单元

s（即图

1.42 中 1 ×

1 × C 的特

征向量）的计

算方式表示

为式（1.22）：

（1.22）

其中，δ 表

示 ReLU

激活函数

，σ 表示 sigmoid 激活函

数。

、 分别是两

个全连接

层

的权值矩阵

。r 则是中间层

的隐层节点

数，论文中指

出这个值是

16。

得到门控单

元 s 后，最后的

输出 表示为

s

和 U 的向量积

，即图 1.42

中的 fscale(·,·) 操

作如式

（1.23）所示

：

（1.23）

其中， 是 的一

个特征通道

的一个特征

图，sc

是门控单

元 s（向量）中的

一个标量值

。

以上就是 SE

块

算法的全部

内容，SE 块可以

从两个角度

理解：

z SE

块学习

了每个特征

图的动态先

验；

z SE 块可以看

作在特征图

维度的自注

意力，因为注

意力机制的

本质也是学

习一组权值

。

1.5.2 SE-Inception 和 SE-ResNet

SE 块的特性

使其能够非

常容易地和

目前主流的

卷积结构结

合，例如论文

中给出的 Inception 结

1.5

注意力：SENet

构和

残差网络结

构，如图 1.43 所示

。它们的结合

方式也非常

简单，只需要

在

Inception 块或者残

差

块之后直

接接上 SE

块即

可。

图 1.43 SE-Inception

和 SE-ResNet

1.5.3 SENet

的复

杂性分析

SENet 的

本质是使用

自注意力机

制根据特征

图的值学习

每个特征图

的权值。U 往往

是一个由几

万个节点值

组成的三维

矩阵，但是我

们得到的

s 却

只有 C 个值，这

种

H × W 程度的压

缩具有非常

高

的可操作

性。例如将 U 展

开成 (W

× H ×C) ×

1 的特征

向量，然后通

过全连接得

到 s，这也是目

前主流

的特

征图到全连

接的连接方

式（Flatten()

操作）。而且

采用这种方

式得到的 s 的

效果往往优

于采

用

SE 块的

策略得到的

。但是 SENet 没这么

做，原因是

SENet 是

可以添加到

网络中的任

意一层之后

的，而全连接

操作往往是

整个网络结

构的性能瓶

颈，尤其是当

网络的节点

数非常大时

。

论文中主要

对比了 ResNet-50

以及

在 ResNet-50 中的每一

层后添加 SE

块

的网络在运

行性能各

方

面的指标。

z 从

计算性能的

方向分析，ResNet-50

需

要约 3.86GFLOPS，而 SE-ResNet-50 仅仅

多了约

0.01GFLOPS。

z 从预

测速度上来

看，ResNet-50 的运行时

间约

190ms，SE-ResNet-50 的运行

时间约 209ms，

多了

10%。

z 从参数数量

上来看，SE-ResNet-50 的参

数数量比 ResNet-50

的

参数数量（2 500 万

个参数）

多了

约

250 万个，约 10%。而

且作者发现

ResNet-50 最后几层的

SE

块可以省掉

，且对性

能影

响并不大，这

样的网络参

数仅多了 4%。

1.5.4

小

结

SENet 的思想非

常简单，即通

过特征图为

自身的每个

通道学习一

个特征权值

，通过单位乘

的方

式得到

一组加权后

的新的特征

权值。SENet

计算特

征权值的方

式是使用全

局平均池化

得到每个特

征

图的一维

表示，再使用

两层全连接

层得到最终

的结果。这个

方法虽然简

单，但是非常

实用，并且

35

第

1

章  基础骨干

网络

36

SENet 在 2017 年的

ILSVRC

上取得了非

常优异的成

绩。1.5.3 节对 SENet 复杂

性的分析引

发了我们

对

SE 块的进一步

联想：如何在

计算量和性

能之间进行

权衡？下面是

我的几点思

考。

（1）先通过感

兴趣区域池

化（region of

interest pooling，ROI 池化）得到

更小（如 3

× 3）的特

征图，再将其

展开作为全

连接的输入

。其中，ROI 池化指

任意大小的

特征图都可

以等分为某

一固定

格式

的池化窗口

的操作。

（2）在网

络的深度和

隐层节点的

数目之间进

行权衡，究竟

是更深的网

络效果更好

还是更宽的

网

络效果更

好。

（3）每一层的

SE 块是否一定

要相同？比如

作者发现浅

层更需要

SE 块

，那么我们能

否给浅层网

络使

用一个

计算量更大

但是性能更

好的 SE

块，而给

深层网络使

用更为简单

、高效的 SE 块，如

单层全连接

等。

1.6

更密：DenseNet

在本

节中，先验知

识包括：

 残差

网络（1.4

节）。

通过

残差网络的

论文，我们知

道残差网络

能够应用在

特别深的网

络中的一个

重要原因是

无论正向

计

算精度还是

反向计算梯

度，信息都能

毫无损失地

从一层传到

另一层。如果

我们的目的

是保证信息

毫

无阻碍地

传播，那么残

差网络的堆

叠残差块便

不是信息流

通最合适的

结构。

基于信

息流通的原

理，一个最简

单的思想便

是在网络的

每个卷积操

作中，将其低

层的所有特

征

作为该网

络的输入，也

就是在一个

层数为 L 的网

络中加入

个

捷径。DenseNet 中一个

密集块

（dense block）的设

计如图

1.44 所示

。为了更好地

保存低层网

络的特征，DenseNet1 是

将不同层的

输

出拼接在

一起，而不是

残差网络中

的单位加操

作。

图 1.44 DenseNet 中一个

密集块的设

计

1　参见 Gao Huang、Zhuang

Liu、Laurens van der Maaten

等人

的论文“Densely Connected Convolutional Networks”。

1.6　更密

：DenseNet

37

1.6.1

DenseNet 算法解析及

源码实现

在

DenseNet 中，如果全部

采用图

1.44 所示

的设计的话

，第 L 层的输入

是之前所有

的特征图拼

接到一起的

结果。考虑到

现今内存

/ 显

存空间的问

题，该设计显

然是无法应

用到网络比

较深的模型

中的，故而 DenseNet 采

用了图

1.45 所示

的堆积密集

块的网络结

构。下面我们

针对图 1.45 详细

介绍

DenseNet 算法。

图

1.45 DenseNet

网络结构

1．密

集块

在密集

块中，第 l

层的

输入 xl 是这个

块中前面所

有层的输出

拼接后的结

果，表示为式

（1.24）：

（1.24）

其中，方括号

[y0, y1, …, yl−1]

表示拼接操

作，即按照特

征图将 l − 1

个输

入拼接成一

个张量（tensor）。

Hl(·) 表示

合成函数。在

实现时，我使

用了 stored_features

变量存

储每个合成

函数的输出

。

def dense_block(x, depth=5,

growth_rate = 3):

nb_input_feature_map = x.shape[3].value

stored_features = x

for i in range(depth):

feature = composite_function(stored_features, growth_rate

= growth_rate)

 stored_features

= concatenate([stored_features, feature], axis=3)

return stored_features

2．合成函数

合

成函数（composite

function）位于

密集块的每

一个节点中

，其输入是拼

接在一起的

特征图，

输出

则是这些特

征图经过 BN →

ReLU → 3 ×

3 卷

积得到的结

果，其中卷积

的特征图的

数量被定义

为成长率 k。在

DenseNet 中，成长率

k 一

般是比较小

的整数，在论

文中，k = 12。为了更

高效地使

用

浅层的特征

图，DenseNet 使用了拼

接操作，但是

拼接在一起

的特征图的

数量一般比

较大。为了

提

高网络的计

算性能，DenseNet先使

用1 ×

1卷积将输

入数据降维

到4k，再使用3 × 3卷

积提取特征

，

作者将这一

过程标准化

为

BN → ReLU →

1 × 1 卷积→

BN → ReLU →

3 × 3 卷积

，这种结构被

定义

为 DenseNet-B。

def composite_function(x,

growth_rate):

 if DenseNetB:

#使用

DenseNet-B时加入1×1卷积

x = BatchNormalization()(x)

x = Activation('relu')(x)

x = Conv2D(kernel_size=(1, 1),

strides=1, filters=4 * growth_rate,

padding='same')(x)

 x

= BatchNormalization()(x)

 x

= Activation('relu')(x)

 output

= Conv2D(kernel_size=(3, 3), strides=1,

filters = growth_rate,

第 1 章

基础骨

干网络

38

 padding='same')(x)

return output

3．成长

率

成长率（growth

rate）k 是

DenseNet 的一个超参

数，反映的是

密集块中每

个节点的输

入数据的

增

长速度。在密

集块中，每个

节点的输出

均是一个

k 维

的特征向量

。假设整个密

集块的输入

数据是

k0 维的

，那么第

l 个节

点的输入便

是 k0 +

k × (l −

1) 维的。作者

通过实验验

证，k 一般取一

个比较小

的

值，作者在实

验中将

k 设置

为 12。

1.6.2

压缩层

在

图 1.45 中，密集块

之间的结构

叫作压缩层

（compression

layer）。压缩层有降

维和降采样

两个

作用。假

设密集块的

输出是 m 维的

特征向量，那

么下一个密

集块的输入

是

θm，其中 θ 是压

缩因子

（compression

factor），是一

个用户自行

设置的超参

数。当 θ 等于 1

时

，密集块的输

入和输出的

维度

相同；当

θ ＜ 1

时，网络叫作

DenseNet-C，在论文中，θ = 0.5。包

含瓶颈层和

压缩层的 DenseNet

叫

作 DenseNet-BC，其中，瓶颈

层的作用是

在卷积网络

中间加入一

个通道数特

别少的 1 ×

1 卷积

核

来减小通

道数量，从而

提高计算效

率。池化层使

用的是 2

× 2 的平

均池化层。

下

面是在

MNIST 数据

集上的 DenseNet 的核

心代码，完整

代码见随书

资料。

def dense_net(input_image, nb_blocks =

2):

 x =

Conv2D(kernel_size=(3,3), filters=8, strides=1, padding='same',

activation='relu')(input_image)

 for block

in range(nb_blocks):

 x

= dense_block(x, depth=NB_DEPTH, growth_rate

= GROWTH_RATE)

 if

not block == nb_blocks-1:

if DenseNetC:

 theta

= COMPRESSION_FACTOR

 nb_transition_filter

= int(x.shape[3].value * theta)

x = Conv2D(kernel_size=(1,1), filters=nb_transition_filter,

strides=1,

 padding='same', activation='relu')(x)

x = AveragePooling2D(pool_size=(2,2), strides=2)(x)

x = Flatten()(x)

x = Dense(100, activation='relu')(x)

outputs = Dense(10, activation='softmax',

kernel_initializer='he_normal')(x)

 return outputs

1.6.3 小结

DenseNet 具

有如下优点

：

z 信息流通更

为顺畅；

z 支持

特征重用；

z 网

络更窄。

由于

DenseNet 需要在内存

中保存密集

块的每个节

点的输出，此

时需要极大

的显存才能

支持较

大规

模的 DenseNet，这也导

致了现在工

业界主流的

算法依旧是

残差网络。

1.7　模

型集成：DPN

39

1.7　模型

集成：DPN

在本节

中，先验知识

包括：

 残差网

络（1.4 节）； 

DenseNet（1.6 节）；

 RNN（4.1

节）。

残

差网络和 DenseNet 是

捷径系列网

络的最为经

典的两个基

础网络，其中

残差网络通

过单位

加的

方式直接将

输入加到输

出的卷积上

，DenseNet 则通过拼接

的方式将输

出与之后的

每一层的输

入进行拼接

。本节介绍的

双路网络（dual path network，DPN）1

则

通过高阶 RNN（high order RNN，

HORNN）2 将

残差网络和

DenseNet 进行了融合

。所谓“双路”，即

一条路是残

差网络，另一

条路是

DenseNet。论文

的动机是通

过对残差网

络和

DenseNet 的分解

，证明残差网

络更侧重于

特征的复用

，

而 DenseNet

则更侧重

于特征的生

成，通过分析

两个模型的

优劣，将两个

模型有针对

性地组合起

来。

论文提出

了拥有两个

模型优点的

DPN，并一举获得

了 2017 年

ILSVRC 物体定

位任务的冠

军。

1.7.1 高阶

RNN、DenseNet 和残

差网络

1．高阶

RNN

假设

ht 是第 t 个

时间片的隐

层节点状态

，xt

是第 t 个时间

片的输入数

据，第 0

个时间

片的隐层状

态为 x0，即 h0 =

x0。假设

k 是当前计算

到的时间片

，对于每个时

间片，ft

k

(·)

表示特

征提取函数

，用

于将输入

函数中的 ht 转

换为对应的

特征；gk(·)

则表示

用于将之前

所有时间片

的特征进行

聚合的函数

，

则高阶 RNN 可以

抽象为式（1.25）。

（1.25）

2．DenseNet 与

高阶 RNN

DenseNet 采用的

是拼接的方

式，而高阶 RNN 则

采用的是单

位加操作，欲

联系

DenseNet 和高

阶

RNN，我们需要将

DenseNet 的拼接操作

转换为高阶

RNN

的单位加操

作。DenseNet 的核心便

是

通过跨层

的拼接实现

信息流的捷

径，表示为式

（1.26）：

（1.26）

即第 k 层的输

入由前 k

− 1 层的

输出拼接之

后经过一个

卷积得到，gk 为

合成函数，由

BN、激活函数

和

1 × 1 卷积组成。我

们这里忽略

BN

和激活函数

，那么 1 × 1

卷积操

作可以表示

为式（1.27）。

（1.27）

从式（1.27）中

我们可以看

出，对若干组

卷积来说，如

果它们输出

的特征图的

通道数是相

同的，

对输出

进行拼接再

进行

1 × 1 卷积等

价于分别对

每组特征单

独进行

1 × 1 卷积

再求和的操

作。

对高阶 RNN 来

说，f 和

g 的权值

是共享的，即

对于所有的

t 和 k，满足

以及

。但是对 DenseNet 来说

，每一层都有

自己的参数

，这也就意味

着对 DenseNet

来说，f

和

g 是不共享的

。因为在 DenseNet

中，当

使用之前层

的特征时，都

对其进行了

变换操作，这

也就

意味着

DenseNet 具有产生新

的特征的能

力。DenseNet 和高阶

RNN 的

关系如图 1.46 所

示，左图是

原

始的 DenseNet 结构，右

图显示了 DenseNet

和

高阶 RNN 的关系

，其中 z−1是时延

（time

delay）单

元，⊕是单位

加操作。

1　参见

Yunpeng

Chen、Jianan Li、Huaxin xiao 等人的论文

“Dual

Path Networks”。

2　参见

Rohollah Soltani、Hui Jiang 的论文

“Higher

Order Recurrent Neural Networks”。

第 1 章

基础骨

干网络

40

图 1.46

DenseNet 和

高阶 RNN 的关系

综上所述，DenseNet

是

不满足 和 时

的特殊的高

阶 RNN，且

DenseNet

具有侧

重于新特征

生成的能力

。

3．DenseNet 与残差网络

假设：如果对

于所有的

t 和

k，均满足 ，此时

DenseNet 将退化为残

差网络，也就

是说残差网

络是一种特

殊形式的

DenseNet。

证

明：我们这里

给式（1.25）添加一

个中间变量

rk，r0 = 0，则式（1.25）可以写

成式（1.28）

和式（1.29）的

形式。

（1.28）

hk =

gk(rk)

（1.29）

将式（1.28）和

式（1.29）组合在一

起便有了式

（1.30）。

（1.30）

式（1.30）中

。可以看

出，式（1.30）展示了

一个非常明

显的残差结

构。

证毕！

从式

（1.30）中我们可以

看出，当φ 之间

存在参数共

享时，即

残差

网络退化

为

一个传统的

RNN，如图 1.47 所示。图

1.47

中，φ(·) 是 RNN 的激活

函数，I(·)

是单位

映射，

是 RNN 的时

延单元。

图 1.47　残

差网络和 RNN

的

关系

1.7　模型集

成：DPN

41

图 1.48 更形象

地展示了残

差网络与 DenseNet

之

间的关系。图

1.48（a）展示了一个

标准的残

差

网络。图 1.48（b）是将

DenseNet 的拼接改变

成单位加之

后的表示，其

中绿色箭头

和橙色箭头

表

示的 1 × 1

卷积

分别表示式

（1.25）中的 和 ，这两

个 1

× 1 卷积都是

有独立系数

的，当图 1.48（b）

中的

时便变成了

图 1.48（c），即图 1.48（c）是满

足 的

DenseNet，也就

是

一个残差网

络。

在图 1.48（b）和图

1.48（c）中可以看到

带下画线的

1

× 1 卷积，这些卷

积用于让图

1.48（c）

和图

1.48（a）对应，目

的是证明共

享参数后 DenseNet 会

退化为残差

网络，并无其

他的作用。

图

1.48

残差网络是

DenseNet 的一种特殊

形式

通过对

比残差网络

和 DenseNet

的原理，我

们来分析一

下两个网络

的优缺点。从

上面的描述

中

我们可以

看出，残差网

络复用了前

面网络提取

的特征，而每

一层的特征

都会原封不

动地传到下

一

层，这样每

一层提取的

特征都有其

不同点，因此

特征的冗余

度比较低。而

DenseNet 的每个

1 × 1

卷积

的参数都不

同，前面的层

不再被后面

的层直接使

用，而是被重

新加工后生

成了新的特

征，这

种结构

可能会造成

后面的层提

取的特征是

前面的层已

经提取过的

特征，所以说

DenseNet 是一个冗

余

度比较高的

网络。通过分

析可以看出

，残差网络的

特征复用率

高，但是冗余

度低，而 DenseNet

则可

以创造新的

特征，但是其

冗余度高。基

于此，作者结

合了两个网

络共同的优

点，创造了

DPN。

1.7.2 DPN

详

解

1．双路架构

基于上面的

分析，双路架

构（dual path architecture，DPA）以残差网

络为主要框

架，保证了特

征的低冗余

度，并在其基

础上添加了

一个非常小

的

DenseNet 分支，用于

生成新的特

征。DPA 的结

构可

以使用式（1.31）到

式（1.34）来表示。

（1.31）

第

1 章

基础骨干

网络

42

（1.32）

（1.33）

hk =

gk(rk )

（1.34）

式（1.31）中的

xk 表示一个

DenseNet 分

支，式（1.32）中的 yk 表

示一个残差

网络分支。式

（1.33）

将两个网络

通过单位加

的方式进行

了合并，式（1.34）使

用了转换函

数 gk 得到了一

个新的特征

。

DPA

的结构如图

1.49 所示，其左侧

是一个 DenseNet，右侧

是一个残差

网络，表示拆

分操作，⊕表

示

单位加操作

。

2．双路网络

图

1.50 展示了真正

的 DPN

结构，其和

图 1.49 的最大不

同在于残差

网络和 DenseNet

共享

了第

一个 1 ×

1 卷

积。在实际计

算 3 ×

3 卷积时，DPN 使

用了分组卷

积来提升网

络的性能。在

设计网络的

超参数时，残

差网络的通

道数也比 DenseNet

的

通道数多，防

止 DenseNet 随着层数

的增加引发

显存

消耗速

度过快的问

题。

图 1.49 DPA 的结构

图

1.50 DPN 的结构

和

其他网络一

样，我们也可

以通过堆叠

网络块的方

式来提升模

型的容量。

1.7.3 小

结

作者通过

一系列非常

精彩的推导

分析出了残

差网络和 DenseNet

各

自的优缺点

，通过将 CNN

抽象

化为高阶 RNN，得

出了残差网

络具有低冗

余度的优点

但是存在特

征重用的缺

点，也得出了

DenseNet

具有可以生

成新特征的

优点但是存

在冗余度过

高的缺点，因

此提出了结

合残差网络

和

DenseNet 的 DPN。

DPN 融合残

差网络和 DenseNet 是

基于投票的

模型集成的

方式实现的

。基于这个方

式，我们也

许

可以从下面

几个角度进

行进一步的

优化：

z 采用更

多种类的网

络分支，如 SENet、NAS

等

；

z 采用更好的

集成方式，例

如加上一个

注意力机制

为不同的网

络结构分支

学习不同的

权值，

因为极

有可能不同

的网络结构

在不同的深

度起着不同

的作用。

1.8　像素

向量：iGPT

43

1.8

像素向

量：iGPT

在本节中

，先验知识包

括：

 Transformer（4.3

节）；  BERT（5.4 节）；

 GPT-1、GPT-2、GPT-3（5.3 节）； 

层

归一化（6.3 节）。

GPT 系

列证明了其

在

NLP 方向强大

的学习能力

。GPT 的训练不需

要人工标注

数据，借助于

语言

模型构

建损失函数

，可以提取到

泛化能力非

常强的预训

练语言模型

。那么能否将

这种无监督

学习的思

想

迁移到图像

分类中呢？本

节要介绍的

图像 GPT1

（iGPT）便使用

GPT-22 的网络结构

进行图像特

征的建

模，然

后将特征直

接应用到下

游的分类任

务中的算法

。实验结果表

明，iGPT 拥有强大

的图像理解

能力，

不仅在

诸多分类数

据集上取得

了非常好的

分类效果，更

惊艳的是它

在图像补全

上的表现，如

图 1.513

所

示，在实

验中，输入图

像的下半部

分会被遮住

，iGPT 使用上半部

分的输入来

预测下半部

分的内容。

图

1.51

iGPT 的图像补全

效果

1　参见

Mark Chen、Alec Radford、Rewon Child

等

人的论文“Generative Pretraining from Pixels”。

2　参

见 Alec Radford、Jeffrey

Wu、Rewon Child 等人的论

文“Language Models

are Unsupervised Multitask Learners”。

3　图片来源

：OpenAI 官网。

第

1 章  基

础骨干网络

因为GPT-2使用的

是且仅是Transformer1

，所

以iGPT也是一个

完全无卷积

或者池化的

神经网络，

引

领了使用 Transformer 完

成

CV 任务的浪

潮。和其他的

OpenAI 的论文非常

类似，iGPT 的论文

并

没有提出

新颖的模型

架构或者算

法思想，甚至

连网络都直

接照搬 GPT-2。这篇

论文的最大

贡献在于

突

破了使用 CNN

解

决图像问题

的思维困境

，赋予了图像

数据一种新

的特征表示

方式，使得 CV 和

NLP 领域之间的

差距缩到了

几乎为

0。基于

这一点，OpenAI 又推

出了用于图

像分类的零

样本模型

CLIP2 和

用于图像生

成的

DALL-e3

，这两个

模型的效果

依旧非常惊

艳。

1.8.1 iGPT

详解

iGPT 包含

预训练和微

调两个阶段

，其中在预训

练阶段，作者

对比了自回

归（auto regressive，AR）

的预测下

一个像素的

任务和类似

BERT（Bidirectional Encoder Representations from

Transformers）的掩码

语言

模型（masked language model，MLM）任务，即

预测被替换

为掩码的像

素。掩码语言

模型任务的

逻辑是使用

上下文来预

测被替换为

掩码符号的

像素的内容

。为了衡量

iGPT 的

提取图像特

征的能力，

作

者使用了线

性探测（linear probe）进行

验证，基于的

原理是：如果

模型能够比

较好地提取

特征，那

么在

这个特征上

直接进行分

类，分类任务

应该会取得

非常好的效

果。因为在线

性探测中，下

一个阶段

的

分类任务只

知道上一个

阶段模型产

生的特征，而

上一个阶段

的模型的结

果对下一个

阶段的分类

任务

来说是

一个“黑盒子

”，因此它能不

受模型架构

的影响而更

精确地衡量

特征的质量

。和线性探测

不同

的是，微

调是指使用

带标签的数

据对包含分

类层和特征

提取部分的

整个网络在

无监督训练

的基础上进

行参数值的

有监督微调

。iGPT

的核心内容

可以概括为

图 1.52，下面我们

对其进行详

细介绍。

图 1.52

iGPT 的

3 个核心部分

1．从 CV

到 NLP

众所周

知，语言模型

的输入是 1

维

的文本数据

，而图像是 2 维

的栅格数据

，如果想要将

Transformer 应用到图像

中，第一步便

是将图像转

换为

1 维的结

构。

Transformer 的核心是

注意力机制

，其中涉及了

大量的矩阵

运算，随着序

列长度的增

加，这些

运算

所涉及的计

算量呈指数

级增长。表 1.3 所

示的是 iGPT

论文

中使用的 3 个

数据集和其

图像样

本的

分辨率，它们

会分别将缩

放到不同的

IR

中来得到模

型的输入图

像。对 CIFAR-10/CIFAR-100

的 32

× 32 × 3

图像

来说，其展开

之后的序列

长度是 3 072，注意

力机制尚且

有能力处理

。但是对

ImageNet

的图

像来说，其展

开之后的序

列长度是 150 528，这

对 Transformer

来说就有

些力不从心

了。为了解决

这个问题，作

者首先将输

入图像进行

降采样，这里

将输入图像

的大小叫作

输入分辨率

（input resolution，IR），论文中的 IR 有

3

组，分别是 322 × 3、482

× 3、642 × 3。

1　参

见 Ashish Vaswani、Noam

Shazeer、Niki Parmar 等人的论

文“Attention Is

All You Need”。

2

参见 Alec Radford、Jong Wook

Kim、Chris Hallacy 等人

的论文“Learning Transferable

Visual Models From Natural

Language 

Supervision”。

3

参见

Aditya Ramesh、Mikhail Pavlov、Gabriel Goh

等人的论文

“Zero-Shot Text-to-Image Generation”。

44

1.8　像素向量：iGPT

45

表

1.3

iGPT 中 3 个数据集

的图像分辨

率

数据集 图

像分辨率

CIFAR-10/CIFAR-100 32

× 32 × 3

STL-10 96 × 96

× 3

ImageNet 224

× 224 × 3

对

最小的 322 × 3

的 IR 来

说，其计算量

依旧是极其

大的，但是如

果再降低分

辨率的话，图

像将

变得人

工不可分。为

了进一步缓

解计算压力

，作者对标准

的

(R, G, B) 图像数据

进行了

k = 512 的

k

均

值聚类，由此

得到的图像

仍能保持颜

色信息，但是

长度比 (R, G,

B) 图像

的短了 。上面

3 组

IR

转化后的

值分别是 322

、482

、642

。作

者将这个分

辨率叫作模

型分辨率（model resolution，MR）。

在

得到

Transformer 能够处

理的缩小了

分辨率的图

像之后，便要

将图像展开

成 1 维结构，iGPT

采

用的是光栅

扫描顺序，或

者叫作滑窗

扫描顺序，如

图 1.52（a）所示。

2．预训

练

具体地讲

，给定一个由

n

个无标签图

像组成的批

次样本 x = (x1,

x2, …, xn)，对于

其中的任意

一个

图像像

素顺序，iGPT

使用

了自回归模

型对其概率

密度进行建

模，如式（1.35）所示

：

（1.35）

其中，图像像

素顺序 π

是单

位排列的，也

就是按上面

说的光栅扫

描顺序排列

的。参数 θ 的优

化是通

过最

小化数据的

负对数似然

训练的，如式

（1.36）所示。

LAR = Ex ～

X[ − logP(x)] （1.36）

除了自

回归模型，论

文中另外一

个预训练任

务是类似 BERT 的

MLM。在 MLM

中，每个像

素

的索引会

有 0.15 的概率出

现在

BERT 掩码 M 中

（被替换为掩

码），我们的目

标便是使用

未被替换

为

掩码的像素

预测被替换

为掩码的像

素，如式（1.37）所示

，其中 x[1,n]\M 表示未

被替换为掩

码字符

的部

分。

LBERT = Ex ～

XEM[ − logP(xi

|x[1,n]\M)]

（1.37）

这一部分

如图 1.52（b）所示。

当

训练

CIFAR-10/CIFAR-100、STL-10 时，使用

的预训练数

据集是 ImageNet。在训

练 ImageNet

时，使用的

预训练数据

集是从网上

爬取到的 1 亿

张图片。

3．网络

结构

对于一

个输入序列

{x1, …, xn}，首先将每个

位置的标志

变成 d

维的嵌

入向量。iGPT 的解

码器由

L 个块

组成，对于第

l

+ 1 个块，它的输

入是 n

个 d 维的

嵌入向量 ，输

出是

n 个 d 维的

嵌入

向量 。iGPT 的

解码块使用

GPT-2 的网络结构

，如式（1.38）所示。

（1.38）

这

里层归一化

（layer normalization，LN）1 既作用于注

意力部分，又

作用于

MLP 部分

。

def block(x,

scope, *, past, hparams):

with tf.variable_scope(scope):

 nx

= x.shape[-1].value

 a,

present = attn(norm(x, 'ln_1'),

'attn', nx, past=past, hparams=hparams)

1　参见 Jimmy Lei

Ba、Jamie Ryan Kiros、Geoffrey E.Hinton

的论文

“Layer Normalization”。

第 1

章  基础骨

干网络

46

x = x +

a

 m =

mlp(norm(x, 'ln_2'), 'mlp', nx*4,

hparams=hparams)

 x =

x + m

return x, present

在进

行

Transformer 的自注意

力的计算时

，作者在原生

的自注意力

的基础上加

入了上三角

掩码，

原生的

自注意力（self-attention）的

计算方式如

式（1.39）所示：

（1.39）

其中

，Q、K、V 分别是基于

输入内容得

到的 3 个不同

的特征矩阵

（详见

Transformer 部分），dk 是

特

征

K 的特征

数。加入上三

角掩码后的

自注意力的

计算方式如

式（1.40）所示：

（1.40）

假设

上三角矩阵

为

b，w = QKT

，mask_attention

的计算方

式如式（1.41）所示

：

（1.41）

其中，ε 是一个

非常小的浮

点数。上三角

掩码

b 的生成

方式和 mask_attention 的核

心代码如下

。

def attention_mask(nd, ns, *,

dtype):

 i =

tf.range(nd)[:, None]

 j

= tf.range(ns)

 m

= i >= j

- ns + nd

return tf.cast(m, dtype)

def

mask_attn_weights(w):

 _, _,

nd, ns = shape_list(w)

b = attention_mask(nd, ns,

dtype=w.dtype)

 b =

tf.reshape(b, [1, 1, nd,

ns])

 w =

w * b -

tf.cast(1e10, w.dtype) * (1

- b)

 return

w

def multihead_attn(q, k,

v):

 # q、k、v

的形状为[batch, heads, sequence, features]

w = tf.matmul(q, k,

transpose_b=True)

 w =

w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))

if not hparams.bert:

w = mask_attn_weights(w)

w = softmax(w)

a = tf.matmul(w, v)

return a

如

上面代码所

示，在训练 BERT

的

MLM 时没有使用

mask_attention（if not hparams.bert）。

最后，通过 LN 得

到解码器的

最终输出。注

意在 iGPT

中，并没

有加入位置

编码，而是希

望模型能

够

自行学到这

种空间位置

关系。但是自

回归的模型

（例如 ELMo）并不是

全部需要自

行学习，因

为

它的光栅扫

描顺序在一

定程度对输

入数据的顺

序进行了建

模。对比传统

的

CNN 方法，iGPT 的

1.8

像

素向量：iGPT

47

一个

特殊点是它

具有排列不

变性，而 CNN

的预

测位置的值

更容易受到

其临近位置

的值的影响

。

iGPT 采用了 GPT-2

的多

层 Transformer 的 架

构， 和

GPT-2 的一个不同

点是使用了

稀疏

Transformer1

提出的

初始化的方

法。

iGPT 提供了 4

个

不同容量的

模型，分别是

iGPT-S、iGPT-M、iGPT-L 以及 iGPT-XL，它们的

不同点取决

于网络的层

数 L

和嵌入向

量的维度 d，它

们具体的值

和参数数量

如表 1.4 所示。

表

1.4　iGPT 中 3

个数据集

的图像分辨

率

模型 L d

参数

数量

iGPT-S 24 512

0.76 亿

iGPT-M 36

1 024 4.55 亿

iGPT-L 48 1 536

13.62 亿

iGPT-XL 60 3

072 68.01 亿

4．微调

在进

行微调（fine-tuning）时，首

先通过序列

尺度上的平

均池化将每

个样本的特

征 nL 变成 d

维

的

特征向量，如

式（1.42）所示。

f L

= < ni

L

>i （1.42）

然后

在 f

L 之上再添

加一个全连

接层得分类

的 logits，而微调的

目标则是最

小化交叉熵

损失 LCLF。

当同时

优化生成损

失 LGEN 和分类损

失时，优化目

标为 LGEN

+ LCLF。这样可

以得到更好

的结果，

其中

LGEN ∈

{LAR,LBERT}。

5．线性探测

iGPT 通

过将平均池

化作用到模

型的每一层

，对比不同层

不同的表征

能力，如式（1.43）所

示。

f l

 =

< ni

l

>i

, 0≤l≤L （1.43）

传统的线

性探测用于

提取最后一

层的特征，但

是 iGPT 中通过语

言模型训练

的模型中最

后一层

并不

是线性探测

效果最好的

一层，效果最

好的是中间

几层，如图

1.53 所

示。不同于传

统的监督模

型，iGPT 的中间层

具有更强的

表征能力，可

能是因为在

CNN 中，浅层的网

络更侧重于

提取图像的

表层信息，例

如颜色、纹理

等，而深层的

网络更侧重

于提取目标

值的信息。在

iGPT

中会预测像

素

点可能的

值，因此不管

深层网络或

者浅层网络

都不太适用

于分类，而中

间的层反而

会有更多的

图像

信息，因

此得到的线

性探测的准

确率更高。

图

1.53

iGPT 中不同层的

性能表现

1　参

见

Rewon Child、Scott Gray、Alec Radford

等人的论

文“Generating Long Sequences with

Sparse Transformers”。

第 1

章  基础

骨干网络

1.8.2

实

验结果分析

图 1.54 展示了不

同容量的模

型在不同的

线性探测下

的准确率，从

中我们可以

得出几条重

要的

结论：

z 对

比 3 个模型的

准确率，可以

看出容量越

大，线性探测

的准确率越

高；

z 对比模型

准确率和验

证损失可以

看出，线性探

测的准确率

和训练的验

证损失呈负

相关；

z 对比相

同损失值下

的不同模型

的准确率，我

们可以看出

模型容量越

大，它的泛化

能力

越强；

z 线

性探测准确

率的上升并

没有出现明

显放缓的趋

势，说明随着

模型容量的

增大，准确率

还

有继续提

升的空间。

图

1.54 iGPT 的 3

个模型的

线性探测的

准确率和验

证损失之间

的关系

图 1.55 则

展示了自回

归和

BERT 分别在

线性探测和

微调，以及在

单独训练和

集成训练下

的对

比结果

，可以得出如

下结论：

z

基于

自回归的训

练方式要优

于基于 BERT 的训

练方式；

z

在 ImageNet 数

据集中，加上

微调之后的

基于 BERT

的训练

方式要优于

基于自回归

的训练方式

；

z 集成了 BERT

和自

回归的方法

的效果是最

优的。

图 1.55 自回

归（下）和

BERT（上）的

效果对比，线

性探测（蓝色

）和微调（红色

）的效果对比

，

单独训练（浅

色）和集成训

练（深色）的效

果对比

48

1.9

Visual Transformer 之 Swin

Transformer

49

1.8.3 小

结

iGPT 打破了使

用卷积操作

进行图像处

理的传统，创

新性地使用

Transformer 进行图像处

理，并

且通过

类似构建语

言模型的方

式得到了泛

化能力非常

强的特征，在

线性探测或

者微调方法

上都取得

了

可以匹敌先

进的有监督

方法的效果

。iGPT 更惊艳的是

在图像补全

方向的效果

，从补全的效

果上

来看，iGPT 似

乎已经学到

了图像的本

质信息。最为

重要的是，我

们没有看到

iGPT

的性能上限

，通

过增大数

据量和模型

容量，iGPT 可以到

达一个新的

高度。

作为一

个使用

Transformer 解决

图像问题的

基石性模型

，iGPT 也有很多的

缺点。

z

iGPT 对计算

资源的要求

是非常高的

，iGPT-L 在 Tesla

V100 上的训练

约需 2 500

天，而同

性能的 MoCo1 模型

大概仅需要

70 天。iGPT

的参数数

量也是同性

能 CNN 的 2

～ 3 倍。

z

iGPT 目前

还只能处理

低分辨率图

片，把 ImageNet 的图片

的分辨率降

低到

32 × 32 无疑将

损失很多信

息，而基于

CNN 的

方法可以通

过滑窗的方

式轻松处理

大分辨率图

片，这个思

想

是值得 iGPT

借鉴

的。对比 iGPT 中使

用的 Transformer，Transformer-XL

则拥有

对长序

列更

强的建模能

力和更快的

预测速度，也

许 Transformer-XL 才是更适

合

iGPT 使用的网

络

结构。

z

iGPT 生成

的补全图像

还是依赖于

输入数据的

分布的，而这

种数据偏差

也是需要解

决的问

题之

一。

1.9

Visual Transformer 之 Swin

Transformer

在本节

中，先验知识

包括：

 Transformer（4.3

节）；  残差

网络（1.4 节）；

 LN（6.3 节）； 

iGPT（1.8 节

）。

自从 Transformer

在 NLP 任务

上取得突破

性进展之后

，业内一直尝

试着把 Transformer

用于

CV

领域。之前的

若干方法，如

iGPT、ViT2 等，都将 Transformer

用在

了图像分类

领域，这些方

法有两

个非

常严峻的问

题：

（1）受限于图

像的矩阵性

质，一个能表

达信息的图

像往往至少

需要几百个

像素点，而建

模这种

包含

几百个长序

列的数据恰

恰是

Transformer 的“天生

”缺陷；

（2）目前多

利用 Transformer

框架来

进行图像分

类，理论上来

讲利用其解

决检测问题

应该也比

较

容易，但是对

于分割这种

密集预测的

场景，Transformer 并不擅

长解决。

本节

提出的

Swin（Shift window）Transformer3 解决

了这两个问

题，并且在分

类、检测、分割

任

务上都取

得了非常好

的效果。Swin

Transformer 的最

大贡献是提

出了一个可

以广泛应用

到所有 CV

领域

的骨干网络

，并且大多数

CNN

中常见的参

数在 Swin Transformer 中也是

可以人工调

整的，例

如可

以调整网络

块数、每一块

的层数以及

输入图像的

大小等。该网

络的架构设

计非常巧妙

，是一个

非常

精彩的将 Transformer 应

用到图像领

域的架构，值

得我们去学

习。

1　参见 Kaiming He、Haoqi

Fan、Yuxin Wu 等人

的论文“Momentum Contrast

for Unsupervised Visual Representation

Learning”。

2　参见

Alexey Dosovitskiy、Lucas

Beyer、Alexander Kolesnikov 等人的论文

“An image

is worth 16 ×16

words: Transformers for

image recognition at scale”。

3　参见 Ze Liu、Yutong

Lin、Yue Cao 等人的

论文“Swin Transformer:

Hierarchical Vision Transformer using

Shifted Windows”。

第 1

章  基

础骨干网络

50

在

Swin Transformer 之前的 ViT

和

iGPT，都使用了小

尺寸的图像

作为输入，这

种直接调整

大

小的策略

无疑会损失

很多信息。与

它们不同的

是，Swin Transformer 的输入是

原始尺寸的

图像，例

如 ImageNet 的

224 ×

224 的图像。另外

Swin Transformer 使用的是

CNN 中

最常用的多

层次的网络

结构，在 CNN 中一

个特别重要

的特征是随

着网络层次

的加深，节点

的感受野在

不断扩大，这

个特

征在Swin Transformer中

也是满足的

。Swin Transformer的这种层次

结构，使得它

可以像FPN1

、U-Net2

等一

样完成分割

或者检测的

任务。Swin Transformer 和 ViT

的对

比如图 1.56 所示

。

图

1.56 Swin Transformer 和

ViT 的对比

本节将结合

Swin Transformer 的

PyTorch 源码对 Swin Transformer

论

文中的算法

细节以及代

码实现展开

介绍，并对该

论文中解释

模糊的点进

行具体分析

。学习完本节

后，你将更了

解 Swin 

Transformer

的结构细

节和设计动

机，现在我们

开始吧！

1.9.1 网络

结构详解

1．基

础结构

Swin Transformer 共提

出了 4

个网络

结构，从小到

大依次是 Swin-T、Swin-S、Swin-B 和

Swin-L，

为了绘图简

单，本节以最

简单的

Swin-T 作为

示例来讲解

。Swin-T 的网络结构

如图 1.57

所示。Swin 

Transformer 最

核心的部分

便是

4 个阶段

中的 Swin Transformer

块，它的

具体结构如

图 1.58 所示，

这一

部分的源码

如下。

class SwinTransformer(nn.Module):

 def

__init__(self, *, hidden_dim, layers,

heads, channels=3, num_classes=1000,

head_dim=32, window_size=7, downscaling_factors=(4, 2,

2, 2),

 relative_pos_embedding=True):

super().__init__()

 self.stage1 =

StageModule(in_channels=channels, hidden_dimension=hidden_dim,

 layers=layers[0],

downscaling_factor=downscaling_factors[0],

 num_heads=heads[0], head_dim=head_dim,

window_size=window_size,

 relative_pos_embedding=relative_pos_embedding)

self.stage2 = StageModule(in_channels=hidden_dim,

hidden_dimension=hidden_dim * 2, layers=layers[1],

1　参见 Tsung-Yi Lin、Piotr

Dollár、Ross Girshtick 等

人的论文“Feature Pyramid

Networks for Object Detection”。

2　参

见 Olaf Ronneberger、Philipp

Fischer、Thomas Brox 等人的论

文“U-Net: Convolutional

Networks for Biomedical Image

Segmentation”。

1.9　Visual Transformer

之 Swin Transformer

51

downscaling_factor=downscaling_factors[1],

 num_heads=heads[1], head_dim=head_dim,

window_size=window_size,

 relative_pos_embedding=relative_pos_embedding)

self.stage3 = StageModule(in_channels=hidden_dim *

2,

 hidden_dimension=hidden_dim *

4, layers=layers[2],

 downscaling_factor=downscaling_factors[2],

num_heads=heads[2], head_dim=head_dim,

 window_size=window_size,

relative_pos_embedding=relative_pos_embedding)

 self.stage4 =

StageModule(in_channels=hidden_dim * 4,

hidden_dimension=hidden_dim * 8, layers=layers[3],

downscaling_factor=downscaling_factors[3],

 num_heads=heads[3], head_dim=head_dim,

window_size=window_size,

 relative_pos_embedding=relative_pos_embedding)

self.mlp_head = nn.Sequential(

nn.LayerNorm(hidden_dim * 8),

nn.Linear(hidden_dim * 8, num_classes)

)

 def forward(self,

img):

 x =

self.stage1(img)

 x =

self.stage2(x)

 x =

self.stage3(x)

 x =

self.stage4(x) # (1, 768,

7, 7)

 x

= x.mean(dim=[2, 3]) #

(1, 768)

 return

self.mlp_head(x)

图 1.57 Swin-T

的网

络结构

从源

码中我们可

以看出 Swin Transformer

的网

络结构非常

简单，由 4 个阶

段和一个输

出组成，

非常

容易扩展。Swin

Transformer 的

4 个阶段的网

络结构是一

样的，每个阶

段仅对几个

基本的超参

数进行调整

，包括隐层节

点个数、网络

层数、多头自

注意的头的

个数、降采样

的尺度等，这

些超参

数在

源码中的具

体值如下所

示，本节也会

以这组超参

数对网络结

构进行详细

讲解。

net = SwinTransformer(

hidden_dim=96,

 layers=(2, 2,

6, 2),

 heads=(3,

6, 12, 24),

第

1 章  基

础骨干网络

52

channels=3,

 num_classes=3,

head_dim=32,

 window_size=7,

downscaling_factors=(4, 2, 2, 2),

relative_pos_embedding=True

)

2．块分裂 /

块合

并

在图 1.57 中图

像之后是

1 个

块分裂（patch partition），再之

后是 1

个线性

嵌入（linear embedding），

二者加

在一起就表

示 1

个块合并

（patch merging）。块合并部分

的源码如下

：

class PatchMerging(nn.Module):

def __init__(self, in_channels, out_channels,

downscaling_factor):

 super().__init__()

self.downscaling_factor = downscaling_factor

self.patch_merge = nn.Unfold(kernel_size=downscaling_factor,

stride=downscaling_factor, padding=0)

 self.linear

= nn.Linear(in_channels * downscaling_factor

** 2, out_channels)

def forward(self, x):

b, c, h, w

= x.shape

 new_h,

new_w = h //

self.downscaling_factor, w // self.downscaling_factor

x = self.patch_merge(x) #

(1, 48, 3136)

x = x.view(b, -1,

new_h, new_w).permute(0, 2, 3,

1) # (1, 56,

56, 48)

 x

= self.linear(x) # (1,

56, 56, 96)

return x

块 合

并 的 作

用 是

对 图 像

进 行

降 采 样

， 类

似 于 CNN 中

的

池 化 层。 块

合

并 主 要 是

通

过

nn.Unfold() 函数实现

降采样的，nn.Unfold() 的

功能是对图

像进行滑窗

，相当于卷积

操作

的第一

步，因此它的

参数包括窗

口的大小和

滑窗的步长

。根据源码中

给出的超参

数我们知道

这一

步降采

样的比例是

4，因此经过 nn.Unfold() 之

后会得到

个

长度为

4 × 4

× 3 = 48

的特

征向量，其中

3 是输入阶段

1 的特征图的

通道数，阶段

1 的输入是

RGB 图

像，因

此通道

数为 3，表示为

式（1.44）。

Z0 = MLP[Unfold(Image)] （1.44）

接着 view 和 permute

将

得到的向量

序列还原为

56 × 56 的二维矩阵

，linear

将长度是 48

的

特征向量映

射到 out_channels

的长度

，因此阶段 1 的

块合并的输

出向量维度

是 (1,56,56,96)。

可以看出

块分裂 / 块合

并起到的作

用类似 CNN

中通

过带有步长

的滑窗来降

低分辨率，再

通过

1 × 1

卷积来

调整通道数

。不同的是，在

CNN 中最常使用

的用于降采

样的最大池

化或者平均

池化往

往会

丢弃一些信

息，例如最大

池化会丢弃

窗口内的低

响应值，而采

用块合并的

策略并不会

丢弃其他

响

应，但它的缺

点是带来运

算量的增加

。在一些需要

提升模型容

量的场景中

，我们可以考

虑使用块

合

并来替代 CNN 中

的池化。

3．Swin

Transformer 的阶

段 n

如我们上

面分析的，图

1.57

中的块分裂

+ 线性嵌入就

表示块合并

，因此 Swin Transformer

的一

个

阶段便可以

看作由块合

并和 Swin Transformer

块组成

，源码如下。

class StageModule(nn.Module):

def __init__(self, in_channels, hidden_dimension,

layers, downscaling_factor,

1.9　Visual

Transformer 之

Swin Transformer

53

num_heads, head_dim, window_size, relative_pos_embedding):

super().__init__()

 assert layers

% 2 == 0,

# 为了确保同

时包含窗口

自注意力和

位移窗口自

注意力，我们

需要确保

 总

层数是2的整

数倍

self.patch_partition = PatchMerging(in_channels=in_channels,

out_channels=hidden_dimension,

 downscaling_factor=downscaling_factor)

self.layers = nn.ModuleList([])

for _ in range(layers

// 2):

 self.layers.append(nn.ModuleList([

SwinBlock(dim=hidden_dimension, heads=num_heads,

 head_dim=head_dim,

mlp_dim=hidden_dimension * 4,

shifted=False, window_size=window_size,

 relative_pos_embedding=relative_pos_embedding),

SwinBlock(dim=hidden_dimension, heads=num_heads,

 head_dim=head_dim,

mlp_dim=hidden_dimension * 4,

shifted=True, window_size=window_size,

 relative_pos_embedding=relative_pos_embedding),

]))

 def forward(self,

x):

 x =

self.patch_partition(x)

 for regular_block,

shifted_block in self.layers:

x = regular_block(x)

x = shifted_block(x)

return x.permute(0, 3, 1,

2)

4．Swin Transformer 块

Swin Transformer 块是

该算法的核

心点，它由窗

口多头自注

意力（window multi-head

self￾attention，W-MSA）和移位

窗口多头自

注意力（shifted-window multi-head self-attention，SW-MSA）

组成

，如图

1.58 所示。出

于这个结构

，Swin Transformer 的层数要为

2

的整数倍，一

层提供给

W-MSA，一

层提供给 SW-MSA。

图

1.58

Swin Transformer 块的具体结

构

第

1 章  基础

骨干网络

54

从

图 1.58 中我们可

以看出输入

该阶段的特

征

Zl − 1 先经过

LN 进

行归一化，再

经过 W-MSA 进行

特

征的学习，接

着通过残差

操作得到 。接

着通过 LN、MLP 和残

差操作，得到

这一层的输

出特征

Zl

。SW-MSA层的

结构和W-MSA层的

类似，不同的

是计算特征

部分分别使

用了SW-MSA和W-MSA。

可以

从上面的源

码中看出它

们除了 shifted

的 bool 值

不同，其他值

是完全一致

的。这一部分

可以

表示为

式（1.45）。

（1.45）

Swin Transformer 块的源码

如下所示，和

论文中不同

的是，LN

操作（PerNorm() 函

数）从自

注意

力之前移到

了自注意力

之后。

class

Residual(nn.Module):

 def __init__(self,

fn):

 super().__init__()

self.fn = fn

def forward(self, x, **kwargs):

return self.fn(x, **kwargs) +

x

class PreNorm(nn.Module):

def __init__(self, dim, fn):

super().__init__()

 self.norm =

nn.LayerNorm(dim)

 self.fn =

fn

 def forward(self,

x, **kwargs):

 return

self.fn(self.norm(x), **kwargs)

class SwinBlock(nn.Module):

def __init__(self, dim, heads,

head_dim, mlp_dim, shifted, window_size,

relative_pos_embedding):

 super().__init__()

self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,

heads=heads, head_dim=head_dim,

 shifted=shifted,

window_size=window_size,

 relative_pos_embedding=relative_pos_embedding)))

self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim,

hidden_dim=mlp_dim)))

 def forward(self,

x):

 x =

self.attention_block(x)

 x =

self.mlp_block(x)

 return x

5．W-MSA

W-MSA，顾名思

义，就是按窗

口的尺寸进

行自注意力

计算，与 SW-MSA 不同

的是，它不会

进

行窗口移

位，源码如下

。我们这里先

忽略 shifted 为 True

的情

况。

class WindowAttention(nn.Module):

def __init__(self, dim, heads,

head_dim, shifted, window_size,

relative_pos_embedding):

 super().__init__()

1.9

Visual Transformer 之 Swin

Transformer

55

 inner_dim

= head_dim * heads

self.heads = heads

self.scale = head_dim **

-0.5

 self.window_size =

window_size

 self.relative_pos_embedding =

relative_pos_embedding # (13, 13)

self.shifted = shifted

if self.shifted:

 displacement

= window_size // 2

self.cyclic_shift = CyclicShift(-displacement)

self.cyclic_back_shift = CyclicShift(displacement)

self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size,

displacement=displacement,

 upper_lower=True, left_right=False),

requires_grad=False) # (49, 49)

self.left_right_mask = nn.Parameter(create_mask(window_size=window_size,

displacement=displacement,

 pper_lower=False, left_right=True),

requires_grad=False) # (49, 49)

self.to_qkv = nn.Linear(dim, inner_dim

* 3, bias=False)

if self.relative_pos_embedding:

 self.relative_indices

= get_relative_distances(window_size) + window_size

- 1

 self.pos_embedding

= nn.Parameter(torch.randn(2 * window_size

- 1,

 2

* window_size - 1))

else:

 self.pos_embedding =

nn.Parameter(torch.randn(window_size ** 2,

window_size ** 2))

self.to_out = nn.Linear(inner_dim, dim)

def forward(self, x):

if self.shifted:

 x

= self.cyclic_shift(x)

 b,

n_h, n_w, _, h

= *x.shape, self.heads #

[1, 56, 56, _,

3]

 qkv =

self.to_qkv(x).chunk(3, dim=-1) # [(1,56,56,96),

(1,56,56,96), (1,56,56,96)]

 nw_h

= n_h // self.window_size

# 8

 nw_w

= n_w // self.window_size

# 8

 #

分成 h/M * w/M

个

窗口

 q, k,

v = map(lambda t:

rearrange(t, 'b (nw_h w_h)

(nw_w w_w) (h d)

-> 

 b

h (nw_h nw_w) (w_h

w_w) d', h=h, w_h=self.window_size,

w_w=self.window_size), qkv)

 #

q、k、v : (1, 3,

64, 49, 32)

# 按窗口

的尺寸逐个

进行自注意

力计算

 dots

= einsum('b h w

i d, b h

w j d ->

b h w i

j', q, k) *

self.scale # (1,3,64,49,49)

if self.relative_pos_embedding:

 dots

+= self.pos_embedding[self.relative_indices[:, :, 0],

self.relative_indices[:, :, 1]]

else:

 dots +=

self.pos_embedding

 if self.shifted:

dots[:, :, -nw_w:] +=

self.upper_lower_mask

 dots[:, :,

nw_w - 1::nw_w] +=

self.left_right_mask

 attn =

dots.softmax(dim=-1) # (1,3,64,49,49)

out = einsum('b h

w i j, b

h w j d

-> b h w

i d', attn, v)

第 1 章

基础骨干网

络

56

 out =

rearrange(out, 'b h (nw_h

nw_w) (w_h w_w) d

-> b (nw_h w_h)

(nw_w w_w) (h d)',

h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h,

nw_w=nw_w) # (1, 56,

56, 96)，窗口合并

out =

self.to_out(out)

 if self.shifted:

out = self.cyclic_back_shift(out)

return out

在 forward()

函数中首

先计算的是

Transformer 中介绍的 Q、K、V 这

3

个特征向量

，所以

to_qkv() 函数进

行的是线性

变换。这里使

用了一个实

现小技巧，即

只使用了一

个隐层节点

数为

inner_dim*3

的线性

变换，然后使

用 chunk(3) 操作将这

3 个特征向量

切开，因此

qkv 是

一个

长度为

3 的张量，每个

张量的维度

是

(56,56,96)。

之后的 map( )

函

数是实现 W-MSA 中

的 W

最 核 心 的

代

码， 该 函 数

是

通 过 einops 的

rearrange 实

现的。einops 是一个

可读性非常

高的实现常

见矩阵操作

的 Python

包，它可以

实现

矩阵转

置、矩阵复制

、矩阵重塑等

操作。最终通

过 rearrange 得到了

3 个

独立窗口的

权值矩阵，

它

们的维度均

是 (3,64,49,32)，这

4 个值的

意思如下。

z 3：多

头自注意力

的头的个数

。

z 64：窗口的个数

，首先通过块

合并将图像

的尺寸降到

56 × 56，因为窗口的

大小为

7，所以

总共剩下 8 × 8

= 64 个

窗口。

z

49：窗口的

像素的个数

。

z 32：隐层节点的

个数。

Swin

Transformer 让计算

区域以窗口

为单位的策

略极大地减

小了网络的

计算量，将复

杂度降低

到

了图像尺寸

的线性比例

。传统的 MSA

和 W-MSA 的

复杂度如式

（1.46）所示。

（1.46）

式（1.46）的计

算省略了 softmax 占

用的计算量

，这里以 Ω(MSA)

为例

，它的具体构

成如下。

z 代码

中的 to_qkv()

函数，用

于生成 Q、K、V 这 3

个

特征向量：Q = x×WQ、K =

x×WK、

V = x

× WV，其

中 x 是输入数

据，W

是用于计

算 3 个特征向

量的权值向

量。假设权值

矩阵

的特征

数是

C，x 的维度

是 (hw,C )，W

的维度是

(C,C )，那么这 3 项的

复杂度是

3hwC2

。

z 计

算

QKT ：Q、K、V 的维度均

是 (hw,C

)，因此它的

复杂度是 (hw)

2

C。

z softmax 之

后乘 V

得到 Z（即

源码中的 out）：因

为 QKT

的维度是

(hw,hw)，所以它的时

间

复杂度是

(hw)

2

C。

z Z 乘矩阵 WZ

得到

最终输出，即

代码中的 to_out() 函

数的结果，它

的时间复杂

度是 hwC2

。

通过 Transformer 的

计算式（1.47），我们

可以有更直

观的理解：在

Transformer

部分中我们

介

绍自注意

力是通过点

乘的方式得

到查询向量

和键向量的

相似度的，即

式（1.47）中的 QKT

。然后

通

过这个相

似度匹配值

向量。因此这

个相似度是

通过逐个元

素进行点乘

计算得到的

。如果比较的

范围

是一幅

图像，那么计

算的瓶颈就

在于整幅图

像的逐像素

比较，因此复

杂度是 (hw)

2

。而 W-MSA 是

在窗口内进

行逐像素比

较的，因此复

杂度是 M2

hw，其中

M 是 W-MSA 的窗口的

大小。式（1.47）

中 dk 是

特征向量 Q

或

K 或 V 的长度。

（1.47）

回

到代码，接下

来的 dots 变量便

是我们刚刚

介绍的

QKT

。关于

加入相对位

置编码，我们

放到

最后介

绍。attn 和

einsum 完成了

式（1.47）的整个流

程，然后再次

使用 rearrange 将维度

调

1.9　Visual Transformer 之

Swin Transformer

57

整回

(56,56,96)，最

后通过 to_out() 将维

度调整为超

参数设置的

输出维度的

值。

这里我们

介绍一下

W-MSA 的

相对位置编

码 B。首先这个

相对位置编

码是加在乘

归一化尺度

之

后的

dots 变量

上的，因此 Z 的

计算方式如

式（1.48）所示。因为

W-MSA

是以窗口为

单位进行特

征匹配的，所

以相对位置

编码的范围

也应该以窗

口为单位，它

的具体实现

见如下代码

。相对位置编

码的具体思

想参考 UniLMv21

。

（1.48）

def get_relative_distances(window_size):

 indices

= torch.tensor(np.array([[x, y] for

x in range(window_size)

for y in range(window_size)]))

distances = indices[None, :,

:] - indices[:, None,

:]

 return distances

单独

使用 W-MSA 得到的

网络的建模

能力是非常

差的，因为它

将每个窗口

当作一个独

立区域进

行

处理而忽略

了窗口之间

交互的必要

性。为了解决

这个问题，Swin

Transformer 提

出了 SW-MSA。

6．SW-MSA

SW-MSA 接在 W-MSA 之

后，因此只要

我们提供一

种和

W-MSA 不同的

窗口切分方

式便可以实

现跨窗口的

通信。SW-MSA 的窗口

切分方式如

图 1.59

所示。我们

之前说过，输

入阶段 1 的图

像的

尺寸是

56

× 56（见图 1.59（a）），W-MSA 的窗口

切分的结果

如图

1.59（b）所示。那

么我们如何

得

到和 W-MSA 不同

的切分方式

呢？

SW-MSA 的思想很

简单，将图像

循环上移和

循环左移半

个窗口

的大

小，那么图 1.59（c）中

的蓝色和红

色区域将分

别被移动到

图像的下侧

和右侧，如图

1.59（d）

所示。如果在

移位的基础

上再按照 W-MSA 切

分窗口，就会

得到和 W-MSA

不同

的窗口切分

方式。

图 1.59（d）中红

色框和蓝色

框分别是 W-MSA

和

SW-MSA 的切分窗口

的结果。这一

部分可以通

过

PyTorch 的

roll() 函数实

现，源码中是

CyclicShift() 函数：

class

CyclicShift(nn.Module):

 def __init__(self,

displacement):

 super().__init__()

self.displacement = displacement

def forward(self, x):

return torch.roll(x, shifts=(self.displacement, self.displacement),

dims=(1, 2))

其中，displacement 的

值是窗口宽

度除以

2。

图 1.59 SW-MSA

的

窗口切分方

式

1　参见 Hangbo

Bao、Li Dong、Furu Wei 等人

的论文“UniLMv2:

Pseudo-Masked Language Models for

Unified Language Model

Pre-Training”。

第 1 章



基础骨干网

络

这种窗口

切分方式引

入了一个新

的问题，即在

移位图像的

最后一行和

最后一列各

引入了一块

移位过来的

区域，如图 1.59（d）所

示。因为位移

图像的最右

边是由原始

图像的左右

两个边拼接

而

成，位移图

像的最下边

由原始图像

的上下两个

边拼接而成

。因为图像的

两个边不具

备明显的语

义相

关性，所

以计算位移

图像的右边

和下边是没

有意义的，即

只需要对比

图 1.59（d）所示的一

个窗口

中相

同颜色的区

域。我们以图

1.59（d）左下角的区

域（1）和右上角

的区域（2）为例

来说明 SW￾MSA

是怎

么解决这个

问题的。

区域

（1）移位行的计

算方式如图

1.60 所示。首先一

个 7

× 7 大小的窗

口通过线性

运算得到 Q、

K、V 这

3 个特征向量

的权值，如我

们介绍的，它

的维度是 (49,32)。在

这

49 行中，前 28 行

是按照

滑窗

的方式遍历

区域（1）的上半

部分得到的

，后 21 行则是遍

历区域（1）的下

半部分得到

的，此

时它们

对应的位置

关系依旧保

持上黄下蓝

。

图 1.60 SW-MSA 的区域（1）移

位行的计算

方式

接着便

计算 QKT

，根据分

块矩阵的矩

阵乘法，我们

知道在图中

相同颜色区

域相互计算

后会保持

颜

色不变，而黄

色和蓝色区

域计算后会

变成绿色，绿

色部分表示

相似度无意

义。在论文中

使用了

upper_lower_mask 将其

替换为掩码

，upper_lower_mask 是由 0

和无穷

大（Inf）组成的二

值

矩阵，最后

通过单位加

得到最终的

dots 变量。

upper_lower_mask

的计算

方式如下。

mask = torch.zeros(window_size

** 2, window_size **

2)

mask[-displacement*window_size:, :-displacement*window_size] =

float('-inf')

mask[:-displacement*window_size, -displacement*window_size:] =

float('-inf')

区

域（2）移位行的

计算方式和

区域（1）的类似

，不同的是区

域（2）是图像循

环左移之后

的

结果，如图

1.61 所示。因为区

域（2）是左右排

列的，所以它

得到的

Q、K、V 是条

纹状的，即先

逐行遍历。在

这 7 行中，都会

先遍历

4 个黄

色区域，再遍

历 3 个红色区

域。两个条纹

状的矩阵相

乘

后，得到的

相似度矩阵

是网络状的

，其中橙色区

域表示无效

区域，因此需

要网格状的

掩码 left_

right_mask 来进行

覆盖。

left_right_mask 的生成

方式如下面

代码所示。

mask =

torch.zeros(window_size ** 2, window_size

** 2)

mask =

rearrange(mask, '(h1 w1) (h2

w2) -> h1 w1

h2 w2', h1=window_size, h2=window_size)

58

1.9　Visual Transformer

之

Swin Transformer

59

mask[:,

-displacement:, :, :-displacement] =

float('-inf')

mask[:, :-displacement, :,

-displacement:] = float('-inf')

mask

= rearrange(mask, 'h1 w1

h2 w2 -> (h1

w1) (h2 w2)')

图

1.61 SW-MSA 的区域（2）移

位行的计算

方式

关于

upper_lower_mask 和

left_right_mask 这两个掩码

的值，读者可

以自己代入

一些值来验

证，可以设置

window_size 的值，然后将

displacement

的值设为 window_size 的

一半即可。

窗

口移位和掩

码的计算是

在

WindowAttention 类的第一

个 if 中实现的

，掩码的相加

是在

第二个

if 中实现的，最

后一个 if 则将

图像复原。

截

至目前，我们

对 Swin-T 的阶段 1

进

行了完整的

梳理，后面 3 个

阶段除了几

个超参数和

图像

的尺寸

与阶段

1 不同

，其他的结构

均保持一致

，这里不赘述

。

7．输出层

最后

我们介绍一

下

Swin Transformer 的输出层

。在阶段 4

完成

计算后，特征

的维度是 (768,7,7)。

Swin Transformer

先

通过全局平

均池化得到

长度为 768 的特

征向量，再通

过 LN

和全连接

得到最终

的

预测结果，如

式（1.49）所示。

（1.49）

1.9.2

Swin Transformer 家族

Swin Transformer

共提出了 4 个

不同尺寸的

模型，它们的

区别在于隐

层节点的长

度、每个阶段

的网络层数

、多头自注意

力机制的头

的个数，具体

值见下面的

代码。

def

swin_t(hidden_dim=96, layers=(2, 2, 6,

2), heads=(3,6,12, 24), **kwargs):

return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads,

**kwargs)

def swin_s(hidden_dim=96, layers=(2,

2, 18, 2), heads=(3,6,12,24),

**kwargs):

 return SwinTransformer(hidden_dim=hidden_dim,

layers=layers, heads=heads, **kwargs)

def

swin_b(hidden_dim=128, layers=(2, 2, 18,

2), heads=(4,8,16,32), **kwargs):

return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads,

**kwargs)

第 1 章

基

础骨干网络

60

def swin_l(hidden_dim=192, layers=(2,

2, 18, 2), heads=(6,12,24,48),

**kwargs):

 return SwinTransformer(hidden_dim=hidden_dim,

layers=layers, heads=heads, **kwargs)

因为

Swin Transformer 是一个

多阶段的网

络结构，而且

每一个阶段

的输出都是

一组特征图

，所

以我们可

以非常方便

地将其迁移

到几乎所有

CV

任务中。作者

的实验结果

也表明，Swin Transformer

在检

测和分割领

域达到了先

进的 CNN

分类模

型的水平。

1.9.3 小

结

Swin

Transformer 是近年来

为数不多的

让人兴奋的

算法，它让人

兴奋的原因

有 3 个。

z 解决了

长期困扰业

界的将 Transformer 应用

到

CV 领域时出

现的速度慢

的问题。

z 设计

非常巧妙，具

有新颖又紧

扣

CNN 的优点，充

分考虑 CNN 的位

移不变性、尺

寸不变

性、感

受野与层次

的关系、分阶

段降低分辨

率以增加通

道数等特点

。没了这些特

点，Swin 

Transformer 是无法被

称为一个骨

干网络的；

z 在

诸多 CV 领域有

先进的表现

。

当然，我们对

Swin Transformer 还是要站在

一个客观的

角度来评价

的。虽然论文

中说 Swin

Transformer 是一个

骨干网络，但

是这样评价

还为时尚早

，原因如下。

z Swin

Transformer 并

没有提供一

个像反卷积

那样的上采

样的算法，因

此对于这个

问题，并

不能

直接使用 Swin

Transformer 替

换骨干网络

，也许可以采

用双线性插

值来实现，但

效果

如何还

需要评估。

z

从

1.9.1 节中我们可

以看出 W-MSA 每个

窗口都有一

组独立的

Q、K、T，因

此 Swin 

Transformer

并不具有

CNN 一个特别重

要的特性：权

值共享。这也

造成了 Swin Transformer

在速

度上和同级

别的 CNN 仍有不

小的差距。所

以就目前来

看，在嵌入式

平台上 CNN

还

有

着不可撼动

的地位。

1.10　Vision

Transformer 之 CSWin Transformer

在

本节中，先验

知识包括：

 Swin Transformer（1.9

节

）；  Transformer（4.3 节）；

 Xception（2.3 节）； 

残差网

络（1.4 节）。

感受野

是影响 CV

模型

效果至关重

要的属性之

一，因为模型

是无法对它

感知不到的

区域建模

的

。在 DeepLab 系列算法

中，空洞卷积

在不增加参

数数量的同

时可以快速

增加感受野

。之前介绍

的

Swin Transformer 仅仅通过移

动窗口来增

加感受野的

方式仍然过

于缓慢，因为

这个算法需

要通过

大量

堆叠网络块

的方式来增

加感受野。本

节要介绍的

CSWin（cross-shape

window）Transformer1 是

Swin Transformer

的改进版

，它提出了通

过十字形的

窗口来实现

自注意力机

制，不仅计算

效率非常

高

，而且能够通

过两层计算

获得全局的

感受野。CSWin Transformer 还提

出了新的编

码方式—局

部

加强位置编

码，进一步提

高了模型的

准确率。

1　参见

Xiaoyi Dong、Jianmin

Bao、Dongdong Chen 等人的论文

“CSWin Transformer:

A General Vision Transformer

Backbone 

with Cross-Shaped

Windows”。

1.10　Vision Transformer

之 CSWin Transformer

61

1.10.1 CSWin Transformer 概述

CSWin Transformer 的网

络结构如图

1.62 所示。它的输

入是一幅

3 通

道彩色图像

，尺寸为

H ×

W × 3，图像

首先经过一

组步长为 4

的

7 × 7 卷积，得到特

征图的尺寸

为

。这一点相

比

之前的直

接无重叠的

拆分是要有

所提升的。之

后 CSWin Transformer

分成 4 个阶

段，每个阶段

之间

通过步

长为

2 的 3 ×

3 卷积

来降采样，这

一点就和 VGG 等

CNN

结构很像了

。

图 1.62 CSWin

Transformer 的网络结

构

1.10.2 十字形窗

口自注意力

机制

本节的

核心是十字

形窗口自注

意力（cross-shaped window self-attention）机制，它

由并行的横

向

自注意力

和纵向自注

意力组成。对

于一个多头

的自注意力

模型，CSWin

Transformer 块将头

的一半分

给

横向自注意

力，另一半分

给纵向自注

意力，然后将

这两个特征

拼接起来，如

图 1.63

所示。假设

网

络有 K 个头

，其中

1, …, K/2 用于横

向自注意力

的计算，K/2

+ 1, …, K

用于

纵向自注意

力的计算。

图

1.63　十字形窗口

自注意力模

型

具体地讲

，我们假设模

型的输入特

征图是

X ，为了

计算它在横

向上的自注

意力，首

先将

它拆分成 个

横条的数据

。其中

sw 是横条

的宽度，在这

4 个不同的阶

段中取不同

的值，

实验结

果表明

[1,2,7,7] 这组

值在速度和

精度上取得

了比较好的

均衡。

对于每

个条状特征

Xi

,

i = 1, 2,

…, M，使用 Transformer 可以得

到它的特征

Yi

，最后将这 M 个

特

征拼接到

一起便得到

了这个头的

输入。我们假

设它属于第

k个头，那么横

向自注意力

H-Attentionk(X

)

的计算方式

如式（1.50）所示。

第

1 章

基础骨干

网络

62

（1.50）

其中，

、 、 是

Q、K、V 这

3 个向量的

映射矩阵。dk = C/K，它

的作用是保

证经过十字

形窗口自注

意力模型之

后特征图的

通道数保持

不变。

纵向自

注意力和横

向自注意力

的计算方式

类似，不同的

是纵向自注

意力取的是

宽度为 sw 的竖

条，表示为式

（1.51）。

（1.51）

最终，这个网

络块的输出

表示为式（1.52）：

（1.52）

其

中，O 表示输出

，

用来调整特

征图的通道

数，并可以将

两个不同方

向的自注意

力特征

进行

融合。

十字形

窗口自注意

力的一个非

常重要的属

性是它只需

要两层就可

以得到全局

感受野，对于

图

像中的一

点

pi, j

，它的当前

层的感受野

是它同行和

同列的像素

点： 。对于

pi, : 上任

意一点

pi,k,

k = 1,…, H，它的

感受野为

，所

以仅需要两

层就可以将

感受野扩充

到全图。

1.10.3 局部

加强位置编

码

因为

Transformer 与输

入顺序无关

，所以需要向

其中加入位

置编码。在 Transformer 的

论文中提

出

的绝对位置

编码（absolute position encoding，APE）和条件

位置编码（conditional position

encoding，

CPE）1 的

位置编码是

直接加到输

入数据 X

之上

的，如图 1.64（a）所示

。相对位置编

码（relative 

position

encoding，RPE）2 将位置编

码加入自注

意力内部，即

直接加入 softmax，如

图 1.64（b）所示。

图 1.64 Transformer 常

见的编码方

式

1　参见 Xiangxiang Chu、Zhi

Tian、Bo Zhang 等人

的论文“Conditional Positional

Encodings for Vision Transformers”。

2　参见

Peter Shaw、Jakob Uszkoreit、Ashish

Vaswani 的论文“Self-Attention with Relative

Position Representations”。

1.10　Vision

Transformer 之 CSWin Transformer

63

本

节提出的局

部加强位置

编码（Local enhanced Position

Encoding，LePE）直接将

位置编码添

加

到了值向

量上，该添加

操作是通过

将位置编码

E 和 V

相乘完成

的。然后通过

一个捷径将

添加了位

置

编码的 V 和通

过自注意力

加权的

V 单位

加到一起，如

图 1.64（c）所示，它的

计算方式如

式（1.53）

所示。

（1.53）

位置

编码 E 是一个

深度卷积，深

度卷积在位

置编码中的

作用是捕获

当前位置的

像素和它周

围

邻居之间

的位置关系

，如式（1.54）所示。从

另一个角度

看，CSWin Transformer 块是一个

由十字形

窗

口自注意力

和

CNN 组成的多

分支的结构

。

（1.54）

1.10.4

CSWin Transformer 块

CSWin

Transformer 块的网络

结构如图 1.65 所

示，它最显著

的特

点是添

加了两个捷

径，并使用 LN 对

特征进行归

一化，计算方

式如式

（1.55）所示

。



（1.55）

1.10.5 CSWin Transformer

的复杂度

最

后我们讨论

一下 CSWin Transformer

的复杂

度，它的计算

方式

如下。

对

于横向自注

意力： ，

，其中 W 有

Q、K、V 共

3 个特征向

量，X 有 M

组，

自注

意力机制共

有 K/2 个头，因此

这一部分的

复杂度如式

（1.56）所示。

（1.56）

， ，因此 QKT

的

复杂度如式

（1.57）所示。

（1.57）

， ，它们的

积的复杂度

如式（1.58）所示。

（1.58）

图

1.65 CSWin Transformer

块的网络结

构

第 1 章

基础

骨干网络

64

同

理对于纵向

自注意力，前

面 3

项的复杂

度依次为 、 、 H

2 × 

W

× C × sw。

最

后一项是拼

接头之后的

值乘 W O

，这一部

分的复杂度

是

H × W ×

C2

。综上，CSWin-Attention

的复

杂度为前面

7 项之和，最终

结果如式（1.59）所

示。

（1.59）

从式（1.59）中可

以看出，在比

较浅的层中

，H 和 W

的值比较

大，出于速度

方面的考虑

，这时

候建议

使用比较小

的 sw ；随着层数

的增加，H

和 W 可

能成比例缩

小，这时候就

可以使用感

受野更

大的

sw

了。

1.10.6 小结

CSWin

Transformer 披着

Transformer 的“外衣”，但的

确是 Transformer

和卷积

的混合算法

。在

模型的最

开始便是一

个有重叠的

7 × 7

卷积，接着每

个阶段可以

看作由十字

形窗口自注

意力和深度

卷积组成的

双分支结构

，在每个阶段

中又添加了

CNN 最为经典的

残差结构，而

每个阶段之

间又使

用步

长为 2

的 3 × 3

卷积

进行降采样

。至于 CSWin Transformer 最大的

创新点—十字

形窗口，其实

在

CNN 领域，2019 年出

现的 CCNet1

就提出

过的类似的

思想。

1.11　MLP? :MLP-Mixer

在本节

中，先验知识

包括：

 LN（6.3 节）；

 MobileNet（2.2 节）。

这

里介绍一个

争议非常大

的号称全部

由

MLP 组成的图

像分类模型

：MLP-Mixer2

。MLP-Mixer 诞

生后，很多

微信公众号

文章中宣称

“CNN 的时代”已经

过去了，那么

MLP-Mixer 真的有这么

神奇

吗？下面

我们来一步

步揭开它的

“神秘面纱”。

1.11.1 网

络结构

MLP-Mixer 的网

络结构如图

1.66

所示，它由 3 个

核心模块组

成：

z

在每个块

上的全连接

；

z 通道混合的

全连接；

z

像素

点混合的全

连接。

它的实

现如下：

class MlpMixer(nn.Module):

num_classes: int

 num_blocks:

int 

 patch_size:

int 

 hidden_dim:

int 

1　参见

Zilong

Huang、Xinggang Wang、Lichao Huang 等人的论文

“CCNet:

Criss-Cross Attention for Semantic

Segmentation”。

2　参见 Ilya

Tolstikhin、Neil Houlsby、Alexander Kolesnikov 等人的

论文“MLP-Mixer:

An all-MLP Architecture for

Vision”。

1.11　MLP? :MLP-Mixer

65

 tokens_mlp_dim: int

channels_mlp_dim: int

@nn.compact

 def __call__(self,

x):

 s =

self.patch_size

 x =

nn.Conv(self.hidden_dim , (s,s), strides=(s,s),

name='stem')(x)

 x =

einops.rearrange(x, 'n h w

c -> n (h

w) c')

 for

_ in range(self.num_blocks):

x = MixerBlock(self.tokens_mlp_dim ,

self.channels_mlp_dim)(x)

 x =

nn.LayerNorm(name='pre_head_layer_norm')(x)

 x =

jnp.mean(x, axis=1)

 return

nn.Dense(self.num_classes , name='head', kernel_init=nn.initializers.zeros)(x)

图 1.66 MLP-Mixer 的网

络结构

1．在每

个块上的全

连接

如图 1.66 所

示，MLP-Mixer

首先将图

像按照滑窗

的方式转换

成一个长度

为 S 的图像块

序列，

假设每

个图像块的

大小为

P，序列

的长度 S = H

× W/P2

，然后

在这个序列

上使用一个

共享的全连

接，

将其编码

为长度为

C 的

特征向量 。

这

时大家应该

已经看出这

一部分实际

上就是一个

步长为

P、卷积

核的大小也

是 P 的 CNN，这

里也

是使用卷积

实现这个操

作的。

2．混合层

MLP-Mixer 的骨干网络

是由 N

个混合

层（mixer layer）组成的，每

个混合层的

网络结构如

图 1.67

所示。混合

层的核心结

构是图

1.67 中的

两个 MLP，其中 MLP1

用

于标志混合

（token-mixing）全连

接块（红

框内），MLP2 用于通

道混合（channel-mixing）全连

接块（蓝框内

），它们的实现

如下：

class

MixerBlock(nn.Module):

 tokens_mlp_dim: int

channels_mlp_dim: int

@nn.compact

第 1 章

基

础骨干网络

66

 def __call__(self,

x):

 y =

nn.LayerNorm()(x)

 y =

jnp.swapaxes(y, 1, 2)

y = MlpBlock(self.tokens_mlp_dim ,

name='token_mixing')(y)

 y =

jnp.swapaxes(y, 1, 2)

x = x+y

y = nn.LayerNorm()(x)

return

x+MlpBlock(self.channels_mlp_dim , name='channel_mixing')(y)

图

1.67　混合层的

网络结构

在

标志混合中

，混合层先对

输入的 X

使用

LN 进行归一化

处理，然后对

其进行转置

，将输入数

据

的格式由通

道为宽、图像

块为高的矩

阵变成图像

块为宽、通道

为高的矩阵

。接着在每个

通道上使

用

一个权值共

享的

MLP 进行标

志之间的特

征加工，再使

用转置将矩

阵还原，最后

使用一个残

差结

构将处

理前后的两

个特征进行

拼接。由于这

一部分实现

的是同一通

道不同标志

之间的混合

，因此叫

作标

志混合。

可以

看出，图 1.67 中的

MLP1 其实就是

MobileNet1 中

介绍的深度

卷积（depthwise convolution），

其中卷

积核和步长

的大小均是

P。这里的

LN 则相

当于对整个

特征图进行

了一次白化

（whitening）。

图像白化是

传统的计算

机视觉中最

常用的归一

化手段，处理

的方式就是

将图像的像

素平均值变

为

0，方差变为

1。

在通道混合

中，混合层也

先使用 LN 对特

征进行归一

化，然后直接

使用一个共

享的 MLP

对特

征

图的通道特

征进行混合

计算。这里的

MLP2 其实就是 1

× 1 卷

积。通道混合

中还使用了

残差结构

进

行特征的拼

接。

在混合层

中每个 MLP 是由

两个全连接

和 GELU

激活函数

2

组成的。因此

混合层本质

上还是一个

由连续两个

深度卷积和

连续两个点

卷积组成的

深度可分离

卷积。

class MlpBlock(nn.Module):

mlp_dim: int

 @nn.compact

def __call__(self, x):

y = nn.Dense(self.mlp_dim)(x)

y = nn.gelu(y)

return nn.Dense(x.shape[-1])(y)

3．输出层

MLP-Mixer 的输出层使

用的是

CNN 中最

常使用的输

出结构：一个

全局平均池

化和一个全

连接。

1　参见

Andrew G.Howard、Menglong Zhu、Bo Chen

等

人的论文“MobileNets: Efficient Convolutional Neural

Networks for Mobile

Vision Applications”。

2　参

见

Dan Hendrycks、Kevin Gimpel 的论文“Gaussian

Error Linear Units (GELUs)”。

1.11　MLP? :MLP-Mixer

1.11.2

讨

论

1．结构

MLP-Mixer 从本

质上来说就

是一个特殊

形式的

CNN，无论

是在每个块

上的全连接

，还是混合

层

中的标志混

合和通道混

合，都是一种

特殊的卷积

形式。虽然说

MLP-Mixer 是一个全部

由 MLP

组成的模

型，但是最终

还是没有脱

离 CNN 的范畴，更

别说“MLP is

all you need”（你只需

要一个

MLP

结构

就够了）这种

耸人听闻的

报道了。这么

看来，LeCun 说 MLP-Mixer 是一

个“挂羊头，卖

狗肉”的算法

也就不奇怪

了。

MLP-Mixer 不仅没有

继承 CNN 的优点

，还失去了

CNN 的

灵活性，例如

全卷积对输

入图像尺

寸

的自由度等

。MLP-Mixer 的最大贡献

可能是给出

了

CNN 的一种全

连接实现。

2．效

果

MLP-Mixer

虽然在 ImageNet 上

取得了不错

的分类效果

，但是对比主

流的 CNN

或者基

于

Transformer 的方法仍

有一些差距

，且它的效果

还依赖于 JFT-300M

数

据集作为预

训练数据集

。当我

们发现

MLP-Mixer 就是一种特

殊的 CNN

之后，对

它的效果也

就不意外了

。

所以各位 CV 领

域的同行们

完全不必惊

慌，也没必要

被一些微信

公众号影响

，继续放心地

研究

CNN 吧！

67

CNN的另

外一个方向

是轻量级CNN，即

在不大幅度

1

降低模型精

度的前提下

，尽可能地压

缩模

型的大

小，以提高运

算的速度。轻

量级 CNN 的第一

个尝试是

SqueezeNet，SqueezeNet 的

策略是使

用

1 ×

1 卷积代替 3 ×

3 卷

积，它对标的

模型是 AlexNet。

轻量

级

CNN 最经典的

策略是深度

可分离卷积

，经典算法包

括 MobileNet v1

和 Xception。深度

可

分离卷积由

深度卷积和

点卷积组成

，深度卷积一

般是以通道

为单位的 3

× 3 卷

积，在卷积过

程中

不同通

道之间没有

信息交换。而

信息交换则

由点卷积完

成，点卷积就

是标准的

1 × 1 卷

积。深度可

分

离卷积的另

一个比较经

典的算法是

MobileNet v2，它将深度可

分离卷积和

残差结构进

行了结合，

并

通过一些列

理论分析和

实验得出了

一种更优的

结合方式。

轻

量级

CNN 的另外

一种策略是

选择在普通

卷积和深度

可分离卷积

之间的一个

折中方案，即

分组

卷积，它

是在 ResNeXt

中提出

的。所谓分组

卷积，是指在

深度卷积中

以几个通道

为一组的卷

积。分

组卷积

的问题是组

与组之间没

有信息交互

，这成了分组

卷积的性能

瓶颈。ShuffleNet v1 提出了

通道

洗牌策

略以加强不

同通道之间

的信息流通

，ShuffleNet v2 则通过分析

整个测试时

间，提出了在

内存

访问方

面更高效的

卷积方式。ShuffleNet

v2 得

出的结构是

一种和 DenseNet 非常

相似的密集

连接结构。

黄

高团队的 CondenseNet 则

是通过为每

个分组学习

一个索引层

的形式来完

成通道之间

的信息流通

。

2.1

SqueezeNet

在本节中，先

验知识包括

：

AlexNet（1.1 节）。

从 LeNet-5 到 DPN，再到

CSWin

Transformer，反映了 CV 的一

个发展方向

：提高精度。这

里

我们开始

对轻量级

CNN 的

介绍：在不大

幅度降低模

型精度的前

提下，最大程

度地提高运

算速度。

提高

运算速度有

两个方法：

z

减

少可学习的

参数的数量

；

z 减少整个网

络的计算量

。

提高运算速

度带来的效

果是非常明

显的：

z 可减少

模型训练和

测试时的计

算量，单个计

算步的速度

更快；

z 可减小

模型文件的

大小，更利于

模型的保存

和传输；

1　“不大

幅度”表示小

幅降、不变或

有提升。

第 2

章

轻量级 CNN

2.1

SqueezeNet

69

z 可学

习参数更少

，网络占用的

显存更小。

SqueezeNet1 正

是诞生在这

个环境下的

一个经典的

网络，它能够

在 ImageNet 数据集上

达到与

AlexNet 近似

的效果，但是

参数数量约

是 AlexNet 的

1/50，结合其

模型压缩技

术—深度压缩

（Deep 

Compression），模型文件大

小约为 AlexNet

的 1/510。

2.1.1 SqueezeNet

的

压缩策略

SqueezeNet 的

模型压缩使

用了 3

个策略

。

z 将 3

× 3 卷积替换

成 1

× 1 卷积：通过

这一步，一个

卷积操作的

参数数量减

少到原来的

1/9。

z

减少 3 × 3

卷积的

通道数：一个

3 × 3 卷积的计算

量是

3 × 3 ×

M × N（其中 M、N

分

别是输

入特

征图和输出

特征图的通

道数），作者认

为这样的计

算量过于庞

大，因此希望

将 M、N

减小以减

少参数数量

。

z 将降采样后

置：作者认为

较大的特征

图含有更多

的信息，因此

将降采样往

分类层移动

。注

意，这样的

操作虽然会

提升网络的

精度，但是有

一个非常严

重的缺点，即

会增加网络

的计

算量。

2.1.2 点

火模块

SqueezeNet 是由

若干个点火

（fire）模块结合

CNN 中

卷积层、降采

样层、全连接

层等组成的

。

一个点火模

块由压缩（squeeze）部

分和扩张（expand）部

分组成（注意

，其和 1.5

节中的

SENet 的

区别）。压缩

部分由一组

连续的 1

× 1 卷积

组成，扩张部

分则由一组

连续的 1

× 1 卷积

和一组连续

的

3

× 3 卷积拼接

组成，因此 3

× 3 卷

积需要使用

same 卷积。点火模

块的结构如

图

2.1 所示。

在点

火模块中，压

缩部分 1

× 1 卷积

的通道数记

作 s1×1，扩张部分

1

× 1 卷积和 3

× 3 卷积

的通道

数分

别记作

e1×1 和 e3×3

2

。在

点火模块中

，作者建议 s1×1 < e1×1

+ e3×3，这

么做相当于

在两个 3 ×

3 卷积

的中间加入

了瓶颈层。图

2.1 中 s1×1

= 3，e1×1 = e3×3

= 4。

图 2.1

点火模

块的结构

1　参

见 Forrest

N.Iandola、Song Han、Matthew W.Moskewicz 等

人 的 论

文“SqueezeNet: AlexNet-level

accuracy with 50x fewer

parameters and< 0.5 MB

model size”。

2　论文中的

图画得不好

，不要错误地

理解成卷积

的层数。

第 2 章

轻量级

CNN

70

下面

代码片段展

示了使用Keras实

现的点火模

块，注意，拼接

特征图的时

候使用的是

拼接操作，

这

样不必要求

e1×1

= e3×3。

def fire_model(x,

s_1x1, e_1x1, e_3x3, fire_name):

# 压缩部分

 squeeze_x

= Conv2D(kernel_size=(1,1),filters=s_1x1,padding='same',

 activation='relu',name=fire_name+'_s1')(x)

# 扩

张部分

 expand_x_1

= Conv2D(kernel_size=(1,1),filters=e_1x1,padding='same',

 activation='relu',name=fire_name+'_e1')(squeeze_x)

expand_x_3 = Conv2D(kernel_size=(3,3),filters=e_3x3,padding='same',

activation='relu',name=fire_name+'_e3')(squeeze_x)

 expand =

merge([expand_x_1, expand_x_3], mode='concat', concat_axis=3)

return expand

图 2.2

展

示了使用 Keras 自

带的 plot_model

功能可

视化的点火

模块，其中 。

图

2.2 Keras

可视化的点

火模块

2.1.3 SqueezeNet 的网

络结构

图 2.3 展

示了 SqueezeNet

的网络

结构，图 2.3（a）所示

的是不加捷

径的 SqueezeNet 的网络

结构，

图 2.3（b）所示

的是加了捷

径的 SqueezeNet 的网络

结构，图

2.3（c）所示

的是捷径跨

不同特征图

个数的卷积

的 SqueezeNet 的网络结

构。还有一些

细节在图 2.3

中

并没有体现

出来：

z 激活函

数默认都使

用 ReLU

；

z fire9 之后接了

一个丢失率

为

0.5 的 Dropout 层；

z 使用

same 卷积。

图

2.4 给出

了 SqueezeNet 的详细网

络参数。

2.1　SqueezeNet

图 2.3

SqueezeNet 的

网络结构

图

2.4 SqueezeNet

的详细网络

参数

71

第 2

章  轻

量级 CNN

72

根据图

2.4，我们的 SqueezeNet 的

Keras 实

现如下面代

码片段所示

。该代码片段

的完整内容

、

模型的参数

汇总，以及 SqueezeNet

的

Keras 可视化见随

书资料。

def squeezeNet(x):

conv1 = Conv2D(input_shape =

(224,224,3), strides = 2,

filters=96,

 kernel_size=(7,7), padding='same',

activation='relu')(x)

 poo1 =

MaxPool2D((2,2))(conv1)

 fire2 =

fire_model(poo1, 16, 64, 64,'fire2')

fire3 = fire_model(fire2, 16,

64, 64,'fire3')

 fire4

= fire_model(fire3, 32, 128,

128,'fire4')

 pool2 =

MaxPool2D((2,2))(fire4)

 fire5 =

fire_model(pool2, 32, 128, 128,'fire5')

fire6 = fire_model(fire5, 48,

192, 192,'fire6')

 fire7

= fire_model(fire6, 48, 192,

192,'fire7')

 fire8 =

fire_model(fire7, 64, 256, 256,'fire8')

pool3 = MaxPool2D((2,2))(fire8)

fire9 = fire_model(pool3, 64,

256, 256,'fire9')

 dropout1

= Dropout(0.5)(fire9)

 conv10

= Conv2D(kernel_size=(1,1), filters=1000, padding='same',

activation='relu')(dropout1)

 gap =

GlobalAveragePooling2D()(conv10)

 return gap

2.1.4 SqueezeNet 的性

能

图

2.3（a）的 SqueezeNet 的正

确率（top-1：57.5%。top-5：80.3%）是高于

AlexNet 的（top-1：

57.2%。top-5：80.3%）。从图 2.4 中我

们可以看出

，SqueezeNet 总共有

1 248 424 个参

数，同性能的

AlexNet

则有 58 304 586

个参数

（主要集中在

全连接层，去

掉之后有 3 729 472

个

）。使用他们提

出

的深度压

缩 1 算法压缩

后，模型的参

数数量可以

降到

421 098 个。

2.1.5

小结

SqueezeNet 的压缩策略

是通过将 3 ×

3 卷

积替换成 1 ×

1 卷

积来达到的

，其参数数量

约是等性能

的 AlexNet 的

2.14%。从参数

数量上来看

，SqueezeNet 的目的达到

了。SqueezeNet 的最大贡

献在于

开拓

了模型压缩

这一方向，之

后的一系列

论文也就此

方向展开。

这

里我们着重

说一下 SqueezeNet 的缺

点。

z

SqueezeNet 侧重的应

用方向是嵌

入式环境，目

前嵌入式环

境的主要问

题是实时性

。

SqueezeNet 通过压缩模

块和扩张模

块的结构，虽

然能减少网

络的参数，但

是丧失了网

络的

并行能

力，测试时间

反而会更长

，这与目前的

主要挑战是

背道而驰的

。

z 论文的题目

非常吸引人

眼球，虽然论

文中将参数

数量减少到

约 1/50，但是问题

的主要症结

在于

AlexNet 本身全

连接节点过

于庞大，参数

的减少和 SqueezeNet 的

设计并没有

关系，考

虑去

掉全连接之

后将参数减

小到约 1/3 更为

合适。

z

SqueezeNet 得到的

模型大小约

5MB，而论文中提

到的 0.5MB 的模型

还要得益于

深度压缩。

虽

然深度压缩

也是 SqueezeNet 论文团

队的论文内

容，但是将 0.5

这

个数列在论

文的题目中

显然不是很

合适。

1　参见 Song

Han、Huizi Mao、William J. Dally

的

论文“Deep Compression: Compressing Deep

Neural Networks with Pruning,

Trained Quantization and Huffman

Coding”。

2.2　MobileNet v1

和 MobileNet v2

73

2.2　MobileNet v1 和

MobileNet v2

在

本节中，先验

知识包括：



ResNet（1.4 节

）；  SqueezeNet（2.1

节）。

MobileNet v11 中使用的

深度可分离

卷积（depthwise

separable convolution）是模型

压缩的一个

最为经典的

策略，它通过

将跨通道的

3 × 3

卷积换成单

通道的 3 × 3

卷积

（深度卷积）加

上跨通道的

1 × 1 卷积（点卷积

）来达到模型

压缩的目的

。

MobileNet v22 在 MobileNet

v1 的深度可

分离卷积的

基础上引入

了残差结构

，并发现了 ReLU 在

通道数较少

的特征图上

有非常严重

的信息损失

问题，由此引

入了线性瓶

颈模块和翻

转卷积模块

。

首先在本节

中我们会详

细介绍两个

版本的 MobileNet，然后

我们会介绍

如何使用 Keras 实

现这

两个算

法。

2.2.1 MobileNet v1

1．回顾：普通

卷积的参数

数量和计算

量

传统的 CNN 是

跨通道的，对

于一个通道

数为

M 的输入

特征图，我们

要得到通道

数为 N 的输出

特征图。普通

卷积会使用

N

个不同的 DK × DK

× M 尺

寸的窗口以

滑窗的形式

遍历输入特

征图，因此

对

于一个尺寸

为

DK × DK 的普通卷

积，它的参数

个数为

DK × DK ×

M × N。一个

普通卷积的

计算可以

表

示为式（2.1），其中

，K

是卷积核，F 是

特征图，G 是卷

积之后的特

征图，k、l 是特征

图的宽和

高

，m、n 分别表示输

入特征图和

输出特征图

的通道数。

（2.1）

它

的一层网络

的计算代价

约为：

DK × DK ×

M × N ×

DW × DH

其中，(DW，DH)

为

特征图的尺

寸。普通卷积

的特征图之

间的卷积核

情况如图 2.5 所

示。

图

2.5　普通卷

积的特征图

之间的卷积

核情况

MobileNet v1

中介

绍的深度可

分离卷积就

用于解决普

通卷积的参

数数量过多

和计算代价

过于高昂

的

问题。深度可

分离卷积由

深度卷积（depthwise convolution）和

点卷积（pointwise convolution）组成

。

2．深度卷积

深

度卷积是指

不跨通道的

卷积，也就是

说特征图的

每个通道有

一个独立的

卷积核，并且

这个卷

积核

作用且仅作

用在这个通

道之上，如图

2.6 所示。

1　参见 Andrew G.Howard、Menglong

Zhu、Bo Chen 等

人的论文“Mobilenets: Efficient

Convolutional Neural Networks for

Mobile 

Vision Applications”。

2　参

见 Mark Sandler、Andrew

Howard、Menglong Zhu 等人的论

文“MobileNetV2: Inverted

Residuals and Linear Bottlenecks”。

第 2 章

轻量

级 CNN

74

图

2.6　深度卷

积示意（3 个通

道）

从图

2.6 和图

2.5 的对比中我

们可以看出

，因为放弃了

卷积时的跨

通道，深度卷

积的参数数

量

为

DK × DK ×

M。深度卷

积的数学表

达式如式（2.2）所

示。

（2.2）

它的计算

代价是普通

卷积的 ，表示

为：

DK × DK ×

M × DW ×

DH

在 Keras 中，我们

可以使用

DepthwiseConv2D() 实

现深度卷积

操作，它有几

个重要的参

数。

z kernel_size

：卷积核的

尺寸，一般设

为 3×3。

z strides

：卷积的步

长。

z padding ：是否加边

。

z activation ：激活函数。

由

于深度卷积

的每个通道

的特征图产

生且仅产生

一个与之对

应的特征图

，也就是说输

出层的特

征

图的通道数

量等于输入

层的特征图

的通道数量

。因此 DepthwiseConv2D() 不需要

控制输出层

的特征图的

通道数量，也

就没有通道

数（filters）这个参数

。

3．点卷积

深度

卷积的操作

虽然非常高

效，但是它仅

相当于对当

前的特征图

的一个通道

施加了一个

滤波

器，并不

会合并若干

个特征，从而

实现跨通道

的特征计算

，而且由于在

深度卷积中

输出特征图

的通

道数等

于输入特征

图的通道数

，因此它并没

有升维或者

降维的功能

。

为了解决这

些问题，MobileNet

v1 中引

入了点卷积

，用于特征合

并以及升维

或者降维。很

自然

地，我们

可以想到使

用 1

× 1 卷积来完

成这个功能

。点卷积的参

数数量为 M

× N，计

算量为：

M ×

N × DW ×

DH

点卷

积示意如图

2.7 所示。

图

2.7　点卷

积示意

4．深度

可分离卷积

合并深度卷

积和点卷积

便可得到 MobileNet

v1 中

的深度可分

离卷积。它的

一组操作（一

次深度

2.2　MobileNet

v1 和 MobileNet v2

75

卷

积加一次点

卷积）涉及的

参数数量为

：DK × DK

× M + M

× N，和普通卷积

的比值如式

（2.3）所示。

（2.3）

它的计

算量为

DK × DK ×

M × DW ×

DH + M ×

N × DW ×

DH，和普

通卷积的比

值如式（2.4）所示

。

（2.4）

所以，对一个

3 ×

3 的卷积而言

，MobileNet v1 的参数数量

和计算代价

均为普通卷

积的

左右。

5．MobileNet v1 的

Keras

实现及实验

结果分析

通

过上面的分

析，我们知道

一个深度可

分离卷积的

一组卷积操

作可以拆分

成一个深度

卷积和一

个

点卷积，由此

形成 MobileNet

v1 的结构

。在这个实验

中我们首先

会搭建一个

普通卷积，然

后将其

改造

成 MobileNet

v1，在 MNIST 数据集

上运行并得

出实验结果

，运行环境分

别为 CPU

和 GPU。

首先

我们搭建的

普通卷积的

结构如下面

代码片段所

示：

def

Simple_NaiveConvNet(input_shape, k):

 inputs

= Input(shape=input_shape)

 x

= Conv2D(filters=32, kernel_size=(3,3), strides=(2,2),

padding='same',

 activation='relu', name='n_conv_1')(inputs)

x = Conv2D(filters=64, kernel_size=(3,3),padding='same',

activation='relu',

 name='n_conv_2')(x)

x = Conv2D(filters=128, kernel_size=(3,3),padding='same',

activation='relu',

 name='n_conv_3')(x)

x = Conv2D(filters=128, kernel_size=(3,3),

strides=(2,2),padding='same',

 activation='relu', name='n_conv_4')(x)

x = GlobalAveragePooling2D(name='n_gap')(x)

x = BatchNormalization(name='n_bn_1')(x)

x = Dense(128, activation='relu',

name='n_fc1')(x)

 x =

BatchNormalization(name='n_bc_2')(x)

 x =

Dense(k, activation='softmax', name='n_output')(x)

model = Model(inputs, x)

return model

通过将 3

× 3 的

Conv2D() 换成

3 × 3 的

DepthwiseConv2D() 加上

1 × 1

的 Conv2D()（第

一层保

留普通卷积

），我们将普通

卷积改造成

了 MobileNet

v1。

def Simple_MobileNetV1(input_shape, k):

inputs = Input(shape=input_shape)

x = Conv2D(filters=32, kernel_size=(3,

3), strides=(2, 2), padding='same',

activation='relu', name='m_conv_1')(inputs)

 x

= DepthwiseConv2D(kernel_size=(3, 3), padding='same',

activation='relu',

 name='m_dc_2')(x)

x = Conv2D(filters=64, kernel_size=(1,

1), padding='same', activation='relu',

name='m_pc_2')(x)

 x =

DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu',

name='m_dc_3')(x)

 x =

Conv2D(filters=128, kernel_size=(1, 1), padding='same',

activation='relu',

 name='m_pc_3')(x)

x = DepthwiseConv2D(kernel_size=(3, 3),

strides=(2, 2), padding='same',

activation='relu', name='m_dc_4')(x)

 x

= Conv2D(filters=128, kernel_size=(1, 1),

padding='same', activation='relu',

 name='m_pc_4')(x)

x = GlobalAveragePooling2D(name='m_gap')(x)

第

2 章  轻量

级

CNN

76

 x

= BatchNormalization(name='m_bn_1')(x)

 x

= Dense(128, activation='relu', name='m_fc1')(x)

x = BatchNormalization(name='m_bc_2')(x)

x = Dense(k, activation='softmax',

name='m_output')(x)

 model =

Model(inputs, x)

 return

model

通过 Summary() 函数

我们可以得

到每个网络

中每层的参

数数量，如图

2.8

所示。图 2.8（a）

所示

的是普通卷

积的参数数

量汇总，图 2.8（b）所

示的是

MobileNet v1 的参

数数量汇总

。

图

2.8　普通卷积

和 MobileNet v1

的参数数

量汇总

普通

卷积的参数

总量为 259 082，去除

未改造的部

分剩余的参

数数量为

239 936，即

图 2.8（a）

中所有卷

积部分的参

数数量。MobileNet

v1 的参

数总量为 48 330，去

除未改造的

部分剩余的

参数数

量为

29 184 个，即图 2.8（b）中所

有卷积和深

度卷积的参

数。两个的比

值为

，符合

我

们之前的推

算。

接着我们

利用 MNIST

数据集

进行实验，我

们在 CPU（Intel Core i7）和

GPU（NVIDIA GeForce GTX

1080 Ti）两个

环境下运行

，得到的收敛

曲线如图 2.9 所

示。在都训练

10

个 epoch 的情况下

，我们

发现

MobileNet v1 的

结果要略差

于普通卷积

，这点完全可

以理解，毕竟

MobileNet v1

的参数更少

。

图 2.9　普通卷积

（蓝）和

MobileNet v1（橙）在 MNIST 上

的收敛曲线

2.2

MobileNet v1 和 MobileNet

v2

当年进行

实验时（2018 年），我

在对比单个

epoch 的训练时间

的时候发现

了一个奇怪

的现

象：在 CPU 上

，MobileNet v1

的训练时间

约为 70s，普通卷

积的训练时

间约为 140s，这和

我们的

预测

是类似的；但

是在

GPU 环境下

，普通卷积和

MobileNet v1 的训练时间

分别约为

40s 和

50s。

MobileNet v1

在 GPU 上的训练

速度反而更

慢了，这是什

么原因呢？

问

题在于

cuDNN 对普

通卷积的并

行支持比较

完善，而在 cuDNN 7

之

前的版本并

不支持深度

卷积，现在虽

然支持了，其

并行性并没

有优化，依旧

采用循环的

形式遍历每

个通道，因此

在 GPU

环境下 MobileNet

v1 训

练速度反而

要慢于普通

卷积。所以说

，是底层框架

训练速度慢

，并不是

MobileNet v1

算法

训练速度慢

。

最后，论文中

给出了两个

超参数 α 和

ρ，分

别用于控制

每层的特征

图的数量和

输入图像的

尺寸，

由于并

没有涉及很

多特有知识

，这里不过多

介绍。

2.2.2 MobileNet

v2

在 MobileNet v2

中，作

者将残差网

络加入了 MobileNet v1 中

，同时分析了

MobileNet

v1 的几

个缺点

并针对性地

做了改进。MobileNet v2

的

改进策略非

常简单，但是

在论文中，分

析缺点的部

分

涉及了流

形学习等内

容，使优化过

程变得非常

复杂。我们在

这里简单总

结一下 MobileNet v2

中给

出

的缺点分

析，希望能对

阅读论文的

读者有所帮

助，对 MobileNet v2

的原理

感兴趣的读

者可以阅读

论

文原文。

当

我们单独去

看特征图的

每个通道的

像素的值的

时候，其实这

些值代表的

特征可以映

射到一个

低

维子空间的

一个流形区

域上。在完成

卷积操作之

后往往会接

一个激活函

数来增强特

征的非线性

，

一个最常见

的激活函数

便是 ReLU。根据我

们在 1.4 节中介

绍的数据处

理不等式，ReLU

一

定会带来

信

息损耗，而且

这种损耗是

没有办法恢

复的。ReLU 带来的

信息损耗在

通道数非常

少的时候更

为明

显。为什

么这么说呢

？我们看图

2.10 所

示的这个例

子，其输入是

一个表示流

形数据的矩

阵，和卷

积操

作类似，其会

经过 n

个 ReLU 操作

得到 n

个通道

的特征图，然

后我们试图

通过这 n 个特

征图还

原输

入数据，还原

程度越高说

明信息损耗

得越少。从图

2.10

中我们可以

看出，当 n 的值

比较小时，

ReLU

的

信息损耗非

常严重，当 n 的

值比较大时

，输入数据的

还原程度越

高。

图

2.10　使用 ReLU 激

活函数的通

道数和信息

损耗之间的

关系

根据对

上面提到的

信息损耗问

题的分析，我

们有两种解

决方案：

z 既然

是 ReLU

导致的信

息损耗，那么

我们就将 ReLU 替

换成线性激

活函数；

z

如果

比较多的通

道数能减少

信息损耗，那

么我们使用

更多的通道

。

1．线性瓶颈层

我们当然不

能把 ReLU 全部换

成线性激活

函数，不然网

络将会退化

为单层神经

网络，一个折

中

方案是在

输出特征图

的通道数较

少的时候（也

就是瓶颈层

部分）使用线

性激活函数

，其他时候使

用

ReLU。代码片段

如下：

def _bottleneck(inputs,

nb_filters, t):

 x

= Conv2D(filters=nb_filters * t,

kernel_size=(1,1), padding='same')(inputs)

77

第

2 章  轻

量级

CNN

78

 x

= Activation(relu6)(x)

 x

= DepthwiseConv2D(kernel_size=(3,3), padding='same')(x)

x = Activation(relu6)(x)

x = Conv2D(filters=nb_filters, kernel_size=(1,1),

padding='same')(x)

 # 不使用

激活函数

if not K.get_variable_shape(inputs)[3] ==

nb_filters:

 inputs =

Conv2D(filters=nb_filters, kernel_size=(1,1), padding='same')(inputs)

outputs = add([x, inputs])

return outputs

这

里使用了 MobileNet

v1 中

的 ReLU6 激活函数

，它将

ReLU 激活函

数的最大值

控制到 6，数

学

形式如式（2.5）所

示：

ReLU6 = min[max(0,x),6] （2.5）

图 2.11（a）展示的

是结合了残

差网络和线

性激活函

数

的 MobileNet

v2 的一个网

络块，图 2.11（b）展示

的是

MobileNet

v1。

2．反转残

差

当激活函

数使用 ReLU

时，我

们可以通过

增加通道数

来减少信息

的损耗，使用

参数 t 来控制

，表示该层的

通道

数是输

入特征图的

通道数的

t 倍

。对于传统的

残差块，t

一般

取小于 1

的小

数，常见的取

值为 0.1，而在 MobileNet

v2 中

这个值一般

在 5 到

10 内，在作

者的实验中

，t = 6。

考虑到残差

网络和 MobileNet v2 的

t 的

不同取值范

围，它

们分别

形成了锥子

形（两头小、中

间大）和沙漏

形（两头

大、中

间小）的结构

，如图

2.12 所示，其

中图 2.12（b）

第一组

特征图之间

使用的是线

性激活函数

。

因为捷径被

转移到了瓶

颈层，所以这

种形式的卷

积块

被叫作

反转残差块

（inverted residual block）。

图 2.12　残差网络

的残差块和

MobileNet v2

的反转残差

块卷积对比

3．MobileNet v2

综上，我们可

以得到 MobileNet

v2 的一

个网络块的

详细参数，如

表 2.1 所示，其中

s

代表步长。

表

2.1　MobileNetv2 的一个网络

块的详细参

数

输入 模型

运算 输出

h

× w × k

1 × 1 Conv

2D，ReLU6 h × w

× (t × k)

h × w ×

t × k 3

× 3 DWConv，s，ReLU6

Linear，1

× 1 Conv 2D

图

2.11 MobileNet v2 的线性瓶颈

层和

MobileNet v1 的深度

可分离卷积

的对比

2.2

MobileNet v1 和 MobileNet

v2

79

MobileNet v2

可

以通过堆叠

瓶颈块的方

式实现，如下

面的代码片

段所示：

def MobileNetV2_relu(input_shape, k):

inputs = Input(shape =

input_shape)

 x =

Conv2D(filters=32, kernel_size=(3,3), padding='same')(inputs)

x = _bottleneck_relu(x, 8,

6)

 x =

MaxPooling2D((2,2))(x)

 x =

_bottleneck_relu(x, 16, 6)

x = _bottleneck_relu(x, 16,

6)

 x =

MaxPooling2D((2,2))(x)

 x =

_bottleneck_relu(x, 32, 6)

x = GlobalAveragePooling2D()(x)

x = Dense(128, activation='relu')(x)

outputs = Dense(k, activation='softmax')(x)

model = Model(inputs, outputs)

return model

2.2.3 小结

在本节中，我

们介绍了两

个版本的

MobileNet，它

们和普通卷

积的对比如

图 2.13 所示。

图

2.13 MobileNet v1、MobileNet v2

和

普通卷积的

对比

如图 2.13（b）所

示，MobileNet v1

最主要的

贡献是提出

了深度可分

离卷积，它又

可以拆分成

深度卷积和

点卷积。MobileNet v2 主要

将残差网络

和深度可分

离卷积进行

了结合，通过

分析单通道

的流形特征

对残差块进

行了改进，包

括对中间层

的扩展（如图

2.13（d）所示）以及瓶

颈层的线性

激活（如图 2.13（c）所

示）。深度可分

离卷积的分

离式设计直

接将模型压

缩到

左右，但

是精度并

没

有损失得非

常严重，这一

点还是非常

震撼的。

深度

可分离卷积

的设计非常

精彩，MobileNet 系列高

速度的特性

给了

MobileNet 很大的

市场空

间，尤

其是在嵌入

式平台领域

中。

第

2 章  轻量

级

CNN

80

最后，不得

不承认 MobileNet

v2 的论

文的一系列

证明非常精

彩，虽然没有

这些证明我

们也能

明白

MobileNet v2

的工作原理

，但是这些证

明过程还是

非常值得仔

细品鉴的，尤

其是对于科

研

人员。

2.3　Xception

在本

节中，先验知

识包括：

 GoogLeNet（1.3 节）；

 ResNet（1.4 节

）。

深度可分离

卷积是

Laurent Sifre 在其

博士论文 1

中

率先提出的

。经典的 MobileNet 系列

算法便采

用

深度可分离

卷积作为其

核心结构。

本

节主要从 Inception 的

角度出发，探

讨 Inception

和深度可

分离卷积的

关系，从一个

全新的角

度

解释深度可

分离卷积。再

结合前沿的

残差网络，一

个新的架构

Xception2 应运而生。Xception 取

义

“极端的 Inception”（extreme Inception），即

Xception 是一种极端

的

Inception，下面我们

来看看它是

如何体现极

端的。

2.3.1 Inception 回顾

Inception 的

核心思想是

将通道分成

若干个感受

野大小不同

的通道。除了

能获得不同

的感受野，

Inception 还

能大幅降低

参数数量。图

2.14

所示的是一

个简单的 Inception 模

型。

图

2.14　一个简

单的 Inception 模型

对

于一个输入

特征图，首先

通过 3 组 1

× 1 卷积

得到 3

组特征

图，它和先使

用一组 1 × 1

卷积

得

到一组特

征图，再将这

组特征图分

成 3 组是完全

等价的（见图

2.15）。假设图

2.14 中 1 ×

1 卷

积核

的数量

都是 k1，3

× 3 卷积核

的数量都是

k2，输入特征图

的通道数为

m，那么这个简

单的 Inception

模型的

参数数量为

：

对比通道数

相同，但是没

有分组的普

通卷积，其参

数数量为 m ×

k1 + 3 ×

3 × k1 ×

k2，约

为 Inception

的 3

倍。

1　参见

Laurent Sifre、Stéphane

Mallat 的论文“Rigid-Motion Scattering for

Image Classification”。

2　参见

Francois

Chollet 的论文“Xception: Deep Learning

with Depthwise Separable Convolutions”。

2.3　Xception

图 2.15

简

单 Inception 模型的等

价形式

2.3.2

Xception 详解

如果 Inception 将

3 × 3 卷积

分成

3 组，那么

考虑一种极

端的情况：将

Inception 的 1

× 1 卷积得

到

的

k1 个通道的

特征图完全

分开，也就是

使用 k1 个不同

的卷积分别

在每个通道

上进行卷积

，涉及

的参数

数量是 m × k1

+ k1 × 3

× 3。为了

对齐普通卷

积的输出通

道数，我们希

望两组卷积

的输出特征

图

相同，这里

我们将 Inception

的 1 × 1

卷

积的通道数

设为 k2，即参数

数量为 m ×

k2 + k2 ×

3 × 3，它的

参

数数量是

普通卷积的

，我们把这种

形式的

Inception 叫作

极端的 Inception，如图

2.16 所示。

图 2.16　极端

的 Inception

在搭建 GoogLeNet 时

，我们一般采

用堆叠 Inception

的方

式，同理在搭

建由极端的

Inception 构

成的网络

时也采用堆

叠的方式，论

文中将这种

形式的网络

结构叫作 Xception。如

果我们看过

深度可

分离

卷积的话就

会发现，它和

Xception 几乎是等价

的，第一个不

同点就是先

计算点卷积

还是先计

算

深度卷积。在

MobileNet v2

中，我们指出

瓶颈层的最

后一层 1 × 1

卷积

核为线性激

活时有助于

减

少信息损

耗，这也就是

Xception 和深度可分

离卷积（准确

地说是 MobileNet

v2）的第

二个不同点

。

结合残差结

构，一个完整

的 Xception 模型如图

2.17

所示。Xception 由入口

流（Entry flow）、中间

流（Middle

flow）和

出口流（Exit flow）组成

，其中入口流

由普通卷积

和深度可分

离卷积组成

，而

中间流和

出口流仅由

深度可分离

卷积组成。

81

第

2 章  轻量级

CNN

82

图

2.17　一个完整的

Xception

模型

图 2.17 中要

注意的几点

：

z Keras 的 SeparableConv()

函数是由

3 × 3 的深度卷积

和

1 × 1 的点卷积

组成的，因此

可

用于升维

和降维；

z 图 2.17

中

的⊕是单位加

操作，即对两

个特征图进

行单位加。

2.3.3 小

结

Xception

的结构和

MobileNet 的非常像，两

个算法的提

出时间近似

，不存在谁抄

袭谁的问题

。

它们从不同

的角度揭示

了深度可分

离卷积的强

大作用：MobileNet 是通

过将普通

3 × 3 卷

积拆分来减

少参数数量

的，而

Xception 则是通

过对 Inception 的充分

解耦来减少

参数数量的

。

2.4　ResNeXt

在本节中，先

验知识包括

：



GoogLeNet（1.3 节）；  残差网络

（1.4

节）；

 MobileNet（2.2 节）。

本节介

绍 ResNeXt1

。ResNeXt 是残差网

络和

Inception 的结合

体，不同于 Inception v4

的

是，残

1　参见 Saining

Xie、Ross Girshick、Piotr Dollár 等

人的论文“Aggregated

Residual Transformations for Deep

Neural Networks”。

2.4　ResNeXt

83

差

网络不需要

人工设计复

杂的 Inception 结构细

节，每一个分

支都采用相

同的拓扑结

构。ResNeXt

的本质是

分组卷积（group convolution），通

过变量基数

（cardinality）来控制组的

数量。分组卷

积

是普通卷

积和深度可

分离卷积的

一个折中方

案，即将特征

图分成若干

组，在每个组

的内部使用

普通

卷积进

行计算。

2.4.1 从全

连接讲起

给

定有 D

个通道

的输入数据

x = [x1,x2,…,xD]，其输入权值

的通道数也

是 D，表示为

w = [w1,w2,…,

wD]，一

个没有偏置

的线性激活

神经元如式

（2.4）所示：

（2.6）

全连接

的拆分 -

转换

- 合并结构如

图 2.18 所示。

这是

一个最简单

的拆分 - 转换

- 合并（split-transform￾merge）结构。具

体地讲，图

2.18 可

以拆分成 3 部

分。

（1）拆分：将数

据 x 拆分成 D

个

特征。

（2）转换：每

个特征经过

一个线性变

换 wi

xi

。

（3）合并：通过

单位加得到

最后的输出

。

2.4.2 简化

Inception

Inception 的结构

也是非常明

显的拆分 -

转

换 - 合并结构

，作者认为 Inception

不

同分支的不

同拓

扑结构

的特征有非

常刻意的“人

工雕琢”的痕

迹，而调整 Inception 的

内部结构往

往涉及大量

的超

参数，调

整这些超参

数是非常困

难的。所以作

者的想法是

每个结构使

用相同的拓

扑结构，那么

这时

候的 Inception 表

示为式（2.5）：

（2.7）

其中

，C 是简化 Inception

的基

数（之后多被

叫作分组卷

积的组数），Ti() 是

任意的变换

，例如一系

列

的卷积操作

等。图 2.19

所示的

便是一个简

化 Inception 的拆分 -

转

换 - 合并结构

，其 Ti()

是由连

续

的卷积（1 × 1

→ 3 × 3

→ 1 × 1）组成

的。

图 2.19　一个简

化 Inception

的拆分 - 转

换 -

合并结构

图 2.18　全连接的

拆分 -

转换 - 合

并结构

第

2 章

轻量级 CNN

84

2.4.3 ResNeXt 详解

结合强大的

残差网络，我

们得到的便

是完整的

ResNeXt，也

就是在式（2.5）中

添加一条捷

径，

表示为式

（2.6）。

（2.8）

ResNeXt

的结构如图

2.20 所示。

图 2.20

ResNeXt 的结

构

在图 2.20

中，单

位加操作之

前是 32 个独立

的通道数为

256 的

1 × 1 卷积，它等

价于将

32 个 3 ×

3

卷

积之后的通

道数为 4 的特

征图拼接起

来之后再使

用一个通道

数为

256 的 1 ×

1 卷积

，如图 2.21 所示。

图

2.20 和图 2.21 所示的

结构可以等

价为提前把

1

× 1 卷积合并的

结构，如图 2.22

所

示。

图 2.21 Inception

v4 拼接在

前的结构 图

2.22　分组卷积的

第

3 种形式

2.4.4 分

组卷积

分组

卷积的雏形

要追溯到 2012 年

深度学习“鼻

祖”AlexNet，其网络结

构如图 1.4

所示

。受限

于当时

的硬件条件

，AlexNet 论文作者不

得不将卷积

操作拆分到

两个 GPU

上运行

，这两个 GPU 的

参

数是不共享

的。

分组卷积

是介于普通

卷积和深度

可分离卷积

之间的一种

折中方案，不

是彻底地为

每个通道单

独

赋予一个

独立的卷积

核，也不是整

个特征图使

用同一个卷

积核。

2.5　ShuffleNet

v1 和 ShuffleNet v2

85

2.4.5 小结

ResNeXt 提出了一种

介于普通卷

积和深度可

分离卷积之

间的策略：分

组卷积。它通

过控制分组

的数量（基数

）来达到两种

卷积的平衡

。分组卷积的

思想源自

Inception，不

同于 Inception 需要人

工设计每个

分支，ResNeXt 的每个

分支的拓扑

结构是相同

的。最后结合

残差网络，得

到的便是最

终

的 ResNeXt。

从上面

的分析中我

们可以看出

ResNeXt 的结构非常

简单，但是其

在

ImageNet 上取得了

优于类

似框

架的残差网

络，这也有 Inception

的

一部分功劳

。

ResNeXt 的超参数确

实比 Inception

v4 的超参

数更少，但是

它直接废除

了 Inception 的囊括不

同

感受野的

特性仿佛不

是很合理。在

更多的环境

中我们发现

Inception v4 的效果是优

于 ResNeXt

的。

类似结

构的 ResNeXt 的运行

速度应该是

优于

Inception v4 的，因为

ResNeXt 的相同拓扑

结构的分支

设计更符合

GPU

的硬件设计

原则。

2.5　ShuffleNet v1

和 ShuffleNet v2

在本

节中，先验知

识包括：

 ResNeXt（2.4 节）； 

Xception（2.3 节

）；

 DenseNet（1.6

节）；  残差网络

（1.4 节）；

 MobileNet（2.2 节）。

在

ResNeXt 中，分

组卷积作为

普通卷积和

深度可分离

卷积之间的

一种折中方

案被采用。这

时

大量的对

整个特征图

的点卷积成

了 ResNeXt

的性能瓶

颈。一种更高

效的策略是

在组内进行

点卷积，

但是

这种组内点

卷积的方式

不利于通道

之间的信息

流通。为了解

决这个问题

，ShuffleNet v11 中提出

了通

道洗牌（channel shuffle）操作

。

在 ShuffleNet

v22 的论文中

作者指出，现

在普遍采用

的用每秒浮

点运算数（floating-point 

operations

per second，FLOPS）评

估模型性能

的方式是非

常不合理的

，因为计算一

批样本的训

练时

间除了

要考虑 FLOPS，还要

考虑很多过

程，如文件

I/O、内

存读取、GPU 执行

等。作者从内

存消

耗成本

、GPU 并行性两个

方向分析了

模型可能带

来的非

FLOPS 的行

动损耗，进而

设计了更加

高效

的 ShuffleNet

v2。ShuffleNet v2 的架

构和 DenseNet

的有异

曲同工之妙

，而且其速度

和精度都要

优于

DenseNet。

2.5.1 ShuffleNet

v1

1．通道洗

牌

通道洗牌

是介于整个

通道的点卷

积和组内点

卷积之间的

一种折中方

案。传统策略

是在整个特

征

图上执行

卷积操作。假

设一个传统

的深度可分

离卷积由一

个

3 × 3 的深度卷

积和一个

1 × 1 的

点卷积

1　参见

Xiangyu Zhang、Xinyu Zhou、Mengxiao

Lin 等人的论文

“ShuffleNet: An Extremely

Efficient Convolutional Neural Network

for Mobile Devices”。

2

参见 Ningning Ma、Xiangyu Zhang、Hai-Tao

Zheng 等人的

论文“ShuffleNet v2: Practical

Guidelines for Efficient CNN

Architecture 

Design”。

第

2 章  轻

量级

CNN

86

组成。其

中输入特征

图的尺寸为

h ×

w × c1，输出特征图

的尺寸为 h

× w × c2，1

× 1 卷

积处的 FLOPS

如式

（2.9）所示。

（2.9）

一般情

况下 c2

是远大

于 9 的，也就是

说深度可分

离卷积的性

能瓶颈主要

体现在点卷

积上。

为了解

决这个问题

，ResNeXt

提出了仅在

分组内进行

点卷积。对于

一个分成了

g 个组的分组

卷

积，其 FLOPS

如式

（2.10）所示。

（2.10）

从式（2.10）中

我们可以看

出组内点卷

积可以非常

有效地解决

性能瓶颈问

题。然而这个

策略的

一个

非常严重的

问题是特征

图之间是不

存在通道特

征的信息交

互的，网络趋

近于由多个

结构类似的

网络构成的

模型，精度大

打折扣，如图

2.23（a）所示。

图 2.23　分组

点卷积（a）和通

道洗牌（b，c）对比

为了解决通

道之间的沟

通问题，ShuffleNet v1

提出

了其最核心

的操作：通道

洗牌。假设分

组特

征图的

尺寸为 w ×

h × g ×

n，其中

g 表示分组的

组数。通道洗

牌的操作细

节如下：

（1）将特

征图展开成

g ×

n × w ×

h 的四维矩阵

；

（2）沿着尺寸为

g ×

n × w ×

h 的矩阵的 g 轴

和

n 轴转置；

（3）将

g 轴和

n 轴平铺

后得到洗牌

之后的特征

图；

（4）进行组内

1 ×

1 卷积。

通道洗

牌的结果如

图 2.23（c）所示，具体

操作细节如

图

2.24 所示，Keras 实现

如下：

def

channel_shuffle(x, groups):

 """

举例：由

3个组构成的

1维向量

 >>> d

= np.array([0,1,2,3,4,5,6,7,8])

 >>>

x = np.reshape(d, (3,3))

>>> x = np.transpose(x,

[1,0])

 >>> x

= np.reshape(x, (9,))

'[0 1 2 3

4 5 6 7

8] --> [0 3

6 1 4 7

2 5 8]'

"""

 height, width,

in_channels = x.shape.as_list()[1:]

channels_per_group = in_channels //

groups

 x =

K.reshape(x, [-1, height, width,

groups, channels_per_group])

2.5　ShuffleNet

v1 和 ShuffleNet v2

87

 x =

K.permute_dimensions(x, (0, 1, 2,

4, 3)) # 移

位

x = K.reshape(x, [-1,

height, width, in_channels])

return x

图 2.24

通道洗

牌过程详解

从代码中我

们也可以看

出，通道洗牌

的操作是步

步可微分的

，因此可以保

证整个 CNN 的可

学

习性。

2．ShuffleNet v1 单元

图 2.25（a）所示的是

一个普通的

带有残差结

构的深度可

分离卷积，如

MobileNet、Xception。

ShuffleNet v1 的结构如图

2.25（b）、图 2.25（c）所示。其中

，图

2.25（b）所示的是

不需要降采

样

的情况，图

2.25（c）所示的是需

要降采样的

情况。

图 2.25

MobileNet、ShuffleNet v1 与降

采样情况

图

2.25（b）和图

2.25（c）已经展

示了 ShuffleNet v1 的全部

实现细节，我

们仔细分析

。

（1）上、下两个红

色部分的 1 × 1

卷

积替换为 1 × 1

的

分组卷积，分

组的组数 g 一

般不会很大

，

论文中选择

的几个值分

别是

1、2、3、4、8。当 g = 1

时，ShuffleNet v1 退

化为 Xception。g

的值需

确保能够被

通道数整除

，保证重塑操

作的有效执

行。

（2）在第一个

1 × 1

卷积之后添

加 2.5.1 节介绍的

通道洗牌操

作。

（3）在图

2.25（c）中，左

侧捷径部分

使用的是步

长为 2 的 3

× 3 平均

池化，右侧使

用的是步长

为 2

的 3 × 3

的深度

卷积。

第 2 章

轻

量级 CNN

（4）ShuffleNet v1

去掉了

3 × 3 卷积之后的

ReLU

激活函数，目

的是减少 ReLU 激

活函数造成

的

信息损耗

，具体原因见

2.2

节。

（5）如果进行

了降采样，为

了保证参数

数量不骤减

，通道数量往

往需要加倍

。所以在图2.25（c）

中

通过拼接操

作来加倍通

道数，而图 2.25（b）中

则通过单位

加来加倍通

道数。

最后基

于 ShuffleNet v1 单元，我们

计算一下残

差网络、ResNeXt

和 ShuffleNet v1 的

FLOPS，

即执行一个

单元需要的

计算量。通道

洗牌处的操

作数非常少

，这里可以忽

略不计。假设

输入特征图

的尺寸为 w × h

× c，瓶

颈处的通道

数为 m ：

我们可

以非常容易

地得到它们

的 FLOPS 的关系：

FResNet

> FResNeXt > FShuffleNet

v1

3．ShuffleNet v1 网

络

ShuffleNet v1 完整网络

的搭建可以

通过堆叠 ShuffleNet

v1 单

元的方式实

现，这里不赘

述。具体

细节

请查看已经

开源的 ShuffleNet

v1 的源

码。

2.5.2 ShuffleNet

v2

1．模型性能

的评估指标

在前文中我

们统一使用

FLOPS 作为评估一

个模型性能

的指标，但是

在 ShuffleNet

v2 的论文

中

作者指出这

个指标是间

接的，因为一

个模型实际

的运行时间

除了要把计

算操作的时

间算进去，

内

存读写、GPU

并行

性、文件 I/O 等的

时间也应该

考虑进去。在

整个模型的

计算周期中

，FLOPS

耗时仅占

50% 左

右，如果我们

能优化另外

50%，就能够在不

损失计算量

的前提下进

一步提高模

型的效率。所

以最直接的

方案是回归

最原始的策

略，即直接在

同一个硬件

上观察每个

模型的运行

时间。

在 ShuffleNet

v2 中，作

者从内存访

问代价（memory access cost，MAC）和

GPU 并

行性的方向

分析了网络

应该怎么设

计才能进一

步减少运行

时间，直接提

高模型的效

率。

2．高效模型

的设计准则

G1：当输入通道

数和输出通

道数相同时

，MAC 最小。

假设一

个卷积操作

的输入特征

图 p 的尺寸是

w ×

h × c1，输出特征图

的尺寸为 w

× h × c2。卷

积操

作的 FLOPS 为

F =

hwc1c2。在计算这个

卷积的过程

中，输入特征

图占用的内

存大小是 hwc1，输

出特

征图占

用的内存大

小是 hwc2，卷积核

占用的内存

大小是

c1c2，总计

：

88

2.5　ShuffleNet

v1 和 ShuffleNet v2

89

其中，B = hwc1c2。上面

的不等式拥

有一个下界

，且在

c1 = c2 时取等

号。也就是说

，当

FLOPS 确定

的时

候，若 c1

= c2，则模型

的运行效率

最高，因为此

时的 MAC 最小。

G2：MAC 与

分组数量 g 成

正比。

在分组

卷积中，FLOPS 为 ，其

MAC 的计算方式

为：

其中，B = hwc1c2/g。根据

G2 可知，我们在

设计网络时

g

的值不应过

大。

G3：较多的分

支数量降低

网络的并行

能力。

分支数

量比较多的

典型网络是

Inception、NASNet 等。作者通过

一组对照实

验来证明这

一准则，

如图

2.26 所示，通过控

制卷积的通

道数来使 5 组

对照实验的

FLOPS

相同。通过实

验，我们发现

它

们按效率

从高到低排

列依次是图

2.26（a）> 图 2.26（b）>

图 2.26（d）> 图 2.26（c）>

图 2.26（e）。

图

2.26　网络分支对

照实验样本

示意

造成这

种现象的原

因是更多的

分支需要更

多的卷积核

来进行加载

和同步操作

。

G4：点单位操作

是非常耗时

的。

我们在计

算 FLOPS

时往往只

考虑卷积中

的乘法操作

，一些点单位

操作（如 ReLU 激活

函数、

第

2 章  轻

量级

CNN

偏置、单

位加等）往往

被忽略掉。作

者指出这些

点单位操作

看似数量很

少，但它们对

模型的速度

影

响非常大

。尤其是深度

可分离卷积

这种 MAC/FLOPS

比值较

大的算法。

图

2.27 中统计了 ShuffleNet

v1 和

MobileNet v2 中各个操作

在

GPU 和 ARM 上的消

耗时间占比

。

图 2.27　模型训练

时间拆分示

意

总结一下

，在设计高性

能网络时，我

们要尽可能

做到：

z 对于 G1，使

用输入通道

数和输出通

道数相同的

卷积操作；

z

对

于 G2，谨慎使用

分组卷积；

z 对

于

G3，减少网络

分支数；

z 对于

G4，减少点单位

操作。

例如，在

ShuffleNet

v1 中使用分组

卷积是违背

G2 的，而每个 ShuffleNet

v1 单

元使用瓶颈

结构

是违背

G1 的，在

MobileNet v2 中使用

大量分支是

违背 G3

的，在深

度可分离卷

积处使用 ReLU6 激

活函数是违

背 G4

的。

虽然 ShuffleNet v2

和

很多算法的

FLOPS 接近，但是从

它的对照实

验中我们可

以看出，

ShuffleNet v2

要比

和它 FLOPS 接近的

模型的速度

要快，Xception、MobileNet v2、ShuffleNet

v1、

ShuffleNet v2 在

GPU 和 ARM 上

的

每 秒 百 万

次

浮 点 运 算

数（millon

float-pointing operations per

second，MFLOPS）和训练速

度的关系，如

图 2.28 所示。

图

2.28　对

照实验结果

3．ShuffleNet v2 的结构

图 2.29（a）和

图 2.29（b）展示了上

面介绍的 ShuffleNet

v1 及

其降采样，图

2.29（c）和图 2.29（d）

展示了

接下来要介

绍的

ShuffleNet v2 及其降

采样。

90

2.5　ShuffleNet v1 和

ShuffleNet v2

91

图

2.29 ShuffleNet v1 及

其降采样与

ShuffleNet

v2 及其降采样

仔细观察图

2.29（c）、图 2.29（d）所示的对

网络的改进

，我们发现了

以下几点。

z

在

图 2.29（c）中，ShuffleNet v2 使用了

一个通道拆

分（channel

split）操作。这个

操作非

常简

单，即将 c 个输

入特征分成

c−c′

和 c′ 两组，一般

情况下 。这种

设计是为了

尽量

控制分

支数，为了满

足 G3。

z 在拆分之

后的两个分

支中，左侧是

一个直接映

射，右侧是一

个输入通道

数和输出通

道数均

相同

的深度可分

离卷积，为了

满足 G1。

z 在右侧

的卷积中，1

× 1 卷

积并没有使

用分组卷积

，为了满足 G2。

z 在

合并的时候

均使用拼接

操作，为了满

足 G4。

z

在堆叠ShuffleNet v2的

时候，通道拼

接、通道洗牌

和通道拆分

可以合并成

1个点单位操

作，

也是为了

满足 G4。

最后当

需要降采样

时，我们通过

不进行通道

拆分的方式

实现通道数

量的加倍，如

图 2.29（d）所

示，非常

简单。

4．ShuffleNet

v2 和 DenseNet

ShuffleNet

v2 之所

以能够得到

非常高的精

度是因为它

和 DenseNet 有着非常

一致的结构

：强壮

的特征

重用（feature reuse）。在 DenseNet 中，作

者大量使用

拼接操作直

接将上一层

的特征图“原

汁原味”地传

到下一个乃

至下几个模

块。从图

2.29（c）中我

们也可以看

出，左侧的直

接映射和

DenseNet 的

特征重用是

非常相似的

。

不同于

DenseNet 的整

个特征图的

直接映射，ShuffleNet v2 只

映射了一半

。恰恰是这一

点不同，使

ShuffleNet v2 有

了和 DenseNet

的升级

版 CondenseNet 相同的思

想。在 CondenseNet

中，作者

通过可视化

DenseNet 的特征重用

和特征图的

距离关系发

现距离越近

的特征图之

间的特征重

用越重要。ShuffleNet 

第

2

章  轻量级 CNN

92

v2 中

第 i

个和第 i + j

个

特征图的重

用特征的数

量是 。也就是

说距离越远

，重用的特征

越少。

2.5.3 小结

截

至本节完稿

，ShuffleNet 算是将轻量

级网络推上

了新的巅峰

，两个版本都

有其独到的

地方。

ShuffleNet v1

中提出

的通道洗牌

操作非常具

有创新性，其

对于解决分

组卷积中的

通道通信问

题非常

简单

、高效。ShuffleNet v2 分析了

衡量模型性

能更直接的

指标：运行时

间。根据对运

行时间的拆

分，

ShuffleNet v2 通过数学

证明、实验证

明或理论分

析等方法提

出了设计高

效模型的 4

条

准则，并根据

这4条准则设

计了ShuffleNet v2。ShuffleNet v2中的通

道拆分也极

具创新性。通

过仔细分析

通道拆分，

我

们发现了

ShuffleNet 和

DenseNet 有异曲同工

之妙，在这里

轻量模型和

高精度模型

交汇在了一

起。

ShuffleNet

v2 的证明、实

验和网络结

构非常精彩

，整篇论文读

完给人一种

畅快淋漓的

感觉，建议读

者读完本节

后拿出论文

通读一遍，一

定会收获很

多。

2.6　CondenseNet

在本节中

，先验知识包

括：

 DenseNet（1.6 节）；

 ResNeXt（2.4 节）；



ShuffleNet（2.5 节）。

CondenseNet1 是

黄高团队对

他们的

DenseNet 的升

级版。他们认

为 DenseNet 的密集连

接其实是

存

在冗余的，它

最大的缺点

便是影响网

络

的效率。首

先，为了缓解

DenseNet 的冗

余问题

，CondenseNet

提出了在训

练的过程

中

对不重要的

权值进行剪

枝，即学习一

个

稀疏的网

络。但是测试

的整个过程

就是一

个简

单的卷积，因

为网络已经

在训练的时

候优化完毕

。其次，为了进

一步提升效

率，

CondenseNet 在 1 ×

1 卷积的

时候使用了

分组

卷积。最

后，CondenseNet 中指出邻

近的特

征重

用更重要，因

此采用了呈

指数级增长

的增长率（growth rate），并

在 DenseNet 的

网络块

之间添加了

捷径。

DenseNet、CondenseNet 的训练

和测试

阶段

的示意如图

2.30

所示。这个图

乍看起

来让

人一头雾水

，对于其中的

细节我们会

在后文中给

出详细的解

析。

1　参见

Gao Huang、Shichen Liu、Laurens van

der Maaten 等人

的论文“CondenseNet: An

Efficient DenseNet using Learned

Group Convolutions”。

图 2.30

CondensetNet 概

览

2.6　CondenseNet

2.6.1 分组卷积

的问题

在 ShuffleNet

中

我们指出分

组卷积存在

通道之间的

信息沟通不

畅和特征多

样性不足的

问题。

CondenseNet 提出的

解决策略是

在训练的过

程中让模型

自主地选择

更好的分组

方式，所以理

论上每

个通

道的特征图

是可以和所

有特征图沟

通到的。图

2.31 所

示的是普通

卷积和分组

卷积的对比

示意，

普通卷

积的问题是

卷积的计算

量庞大，虽然

模型容量更

大但不适用

于轻量模型

；分组卷积的

特点是

信息

沟通只发生

在同一个组

内部，虽然减

小了计算量

但是缺乏组

与组之间的

信息交互。

图

2.31　普通卷积和

分组卷积的

对比示意

2.6.2 可

学习分组卷

积

如图 2.32 所示

，可学习分组

卷积（learned group

convolution）可以分

成两个阶段

：浓缩（condensing）

阶段和

优化（optimizing）阶段。其

中浓缩阶段

用于剪枝没

用的特征，优

化阶段用于

优化剪枝之

后

的网络。

图

2.32

C = 3 时的

CondenseNet 的训练

情况

1．CondenseNet 的浓缩

过程

浓缩的

目的是对普

通卷积进行

剪枝，将密集

连接剪枝成

稀疏连接，它

的核心参数

有两个，分组

数 G 和浓缩率

C。在图 2.32

所示的

例子中，分组

数 G = 3，浓缩率

C = 3，它

表示稀疏连

接只保留原

93

第

2 章  轻量级

CNN

94

来密集连接

的 。

具体地讲

，浓缩率为

C 的

CondenseNet 会有 C

− 1 个浓缩

阶段。它的浓

缩阶段 1（图

2.32 的

最

左侧）的初

始化是普通

的 CNN，在训练该

网络时使用

了分组

Lasso 正则

项，这样学到

的特征会呈

现结构化稀

疏分布，好处

是在后面剪

枝部分不会

过分影响精

度。在每次浓

缩阶段训练

完成之后会

有

的特征被

剪枝掉。也就

是说，经过 C

− 1 个

浓缩阶段后

，仅有 的特征

被保留下来

。

假设输入特

征图的通道

数是 R，输出特

征图的通道

数是 O，图像的

尺寸是 W

× H。那么

这时普

通卷

积的计算本

质上是通过

一个大小为

R ×

O × W ×

H 的 4 维张量来

实现的。CondenseNet

的浓

缩阶

段一般

发生在 1 ×

1 卷积

部分，那么这

个 4 维的张量

本质就是一

个大小为

O × R 的

矩阵。对分组

卷积

而言，假

设组数为 G，那

么普通卷积

的计算矩阵

被分成了 G 组

，每组的大小

为

，我们假设

它

们为 {F1,F2,…,FG}。其中

，Fg,i,j 表示第

g 组的

第 i 个输入到

j

个输出的权

值矩阵。

在上

面我们介绍

到每个浓缩

阶段中会有

的特征被剪

枝，那么如何

确定哪些特

征应该被剪

枝

呢？因为经

过分组 Lasso

正则

化之后得到

的是稀疏的

特征，我们一

般可以认为

L1 范数和越大

，该

特征越重

要，而不重要

的特征的 L1

范

数和往往更

接近 0，因此其

被剪枝对模

型性能的影

响更小。L1

范数

和的计算方

式如式（2.11）所示

。

（2.11）

如果只按照

上面的方式

进行剪枝，那

么虽然得到

的连接是稀

疏的，但是因

为每个特征

的连接都

不

通，所以无法

将它们分组

。为了确保能

够引入分组

卷积，我们必

须保证一个

组内的特征

拥有类似

的

稀疏模式，即

使用组级别

的稀疏模式

来代替权值

级别的稀疏

模式。在 CondenseNet

中，这

个模式

叫作

group lasso。为了获得组

级别的稀疏

，CondenseNet 在训练过程

中使用了

group lasso 正

则项 1

，

正则项

的内容如式

（2.12）所示。通过 group lasso

正

则项得到的

权值矩阵更

倾向于以组

为单位将权

值矩阵的一

列均向 0 逼近

，这样便可以

得到以组为

单位的稀疏

模式。图 2.32

所示

的每个浓缩

阶段

的连接

便是通过添

加 group lasso

正则项实

现的。

（2.12）

注意，CondenseNet 的

剪枝并不是

直接将这个

特征删除，而

是通过掩码

的形式将被

剪枝的特

征

置 0，因此在训

练的过程中

CondenseNet 的训练时间

并没有减少

，反而需要更

多的显存来

保存

掩码。

2．CondenseNet 的

优化过程

浓

缩过程之后

是优化过程

，优化过程会

针对浓缩过

程得到的剪

枝之后的网

络结构进行

长时间的

学

习以得到最

终的结果。在

作者的实验

中，CondenseNet

用于优化

过程的总 epoch 数

和浓缩过程

的

是相同的

。具体地讲，我

们假设网络

的总训练

epoch 数

是 M，那么浓缩

过程和优化

过程的 epoch

数

均

是 。因为浓缩

过程要分成

C −

1 个阶段，所以

每个阶段的

epoch 数是 。图

2.33 所示

的

是 CondenseNet

在浓缩

率 C = 4

时的训练

过程和损失

函数的收敛

过程。

1　参见 Ming

Yuan、Yi Lin 的

论文“Model selection

and estimation in regression

with grouped variables”。

2.6

CondenseNet

图 2.33 C

= 4 时的

CondenseNet 的训练

epoch 分布

情况、训练损

失以及余弦

学习率

图 2.33

所

示的训练过

程基于 CIFAR-10 数据

集，CondenseNet 的浓缩率

C

= 4，所以有 3 个浓

缩

阶段。学习

率采用的是

余弦学习率

。每次浓缩之

后损失值会

有明显的变

化，因为这时

候我们会剪

枝

一部分特

征，之所以最

后一个阶段

损失值变化

得最为明显

是因为这一

阶段被剪枝

的特征的比

例最高

（3 个阶

段被剪枝的

特征的比例

依次是

、 和 ）。

3．索

引层

经过训

练过程的剪

枝之后我们

得到了一个

稀疏结构，如

图 2.32 最右侧所

示。但是在推

理时，

剪枝的

形式是不能

用开源框架

提供的分组

卷积操作得

到的，而如果

使用训练过

程中的掩码

的形式便

不

能提升推理

速度，剪枝的

意义将不复

存在。

为了解

决这个问题

，在测试的时

候 CondenseNet 引入了索

引层（index

layer），索引层

的作用是

将

输入特征图

重新整理成

组，然后使用

分组卷积的

高效的特性

。例如，图 2.32 中，组

1

使用的输

入

特征图是 (3,7,9,12)，组

2 使用的输入

特征图是

(1,5,10,12)，组

3 使用的输入

特征图是 (5,6,8,11)，

索

引层的作用

就是将输入

特征图排列

成

(3,7,9,12,1,5,10,12,5,6,8,11) 的形式，之

后便可以只

使用标准

的

分组卷积，如

图 2.34

所示。在 PyTorch 中

，我们可以使

用 index_select

快速实现

索引层的功

能。

图 2.34 CondenseNet

的测试

和索引层示

意

95

第 2

章  轻量

级 CNN

2.6.3 架构设计

在 CondenseNet 中作者对

DenseNet

做了两点改

进。

z 增长率的

指数级增长

：增长率 k

是在

DenseNet 中提出的一

个超参数，反

映的是每个

密集

块中特

征图通道数

的增长速度

，例如一个网

络块中第 1

层

的特征图的

通道数是 k0，那

么第

i 层的通

道数即

k0 + k·(i −

1)，DenseNet 的增

长方式是线

性的。通过可

视化 DenseNet 中特

征

重用的热力

图，作者发现

对一个深层

的网络来说

，越接近输出

层的特征对

结果的贡献

越

大。为了实

现上述目的

，CondenseNet 使用了指数

级增长的增

长率。按照上

面给出的定

义，

第

i 层的通

道数是 k =

2i−1

·k0，也就

是说越接近

输出层的地

方保留的特

征图越多。增

长率

的指数

级变化会带

来准确率的

提升，但会对

速度产生负

面影响。更多

时候，我们会

将增长

率的

变化方式作

为平衡速度

与精度的超

参数。

z 全密集

连接：在 DenseNet 中，块

之间是没有

捷径的，CondenseNet

在块

之间也增加

了捷

径，结合

平均池化，用

于实现不同

尺寸的特征

图之间的拼

接，进而实现

更强的特征

重用，

如图 2.35

所

示。

图 2.35 CondenseNet

块之间

的全密集连

接

2.6.4 小结

CondenseNet

最大

的创新点是

将模型剪枝

和分组卷积

进行了有机

的结合。稀疏

卷积和分组

卷积

的共同

点是只有部

分权值参与

计算，不同点

是稀疏卷积

的有效权值

是无规律的

，而分组卷积

的有效

权值

是以组为单

位的，所以如

果要连接它

们，最关键的

步骤是要将

稀疏卷积的

无规律的特

征整理成

分

组卷积的有

规律的特征

。而使用

group lasso 正则

化可以实现

这一关键步

骤，通过 group

lasso 正

则

化我们可以

得到以组为

单位的稀疏

模式，也就可

以以组为单

位进行特征

剪枝了。上述

操作均发生

在训练阶段

，为了模型推

理时的高速

运行，需要在

剪枝之后添

加一个索引

层来将带有

剪枝的分组

卷

积转换为

普通的分组

卷积。

对比 ShuffleNet 通

过通道交换

的方式来进

行不同组的

通道之间的

通信，CondenseNet 则通过

剪

枝来学习

哪些特征需

要通信、哪些

权值要被剪

枝，所以理论

上要比 ShuffleNet 拥有

更好的效果

。对

于

DenseNet 的改进

，指数级的增

长率和捷径

连接则不具

有很强的创

新性。

96

模型架

构搜索最早

可以追溯到

PolyNet。PolyNet

将 CNN 归纳为一

个多项式，将

多项式展开

并在展

开式

的基础上设

计了更多的

网络模型。但

是它并没有

使用强化学

习，探索的过

程由人工设

计并完成。

目

前公认的基

于强化学习

的神经架构

搜索算法在

以 Quoc V.Le 为核心的

Google

Deep Mind 团队

的神经

架构搜索（neural

architecture search，NAS）系

列论文中提

出，他们的核

心观点是使

用强化

学习

来生成一个

完整的网络

或一个可以

重复使用的

网络节点。NAS 在

该系列的第

一篇论文中

提出，

它使用

强化学习在

CIFAR-10 上学习到了

一个类似于

DenseNet 的完整的密

集连接的网

络。

NASNet

解决了 NAS 不

能应用在 ImageNet

上

的问题，它学

习的不再是

一个完整的

网络而是一

个网络单元

。这种单元的

结构往往比

NAS 网络要简单

得多，因此学

习起来效率

更高；而且通

过堆

叠更多

NASNet 单元的方式

可以非常方

便地将其迁

移到其他任

何数据集，包

括

ImageNet。PNASNet

则是一个

性能更高的

强化学习方

法，其具有比

NASNet 更小的搜索

空间，而且使

用了启发式

搜索、

策略函

数等强化学

习领域的方

法，优化了网

络超参数的

学习过程。

除

了强化学习

，AmoebaNet 使用了遗传

算法的进化

（evolution）策略实现了

模型架构的

学习过

程，它

主要提出了

一个叫作年

龄进化（aging evolution，AE）的进

化策略，使得

训练过程可

以更加

关注

网络结构而

非模型参数

。

模型架构搜

索的另一个

方向是搜索

一个更轻量

级的模型架

构，MnasNet 将网络时

延作为了优

化目标

的约

束条件，得到

了超过

MobileNet v2 的速

度和精度。MobileNet v3

使

用了全局优

化和局部优

化“两步走”

的

策略，并结合

人工设计对

耗时的单元

进行了调整

。EfficientNet v1 从最基础的

超参数—图像

分辨率、

模型

的深度和宽

度 3 个方向对

模型架构进

行了搜索，探

索出了一个

基线（baseline）模型，在

基线模型的

基础上可以

快速缩放出

更多的模型

。EfficientNet v2

将训练速度

也作为优化

指标之一，并

深入探讨了

模型

优化和

正则项（数据

扩充、正则等

）之间的关系

，提出了递增

式的自动机

器学习（AutoML）方法

。

通过上面的

分析我们可

以看出 CV

中的

经典分类网

络有 3 个主流

的方向，分别

是提升精度

、轻

量化和基

于强化学习

的模型架构

搜索。3

个方向

相辅相成，相

互借鉴又相

互超越，共同

为 CV 的蓬

勃发

展做出贡献

。

3.1　PolyNet

在本节中，先

验知识包括

：



GoogLeNet（1.3 节）；  残差网络

（1.4

节）；

 DenseNet（1.6 节）；

 Dropout（6.1 节）。

第

3 章

模型架构搜

索

第

3 章  模型

架构搜索

98

在

CV 领域，优化卷

积模型的骨

干网络有 3

个

主要方向：精

度、速度、模型

架构搜索。其

中，

Inception 和残差网

络是骨干网

络中最具代

表性的两个

模型，它们分

别从模型的

深度和宽度

进行了探

索

。在

Inception-ResNet 中，它将 Inception 模

块和残差模

块进行了整

合，进一步提

升了模型的

建模

能力。

本

节要介绍的

PolyNet1 可以看作 Inception-ResNet

的

进一步扩展

，它从多项式

的角度推出

了

更加复杂

且效果更好

的混合模型

，并通过实验

得出了这些

复杂模型的

最优混合形

式，命名为 Very

Deep PolyNet。它

认为虽然增

加网络的深

度和宽度能

带来性能的

提升，但是随

着深度或者

宽度的

不断

增加，由此带

来的收益会

很快趋于平

稳，这时候如

果从结构多

样性的角度

出发优化模

型，带

来的效

益也许会优

于增加深度

或者宽度带

来的效益，这

为我们优化

模型架构提

供了一个新

的

方向。

PolyNet 最大

的创新点是

将网络结构

建模为多项

式的形式，再

通过对多项

式的展开和

改造，得

到更

多的模型架

构。PolyNet

虽然没有

像 NAS 等使用强

化学习来搜

索模型结果

，但是这种人

工搜索

也可

以归类为

AutoML 技

术的一种。

3.1.1 结

构多样性

当

前提高网络

表达能力的

一个最常见

的策略是增

加网络深度

，但是如图 3.1 所

示，随着网络

深

度的增加

，网络的收益

很快趋于平

稳。另一个模

型优化的策

略是增加网

络的宽度，例

如增加特征

图

的数量。但

是增加网络

的宽度是非

常不经济的

，因为每增加

k 个参数，其计

算复杂度和

占用的显存

都要增加 k

2

。而

且有实验证

明，卷积中的

特征图存在

很大的冗余

，更宽的网络

往往会继续

增大这种

冗

余，模型也很

快进入性能

增长的平缓

期。

图 3.1

模型准

确率随网络

深度增加而

提升

因此作

者基于 Inception-ResNet 的思

想，希望通过

更复杂的网

络块结构来

获得比增加

深度或者

增

加宽度所得

到的更大的

效益，这种策

略在真实场

景中还是非

常有用的，例

如在有限的

硬件资源条

件下最大化

模型的精度

。

3.1.2 多项式模型

PolyNet 是从多项式

的角度推导

网络块结构

的。首先，一个

经典的残差

网络块可以

表示为：

(I + F)·x =

x + F·x =

x + F(x) （3.1）

1　参见

Xingcheng Zhang、Zhizhong Li、Chen

Change Loy 等人的论文

“PolyNet: A

Pursuit of Structural Diversity

in Very Deep

Networks”。

3.1　PolyNet

99

其中，x 是输入

，I 是单位映射

，“ +

”表示单位加

操作，F 表示在

残差网络中

的非线性变

换。如果

F 是

Inception 的

话，式（3.1）便是 Inception-ResNet 的

表达式，如图

3.2

所示。

图 3.2　网络

块结构

下面

我们将 F 看作

Inception，然后通过将

式（3.1）表示为更

复杂的多项

式的形式来

推导出几

个

更复杂的结

构。

z poly-2：I + F

+ F2

。在这个结

构中，网络有

3 个分支，左侧

路径是一个

直接映射，中

间路径

是一

个 Inception，右侧路径

是两个连续

的 Inception，如图 3.3（a）所示

。在这个网络

中，

所有 Inception 的参

数是共享的

，所以不会引

入额外的参

数。由于参数

共享，我们可

以推

出它的

等价形式，如

图

3.3（b）所示。因为

I + F +

F2

 = I

+ (I + F)F，这种形式的

网络的计算

量少了

1/3。

z mpoly-2：I +

F + GF。这个

网络块的结

构和图 3.3（b）所示

的相同，不同

之处是两个

Inception

的参数不共

享。也可以将

其表示为 I + (I

+ G)F，如

图 3.3（c）所示。它具

有更强

的表

达能力，但是

参数数量也

加倍了。

z 2-way ：I +

F + G。这个

结构是向网

络中添加一

个额外且参

数不共享的

残差块，思想

和多

路

- 残差

网络相同，如

图 3.3（d）所示。

上面

介绍的多项

式模型都是

二次幂的。但

是只要我们

不限制多项

式的幂数，便

可以衍生出

无限

的网络

模型。但是出

于对计算速

度的考虑，我

们仅考虑下

面 3 个三次幂

的结构。

z

poly-3：I + F +

F2

 + F3

。

z mploy-3：I +

F + GF +

HGF。

z 3-way ：I

+ F + G

+ H。

回顾

我们在 1.6

节介

绍的 DenseNet，你会发

现 DenseNet 本质上也

是一个多项

式模型。一个

含

有 n 个卷积

块的 DenseNet，可以用

式（3.2）表示：

I ⊕ C ⊕

C2 ⊕…⊕ Cn （3.2）

其中

，C 表示一个普

通的卷积操

作，⊕表示特征

拼接。

第 3

章  模

型架构搜索

图 3.3

PolyNet 的几种网

络块

3.1.3 对照实

验

如图 3.4 所示

，我们把 Inception-ResNet

分成

A、B、C 共 3 个阶段，它

们处理的特

征图尺寸分

别是

35 × 35、17 ×

17、8 × 8。如果将

A、B、C 分别替换为

3.1.2

节中提出的

6 个模型（3 个二

次幂

模型，3

个

三次幂模型

），我们可以得

到共 18 个不同

的网络结构

。给它们相同

的超参数和

训练集，

我们

得到的对照

实验结果如

图

3.5 所示。图 3.5（a）比

较的是训练

时间和精度

的关系，图 3.5（b）

比

较的是参数

数量和精度

的关系。其中

，Inception-ResNet 3-6-3 表示图 3.4（b）网络

。

图 3.4 Inception 的

3 个阶段

和 PolyNet 提供的替

换方式

通过

图 3.5 我们可以

得到下面几

条重要信息

：

z

阶段 B 的替换

最有效；

z

阶段

B 中使用 mpoly-3 最有

效，poly-3

次之，但是

poly-3 的参数数量

要少于 mpoly-3 的；

z 阶

段 A 和阶段

C 均

是使用 3-way 替换

最有效，但是

引入的参数

最多；

z 3 路 Inception

的结

构一般要优

于 2 路 Inception

的。

另外

一种策略是

使用 3 路

Inception 的混

合模型，在论

文中使用的

是 4 组

3-way → mpoly-

3

→ poly-3 替换 Inception-ResNet

中

的阶段 B，实验

结果表明这

种替换的效

果要优于任

何形式的非

混

合模型。

100

3.1　PolyNet

图

3.5 PolyNet

精度

3.1.4 Very Deep

PolyNet

基于上

面提出的几

个模型，作者

提出了骨干

网络 Very Deep

PolyNet，结构如

下。

z 阶段 A

：包含

10 个 2-way 的基础模

块。

z 阶段 B ：包含

10

个 poly-3、10 个 2-way

的基础

模块（即 20 个基

础模型）。

z

阶段

C ：包含 5 个

poly-3、5 个 2-way 的

基础模块（即

10

个基础模块

）。

初始化：作者

发现如果先

搭好网络再

使用随机初

始化的策略

非常容易导

致模型不稳

定，因此使

用

了两个策略

。

（1）插入初始化

（initialization

by insertion）：先使用迁移

学习训练一

个 Inception-ResNet 模型，

再通

过向其中插

入一个 Inception 块构

成 poly-2。

（2）交叉插入

（interleaved）：当加倍网络

的深度时，将

新加入的随

机初始化的

模型交叉地

插入

迁移学

习的模型中

效果很好。

初

始化如图 3.6

所

示。

随机路径

：受到 Dropout 的启发

，PolyNet

在训练的时

候会随机丢

掉多项式网

络块中的一

项或

几项，如

图 3.7 所示。这一

步相当于数

据扩充，对提

升模型的泛

化能力非常

有帮助。

加权

路径：简单版

本的多项式

结构容易导

致模型不稳

定，Very Deep PolyNet 提出的策

略是为

Inception 部分

乘权值 β，例如

2-way 的表达式将

由

I + F +

G 变成 I +

βF + βG，论文

给出的 β

的参

考

值是 0.3。

101

第 3 章

模型架构搜

索

图 3.6　初始化

图 3.7

随机路径

3.1.5 小结

PolyNet 从多项

式的角度提

出了更多由

Inception

和残差块组

合而成的网

络结构，模型

并没有创

新

性。其最大的

优点在于从

多项式的角

度出发，并且

我们从这个

角度发现了

PolyNet 和 DenseNet

有异曲同

工之妙。

论文

中最优结构

的选择是通

过实验得出

的，如果能结

合数学推导

得出前因后

果，PolyNet 将上升

到

另一个水平

。结合上面的

网络块，作者

提出了混合

模型

Very Deep PolyNet 并在

ImageNet 取

得了约

4.25% 的

top-5 的

错误率。

最后

训练的时候

使用的初始

化策略和基

于集成思想

的随机路径

非常有实用

价值。PolyNet 最大

的

贡献在于开

辟了使用数

学表达式的

形式对模型

架构进行搜

索的道路。与

之对应的另

一种方式是

我

们接下来

要介绍的使

用强化学习

来对网络结

构进行设计

。

102

3.2

NAS

103

3.2　NAS

在本节中，先

验知识包括

：

 AlexNet（1.1 节）；

 LSTM（4.1 节）；



残差网

络（1.4 节）；  注意力

机制（4.2

节）。

CNN 和 RNN

是

目前主流的

CNN 框架，这些网

络均由人工

设计，然而设

计这些网络

是非常困

难

的，它依靠开

发者的经验

。Quoc V.

Le 等人在神经

架构搜索（neural architecture search，NAS）1

的

论文中提出

了使用强化

学习（reinforcement learning，RL）学习一

个 CNN 的完整架

构（后面简称

NAS-CNN）或者一个

RNN 单

元（后面简称

NAS-RNN）。在 CIFAR-10 图像数据

集上，NAS-CNN

的准确

率已经逼近

当时效果最

好的DenseNet，在Penn Treebank语言

模型文本数

据集上2

，NAS-RNN

的表

现要优于

LSTM。

这

篇论文提出

了 NAS，算法的主

要目的是使

用强化学习

寻找最优网

络，包括一个

图像分类网

络的卷积部

分（表示层）和

RNN 的一个类似

于

LSTM 的门控单

元。和之前介

绍的算法不

同的是，

NAS 学习

的是网络的

超参数而不

是参数。超参

数的一个特

点是不能通

过反向传播

来优化，因此

需

要借助强

化学习的采

样策略来实

现超参数的

优化。

现在的

神经网络一

般采用堆叠

连续的网络

块的方式搭

建而成，这种

堆叠的超参

数可以通过

一个

序列来

表示。而这种

序列的表示

正是 RNN

所擅

长

的工作，所以

NAS 会使用一个

由 RNN

构成

的控

制器（controller），以概率

P 随机采样一

个

网络结构

A，接着训练这

个网络并得

到其在验

证

集上的精度

作为奖励 R（reward），最

后使用

奖励

R 更新控制器

的参数，如此

循环执行直

到

模型收敛

或达到停止

条件，如图 3.8 所

示。

3.2.1

NAS-CNN

首先我们

考虑最简单

的只由卷积

层构成的 CNN，这

种类型的网

络是很容易

用由 RNN

构成的

控制

器来表

示的。具体地

讲，我们将控

制器分成 N 段

，每一段有若

干个输出，每

个输出表示

CNN

的一个

超参

数，如卷积核

的数量、卷积

核的高度、卷

积核的宽度

、纵向步长、横

向步长等，如

图 3.9 所示。

NAS 的控

制器选择了

LSTM 节点的 RNN，那唯

一剩下的难

点便是如何

更新控制器

的参数

θc 了。

控

制器每生成

一个网络可

以看作一个

行动，记作 a1:T（action），其

中

T 是要预测

的超参数的

数

量。当模型

收敛时其在

验证集上的

精度是 R。我们

使用

R 来作为

强化学习的

奖励，也就是

说通过调

整

参数 θc

来最大

化 R 的期望，表

示为式（3.3）。

（3.3）

由于

R 是不可导的

，因此我们需

要一种可以

更新 θc 的策略

，NAS

中采用的是

Williams 等人提

出的

强化规则（reinforce rule）3

，如

式（3.3）所示。

1　参见

Barret Zoph、Quoc

V. Le 的论文“Neural Architecture

Search with Reinforcement Learning”。

2　参见

Mitchell Marcus、Beatrice Santorini、Mary

Ann Marcinkiewicz 的 论

文“Building a Large Annotated

Corpus of English: The

Penn Treebank”。

3　参见

Ronald

J.Williams 的论文“Simple Statistical Gradient-Following

Algorithms for Connectionist Reinforcement

Learning”。

图 3.8 NAS

的

算法流程

第

3 章

模型架构

搜索

104

（3.4）

图

3.9 NAS-CNN 的控

制器结构

式

（3.4）近似等价于

：

（3.5）

其中，m 是每个

批次中网络

的数量。

式（3.5）是

对梯度的无

偏估计，但是

往往方差比

较大，为了减

小方差，算法

中使用的是

更

新值：

基线

b 是以前训练

的架构的精

度的指数移

动平均值。

上

面得到的控

制器的搜索

空间是不包

含跳跃连接

的，所以不能

产生类似于

残差网络或

者

Inception

的网络。NAS-CNN 是

通过在上面

的控制器中

添加注意力

机制来添加

跳跃连接的

，如图 3.10

所示。

图

3.10 NAS-CNN 中加入跳跃

连接的控制

器结构

在第

N 层，我们添加

N − 1

个锚点来确

定是否需要

在该层和之

前的某一层

之间添加跳

跃连接，

这个

锚点是通过

两层的隐层

节点状态和

sigmoid 激活函数来

完成判断的

。具体地讲，我

们可以将第

i 个节点与第

j

个节点之间

是否有捷径

表示为式（3.6）：

P( j 层

是

i 层的输入

) = sigmoid[v

T

 tanh(Wprev·hj +

Wcurr + hi

)]

（3.6）

其中，hi 和 hj

分别

是当前层和

第 j 层在控制

器中隐层节

点的状态，j ∈

[0, N − 1]。Wprev、Wcurr

和

v

T

是可学习的

参数，注意，跳

跃连接的添

加并不会影

响更新策略

。

由于添加了

跳跃连接，由

训练得到的

参数可能会

产生许多问

题，例如某个

层和其他所

有层都没

3.2　NAS

有

产生连接等

，因此对于几

个特殊情况

我们需要注

意：

z

如果一个

层和其之前

的所有层都

没有产生跳

跃连接，那么

这层将作为

输入层；

z 如果

一个层和其

之后的所有

层都没有产

生跳跃连接

，那么这层将

作为输出层

，并和所有输

出层拼接之

后作为分类

器的输入；

z

如

果输入层拼

接了多个尺

寸的输入，我

们可以给小

尺寸输入添

加值为 0 的边

距让其尺寸

与

大尺寸输

入对齐。

除了

卷积和跳跃

连接，池化、BN、Dropout 等

策略也可以

通过相同的

方式添加到

控制器中，只

不过这时候

需要引入更

多的策略相

关参数。

经过

训练之后，在

CIFAR-10 上得到的

CNN 如

图 3.11 所示，它是

NAS-CNN

生成的密集

连接

的网络

结构。其中，FH 指

卷积核的高

度，FW 指卷积核

的宽度，N

指卷

积核的个数

。

图 3.11　在

CIFAR-10 上得到

的 CNN

105

第 3 章

模型

架构搜索

从

图 3.11 中我们可

以发现，NAS-CNN

和 DenseNet 有

很多相通的

地方：

z

都用密

集连接；

z 特征

图的个数都

比较少；

z

特征

图之间都采

用拼接的方

式进行连接

。

在 NAS-CNN 的实验中

，使用的是

CIFAR-10 数

据集。搜索空

间如下：

z 卷积

核的高度的

范围是

{1,3,5,7} ；

z 卷积

核的宽度的

范围也是

{1,3,5,7} ；

z 通

道个数的范

围是

{24,36,48,64} ；

z 步长分

为固定为

1 和

范围为 {1,2,3} 两种

情况；

z 再加上

BN 和跳跃连接

。

控制器使用

的是含有

35 个

隐层节点的

LSTM。

3.2.2 NAS-RNN

在这篇论文

中，作者采用

强化学习的

方法同样生

成了类似于

LSTM 的门控机制

的一个 RNN 单

元

。控制器的参

数更新方法

和 3.2.1 节所述的

类似，这里我

们主要介绍

如何使用一

个 RNN

控制器来

描述一个 RNN 单

元。

传统

RNN 的输

入是 xt 和

ht−1，输出

是 ht

，计算方式

是 ht

= tanh(W1xt

 +

W2ht−1)。LSTM 的输入是

xt

、ht−1 以及单元状

态

ct − 1，输出是 ht

和

ct

，所以 LSTM 的门控

单元可以看

作一个将

xt

、ht−1 和

ct−1

作为根节点

的树结构，树

中的各个节

点表示

LSTM 的不

同的门控单

元，如图 3.12 所示

。

练习：你能从

图 3.12 中找出 LSTM

的

输入门、输出

门以及遗忘

门吗？

图 3.12 LSTM

的计

算

和 LSTM 一样，NAS-RNN

也

需要输入一

个 ct−1，输出一个

ct

，并在控制器

的最后两个

单元中

控制

如何使用

ct−1 以

及如何计算

ct

。

NAS-RNN

的控制器如

图 3.13 所示，在这

个树结构中

有两个叶子

节点和一个

中间节点，这

种有

106

3.2　NAS

两个叶

子节点的情

况简称为 base2，而

图

3.12 中的 LSTM 则是

base4。叶子节点的

索引是

0、1，中

间

节点的索引

是 2，如图 3.13（a）所示

。也就是说，控

制器需要预

测

3 个网络块

，每个网络块

包

含一个操

作（加、点乘等

）和一个激活

函数（ReLU、sigmoid、tanh 等）。在

3 个

网络块之后

接的

是一个

添加单元（cell inject），用

于控制

ct−1 的使

用，最后是一

个指示单元

（cell indices），确定哪

些树

用于计算

ct

，如

图 3.13（b）所示。

图

3.13 NAS-RNN 的

控制器生成

RNN 节点示意

详

细分析一下

图 3.13：

z 控制器为

索引为

0 的树

节点，预测的

操作和激活

函数分别是

单位加和 tanh，意

味着 a0

= tanh 

(W1

× xt

 +

W2 × ht−1) ；

z 控制器

为索引为 1 的

树节点，预测

的操作和激

活函数分别

是

ElemMult 和 ReLU，意味着

a1 =

ReLU[(W3 × xt

)

⊙ (W4 × ht−1)]

；

z 控制器为指

示单元的第

二个元素预

测的值为 0，添

加单元的操

作和激活函

数是单位加

和

ReLU，意味着 a0 值

需要更新为

a0

new

= ReLU(a0 + ct−1)，注意这里不

需要额外的

参数；

z 控制器

为索引为 2 的

节点预测的

操作和激活

函数分别是

ElemMult

和 sigmoid，意味着

a2 =

sigmoid(a0

new ⊙ a1)，因

为

a2 是最大的

树的索引，所

以 ht

= a2；

z 控制器为

指示单元的

第一个元素

预测的值是

1，意思是

ct 要先

使用索引为

1 的树再使用

激

活函数的

值，即

ct

 = (W3

× xt

) ⊙

(W4 × ht−1)。

上面是

以

base2 的超参数

为例进行讲

解的，如图 3.13（c）所

示。在实际中

使用的是 base8，得

到图

3.14 所示的

两个 RNN 单元。图

3.14（a）所示的是不

包含

max 和 sin 的搜

索空间，图

3.14（b）

所

示的是包含

max 和 sin

的搜索空

间（控制器并

没有选择 sin）。

在

生成 NAS-RNN

的实验

中，使用的是

Penn Treebank 数据集。操作

的范围是 [add,elem_mult]，

激

活函数的范

围是 [identity,tanh,sigmoid,relu]。

107

第

3 章  模

型架构搜索

108

图 3.14 NAS-CNN 生成的网

络节点的计

算

3.2.3 小结

Google 的

Quoc V. Le 团

队是

AutoML 领域的

领军者，他们

除了具有很

多机构难以

企及的硬件

资源，

还拥有

扎实的技术

积累。Quoc V.

Le 等人的

NAS 的论文开创

性地使用了

强化学习进

行模型架构

的搜索，

提出

了

NAS-CNN 和 NAS-RNN 两个架

构，二者的共

同点是都使

用一个

RNN 作为

控制器来描

述生成的

网

络结构，并使

用生成的网

络结构在验

证集上的表

现，结合强化

学习算法来

训练控制器

的参数。

Quoc

V.Le 等人

的 NAS 的论文可

以说是

AutoML 领域

的基石级别

论文，极具创

新性，不止是

其

算法足够

新颖，更重要

的是 Quoc

V. Le 团队开

辟的使用强

化学习来学

习网络结构

可能在未来

几年引

领模

型自动生成

的方向，尤其

是在硬件资

源不再那么

昂贵的时候

。论文的探讨

比较基础，留

下了大

量的

待开发空间

供科研工作

者探索，期待

未来几年出

现更高效、更

精确的模型

。

3.3　NASNet

在本节中，先

验知识包括

：

 NAS（3.2 节）

在

3.2 节中我

们介绍了如

何使用强化

学习学习一

个完整的 CNN 或

一个独立的

RNN

单元，这种

数

据集敏感（dataset interest）的

网络的效果

是目前最优

的。但是 NAS

提出

的网络的计

算代价是相

当大的，仅仅

在 CIFAR-10 上学习一

个网络就需

要 500

块 GPU 运行 28

天

才能找到最

优结构。这使

得 NAS 很难迁移

到大数据集

上，更不要提

ImageNet 这样几百

GB 的

数据集了。而

在目前的行

业规

则上，如

果不能在 ImageNet

上

取得令人信

服的结果，这

样的网络结

构很难令人

信服。

为了将

NAS 迁移到大数

据集乃至 ImageNet

上

，这篇论文提

出了在小数

据集（CIFAR-10）上

学习

一个网络单

元，然后通过

堆叠更多的

网络单元的

方式将网络

迁移到更复

杂、尺寸更大

的数据

集上

。因此这篇论

文的最大贡

献便是介绍

了如何使用

强化学习学

习这些网络

单元。作者将

用于

ImageNet

的 NAS 简称

为 NASNet1

，实验结果

证明了 NASNet 的有

效性，其在 ImageNet

数

据集上的

1　参

见 Barret

Zoph、Vijay Vasudevan、Jonathon Shlens 等人的论

文“Learning

Transferable Architectures for Scalable

Image 

Recognition”。

3.3

NASNet

top-1 准确率和

top-5 准确率均取

得了当时最

优的效果。

阅

读本节前，强

烈建议看完

3.2 节，因为本节

并不会涉及

强化学习部

分，只会介绍

控制器是如

何学习一个

NASNet 网络块的。

3.3.1

NASNet 控

制器

在 NASNet

中，完

整的网络结

构还是需要

手动设计的

，NASNet 学习的是完

整网络中被

堆叠、

被重复

使用的网络

单元。为了便

于将网络迁

移到不同的

数据集上，我

们需要学习

两种类型的

网络块。

z

普通

单元（normal cell）：输出特

征图和输入

特征图的尺

寸相同。

z 缩减

单元（reduction

cell）：输出特

征图相对输

入特征图进

行了一次降

采样。在缩减

单元

中，使用

特征图作为

输入的操作

（卷积或者池

化）的默认步

长为 2。

NASNet

的控制

器的结构示

意如图 3.15 所示

，每个网络单

元由 B

个网络

块组成，在实

验中

B = 5。每个块

的具体形式

如图

3.15（b）所示，每

个块由并行

的两个卷积

组成，它们会

由控制器决

定选择哪些

特征图作为

输入（灰色部

分）以及使用

哪些操作（黄

色部分）来计

算输入的特

征图。最

后它

们会由控制

器决定如何

合并特征图

。

图 3.15

NASNet 的控制器

的结构示意

更精确地讲

，NASNet 网络单元的

计算分为 5

步

：

（1）从第 i −

1 个特征

图或者第 i 个

特征图或者

之前已经生

成的网络块

中选择一个

特征图作为

隐

层 A 的输入

；

（2）采用和第（1）步

类似的方法

为隐层

B 选择

一个输入；

（3）为

第（1）步的特征

图选择一个

操作；

（4）为第（2）步

的特征图选

择一个操作

；

（5）选择一个合

并操作，合并

第（3）步和第（4）步

得到的特征

图。

图 3.16 所示的

是学习到的

网络单元，从

中可以看到

2

种不同的输

入特征图的

情况。

在第（3）步

和第（4）步中我

们可以选择

的操作（卷积

类型空间）有

：

z 直接映射；

z 1 × 1

卷

积；

z 3 ×

3 卷积；

z 3

× 3 深度

可分离卷积

；

z

3 × 3 空洞卷积；

z 3 × 3

平

均池化；

z 3 ×

3 最大

池化；

z 1

× 3 卷积 +

3 × 1 卷

积；

109

第 3 章

模型

架构搜索

图

3.16 NASNet 生成的

CNN 单元

z 5 ×

5 深度可分离

卷积；

z 5

× 5 最大池

化；

z

7 × 7 深度可分

离卷积；

z 7 × 7

最大

池化；

z 1 ×

7 卷积 + 7

× 1 卷

积。

在第（5）步中

可以选择的

合并操作有

单位加、拼接

。

最后所有生

成的特征图

通过拼接操

作合成一个

完整的特征

图。

为了能让

控制器同时

预测普通单

元和缩减单

元，RNN 会有 2

× 5 × B

个输

出，其中前 5 × B

个

输

出预测普

通单元的 B 个

块（如图

3.15（a）所示

的每个块有

5 个输出），后 5 ×

B 个

输出预测缩

减单

元的 B

个

块。RNN 使用的是

单层 LSTM，隐层节

点数为 100。

3.3.2 NASNet 的强

化学习

NASNet

的强

化学习思路

和 NAS 的相同，有

几个技术细

节这里说明

一下：

z

NASNet 进行迁

移学习时使

用的优化策

略是最近策

略优化（proximal policy optimization，

PPO）1

；

z 作者

尝试了均匀

分布的搜索

策略，效果略

差于

PPO。

3.3.3 计划 DropPath

在

优化类似于

Inception 的多分支结

构时，以一定

概率随机丢

弃掉部分分

支是避免过

拟合的

一种

非常有效的

策略，如 DropPath2

。但是

DropPath 对 NASNet 不是非常

有效。在

NASNet 的计

划

1　参见

John Schulman、Filip Wolski、Prafulla Dhariwal

等人

的论文“Proximal Policy Optimization Algorithms”。

2　参见

Gustav Larsson、Michael Maire、Gregory

Shakhnarovich 的 论 文“FractalNet:

Ultra-Deep Neural Networks without

Residuals”。

110

3.3　NASNet

111

DropPath（scheduled DropPath）中，丢

弃的概率会

随着训练时

间的增加而

线性增加。这

么做的动

机

很好理解：训

练的次数越

多，模型越容

易过拟合，DropPath

的

避免过拟合

的作用才能

发挥得越

充

分。

3.3.4 其他超参

数

在 NASNet 中，强化

学习的搜索

空间大大减

小，

很多超参

数已经固定

，仅有少部分

需要人工设

定。这

里介绍

一下 NASNet 需要人

为设定的超

参数。

z

激活函

数统一使用

ReLU，实验结果表

明

ELU 非线性效

果略差于 ReLU。

z 全

部使用有效

卷积，边距（padding）值

由卷

积核大

小决定。

z

缩减

单元的特征

图的数量需

要乘 2，普通单

元数量不变

。初始数量人

为设定，一般

来说

数量越

多，计算越慢

，效果越好。

z

普

通单元的重

复次数（图 3.17 中

的 N）人为

设定

。

z 深度可分离

卷积在深度

卷积和点卷

积中不使

用

BN

或 ReLU。

z 使用深度

可分离卷积

时，该算法执

行两次。

z 所有

卷积遵循 ReLU →

卷

积→ BN 的计算

顺

序。

z 为了保持

特征图的数

量的一致性

，必要的时

候

添加 1

× 1 卷积。

堆

叠单元在

CIFAR-10 和

ImageNet 上得到的网

络

结构如图

3.17

所示。

3.3.5 小结

NASNet

最

大的贡献是

解决了 NAS 无法

应用到大数

据集上的问

题，它使用的

策略是先在

小数

据集上

学习一个网

络单元，然后

在大数据集

上堆叠更多

的单元来完

成模型迁移

。

NASNet 已经不再是

数据集敏感

的网络了，因

为其中大量

的参数都是

人为设定的

，网络的搜索

空间更倾向

于密集连接

的方式。这种

人为设定参

数的正面影

响就是缩小

了强化学习

的搜索空间

，从

而提高了

运算速度，在

相同的硬件

环境下，NASNet 的速

度要比

NAS 的快

7 倍。

NASNet

的网络单

元本质上是

一个更复杂

的 Inception，可以通过

堆叠网络单

元的方式将

其迁移

到任

意分类任务

，乃至任意类

型的任务中

。论文中使用

NASNet 进行物体检

测的效果也

要优于其他

网络。

本节使

用 CIFAR-10 得到的网

络单元其实

并不是非常

具有代表性

，理想的数据

集应该是

ImageNet。但

是现在由于

硬件的计算

能力有限，无

法在

ImageNet 上完成

网络单元的

学习，随着硬

件性能的提

升，基于 ImageNet 的

NASNet 一

定会出现。

图

3.17 NASNet

在 CIFAR-10 和 ImageNet

上的网

络结构

第 3 章



模型架构搜

索

112

3.4　PNASNet

在本节中

，先验知识包

括：

 NAS（3.2 节）；

 NASNet（3.3 节）。

在

3.2 节

和 3.3 节中我们

介绍了如何

使用强化学

习训练

CNN 的超

参数。NAS 在 NAS

系列

文章

的第一

篇中被提出

，这一篇中还

提出了使用

强化学习训

练控制器（RNN），该

控制器的输

出是 CNN

的超参

数，可以生成

一个完整的

CNN。NASNet

提出学习网

络的一个单

元比直接学

习整个网络

效率

更高且

更容易迁移

到其他数据

集，并在 ImageNet 上取

得了当时最

优的效果。

约

翰斯·霍普金

斯大学在读

博士刘晨曦

在 Google 实习时发

布了一篇论

文，基于 NASNet

提出

了 PNASNet1

。PNASNet 的训练时

间降为

NASNet 的 1/8 并

且在

ImageNet 上取得

了比 NASNet 更优

的

效果。其主要

的优化策略

如下。

z 更小的

搜索空间。

z

基

于顺序模型

的优化（sequential model-based optimization，SMBO）策略

：一种启发式

搜

索的策略

，训练的模型

从简单到复

杂，从剪枝的

空间中进行

搜索。

z 代理函

数：使用代理

函数预测模

型的精度，省

去了耗时的

训练过程。

3.4.1 更

小的搜索空

间

回顾 NASNet 的控

制器，它是一

个有 2

× B × 5

个输出

的 LSTM，其中 2 表示

分别学习普

通单元

和缩

减单元，B 表示

每个网络单

元有 B 个网络

块，5

表示网络

块有 5 个需要

学习的超参

数，记作 (I1,

I2, O1, O2, C)。

用于

预测网络块

隐层状态的

输入之一，它

会从之前一

个、之前两个

，或者已经

生

成好的网络

块中选择一

个作为输入

，I2 同理。 用于预

测作用于两

个输入的操

作，它共

有 13 个

，具体的操作

类型见 NASNet。

表示

O1 和 O2 的合并方

式，有单位加

和拼接两种

操作。

因此 NASNet 的

搜索空间的

大小为：

(22

× 132 × 32

× 132 × 42

× 132 × 52

× 132 × 62

× 132 × 2)2

≈ 2.0 × 1034

PNASNet 的控

制器的运作

方式和 NASNet 的类

似，但也有几

点不同。

只有

普通单元：PNASNet 只

学习了普通

单元，是否进

行降采样由

用户自己设

置。当使用降

采样

时，它使

用和普通单

元完全相同

的架构，只是

要加入步长

2 并把特征图

的数量乘

2。这

种操作使控

制器的输出

节点数变为

B × 5。

更小的

 ：在观

察 NASNet 的实验结

果时，我们发

现有

5 个操作

是从未被使

用过的，因此

我

们将它们

从搜索空间

中删去，保留

的操作有 8

个

：

z 直接映射；

z

3 × 3 深

度可分离卷

积；

z 3 × 3

空洞卷积

；

z 3 ×

3 平均池化；

z 3

× 3 最

大池化；

z

5 × 5 深度

可分离卷积

；

z 7 × 7

深度可分离

卷积；

1　参见 Chenxi

Liu、Barret Zoph、Maxim Neumann 等

人的论文“Progressive

Neural Architecture Search”。

3.4

PNASNet

113

z 1

× 7 卷

积 +

7 × 1 卷积。

合并

 ：通过观察 NASNet 的

实验结果，作

者发现拼接

操作也从未

被使用，因此

我们也可以

将

这种操作

从搜索空间

中删掉。因此

PASNet 的超参数是

4 个值的集合

(I1,I2,O1,O2)。

因此

PNASNet 的搜索

空间的大小

是：

22 ×

82 × 32 ×

82 × 42 ×

82 × 52 ×

82 × 62 ×

82 ≈ 5.6 ×

1014

我们可以

编写一些规

则来对两个

隐层状态对

称的情况进

行去重，但即

使排除掉对

称的情况，

NASNet 的

搜索空间的

大小仍然为

1028

的数量级，PNASNet 的

搜索空间仍

然为 1012 的数量

级。

3.4.2 SMBO

尽管已经

将搜索空间

优化到了 1012

的

数量级，但是

这个规模依

然十分庞大

。在这个搜索

空间内

部进

行搜索依旧

非常耗时。为

了提高模型

的搜索效率

，这篇论文提

出了 SMBO，它在模

型的搜索空

间中进行优

化时会剪枝

一些分支从

而缩小模型

的搜索空间

，提升搜索速

度。换一个说

法，SMBO 的

搜索采

用递进（progressive）的形

式，它的网络

块数会从 1 个

开始逐渐增

加到 B

个。

当网

络块数 b =

1 时，它

的搜索空间

为 2

2

× 82

 =

256（不考虑对

称情况），也就

是可以生成

256 个不

同的网

络块 B1。这个搜

索空间并不

大，我们可以

枚举出所有

情况并训练

由它们组成

的网络

M1。记构

成网络的超

参数为 S1。接着

我们训练所

有的网络，得

到训练后的

模型 C1。通过验

证集我们可

以得到

每个

模型在验证

集上的精度

A1。有了网络超

参数

S1 和它们

对应的精度

A1，PNASNet 引入了代理

函

数

π 来建模

参数（特征）和

精度（标签）的

直接关系，这

样我们就可

以省去非常

耗时的模型

训练的

过程

了。由代理函

数得到的精

度叫作代理

精度，代理精

度并不非常

准确，因为在

剪枝时我们

不需要

得到

非常准确的

精度，代理精

度的作用是

快速地为我

们确定需要

剪枝的模型

。代理函数的

细节我们

会

在 3.4.3 节中详细

分析，这里你

只需要把它

看作从网络

超参数 S1

到它

对应的精度

A1 的映射即可

。

当网络块数

b =

2 时，它的搜索

空间为 22 ×

82 × 32 ×

82

 = 147

456，它的

实际意义是

在 b = 1

的基础

上

再扩展一个

网络块，表示

为 S2′。使用 b

= 1 时得

到的代理函

数 π

可以为每

个扩展模型

非常快速地

预测一个精

度，表示为 A2′。接

着我们会根

据代理精度

选取 top-K 个扩展

模型

S2，一般 K 的

值远小

于搜

索空间。仿照

之前的过程

，我们会依次

使用

S2 搭建 CNN C2，使

用

C2 得到模型

在验证集上

的

精度 A2，最后

我们使用得

到的

(S2,A2) 更新代

理函数 π。

仿照

之前的过程

，我们可以使

用

b ≥ 2 更新的代

理函数

π 得到

b + 1

的 top-K 的扩展结

构并得

到新

的代理函数

π。以此类推，直

到

b = B，如算法 1

和

图 3.18 所示。

算法

1

递进神经架

构搜索（PNAS）

输入

：B（块的最大数

量）、E（最大epoch数）、F（第

一层的卷积

核的数量）、

 K（搜

索宽度）、N（回滚

单元的次数

）、trainSet（训练集）、valSet（验证

集）

1: S1 = B1

// 单个网络

块中的候选

模型集合

 2:

M1 = cell-to-CNN(S1, N,

F) // 根

据特定的单

元构建神经

网络

3: C1 = train-CNN(M1,

E, trainSet) // 训练代

理CNN

4: π = fit(S1,

A1) // 从头训练

奖励预测模

型

5: for b =

2 : B do

6: Sb

′ =

expand-cell(Sb − 1) //

根据多个

网络块扩充

候选模型集

合

 7: =

predict(Sb

′. π) //

使用奖励

预测模型预

测准确率

 8: Sb

= top-K(SB′ , ,

K) // 根

据预测结果

选择K个模型

第 3

章  模型架

构搜索

114

9: Mb = cell-to-CNN(Sb,N,

F)

10: Cb =

train-CNN(Mb, E, trainSet)

11:

Ab = eval-CNN(Cb, valSet)

12: π = update-predictor(Sb,

Ab, π) // 使用

新数据微调

奖励预测模

型

13: end for

14:

return top-K(SB, AB, 1)

图 3.18 SMBO 流程（B

= 3）

3.4.3 代

理函数

3.4.2 节中

介绍 SMBO 时，代理

函数

π 在其中

发挥了至关

重要的作用

，从上面的流

程中我们知

道代理函数

必须有下面

3 条特征。

z

处理

变长数据：在

SMBO 中我们会使

用网络块数

为 b 的数据更

新模型并在

网络块数为

b

+ 1 的扩展模型

上预测精度

。

z

正相关：因为

代理精度 的

作用是选取

top-K 个扩展模型

，所以其预测

的精度不一

定准确；

但选

取的

top-K 个扩展

模型要尽可

能准确，所以

保证代理函

数预测的精

度至少和实

际精度

是正

相关的。

z

样本

有效：在 SMBO 中我

们用于训练

模型的样本

数量是 K，K

的值

一般会很小

，所以我

们希

望代理函数

在小数据集

上也能有好

的表现。

3.4　PNASNet

处理

变长数据的

一个非常经

典的模型便

是 RNN，因为它可

以将输入数

据按照网络

块切分成时

间

片。具体地

讲，LSTM的输入是

尺寸为4 ×b的超

参数Sb，其中4指

的是超参数

的4个元素(I1,I2,O1,O2)。

输

入 LSTM 之前，(I1,I2) 经过

独热（one-hot）编码后

会通过一个

共享的嵌入

层进行编码

，(O1,O2)

也会先

经过

独热编码再

通过另外一

个共享的嵌

入层进行编

码。最后的隐

层节点经过

一个激活函

数为 sigmoid

的全连

接得到最后

的预测精度

。损失函数使

用

L1 损失，即最

小化预测值

和目标值之

差的绝对值

。

作者采用了

一组 MLP

作为对

照试验，编码

方式是将每

个超参数转

换成一个 D 维

的特征向量

，4

个超参数拼

接之后会得

到一个

4 维的

特征向量。如

果网络块数

b ＞ 1，我们则取这

b

个特征向量

的

均值作为

输入，这样不

管有几个网

络块，MLP的输入

数据的维度

都是4。损失函

数同样使用

L1损失。

由于样

本数非常少

，作者使用的

是由 5

个模型

组成的集成

模型。为了验

证代理函数

在变长数据

上的表示能

力，作者在 LSTM 和

MLP 上做了一组

排序相关性

的对照试验

。分析出的结

论是在相同

网络块数下

，LSTM

优于 MLP，但是在

预测网络块

多一个的模

型上 MLP 要优于

LSTM，原因可能

是

LSTM 过拟合了。

3.4.4 PNASNet

的

实验结果

增

进式的结构

：根据 3.4.2 节介绍

的

SMBO 流程，PNASNet 可以

非常容易地

得出网络块

数小于

或等

于

B 的所有模

型，其结果如

图 3.19 所示。

图 3.19 PNASNet 得

出的

B = 1,2,3,4,5 的几个

网络单元，推

荐使用

B = 5

115

第 3 章

模型架构搜

索

116

迁移到 ImageNet ：NAS

中

提出学习数

据集敏感的

网络结构，但

是把 NASNet 和 PNASNet

在

CIFAR-10 上

学习到的网

络结构迁移

到 ImageNet

上也可以

取得非常好

的效果。作者

通过一组不

同网

络单元

在CIFAR-10和ImageNet上的实

验验证了在

CIFAR-10和ImageNet上的网络

结构的强相

关性，

实验结

果如图 3.20

所示

。

图 3.20　在

CIFAR-10 和 ImageNet 上的

网络结构的

强相关性

3.4.5 小

结

PNASNet 是继

NAS 和 NASNet 之

后的第三个

模型架构搜

索的算法，其

重点是强化

学习的搜索

空间的优化

，几个优化的

策略也是以

此为目的的

。更少的参数

是为了缩小

搜索空间，SMBO

是

为了

使用剪

枝策略来优

化强化学习

探索的区域

，而代理函数

则提供了比

随机采样更

有效的采样

策略。剪

枝策

略和代理函

数是强化学

习中最常见

的技巧，如 AlphaGo。在

AutoML

方向，如何优

化剪枝策略

和布局搜索

空间也是一

个非常重要

的方向。

3.5　AmoebaNet

在本

节中，先验知

识包括：

 NASNet（3.3 节）； 

残

差网络（1.4 节）。

在

讲解 AmoebaNet1

之前，先

给大家讲一

个故事：在一

个物质资源

非常匮乏的

外星球上居

住着

一种只

能进行无性

繁殖的外星

人，这个星球

上的资源匮

乏到只够养

活 P 个外星人

。然而外星人

为了

种族进

化还是要产

生新的后代

的，那么谁有

资格产生后

代呢？最优秀

的那个外星

人 A 提出：最优

秀的外星人

才有资格产

生后代。其他

外星人有意

见了，因为他

们担心整个

星球都是 A

家

族的人进

而

破坏了基因

多样性。于是

他们提出了

一个折中方

案：每次随机

抽 S 个候选者

参与竞争，里

面最优

秀的

才有资格产

生后代。如果

A 被抽中了，那

A 是里面最优

秀的，就让 A

产

生后代，如果

A 没有

1　参见

Esteban Real、Alok Aggarwal、Yanping Huang

等

人的论文“Regularized Evolution for Image

Classifier Architecture 

Search”。

3.5　AmoebaNet

被

抽中，也给其

他不是很优

秀的外星人

一个机会。这

样既保证了

优秀基因容

易产生更多

后代，也保

证

了星球上的

基因多样性

。接着，由于产

生了一个新

的外星人，但

是星球上的

资源有限，因

此必须

流放

一个外星人

，以给新的外

星人让位置

。那么如何决

定哪个外星

人要被流放

呢？ A 又提出：我

们流放最不

优秀的那个

吧！其他外星

人又不高兴

了，产生后代

的时候 A

的概

率最高，流放

的时候

又轮

不到 A 了，久而

久之这个星

球上又全是

A

的后代了。经

过商议，它们

提出了一个

最简单的方

法：流放岁数

最大的那个

。

故事讲完，我

们步入正题

。在我们之前

介绍的 NAS 系列

算法中，模型

架构的搜索

均是通过强

化学

习实现

的。本节要介

绍的 AmoebaNet 是通过

遗传算法的

进化策略实

现模型架构

的学习过程

的。该算法的

主要特点是

在进化过程

中引入了年

龄的概念，使

进化时更倾

向于选择更

为年轻的性

能好的结构

，这样确

保了

进化过程中

的多样性，具

有优胜劣汰

的特点，这个

过程叫作年

龄进化（AE）。作者

为他的网络

取名

AmoebaNet，Amoeba 译为变

形体，是对形

态不固定的

生物体的统

称，作者也借

这个词来表

达 AE 拥有探

索

更广的搜索

空间的能力

。AmoebaNet 取得了当时

在 ImageNet 数据集上

top-1

和 top-5 的最高准

确率。

3.5.1

搜索空

间

AmoebaNet 使用的是

和 NASNet

相同的搜

索空间。仿照

NASNet，AmoebaNet 也学习两个

单元：

普通单

元和缩减单

元。在这里，两

个单元是完

全独立的。然

后通过重复

堆叠普通单

元和缩减单

元，

我们可以

得到一个完

整的网络，如

图

3.21（a）所示。

在 AmoebaNet 中

，普通单元的

步长始终为

1，因此不会改

变特征图的

尺寸，缩减单

元的步长为

2，因此会将特

征图的尺寸

缩减为原来

的

1/2。因此我们

可以连续堆

叠更多的普

通单元以获

得更大的

模

型容量（不能

堆叠缩减单

元），如图3.21（a）中普

通单元右侧

的 × N符号所示

。在堆叠普通

单元时，

AmoebaNet 使用

了捷径的机

制，即普通单

元的一个输

入来自上一

层，另外一个

输入来自上

一层的上

一

层，如图 3.21（b）所示

。

图 3.21 AmoebaNet 的搜索空

间

117

第 3 章

模型

架构搜索

在

每个卷积操

作中，我们需

要学习两个

参数。

z 卷积操

作的类型：卷

积类型空间

参考

NASNet 部分。

z 卷

积核的输入

：从该单元中

所有可以选

择的特征图

中选择两个

，每个特征图

选择一个操

作，通过合并

这两个特征

图得到新的

特征图。最后

将所有没有

扇出（fan-out）的特征

图合

并作为

最终的输出

。

上面所说的

合并均是单

位加操作，因

此特征图的

个数不会改

变。举例说明

这个过程，根

据图

3.21 中的跳

跃连接，每个

单元有两个

输入，对应图

3.21（c）中的

0、1。那么第

一个操作（红

圈部分）

选择

0、1 作为输入进

行平均池化

和最大池化

操作以构成

新的特征图

2。接着第二个

操作可以从

（0、

1、2）中选择两个

作为输入，形

成特征图

3，依

此类推可以

得到特征图

4、5、6、7。

最终 AmoebaNet 骨干网

络仅仅有两

个变量需要

决定，一个是

每个特征图

的卷积核数

量

F，另

一个是

堆叠的普通

单元的个数

N，这两个参数

为人工设定

的超参数，作

者也实验了

N 和 F

的各种

组

合。

3.5.2 年龄进化

AmoebaNet

的进化算法

AE 如算法 2 所示

。

算法 2 AE

1: population←empty queue //

群体

 2: history←Ø

// 将

会包含所有

的模型

 3:

while |population| < P

do

 4: model.arch←RANDOMARCHITECTURE()

5: model.accuracy←TRAINANDEVAL(model.arch)

 6:

add model to right

of population

 7:

add model to history

8: end while

9: while |history| <

C do // 优化

C轮

10: sample←Ø 

11:

while |sample| < S

do

12: candidate←random element

from population // 在poplulation中的元

素

13: add candidate to

sample

14: end while

15: parent←highest-accuracy model in

sample

16: child.arch←MUTATE(parent.arch)

17:

child.accuracy←TRAINANDEVAL(child.arch)

18: add child

to right of population

19: add child to

history

20: remove dead

from left of population

// 最老的

21: discard

dead

22: end while

23: return highest-accuracy model

in history

118

3.5

AmoebaNet

119

在

介绍代码之

前，我们先看

3 个事实：

z 优秀

的对象更容

易留下后代

；

z 年轻的对象

比年老的对

象更受欢迎

；

z 无论多么优

秀的对象都

会有死去的

一天。

这 3

个事

实正是我从

上面的代码

中总结出来

的，也是用 AE 来

进化网络的

动机，现在我

们来看

看

AE 是

如何体现这

3 点的。

第

1 行代

码的作用是

使用 queue（队列）初

始化一个 population（种

族）变量。在

AE 中

每个变

量都

有一个固定

的生存周期

，这个生存周

期便是通过

队列来实现

的，因为队列

的“先进先出

”的特

征正好

符合

AE 的生命

周期的特征

。population 的作用是保

存当前的存

活模型，而只

有存活的模

型

才有产生

后代的能力

。

第 2 行的 history（历史

）用来保存所

有训练好的

模型。

第 3 行代

码的作用是

使用随机初

始化的方式

产生第一代

存活的模型

，个数正是循

环的终止条

件

P。P

的值在实

验中给出了

20、64、100 这 3 个，其中

P = 100 的

时候得到了

最优解。

while 循环

（第 4 ～

8 行）中先随

机初始化一

个网络，然后

训练并得到

模型在验证

集上的精

度

，最后将网络

的架构和精

度保存到 population

和

history 变量中。这里

所有的模型

评估都是在

CIFAR-10 上完成的。首

先注意保存

的是架构而

不是模型，保

存的变量的

内容不会很

多，因此并不

会占用特别

多的内存。其

次由于 population

是一

个队列，因此

需要从右侧

插入。而 history 变量

插入

时则没

有这个要求

。

第 9 ～ 22

行的第 2 个

while 循环表示的

是进化的时

长，即不停地

向

history 中添加产

生的优秀

模

型，直到history中模

型的数量达

到C个。C的值越

大就越有可

能进化出一

个性能更为

优秀的模型

。

我们也可以

选择在模型

开始收敛的

时候结束进

化。在作者的

实验中

C = 20 000。

第 10 行

的 sample

变量用于

从存活的样

本中随机选

取 S 个模型进

行竞争。

第

3 个

while 循环中的代

码（第 11

～ 14 行）用于

随机选择候

选父代。

第

15 ～ 16 行

代码从

S 个模

型中选择精

度最高的来

产生后代。这

个有权利产

生后代的模

型命名

为 parent。论

文实验中

S 设

定的值有 2、16、20、25、50，其

中效果最好

的值是 25。

第 17 行

代码使用变

异（mutation）操作产生

父代的后代

，变量名是 child。变

异的操作包

括卷

积操作

变异和隐层

状态变异，如

图 3.22 所示。在

每

次变异中，只

会进行一次

变异操作，抑

或是操

作变

异，抑或是输

入变异。

第 18 ～

19 行

代码依次训

练这个后代

网络结构

并

将它依次插

入 population

和 history 中。

第

20 ～ 21 行

代码从

population 顶端

移除最

老的

架构，这一行

也是 AE

最核心

的部分。另外

一种很多人

想到的策略

是移除效果

最差的那个

架

构，这个方

法在论文中

叫作非年龄

进化（non-aging 

evolution，NAE）。作者没

有选择

NAE 的原

因是如

果一

个模型效果

足够好，那么

他有很大概

率在被淘

汰

之前在

population 中留

下自己的后

代。如果按照

NAE 的思路淘汰

差样本的话

，population 中留下的

样

本很有可能

来自同一个

祖先，导致得

到的架构由

图 3.22 AmoebaNet 的变异操

作

第 3 章

模型

架构搜索

于

多样性非常

差，非常容易

出现局部最

优值问题。这

种情况在遗

传学中也有

一个名字：近

亲繁殖。而

AE 通

过移除保留

时间最长的

模型增加了

历史模型的

多样性。

最后

一行代码从

所有训练过

的模型中选

择最好的那

个作为最终

的输出。

再回

去看看开篇

的那个故事

，讲的就是 AE 算

法。

3.5.3 AmoebaNet 的网络结

构

通过上面

的进化策略

产生的网络

结构如图

3.23 所

示，作者将其

命名为 AmoebaNet-A ：

图 3.23 AmoebaNet-A 结

构

在图 3.23 中还

有两个要手

动设置的参

数，一个参数

是连续堆叠

的普通单元

的个数 N，另一

个

是卷积核

的数量。在第

一次缩减之

前卷积核的

数量是 F，后面

每经过一次

缩减，卷积核

的数量乘 2。

这

两个参数是

需要人工设

置的超参数

。

实验结果表

明，当 AmoebaNet 的参数

数量（N =

6，F = 190）达到了

NASNet 以及

PNASNet 的

量级

（超过 80

× 107

）时，AmoebaNet 和其

他两个网络

在

ImageNet 上的精度

是非常接近

的。虽然

AmoebaNet 得到

的网络和

NASNet 以

及 PNASNet 非常接近

，但是其基于

AE

的收敛速度

是要明显

快

于基于强化

学习（reinforcement learning，RL）的收敛

速度的。

而

AmoebaNet 的

参数数量达

到了 4.69 ×

108 时，AmoebaNet-A 取得

了目前在 ImageNet

上

最优的测

试

结果。但是不

知道是得益

于 AmoebaNet 的网络结

构还是其巨

大的参数数

量带来的模

型容量的巨

大

提升。

最后

作者通过一

些传统的进

化算法得到

了 AmoebaNet-B、AmoebaNet-C、AmoebaNet-D 这

3 个

模型

。由于它们的

效果并不如

AmoebaNet-A，因此这里不

过多介绍，感

兴趣的读者

可以去阅读

论

文的附录

D

部分。

从模型

的精度上来

看 AE、强化学习

得到的同等

量级参数的

架构在 ImageNet

上的

表现是几乎

相

同的，因此

我们无法贸

然地下结论

说 AE 得到的模

型要优于强

化学习。但是

AE

的收敛速度

快于强

化学

习是非常容

易从实验结

果中得到的

。另外作者添

加了一个随

机搜索（random search，RS）进行

对照实验，3 个

方法的收敛

曲线如图

3.24 所

示。

120

3.6

MnasNet

121

图 3.24

AE（Evolution）、强化学

习（RL）及 RS 的收敛

曲线

3.5.4

小结

这

篇论文在 NASNet 的

搜索空间的

基础上尝试

使用进化策

略来搜索网

络的架构，并

提出了一个

叫作

AE 的进化

策略。AE 可以看

作一个带有

正则项的进

化策略，它使

得训练过程

可以更加关

注网

络结构

而非模型参

数。无论是将

RL

和 NAE 对比还是

将 NAE

和 AE 对比，AE 在

收敛速度上

均有

明显的

优势。同时 AE 的

算法非常简

单，正如 3.5.2

节的

算法 2 所示的

它只有 P、C、S

这 3 个

参数，

对比之

下强化学习

需要构建一

个由

LSTM 构成的

控制器，AE 的超

参数明显少

了很多。

最后

，作者得到了

一个当时在

ImageNet上分类效果

最好的AmoebaNet-A，虽然

它的参数有

4.69

亿个。

3.6　MnasNet

在本节

中，先验知识

包括：

 MobileNet v2（2.2 节）；

 SENet（1.5 节）；



NAS（3.2 节

）。

在之前介绍

的 NAS

系列算法

中，我们都使

用了准确率

作为模型架

构搜索的唯

一指标，通过

这种方式搜

索出来的模

型往往拥有

分支复杂、并

行性很差而

且速度很慢

的网络结构

，这种网络结

构因为速度

的限制往往

很难应用到

移动端环境

。在一些搜索

轻量级模型

的算法中，它

们都选择了

FLOPS 作为评价指

标，但是 MnasNet（Mobile NASNet）1

中指

出这个指标

并不能真实

地反映移动

端

的推理速

度，因此 MnasNet 中提

出了将识别

准确率和实

际移动端的

推理延迟共

同作为优化

目标，

MnasNet 的搜索

空间则是直

接使用了 MobileNet v2

的

搜索空间，这

极大缩小了

搜索空间，提

升了搜索

效

率，得到了当

时在 ImageNet 上最高

的

top-1 的识别准

确率以及比

MobileNet v2 更快的推理

速度。

1　参见 Mingxing Tan、Bo

Chen、Ruoming Pang 等

人的论文“MnasNet: Platform-Aware

Neural Architecture Search for

Mobile”。

第

3 章

模型架构

搜索

122

在设计

模型架构搜

索算法时，有

3 个最重要的

点。

z 优化目标

：决定了搜索

出来的网络

结构

的性能

和效率。

z

搜索

空间：决定了

网络由哪些

基本模块

组

成，以及搜索

的效率如何

。

z 优化策略：决

定了强化学

习的收敛速

度。

本文将从

这 3 个角度出

发，来对 MnasNet

进

行

讲解，MnasNet 的强化

学习流程如

图 3.25

所示。

3.6.1 优化

目标

在上文

中我们介绍

了，MnasNet

是一个同

时侧重于推

理速度和推

理准确率的

模型，因此

MnasNet 同

时将准确率

ACC(m) 和推理延迟

LAT(m)

作为优化目

标。在实际应

用场景中优

化目标可

以

定义为：在满

足延迟小于

T 的前提下，最

大化搜索模

型的识别准

确率，表示为

式（3.7）。

（3.7）

MnasNet 的推理延

迟是在真实

硬件上测出

来的真实值

，作者使用的

设备是 Google 的

Pixel 手

机，

而传统的

方法是使用

FLOPS 作为性能评

价指标。作者

指出，实验结

果表明，受限

于嵌入式平

台复

杂的硬

件环境，FLOPS 并不

是一个准确

的性能评价

指标，例如 MobileNet 和

NASNet

的 FLOPS 非

常接近

（分别为

5.75 × 108 和

5.64 × 108

），但

是它们的速

度差异非常

大（分别为

113ms 和

183ms）。

对于式（3.7）这种

表示方式，训

练出来的模

型并不能提

供多个帕累

托最优解（Pareto

Optimal）。帕

累托最优解

在这里指的

是如果模型

能够在不降

低准确率的

情况下提升

推理速度或

者在

不降低

推理速度的

情况下提升

准确率，那么

它就是一个

帕累托最优

解。但显然以

式（3.5）作为优

化

目标的话并

不能最大化

提升推理速

度。

根据我们

的知识，有很

多方式可以

提供帕累托

最优解，如加

权损失等。MnasNet

使

用的是加权

乘积的方式

，这时候优化

目标定义为

式（3.8）：

（3.8）

其中 w

为权

值。w 表示为式

（3.9）：

（3.9）

那么如何设

置

α 和 β 呢？一个

比较直观的

方法是参考

式（3.7），当满足时

延时，便不再

优

化速度，当

速度大于约

束时延时，同

时优化速度

和准确率，作

者将这种约

束叫作硬约

束（hard 

constraint）。硬约束的

一组可用的

值是 α

= 0, β =

− 1。

根据作

者的经验，作

者发现了这

么一条规律

：如果一个模

型的推理速

度慢了 ，那么

它的识别

准

确率大约提

升 5%。根据这条

规律，快的模

型（M1）和慢的模

型（M2）的奖励也

可以满足这

个

平衡条件

，那么根据这

条规律得到

的奖励函数

可以表示为

式（3.10）。

（3.10）

求解式（3.10）得

到 β ≈ −

0.07，因此在这

里使用了 α = β

= − 0.07，作

者将这种约

束叫作软约

束

图

3.25 MnasNet 的强化

学习流程

3.6

MnasNet

（soft constraint）。使

用软约束的

优点是模型

可能同时学

习出速度快

但准确率略

低或者速度

慢但准确率

非常高的模

型，更有可能

的是速度和

准确率两个

值同时优化

。这两个约束

的函数曲线

如图 3.26

所示。

图

3.26 MnasNet 的硬约束（上

）和软约束（下

）的函数曲线

根据图

3.27 所示

，MnasNet 在两组约束

上的实验结

果我们可以

看出，硬约束

上搜索出的

模型更

倾向

于速度快的

模型，但是准

确率会略低

，而软约束上

搜索出来的

模型拥有更

广的时间范

围，并且

准确

率会更高。

图

3.27 MnasNet 在硬约束和

软约束上的

实验结果

123

第

3 章

模型架构

搜索

3.6.2 搜索空

间

在之前的

论文中，都是

通过搜索一

个网络块，然

后将网络块

堆叠成一个

网络的方式

来搭建网络

的。作者认为

这种方式并

不是一种合

适的方式，因

为在深度学

习中不同的

网络层有着

不同的功能

，

如果笼统地

给每一个网

络块都套用

相同的结构

的话，很难保

证这个网络

块作用到全

网络。之前之

所

以采用网

络块的方式

进行搜索，是

因为搜索空

间过于庞大

，基于全图的

搜索空间更

是基于网络

块的

搜索空

间的指数倍

，但是如果搜

索空间足够

小的话，就能

够基于全图

进行结构搜

索。

在

MnasNet 中，作者

提出了分解

层次搜索空

间（factorized hierarchical search

space）来进行搜

索空间的构

造，如图 3.28 所示

。分解层次搜

索空间是由

机器搜索结

合人工搭建

共同构建神

经网络

的。具

体地讲，它将

一个网络分

成若干个网

络块，每一个

网络块又由

若干个相同

的层组成，然

后为

每一个

层单独进行

结构搜索。如

果块与块之

间需要进行

降采样的话

，则第一层的

步长为 2，其他

层

的步长为

1。另外在每一

层中都有一

个单位连接

。

图

3.28 MnasNet 的分解层

次搜索空间

这么做的好

处有两点：

z

不

同层次的网

络块对速度

的影响不同

，例如接近输

入层的网络

的输入特征

图的分辨率

越

大，对运行

速度的影响

也越大；

z 不同

的网络层有

了不同的结

构后，对准确

率的提升也

有帮助。

MnasNet 能对

每一层进行

独立搜索的

一个重要原

因是它使用

了更小的搜

索范围，它不

是从大量

的

基本操作中

进行搜索，而

是使用了从

MobileNet v2

拆分出的操

作单元，如下

。

z 卷积操作，包

括常规卷积

（Conv）、深度卷积（DWConv）和

MobileNet v2

中提出的可

逆

瓶颈卷积

（mobile inverted bottleneck

Conv，MBConv）。

z 卷积核的尺

寸：3 ×

3 和 5 ×

5。

z SENet 的压缩

-

激发（Squeeze-and-Excitation）模块，以

及对应的 SERatio ：0 和

0.25。

z 跳跃相关操

作：池化、单位

连接和无跳

跃连接。

z 输出

通道数：Fi

。

z 每个

网络块的层

数：Ni

。

对于每个

网络块的层

数 Ni

，MnasNet 使用的是

相对于

MobileNet v2 的值

，搜索空间是

{0, +

1,

−1}；对于输出通

道数，它使用

的也是相对

于 MobileNet v2

的值，搜索

空间为 {0.75,1.0,1.25}。可以

看出MnasNet在搜索

空间上极大

参考了MobileNet v2，因此

得到的模型

也和MobileNet v2非常相

似，

如图 3.29 所示

。从这个结果

可以看出，人

工设计的模

型其实是非

常优秀的。

124

3.6　MnasNet

图

3.29 MnasNet

的网络结构

在图 3.29 中，SepConv 表示

深度可分离

卷积，由

1 × 1 深度

卷积核组成

；MBConv

表示可逆瓶

颈卷积。和之

前我们得到

的经验不同

的是，MnasNet 搜索到

了 5 ×

5 卷积，而实

验结果也表

明这种

5 ×

5 的卷

积核使模型

在速度和精

度之间得到

了很好的平

衡。

3.6.3 优化策略

MnasNet

使用的是和

NASNet 相同的搜索

算法，即将要

搜索的网络

结构表示成

一个由若干

个标志（token）组成

的列表，表示

为 α1:T，而这个列

表可以通过

一个序列模

型来生成。假

设这

个序列

模型的参数

是

θ，那么强化

学习的目标

可以表示为

最大化生成

模型的奖励

R(m)，表示为

125

第 3

章

模型架构搜

索

126

式（3.11）：

（3.11）

其中，m 是

由标志序列

得到的模型

。

首先通过一

个基于

RNN 的控

制器生成一

个模型，然后

训练这个模

型，得到它在

验证集上的

准

确率 ACC(m)，再通

过移动设备

得到这个模

型的推理时

延

LAT(m)，最后使用

最近策略优

化算法，

以 ACC(m) 和

LAT(m)

共同作为奖

励（式（3.8））优化控

制器的参数

θ。

3.6.4 小结

当我们

设计一个要

考虑速度的

网络时，将速

度相关指标

加入强化学

习的奖励中

是非常自然

的想

法，MnasNet 抛弃

了常用的 FLOPS 而

使用了直接

的在移动设

备上的推理

延迟。但是在

MnasNet

的

论文中只

使用了一种

品牌的手机

，是否其网络

的设计过于

拟合了这个

型号的手机

的性能？是否

会因

为手机

硬件的不同

而在其他型

号的手机上

表现不好？

MnasNet

的

搜索空间很

大程度上参

考了 MobileNet v2，其通过

强化学习的

方式得到了

超越其他

模

型架构搜索

算法和其参

照的

MobileNet v2 算法的

搜索空间，从

中可以看出

人工设计和

强化学习互

相配合应该

是一个更好

的发展方向

。MnasNet 中提出的层

次搜索空间

可以生成每

个网络块都

不同的

网络

结构，这对提

升网络的表

现也是有很

大帮助的，但

这点也得益

于参考 MobileNet v2 设计

搜索空

间后

大幅降低了

搜索难度。

3.7　MobileNet v3

在

本节中，先验

知识包括：

 MobileNet（2.2 节

）；

 MnasNet（3.6 节）；



SENet（1.5 节）。

MobileNet v31

是由 Google 大

脑提出的，它

的前身是 MnasNet，是

通过在

MnasNet 基础

上的进

一步

优化实现的

。MobileNet v3

的设计初衷

和 MnasNet 的相同：通

过 AutoML

技术搜索

出一个既快

又准的网络

结构。MobileNet v3 的主要

贡献有 4

点：

z 使

用平台相关

神经架构搜

索（platform-aware NAS）得到网络

的初始结构

；

z 使用 NetAdapt2 对网络

的部分层进

行局部优化

；

z 对网络中耗

时的结构进

行重新设计

；

z 设计了一组

运行速度更

快的激活函

数。

另外，作者

基于 MobileNet v3 和

R-ASPP 设计

了一个又快

又准的场景

分割网络 LR-ASPP。

MobileNet

v3 使

用的是块级

别搜索（block-wise search）和层

级别搜索（layer-wise search）相

结

合的模型

架构搜索方

案。所谓块级

别搜索是指

通过强化学

习搜索这个

块内具体的

结构细节，而

层级

别搜索

是指固定一

个网络的整

体结构，然后

单独对网络

的一层进行

学习。因此在

介绍这两个

搜索策

略之

前，我们首先

需要知道 MobileNet

v3 参

考了哪些网

络结构。

1　参见

Andrew

Howard、Mark Sandler、Grace Chu 等人的论文

“Searching

for MobileNetV3”。

2　参见

Tien-Ju Yang、Andrew Howard、Bo Chen

等人的

论文“NetAdapt: Platform-Aware Neural Network

Adaptation for Mobile

Applications”。

3.7　MobileNet v3

3.7.1 参考结

构

MobileNet v3

参考的网

络结构有 3 个

，它们分别是

MobileNet v1、MobileNet

v2 和 MnasNet。

MobileNet

v1 提出的模

块是深度可

分离卷积，深

度可分离卷

积由以通道

为单位的 3 ×

3 卷

积（深度

卷积

）和跨通道的

1 ×

1 卷积（点卷积

）组成。MobileNet v2 提出的

模块是线性

瓶颈的逆残

差结构

（Inverted Residual with Linear

Bottleneck），如图

2.12（b）所示。MnasNet 则在 MobileNet v2

的

基

础上加入

了 SENet 中提出的

压缩

- 激发模

块。SENet 和 MnasNet

的残差

结构使用了

不同的结合

方式，

其中SENet将

压缩-激发模

块放在了残

差模块之后

，而MnasNet则将这个

模块放在了

残差模块内

部，

如图 3.30

所示

。

图 3.30 MobileNet

v3 的结构，它

将压缩 - 激发

模块放在了

残差模块内

部

3.7.2 网络搜索

如之前我们

所介绍的，MobileNet v3 的

搜索分成两

步，先通过块

级别搜索得

到网络的整

体架

构，再通

过层级别搜

索对网络层

进行微调。这

两步分别在

全局和局部

的搜索上起

到了互补的

作用，

是 MobileNet v3

论文

的一大创新

点。

1．平台相关

的块级别搜

索

这里的标

题可以分成

3 部分来理解

。

（1）网络结构搜

索：即采用了

AutoML 的方式进行

结构设计，包

括由 RNN 组成的

控制器和搜

索空间，MobileNet

v3 采用

了和 MnasNet 相同的

结构，而且使

用了

MnasNet-A1 作为初

始结构。

（2）平台

相关的：平台

相关的核心

思想在于在

特定平台下

训练一个速

度和精度平

衡的模型。在

MnasNet 中我们就介

绍了一个多

目标的奖励

函数

ACC(m) × [LAT(m)/TAR]w 来近似

帕累托最优

解。

作者观察

到小模型的

精度会随着

延时的变大

先增加，因此

使用了一个

更小的权重

因子 w = −

0.15

（MnasNet 中这个

值是 −

0.07）来补偿

不同延时下

更大的精度

变化。

（3）块级别

搜索：与之对

应的是层级

别搜索。块级

别搜索相当

于全局搜索

，通过这种方

式得

到了一

个种子模型

。然后使用层

级别搜索对

这个种子模

型进行微调

，最终得到的

是 MobileNet

v3-

Small。

2．层级别搜

索：NetAdapt

MobileNet v3 的层级别

搜索使用的

是 NetAdapt

提出的策

略，NetAdapt 是一个局

部优化方法

，

局部优化是

指它可以只

对一个网络

的部分层进

行优化。NetAdapt 的优

化流程如图

3.31

所示。首先

我

们需要一个

种子网络，它

已经在上文

中得到了，然

后不停迭代

直到得到的

最终网络的

性能经验度

量（empirical measurement）满足我们

的性能预算

要求。在这个

迭代中我们

会在约束范

围内不停地

127

第

3 章  模型架

构搜索

128

随机

生成新的网

络并对它们

的准确率和

运行时间进

行评估。

图 3.31

NetAdapt 的

优化流程

NetAdapt 的

优化目标是

最大化准确

率，而

MobileNet v3 的目标

是最大化准

确率的改变

值和延

时改

变值的比值

：

。其中 ΔLAT 要满足

算法 3

中的目

标延时。如算

法 3 所展示的

，MobileNet

v3 支持两种类

型的候选（proposal）：

z 改

变扩展层的

大小，即使用

1

× 1 卷积增加通

道数；

z

改变一

个块中瓶颈

层（残差连接

的最后一个

模块）的大小

，因为我们要

使用残差进

行特征

的单

位加。

算法 3

NetAdapt 算

法

输入：种子

网络Net0

目标延

时Bud

 资源衰减

计划ΔRi

输出：满

足目标资源

需求的网络



1: i = 0

2: Resi

 =

TakeEmpiricalMeasurement(Neti

)

 3:

while Resi

 >

Bud do

 4:

Con = Resi

− ΔRi

; //

收紧运算资

源

 5: for

k from 1 to

K do

 6:

// 减小扩张

层的大小

 7:

// 减

小瓶颈层的

大小

 8:

// 模型初

始化并微调

模型T步，得到

Netk、Resk

 9:

end for

3.7　MobileNet

v3

129

10: Neti+1,

Resi+1←PickHighestAccuracy(Netk, Resk)

11: i

= i + 1

12: end while

13:

// 对网络Neti进行

训练，得到

14: return

算

法 3 中，ΔRi,j 表示资

源衰减计划

（resource

reduction schedule），是一个类似

于学习率衰

减计

划（learning rate

reduction schedule）的变

量，表示第 i 次

迭代中第

j 个

资源约束收

紧了多少，并

且

随着迭代

的进行，它的

值也会变化

。TakeEmpiricalMeasurement() 是用来评估

网络准确率

和速度等

指

标的函数。

3.7.3 人

工设计

除使

用

NAS 得到网络

之外，MobileNet v3 还可对

得到的网络

进行修改，它

的修改有两

点：

z 对计算耗

时的模块进

行重新设计

；

z 设计更快的

激活函数

h-swish。

1．重

新设计耗时

模块

MobileNet v3

的第一

个修改是对

输出模块的

重新设计，它

的改动如图

3.32 所示。图 3.32（a）

所示

的是原始的

输出模块，其

作用是高效

地生成输出

层，它的维度

是

1 × 1 ×

C，其中 C 是类

别数，

图

3.32 中 C 的

值是

1 000。在 MobileNet v2

中，这

一部分的输

入特征图的

尺寸是 7 × 7，因为

它的

全局平

均池化的操

作的位置比

较靠后，造成

了很多改变

特征数的卷

积操作都是

在 7 × 7

的特征图

上

进行的。作

者认为如果

仅仅为了得

到输出层，那

完全可以把

全局平均池

化操作提前

，然后后续的

卷

积等操作

在 1

× 1 的特征图

上运行即可

。基于这个思

想，MobileNet v3

提出了图

3.32（b）所示的输

出

模块，对比原

始输出模块

，新的输出模

块不仅速度

大幅提升，而

且没有降低

准确率。

图 3.32

原

始耗时的输

出模块和 MobileNet v3 重

新设计的输

出模块

MobileNet v3 中可

以手动调整

输入层之后

的卷积核的

数量。通过 3.7.2

节

中的方案搜

索出来的

网

络结构在输

入图像之后

的特征图通

道数是 32，我们

知道在 CNN

中，图

像之后的卷

积操作往往

可

以看作一

些特定形状

的滤波器。作

者发现这一

部分卷积核

学习好之后

含有大量的

彼此的镜像

，因此

将通道

数降到了 16。

3.7　MobileNet v3

131

结

合 swish 和式（3.13），MobileNet v3

推出

了它的激活

函数 h-swish，如式（3.14）所

示。h-swish

比式（3.13）有更

高的精度且

不会像 sigmoid

等激

活函数有饱

和的问题。更

重要的是 h-swish 的

运算

速度要

明显快于

swish 和

sigmoid。它的函数曲

线如图 3.35 所示

。

（3.14）

图 3.35 h-swish

的函数曲

线

对比 ReLU6，h-swish 的速

度要略慢但

是拥有更高

的精度。考虑

到模型的速

度与准确率

的平衡，

在 MobileNet v3 中

作者在前半

部分图像尺

寸较大的时

候使用了速

度更快的

ReLU（不

是 ReLU6），

而在后半

部分使用了

准确率更高

的 h-swish。

3.7.4 修改 SE 块

在

SENet 中，压缩 - 激发

模块的通道

数是卷积通

道数的

1/16，MobileNet v3 改成

了 1/4。

通过上面

的方式，最终

得到的 MobileNet v3-Large 和

MobileNet v3-Small 的

结构分别如

表 3.1

和

表 3.2 所示

。其中，SE

表示是

否使用压缩

- 激发操作；激

活函数中 RE 表

示

ReLU，HS 表示 h-swish；

运算

中

NBN 表示不使

用 BN 操作。

表 3.1　MobileNet v3-Large

的

结构

输入 运

算 扩展层大

小

输出通道

数 SE 激活函数

步长

2242

× 3 conv2d —

16 — HS 2

1122 × 16 bneck,

3 × 3 16

16 — RE 1

1122 × 16 bneck,

3 × 3 64

24 — RE 2

562 × 24 bneck,

3 × 3 72

24 — RE 1

562 × 24 bneck,

5 × 5 72

40 √ RE 2

282 × 40 bneck,

5 × 5 120

40 √ RE 1

282 × 40 bneck,

5 × 5 120

40 √ RE 1

282 × 40 bneck,

3 × 3 240

80 — HS 2

142 × 80 bneck,

3 × 3 200

80 — HS 1

142 × 80 bneck,

3 × 3 184

80 — HS 1

142 × 80 bneck,

3 × 3 184

80 — HS 1

142 × 80 bneck,

3 × 3 480

112 √ HS 1

第 3 章

模

型架构搜索

130

2．h-swish 激活函数

MobileNet

v3 的

第二个修改

是提出了 h-swish 激

活函数，那么

它的动机是

什么呢？为了

清楚地

解答

这个问题，我

们通过纵观

激活函数的

发展史来拨

开迷雾。sigmoid 和 tanh 是

最早被使用

的激活

函数

之一，它们的

问题是存在

“饱和”现象。为

了解决这个

问题，AlexNet 中选择

使用了 ReLU，

这就

产生了两个

分支，它们分

别是基于

sigmoid 和

ReLU 进行优化。

sigmoid

的

一个优化方

案是 swish，它定义

为：swish(x) = x·sigmoid(βx)。其中

β 是一

个可以学

习

的参数，它的

函数曲线如

图 3.33

所示。但是

在大多数情

况下 β 并不需

要学习，而是

被设置为 1，

此

时的 swish 叫作“swish-1”，它

的表达形式

为 xσ(x)。swish-1

还有一个

名字叫作 SiLU，SiLU 最

开

始在

GELU1 中提

出。

图 3.33

swish 激活函

数的函数曲

线

ReLU 的改进有

很多，如比较

著名的

Leaky ReLU、PReLU、ReLU6 等。我

们这里介绍

和

h-swish

相关的 ReLU6。ReLU6 的

激活函数如

式（3.12）所示，它的

思想很简单

，就是对 ReLU

得

到

的结果进行

最大值为 6 的

截断。通过式

（3.13）的变化可以

将

ReLU6 的函数曲

线变成以 (0,0.5)

为

中心、值域为

[0,1]

的曲线，如图

3.34 所示。

y =

min[max(x,0),6] （3.12）

（3.13）

图

3.34　式（3.13）的

函数曲线

1　参

见

Dan Hendrycks、Kevin Gimpel 的论文“Gaussian

Error Linear Units (GELUs)”。

第

3 章  模型架构

搜索

132

输入 运

算 扩展层大

小

输出通道

数 SE 激活函数

步长

142

× 112 bneck, 3

× 3 672 112

√ HS 1

142

× 112 bneck, 5

× 5 672 160

√ HS 2

72

× 160 bneck, 5

× 5 960 160

√ HS 1

72

× 160 bneck, 5

× 5 960 160

√ HS 1

72

× 160 conv2d, 1

× 1 — 960

— HS 1

72

× 960 pool, 7

× 7 — —

— — 1

72

× 960 conv2d, 1

× 1, NBN —

1 280 — HS

1

12 × 1

280 conv2d, 1 ×

1, NBN — k

— — 1

表

3.2　MobileNet v3-Small 的结

构

输入 运算

扩展层大小

输出通道数

SE 激活函数 步

长

2242 × 3 conv2d,

3 × 3 —

16 — HS 2

1122 × 16 bneck,

3 × 3 16

16 √ RE 2

562 × 16 bneck,

3 × 3 72

24 — RE 2

282 × 24 bneck,

3 × 3 88

24 — RE 1

282 × 24 bneck,

5 × 5 96

40 √ HS 2

142 × 40 bneck,

5 × 5 240

40 √ HS 1

142 × 40 bneck,

5 × 5 240

40 √ HS 1

142 × 40 bneck,

5 × 5 120

48 √ HS 1

142 × 48 bneck,

5 × 5 144

48 √ HS 1

142 × 48 bneck,

5 × 5 288

96 √ HS 2

72 × 96 bneck,

5 × 5 576

96 √ HS 1

72 × 96 bneck,

5 × 5 576

96 √ HS 1

72 × 96 conv2d,

1 × 1 —

576 √ HS 1

72 × 576 pool,

7 × 7 —

— — — 1

12 × 576 conv2d,

1 × 1, NBN

— 1 024 —

HS 1

12 ×

1 024 conv2d, 1

× 1, NBN —

k — — 1

3.7.5 Lite R-ASPP

基于

MobileNet v3 的结

构，作者又设

计了用于分

割网络的 Lite

R-ASPP（LR-ASPP），如

图 3.36

所示。其中

，R-ASPP 是

DeepLab v21 中提出的

空洞空间金

字塔池化（atrous spatial

pyramid pooling，

ASPP）的

简化设计，R-ASPP 仅

由

1 × 1 卷积核、全

局平均池化

组成，而

LR-ASPP 则是

比 R-ASPP

更快的网

络。LR-ASPP

使用了一

个大型卷积

核、大步长的

全局平均池

化和一个 1 × 1

卷

积。作者在

MobileNet v3 的

最后一个块

后加上空洞

卷积来提取

密集特征，然

后添加了一

个低层级的

特征来捕获

细节信息。

1　参

见 Liang-ChiehChen、George Papandreou、Iasonas

Kokkinos 等人的论

文“DeepLab: Semantic Image

Segmentation with

 Deep

Convolutional Nets, Atrous Convolution,

and Fully Connected CRFs”。

续表　　

3.8

EfficientNet v1

133

图

3.36 LR-ASPP 的

网络结构

3.7.6

小

结

MobileNet v3 是一个结

合了

NAS 技术和

人工设计的

网络结构。在

搜索时，它使

用了先全局

搜

索的平台

相关 NAS，接着使

用了对局部

进行优化的

NetAdapt

进行微调。在

人工设计时

，作者手动

对

耗时部分进

行调整，并设

计了新的激

活函数 h-swish。从技

术角度讲，本

文是一篇创

新点很多但

内容有些零

散的论文，这

种分阶段 NAS

加

上人工设计

的思想也有

一定的参考

价值。

3.8　EfficientNet v1

在本节

中，先验知识

包括：

 MobileNet（2.2 节）；

 MnasNet（3.6 节）；



残

差网络（1.4 节）。

当

我们训练一

个模型时，在

硬件资源固

定的情况下

，我们可以先

选取一个基

线模型，然后

在它的

基础

上通过增大

网络的深度

、宽度或者输

入图像的分

辨率这

3 个参

数的值来提

升这个网络

的泛化能力

或者通过减

小这 3 个参数

的值来使其

能够在硬件

资源有限的

平台上运行

。现有很多方

法都是单一

地改

变某一

个参数，但是

EfficientNet v11 指出这 3

个参

数其实是相

互影响的，因

此有必要重

新设计一个

模

型缩放的

标准来合理

地对这 3 个参

数进行统一

调整，最后通

过模型架构

搜索的策略

来学习这

3 个

参数。

EfficientNet v1

得到了

当时在 ImageNet 上最

高的准确率

，以及比类似

准确率的模

型更快的速

度。

3.8.1

背景知识

1．模型缩放

模

型缩放（model scaling）是指

希望通过一

个基线模型

（如图 3.37（a））衍生出

多个模型，然

后根据不同

的硬件环境

选择不同的

模型。常见的

模型缩放策

略有

3 个，如图

3.37（b）、图 3.37（c）、

图

3.37（d）所示：

z 缩

放网络宽度

；

1

参见 Mingxing Tan、Quoc V.Le

的论文

“Efficientnet: Rethinking Model Scaling

for Convolutional Neural Networks”。

第 3 章

模型架

构搜索

z 缩放

网络深度；

z

缩

放输入图像

的分辨率。

图

3.37　网络缩放的

3 个策略与复

合缩放

网络

宽度：模型缩

放的一个策

略是改变网

络的宽度，改

变网络的宽

度意味着模

型拥有更多

的卷

积核来

提取不同种

类的特征。但

是有实验结

果表明 CNN 中的

卷积核存在

大量的冗余

，因此网络的

性能随着宽

度的增加将

很快陷入瓶

颈。

网络深度

：改变网络深

度是常见的

一个缩放策

略，因为更深

的网络将意

味着模型拥

有更强的表

征能力。但是

随着网络深

度的增加，可

能会产生梯

度消失、模型

退化等问题

。虽然残差网

络等算法

解

决了这个问

题，但是模型

的收益会随

着深度的增

加变得越来

越平缓。

图像

分辨率：最后

一个模型缩

放的策略是

改变输入图

像的分辨率

，但是实验结

果表明网络

性能

的收益

随着分辨率

的增加将很

快消失。我们

放大输入图

像的一个目

的是获取图

像中更多的

纹理信

息，但

如果卷积核

不够的话，这

些纹理信息

也是无法捕

获的。

综上，我

们可以得出

，单纯在一个

维度上进行

模型缩放，模

型的收益很

快会陷入瓶

颈，如图

3.38 所示

。

图 3.38　模型收益

与 3

个缩放指

标的关系，从

左到右依次

是宽度、深度

和输入图像

的分辨率

134

3.8　EfficientNet

v1

135

2．复

合缩放

实际

上，宽度、深度

和输入图像

的分辨率这

3

个指标是互

相关联的，原

因如下。

z 更大

的输入尺寸

意味着深层

部分的卷积

操作需要拥

有更大的感

受野，这样模

型分类才能

拥

有全局视

野。在不使用

空洞卷积的

前提下，往往

只能通过增

加网络深度

来增大深层

网络的

感受

野。

z 更高的分

辨率意味着

图像拥有更

多的纹理信

息，而更多的

纹理信息意

味着需要更

多的通道

来

学习，也就意

味着增大输

入图像分辨

率的时候也

需要增加网

络的宽度。

基

于上面的分

析，我们发现

单纯地缩放

一个指标从

理论上来讲

是有问题的

，一种更合理

的方法

是 3 个

指标一起调

整。那么怎么

平衡

3 个指标

的调整幅度

呢，这就是 EfficientNet v1

所

做的工作。

3.8.2 EfficientNet v1

详

解

1．问题建模

一个神经网

络的一层可

以定义为式

（3.15），其中 i 表示卷

积运算，Xi

是输

入向量，它的

维度

是（Hi

,Wi

,Ci

），Yi 是输

出向量，表示

为式（3.15）：

（3.15）

一个神

经网络

N 是由

若干个网络

层组成的，根

据 VGG 中的网络

块的形式，每

个块的网络

结

构是完全

相同的，在块

与块之间会

进行降采样

以及通道数

的增减，那么

一个 CNN 可以定

义为式

（3.16）：

（3.16）

其中

， 表示在第 i

个

阶段中，网络

层 i 会重复 Li

次

，如图 3.37（a）所示。其

输入图像的

维度是

(224,224,3)，经过

一系列卷积

操作后维度

变成了 (7,7,512)。

根据

模型缩放的

概念，模型缩

放就是对式

（3.16）中的 Li

（深度）、Ci

（宽

度）以及

Hi 和 Wi

（输

入图像的分

辨率）进行改

变，假设这

3 个

维度的缩放

参数分别为

d、w、r，那么符合缩

放要求的建

模可以定义

为通过调整

d、w、r 这 3

个值，在固

定计算资源

（如显存、速度

要求等）的前

提下最大

化

模型的识别

准确率，因此

有了式（3.17）的定

义。

（3.17）

2．复合模型

缩放

这篇论

文的核心便

是复合缩放

方法（compound scaling method），它使用

一个系数φ 共

同调整

d、

w、r 这 3

个

值，具体表示

为式（3.18）：

（3.18）

其中，(α,β,γ) 是

通过网格搜

索（grid

search）得到的一

组参数，φ 是由

用户根据自

己的条件设

置的

一个超

参数。因为考

虑到提升网

络的深度是

最有效的策

略，所以将其

他两个维度

的缩放进行

了平

第

3 章  模

型架构搜索

136

方操作，使得

模型更倾向

于提升网络

的深度。当一

个网络缩放

完毕后，它的

FLOPS 提升的倍数

为

，约束 则将

FLOPS

的最大值控

制到了 2

φ 。

3．EfficientNet v1 的网

络结构

在进

行

NAS 时，作者参

考了当时最

优的 MnasNet。首先它

使用了和 MnasNet

相

同的搜索空

间，

不同的是

EfficientNet 并没有使用

MnasNet 中的时间延

迟作为优化

目标，而是使

用了

FLOPS，表示

为

式（3.19）：

 ACC(m)

× [FLOPS(m)/T]

w （3.19）

其中，w 是用

来平衡准确

率和 FLOPS 的一个

超参数。

因为

使用了相同

的搜索空间

，EfficientNet v1 得到的网络

结构也和 MnasNet

非

常相似，作者

将

其命名为

EfficientNet-B0，如图 3.39 所示。

图

3.39 EfficientNet-B0

EfficientNet-B0 的核心结构

是在

MnasNet 和 MobileNet v2

中使

用的可逆瓶

颈卷积层（MBConv），

另

外为了提升

精度，作者向

其中添加了

SENet 中的压缩 -

激

发结构。

在 EfficientNet-B0 的

基础上，作者

通过下面两

步得到了

EfficientNet-B0 到

EfficientNet-B7：

（1）固定φ =

1，基于式

（3.17）和式（3.18），我们对

α、β、γ 进行网格搜

索。根据作者

的实验，

得出

最好的值为

α =

1.2、β = 1.1、γ =

1.15；

（2）固定 α、β、γ，使用φ 对

模型进行缩

放，得到了

EfficientNet-B0 到

EfficientNet-B7，它们

的准确

率如图 3.40

所示

。

图 3.40 EfficientNet-B0

到 EfficientNet-B7 的准确

率、参数数量

和 FLOPS

图3.40中，EfficientNet-B7达到

了最高的准

确率，但是相

比同性能的

Gpipe1

，EfficientNet-B7参

1　参见

Yanping Huang、Youlong Cheng、Ankur Bapna

等人

的论文“GPipe: Efficient Training of

Giant Neural Networks using

Pipeline Parallelism”。

3.9　EfficientNet

v2

137

数数

量减小到 1/8.4，速

度增加到

6.1 倍

。

在进行以 EfficientNet-B0

为

基础的缩放

时，为了得到

更快的训练

和预测速度

，我们通常需

要将

通道数

调整为 8 的整

数倍，因为目

前主流的显

卡的底层设

计都是采用

二分法对通

道进行分配

计算

的，8 的整

数倍个通道

能提高显卡

的并行效率

1

。

4．复合缩放应

用到其他网

络

作者同样

将复合缩放

应用到现有

的一些比较

流行的网络

中，如 MobileNet 和残差

网络，实验结

果表明应用

复合缩放后

，模型的准确

率均有了一

定程度的提

升。

3.8.3

小结

网络

的宽度、深度

、输入图像分

辨率是“调参

侠”最常调整

的 3 个超参数

，EfficientNet

v1 从

这 3

个参数

入手，为这 3 个

参数的调整

提供了非常

好的指导方

向，即先确定

α、β、γ，然后根据硬

件调整φ。在 EfficientNet

v1 中

，α、β、γ 这 3

个参数的

搜索使用了

网格搜索，有

没有更快的

类似于

强化

学习中启发

式搜索的参

数搜索方法

呢？期待后续

工作给出答

案。另外 EfficientNet-v1 算法

的创

新性略

显不足，并没

有像 NAS 系列的

网络那样给

出自己的搜

索空间等，但

是该算法的

准确率很高

。

3.9

EfficientNet v2

在本节中，先

验知识包括

：



SENet（1.5 节）；  残差网络

（1.4

节）；

 MobileNet（2.2 节）；

 EfficientNet（3.8 节）；



Dropout（6.1 节）。

Swin Transformer

为

Transformer 阵营夺下 ImageNet 的

top-1

准确率的阵

地（86.4%）不久，以

Quoc V. Le

为

首的 CNN 阵营又

通过“大杀器

”AutoML 再次抢占了

这个阵地（87.3%），而

得到这

个 top-1 准

确率的模型

便是我们这

里要介绍的

EfficientNet v22

。

那么 EfficientNet v2

是如何

做到的呢？对

比其他 AutoML 算法

，EfficientNet v2

深入探索了

输入

图像尺

寸和模型的

正则尺度的

关系，并提出

了递增式的

AutoML 算法，即通过

逐渐增加输

入图像的

尺

寸并不断调

整与之匹配

的模型的正

则尺度来进

行网络的构

建。EfficientNet

v2 的另外一

个贡献是

把

训练速度作

为优化目标

之一。通过对

近年 CNN

的若干

算法的总结

，作者发现了

影响训练速

度的

几个重

要因素，并以

这些因素作

为出发点，对

模型的搜索

空间进行了

约束。

3.9.1 算法动

机

1．训练速度

在训练网络

时，作者发现

了几个影响

训练速度的

因素，它们分

别是：

z 使用大

的图像作为

输入会使训

练变慢；

z

在网

络的浅层中

，深度可分离

卷积会比普

通卷积要慢

；

1　参见 Jiahui

Yu、Thomas Huang 的论文

“Universally Slimmable

Networks and Improved Training

Techniques”。

2　参见 Mingxing

Tan、Quoc V.Le 的论文

“EfficientNetV2: Smaller

Models and Faster Training”。

第 3 章

模型架

构搜索

138

z 像

EfficientNet v1 中

使用相同的

尺度对模型

的每个阶段

进行缩放并

不是最优选

择。

第一个因

素比较容易

理解。在我们

的训练环境

（显存容量）固

定时，大的训

练图像意味

着只能

使用

更小的批次

大小，因此训

练速度会变

慢。

第二个因

素和我们之

前对于深度

可分离卷积

的认知相冲

突。在我们的

认知中，深度

可分离卷积

一直是比普

通卷积速度

要快、参数数

量要少的一

个操作。出现

这个冲突的

原因在于现

在的一些加

速

设备或者

移动设备在

普通卷积上

拥有更好的

优化，但是对

深度可分离

卷积的优化

有所欠缺，因

此在

某些条

件下普通卷

积会拥有比

深度可分离

卷积更快的

计算速度。那

么究竟是硬

件对提速更

重要还是

算

法设计对提

速更重要呢

，这就需要我

们通过实验

结果来验证

了。在这篇论

文中，作者对

比了在

EfficientNet v1 的不

同融合阶段

将可逆瓶颈

卷积层（MBConv）替换

为融合可逆

瓶颈卷积层

（Fused￾MBConv）1

的表现（见表

3.3），发现将阶段

1 ～ 3 的

MBConv 替换为 Fused-MBConv 可

提高预测速

度和准确率

。其中将融合

阶段

1 ～ 3 替换的

模型在准确

率和

TPU 速度上

取得了最好

的表现，将融

合阶段 1 ～

5 替换

的模型在 V100 上

取得了最好

的效果，而全

融合或者全

不融合都不

是最好的选

择。

Fused-MBConv 和 MBConv 的不同

点在于

Fused-MBConv 将 MBConv 的

深度可分离

卷积替换成

了普

通卷积

，如图 3.41 所示。

表

3.3

在不同的融

合阶段替换

MBConv 为 Fused-MBConv 的表现

参

数数量（106

） FLOPS(109

)

top-1 准确

率 TPU 速度

imgs/sec/core

V100 效果

imgs/sec/gpu

不融合

19.3 4.5 82.8% 262

155

融合

阶段 1 ～

3 20.0 7.5 83.1%

362 216

融合阶

段 1

～ 5 43.4 21.3

83.1% 327 223

融合阶段

1

～ 7 132.0 34.4

81.7% 254 206

图

3.41 MBConv 和 Fused-MBConv

EfficientNet v1 是通过

固定 α、β

和 γ，使用

φ 对模型进行

尺度的缩放

的，它的缩放

对于

EfficientNet

v1 的每一

个阶段是完

全相同的。但

是这么做其

实是不合理

的，因为网络

的不同阶段

的

不同尺度

对于模型的

速度和准确

率的影响是

不同的，所以

在 EfficientNet

v2 中使用了

不同层的非

均

1　参见

Suyog Gupta、Berkin Akin 的论

文“Accelerator-aware

Neural Network Design using

AutoML”。

3.9　EfficientNet v2

139

匀分布的

缩放方法。

2．模

型准确率

EfficientNet

v2 中

，当我们使用

不同尺寸的

输入图像训

练网络时，大

尺寸的输入

图像训练的

模

型要比小

尺寸的输入

图像训练的

模型更容易

过拟合。因为

大尺寸的输

入图像包含

更多的图像

细节信

息，而

正是这种训

练集和测试

集在细节上

的分布不一

致导致了模

型的过拟合

问题。因此作

者认为

大尺

寸的输入图

像应该使用

更大的正则

尺度，而且通

过实验验证

了这一点，通

过一组不同

输入图

像尺

寸和随机扩

充尺度的 RandAug1 的

对照实验，验

证了输入图

像尺寸和正

则尺度的正

比关系，如

表

3.4 所示。

表 3.4

输入

图像尺寸和

随机扩充尺

度在 ImageNet 上 top-1

的准

确率

输入图

像尺寸 = 128

输入

图像尺寸 = 192 输

入图像尺寸

=

300

随机扩充尺

度 = 5

78.3±0.16 81.2±0.06 82.5±0.05

随机扩充

尺度

= 10 78.0±0.08 81.6±0.08

82.7±0.08

随机扩

充尺度 = 15

77.7±0.15 81.5±0.05 83.2±0.09

3.9.2

EfficientNet v2 详解

EfficientNet v2

算法包括两

个核心方面

：

z 使用新的搜

索空间和奖

励函数搜索

一个新的模

型架构；

z

使用

渐进学习（progressively learning）动

态地调整正

则尺度和输

入图像尺寸

的关系来对

网

络进行训

练。

1．NAS

EfficientNet v2 的搜索空

间如下。

z

卷积

类型：MBConv、Fused-MBConv。

z 卷积核

尺寸：3 ×

3、5 × 5。

z

每一个

阶段的层数

。

z MobileNet v2

提出的扩张

因子（expansion factor）：{1,4,6}。

z 是否使

用

SENet 结构。

在 EfficientNet

v2 中

，残差是默认

添加的，网络

的通道数使

用的是 EfficientNet-B4 值。在

EfficientNet

v1 中我们说过

，为了提高模

型的计算速

度，模型的通

道数最好为

8 的整数倍。在

EfficientNet-B0 中，它的通道

数依次是

{16,24,40,80,112,192,320,1280}，EfficientNet-B4 的

通道数的

扩

充比例是 1.4，然

后通过除以

8

的约束可以

得到 EfficientNet-B4 的网络

结构。可以通

过以下代码

进行计算：

new_filters

= max(8, int(B0_channel*beta+4)//8 *

8)

EfficientNet v2 的

奖励函数为

：

r = A·Sw

·Pv

其中，A 是模型

准确率，S 是每

个训练步骤

的时长，P 是参

数数量。w

和 v 是

控制奖励比

例的两个

1

参

见 Ekin D. Cubuk、Barret

Zoph、Jonathon Shlens 等人的论

文“RandAugment: Practical

automated data augmentation with

a reduced search space”。

第 3 章

模型

架构搜索

140

超

参数，其中 w

= − 0.07，v =

− 0.05。因

为 EfficientNet v2

比 EfficientNet v1 的搜索

空间小了很

多，

所以使用

了以 EfficientNet-B4 为基础

的网格搜索

。

最终

EfficientNet v2-S（小）的网

络结构如表

3.5 所示。通过表

3.5 和

EfficientNet v1 的网络结

构的对比，我

们可以看出

EfficientNet v2

具有如下的

特点。

z 如我们

在表 3.3

中验证

的，EfficientNet v2 使用了 MBConv

和

Fused-MBConv 的混合结构

。

z EfficientNet

v2 使用了更小

的扩张因子

，通过这种结

构得到的网

络参数更小

。

z EfficientNet

v2 使用的都是

3 × 3

的卷积核，但

是每个阶段

拥有更多的

层数。

z 前 3

个阶

段并没有使

用 SENet。

表 3.5

EfficientNet v2-S（小）的网

络结构

阶段

操作 步长

通

道数 层数

0 Conv

3 × 3 2

24 1

1 Fused-MBConv1,

卷

积核大小 3 × 3

1 24 2

2

Fused-MBConv4, 卷

积核大小 3 ×

3 2 48 4

3 Fused-MBConv4, 卷

积核大小 3

× 3 2 64

4

4 MBConv4, 卷

积核大小

3 × 3, SERatio

0.25 2 128 6

5 MBConv6, 卷

积核大小 3

× 3, SERatio 0.25

1 160 9

6

MBConv6, 卷

积核大小 3 ×

3, SERatio 0.25 2

272 15

7 Conv

1 × 1 &

池

化 & 全连接 —

1792 1

由

EfficientNet v2-S

扩张至 EfficientNet v2-M/L（中 /

大

）使用的是和

EfficientNet v1 类似的策略

，

但是

EfficientNet v2 也做了

几点优化：

z

将

输入图像的

最大尺寸限

制到 480；

z 递增式

地为阶段

5 和

阶段 6 添加一

些网络层。

2．渐

进学习

我们

介绍过模型

的正则尺度

和输入图像

的分辨率有

近乎正比的

关系，因此当

我们在调整

一个网

络的

输入图像的

尺寸时，我们

需要对应地

调整网络的

正则内容才

能更大程度

地发挥网络

的性能。这

就

是

EfficientNet v2 中提出的

渐进学习。

EfficientNet

v2 的

渐进学习分

成两步：

（1）在训

练的早期，使

用更小尺寸

的输入图像

和更弱的正

则；

（2）然后逐渐

增大输入图

像的分辨率

和网络尺寸

以及使用更

强的正则。

在

EfficientNet v2 中使用的正

则类型有 3

类

，具体如下。

z Dropout。

z

随

机扩充：在随

机扩充中，一

些常见的扩

充策略，如图

像翻转、随机

裁剪等，都会

等概率

地被

选到，它通过

尺度参数 来

控制扩充的

程度， 越大，图

像变化越大

。

z Mixup1

：它是通过融

合图像来达

成数据扩充

的目的的一

个算法。通常

它需要两幅

输入图像以

及它们的标

签（假设它们

的标签分别

为 (xi,

yi) 和 (xj, yj)），然后通

过融合参数

λ

完成两幅图

像的合并： 和

。其中，λ 是用来

控制图像融

合的尺度的

。

更具体地讲

，假设整个训

练过程有

N 个

训练步，我们

训练图像的

目标尺寸是

Se，然后正则尺

度的参数序

列为 ，其中 k

是

正则的种类

数。我们首先

将 N 个训练步

分成 M

个阶段

，对于第

i 个阶

段我们选择

一组输入图

像的尺寸 Si

和

它的正则尺

度φ i

。在 EfficientNet

v2 中是使

用线性插值

1　参见 Hongyi

Zhang、Moustapha Cisse、Yann N. Dauphin

等人的

论文“mixup: Beyond empirical risk

minimization”。

3.10　RegNet

141

来确定

这两个值的

。因为在全 CNN 中

，模型的参数

和输入图像

的尺寸以及

正则的内容

无关，所以

可

以直接复用

不同图像尺

寸和正则尺

度训练得到

的网络。

3.9.3 小结

2021 年伊始，基于

视觉的 Transformer

和 CNN 阵

营的竞争好

像达到了一

个白热化的

阶段，随

之而

来的便是

ImageNet top-1 准

确率的不断

刷新。本文介

绍的 EfficientNet

v2 更多的

价值在于提

出

了图像尺

寸和正则尺

度的关系，整

个渐进学习

的过程是过

拟合难度从

易到难的迭

代式开发，这

种多

图像尺

寸加正则尺

度无疑将大

幅提高模型

的泛化能力

。

从 NAS 到 EfficientNet

v2，我们也

看出了 Auto ML 的一

个发展趋势

：从单一的准

确率的优化

向

着定制化

的场景的优

化发展，而每

一个场景都

会衍生出很

多优秀的算

法，接下来有

怎样的发展

方向

呢？让我

们拭目以待

吧！

3.10　RegNet

在本节中

，先验知识包

括：

 残差网络

（1.4 节）；

 ResNeXt（2.4 节）；



MobileNet（2.2 节）；  NAS（3.2

节）。

在

我们之前介

绍的以 NAS 为代

表的

Auto ML 算法中

，它们都是专

注于单个网

络实例架构

的设

计和优

化。这一类的

算法有两个

核心点：一个

是优化模型

的搜索空间

，另一个是优

化模型的搜

索策

略。通常

对这些模型

质量的评估

也是通过对

搜索出的单

个模型实例

在某些数据

集上的速度

和精度的

表

现来完成的

，相当于评估

在这个搜索

空间中搜索

出的最优解

。而我们这里

要介绍的 RegNet1 有

所

不同，它不

是搜索出单

个网络实例

，而是搜索出

一个简单的

、易于理解的

、便于量化的

搜索空间，

RegNet 中

也将它叫作

设计空间（design space）。而

通过搜索出

的设计空间

，我们可以得

到模型的一

系列设计准

则，然后将其

推广到其他

不同的场景

中。

3.10.1 设计空间

RegNet 论文中涉及

3 个问题。

z 什么

是设计空间

？

z 如何评估设

计空间的质

量？

z 怎么优化

设计空间？

下

面我们以这

3 个问题为线

索，逐步揭开

RegNet

的“神秘面纱

”。

1．基础概念定

义

设计空间

是 Radosavovic

在其名为

“On Network Design Spaces

for Visual Recognition”2 的论文中提

出的

概念，这

一篇论文一

般也被看作

RegNet 系列的第一

篇论文。在这

篇论文中，作

者给出了设

计空间的定

义和评估标

准，为RegNet提供了

理论和统计

基础。这里我

们先给出包

含设计空间

在内的几个

重要定义。

（1）模

型族（model family）：模型族

是拥有相同

高级网络结

构和设计原

则的一组模

型架构。像

1　参

见 Ilija Radosavovic、Raj

Prateek Kosaraju、Ross Girshick 等人的论

文“Designing

Network Design Spaces”。

2

参见 Ilija Radosavovic、Justin Johnson、Saining

Xie 等人

的论文“On Network Design

Spaces for Visual Recognition”。

第 3 章

模型架构搜

索

142

残差网络

、DenseNet 等都是拥有

非常明显的

结构特征和

设计原则的

。

（2）设计空间（design

space）：设

计空间是从

模型族中实

例化出的一

组具体的架

构，它是模型

族的参数化

以及模型族

中每个参数

的取值范围

。通过固定设

计空间中的

超参数的值

，我们便可以

得

到一个具

体的模型实

例。

（3）模型分布

（model distribution）：为了分析设

计空间的质

量，我们需要

根据这个设

计空间

中的

模型的质量

来间接地评

估设计空间

的质量。如果

我们采样设

计空间中的

所有样本，那

么这个数

量

将是指数级

的。所以这里

一般会采用

随机采样的

方法来采集

设计空间中

的样本，这种

通过采样来

生成数据的

方式在上述

论文中被叫

作数据生成

（data generation）。通过分析设

计空间中采

集的样

本的

准确率，我们

可以得到这

个设计空间

的模型分布

。

笔记　在残差

网络中，业内

被广泛使用

的结构包括

ResNet-18、ResNet-32、ResNet-50、

ResNet-101、ResNet-152 等。它们都属

于残差结构

的模型族，这

个族中最重

要的超参数

是

模型的深

度。

2．模型分布

估计

点估计

与分布估计

：在 NAS

等算法中

，都是通过在

搜索空间中

搜索出的最

优实例来表

示这个

搜索

空间的质量

的，这个方法

在本章其他

节的算法中

叫作点估计

（point estimation）。但是仅仅通

过最优实例

得到的结论

是有偏的，因

为无论是在

模型的搜索

过程中，还是

在模型的训

练过程中都

存

在非常大

的随机性。所

以

Radosavovic 等人提出

了使用设计

空间中的模

型分布来估

计设计空间

的质

量，这也

是 RegNet

重要的基

础思想。

那么

如何根据采

集的样本评

估一个搜索

空间的质量

呢？在上述论

文中，他们使

用了经验分

布函

数（empirical distribution

function，EDF）作为

评估指标。EDF 的

定义如式（3.20）所

示。它的物理

意义是取错

误率 ei 小于错

误率

e 的样本

总数：

（3.20）

其中，1是

指示函数（indicator

function），它

用来指示变

量x是否在某

一集合A中，定

义如式（3.21）所示

。

（3.21）

对比点估计

，分布估计是

一个对采样

数更不敏感

的策略，为什

么这么说呢

？这里我们举

例说明

一下

原因。在都是

残差网络的

设计空间中

，我们分别搜

索

100 个和 1000 个样

本（分别是模

型族

B 和

模型

族 M），通过对比

两个模型族

的点估计和

分布估计，我

们可以得到

图

3.42 所示的对

比结果。

图 3.42

点

估计和分布

估计

3.10　RegNet

143

图 3.42（a）展示

了进行了 5 000

次

随机实验的

最小误差差

异（测试数据

集是 CIFAR-10），从

图中

可以看出，90% 的

情况下模型

族

B 的错误率

高于模型族

M 的错误率，但

是它们都采

样自同一

个

设计空间，因

此点估计对

于设计空间

的评估是有

误导性的。

图

3.42（b）展示了两个

模型族的误

差分布，从图

中我们可以

看出 3 个重要

信息：

z

错误率

最初为 5% 左右

，并且开始有

一小段比较

平缓的积累

。表明我们采

样出的模型

有非

常小的

比例可以达

到

5% 左右的错

误率；

z 错误率

从

10% 之后逐渐

趋于平缓，表

明错误率很

少有高于 10% 的

；

z 最重要的，两

个模型族的

分布曲线基

本一致，这说

明了分布估

计比点估计

更能评估设

计

空间。

KS-

检验

：通过对比 EDF 曲

线的方式评

估设计空间

是过于主观

的，一个更科

学的描述是

将这

个指标

量化。在统计

学中，对于

EDF 曲

线，我们通常

使用 KS- 检验（Kolmogorov-Smirnov

test）来

检验数据是

否符合某种

分布或者来

比较两个数

据集是否符

合同一个分

布，它表示为

式（3.22）：

D = sup|F1(x)

− F2(x)| （3.22）

其中，sup

是上

确界函数，D 的

物理意义是

图 3.42（b）中两条曲

线在某一点

的垂直方向

上的差值。

在

Python

中，我们可以

通过 SciPy 包的 kstest()

函

数直接计算

两个 EDF 的 KS-

检验

的值。通过

kstest()，我

们可以同时

得到两个分

布的差值 D 和

置信度

p-value，p-value 也可

以通过查表

得到。在论文

的实验中，他

们得到 D =

0.079，p-value = 0.6，因此

可以得出两

个分布有一

个高的

置信

度让我们无

法拒绝它们

是同一个分

布的假设。这

里使用了双

重否定，所以

意思也就是

这两个

EDF 表示

同一个分布

。

复杂度与 EDF

：在

固定设计空

间之后，模型

复杂度是影

响 EDF 分布的一

个重要因素

，所

以我们需

要考虑复杂

度和模型分

布之间的关

系。图

3.43（a）中是复

杂度不同的

两个模型，其

中

ResNeXt-B 的参数数

量约是 ResNeXt-A

的 4.6 倍

（见表 3.6）。从中可

以看出，虽然

两个模型取

自同

一个设

计空间，但是

因为它们的

复杂度不同

，所以得到的

EDF 曲线也不同

。从图 3.43（b）中也可

以看出模型

的错误率是

和参数数量

（复杂度）成反

比的，而且在

这个图中可

以看出随着

参数数量的

不断提升，模

型的错误率

还有继续下

降的空间。

图

3.43

模型复杂度

第 3 章

模型架

构搜索

144

表 3.6

几

个重要对照

实验的设计

空间

深度 宽

度 瓶颈率

组

数 参数数量

普通网络 1 249

1 625 612 —

— 1 259 712

残

差网络 1 249 1

625 612 — —

1 259 712

ResNeXt-A

1 165 162 565

143 143 11 390

625

ResNeXt-B 1 165

6 410 245 143

1 165 52 734

375

为了

准确地评估

设计空间的

质量，我们需

要排除复杂

度对 EDF 曲线的

影响。在论文

中，他们

提出

了标准化比

较（normalized comparison）来解决这

个问题。具体

地讲，给定 n 个

模型的集合

，

标准化比较

的思想是为

每个模型赋

予一个权值

wi

，其中 。标准化

比较可以用

在复杂度上

，

假设第i个模

型的复杂度

是ci

，此时标准

复杂度EDF（normalized complexity EDF）定义

为式（3.23）。

（3.23）

同理，我

们也可以定

义标准错误

EDF（normalized error EDF）为式（3.24）。

（3.24）

式（3.23）和式

（3.24）的权值 wi 是通

过分箱（bin）的方

式计算的。具

体地讲，我们

首先使用

均

匀分布将复

杂度分成

k 个

箱，对于第 j 个

箱的

mj 个模型

中的任意一

个模型，权值

wj 的计算方式

为 。论文中分

别基于参数

数量和

FLOPS 进行

了加权计算

，实验结果表

明加权的方

式均能

够减

小两个分布

之间的差异

，如图 3.44

所示。

图

3.44　基于参数数

量和 FLOPS

加权之

后两个不同

复杂度的模

型的 EDF 曲线差

异

采样策略

：设计空间质

量的评估仍

要依靠从设

计空间中采

集到的模型

，所以采样策

略也是影响

EDF

曲线分布一

个重要的因

素。这里我们

从搜索策略

和样本大小

两个角度对

采样策略进

行介绍。

随机

搜索：在论文

中，他们采用

了随机搜索

的策略。对于

一个大小为

n 的模型池，如

果采样大

小

为

m，首先我们

会从模型池

中随机采集

m 个样本，然后

得到这 m 个样

本的最小值

。然后我们重

复这个实验

次，得到抽样

的均值。为了

避免复杂度

的影响，我们

同样需要根

据复杂度对

采集的样

本

进行加权。

3.10　RegNet

145

最

小样本大小

：图 3.45（a）是根据不

同样本数得

到的 EDF 曲线，从

图中可以看

出，当

n ＞

100 时，EDF

曲线

便基本不变

化了，而 n = 1000

和 n = 10

000 的

曲线基本一

致，所以 n 的值

在

100

到 1000 范围内

比较合理，如

图

3.45（b）所示。KS（Kolmogorov-Smirnov）统计

量，基于经验

累

积分布函

数，是一种非

参数的检验

方法，用于检

验两个分布

是否一致。

图

3.45　不同样本大

小下的

EDF 曲线

EDF 效果评估：得

到 EDF

曲线的分

布之后，我们

可以从图中

得到什么信

息呢？从 EDF 曲线

中，我

们可以

看出的一个

信息便是模

型中样本的

占比情况，EDF曲

线所在坐标

系的横轴表

示错误率，纵

轴的意义

是

小于该错误

率的模型数

占总采样数

的比例。从图

3.46 中可以看出

，例如基于残

差网络设计

空间采样的

模型中，约有

80% 的模型错误

率小于 8%，而基

于普通

CNN 的模

型中，错误率

小于 8% 的比例

仅为

20%。

图 3.46　普通

CNN

和残差网络

的 EDF 曲线

因此

我们可以看

出曲线越靠

左，表明设计

空间中低错

误率的样本

越多，则设计

空间的质量

越

高。对于表

3.6 中的普通网

络（类似于 VGG 的

线性结构）和

残差网络，因

为它们设计

空间的差异

（是否包含残

差结构），所以

从图

3.46 中可以

看出残差网

络的曲线要

比普通 CNN 的曲

线偏左很多

。

我们可以通

过曲线下方

的面积来量

化哪条曲线

“偏左”，如式（3.25）所

示。

（3.25）

3.10.2 RegNet

详解

在前

文中我们介

绍了如何评

价一个设计

空间，但是只

会评价一个

设计空间是

没有意义的

，我

第 3

章  模型

架构搜索

们

需要根据评

价指标来进

行设计空间

的设计（design

space design），也就

是我们这里

要介绍的

RegNet。RegNet 论

文的核心并

不是设计一

个网络模型

，而是设计一

个网络的集

合，或者说是

网络

的设计

空间。我们可

以根据这个

设计空间来

抽象出模型

的设计准则

，从而将这些

准则迁移到

其他不

同的

硬件环境中

。根据环境的

不同，我们可

以在这些设

计准则的基

础之上灵活

地调整网络

的细节。

而 RegNet

便

是这个设计

空间中包含

的诸多简单

且有效的模

型之一。

1．设计

空间设计

RegNet 的

设计空间设

计遵循递进

式的设计方

法。在图

3.47 中，A、B、C 是

3 个模型族。在

设

计流程中

，输入是初始

的设计空间

，输出是更简

单、效果更好

的设计空间

，通过这样逐

步迭代的方

式来完成设

计空间的优

化。正如图 3.47 中

所展示的，参

数集合满足

A ⊆

B ⊆ C，但是它们的

EDF 曲

线的效果

却是 C ＜ B

＜ A。

图 3.47

RegNet 的递

进式设计空

间设计

在 RegNet

论

文中，A 是设计

空间的初始

空间，它被叫

作 AnyNetX 或者

AnyNetXA，它是

一个

设计空

间组合数较

多、效果较差

的网络。它的

每一轮设计

空间的优化

得到的新的

设计空间依

次叫作

AnyNetXB、AnyNetXC 等。所

以问题的核

心便是如何

根据评估的

设计空间的

质量的优劣

来优化设计

空间。设计空

间的优化分

为

4 个主要步

骤：

（1）从初始的

输入空间中

采样 n

个模型

，训练这 n 个模

型得到每个

模型的错误

率；

（2）绘制这

n 个

模型的 EDF 曲线

；

（3）可视化设计

空间中的若

干个重要指

标；

（4）人工分析

这些指标并

对设计空间

进行优化。

因

为初始的设

计空间质量

比较差，所以

我们需要进

行更广的探

索。在这里我

们选择随机

生成

500

个模型

，每个模型训

练 10 个 epoch。下面我

们详细分析

5

个步骤。

（1）初始

设计空间 AnyNetX。在

深度学习中

，几乎每个深

度学习网络

都可以抽象

成 3

个模块，

如

图 3.48（a）所示。

z

stem ：这一

层又被叫作

输入层，它用

来处理不同

类型的输入

数据。

z body

：这一层

又被叫作骨

干层，它通过

堆积大量的

网络模块来

增加模型的

容量，提取多

种类型的数

据特征，增强

模型的表征

能力。骨干层

又由若干个

块（RegNet 论文中叫

作阶

段）组成

，如图 3.48（b）所示，每

个块由降采

样层分隔开

来，降采样层

一般表示步

长为

2 的卷积

或者池化操

作。每个块由

若干个层（RegNet 论

文中叫作块

）组成，如图 3.48（c）

146

3.10　RegNet

所

示，层一般表

示一个卷积

操作，在这里

我们可以定

义卷积的一

些超参数，如

通道数、卷

积

类型、卷积核

的大小等。

z head ：这

一层又被叫

作输出层，它

根据不同的

任务类型（分

类、回归）和内

容（类别数、

回

归分支数）等

来调整输出

层的结构。

图

3.48　深度学习网

络结构

AnyNetX

同样

使用这个包

含 3 个模块的

架构，它的优

化完全是在

body 部分进行的

。在

AnyNetX 的 body 模块中

，共有

4 个网络

块，每个网络

块共有 4 组超

参数，如下。

z 块

的层数：di

。它的

范围满足 1

≤ di ≤ 16。

z 每

一层的通道

数：wj

。它的范围

是 8k,

其中 1 ≤ k

≤ 128。

z MobileNet

v2 中介

绍的瓶颈层

的瓶颈率（bottleneck ratio）：bi ∈

{1,2,4}。

z 分

组卷积的组

数：gi

= 2k

，0 ≤

k ≤ 5。

所以

AnyNetX 共有

16 个超参数，它

的设计空间

的样本数量

级为 PA

= (16 × 128

× 3 × 6)4

≈1.8 ×

1018。

（2）实现

AnyNetXB。得

到了 AnyNetX（AnyNetXA）的设计

空间后，我们

需要对其进

行优化。

这一

部分优化是

由人工完成

的，需要由人

工根据各个

指标的不同

影响来决定

优化的策略

。在人工优

化

时，我们要优

先考虑下面

的优化准则

：

z 简化设计空

间的结构；

z 提

高设计空间

的可解释性

；

z 提高设计空

间的质量；

z 保

持模型在设

计空间中的

多样性。

AnyNetXB 针对

AnyNetXA 的优化固定

了每一层的

瓶颈率。从图

3.49（图 3.49

至图 3.52 的方

括

号中左侧

表示最小错

误率，右侧表

示平均错误

率）中可以看

出，固定瓶颈

率之后两个

设计空间的

EDF

分布基本保

持一致，而且

设计空间的

样本数量得

到了减少。AnyNetXB 的

设计空间的

样本数量

级

为 PB

= (16 × 128

× 6)4 × 3≈6.8

× 1016。

（3）实现 AnyNetXC。设计

空间优化的

第二步是在

AnyNetXB

的基础上共

享分组卷积

的分组数，

即

gi

 =

g。从图 3.50 中可以

看出共享分

组卷积的分

组数之后，AnyNetXC 较

AnyNetXB

效果并没有

明

显变坏，因

此共享分组

数是一个有

效的策略。因

为分组卷积

有 6 个值，所以

AnyNetXC

的设计空间

的样本数量

级为 PC = (16

× 128)4 × 3

× 6≈3.2 × 1014。

147

第 3 章

模

型架构搜索

148

图 3.49 AnyNetXA

与 AnyNetXB 对比 图

3.50

AnyNetXB 与 AnyNetXC 对比

（4）实现

AnyNetXD。AnyNetXD 在 AnyNetXC 的基础上

探讨了通道

数的变化对

模型的影响

。从

图 3.51 中可以

看出，当我们

共享通道数

或者使用递

减的通道数

时，设计空间

的质量都会

降低。而

当我

们使用递增

的通道数时

，设计空间的

质量明显提

升。因此在

AnyNetD 中

使用了通道

数递增的方

案，在这个方

案下，AnyNetXD 的设计

空间的样本

数量级为 。

（5）实

现 AnyNetXE。AnyNetXE 分析了每

一个网络块

中的层数对

模型的影响

。从图 3.52

中可

以

看出，当层数

不断增加时

，设计空间的

质量会得到

提升，而其他

两个方案会

严重降低设

计空间的

质

量。在这个方

案下，AnyNetXE 的设计

空间的样本

数量级为

。

图

3.51 AnyNetXD 中不同层的

不同通道数

情况

下的 EDF 曲

线

图

3.52 AnyNetXE 中不同

块的不同层

数情况

下的

EDF

曲线

2．RegNet 相关细

节

（1）RegNetX

的计算。从

AnyNetA 到 AnyNetE，我们把设

计空间的参

数数量减少

了约 107

个数

量

级，同时我们

得出了模型

设计的 4 条准

则：

z 瓶颈率共

享；

z 分组卷积

的分组数共

享；

z 通道数逐

渐增加；

z 每一

个块的层数

逐渐增加。

3.10　RegNet

149

接

下来介绍的

RegNet

是对 AnyNetE 进一步

优化得到的

效果更好、参

数数量级更

小的设计空

间。

它首先研

究的是通道

数以怎样的

趋势增长才

是最合理的

。图

3.53 中的浅灰

色折线展示

了在 AnyNetE

中随着

层数增加采

样的

20 个最优

模型通道数

的变化情况

。根据这 20 个最

优模型的折

线图，我们可

以拟合出一

条随通道数

变化的曲线

，如图3.53中的深

色曲线。注意

，在纵轴它的

单位是2n

，在1∶1

的

坐标系中深

色曲线其实

更逼近一条

直线，表示为

式（3.26）。

（3.26）

在式（3.26）中，它

有

3 个参数，即

初始的宽度

w0 ＞ 0、直线的坡度

wa

＞ 0、网络的深度

d，

它生成的是

每一层的通

道数 u。

通过式

（3.26），我们得到的

每一层的通

道数有两个

问题：

z 直接通

过式（3.26）计算得

到的通道数

可能是一个

浮点数；

z

在每

一个网络块

中，每一层网

络的通道数

不同。

为了将

每一个网络

块中的所有

层的通道数

都转换为相

同大小的整

数，RegNet 做了如下

的工作。

首先

它引入了超

参数

wm 和一个

中间变量 sj

，计

算方式如下

：

z 对于第 j 个网

络层，计算它

的

sj

，使其满足

；

z 为了将

uj 转换

为整数，我们

将 sj 四舍五入

（表示为

sj），然后

我们得到每

一层的通道

数，

表示为 ；

z

对

通道数进行

进一步的量

化，使每个网

络块的通道

数都保持相

同，每个网络

块的层数即

sj 等于 i 的个数

，表示为式（3.27）

（3.27）

综

上，RegNetX 是由 6

个参

数构成的设

计空间，它们

是 d、w0、wa、wm、b、g。其中 d ＜

64，

w0, wa ＜

256，1.5 ≤ wm ≤

3。并且

通过上述步

骤可得到每

个网络块的

通道数和层

数。RegNetX 的

样本数

量级为 3.0

× 108

，EDF 曲线

如图

3.54 所示。RegNetX 使

用网格搜索

优化即可。

图

3.53

AnyNetE 中采样的 20 个

最优模型的

通道数变化

示意

图 3.54 RegNetX 的

EDF 曲

线

（2）RegNet 与模型复

杂度。截至目

前，我们得到

的

RegNetX 是一个质

量比较高的

设计空间，

在

这个设计空

间的基础上

再进行优化

的话则不需

要采样过多

的模型（100 个足

够），而是需要

进行

更长时

间的训练（25 个

epoch）。这样我们可

以在更长期

的训练中观

察更细粒度

的趋势。从图

3.55

所示的实验

结果中我们

可以看出效

果比较好的

模型通常具

有如下的特

征：

第

3 章  模型

架构搜索

150

z 在

每个网络块

中 20

个层是比

较好的（这与

我们的“越深

的网络效果

越好”的认识

是相违

背的

）；

z 瓶颈率为

1 时

效果最好，也

就是不使用

沙漏型或者

纺锤形的网

络结构；

z 通道

数变化梯度

wm

在 2.5 左右时效

果最好；

z

其他

几个参数（g、wa、wm）与

模型复杂度

成正比的关

系。

图 3.55 RegNetX

的参数

与模型复杂

度的关系

（3）推

理时间。图 3.56 展

示了推理时

间与

FLOPS 和网络

激活的直接

关系，这里的

激活指的是

所有卷积层

的输出向量

的尺度。表 3.7 列

出了常见的

卷积操作的

FLOPS、参数数量以

及激活之间

的

具体值，其

中 w 为通道数

，r 为图像尺寸

，g

为分组数。从

图 3.56 中可以看

出，推理时间

与激活的

关

系更为密切

。

图 3.56　推理时间

与 FLOPS

和网络激

活的关系

表

3.7　常见卷积操

作的 FLOPS、参数数

量以及网络

激活

FLOPS 参数数

量 网络激活

1 ×

1 卷积 w2

r

2 w2 wr2

3

× 3 卷积 32

w2

r

2 32

w2 wr2

3 ×

3 分

组卷积 32

wgr2

32

wg wr2

3

× 3 深度

卷积 32

wr2 32

w wr2

3.10　RegNet

基于上

面的分析，作

者对 RegNetX

进行了

进一步的优

化，包括：

z 设置

b =

1，d ≤ 40，wm ＞

2，这可以进一

步缩小搜索

空间；

z 限制激

活和参数数

量，通过这一

步可以得到

速度更快、参

数数量更少

，但是错误率

降低幅

度不

大的模型。

除

了这些，作者

还讨论了几

个提升模型

效果的小技

巧：

z 输入图像

的分辨率为

224 ×

224 时，效果最好

；

z 压缩

- 激发模

块可以提升

模型表现，RegNetX 在

添加了压缩

- 激发模块之

后被叫作

RegNetY，它

也是论文中

效果最好的

模型。

3.10.3 小结

在

这里我们讨

论了一个比

较新颖的模

型架构搜索

范式，不同于

NAS

系列介绍的

搜索一个具

体

的网络模

型，这篇论文

介绍的是如

何搜索一个

设计空间。它

包含两个主

要的知识点

：一是什么是

设

计空间；二

是如何根据

实验结果来

递进式地优

化设计空间

。本文的创新

点是很多的

，从论文中可

以

看出实验

量非常足。从

实验结果来

看，搜索出的

设计空间也

非常逼近

ResNeXt 的

结构，可以看

出

人工设计

的强大。对比

EfficientNet-B5，RegNetY-32GF 的准确率更

高

1

（79.9% > 78.5%），推理速

度

更快（113ms < 504ms）。

最后我

们讨论一下

RegNet

的一些问题

。

z 为了节约训

练资源，初始

的设计空间

的优化只训

练了 10

个 epoch，这种

未完全收敛

的模型

能否

充分代表设

计空间的真

正质量值得

商榷。1

z

设计空

间的迭代优

化需要人工

干预，而且过

于依赖人工

经验，这种只

固定一个变

量和忽略

了

变量之间的

依赖性的方

式是否合理

值得讨论。

z 结

果不具备足

够的说服力

。在作者对比

的先进的

EfficientNet 的

论文中，给出

的最优的

EfficientNet-B7 在

ImageNet

上的准确率

达到了 84.3%，而 RegNet 最

优的

RegNetY-32GF

的准确

率只有 79.9%。虽然

它们的迭代

数不同，但论

文中并没有

给出一个 RegNet

在

充分

训练前

提下得到的

最优模型。

1　这

里对比的准

确率和

EfficientNet 的论

文中给出的

83.6% 的差距略大

，因为在这里

作者控制了

epoch 这个超参数

。作者

只训练

了 100 个 epoch，而在

EfficientNet 论

文的实验中

，他训练了 350 个

epoch。

151

第二篇 自然

语言处理

“一

个有纸、笔、橡

皮擦并且坚

持严格的行

为准则的人

，实质上就是

一台通用图

灵机。”

—Alan Mathison Turing

自然语

言处理（NLP）指的

是对人类语

言进行自动

化的计算与

处理。它一般

使用人类语

言作为输

入

，然后产生满

足特定要求

的输出，抑或

是一个类别

，抑或是一串

文本序列。自

然语言处理

的难点在

于

人类语言的

歧义性、可变

性以及病态

性。人类语言

本质上是符

号化的，但是

计算机能够

处理的信息

是数字化的

。所以早期的

NLP 都是基于统

计机器学习

的，例如经典

的贝叶斯算

法。在使用深

度学习之

前

，核心的

NLP 技术

以有监督学

习为主导，例

如 SVM、逻辑回归

等。

2014

年前后 NLP 领

域开始了向

深度学习的

转型，其中最

具代表性的

算法便是 RNN，它

减轻

了对马

尔可夫假设

的依赖性，被

普遍用于序

列模型中。RNN 系

列方法的另

一个优点是

可以处理任

意长度的数

据，然后生成

有效的特征

向量。这使 NLP 在

预训练语言

模型、机器翻

译、智能对话

等

领域有了

重大的突破

。RNN 系列的经典

模型有 LSTM、GRU、SRU 等。NLP

领

域的另一个

经典算

法便

是注意力机

制，注意力机

制最早应用

在机器翻译

方向，它通过

为编码器的

每个特征学

习一个权

值

来选择对当

前时间片重

要的特征，进

而提升模型

的表征能力

。注意力机制

的特点是速

度快、参

数少

且效果好。NLP

领

域近年来最

具突破性的

进展是在 2017 年

提出的 Transformer，它完

全抛弃了

RNN 的

循环结构，由

一系列自注

意力机制组

成。Transformer 是一个表

征能力非常

强的模型，被

广

泛应用到

NLP

的各个方向

，并且最近几

年也被 CV 领域

所采用。Transformer 诞生

之后，NLP

领域

激

活了两个方

向，一个是针

对 Transformer 的优化，例

如更擅长处

理长序列数

据的

Transformer-XL、

Performer 等，另一

个是使用 Transformer

构

建预训练语

言模型，其中

最经典的便

是 BERT。

4.1　LSTM

和 GRU

4.1.1 序列模

型的背景

1．RNN

在

使用深度学

习处理时序

问题时，RNN 是最

常使用的模

型之一。RNN 之所

以在时序数

据上有

着优

异的表现是

因为 RNN 在 t

时间

片时会将 t − 1

时

间片的隐层

节点状态作

为当前时间

片的输入，

也

就是 RNN 具有图

4.1

所示的结构

。这样做有效

的原因是将

之前时间片

的信息用于

计算当前时

间

片的内容

，赋予了模型

捕捉时间片

之间依赖关

系的能力，而

传统的 MLP 的隐

层节点的输

出只取决

于

当前时间片

的输入特征

，自然就没有

这个能力。

第

4 章

基础序列

模型

第 4 章

基

础序列模型

156

图 4.1 RNN

的链式结

构，其中 A 表示

一个 RNN

单元

RNN 的

隐层节点的

数学表达式

可以为式（4.1）：

（4.1）

而

传统的深度

神经网络（DNN）的

隐层节点可

以表示为式

（4.2）：

（4.2）

对比 RNN

和 DNN 的隐

层节点的计

算方式，我们

发现唯一的

不同之处在

于 RNN

将上个时

间片

的隐层

节点状态 ht−1 也

作为神经网

络单元的输

入，这也是

RNN 擅

长处理时序

数据最重要

的原因。

所以

，RNN 的隐层节点

状态

ht−1 有两个

作用：

z 计算在

该时刻的预

测值

；

z 计算下

个时间片的

隐层节点状

态 ht

。

RNN 的该特性

也使 RNN

可以广

泛地应用于

具有序列特

征的数据，例

如 OCR、语音识别

、股

票预测等

领域的数据

。

2．长期依赖

在

深度学习领

域（尤其是 RNN）中

，“长期依赖”（long term dependency）问

题是普遍存

在的。

长期依

赖产生的原

因是当神经

网络的节点

经过若干个

阶段的计算

后，之前比较

长的时间片

的特征已

经

被覆盖，例如

eg1: The cat,

which already ate a

bunch of food, was

full.

 | |

| | | |

| | | |

|

 t0 t1

t2 t3 t4 t5

t6 t7 t8 t9

t10

eg2: The cats,

which already ate a

bunch of food, were

full.

 | |

| | | |

| | | |

|

 t0 t1

t2 t3 t4 t5

t6 t7 t8 t9

t10

我们想预测

full 之前系动词

的单复数情

况，显然其取

决于第二个

单词 cat/cats

的单复

数情况，

而非

其前面的单

词 food。根据图 4.1

展

示的 RNN 的结构

，随着数据时

间片的增加

，RNN 更关注的

可

能是距离其

更近的 food，而非

若干个时间

片之前的 cat/cats，即

长期依赖，如

图 4.2

所示。

图 4.2 RNN

的

长期依赖

3．梯

度消失 / 爆炸

梯度消失和

梯度爆炸也

是导致

RNN 模型

训练困难的

关键原因之

一，梯度消失

和梯度爆炸

是由

4.1　LSTM

和 GRU

RNN 的权

值矩阵循环

相乘导致的

，因为相同函

数的多次组

合会导致极

端的非线性

行为，使得到

的

梯度值过

大或过小。梯

度消失和梯

度爆炸主要

存在于 RNN 中，因

为 RNN

中每个时

间片使用相

同

的权值矩

阵。对于一个

DNN，虽然也涉及

多个矩阵的

相乘，但是通

过精心设计

权值的比例

可以避

免梯

度消失和梯

度爆炸的问

题。

处理梯度

爆炸可以采

用梯度截断

的方法。所谓

梯度截断是

指将值超过

阈值

θ 的梯度

手动降到 θ。

虽

然梯度截断

会在一定程

度上改变梯

度的方向，但

梯度截断的

方向依旧是

损失函数减

小的方向。

对

比梯度爆炸

，梯度消失不

能简单地通

过类似梯度

截断的阈值

式方法来解

决，因为长期

依赖的

现象

会产生很小

的梯度。在前

面的例子中

，我们希望在

t9 时刻能够读

到 t1

时刻的特

征，在这期间

内我们自然

不希望隐层

节点状态发

生很大的变

化，所以 [t2, t8] 时刻

的梯度要尽

可能小，才能

保证

梯度变

化小。很明显

，如果我们刻

意提高小梯

度的值将会

使模型失去

解决长期依

赖问题的能

力。

4.1.2 LSTM

LSTM1

是具有记

忆长短期信

息的能力的

神经网络。LSTM 在

1997 年由 Hochreiter

和 Schmidhuber

首先

提出，由于深

度学习在 2012

年

的兴起，LSTM 又受

到了若干代

“大牛”的影响

，因此形成了

比较系统且

完整的框架

，并且在很多

领域得到了

广泛的应用

。本文着重介

绍“深度学习

”时代的

LSTM。

LSTM

用于

解决我们提

到的长期依

赖问题。传统

的 RNN 节点输出

仅由权值、偏

置以及激活

函

数决定（见

图

4.3）。RNN 是一个链

式结构，每个

时间片使用

的是相同的

参数。

图 4.3

RNN 单元

而 LSTM 之所以能

够解决

RNN 的长

期依赖问题

，是因为 LSTM 引入

了门（gate）机制用

于控

制特征

的流通和损

失。对于上面

的例子，LSTM 可以

做到在 t9 时刻

将

t2 时刻的特

征传过来，这

样

就可以非

常有效地判

断 t9

时刻使用

的是单数还

是复数了。LSTM 是

由一系列 LSTM 单

元组成的，

其

链式结构如

图 4.4 所示。

图

4.4 LSTM 单

元链式结构

1　参见

Sepp Hochreiter、Jürgen Schmidhuber 的论文

“Long

Short-Term Memory”。

157

第

4 章  基础序

列模型

158

在详

细讲解 LSTM 的结

构之前，我们

先给出

LSTM 单元

中每个符号

的含义。在 LSTM 单

元中，

每个黄

色方框表示

一个神经网

络层，由权值

、偏置以及激

活函数组成

；每个粉色圆

圈表示元素

级别

操作；箭

头表示向量

流向；相交的

箭头表示向

量的拼接；分

叉的箭头表

示向量的复

制。总结如图

4.5 所示。

图

4.5 LSTM 单元

中的符号含

义

LSTM

的核心部

分是在图 4.4 中

上面类似于

传送带的部

分，这一部分

一般叫作单

元状态（cell

state），如图

4.6 所示，它自始

至终存在于

LSTM 的整个链式

结构中。其中

：

（4.3）

式（4.3）中，ft 叫作遗

忘门（forget gate），表示 Ct−1

的

哪些特征被

用于计算 Ct

。ft 是

一个向量，

向

量的每个元

素均位于[0,1]。通

常我们使用

sigmoid作为激活函

数，sigmoid的输出是

一个位于[0,1]

的

值，但是当你

观察一个训

练好的 LSTM 时，你

会发现门的

值绝大多数

都非常接近

0

或者 1，其余

的

值少之又少

。图 4.6

中的  表示

ft 和

Ct−1 之间的单

位乘的关系

，其中 ft 如图

4.7 所

示。

图 4.6

LSTM 的单元

状态（1） 图 4.7

LSTM 的单

元状态（2）

在图

4.8 中，

表示单元

状态更新值

，由输入数据

xt 和隐层节点

状态 ht−1 经由神

经网络层得

到，

单元状态

更新值的激

活函数通常

使用 tanh ；it 叫作输

入门（input

gate），同 ft 一样

，也是一个元

素

位于

[0,1] 的向

量，所以同样

由 xt 和

ht−1 经由 sigmoid 激

活函数计算

得到。it

用于控

制 的哪些特

征

用于更新

Ct

，使用方式和

ft

相同，如图 4.9 所

示。

图

4.8 LSTM 的输入

门和单元状

态更新值的

计算方式

最

后，为了计算

预测值

和生

成下个时间

片完整的输

入，我们需要

计算隐层节

点的输出 ht

，

LSTM

的

输出门如图

4.10 所示。ht 由输出

门（output gate）ot

和单元状

态 Ct 得到，其中

ot 的计算方

式

与 ft 以及 it

相同

。在 GRU 的论文（见

4.1.3 节）中指出，通

过将

bo 的均值

初始化为 1，可

以使

LSTM

达到同

GRU 近似的效果

。

4.1　LSTM

和 GRU

159

图

4.9 LSTM 的输入

门的使用方

法 图

4.10 LSTM 的输出

门

4.1.3

GRU

GRU1 是 LSTM

的简化

版，是一种基

于门控机制

的 RNN 单元，但是

GRU 的结构更简

单，速度

更快

，如图 4.11 所示。

图

4.11

GRU 的基本结构

在图 4.11 中，有两

个门，即重置

门（reset

gate）和更新门

（update gate），两个门均是

通过当

前时

间片的输入

数据 xt

和上一

个时间片的

隐层节点状

态 ht−1 计算而来

的。

重置门

rj 表

达式如式（4.4）所

示：

（4.4）

更新门

zj 表

达式如式（4.5）所

示：

（4.5）

其中，σ

是 sigmoid 激

活函数。

重置

门

rj 用于控制

前一时刻的

状态 ht−1 对更新

值的影响，当

前一时刻的

状态对当前

状态的影响

不大时

rj

 = 0，则更

新值只受该

时刻的输入

数据

xt 的影响

，如式（4.6）所示：

（4.6）

其

中，⊙表示向量

按元素相乘

。而

zt 用于控制

该时间片的

隐层节点使

用多少比例

的前一时刻

的状态、

多少

比例的更新

值，当 zt

= 1 时，则完

全使用前一

时刻的状态

，即 ht

= ht−1，相当于残

差网络的捷

径，

如式（4.7）所示

：

（4.7）

GRU 的两个门机

制是可以通

过随机梯度

下降法（stochastic gradient descent，SGD）和整

个网络

的参

数共同训练

的。

4.1.4 其他 LSTM

联想

之前介绍的

GRU，可以看出 LSTM 的

隐层节点的

门的数量和

工作方式貌

似是非常灵

活的，

1

参见 Kyunghyun Cho、Bart van

Merriënboer、Caglar Gulcehre 等

人的论文“Learning Phrase

Representations using RNN

Encoder-Decoder for Statistical Machine

Translation”。

第

4 章

基础序列

模型

160

那么是

否存在一个

最好的模型

或者比 LSTM

和 GRU 性

能更好的模

型呢？ Rafal

等人 1 采

集了能采集

到的 100

个最好

的模型，然后

在这 100 个模型

的基础上通

过变异的形

式产生了 10

000 个

新的模型。

然

后通过在字

符串、结构化

文档、语言模

型、音频 4

个场

景中的实验

比较了这 10 000 个

模型，得

出的

重要结论总

结如下：

z GRU、LSTM 是表

现最好的模

型；

z GRU 在除了语

言模型的场

景中的表现

均超过 LSTM

；

z LSTM 的输

出门的偏置

的均值初始

化为

1 时，LSTM 的性

能接近 GRU

；

z 在 LSTM

中

，门的重要性

排序是遗忘

门 > 输入门 >

输

出门。

4.2　注意力

机制

在本节

中，先验知识

包括：

 LSTM 和 GRU（4.1

节）。

在

传统的 RNN 编码

器

- 解码器模

型中，在编码

器计算第 t 个

时间片的隐

层节点状态

时，我们将

t − 1 时

刻的状态

ht−1 和

t 时刻的数据

xt 输入

t 时刻的

RNN 单元中，得到

t 时刻的状态

ht

，经过 T 个时

间

片后，得到长

度等于隐层

节点数量的

特征向量

c。在

解码的过程

中，将特征向

量 c 和上个时

间片预

测的

输出

yt′−1 输入 RNN 的

单元中，得到

该时刻的输

出

yt′

，经过 T′ 个时

间片后得到

预测的文本

序列。

但在一

些应用中，例

如句子长度

特别长的机

器翻译场景

中，传统的 RNN 编

码器 -

解码器

模型

的表现

非常不理想

。一个重要的

原因是 t′ 时刻

的输出可能

更关心输入

序列的某些

部分是什么

内容，

而和其

他部分有什

么关系并不

大。例如在机

器翻译中，当

前时间片的

输出可能仅

注重原句子

的某几

个单

词而不是整

个句子。

Bahdanau 等人

在他们的机

器翻译的论

文中率先提

出了注

意力

机制 2 的思想

，通过注意力

机制，模型可

以同时学习

原句

子和目

标句子的对

齐关系和翻

译关系。在编

码过程中，将

原

句子编码

成一组特征

向量的集合

，在翻译时，每

个时间片会

在该集合自

行选择特征

向量的一个

子集用于产

生输出结果

。

4.2.1 机器翻译的

注意力机制

在 Bahdanau

等人的论

文中，他们使

用的也是 RNN 编

码

器

- 解码器

结构。不同于

传统的单向

RNN 编码，在编码

过程

中，作者

使用的是双

向

RNN（bi-RNN），每个 RNN 单元

使用

的是

GRU，它

用于生成输

入数据的特

征向量。在解

码过程中，

作

者使用的是

基于注意力

机制的单向

GRU 结构，它用于

生成

最终的

翻译结果。算

法结构如图

4.12

所示。下面我

们详细介

绍

这两个模块

的具体内容

。

1　参见

Rafal Jozefowicz、Wojciech Zaremba、Ilya Sutskever

的论文

“An Empirical Exploration of

Recurrent Network Architectures”。

2

参见 Dzmitry Bahdanau、KyungHyun Cho、Yoshua

Bengio 的论文

“Neural Machine Translation

by Jointly Learning to

Align and 

Translate”。

图 4.12　算法结构

4.2　注意力机制

161

论文中的模

型分成编码

器和解码器

两个部分，编

码器用于将

输入数据编

码成特征向

量，使用的

是

含有注意力

机制的双向

GRU 结构，解码器

使用的是一

个单向的 GRU，下

面我们介绍

编码器和解

码器的具体

计算方式。

1．编

码器

双向 RNN 含

有正向和反

向两个方向

，对于含有

T 个

时间片的源

句子 X =

{x1, x2, …, xT}，正向

的

输入数据是

x1 → x2 →…→

xT，第 t 个时间片

的隐层节点

状态 表示为

式（4.8）：

（4.8）

反向数据

的输入序列

是 xT →

xT−1 →…→ x1，第 t

个时间

片的隐层节

点状态 表示

为式（4.9）：

（4.9）

其中，f

使

用的是 GRU 的单

元，则第 t

个时

间片的特征

ht 是前向和后

向两个特征

向量拼接到

一起

的结果

，表示为式（4.10）：

（4.10）

2．解

码器

在解码

的过程中，传

统的 RNN 编码器

-

解码器将整

个句子的特

征向量作为

输入，其中编

码器

的每个

时间片的特

征是没有差

别的，表示为

式（4.11）：

st

= f(st−1, yt−1,c) （4.11）

注意力模

型使用所有

特征向量的

加权和，通过

对特征向量

的权值的学

习，我们可以

使用对当前

时间片最重

要的特征向

量的子集 ci

，如

式（4.12）所示：

（4.12）

其中

，ci 是 ht 的加权和

，eit

是输出序列

第 i 个时间片

的对齐模型

，表示的是该

时刻和输入

数据每个

时

间片的相关

程度，使用前

一时刻的状

态

si−1 和第 t 个输

入数据

ht 计算

得到。在作者

的实验中，a

使

用的是 tanh

激活

函数。

4.2.2 图解注

意力机制

4.2.1

节

中我们对注

意力机制原

始论文中一

部分的核心

内容进行了

介绍，在本节

中，我们将使

用组图的形

式生动地展

示注意力机

制结构，并对

现在 NLP 领域里

比较流行的

注意力机制

进行梳理。

这

组图的动图

，读者可移步

至本书作者

的知乎专栏

查看。

1．背景知

识

我们最为

熟悉的神经

机器翻译（neural machine translation，NMT）模

型便是经典

的序列到序

列模

型 1

，这篇

论文从一个

序列到序列

模型开始介

绍，然后进一

步介绍如何

将注意力机

制应用到 NMT

中

。

在序列到序

列模型中，一

般使用两个

RNN，一个作为编

码器，另一个

作为解码器

：编码器的

作

用是将输入

数据编码成

一个特征向

量，解码器的

作用是将这

个特征向量

解码成预测

结果，如图

4.13

所

示。

1　参见 Ilya

Sutskever、Oriol Vinyals、Quoc V. Le

的论

文“Sequence to Sequence Learning

with Neural Networks”。

第

4 章  基础

序列模型

图

4.13　长度为 4 的序

列到序列模

型

这个模型

的问题是只

输出了编码

器的最后一

个节点的结

果，但是对一

个序列特别

长的模型来

说，这种方式

无疑会遗忘

大量前面时

间片的特征

，如图 4.14 所示。

解

码器

编码器

图 4.14　长度为 64

的

序列到序列

模型

既然如

此，我们为什

么不给解码

器提供更好

的特征呢？与

其输入最后

一个时间片

的结果，不如

将每个时间

片的输出都

提供给解码

器。那么让解

码器正确使

用这些特征

就是我们这

里介绍的注

意力

机制的

作用，如图 4.15

所

示。

图 4.15　在编码

器和解码器

中间添加一

个注意力层

在这里，注意

力层是介于

编码器和解

码器之间的

一个接口，用

于将编码器

的编码结果

以一种更

有

效的方式传

递给解码器

。一个特别简

单且有效的

方式就是让

解码器知道

哪些特征重

要，哪些特征

不重要，即让

解码器明白

如何对齐当

前时间片的

预测结果和

输入编码，如

图 4.16 所示。注意

力模

型学习

了编码器和

解码器的对

齐方式，因此

也被叫作对

齐模型（alignment

model）。

如 Luong 等

人

1 的论文中

介绍的，注意

力有两种类

型，一种作用

到编码器的

全部时间片

，这种

注意力

叫作全局注

意力（global attention），另一种

只作用到时

间片的一个

子集，叫作局

部注意力

（local attention），这

里要介绍的

注意力都是

全局的。

1　参见

Minh-Thang

Luong、Hieu Pham、Christopher D. Manning

的论文“Effective Approaches to Attention-based

Neural Machine 

Translation”。

162

4.2　注意

力机制

图

4.16　注

意力机制中

的对齐，线的

颜色越深表

示权重越大

2．注意力介绍

根据上面的

介绍，注意力

的计算可以

分为 6 步。

第一

步：生成编码

节点。

将输入

数据依次输

入 RNN 中，得到编

码器每个时

间片的隐层

状态的编码

结果（绿色），并

将

编码器的

最后一个输

出作为解码

器的第一个

输入隐层状

态（红色）。

在图

4.17 所示的例子

中，有 4

个编码

器的隐层状

态和 1 个解码

器的隐层状

态。

图

4.17　生成编

码节点（有动

图）

第二步：为

每个编码器

的隐层状态

计算一个得

分。

使用当前

编码器的当

前时间片的

隐层状态和

解码器的隐

层状态计算

一个得分，如

图

4.18 所示，

得分

的计算方式

有多种（见 4.2.3

节

），这里使用的

是点乘操作

。

解码器隐层

节点状态 = [10,

5, 10]

编

码器隐层节

点状态得分

---------------------

[0, 1, 1] 15

(= 10x0 + 5x1

+ 10x1, 点乘)

[5, 0, 1] 60

[1, 1, 0] 15

[0, 5, 1] 35

163

第 4 章

基

础序列模型

图 4.18　为每个编

码器的隐层

状态计算一

个得分（有动

图）

第三步：使

用

softmax 对得分进

行归一化。

将

softmax 作用到第二

步的得分之

上，得到和为

1

的分数，如图

4.19 所示。下面例

子给出的 0

和

1

其实不是很

符合真实情

况，因为在实

际场景中这

个值往往是

介于 0 和 1

之间

的一个浮点

数。

图 4.19　使用

softmax 对

得分进行归

一化（有动图

）

编码器隐层

节点状态 得

分

softmax得分

-----------------------------

 [0,

1, 1] 15 0

[5, 0, 1] 60

1

 [1, 1,

0] 15 0

[0, 5, 1] 35

0

第四

步：使用得分

对隐层状态

进行加权。

将

得分和隐层

状态进行点

乘操作，得到

加权之后的

特征，这个特

征也叫作对

齐特征（alignment

vector）或者

注意力特征

（attention vector），如图 4.20 所示。

164

4.2　注

意力机制

图

4.20

使用得分对

隐层状态进

行加权（有动

图）

编码器隐

层节点状态

得分 softmax得分 对

齐

---------------------------------

 [0, 1,

1] 15 0 [0,

0, 0]

 [5,

0, 1] 60 1

[5, 0, 1]

[1, 1, 0] 15

0 [0, 0, 0]

[0, 5, 1] 35

0 [0, 0, 0]

第五步：特

征相加。

这一

步是将加权

之后的特征

相加，得到最

终的编码器

的特征向量

，如图 4.21 所示。

图

4.21　特征相加（有

动图）

编码器

隐层节点状

态 得分

softmax得分

对齐

---------------------------------

 [0,

1, 1] 15 0

[0, 0, 0]

[5, 0, 1] 60

1 [5, 0, 1]

[1, 1, 0] 15

0 [0, 0, 0]

[0, 5, 1] 35

0 [0, 0, 0]

内容向

量= [0+5+0+0, 0+0+0+0, 0+1+0+0]

= [5, 0, 1]

165

第 4 章

基础

序列模型

166

第

六步：将特征

向量应用到

解码器。

最后

一步是将含

有注意力的

编码器编码

的结果提供

给解码器进

行解码，解码

过程如图

4.22 所

示。注意，每个

时间片的注

意力的结果

会随着解码

器隐层节点

状态的改变

而更改。

图 4.22

解

码过程（有动

图）

4.2.3 经典注意

力模型

在介

绍不同的论

文中提到的

不同的注意

力之前，我们

先看一下常

用的注意力

得分的计算

方式，

如图 4.23 所

示。基于内容

的注意力（content based

attention）是

在神经图灵

机 1 中提出的

基于内容相

似度的注意

力机制，采用

这种方式得

到的高权值

特征向量往

往意味着编

码器隐层状

态和解码器

隐层

状态拥

有很高的相

似度。另外点

乘（dot-product）2

和缩放点

乘（scaled dot-product）3 也起着相

似度度量

的

作用。表

4.1 是常

用的注意力

得分计算方

式的汇总。4

表

4.1　常用的注意

力得分计算

方式的汇总

名称

对齐评

分函数 出处

基于内容相

似度 score(st

,hi

) = cos[st

,hi

] (Graves, Wayne, and

Danihelka 2014)1

相加 (Bahdanau,

Cho, and Bengio 2014)4

基

于位置 at;i

 =

softmax(Wa St

) (Luong,

Pham, and Manning 2015)2

通用

(Luong, Pham, and Manning

2015)2

点乘 (Luong, Pham,

and Manning 2015)2

缩放点

乘

(Vaswani et al. 2017)3

1　参见 Alex Graves、Greg

Wayne、Ivo Danihelka 的论

文“Neural Turing

Machines”。

2　参见 Minh-Thang

Luong、Hieu Pham、Christopher D. Manning

的论

文“Effective Approaches to Attention-based

Neural Machine 

Translation”。

3　参见 Ashish Vaswani、Noam

Shazeer、Niki Parmar 等人

的论文“Attention is

All You Need”。

4

参见

Dzmitry Bahdanau、KyungHyun Cho、Yoshua Bengio

的论文“Neural Machine Translation by

Jointly Learning to Align

and Translate”。

4.2　注意

力机制

图 4.23　常

用的注意力

得分的计算

方式

下面我

们介绍

3 个经

典的注意力

模型，如图 4.24、图

4.25、图 4.26

所示，并给

出这 3 个模型

的双语评估

替补（billingual evaluation

understudy，BLEU）值。

1．Bahdanau 等人

的注意力模

型

z

我们刚刚

介绍过，它的

编码器使用

的是双向 GRU，解

码器使用的

是单向 GRU，解码

器的

初始化

输入是反向

GRU

的输出。

z 注意

力操作选择

的是相加或

者拼接。

z

解码

器的输入特

征是由上一

个时间片的

预测结果和

解码器的编

码结果拼接

而成的。

z BLEU 值为

26.75。

图 4.24 Bahdanau 等人的注

意力模型

167

第

4 章

基础序列

模型

2．Luong 等人的

注意力模型

z 编码器和解

码器都使用

两层的

LSTM。

z 解码

器的初始化

隐层状态分

别是两个解

码器的最后

一个时间片

的输出。

z

在论

文中他们尝

试了相加 / 拼

接、点乘、基于

位置和通用

等得分计算

方式。

z

将解码

器得到的结

果和编码器

的结果进行

拼接，送入一

个前馈神经

网络中得到

最终的

结果

。

z BLEU

值为 25.9。

图 4.25

Luong 等人

的注意力模

型

3．GNMT 的注意力

模型

z 编码器

使用一个 8 层

的

LSTM。第一层是

双向的 LSTM，把它

们的特征拼

接后提供给

第二

层，在后

面的每一层

LSTM 中都使用残

差结构进行

连接。

z 解码器

使用 8 层单向

LSTM

并使用残差

结构进行连

接。

z 得分计算

式和 Bahdanau

等人的

注意力模型

的相同，为相

加或者拼接

。

z 拼接方式也

和 Bahdanau

等人的注

意力模型的

相同。

z 英法翻

译的 BLEU

为 38.95，英德

翻译的 BLEU 为

24.17。

168

4.2　注

意力机制

图

4.26 GNMT 的注意力模

型 1

1　参见 Yonghui Wu、Mike

Schuster、Zhifeng Chen 等人

的论文“Google’s Neural

Machine Translation System: Bridging

the 

Gap between

Human and Machine Translation”。

169

第 4 章



基础序列模

型

170

4.2.4 小结

注意

力机制是当

前深度学习

中非常实用

的一个模块

，注意力机制

虽然简单，但

其中蕴含了

一些

使用技

巧。这篇论文

通过生动的

图示（动图），将

注意力机制

的作用讲得

非常清楚，对

于初次接触

注意力机制

的读者帮助

还是很大的

。另外，本节介

绍的 3 个注意

力模型是非

常经典的机

器翻译框

架

，其中的网络

结构给后面

类似的模型

提供了很好

的参考。

4.3　Transformer

在本

节中，先验知

识包括：

 LSTM 和 GRU（4.1

节

）；  注意力机制

（4.2 节）；

 残差网络

（1.4 节）。

注意力机

制由

Bengio 团队于

2014 年提出并在

近年广泛地

应用在深度

学习中的各

个领域，例如

在 CV

领域中用

于捕捉图像

上的感受野

，或者在 NLP 领域

中用于定位

关键标志或

者特征。Google

团队

提出的用于

生成词向量

的

BERT1 算法在 NLP 的

11

项任务中实

现了效果的

大幅提升，这

堪称

2018 年深度

学习领域最

振奋人心的

消息。而 BERT

算法

最重要的部

分便是本文

提出的 Transformer。

正如

论文的题目

所说的，Transformer 抛弃

了传统的

CNN 和

RNN，整个网络结

构完全由注

意

力机制组

成。更准确地

讲，Transformer由且仅由

自注意力（self-attenion）模

块和前馈神

经网络（feed

forward neural network，FFNN）组成

。一个基于 Transformer

的

可训练的神

经网络可以

通过堆叠

Transformer 的

方式搭建，作

者在实验中

搭建了编码

器和解码器

各 6

层、总共 12 层

的编码器 -

解

码

器结构，并

在机器翻译

中创造 BLEU 值的

新高。

作者采

用注意力机

制的原因是

考虑到 RNN（或者

LSTM、GRU 等）的计算是

按顺序进行

的，也就是说

RNN 相关算法只

能从左向右

依次计算或

者从右向左

依次计算，这

种机制带来

了两个

问题

：

z 时间片 t

的计

算依赖 t − 1

时刻

的计算结果

，这样限制了

模型的并行

能力；

z 顺序计

算的过程中

信息会丢失

，尽管 LSTM

等门机

制的结构在

一定程度上

解决了长期

依

赖的问题

，但是对于特

别长期的依

赖现象 LSTM 也无

能为力。

Transformer 解决

了上面两个

问题，首先它

使用了注意

力机制，将序

列中任意两

个位置之间

的距

离缩小

为一个常量

；其次它的结

构不是类似

RNN 的顺序结构

，因此具有更

好的并行性

，符合现有

的

GPU 框架。论文中

给出的 Transformer 的定

义是：Transformer

is the first transduction

model relying 

entirely

on Self-Attention to compute

representations of its input

and output without using

sequence aligned 

RNNs

or convolution（Transformer 是第一

个没有使用

序列对齐 RNN

或

卷积输入和

输出，完全依

赖

自注意力

机制的转换

模型）。

遗憾的

是，作者的论

文比较难懂

，对于 Transformer

的结构

细节和实现

方式并没有

解释清楚。

尤

其是论文中

的 Q、V、K 究竟代表

什么，作者并

没有说明。本

书借鉴了

Jay Alammer 在

其博客中

对

Transfomer

的解读，感兴

趣的读者可

搜索学习。

1　参

见 Jacob

Devlin、Ming-Wei Chang、Kenton Lee 等人的论

文“BERT:

Pre-training of Deep Bidirectional

Transformers for Language

Understanding”。

4.3　Transformer

4.3.1

Transformer 详解

1．高层

Transformer

论文中验证

Transformer

的实验是基

于机器翻译

的，下面我们

就以机器翻

译为例详细

剖析

Transformer 的结构

。在机器翻译

中，Transformer 的输入是

原句子，输出

是翻译结果

，可概括为图

4.27。

图 4.27 Transformer 用于机器

翻译

Transformer 的结构

本质上是一

个编码器 - 解

码器结构，那

么图

4.27 可以表

示为图 4.28 所示

的结构。

图 4.28 Transformer 的

编码器

- 解码

器结构

如论

文中所设置

的，编码器由

6 个编码块组

成，同样解码

器由

6 个解码

块组成。与所

有的生成

模

型相同的是

，编码器生成

的特征向量

会作为解码

器的输入，如

图 4.29

所示。

图 4.29 Transformer

的

编码器和解

码器均由 6 个

网络块堆叠

而成

171

第 4 章

基

础序列模型

172

我们继续分

析每个编码

器的详细结

构：在 Transformer 的编码

器中，数据首

先会经过一

个叫作

自注

意力的模块

得到一个加

权之后的特

征向量 Z，这个

Z 便是论文的

式 1

中的 Attention(Q,K,V)，

如式

（4.13）所示。

（4.13）

第一次

看到式（4.13）你可

能会一头雾

水，在后文中

我们会揭开

它背后的实

际含义，在这

一段

暂时将

其叫作 Z。

得到

Z

之后，将 Z 送到

编码器的下

一个模块，即

FFNN。这个全连接

网络有两层

，第一层的激

活函数是 ReLU，第

二层的激活

函数是线性

的，可以表示

为式（4.14）。

FFNN(Z) = max(0,ZW1 +

b1)W2 + b2 （4.14）

编码器

的结构如图

4.30 所示，Transformer 的编码

器由自注意

力和 FFNN

组成。

图

4.30　编码器的结

构

解码器的

结构如图

4.31 所

示，Transformer 的解码器

由自注意力

、编码器 -

解码

器注意力以

及

FFNN 组成，它和

编码器的不

同之处在于

多了一个编

码器 -

解码器

注意力（encoder-decoder attention），

两个

注意力分别

用于计算输

入和输出的

权值。

z

自注意

力：当前翻译

和已经翻译

的前文之间

的关系。

z 编码

器 -

解码器注

意力：当前翻

译和编码的

特征向量之

间的关系。

图

4.31　解码器的结

构

2．输入编码

第一部分介

绍的是

Transformer 的主

要框架，下面

我们将介绍

它的输入数

据。单词的输

入编码

如图

4.32 所示，首先通

过

Word2vec 等词嵌入

方法将输入

语料转化成

特征向量，论

文中使用的

词嵌

入的维

度 dmodel

= 512。

图 4.32

单词的

输入编码

4.3　Transformer

在

最底层的网

络块中，x

将直

接作为 Transformer 的输

入，而在其他

层中，输入则

是上一个网

络块的输出

。为了使画图

简单，我们使

用简单的例

子来表示接

下来的过程

，如图 4.33

所示。

图

4.33　输入编码作

为一个张量

输入编码器

中

3．自注意力

自注意力是

Transformer

最核心的内

容，然而作者

并没有详细

讲解，下面我

们来补充一

下作者

遗漏

的地方。回想

Bahdanau 等人提出的

注意力机制

，其核心内容

是为输入向

量的每个单

词学习一

个

权重，例如在

下面的例子

中我们判断

it

指代的内容

。

The animal did

not cross the street

because it was too

tired

通过加权之

后可以得到

类似图 4.34 所示

的加权情况

，在讲解自注

意力的时候

我们也会使

用与

图 4.34 类似

的表示方式

。

图

4.34　经典注意

力模块可视

化示例

在自

注意力模块

中，每个单词

有 3

个不同的

向量，它们分

别是查询向

量（Query，Q）、键向量

（Key，K）和

值向量（Value，V），长度

均是 64，Q、K、V 的计算

示例如图

4.35 所

示。它们是由

嵌

入向量 X

乘

3 个不同的权

值矩阵 WQ、WK、WV 得到

的，这

3 个矩阵

的尺寸是相

同的，均是 512 ×

64。

173

第

4 章

基础序列

模型

174

图 4.35

Q、K、V 的计

算示例

那么

查询、键、值是

什么意思呢

？它们在自注

意力模块的

计算中扮演

着什么角色

呢？我们先看

一下自注意

力模块的计

算过程，整个

过程可以分

成 7

步。

（1）如上文

，将输入单词

转化成嵌入

向量。

（2）根据嵌

入向量得到

q、k、v 这

3 个向量。

（3）为

每个向量计

算一个得分

：score =

q·k。

（4）为了梯度的

稳定，Transformer 使用了

得分归一化

，即除以 。

（5）对得

分施以 softmax 激活

函数。

（6）softmax

点乘值

v，得到加权的

每个输入向

量的得分 v。

（7）将

得分相加之

后得到最终

的输出结果

。

上面的步骤

可以表示为

图

4.36。

图 4.36　自注意

力计算示例

4.3

Transformer

实际计算过

程中采用基

于矩阵的计

算方式，那么

论文中的 Q、K、V 的

矩阵表示如

图

4.37 所示。

图 4.37

总

结为图 4.38 所示

的矩阵表示

。

图

4.37 Q、K、V 的矩阵表

示

图

4.38 所示的

也就是式（4.13）的

计算方式。

图

4.38　自注意力的

矩阵表示

在

自注意力中

需要强调的

最后一点是

其采用了残

差网络中的

捷径结构，目

的是解决深

度学习中

的

退化问题，自

注意力中的

捷径连接如

图 4.39 所示。

图 4.39　自

注意力中的

捷径连接

175

第

4 章  基础序列

模型

176

查询、键

、值的概念取

自信息检索

系统。举个简

单的搜索例

子，当你在某

电商平台搜

索某件商

品

（如年轻女士

冬季穿的红

色薄款羽绒

服）时，你在搜

索引擎中输

入的内容便

是查询，然后

搜索引

擎根

据查询为你

匹配键（如商

品的种类、颜

色、描述等），再

根据查询和

键的相似度

得到匹配的

值

（商品内容

）。

自注意力中

的 Q、K、V 也起着类

似的作用，在

矩阵计算中

，点乘是计算

两个矩阵相

似度的方

法

之一，因此式

（4.13）中使用了 QKT 进

行相似度的

计算。接着便

根据相似度

进行输出的

匹配，这

里使

用了加权匹

配的方式，而

权值就是查

询与键的相

似度。

4．多头注

意力

多头注

意力（multi-head attention）相当于

h 个不同的自

注意力的集

成，在这里我

们以

h = 8 举

例说

明。多头注意

力的输出分

成 3 步：

（1）将数据

X

分别输入 8 个

自注意力中

，得到 8

个加权

后的特征矩

阵 ；

（2）将 8

个 Zi 按列

拼成一个大

的特征矩阵

；

（3）特征矩阵经

过一层全连

接后得到输

出

Z。

多头注意

力的整个过

程如图 4.40 所示

。

图 4.40　多头注意

力的整个过

程

同自注意

力一样，多头

注意力也加

入了捷径机

制。

5．编码器 - 解

码器注意力

在Transformer中，解码器

比编码器多

了个编码器

-解码器注意

力。在编码器

-解码器注意

力中，

Q

来自解

码器的输出

，K 和 V 则来自编

码器的输出

。其计算方式

完全和图

4.36 所

示的相同。

由

于在机器翻

译中，解码过

程是顺序操

作的，即当解

码第 k

个特征

向量时，我们

只能看到

第

k − 1

个及其之前

的解码结果

，论文中把这

种情况下的

多头注意力

叫作掩码多

头注意力（masked 

multi-head attention）。

6．损

失层

解码器

解码之后，解

码的特征向

量经过激活

函数为 softmax 的全

连接层之后

得到反映每

个单词

概率

的输出向量

。此时我们便

可以通过 CTC 等

损失函数训

练模型了。

4.3

Transformer

而

一个完整可

训练的网络

结构便是编

码器和解码

器（各 N 个，N

= 6）堆叠

而成的，通过

堆叠

的方式

我们可以得

到图 4.41

所示的

Transformer 的完整结构

（即论文中的

图 1）。

图

4.41 Transformer 的完整

结构

4.3.2

位置嵌

入

截至目前

，我们介绍的

Transformer 模型并没有

捕捉顺序序

列的能力，也

就是说无论

句子的结

构

被怎么打乱

，Transformer

都会得到类

似的结果。换

句话说，Transformer 只是

一个功能更

强大的

词袋

模型而已。为

了解决这个

问题，论文中

在编码词向

量时引入了

位置嵌入（position embedding）

的

特征。具体地

说，位置嵌入

会在词向量

中加入单词

的位置信息

，这样 Transformer 就能区

分不同

位置

的单词了。

那

么怎么编码

位置信息的

呢？常见的方

式有根据数

据学习和自

己设计编码

规则。在这里

作者采

用了

第二种方式

。那么位置嵌

入该是什么

样子的呢？通

常位置嵌入

是一个长度

为 dmodel 的特征向

量，

这样便于

和词向量进

行单位加的

操作，如图 4.42 所

示。

177

第 4 章

基础

序列模型

178

图

4.42　位置嵌入

论

文给出的编

码式如式（4.15）所

示：

（4.15）

在式（4.15）中，pos 表

示单词的位

置，i

表示单词

的维度。关于

位置嵌入的

实现可在 Google

开

源的代码中

get_timing_signal_1d() 函数处找到

对应的代码

。

作者这么设

计是因为考

虑到在 NLP 任务

中，除了单词

的绝对位置

，单词的相对

位置也非常

重

要。根据式

sin(α

+ β) = sinαcosβ

+ cosαsinβ 和 cos(α

+ β) = cosαcosβ

+ sinαsinβ，表明位置

k + p

的位置

向量

可以表示为

位置 k 的特征

向量的线性

变化，这为模

型捕捉单词

之间的相对

位置关系提

供了非常

大

的便利。

4.3.3 小结

Transformer 优点如下。

（1）Transformer 最

终没有逃脱

传统深度学

习的“套路”，只

是一个全连

接（或者是一

维卷积）

加注

意力的结合

体。但是其设

计已经足够

创新，因为其

抛弃了在 NLP

中

最根本的 RNN 或

者 CNN

并且取得

了非常不错

的效果。算法

的设计非常

精彩，值得每

个深度学习

开发相关的

人员仔细研

究和

品位。

（2）Transformer 带

来性能提升

的关键是它

使任意两个

单词的距离

是

1，这对解决

NLP 中棘手

的长

期依赖问题

是非常有效

的。

（3）Transformer

可以应用

在 NLP 的机器翻

译领域，是一

个非常有科

研潜力的方

向。

（4）Transformer

算法的并

行性非常好

，符合目前的

硬件（主要指

GPU）环境要求。

Transformer 缺

点如下。

（1）粗暴

地抛弃

RNN 和 CNN 虽

然非常“炫技

”，但是这使模

型丧失了捕

捉局部特征

的能力，

4.4　Transformer-XL

179

RNN

+ CNN + Transformer

的结

合体可能会

带来更好的

效果。

（2）Transformer 失去的

位置信息在

NLP 中非常重要

，而论文中将

位置嵌入加

入特征向量

只是

一个权

宜之计，并没

有改变 Transformer 结构

上的固有缺

陷。现阶段，优

化位置嵌入

是一个非常

活

跃的科研

方向。

4.4　Transformer-XL

在本节

中，先验知识

包括：



Transformer （4.3 节）； 

残差

网络（1.4 节）。

序列

模型捕获数

据长期依赖

的能力在任

何 NLP

任务中都

是至关重要

的，LSTM 通过引进

门机

制将 RNN

的

捕获长期依

赖的能力提

升到 200 个左右

，Transformer 则进一步提

升了捕获长

期依赖的

能

力，但是 Transformer 的捕

获长期依赖

的能力是无

限提升的吗

？如果有一个

需要具备捕

获几千个

时

间片的能力

的模型才能

完成的任务

，Transformer

能够完成吗

？从目前 Transformer 的设

计来看，

它是

做不到的。

这

篇论文介绍

的 Transformer-XL（extra long）1 则可进一

步提升

Transformer 捕获

长期依赖的

能

力。它的核

心算法包含

两部分：片段

递归（segment-level recurrence）机制和

相对位置编

码（relative

positional encoding）机制。Transformer-XL 带来

的提升包括

：

z

提升捕获长

期依赖的能

力；

z 解决上下

文碎片问题

（context segmentation

problem）；

z 提升模型的

预测速度和

准确率。

本节

含有大量图

片，用于形象

地解释

Transformer-XL 的算

法原理，图片

对应的动图

读者可搜索

Google 在其官网发

表的一篇文

章“Transformer-XL: Unleashing

the Potential of Attention

Models”。

4.4.1 Transformer 的缺点

在

4.3 节中，我们介

绍了 Transformer 的基本

原理，在了解

Transformer-XL

之前我们先

看一下

Transformer 的缺

点。

1．输入

NLP 相关

的任务都很

难避免处理

输入为变长

数据的场景

，处理变长数

据的方案有

两个，一是

将

数据输入类

似前馈神经

网络的模型

中得到长度

固定的特征

向量，这个方

案往往因为

计算资源的

限

制很难执

行；另一个是

通过数据分

段或者加边

的方式将数

据填充到固

定长度。Transformer

采取

的

便是第二

个方案，这个

固定长度用

L 来表示，L 的值

在

Transformer 的论文中

为 512。

将数据分

完段之后，接

下来便是将

分段的数据

依次输入网

络中进行模

型的训练，如

图

4.43

所示。

这种

分段式的提

供数据的方

式的一个很

大的问题是

数据并不会

在段与段之

间流通，因此

模型能

够捕

获的长期依

赖的上限便

是段的长度

。另外这种将

数据分段，而

不考虑段与

段之间的关

系的方式

是

非常粗暴的

，模型的能力

无疑是要“打

折”的。这个问

题便是我们

所说的上下

文碎片问题

。

1　参见 Zihang

Dai、Zhilin Yang、Yiming Yang 等人的

论文“Transformer-XL:

Attentive Language Models Beyond

a Fixed￾Length Context”。

第

4 章  基

础序列模型

180

图 4.43 Transformer 对变长数

据的处理方

式（有动图）

2．自

注意力

这里

以单头 Transformer 为例

进行说明，对

于一个长度

为

n 的输入序

列 x =

(x1,…,xn)，通过

Transformer 得到

的序列为 Z

= (Z1,…,Zn)，Zi 的

计算方式为

x 中各元素的

加权和，如式

（4.16）所示：

（4.16）

权值 αi, j

是

通过 softmax 运算得

到的如式（4.17）所

示：

（4.17）

ei, j 则是通过

Q、K 两个矩阵得

到的如式（4.18）所

示：

（4.18）

其中，WQ、WK、WV 是 3

个

权值矩阵。

3．测

试

Transformer 是一个自

回归模型，也

就是说在测

试时依次取

时间片为

L 的

分段，然后将

整个分

段提

供给模型后

预测一个结

果，如图 4.44

所示

。在下个时间

片时再将这

个分段向右

移一个单位

，

这个新的片

段也将通过

整个网络的

计算得到一

个值。Transformer 的这个

特性导致其

预测阶段的

计

算量是非

常大的，这也

限制了其应

用领域。

4．绝对

位置编码

Transformer 的

位置编码是

以分段为单

位的，它使用

的是无参数

的 sinusoid

编码矩阵

，表示为

，Ui 表示

的是在这个

分段中第 i

个

元素的相对

位置，Lmax 表示的

是能编码的

最大长度。然

后这个位置

编码会通过

单位加的形

式和词嵌入

（word embedding）合并成一个

矩阵，表示为

式（4.19）：

（4.19）

其中， 表示

第 t 个片段

sτ 的

词嵌入，f 表示

转换方程。从

式（4.19）中我们可

以看出，对

第

t

个和第 t + 1

个片

段来说，它们

的位置编码

是完全相同

的，我们完全

无法确认它

们属于哪个

片段

或者说

它们在分段

之前的输入

数据中的相

对位置。

4.4　Transformer-XL

181

图 4.44 Transformer

的

推理过程（有

动图）

在 Transformer 中，自

注意力可以

表示为式（4.20）1

：

（4.20）

考

虑到词嵌入

，QT

K

的完整表达

式为式（4.21）：

（4.21）

我们

使用乘法分

配律将其展

开，展开式（4.22）会

在后面使用

：

（4.22）

Transformer 的问题是无

论对于第几

个片段，它们

的位置编码

U1:L 都是一样的

，也就是说，

Transformer

的

位置编码是

相对于片段

的绝对位置

编码（absolute position encoding），与当前

内容在

原始

句子中的相

对位置是没

有关系的。

4.4.2 相

对位置编码

最先介绍相

对位置编码

（relative positione encoding，RPE）的是

Shaw P 等人的

论文 2

。对比

RNN 系

列的模型，Transformer 的

一个缺点是

没有从网络

结构上对位

置信息进行

处理，而只是

把位

置编码

加入输入层

。RPE 的目的就是

弥补 Transformer 的这个

天然缺陷，它

的做法是把

相对位置编

码加入自注

意力模型的

内部。

如图 4.45 所

示，输入序列

为“I think

therefore I am”，对 RNN

来说两

个“I”接收到的

信息是

不同

的，第一个“I”接

收的隐层状

态是初始化

的值，第二个

“I”接收的隐层

状态是经过

“I think

therefore”编码的。

而对

Transformer 来说，在没有

位置编码的

情况下，尽管

两个“I”在句子

中的位置不

同，但是

1

在 Transformer 的

式中，权值的

计算方式为

QKT

，而

Transformer-XL 论文中使

用的是 QT

K。这里

区别其实不

是很大，

因为

它们本质上

都是通过点

乘计算相似

度的。部分论

文对 Transformer-XL 的表达

式进行了修

正。

2

参见 Peter Shaw、Jakob Uszkoreit、Ashish

Vaswani 的论

文“Self-Attention with Relative

Position Representations”。

第 4

章  基础

序列模型

182

两

个“I”的输入信

息是完全一

致的，如图 4.46 所

示。正如我们

在分析 Transformer

的论

文时所知，只

在输入中加

入位置信息

显然是不够

的，Transformer 还应该在

其结构中加

入序列信息

。这样做的好

处

是当我们

在计算权值

或者特征值

的时候，可额

外添加位置

信息，这将有

助于这两个

变量的计算

。

图

4.45 RNN 结构具有

编码相对位

置的能力 图

4.46

Transformer 不具有编码

相对位置的

结构特征

RPE 提

出的模型的

原理是在计

算第

i 个元素

与第 j 个元素

之间的注意

力的值和权

值的时候加

入

i 与 j 之间的

距离编码，因

为加入的是

i

与 j 之间的相

对位置关系

，所以叫作相

对位置编码

。

例如对一个

长度为

5 的序

列，它共有 9 个

相对位置编

码信息（当前

位置编码、当

前位置的前

4

个位置解码

和当前位置

的后 4 个位置

解码），如表 4.2

所

示。

表 4.2　长度为

5

的序列，其 9 个

相对位置编

码信息

索引

说明

值

0 位置

i 与位置

i − 4 之间

的距离

−4

1 位置

i 与位置

i − 3 之间

的距离

−3

2 位置

i 与位置

i − 2 之间

的距离

−2

3 位置

i 与位置

i − 1 之间

的距离

−1

4 位置

i 与位置

i 之间

的距离 0

5

位置

i 与位置 i +

1 之间

的距离 1

6

位置

i 与位置 i +

2 之间

的距离 2

7

位置

i 与位置 i +

3 之间

的距离 3

8

位置

i 与位置 i +

4 之间

的距离 4

通过

加入上面的

相对位置编

码信息，我们

再对比一下

“I

think therefore I am”中的两个“I”有

什么不同，如

图

4.47 所示。图 4.47（a）所

示的是第一

个“I”的相对位

置编码信息

，图 4.47（b）

所示的是

第二个“I”的相

对位置编码

信息，RPE 并没有

根据输入序

列的长度来

确定需要考

虑的相

对位

置之间的距

离，而是用了

一个固定的

常数 k，也就是

说我们需要

学习的相对

位置编码的

序列长

度为

2k + 1。对于 k

的取值

，论文中给出

了不同值的

对照实验结

果，结论是当

k ≥ 2 时，得到的效

果非常接近

。

图 4.47　加入相对

位置编码后

RPE 需要学习两

个相对位置

向量，一个是

用于计算第

i

个词的特征

Zi

，另一个是用

于计算第 i

4.4

Transformer-XL

183

个

词到第 j

个词

的权值系数

ei,j

，不同于投影

矩阵，这两个

嵌入向量在

注意力头之

间是共享的

。

对比 4.4.1

节自注

意力部分的

式（4.17）和式（4.18），RPE 在自

注意力中添

加了两个可

学习的

变量

和 。其中

Zi 的计

算方式改为

式（4.23）。

（4.23）

ei,

j 的变化和

Zi 的基本相同

，如式（4.24）所示。

（4.24）

这

里用加法的

原因是可使

计算效率更

高。 和 的计算

方式相同，即

在 [

− k,k] 内计算相

对距

离，超出

范围的用

0 或

者 k 进行截断

，如式（4.25）所示。

（4.25）

4.4.3 Transformer-XL 详

解

Transformer-XL 旨在解决

Transformer 的上下文碎

片、推理速度

慢和长期依

赖这 3

个问题

，为了

解决上

下文碎片和

推理速度慢

的问题，作者

推出了片段

递归机制，为

了解决长期

依赖问题，作

者对

绝对位

置编码进行

了改进，并推

出了相对位

置编码机制

。下面我们将

分别详细介

绍这两个优

化点。

1．片段递

归

和 Transformer 一样，Transformer-XL 在

训练的时候

也是以固定

长度的片段

作为输入的

，不同

的是 Transformer-XL 的

上个片段的

隐层状态会

被缓存下来

，然后在计算

当前片段的

时候重复使

用上

个时间

片的隐层状

态。因为上个

片段的特征

在当前片段

重复使用，也

就赋予了

Transformer-XL 建

模

长期依赖

的能力。

那么

Transformer-XL

是如何重用

上个片段的

隐层状态的

呢？我们通过

数学的形式

具体说明。长

度为 L 的两个

连续片段表

示为 sτ

= [xτ∶1,…,xτ∶L] 和 sτ+1

= [xτ+1∶1,…,xτ+1∶L]。sτ 的隐

层节点的状

态表示为

，其

中

d 是隐层节

点的维度。sτ+1 的

隐层节点的

状态 的计算

过程如下：

其

中，SG(·) 表示停止

求梯度（stop-gradient），即表

示这一部分

并不参与反

向传播的计

算，

表示两个

隐层节点在

长度维度进

行拼接，W 是模

型需要学习

的参数。注意

，

和 使用

的是

扩展了上个

片段的隐层

状态的 ，而

使

用的是未拼

接的 。Transformer-XL 的训练

过程如

图

4.48 所

示。

片段递归

的另一个好

处是提升推

理速度，对比

Transformer 的自回归架

构每次只能

前进一个时

间片，Transformer-XL

的推理

过程（见图 4.49）通

过直接复用

上个片段的

表示而不是

从头计算，将

推理过程提

升到以片段

为单位进行

推理，这种简

化带来的速

度提升是成

百上千倍的

。

Transformer-XL 是一个典型

的用空间换

时间的方案

，因为这个方

案需要对上

个片段的隐

层状态

进行

缓存，无疑将

增大模型的

显存占用量

，但依照目前

硬件的发展

速度来看，对

于速度和准

确率都

大幅

提升的模型

，显存是不会

成为它的瓶

颈的。而且只

要显存足够

大，我们可以

复用更多的

之前片

段的

隐层状态。

第

4

章  基础序列

模型

184

图 4.48 Transformer-XL 的训

练过程（有动

图）

图 4.49 Transformer-XL 的推理

过程（有动图

）

从这个角度

看，Transformer-XL 是一个和

残差网络思

想非常接近

的模型，它相

当于在两个

片段

之间添

加了一条捷

径。而复用更

多片段的结

构则是采用

DenseNet 思想的模型

。

2．Transformer-XL 的相对位置

编码

Transformer-XL 的相对

位置编码参

考了

RPE 中把相

对位置编码

加入自注意

力中的思想

。

Transformer-XL 在

4.4.1 节式（4.22）的基

础上做了若

干改变，得到

了如式（4.26）所示

的计算方法

。

（4.26）

z

第一个变化

出现在了（a）、（b）、（c）、（d）中

，Wk 被拆分成 Wk,E 和

Wk,R，也就是说输

入

序列和位

置编码不再

共享权值。

z 第

二个变化在

（b）和（d）中，将绝对

位置编码 Uj

换

成了相对位

置编码 Ri − j

，其中

R 是

Transformer 中采用的

不需要学习

的

sinusoid 编码矩阵

，原因正如 4.4.2 节

所介绍的，相

对

位置比绝

对位置更为

重要。

4.4　Transformer-XL

185

z 第 三 个

变

化 在（c）、（d） 中， 引

入

了 两 个 新

的

可 学 习 参

数

和 来替换

Transformer 中的查询向

量 ，表明对于

所有的查询

位置，对应的

查询（位置）向

量是

相同的

，即无论查询

位置如何，对

不同词的注

意力偏差都

保持一致。

改

进之后式（4.26）中

的 4 个部分有

了各自的含

义。

（a）没有考虑

位置编码的

原始分数，只

是基于内容

的寻址。

（b）相对

于当前内容

的位置偏差

。

（c）从内容层面

衡量键的重

要性，表示全

局的内容偏

置。

（d）从相对位

置层面衡量

键的重要性

，表示全局的

位置偏置。

式

（4.26）使用乘法分

配律得到的

表达式如式

（4.27）所示。

（4.27）

4.4.4 小结

Transformer 由

于自回归的

特性，每个时

间片的预测

都需要从头

开始，这样的

推理速度限

制了

它在很

多场景的应

用。Transformer-XL 提出的片

段递归机制

，使得推理过

程以段为单

位，段的长度

越长，无疑提

速越明显。从

实验结果来

看，Transformer-XL

提速了 300 到

1 800

倍，为 Transformer￾XL 的使用

提供了基础

支撑。同时递

归机制增加

了 Transformer-XL

可建模的

长期依赖的

长度

（O(NL)），这对提

升模型的泛

化能力也是

很有帮助的

。

仿照 RPE，Transformer-XL

提出了

自己的相对

位置编码算

法，此编码算

法对比 Transformer

和 RPE

的

编码算法有

性能上的提

升，而且从理

论角度有可

解释性。Google 推出

的 XLNet 也以

Transformer-XL 为基

础，我们会在

后文中进行

分析。

基于深

度学习的预

训练语言模

型最早可以

追溯到 ELMo，它采

用了以双向

LSTM

为基础的网

络结

构，训练

目标是优化

语言模型。2018 年

BERT 一举在

NLP 领域

的 11 个大方向

刷新了精度

，它采用的模

型便是前文

提到的

Transformer。BERT 提出

之后针对 BERT 的

“魔改”也成了

预训练语言

模型训练的

一

个热门方

向，例如使用

多任务的 MT-DNN，针

对 BERT 精心设计

一组实验实

现的充分发

挥

BERT 潜能的

RoBERTa、跨

语言模型的

XLM、速度更快的

ALBERT 和使用

Transformer-XL 作为

基础模型的

XLNet 等。

凭借

Transformer 强大

的表征能力

，OpenAI 提出的 GPT

系列

通过海量的

数据和庞大

的模型不

断

试探着 Transformer 的最

佳效果，虽然

GPT

系列模型的

表现效果很

令人惊艳，但

它的训练成

本让

很多企

业望而却步

。

BERT 之后的预训

练语言模型

绝大多数用

的是

Transformer 的框架

，它们性能提

升的一个原

因

便是设计

了更多、更合

理的无监督

任务。BERT 使用的

无监督任务

是掩码语言

模型和下一

句预测模

型

。ALBERT 使用的无监

督任务是句

子顺序预测

任务等。

除了

上面我们介

绍的这些预

训练模型，XuHan 等

人对近年来

流行的预训

练模型进行

了梳理，如

图

5.1 所示，感兴趣

的读者可以

自行搜索相

关论文。

预训

练语言模型

另一个火热

的研究方向

是将语言模

型和知识图

谱进行结合

，例如清华大

学提出

的

ERNIE-T 以

及百度提出

的 ERNIE-B 和

ERNIE 2.0。

图 5.1

近年

来流行的预

训练模型

第

5 章

模型预训

练

5.1　RNN 语言模型

187

5.1　RNN 语言模型

在

本节中，先验

知识包括：

 LSTM（4.1 节

）。

文本数据的

上下文关系

为训练语言

模型提供了

天然的数据

，常见的上下

文关系如下

。

z 传统语言模

型：使用之前

的序列预测

下一个出现

的单词。

z 掩码

语言模型（MLM）：句

子中每个单

词有一定的

概率被替换

为掩码，MLM

根据

出现的

单词

预测被替换

为掩码的单

词。

z 下一句预

测（next

sentence prediction，NSP）模型：用于

判断两个句

子是否为连

续的上下文

。

这种数据的

优点是无须

人工标注，因

此在很多文

献中使用这

种数据的深

度学习方法

被叫作无

监

督学习。其实

从本质上说

MLM

并不是传统

意义上的无

监督任务，因

为它有明确

的 (x, y) 数据

-

标签

对以及对应

的损失函数

，这种任务通

常被叫作自

监督任务。2018 年

被称作“NLP 的

ImageNet

时

刻”，因为在 2018 年

之后模型基

本使用语言

模型进行海

量数据的特

征提取，如

ELMo1

、BERT、

GPT2,3,4 系

列等。在了解

这些前沿的

算法之前，我

们先看一个

传统的基于

RNN

的语言模型

5

。

在深度学习

兴起之前，NLP 领

域一直是“统

计模型的天

下”，例如词对

齐算法

GIZA + + 、统计

机器翻译开

源框架

MOSES 等。在

语言模型方

向，n-gram 是当时最

为流行的语

言模型方法

。一个最

常用

的

n-gram 方法是回

退（backoff）n-gram，因为 n 值较

大时容易产

生特别稀疏

的数据，这时

候

回退 n-gram 会使

用 (n

− 1)-gram 的值代替

n-gram。

n-gram

的问题是捕

捉句子中长

期依赖的能

力非常有限

，解决这个问

题的策略有

缓存（cache）

模型和

基于类的（class-based）模

型，但是提升

程度有限。另

外 n-gram 算法过于

简单，对于其

是否

有能力

取得令人信

服的效果的

确要打一个

大的问号。

一

个更早使用

神经网络进

行语言模型

学习的策略

是使用前馈

神经网络进

行学习，该策

略由

Bengio 团队提

出。他们要求

输入的数据

为固定长度

的，从另一个

角度看该策

略就是一个

使用神经网

络编码的

n-gram 模

型，无法解决

长期依赖问

题。基于这个

问题，Radford 等人的

论文 2

使用了

RNN

作为语言模

型的学习框

架。

Radford 等人的论

文介绍了如

何使用

RNN 构建

语言模型，至

此打开了循

环神经语言

模型的篇

章

。由于算法比

较简单，我们

在这里多介

绍一些实验

中使用的技

巧，如动态测

试过程等，希

望能对

大家

以后的实验

设计有所帮

助。

5.1.1 语言模型

中的 RNN

Radford

等人的

论文中使用

了最简单的

RNN 版本，而现在

市场上普遍

选择 LSTM、GRU 甚至

NAS 等

能更长时间

捕捉长期依

赖的节点的

模型。在 RNN 中，xt

读

取的是 t − 1

时刻

的状态 st−1 和 t

时

刻的数据 wt ；wt 是

t

时刻单词的

独热编码，单

词量在 3 万～ 20

万

；st−1 是 t −

1 时刻的隐

层状态，

1　参见

Matthew

E. Peters、Mark Neumann、Mohit Iyyer

等人的论文

“Deep contextualized word representations”。

2　参见 Alec Radford、Karthik

Narasimhan、Tim Salimans 等人的

论文“Improving Language

Understanding by Generative

Pre-Training”。

3　参见 Alec

Radford、Jeffrey Wu、Rewon Child 等

人的论文“Language

Models are Unsupervised Multitask

Learners”。

4　参

见 Tom

B. Brown、Benjamin Mann、Nick Ryder

等人的论

文“Language Models are Few-Shot

Learners”。

5　参见 Tomáš

Mikolov、Martin Karafiát、Lukáš Burget 等人

的论文“Recurrent

neural network based language

model”。

第 5 章



模型预训练

188

实验中隐层

节点数一般

是 30 ～

500 个，t = 0

时使用

0.1 进行初始化

。上面过程表

示为式（5.1）。

（5.1）

t时刻

的隐层状态

是xt经过sigmoid激活

函数σ得到的

值，其中uji是权

值矩阵，如式

（5.2）所示。

（5.2）

有的时

候我们需要

在每个时间

片中有一个

输出，只需要

在隐层状态

sj,t 处添加一个

softmax 激活

函数即

可，如式（5.3）所示

。

（5.3）

5.1.2 训练数据

训

练语言模型

的数据是不

需要人工标

注的，我们要

做的就是寻

找大量高质

量的单语言

语料数

据。在

制作训练数

据和训练标

签时，我们取

0 ～ t

− 1 时刻的单词

作为网络输

入，t 时刻的单

词作为

标签

值。

由于输出

使用了 softmax 激活

函数，因此损

失函数的计

算使用的是

交叉熵，输出

层的误差向

量

如式（5.4）所示

。

（5.4）

式（5.4）中 是独热

编码的模型

预测值，yt

是标

签值，更新过

程使用标准

的 SGD 即可。

5.1.3

训练

细节

初始化

：使用均值为

0、方差为 0.1 的高

斯分布进行

初始化。

学习

率：初始值为

0.1，当模型在验

证集上的精

度不再提升

时将学习率

减半，一般 10 ～ 20

个

epoch 之后模型就

收敛了。

正则

：即使采用过

大的隐层，网

络也不会过

度训练，并且

实验结果表

明添加正则

项不会很有

帮助。

动态模

型：在测试常

见的机器

/ 深

度学习模型

的时候测试

数据并不会

用来更新模

型。在这篇论

文中作者认

为测试数据

（如反复出现

的人名）应该

参与模型的

更新，作者将

这种情况叫

作动态模型

。

实验结果表

明动态模型

可以大大降

低模型的困

惑度。

稀有类

：为了提高模

型的性能，作

者将低于阈

值的词合并

到稀有类中

。词概率的计

算方式如式

（5.5）所示：

（5.5）

其中，Crare 是

词表中词频

低于阈值的

单词的个数

，所有的低频

词都被平等

对待，即它们

的概率分布

是均等的。

5.2

ELMo

189

5.2　ELMo

在

本节中，先验

知识包括：

 LSTM（4.1 节

）；

 语言模型（5.1 节

）；



LN（6.3 节）。

为了让机

器能够理解

文本数据，我

们需要将文

本转换成数

值化的表示

方式。传统的

独热编码存

在

诸多问题

，例如无法衡

量相似数据

之间的相似

关系等。在

ELMo 之

前一个常用

的方法是使

用 Word2vec

或者GloVe1

。Word2vec使用

一组固定长

度的向量来

表示一个单

词，它的优点

是可以捕获

单词的语义

特征

和单词

之间的相似

性。但是 Word2vec 也存

在几个问题

，首先每个句

子的特征向

量只与其自

身有关系，

而

不能捕获上

下文的相关

性，其次每个

单词的特征

向量是唯一

的，因此不能

解决单词多

义性的问题

。

本文提出了

ELMo 模型来解决

这两个问题

，ELMo 的核心要点

为：

z 使用大规

模的无标签

语料库训练

双向 LSTM 语言模

型，ELMo（Embedding

from Language 

Model）也因此得

名；

z 将 ELMo 得到的

特征向量送

到下游任务

中，得到任务

相关的预测

结果。

ELMo 的基于

海量数据的

无监督学习

的思想对后

面的 BERT 和

GPT 系列

有很大的启

发，下面

我们

来一睹 ELMo

的“芳

容”。

5.2.1 双向语言

模型

如之前

所介绍的，ELMo

是

一个语言模

型，其基本结

构如图 5.2 所示

，其中 E

表示模

型输入的

嵌

入向量，T 表示

第 i

个时间片

的预测结果

。它的核心结

构是一个双

向的 LSTM，目标函

数就是最

大

化正反两个

方向的语言

模型的最大

似然。

图

5.2 ELMo 的基

本结构

对于

一个给定的

句子

(t1, t2, …, tN)，前向语

言模型会根

据给定的前

k

− 1 个单词来预

测第 k

个单

词

，表示为式（5.6）。

（5.6）

对

于一个

L 层的

LSTM，其输入是 （使

用 Word2vec

或其他方

式得到的标

志嵌入），多层

LSTM 的每一层都

会输出一个

内容相关的

表示 ，那么它

最顶层的输

出则表示为

。

在语言模型

中，

被用于预

测第 k 个时间

片的输出结

果，双向语言

模型的前向

部分如图 5.3

所

示。

后向语言

模型则是根

据后面的 (tk+1,…, tN)

单

词预测第 k 个

单词，表示为

式（5.7）。

1

参见 Jeffrey Pennington、Richard Socher、Christopher

D. Manning 的论

文“GloVe: Global

Vectors for Word Representation”。

第 5 章

模型

预训练

190

（5.7）

图

5.3　双

向语言模型

的前向部分

同样，对于一

个 L 层的反向

LSTM，其输入为

，每

一层都会输

出一个 ，最后

一层的输出

为 。

双向语言

模型则是上

面两个

LSTM 的结

合，通过最大

化对数似然

估计来完成

模型的训练

，如

式（5.8）所示：

（5.8）

其

中， 是标志嵌

入（token embedding）， 是输出层

的参数，整个

过程如图

5.4 所

示。

图 5.4

ELMo 双向语

言模型的可

视化

5.2　ELMo

191

5.2.2 ELMo 详解

ELMo的

作用便是根

据5.2.1节的双向

语言模型，为

每个单词学

习一个嵌入

向量，如图5.5所

示。

对一个 L 层

的双向语言

模型来说，每

个单词是

2L + 1 个

特征向量的

结合，它们分

别是前向

LSTM

的

L 个输出、后向

LSTM 的

L 个输出和

这个单词的

词嵌入，如式

（5.9）所示。

（5.9）

注意，两

个等式的下

标分别从

1 和

0 开始，当 j

= 0 时， ，当

j

＞ 0 时， 。

图 5.5　使用双

向语言模型

得到 ELMo

的词向

量

将 ELMo 应

用 到

下 游 任

务 时

， 我 们

需 要 将

L +

1 个 节 点

特 征

向 量 整

合 成

一 个， 表

示 为

，Θe 为 ELMo

模型中的

全部参数，一

个最简单的

方式便是直

接取最后一

层的输

出作

为 ELMo 生成的嵌

入向量，表示

为

。但是考虑

到不同深度

的 LSTM 层表征不

同的

特征，例

如浅层的特

征具有更强

的句法表征

能力，而越深

的层则更具

有语义表征

能力，因此作

者根

据这个

原理，提出了

式（5.10）所示的与

任务有关的

聚合方式：

（5.10）

其

中，s

task

是 softmax 之后的

概率值，是可

以学习的参

数，相当于注

意力。标量参

数 γ

task 用于对整

个 ELMo 向量按比

例缩放，γ

的值

对不同任务

的优化至关

重要。Θtask 是模型

的全部参数

。

图 5.6

可视化了

在不同的任

务中学到的

不同的值，可

以看出不同

任务之间的

权值差异还

是很大的。

图

5.6 在不同的任

务中学到的

不同的值

第

5

章  模型预训

练

192

另外，由于

每个双向语

言模型的层

有着不同的

分布，在某些

情况下加入

LN1 会很有帮助

。

5.2.3 应用

ELMo 到下游

任务

论文中

提到了多种

使用 ELMo

的方式

，而选择使用

何种方式往

往是由任务

决定的，目前

来看貌

似没

有一个大一

统的理论来

说明某种类

型的任务就

适应某种使

用方式。下面

我们将列出

这些方式，

如

果你在任务

中用到了 ELMo，不

妨将其作为

一个超参数

：

（1）将词嵌入和

ELMo 的输出拼接

得到的 [xk;ELMotask

k

] 作为

模型的输入

；

（2）将隐层节点

的输出与 ELMo

的

输出拼接得

到 [hk;ELMotask

k ]

；

（3）将 ELMo 作用到

输入层；

（4）将 ELMo 作

用到输出层

。

另外两个可

以提升下游

任务效果的

方式包括：

z 使

用 Dropout 或者

L2 正则

化；

z 根据任务

使用海量数

据对

ELMo 模型进

行微调。

5.2.4 小结

ELMo

模型有 3 个优

点。

z

ELMo 具有处理

一词多义的

能力。因为 ELMo 中

每个单词的

嵌入并不是

固定的，在将

这个

单词的

嵌入输入双

向 LSTM 之后，它的

值会随着上

下文内容的

不同而改变

，从而学到和

上下文相关

的嵌入。

z

ELMo 具有

不同层次的

表征能力。我

们知道，对一

个多层的网

络来说，不同

的深度具有

不

同的表征

能力，越接近

输入层的网

络层学到的

特征越接近

输入的原始

特征，而越接

近输出

层的

网络层学到

的特征则越

具有好的语

义表征能力

。ELMo

使用了对多

层 LSTM 的输出

进

行自适应加

权的结构注

意力机制，它

可以根据下

游任务自适

应调整

ELMo 的输

出，让

ELMo 与下游

任务相适应

。

z ELMo 具有非常强

大的灵活性

：除了 ELMo

本身的

输入可以调

整，ELMo 还可以以

各种形

式和

下游任务进

行结合。通过

ELMo 得到的仅是

当前时间片

的输入的编

码结果，因此

可以

将 ELMo 加入

输入层、隐层

和输出层。

ELMo

是

最早一批将

深度学习应

用到词向量

学习的任务

中的模型，它

的思想对后

续的 BERT 等

产生

了巨大的影

响。另外使用

ELMo

的策略绝对

不局限于 5.2.3 节

中所介绍的

，读者在使用

ELMo

时可以自行

探索最适合

自己任务的

结合策略。

5.3　GPT-1、GPT-2 和

GPT-3

在本节中，先

验知识包括

：

 Transformer（4.3 节）； 

BERT（5.4 节）；

 LN（6.3

节）；  残差

网络（1.4 节）。

1　参见

Jimmy Lei Ba、Jamie

Ryan Kiros、Geoffrey E. Hinton

的论文“Layer Normalization”。

5.3　GPT-1、GPT-2

和 GPT-3

193

通

用预训练

Transformer（generative pre-trained Transformer，GPT）系

列是由 OpenAI

提出

的非

常强大

的预训练语

言模型，这一

系列的模型

可以在非常

复杂的 NLP 任务

中取得非常

惊艳的效果

，

如文章生成

、代码生成、机

器翻译、问答

等，而完成这

些任务并不

需要有监督

学习进行模

型微调。

对于

一个新的任

务，在使用 GPT 作

为预训练模

型之后，我们

训练这个任

务时仅需要

极少的数据

，

便可以得到

近似使用大

数据训练的

模型的结果

。

当然，如此强

大的功能并

不是一个简

单的模型能

搞定的，GPT 模型

的训练需要

超大的训练

语

料、超多的

模型参数和

超强的计算

资源。GPT

系列的

模型架构秉

承了不断堆

叠 Transformer 的思想，

通

过不断提升

训练语料的

规模和质量

、增加网络的

参数数量来

完成

GPT 系列的

迭代更新，如

表 5.1

所示。GPT

也证

明了，通过不

断提升模型

容量和语料

规模，模型的

能力是可以

不断提升的

。

表 5.1　GPT

系列的发

布时间、参数

数量和预训

练数据量

模

型 发布时间

参数数量 预

训练数据量

GPT

2018 年 6 月

1.17 亿 约 5GB

GPT-2 2019 年

2 月

15 亿 约 40GB

GPT-3 2020 年 5

月

1 750 亿 约

45TB

本节会

依次介绍 GPT-1、GPT-2，GPT-3，并

介绍它们基

于上个版本

的改进点，共

包括 4

个主

要

方向：算法的

思想和目标

、使用的数据

集和预处理

方式、模型架

构以及算法

的性能。

5.3.1 GPT-1：无监

督学习

在 GPT-1（和

ELMo 同一年）之前

，传统的 NLP

模型

往往使用大

量的数据对

有监督的模

型进

行任务

相关的模型

训练，但是这

种有监督学

习的任务存

在两个缺点

。

z 需要大量的

标注数据。高

质量的标注

数据往往很

难获得，因为

在很多任务

中，数据的标

签

并不是唯

一的或者实

例标签并不

存在明确的

边界。

z 根据一

个任务训练

的模型很难

泛化到其他

任务中，并且

这个模型只

能叫作“领域

专家”，

而不是

真正地理解

了

NLP。

这里介绍

的 GPT-1 的思想是

先通过在无

标签的数据

上学习一个

通用的语言

模型，然后根

据特定

任务

进行微调，其

处理的有监

督任务如下

。

z 自然语言推

理（natural language

inference）：判断两个

句子是包含

（entailment）关系、矛

盾（contradiction）关

系，还是中立

（neutral）关系。

z 问答和

常识推理（question

answering and commonsense reasoning）：类

似于多选题

，输入一

篇文

章、一个问题

以及若干个

候选答案，输

出每个答案

的预测概率

。

z 语义相似度

（semantic similarity）：判断两个句

子语义上是

否相关。

z 分类

（classification）：判断输入文

本属于指定

的哪个类别

。

将无监督学

习作用于有

监督模型的

预训练目标

，叫作通用预

训练。

1．GPT-1

的训练

GPT-1 的训练分为

自监督预训

练和有监督

的模型微调

，下面进行详

细介绍。

自监

督预训练：GPT-1 的

自监督预训

练是基于语

言模型实现

的，给定一个

无标签的序

列

，语言模型

的优化目标

是最大化式

（5.11）的似然值：

（5.11）

第

5 章

模型预训

练

194

其中，k 是滑

动窗口的大

小，P

是条件概

率，Θ 是模型的

参数。这些参

数使用 SGD 进行

优化。

在GPT-1中，使

用了12个Transformer块作

为解码器，

每

个 Transformer 块采用多

头的自注意

力机制，然后

通过

全连接

得到输出的

概率分布，如

图 5.7 所示。

（5.12）

式（5.12）中

，

是当前时间

片的上下文

标

志，n 是层数

，We 是词嵌入矩

阵，Wp

是位置嵌

入矩阵。

有监

督的模型微

调：当得到无

监督的预训

练模型之

后

，我们将它的

值直接应用

到有监督任

务中。对于一

个

有标签的

数据集

C，每个

实例有 m 个输

入标志 {x1,…,xm}，

它对

应标签 y。首先

将这些标志

输入训练好

的预训练模

型中，得到最

终的特征向

量 。然后通过

全

连接层得

到预测结果

y，如式（5.13）所示：

（5.13）

其

中，Wy 为全连接

层的参数。有

监督的目标

则是最大化

式（5.13）的值，如式

（5.14）所示。

（5.14）

作者并

没有直接使

用损失函数

L2，而是向其中

加入了损失

函数 L1，并使用

λ 进行两个任

务权值

的调

整，λ

的值一般

为 0.5，如式（5.15）所示

。

（5.15）

当进行有监

督的模型微

调的时候，我

们只训练输

出层的

Wy 和分

隔符（delimiter）的嵌入

值。

2．任务相关

的输入变换

在上文中，我

们介绍了 GPT-1

处

理的 4 个不同

的任务，这些

任务有的只

有一个输入

，有的则有

多

组形式的输

入。对于不同

的输入，GPT-1

有不

同的处理方

式，如图 5.8 所示

，具体介绍如

下。

z

分类：将起

始和终止标

志加入原始

序列两端，输

入 Transformer 中得到特

征向量，最后

经

过全连接

得到预测的

概率分布。

z 自

然语言推理

：将前提（premise）和假

设（hypothesis）用分隔符

隔开，两端加

上起始和

终

止标志，再依

次通过 Transformer

和全

连接得到预

测结果。

z 语义

相似度：输入

的两个句子

，正向和反向

各拼接一次

，然后分别输

入 Transformer，得

到的特

征向量拼接

后再通过全

连接得到预

测结果。

z 问答

和常识推理

：将 n

个选项的

问题抽象化

为 n 个二分类

问题，即每个

选项分别和

内容进

行拼

接，然后送入

Transformer

和全连接中

，最后选择置

信度最高的

作为预测结

果。

3．GPT-1 的数据集

GPT-1 使用了

BooksCorpus 数据

集 1

，作者选这

个数据集的

原因有二：数

据集拥有更

长的上下

文

，使得模型能

学得更长期

的依赖关系

；该数据集很

难在下游数

据集上见到

，更能验证模

型的泛化

能

力。

1　参见

Yukun Zhu、Ryan Kiros、Richard Zemel

等人

的论文“Aligning Books and Movies:

Towards Story-like Visual Explanations

by 

Watching Movies

and Reading Books”。

图

5.7 GPT 系

列的基本框

架

5.3

GPT-1、GPT-2 和 GPT-3

图

5.8 Transformer 的基

本结构 (a)

和 GPT-1 应

用到不同任

务上输入数

据的变换方

式 (b)

4．训练细节

GPT-1 使用了 12 层的

Transformer

和掩码自注

意力头，掩码

使模型看不

见之后的信

息，得到的

模

型泛化能力

更强。其中，自

监督预训练

使用的超参

数为：

（1）使用字

节对编码（byte pair

encoding，BPE），共

有 40 000 个字节对

；

（2）词编码的长

度为 768；

（3）位置编

码也需要学

习；

（4）12

个 Transformer 块，每个

Transformer 块有

12 个头；

（5）位

置编码的长

度是 3072；

（6）注意力

、残差、Dropout 等机制

用来进行正

则化，丢弃比

例为 0.1；

（7）激活函

数为

GELU ；

（8）训练的

批次大小为

64，学习率为 2.5

× 10−4

，序

列长度为 512，序

列

epoch 数为 100；

（9）模型

参数数量约

为

1.17 亿。

有监督

微调使用的

参数细节为

：

z

自监督部分

的模型也会

用来微调；

z 训

练的 epoch

数为 3，学

习率为 6.25 ×

10−5

，这表

明模型在自

监督部分学

到了大量有

用的

特征。

5．GPT-1

的

性能

在有监

督学习的 12 个

任务中，GPT-1

在 9 个

任务上的表

现超过了当

时最优的模

型。在没有见

过数据的零

样本学习任

务中，基于 GPT-1

的

模型的表现

要比基于 LSTM 的

模型的表现

稳定，且随

着

训练次数的

增加，GPT-1

的性能

也逐渐提升

，表明 GPT-1 有非常

强的泛化能

力，能够用到

和有监

督任

务无关的其

他

NLP 任务中。GPT-1 证

明了 Transformer

学习语

言模型的强

大能力，在 GPT-1 得

到的词向量

基础上进行

下游任务的

学习，能够让

下游任务取

得更好的泛

化能力。对于

下游任务的

训

练，GPT-1

往往只

需要简单的

微调便能取

得非常好的

效果。

GPT-1 在未经

微调的任务

上虽然也有

一定效果，但

是其泛化能

力远远低于

经过微调的

有监督任

务

，说明了

GPT-1 只是

一个简单的

领域专家，而

非通用的语

言学家。

195

第

5 章

模型预训练

196

5.3.2

GPT-2：多任务学习

GPT-2 的目标是训

练一个泛化

能力更强的

预训练语言

模型，它并没

有对 GPT-1 的网络

进行过多的

结

构上的创

新与设计，只

是使用了更

多的网络参

数和更大的

数据集。下面

我们对 GPT-2 展开

详细的介绍

。

1．GPT-2

的核心思想

GPT-2 的学习目标

是使用自监

督的预训练

模型做有监

督的任务。因

为文本数据

的时序性，一

个

输出序列

可以表示为

一系列条件

概率的乘积

，如式（5.16）所示。

（5.16）

上

式也可以表

示为 P(sn−k, …, sn|s1,s2,

…, sn−k−1)，它的实

际意义是根

据已知的上

文输入 {s1, s2,

…, 

sn−k−1} 预测

未知的下文

输出

{sn−k, …, sk}，因此语

言模型可以

表示为 P(

输出

| 输入 )。对于一

个有

监督的

任务，它可以

建模为

P( 输出

| 输入,任务 )

的

形式。在 decaNLP1 中，McCann 等

人提出的

MQAN 模

型可以将机

器翻译、自然

语言推理、语

义分析、关系

提取等 10 类任

务统一建模

为一个分

类

任务，而无须

再为每一个

子任务单独

设计一个模

型。

基于上面

的思想，作者

认为，当一个

语言模型的

容量足够大

时，它就足以

覆盖所有的

有监督任

务

，也就是说所

有的有监督

任务都是自

监督语言模

型的子集。例

如当模型训

练完“Micheal Jordan

is the best basketball

player in the history”语料的

语言模型之

后，便也学会

了（question

：“who is the

best basketball player in

the history ?”，answer:“Micheal Jordan”）的问答任

务。

综上，GPT-2 的核

心思想可以

概括为：任何

有监督任务

都是语言模

型的子集，当

模型的容量

非

常大且数

据量足够丰

富时，仅仅靠

训练语言模

型的学习便

可以完成其

他有监督任

务。

2．GPT-2

的数据集

GPT-2 的数据集取

自 Reddit 上高赞的

文章，命名为

WebText。数据集共有

约

800 万篇文章

，累

计大小约

40GB。为了避免和

测试集的冲

突，WebText 移除了涉

及

Wikipedia 的文章。

3．模

型参数

模型

具体参数如

下：

z 同样使用

了字节对编

码构建字典

，字典的单词

数为 50 257；

z 滑动窗

口的大小为

1024；

z 批次大小为

512；

z LN 移动到了每

一块的输入

部分，在每个

自注意力之

后额外添加

了一个 LN

；

z 将残

差层的初始

化值用 进行

缩放，其中

N 是

残差层的个

数。

GPT-2 训练了

4 个

层数和词向

量长度不同

的模型，具体

值如表 5.2 所示

。通过论文中

提供的实

验

数据，我们可

以看出随着

模型的增大

，模型的效果

是不断提升

的。

表 5.2　GPT-2

训练的

4 个模型的参

数数量、层数

和词向量长

度

参数数量

层数 词向量

长度

1.17 × 108

(GPT-1)

12 768

3.45 ×

108 24 1 024

7.62 × 108 36

1 280

1.542 ×

109 48 1 600

1　参见 Bryan McCann、Nitish

Shirish Keskar、Caiming Xiong 等

人的论文“The

Natural Language Decathlon: Multitask

Learning 

as Question

Answering”。

5.3　GPT-1、GPT-2 和

GPT-3

4．GPT-2 的性能

对比

GPT-1，GPT-2 引入了更多

的参数和训

练集，它通过

提示学习（prompt

learning）的

思想，

将更复

杂的语义信

息加入预训

练任务中，因

此可以在很

多无监督的

数据集下取

得当时最优

的效果，

具体

如下。

z

在 8 个语

言模型任务

中，仅仅通过

零样本学习

（zero-shot learning），GPT-2

就达到了 7 个

任务中的最

优值。

z

在“Children’s Book Test”数据

集上的命名

实体识别任

务中，GPT-2 超过了

当时最优的

方

法约 7%。

z “LAMBADA”是测

试模型捕捉

长期依赖的

能力的数据

集，GPT-2

将困惑度

从 99.8% 降到

了

8.6%。

z 在

阅读理解数

据中，GPT-2 超过了

4

个基线模型

中的 3 个。

z

在法

译英任务中

，GPT-2 在零样本学

习的基础上

，表现超过了

大多数的自

监督方法，但

是

比有监督

的先进模型

要差。

z

GPT-2 在文本

总结的表现

不理想，但是

它的效果和

有监督的模

型非常接近

。

5．总结

GPT-2

的最大

贡献是验证

了通过海量

数据和大量

参数训练出

来的预训练

语言模型可

以迁移到其

他类别任务

中而不需要

额外的训练

。但是很多实

验也表明，GPT-2 的

自监督学习

的能力还有

很大的

提升

空间，甚至在

有些任务上

的表现不比

随机的好。尽

管 GPT-2

在有些零

样本的任务

上表现不错

，

但是我们仍

不清楚 GPT-2 的这

种策略究竟

能做成什么

样子。GPT-2

表明随

着模型容量

和数据量的

增

大，其潜能

还有进一步

开发的空间

，基于这个思

想，诞生了我

们下面要介

绍的 GPT-3。

5.3.3

GPT-3：海量参

数

截至编写

本节时，GPT-3 是目

前最强大的

预训练模型

，仅仅需要零

样本学习或

者少样本学

习，

GPT-3

就可以在

下游任务表

现得非常好

。除了几个常

见的 NLP 任务，GPT-3 在

很多非常困

难的任

务上

也有惊艳的

表现，例如撰

写人类难以

判断是否由

机器生成的

文章，甚至编

写 SQL 查询语句

、

React

或者 JavaScript 代码等

。而这些强大

能力则依赖

于 GPT-3

1 750 亿的参数

数量、45TB 的训练

数据以及高

达

1200 万美元的

训练费用。

1．情

境学习

情境

学习（in-context

learning）是这篇

论文中介绍

的一个重要

概念，要理解

情境学习，我

们需

要先理

解元学习（meta-learning）1

。对

一个少样本

的任务来说

，模型的初始

化值非常重

要，以一个好

的初始化值

作为起点，模

型能够尽快

收敛，使得到

的结果非常

快地逼近全

局最优解。元

学习的核心

思想在于通

过少量的数

据寻找一个

合适的初始

化范围，使得

模型能够在

有限的数据

集上快速拟

合，

并获得不

错的效果。

这

里介绍的是

模型无关元

学习（model-agnostic meta-learning，MAML）算法 2

，如

算法

1 所示。

正

常的监督学

习将数据打

包成批次进

行学习，但是

元学习将任

务打包成批

次，每个批次

分为支持集

（support set）和质询集（query

set），类

似于学习任

务中的训练

集和测试集

。

1　参见 Chelsea

Finn、Pieter Abbeel、Sergey Levine 的论文

“Model-Agnostic

Meta-Learning for Fast Adaptation

of Deep Networks”

和

Aravind Rajeswaran、Chelsea Finn、Sham Kakade

等人的论

文“Meta-Learning with Implicit Gradients”。

2　参见 Chelsea Finn、Pieter

Abbeel、Sergey Levine 的论

文“Model-Agnostic Meta-Learning

for Fast Adaptation of

Deep 

Networks”。

197

第 5 章

模型

预训练

198

算法

1　模型无关元

学习

输入：p(T )为

基于任务的

分布

输入：α、β为

步长（超参数

）

1: 随机初始化

θ

 2:

while未完成do

 3: 采样

一批任务Ti

～p(T )

 4:

for Ti do

5: 使

用K个样本评

估

 6:

计算自适

应参数和梯

度下降：

 7: end

for

 8: 更新



9: end while

对一个网络

模型

f，其参数

表示为 θ，它的

初始化值叫

作元初始化

（meta-initialization）。MAML

的目标则是

学习一组元

初始化，并将

其快速应用

到其他任务

中。MAML 的迭代涉

及两次参数

更新，

分别是

内循环（inner loop）和外

循环（outer loop）。内循环

是根据任

务

标签快速地

对具体的任

务进行学习

和适应，而外

循环则是对

元初始

化进

行更新。直观

地理解，我们

用一组元初

始化去学习

多个任务，如

果每个任务

都学得比较

好，则说明这

组元初始化

是一组不错

的初始化

值

，否则我们就

对这组值进

行更新，如图

5.9 所示。目前的

实验结果

表

明从元学习

过渡到学习

一个通用的

预训练语言

模型还是有

很多工作

要

做的。

GPT-3 中介绍

的情境学习

是元学习的

内循环，而基

于语言模型

的

SGD

则是外循

环，如图 5.10 所示

。

图

5.10 GPT-3 中的内循

环和外循环

图 5.9

元学习的

可视化结果

5.3　GPT-1、GPT-2 和 GPT-3

199

除了引入

大量数据，模

型优化的另

外一个方向

则是提供容

量足够大的

Transformer 模型来对语

言模型进行

建模。而近年

来使用大规

模的网络来

训练语言模

型也成了非

常行之有效

的策略，这也

促

使

GPT-3 一口气

将模型参数

数量提高到

1750 亿个。

2．少样本

学习、一次学

习、零样本学

习

在少样本

学习中，提供

若干个（10 ～ 100 个）示

例和任务描

述供模型学

习。一次学习

（one-shot

learning）则仅提供一

个示例和任

务描述。而零

样本学习不

提供示例，只

是在测试时

提供任务的

具

体描述。作

者对这 3 种学

习方式分别

进行了实验

，实验结果表

明，3

种学习方

式的效果都

会随着模

型

容量的上升

而提升，且少

样本学习 > 一

次学习

> 零样

本学习，如图

5.11 所示。

图

5.11　随着

参数数量的

提升，3 种学习

方式的模型

的效果均有

了不同程度

的提升

从理

论上讲

GPT-3 也是

支持微调的

，但是微调需

要利用海量

的标注数据

进行训练才

能获得比较

好的效果，而

这样会造成

在其他未训

练过的任务

上表现差，所

以 GPT-3 并没有尝

试微调。

3．数据

集

GPT-3 共训练了

5 个不同的语

料数据集，分

别是低质量

的

Common Crawl、高质量的

WebText2、

Books1、Books2 和

Wikipedia，GPT-3 根据数据

集质量的不

同赋予了其

不同的权值

，权值越高的

数

据集在训

练的时候越

容易抽样到

，如表 5.3

所示。

表

5.3　GPT-3 使用的数据

集的数据量

样本占比

数

据集 数据量

( 标志数 )

训练

时的样本占

比 训练 3 000

亿个

标志时经过

的 epoch 数

Common

Crawl (filtered) 4100 亿

60% 0.44

WebText2 190

亿 22% 2.9

Books1

120 亿

8% 1.9

Books2

550 亿 8% 0.43

Wikipedia 30 亿 3%

3.4

4．模型

GPT-3 沿

用了

GPT-2 的结构

，但是在网络

容量上做了

很大的提升

，具体如下：

z GPT-3

采

用了 96 层的多

头 Transformer，头的个数

为

96；

z 词向量的

长度是 12

888；

z 上下

文滑窗的窗

口大小提升

至 2048

个标志；

第

5 章

模型预训

练

200

z 使用了交

替密集（alternating

dense）和局

部带状稀疏

注意力（locally banded sparse attention）1

。

5．GPT-3 的性

能

仅仅用“令

人惊艳”很难

描述

GPT-3 的优秀

表现。首先，在

大量的语言

模型数据集

中，GPT-3

超过了绝

大多数的零

样本学习或

者少样本学

习的先进方

法。另外 GPT-3

在很

多复杂的 NLP 任

务中

也超过

了微调之后

的先进方法

，如闭卷问答

、模式解析、机

器翻译等。除

了这些传统

的

NLP 任务，

GPT-3 在一

些其他的领

域也取得了

非常惊人的

效果，如进行

数学加法、文

章生成、编写

代码等。

5.3.4 小结

GPT 系列从 1

到 3，全

部采用的是

Transformer 架构，可以说

并没有创新

性的模型架

构设计。

在

Microsoft 的

资金支持下

，这更像是一

场“赤裸裸的

炫富”：1750 亿的参

数，31 个分工明

确的作

者，超

强算力的计

算机（285 000 个 CPU、10

000 个 GPU），1200 万

美元的训练

费用，45TB

的训

练

数据（Wikipedia 的全部

数据只相当

于其中的 0.6%）。这

种规模的模

型是一般中

小企业无法

承受

的，而个

人花费巨额

配置的单卡

机器也就只

能做做微调

或者打打游

戏，甚至在训

练 GPT-3 时出现了

一个 bug，OpenAI

自己也

没有资金重

新训练了。

理

解了 GPT-3 的原理

，我们就能客

观地看待媒

体对

GPT-3 的过分

神化了。GPT-3 的本

质还是通过

海量的参数

学习海量的

数据，然后依

赖 Transformer

强大的拟

合能力使得

模型能够收

敛。基于这个

原

因，GPT-3 学到的

模型分布也

很难摆脱它

的数据集的

分布情况。得

益于庞大的

数据集，GPT-3 可以

完

成一些令

人感到惊喜

的任务，但是

GPT-3 也不是万能

的，对一些明

显不在这个

分布或者和

这个分布有

冲突的任务

来说，GPT-3 还是无

能为力的。例

如通过目前

的测试来看

，GPT-3 还是有很多

缺点的：

z 对于

一些命题没

有意义的问

题，GPT-3 不会判断

命题有效与

否，而是拟合

一个没有意

义的

答案出

来；

z 由于 40TB 的海

量数据的存

在，很难保证

GPT-3

生成的文章

不包含一些

非常敏感的

内容，

如种族

歧视、性别歧

视、宗教偏见

等；

z 受限于

Transformer 的

建模能力，GPT-3 并

不能保证生

成的一篇长

文章或者一

本图书的连

贯性，存在下

文不停重复

上文的问题

。

GPT-3

对 AI 领域的影

响无疑是深

远的，如此性

能强大的语

言模型，为下

游各种类型

的 NLP

任

务提供

了非常优秀

的预训练语

言模型，在此

基础上必将

落地更多有

趣的 AI 应用。近

年来，硬件的

性能在飞速

发展，而算法

的研究似乎

遇到了瓶颈

，GPT-3

给“冷清”的 AI 领

域注入了一

剂“强心

剂”，告

诉各大硬件

厂商它们的

工作还要加

油，只要算力

足够强，AI

的性

能还有不断

提升的空间

。

同时 GPT-3 如此高

昂的计算代

价也引发了

一些关于

AI 领

域垄断的担

心，对于如此

高的算力要

求，中小企业

是否有能力

负担，或者对

这些企业来

说，是否有必

要花这么多

钱训练一个

预训练语言

模型。长此以

往，恐怕会形

成 AI“巨头”对算

力要求高的

算法的技术

垄断。

5.4

BERT

在本节

中，先验知识

包括：

 Transformer（4.3

节）；  ELMo（5.2 节）；

 GPT（5.3 节

）。

1

参见 Rewon Child、Scott Gray、Alec

Radford 等人的

论文“Generating Long Sequences

with Sparse Transformers”。

5.4

BERT

BERT 被提出

之后，作为 Word2vec、ELMo

以

及 GPT-1 的替代者

，其在 NLP

领域的

11 个方

向大幅

刷新了精度

，可以说是近

年来自残差

网络后最具

有突破性的

一项技术了

。论文的主要

特点

如下：

z 使

用了 Transformer 作为算

法的主要框

架，Transformer

能更彻底

地捕捉语句

中的双向

关

系；

z 使用了

MLM 和

NSP 任务的多任

务训练目标

；

z

使用更强大

的机器训练

更大规模的

数据，使 BERT 的结

果达到了全

新的高度，并

且 Google

开源了 BERT 模

型，用户可以

直接使用 BERT

作

为 Word2vec 的转换矩

阵并高效地

将其应

用到

自己的任务

中。

BERT 的本质是

通过在海量

语料的基础

上运行自监

督学习方法

为单词学习

一个好的特

征表示，

所谓

自监督学习

是指在没有

人工标注的

数据上运行

的监督学习

。在以后特定

的 NLP

任务中，我

们

可以直接

使用 BERT 的特征

表示作为该

任务的词嵌

入特征。所以

BERT

提供的是一

个可以供其

他任

务迁移

学习的模型

，该模型可以

根据任务微

调或者固定

之后作为特

征提取器。

5.4.1 BERT

详

解

1．网络结构

BERT 的网络结构

使用的是 4.3

节

中介绍的多

层 Transformer 结构，其最

大的特点是

抛弃了传统

的 RNN

和 CNN，通过自

注意力机制

将任意位置

的两个单词

的距离转换

成 1，有效地解

决了 NLP

中

棘手

的长期依赖

问题。Transformer 的结构

在 NLP

领域中已

经得到了广

泛应用，并且

作者已经将

其

发布在 TensorFlow 的

tensor2tensor

库中。

Transformer 的完整

结构如图 4.41

所

示。Transformer 采用的是

编码器 - 解码

器的结构，由

若干个编码

器和解码器

堆叠而成。图

4.41

的左侧部分

为编码器，由

多头注意力

和一个全连

接组

成，用于

将输入语料

转化成特征

向量。图 4.41 的右

侧部分是解

码器，其输入

为编码器的

输出和

已经

预测的结果

，由掩码多头

注意力、多头

注意力和一

个全连接组

成，用于输出

最后结果的

条件

概率。

图

4.41 所示的是一

个

Transformer 块，对应图

5.12 中的一个“Trm”。

图

5.12

BERT 的网络结构

201

第 5

章  模型预

训练

BERT

提供了

简单和复杂

两个模型，对

应的超参数

分别如下。

z BERTBASE ：L

= 12，H = 768，A

= 12，参

数总量 1.1 ×

108

。

z BERTLARGE

：L = 24，H =

1024，A = 16，参数

总量 3.4

× 108

。

在上面

的超参数中

，L

表示网络的

层数（即 Transformer 块的

数量），A 表示多

头注意力模

块中

自注意

力的头的数

量，隐层节点

数是 H。

论文中

还对比了 OpenAI

GPT 和

ELMo，它们两个的

结构如图 5.13 所

示。

图 5.13 OpenAI GPT

和 ELMo

BERT 相对

这两个算法

的优点是只

有

BERT 表征会基

于所有层中

的左右两侧

语境。BERT 能做

到

这一点得益

于

Transformer 中自注意

力机制将任

意位置的两

个单词的距

离转换成了

1。

2．输入表示

BERT

的

输入的编码

向量（长度是

512）是 3 个嵌入特

征的单位和

，如图 5.14

所示。这

3 个词

的嵌入

特征如下。

图

5.14

BERT 输入的编码

向量是标志

嵌入、位置嵌

入和分割嵌

入的单位和

z 词级别（WordPiece）嵌入

或标志嵌入

（token embedding）：词级别嵌入

是指将单词

分成一

组有

限的公共子

词单元，这样

能在单词的

有效性和字

符的灵活性

之间取得平

衡。例如图

5.14 的

示例中“playing”被拆

分成了“play”和“ing”。

z

分

割嵌入（segment embedding）：用于

区分两个句

子，例如 B 是不

是

A 的下文（对

话场景、

问答

场景等）。对于

句子对，第一

个句子的特

征值是 0，第二

个句子的特

征值是

1。

202

5.4　BERT

z 位置

嵌入（position embedding）：位置嵌

入是指将单

词的位置信

息编码成特

征向量。位置

嵌入是向模

型中引入单

词位置关系

的至关重要

的一环。

最后

，说明一下图

5.14

中的两个特

殊符号 [CLS] 和 [SEP]，其

中

[CLS] 表示该特

征用于分类

模

型，对非分

类模型，该符

号可以省去

；[SEP] 表示分句符

号，用于断开

输入语料中

的两个句子

。

3．预训练任务

BERT 是一个多任

务模型，它的

任务由两个

自监督任务

组成，即掩码

语言模型（MLM）和

下一

句预测

（NSP）模型。

任务

1：掩

码语言模型

MLM 的核心思想

取自 Wilson Taylor

在 1953 年发

表的一篇论

文 1

。所谓 MLM 是指

在训练的时

候

随即从输

入预料上掩

码掉一些单

词，然后通过

上下文预测

该单词，该任

务非常像我

们在中学时

期经

常做的

完形填空。正

如传统的语

言模型算法

和 RNN 匹配那样

，MLM 的这个性质

和

Transformer 的

结构是

非常匹配的

。

在

BERT 的实验中

，15% 的词级别标

志会被随机

替换为掩码

。在训练模型

时，一个句子

会被多

次输

入模型中用

于参数学习

，但是

Google 并没有

在每次都将

这些单词替

换为掩码，而

是在确定要

替换为掩码

的单词之后

，将其 80% 直接替

换为“[mask]”，10%

替换为

其他任意单

词，10% 保留原

始

标志。

z

80% ：my dog is

hairy -> my dog

is [mask]。

z 10%

：my dog is hairy

-> my dog is

apple。

z 10% ：my

dog is hairy ->

my dog is hairy。

这么做

的原因是如

果句子中的

某个标志 100% 会

被替换为掩

码，那么在微

调的时候模

型就会有

一

些没有见过

的单词。加入

随机标志的

原因是

Transformer 要保

持对每个输

入标志的分

布式表征，

否

则模型就会

记住这个 [mask]

是

标志“hairy”。至于单

词替换带来

的负面影响

，因为一个单

词被随

机替

换掉的概率

只有 15% ×

10% = 1.5%，这个负

面影响其实

是可以忽略

不计的。

另外

论文指出每

次只预测

15% 的

单词，因此模

型收敛得比

较慢。

任务 2：下

一句预测模

型

NSP 的任务是

判断句子B是

不是句子A的

下文，如果是

的话输出“IsNext”，否

则输出“NotNext”。

训练

数据的生成

方式是从语

料中随机抽

取连续的两

句话，其中 50%

的

概率抽取的

两句话符合

IsNext 关系，另外 50% 的

概率第二句

话是随机从

语料中提取

的，符合

NotNext 关系

。这个关系保

存在图 5.14 中的

[CLS]

符号中。

4．微调

在海量单语

料上训练完

BERT 之后，便可以

将其应用到

NLP 的各个任务

中了。对

NSP 任务

来

说，其条件

概率表示为

P =

softmax（CWT

），其中 C 是

BERT 输出

中的 [CLS] 符号生

成的特征向

量，

W 是可学习

的权值矩阵

。

对其他任务

来说，我们也

可以根据 BERT

的

输出信息做

出对应的预

测。图 5.15 展示了

BERT

在

11 个不同任

务中的模型

，它们只需要

在 BERT 的基础上

再添加一个

输出层便可

以完成对特

定任

务的微

调。这些任务

类似于我们

做过的试卷

，其中有选择

题、简答题等

。图 5.15 中 Tok

表示不

同

的标志，E 表

示嵌入向量

，Ti 表示第

i 个标

志在经过 BERT 处

理之后得到

的特征向量

。

1　参见 Wilson L.Taylor

的论文

“Cloze Procedure”: A New

Tool for Measuring Readability”。

203

第 5 章

模型预

训练

图 5.15 BERT

在 11 个

不同任务中

的模型

微调

的任务如下

。

（1）基于句子对

的分类任务

。

z MNLI：给定一个前

提，根据这个

前提去推断

假设与前提

的关系。该任

务的关系分

为 3

种，

即包含

关系、矛盾关

系和中立关

系。所以这个

问题本质上

是一个分类

问题，我们需

要做的

是去

发掘前提和

假设这两个

句子对之间

的交互信息

。

z

QQP ：基于 Quora，判断 Quora

上

的两个问句

是否表示的

是一样的意

思。

z QNLI ：用于判断

文本是否包

含问题的答

案，类似于我

们做阅读理

解时定位问

题所在的

段

落。

z STS-B ：预测两个

句子的相似

性，包括

5 个级

别。

z MRPC

：判断两个

句子是不是

等价的。

z RTE ：类似

于

MNLI，但是只是

对包含关系

的二分类判

断，而且数据

集更小。

z SWAG ：从

4 个

句子中选择

可能为前句

下文的那个

。

（2）基于单个句

子的分类任

务。

z

SST-2：电影评价

的情感分析

。

204

5.5　BERT“魔改”之

RoBERTa、ALBERT、MT-DNN 和 XLM

205

z CoLA ：句

子语义判断

，是不是可接

受的（acceptable）。

对于

GLUE 数

据集的分类

任务（MNLI、QQP、QNLI、SST-B、MRPC、RTE、SST-2、CoLA），

BERT 的微调

方法是根据

[CLS]

标志生成一

组特征向量

C，并通过一层

全连接进行

微调。损失函

数

根据任务

类型自行设

计，例如多分

类的 softmax 或者二

分类的

sigmoid。

SWAG 的微

调方法与 GLUE

数

据集上的分

类任务的微

调方法类似

，只不过其输

出是 4 个可能

选项的 softmax

函数

值，如式（5.17）所示

。

（5.17）

（3）问答任务。

SQuAD

v1.1：给

定一个句子

（通常是一个

问题）和一段

描述文本，输

出这个问题

的答案，类

似

于做阅读理

解的简答题

。如图 5.15（c）表示的

，SQuAD v1.1

的输入是问

题和描述文

本的句子对

，

输出是特征

向量，通过在

上一层激活

函数为 softmax 的全

连接来获得

输出文本的

条件概率，其

中全

连接的

输出节点个

数是语料中

标志的个数

，如式（5.18）所示。

（5.18）

（4）命

名实体识别

。

CoNLL-2003

NER：判断一个句

子中的单词

是不是个人

实体（person）、组织实

体（organization）、

位置实体

（location）、其他实体（miscellaneous）或

者无命名实

体（other）。微调 CoNLL-2003 NER

时将

整个句子作

为输入，在每

个时间片输

出一个概率

，并通过 softmax 得到

这个标志的

实体类别。

5.4.2

小

结

BERT 在 2018

年之后

火得“一塌糊

涂”不是没有

原因的：

z 使用

Transformer 结构将已经

走向瓶颈期

的

Word2vec 带向了一

个新的方向

，并将强大的

Transformer 应用到实际

的词向量任

务中；

z

11 个 NLP 任务

的精度大幅

提升足以震

惊整个深度

学习领域；

z 无

私地开源了

多种语言的

源码和模型

，具有非常高

的商业价值

。

z 迁移学习又

一次胜利，而

且这次是在

NLP

领域的“大胜

”。

BERT 算法还有很

大的优化空

间，例如我们

在 Transformer

中讲的如

何让模型有

捕捉标志序

列

关系的能

力，而不是简

单依靠位置

嵌入。BERT 的训练

在目前的计

算资源下很

难完成，论文

中提到

BERTLARGE

需要

在 64 块 TPU

芯片上

训练 4 天，而一

块 TPU

的速度约

是目前主流

GPU 的 7 ～

8 倍。

非常幸

运的是 Google

开源

了各种语言

的模型，免去

了我们自己

训练的工作

。

5.5　BERT“魔改”之 RoBERTa、ALBERT、MT-DNN

和 XLM

在

本节中，先验

知识包括：



BERT（5.4 节

）；  GPT（5.3

节）。

第 5 章

模型

预训练

5.5.1 成熟

版 BERT

：RoBERTa

在 RoBERTa1 中，作者

指出

BERT 的原始

论文中的训

练超参数其

实并不能充

分发挥 BERT 的性

能，RoBERTa

旨在设计

一组实验，充

分发挥 BERT 的性

能，可以说 RoBERTa

是

BERT 的成熟版。

在

原始 BERT

的基础

上，RoBERTa 主要做了

如下改进：

z 使

用动态的掩

码；

z 移除 NSP 任务

；

z 使用更大字

节级的字典

；

z 使用更大的

批次大小；

z 使

用更长的训

练步数；

z 使用

更多的训练

数据。

1．动态掩

码

原始的 BERT 使

用的是静态

掩码，也就是

在数据预处

理阶段对序

列进行掩码

，因此输入模

型

中的每一

个被替换为

掩码的句子

是一样的。而

RoBERTa 使用的是动

态掩码，也就

是在训练阶

段

随机生成

掩码序列，因

此每次输入

网络中的序

列是不同的

。动态掩码的

引入增加了

数据的多样

性，

能提升网

络的性能，也

符合我们的

直觉。可以看

出，动态掩码

和

XLNet2 中的排列

语言模型是

非常

像的。

2．NSP

任

务

在 RoBERTa 中，作者

做了以下几

组实验来验

证

NSP 任务的有

效性。

z 分割对

+

NSP ：传统的 BERT 的输

入和

NSP 任务，每

个输入的序

列长度均小

于 512。

z

句子对 + NSP ：输

入和

BERT 相同，但

是每一对的

长度远小于

512，因此采用的

批次大小

大

于 512。

z 完整句子

（full sentence）：不截断句子

，句子可能跨

文档，不使用

NSP 损失。

z 文章中

的句子（doc sentence）：数据

同完整句子

，但是使用动

态的批次大

小。

实验结果

表明，不使用

NSP

损失的任务

要略优于使

用 NSP 损失的任

务，文章中的

句子的效果

要优于完整

的句子。

3．字节

级字典

BERT使用

的是词级别

编码，字节对

编码3

便是词

级别编码的

一种。字节对

编码是字符

级和词表

级

表征的混合

，它是一种简

单的数据压

缩形式，使用

数据中不存

在的一个字

节替换最常

出现的连续

字节，从而实

现数据的压

缩。字节对编

码词表的大

小通常在 10KB

～ 100KB，词

表中的元素

大多是

Unicode 编码

，RoBERTa

效仿 GPT-2 使用字

节替代了 Unicode

编

码，将词表的

大小控制到

了 5 万，

并且没

有引入未知

（unknown）标识符，而

BERT 使

用的词表的

大小约为 3 万

。但是实验结

果表明

这个

改动对准确

率的影响不

是很大。

4．其他

优化

z 批次大

小：BERT

的批次大

小为 256，RoBERTa 通过实

验证明批次

大小越大，模

型的效果

越

好，最终使用

了

8000 的批次大

小。

1　参见

Yinhan Liu、Myle Ott、Naman Goyal

等人

的论文“RoBERTa: A Robustly Optimized

BERT Pretraining Approach”。

2

参见

Zhilin Yang、Zihang Dai、Yiming Yang

等人的论文

“XLNet: Generalized Autoregressive Pretraining

for Language 

Understanding”。

3　参见 Rico Sennrich、Barry

Haddow、Alexandra Birch 的论文

“Neural Machine

Translation of Rare Words

with Subword 

Units”。

206

5.5　BERT“魔改”之 RoBERTa、ALBERT、MT-DNN

和 XLM

z 更

多步数：实验

结果表明，训

练步数越多

，模型效果越

好，最终

RoBERTa 的训

练步数为

50 万

。

z 更多的训练

数据：训练数

据从 BERT 的

16GB 扩充

到了 160GB。

z

其他超

参数：Adam 的 β2 从

0.999 调

整为 0.98，峰值学

习率从 1

× 10−4 调整

为 4

× 10−4

，使用

预热

（warm

up）策略来调整

学习率。

z 预热

：在训练初始

阶段使用比

较小的学习

率来启动，然

后切换到大

的学习率后

进行衰减。

5.5.2

更

快的 BERT ：ALBERT

ALBERT1

是“A Lite BERT”的缩

写，顾名思义

ALBERT 是一个更快

的

BERT 模型。ALBERT

将 BERT

速

度慢的原因

归为两类。

内

存限制和通

信开销：BERTLARGE 采用

由 24

层 Transformer 组成的

结构，总共约

有 3.4

亿的参

数

数量。对 BERT 的结

构进行些许

修改都要从

头训练这个

模型，从训练

一个

BERTLARGE 消耗的

资源

来看，这

并不是所有

个人和企业

都有实力完

成的。

模型退

化：在调整

BERT 的

超参数的过

程中，BERT 也会遇

到模型退化

问题，例如将

隐层节点

的

个数从

1 024 个增

加到 2

048 个，模型

的准确率反

而下降了。

为

了解决这个

问题，ALBERT 做了如

下

3 点改进。

1．嵌

入参数分解

在 BERT

中我们通

常需要把单

词的独热编

码转换成一

个特征向量

，这个特征向

量的长度 H 往

往

比较大。而

当我们的词

表

V 也很大时

，仅嵌入矩阵

就要有 V× H

个参

数，在 BERT 中，这个

矩阵的

参数

数量达到了

千万级别。ALBERT

将

嵌入矩阵拆

分成大小分

别为 V×E 和 E×

H 的小

矩阵，一般 E

小

于或等于

H。

2．跨

层参数共享

在 BERT 中每一层

都有独立的

参数，为了减

少参数数量

，ALBERT

中使用了参

数共享的概

念。

所谓共享

是指只训练

一层 Transformer 的参数

，然后在之后

的网络中共

享这个参数

。ALBERT

指出

可以共

享一层中的

一部分，也可

以共享整个

层，ALBERT 使用的是

共享整个层

的方式。

3．句子

顺序预测

很

多算法都证

明了 BERT 的 NSP

并不

是一个有效

的预训练任

务。ALBERT 中提出的

句子顺序

预

测（sentence order

prediction，SOP）是一个比

NSP 有效的任务

。它的正样本

是随机采样

的连续

的两

个句子，负样

本是将这两

个句子顺序

交换后得到

的样本。SOP 任务

用来预测这

两个句子的

顺序

是正确

的还是错误

的。

5.5.3 多任务 BERT

：MT-DNN

MT-DNN2 是

一个采用 BERT

架

构的算法，不

同的是 MT-DNN 在下

游任务中引

入了多任务

学习

机制。它

的网络结构

采用的是多

任务模型中

经常用的策

略，即在特征

提取部分共

享权值，在任

务相

关部分

参数独立，如

图 5.16 所示。

1

参见

Zhenzhong Lan、Mingda Chen、Sebastian Goodman

等人的论文

“Albert: A lite bert

for self-supervised learning of

language 

representations”。

2

参见 Xiaodong Liu、PengchengHe、Weizhu Chen

等人的

论文“Multi-Task Deep Neural Networks

for Natural Language

Understanding”。

207

第 5

章  模

型预训练

208

图

5.16 MT-DNN 的网络结构

MT-DNN 共使用了

4 类

自然语言理

解（natural language understand，NLU）任务。

z 单句

分类任务（single-sentence classification task，SCT）：判

断一个句子

所属的类别

。

z 文本相似度

评分（similarity text scoring，STS）：判断两

个句子的相

似性。

z 句子对

关系分类（pairwise text classification，PTC）：判

断两个句子

的关系，包括

继承、冲

突、中

立等。

z 相关性

排序（relevance ranking

task，RRT）：给定一

个查询语句

和若干个候

选，判断查询

与候选之间

的相关性排

序。

在 MT-DNN 的多任

务中，分类任

务使用交叉

熵损失函数

，回归任务使

用最小均方

误差作为目

标值。

1．预训练

MT-DNN 的预训练采

用了和 BERT 一致

的结构和编

码方式，包括

基于

Transformer 的网络

结构

和编码

方式以及预

训练的任务

。在 MT-DNN

中这一部

分是被所有

任务共享的

，因此叫作共

享层，

共享层

由词典编码

器和 Transformer 编码器

组成，经过它

们得到的特

征向量分别

是图

5.16 中的

l1 和

l2。

2．单句分类

在

单句分类中

，对于一个给

定的句子 X，它

的标签值为

c。经过共享层

得到特征向

量 l2，其中

x 是 l2 中

标志为

[cls] 的特

征向量，SST 任务

表示为式（5.19）。

（5.19）

3．文

本相似度评

分

在 MT-DNN 中，两个

句子（X1、X2）会通过

[sep]

标识切分后

共同送入网

络中，它们的

相似度

通过

x 进行直接计

算，如式（5.20）所示

。

（5.20）

4．句子对关系

分类

给定前

提 G =

(g1,…, gm) 和假设 H

= (h1,…, hn)，句

子对关系分

类任务的目

标是判断 G

与

H 的逻

5.5　BERT“魔改”之

RoBERTa、ALBERT、MT-DNN

和 XLM

209

辑关系

R。MT-DNN 的

这个任务采

用了 SAN（stochastic answer

network）1 的网络

结构。SAN 的迭代

计算过程如

图 5.17

所示，它是

一个多步推

理模型。

图 5.17 SAN

的

迭代计算过

程

我们使用

共享层分别

得出 G 和

H 的工

作内存（working memory），分别

表示为 和

。

在

这两个工作

内存的基础

上执行 T 步推

理，其中

T 是一

个超参数。在

开始阶段，初

始状态 s0

使用

的是

MH 的加权

和，如式（5.21）所示

。

（5.21）

使用

MG 计算输

入特征 xt

，其中

t

∈ 0, 1, …,

T − 1，如式（5.22）所示。

（5.22）

使

用 GRU 迭代更新

st ：st

= GRU(st−1,xt

)。

计算每个时

间片预测的

关系概率：

。

最

终的概率分

布是所有时

间片的均值

： 。

5．相关性排序

对于一个候

选对

(Q,A)，相关度

的计算方式

如式（5.23）所示。

（5.23）

它

的损失函数

如式（5.24）所示：

（5.24）

其

中，A是候选假

设的集合，由

一个正样本

A+

和|A| − 1个负样本

组成。

5.5.4 多语言

BERT ：XLM

上面介绍的

语言模型都

是单语言的

预训练语言

模型，其他语

言类型的预

训练语言模

型都是在此

1

参见 Xiaodong Liu、Kevin Duh、Jianfeng

Gao 的论文

“Stochastic Answer Networks

for Natural Language Inference”。

第 5 章

模型预

训练

210

基础上

换一个其他

语言的语料

库进行从头

训练的。这里

要介绍的 XLM1

是

一个跨语言

的预训练语

言

模型，它由

基于单语料

的无监督任

务和基于平

行语料的有

监督任务组

成。XLM 在很多跨

语言的场

景

中取得了媲

美主流算法

的效果，如机

器翻译等。

1．跨

语言共享字

典

在 XLM 中，所有

语言通过字

节级编码共

享字典。但是

不同语言的

语料库的大

小差异非常

大，

传统的字

节级编码会

导致字典偏

向于高频语

料库，导致低

频语料库被

切分成以词

为单位。为了

解决

这个问

题，作者提出

了一个新的

采样算法。

假

设一个数据

集由 N

个语言

构成，那么这

个数据集可

以表示为 {Ci

}i=1,…,N。一

个句子被采

样的

概率

{qi

}i=1,…,N 服

从多项分布

，表示为式（5.25）：

（5.25）

其

中，ni是第i个语

言的样本数

，a在实验中的

值是0.5。通过这

个采样方法

，低频样本会

有更高的概

率被采样到

。

2．因果语言模

型

因果语言

模型（causal language

model，CLM）通过一

个单词之前

时间片的单

词预测当前

时

间片的单

词，表示为 P(wt

|w1,…,wt

− 1,θ)。它

的思想和单

向 RNN 语言模型

一致，但是使

用的是

Transformer，也就

是 GPT 中使用的

模型。

3．XLM

中的 MLM

XLM 中

也使用了

MLM，它

和 BERT 中的 MLM

有 3 点

不同；

z

XLM 的文本

流并不限制

句子的个数

，直到长度大

于 256 后进行截

断；

z 对于低频

词，XLM 使用了上

采样的方法

进行扩充；

z

XLM 中

的 MLM 在输入中

加入了语种

编码。

XLM 中的 MLM 如

图

5.18 所示。

图 5.18

XLM 中

的 MLM

4．翻译语言

模型

为了训

练 XLM 对平行语

料的学习能

力，XLM 引入了翻

译语言模型

（translation

language 

model，TLM）。在 TLM

中，输入是

拼接的平行

语料，然后通

过将任意语

言的一部分

随机替换为

掩

码来进行

建模。通过 TLM，XLM 不

仅学习了语

言内部的相

互关系，而且

学会了语言

之间的对齐

关

系。和 MLM 一样

，TLM 的输入也加

入了语种编

码，如图

5.19 所示

。

5．XLM 的应用场景

跨语言分类

：这个场景使

用的是

XNLI 数据

集，XNLI 是一个包

含 15

种语言的

文本分类数

据集。

在这个

场景中，作者

使用英语进

行微调，然后

在其他语言

上进行测试

。

1　参见

Guillaume Lample、Alexis Conneau 的论文

“Cross-lingual

Language Model Pretraining”。

5.6

XLNet

211

图 5.19

XLM 的 TLM

无监督

机器翻译：机

器翻译模型

一般采用的

是编码器

- 解

码器架构，使

用 XLM 对机器翻

译模

型进行

初始化有多

种方式，即编

码器和解码

器可以分别

使用随机初

始化、使用 CLM 预

训练、使用

MLM

预

训练。

有监督

机器翻译：编

码器和解码

器都使用 MLM 训

练的模型进

行初始化。

低

频语言模型

：通过 XLM 训练的

低频语料的

语言模型的

效果比只使

用单一语料

训练的语言

模

型的效果

要好很多。

无

监督跨语言

词嵌入：通过

对比余弦相

似度（cosine similarity）等指标

，作者发现 XLM 的

效

果也比其

他无监督模

型的效果要

好。

5.5.5 小结

因为

BERT

的流行，所以

很多后续算

法都采用了

BERT 作为基线进

行比较。RoBERTa 在不

修

改

BERT 模型的

基础上，通过

一些训练技

巧将 BERT 的准确

率提升到更

高的值，之后

设计的新模

型

恐怕要和

RoBERTa 对比才更有

说服力。而 RoBERTa 和

XLNet

孰优孰劣则

恐怕需要更

为严谨的实

验和理论分

析才能验证

。

BERT 的流行也引

发了“魔改”BERT 的

热潮，这篇论

文介绍了几

个经典且效

果比较好的

BERT

衍生算法。其

中，MT-DNN 通过引入

多任务的方

式来提高模

型的泛化能

力；XLM 引入了多

语

言模型任

务，在高频语

料和低频语

料上均取得

了非常好的

效果；ALBERT

通过矩

阵分解、权值

共享

等方案

大幅降低了

BERT 的计算量，大

幅提升了训

练和预测速

度。

5.6

XLNet

在本节中

，先验知识包

括：

 Transformer（4.3

节）；  Transformer-XL（4.4 节）；

 ELMo（5.2 节）； 

BERT（5.4 节

）。

自回归（auto regressive，AR）语言

模型和自编

码（auto

encoder，AE）语言模型

是构建语言

模型

最常用

的两个基础

算法，例如我

们之前分析

过的 GPT 系列就

是经典的自

回归语言模

型。自编码语

言

模型的典

型代表则是

近年流行的

BERT。但是这两个

模型都有其

各自的优缺

点。自回归语

言模型的

优

点是符合真

实的建模场

景，因为很多

NLP 任务都是从

前向后的，缺

点则是模型

只能用到之

前时间

第

5 章

模型预训练

212

片的信息，ELMo

虽

然用了双向

的 LSTM 构建模型

，但是效果并

不好。BERT 的优点

是能够同时

用到

被预测

单词的上文

和下文，缺点

主要是掩码

导致训练阶

段和微调阶

段不一致，因

为在微调阶

段是看

不到

掩码的。基于

这些优缺点

，作者提出了

一种结合了

自回归和自

编码两个方

法优点的模

型 XLNet。

首先，XLNet

可以

通过最大化

所有因式分

解顺序（factorization order）的排

列变换的结

果，学习双向

语境信息。其

次，XLNet 使用自回

归语言模型

，解决了 BERT

的训

练阶段和微

调阶段不一

致的问题。

XLNet 在

20 个任务上实

现了对

BERT 的全

面超越，并且

在其中 18 个任

务上取得了

更优的准确

率。

5.6.1 背景知识

1．自回归语言

模型

顾名思

义，自回归语

言模型使用

自回归模型

对语言模型

进行建模。自

回归语言模

型最常见的

结

构便是使

用

LSTM 或者 Transformer 按照

从左到右的

顺序依次预

测每个时间

片的输出结

果，如

GPT

系列等

。另外一种结

构是同时按

照从左到右

和从右到左

的顺序进行

自回归，其中

典型的代表

便是

ELMo。ELMo 虽然同

时用到了上

下文的信息

，但其仍然是

一个自回归

语言模型，因

为

ELMo 分别

进行

了从左到右

的自回归和

从右到左的

自回归，然后

将这两个模

型的隐层节

点状态拼接

到一起。在

进

行某一个语

言模型建模

时，ELMo

并没有使

用另外一个

语言模型的

特征，它其实

就是自回归

语言

模型的

拼接，并没有

跳出自回归

语言模型的

范畴。

什么是

自回归语言

模型？具体地

讲，给定一个

文本序列 X

= {x1,…,xT}，它

可以通过正

向的条件

概

率 或者反向

的条件概率

对这个序列

进行建模。可

以看出，

在进

行单向语言

模型的训练

时，并不会用

到另外一侧

的文本信息

，这对语言模

型的效果的

影响是非

常

大的。在进行

优化时，一个

前向的自回

归语言模型

通过最大化

每个时间片

预测结果的

最大似然来

完成语言模

型的预训练

，如式（5.26）所示：

（5.26）

其

中，X

＜ t 表示所有

时间片小于

t 时间片的数

据，即

X1:t−1。hθ (X1:t−1) 是 RNN

或者

Transformer，e(x)

是序列 x 的嵌

入。

2．自编码语

言模型

给定

一个输入文

本序列 X，在 BERT

的

掩码语言模

型中，我们首

先按照一定

的比例构建

一个被

替换

为掩码的序

列 。假设被替

换为掩码的

单词是 ，我们

的训练目标

便是根据

预

测 。自编码的

模型通过预

测被替换为

掩码的单词

来实现语言

模型的训练

，表示为式（5.27）：

（5.27）

其

中，mt

= 1 时表示 t

时

刻是一个掩

码，Hθ 是一个 Transformer，它

将长度为 T

的

序列编码成

长度为

T 的特

征向量 。对比

式（5.26）和式（5.27），我们

发现自回

归

可以看到的

时间片信息

的范围是从

1 到 t −

1，而自编码

是可以看到

所有时间片

的信息的。

输

入噪声：在 BERT 中

，句子中的每

个单词会有

15%

的概率被替

换为掩码，然

后通过句子

的

其他部分

来对当前时

间片的掩码

进行预测。可

以看出，BERT 本质

上是一个去

噪自编码器

（denosing

autoencoder），而其中的掩

码就相当于

噪声数据。加

入掩码之后

确实可以比

较方便编码

同时使用单

词的上下文

信息，但是这

个掩码在微

调的时候是

不存在的，导

致了训练阶

段和微调阶

段的不一致

问

题，因此在

式（5.27）中使用了

“≈”。

5.6　XLNet

213

独立性假设

：BERT 的另外一个

问题基于一

个句子的被

替换为掩码

的部分都是

相互独立的

，因

为在预测

一个时间片

的内容时，其

他被替换为

掩码的内容

并不能提供

有用的文本

信息，所以这

个假

设并不

成立。例如“New York is a

city”这

句话，被替换

为掩码的单

词是“New”和“York”，句子

变成了“[mask] [mask] is a

city”，当预

测“New”时，我们并

不知道另外

一个被替换

为掩码的单

词

是“York”，因此很

难得到正确

的预测结果

。

综上，XLNet 的出发

点就是能否

融合自回归

和自编码的

优点，创造一

个既能看见

上下文，又能

保证训练和

微调阶段一

致的模型。

5.6.2 XLNet 详

解

1．排列语言

模型

XLNet 提出了

排列语言模

型（permutation language model，PLM），PLM

其实就是

采用将自回

归和自编码

进行融合的

一个小技巧

，例如对一个

x1 → x2 →

x3 → x4 的序列来说

，假设我们要

预测

的序列

是 x3，我们需要

同时看到 x1、x2、x4，这

样才能解决

自回归的不

能同时看到

上下文的问

题。

具体实现

方式是，首先

将输入序列

进行打乱，再

从中随机选

择一个序列

作为输入，比

如选取到的

排

列为 x2 → x4

→ x3 → x1，那么

这个时候按

照自回归的

顺序依次对

这个序列进

行预测，在预

测

x3 的

时候就

能同时看到

它的上文 x2

和

下文 x4 的信息

了。所以 PLM

本质

上是一个先

进行打乱，再

从左

到右依

次预测的自

回归语言模

型，表示为式

（5.28）：

（5.28）

其中，ZT

是长度

为 T 的序列的

所有可能的

排列，Z 是其中

的一个排列

，

是该排列中

所有介于 1

和

t 之间的序列

，

表示为式（5.29）。

（5.29）

从

式（5.29）中我们也

可以看出 PLM

的

形式和自回

归的相同，它

做的就是从

所有的序列

中随

机抽样

一种，然后按

照自回归的

顺序依次进

行输入。XLNet 就是

通过这种方

式同时保证

看到上下文

且没有引入

标记的。

这种

全排列的方

式破坏了序

列

X 原始的排

列顺序，如果

不加入位置

编码的话，模

型是无法根

据

要预测时

间片的真实

位置进行预

测的。对于一

个句子的两

个序列 Z

(1) 和 Z

(2)，我

们假设这个

句子满足

式

（5.30）。例如句子“New York is a

city”的

两个序列“New is a city

York”和

“New is a York

city”。

（5.30）

式（5.30）中，i、j 是要预

测的单词在

句子中的实

际位置。那么

根据式（5.27），这两

个时间片

的

似然概率如

式（5.31）所示。

（5.31）

从式

（5.31）中我们可以

看出，不同目

标位置的 i

和

j 的预测结果

是完全一致

的，但是很明

显

它们的标

签值是不相

同的。为了避

免这个问题

，XLNet 中加入了目

标位置编码

Zt

，具体计算方

式如

式（5.32）所示

：

（5.32）

第

5 章  模型预

训练

214

其中， 表

示加入了位

置编码的一

种新的编码

方式，叫作双

流自注意力

（two-stream self-attention），我们将在后

文进行介绍

。

在上面的讲

解过程中，虽

然采用的是

先全排列再

随机抽取的

策

略（相当于

把原始输入

随机打乱），但

是我们在微

调的时候，并

不

能将句子

随机打乱后

再输入模型

。所以在训练

的时候，输入

还必须

保持

句子原始的

输入顺序，然

后在网络中

做一些操作

，来实现随机

打乱。具体地

讲，XLNet

采用了注

意力掩码的

模块来实现

对句子的

打

乱，如图 5.20 所示

，白色表示被

替换为掩码

的序列，红色

表示可

见的

序列。我们首

先生成序列

1 → 2 →

3 → 4 的一个随机

序列，如

3 → 2 →

4 → 1，然后

根据这个序

列生成掩码

，那么将掩码

和原始序

列

相乘后便可

以得到前面

介绍的

PLM 的功

能，例如在预

测 4 时，1

被替换

为掩码，2 和 3 可

见。

2．双流自注

意力

在式（5.32）中

，我们介绍了

一个新的编

码方式 ，这里

我们详细介

绍它的具体

内容。

根据上

面的介绍，我

们希望

满足

两个特性：

z 为

了预测 ，

只能

使用位置信

息 Zt 而不能使

用内容信息

，因为当知道

了 t

时

刻的内

容时，再对它

进行预测就

没有意义了

；

z 为了预测

Zt 之

后的单词， 必

须编码 ，这样

才能保证之

后的时间片

预测可以使

用上文的信

息。

这种特性

在传统的 Transformer-XL 中

是互相矛盾

的，因此作者

引入了双流

自注意力。双

流自注

意力

由内容流（content

stream）和

查询流（query stream）组成

，单看内容流

，如图 5.21 所示，

它

就是一个传

统的 Transformer-XL，因为它

同时使用位

置编码和内

容编码，表示

为式（5.33）：

（5.33）

其中，m

表

示自注意力

的层数，m = 1,…, M。在编

码

h1

(0) 时，键和值

会用到 h1

(0)、h2

(0)、h3

(0)、h4

(0)。

另外

一条分支是

查询流 ，如图

5.22 所示。在计算

查询时，只会

用到之前时

间片 h2

(0)、h3

(0)、

h4

(0)

的内容

信息，因此 Q = g1

(0) 查

询流起到了

和 BERT 中掩码操

作类似的功

能。因为

XLNet 抛弃

了

掩码操作

，于是在查询

流中，它直接

忽略了当前

预测时间片

的实际内容

，而只保留了

这个时间片

的

位置信息

，如式（5.34）所示（注

意，h

的下标是

小于而不是

小于或等于

）。

（5.34）

图 5.21

XLNet 的内容流

图 5.22 XLNet

的查询流

根据式（5.33）和式

（5.34）我们可以看

出，内容流和

查询流在计

算时，分别使

用的是它们

自己的上一

个时间片的

内容向量 和

查询向量 ，但

是在计算 K

和

V 时，则使用了

内容向量

图

5.20 XLNet

的注意力掩

码

5.6　XLNet

215

，这也就保

证了特性 2。但

是在计算查

询流 时，K 和

V 使

用的是 ，保证

了查询流不

能

看到

的内

容，因此保证

了特性 1。

在初

始化的时候

，查询流的值

初始化为一

个可以训练

的向量，即 ，内

容流使用单

词对

应的嵌

入向量进行

初始化 。

在微

调的时候，XLNet 会

将查询流去

掉而只用内

容流，最后在

计算式的时

候再用最上

面的

Q

向量 。

图

5.23

系统概括了

XLNet 的排列语言

模型和双流

自注意力机

制，这里我们

结合图 5.23 再举

例

说明一下

XLNet。假设一个长

度为 4 的序列

经过排列之

后顺序变成

了 3

→ 2 → 4

→ 1，首先我们

会

根据这个

排列方式为

内容流和查

询流各生成

一组注意力

掩码，其中查

询流不能看

到其本身，内

容

流可以看

到，两组掩码

如图

5.23（c）的右侧

所示。然后将

注意力掩码

作用到多层

Transformer(-XL)

的每一层，进

行模型的训

练。在图 5.23 所示

的例子中

Transformer(-XL) 的

层数为 2。在初

始化的时

候

，蓝色的内容

向量使用词

向量作为初

始值，绿色的

查询向量使

用可训练的

w

作为初始值

。最后，

在最终

预测的时候

，使用查询向

量的结果计

算概率分布

。

图 5.23

XLNet 的排列语

言模型和双

流自注意力

3．部分预测

如

果按照上面

介绍的方法

从头开始预

测一个句子

，在没有上下

文的情况下

，前几个时间

片的内

容基

本上是不可

能预测的，这

使得训练很

难收敛。为了

解决这个问

题，在训练时

，XLNet

只对部分

句

子进行预测

。具体地讲，XLNet 中

引入了一个

分隔符 c，对于

一个随机打

乱的序列

Z，它

被分成

非目

标子序列 Z≤c 和

目标子序列

Z＞c，此时优化目

标变成了根

据非目标子

序列预测目

标子序列的

条

件概率模

型，表示为式

（5.35）。

（5.35）

那么 XLNet

中是如

何设置这个

分隔符的呢

？ XLNet 设置了一个

超参数 K，一个

句子中的

1/K

个

单词是需要

被预测的，即

|Z|/(|Z − c|)≈K，实验结果表

明最佳的

K 值

介于 6 和

7 之间

，将其转换为

百分比后对

应的值介于

14.3% 与 16.7%

之间。巧合

的是，在 BERT 中一

个单词被替

换为掩码的

概

率是

15%，恰好

也在这个范

围之内。

4．Transformer-XL

对 XLNet

来

说，Transformer 或 者 Transformer-XL

从 理

论 上 来

说 都

是 可 以

的， 考

虑 到

第

5 章  模

型预训练

216

Transformer-XL 的

相对位置编

码和分段自

回归机制的

特性，作者选

择了 Transformer-XL

作为基

础框

架，XLNet 也因

此得名。

对相

对位置编码

来说，很明显

我们应使用

原始输入的

位置编码，因

为随机排列

之后的位置

编码

是没有

意义的。因为

Transformer-XL 的分段自回

归机制将输

入序列分成

若干个片段

，然后递归式

地

将前一个

片段的节点

特征提供给

当前片段，那

么它是如何

与片段自回

归机制进行

配合的呢？举

例来

说，假设

一个长序列

s

被分成两个

片段 和 ， 和

Z 是

两个随机打

乱的序列。在

计算当

前时

间片的特征

编码时，Transformer-XL 使用

的是上一个

时间片这个

片段的特征

和当前时间

片

前一个片

段的特征 ，式

（5.36）中 [·,·] 表示拼接

操作。

（5.36）

5.6.3 小结

通

过上面的分

析我们发现

XLNet

和 BERT 在结构上

看似有很多

不同点，但是

在本质上它

们是

非常相

似的，它们都

通过某些方

法使用句子

中的一部分

来预测句子

中的另外一

部分，甚至它

们连要

预测

的单词的比

例都控制在

了 15%。不同的是

，BERT 非常显式地

使用掩码，而

XLNet 则使用了

注

意力掩码在

网络内部进

行单词的掩

码，从而解决

了论文中所

说的训练阶

段和微调阶

段不一致的

问

题。当整个

句子中只有

一个单词需

要预测时，XLNet 和

BERT 基本是等价

的。

从模型的

角度讲，XLNet 的提

升点在于使

用了 Transformer-XL，解决了

Transformer“天生”不

善于

处理长文本

的缺点。另外

从实验结果

上来看，全面

的提升不能

减少使用比

BERT

更多的训练

数

据带来的

影响，因此我

们也不能过

分地神化 XLNet。

5.7

ERNIE（清

华大学）

在本

节中，先验知

识包括：

 Transformer（4.3

节）；  BERT（5.4 节

）。

AI 的跨方向融

合一直是非

常火热的研

究领域，其中

一个典型的

应用便是将

知识图谱

（knowledge graph）和

预训练语言

模型进行融

合。知识图谱

的引入，将纯

文本的预训

练任务变成

文

本加知识

的预训练任

务。从这个角

度讲，加入知

识图谱的任

务会使模型

拥有更强的

语义理解能

力，

因此模型

也会变得更

加“智能”。

巧合

的是在 2019

年同

一时期，我们

的清华团队

和百度团队

分别提出了

自己的结合

了知识图谱

的预训练语

言模型 ERNIE（Enhanced language Representation

with Informative Entity），为了

区分这

两个

命名“撞车”的

模型，在后文

中我们分别

将其称为

ERNIE-T1 和

ERNIE-B2

。本节我们先

介绍清

华团

队提出的

ERNIE-T。

ERNIE-T 的

核心在于引

入了知识图

谱中的命名

实体，并根据

命名实体进

行了下面 3

点

优化：

z 设计了

可以融合 BERT

和

知识图谱两

个异构信息

的网络结构

；

z 加入了基于

知识图谱的

无监督任务

去噪实体自

编码器；

1

参见

Zhengyan Zhang、Xu Han、Zhiyuan Liu

等人的论文

“ERNIE: Enhanced Language Representation

with Informative 

Entities”。

2　参见 Yu Sun、Shuohuan

Wang、Yukun Li 等人的

论文“ERNIE: Enhanced

Representation through Knowledge Integration”。

5.7　ERNIE（清华大

学）

217

z

加入了知

识图谱的实

体类别（entity typing）识别

任务和关系

分类（relation classification）

任务进

行模型微调

。

5.7.1 加入知识图

谱的动机

ERNIE-T 的

基本思路就

是在基于

BERT 的

语言模型中

加入知识图

谱中的命名

实体的先验

知识，

它的动

机非常直观

，例如下面的

句子，我们的

任务目标是

对“Bob Dylan”进行实体

类别识别。所

谓实体类别

识别，是指给

定一个实体

指代（entity

mention），根据实

体的上下文

信息来对实

体的具

体语

义类别进行

预测。例如在

示例 1 中，Bob

Dylan 的实

体类别的值

为“词曲作者

”和“作家”，

因为

“Blowin’in the

wind”是一首歌，“Chronicies ：Volume One”是

一本书。对 BERT

来

说，仅仅

通过

上下文是很

难预测出正

确的结果的

，因为它没有

关于这两个

作品类别的

信息。

示例 1：Bob

Dylan wrote Blowin’in the

Wind in 1962, and

wrote Chronicies: Volume One

in 2004.

但

是如果我们

引入了实体

额外的知识

图谱呢？在图

5.24 中，蓝色实线

表示图谱中

存在的事实

，

红色和绿色

虚线表示从

红色上下文

和绿色上下

文中提取的

事实。根据图

谱提供的信

息，我们知道

了 Blowin’in the wind”是一首歌

以及“Chronicies

：Volume One”是一本

书，那么我们

再对“Boby 

Dylan”进行实

体类别识别

就容易多了

。

图 5.24　示例 1

对应

的知识图谱

5.7.2 异构信息融

合

因为词向

量和实体的

知识图谱是

两个异构的

信息，那么如

何设计一个

网络来融合

这两个信息

则

是我们需

要解决的首

要问题。如图

5.25

所示，ERNIE-T 由两个

模块组成，它

们是底层的

文本编码

器

（textual Encoder，T-Encoder）和上层的知

识编码器（knowledge

Encoder，K-Encoder）。

图

5.25（a）展示的是 ERNIE-T 的

整体结构，图

5.25（b）展示的是

K- 编

码器的展开

结构，展

示了

如何融合文

本特征和知

识特征。

1．T-

编码

器

在 T- 编码器

（T-Encoder）中，我们使用

N

个多头 Transformer 将输

入标志编码

成特征向量

，这个

特征向

量由词嵌入

、分割嵌入和

位置嵌入

3 部

分组成，可以

看出这一部

分就是一个

标准的 BERT。我

们

将输入标志

定义为

{w1,…,wn}，T- 编码

器的生成特

征 {w1,…,wn} 的计算过

程可以抽象

为式（5.37）。

{w1,…,wn} = T-Encoder({w1,…,wn}) （5.37）

第 5 章

模

型预训练

218

图

5.25 ERNIE-T

的两层网络

结构

5.7　ERNIE（清华大

学）

219

2．K- 编码器

对

于输入标志

序列 {w1,…,wn}，它的实

体序列表示

为

{e1,…,en}，其中 m ＜ n。在将

数据输入

K-

编

码器之前，我

们首先使用

TransE1 将其转换为

实体嵌入 {e1,…,em}。关

于

TransE 的计算方

法，我们

放在

后面讲解。

在

K-

编码器中，它

会同时将这

两个序列作

为输入并同

时得到这两

个序列的输

出，表示为式

（5.38）2

。

（5.38）

式（5.38）中，

和 是 K- 编

码器输出的

词向量和实

体向量。

那么

，K-Encoder 是如何将两

者进行融合

的呢？如图 5.25 所

示，K-

编码器是

由 M 个整合器

（aggregrator）组成的，对于

第 i

个整合器

的输入 和 ，它

首先通过两

组

参数不共

享的多头注

意力（MH-ATTs）得到两

组特征向量

，如式（5.39）所示。

（5.39）

对

于第j个词向

量wj及其对齐

的实体ej

，它们

的计算方式

如式（5.40）所示，其

中W表示权值

矩阵。

（5.40）

式（5.40）中最

关键的是变

量 hj

，它实现了

词向量和实

体向量的融

合。而对于没

有实体的标

志，

它的计算

方式如式（5.41）所

示。

（5.41）

上式中 σ 是

GELU

激活函数，它

有两组近似

表达式，如式

（5.42）或者式（5.43）所示

，它的

曲线如

图 5.26 所示。

（5.42）

（5.43）

图 5.26

GELU 函

数曲线

1　参见

Antoine

Bordes、Nicolas Usunier、Alberto Garcia-Durán 等人的论文

“Translating

Embeddings for Modeling Multi￾relational

Data”。

2　论文中 e

的角

标为 n，应该是

个笔误。

第 5

章

模型预训练

220

综上，第 i

个整

合器可以简

化为式（5.44）。

（5.44）

3．TransE

在多

关系数据的

知识图谱中

，图中的节点

是实体，节点

之间的边表

示一个三元

组

(h,l,t)（head、

label、tail），其中标签

（label）表示实体头

（head）和实体尾（tail）之

间的关系，我

们要学习的

便

是这两个

实体之间的

关系 l。

TransE 的训练

思想是使用

嵌入之间的

转移来表示

关系，我们希

望实体头沿

着标签关系

转移可以

得

到实体尾，即

如果存在关

系（h,l,t），我们希望

尽量满足关

系 h

+ l≈t，反之则尽

量远离。在 TransE

中

，我们使用

d(h,l,t) 来

表示 3 个变量

的距离，其中

d

可以是 l1 或者

l2 距离，综上

TransE 的

损失函

数可

以表示为式

（5.45）：

（5.45）

其中，[x]+ 表示 x 的

正数部分，并

且

表示三元

组中的头部

或

者尾部会

被替换成其

他的实体，用

来构造不正

确的三元组

。TransE 使用 SGD

来对式

（5.45）进行

训练。在

ERNIE-T 中，使用的是

基于 Wikipedia

的数据

构建的知识

图谱，包含 5 040 986

个

实体和

24 267 796

个三

元组。

5.7.3 DAE

去噪实

体自编码器

（denosing

entity auto-encoder，DAE）是指随机被

替换为掩码

的一些实体

，

并要求模型

基于与实体

对齐的标志

，从给定的实

体序列中预

测最有可能

的实体类别

，第 i

个标志 wi

的

实体分布表

示为式（5.46）：

（5.46）

其中

，linear 表示线性层

。DAE 的损失函数

使用的是交

叉熵损失函

数。

如果使用

TransE

进行实体嵌

入映射的话

，可能会带来

两个问题，一

是错误的映

射，二是空映

射。为了解决

这些问题，DAE 将

数据分成了

下面 3 种方式

处理：

z 将 5% 的标

志对齐的实

体替换为一

个随机的实

体，用于解决

实体没有对

齐的问题；

z 将

15% 的标志对齐

的实体替换

为掩码，用于

解决没有找

到标志对应

的实体的问

题；

z

剩下的不

做任何处理

，用于让模型

学习到更多

的语义信息

。

最终，ERNIE-T 效仿 BERT

使

用了 MLM 和 NSP，再加

上这里的

DAE 共

同作为预训

练的目标，

损

失函数采用

MLM、NSP 模型、DAE

加和的

形式，如式（5.47）所

示。

（5.47）

5.7.4 ERNIE-T

的微调

ERNIE-T 的

微调包含 3

类

无监督任务

，如图 5.27 所示，它

们依次如下

。

z

NLP 分类任务：采

用和 BERT 相同的

常见

NLP 任务。

z 实

体类别识别

：预测实体的

语义类别。

z 关

系分类：用于

预测两个实

体之间的关

系，如因果关

系等。

5.8　ERNIE（百度）和

ERNIE

2.0

221

图 5.27

ERNIE-T 的任务

3 类

任务中，实体

类别识别和

关系分类是

ERNIE-T

中额外添加

的基于知识

图谱的任务

，这里

我们对

它们进行详

细分析。

1．实体

类别识别

在

实体类别识

别中，ERNIE-T

使用了

[ENT] 标识符来标

注实体指代

的位置，例如

图 5.27 中的

实体

“mark twain”。这样模型在

进行实体类

别识别时便

可以根据 [ENT] 来

确定实体指

代的位置，并

根据整个句

子来确定上

下文的信息

。最后根据

[ENT] 标

识符来预测

实体的类别

。

2．关系分类

关

系分类是指

确定实体头

和实体尾之

间的关系，因

此

ERNIE-T 采用了 [HD] 和

[TL]

两个标识

符

来确定位置

。然后根据 [CLS] 来

确定关系的

类别。

因为在

关系分类和

实体类别识

别中引入了

额外的位置

标注符，所以

ERNIE-T 使用了占位

符（图

5.28 中的虚

线框）来对齐

各个任务。

5.7.5 小

结

多方向跨

界融合一直

是 AI

界非常重

要的领域，ERNIE-T 便

是一个将 NLP 和

知识图谱进

行融合

的经

典范例。通过

提取知识图

谱中的实体

以及实体之

间的关系信

息，给予了模

型更强的表

达语义信

息

的能力。ERNIE-T 的跨

界融合也深

入模型训练

的各个方向

，从模型设计

到预训练，再

到模型微

调

都根据知识

图谱进行了

针对性的设

计。

作为一篇

知识图谱和

NLP 跨界融合的

前沿性论文

，ERNIE-T 的论文依旧

存在几个问

题：

z

模型严重

依赖于构建

的知识图谱

，如果知识图

谱有分布倾

向的话，可能

会限制模型

的泛化

能力

；

z 构建知识图

谱的计算量

比较大，而构

建泛化能力

强的知识图

谱对数据的

要求过高；

z ERNIE-T 的

融合方式有

些粗暴。

5.8

ERNIE（百度

）和 ERNIE 2.0

在本节中

，先验知识包

括：

 Transformer（4.3 节）； 

BERT（5.4 节）。

之前

我们介绍了

清华团队提

出的 ERNIE，在同一

年百度也发

表了一篇与

其命名相同

的

ERNIE，

第 5 章

模型

预训练

而且

非常雷同的

是它们都引

入了外部信

息作为辅助

，为了区分我

们将它们分

别命名为 ERNIE-T 和

ERNIE-B。在同一年，百

度又推出了

ERNIE-B

的下一个版

本，这里我们

将它命名为

ERNIE 2.01

。

ERNIE-B

的核心点在

于它在训练

的过程中加

入了短语级

别掩码和实

体级别掩码

，这两个额外

的掩码都是

通过外部知

识获得的。ERNIE-B 的

另外一个核

心点在于通

过百度贴吧

的海量数据

，引

入了一个

对话语言模

型无监督任

务，通过预测

查询和响应

的关系来计

算损失值。

ERNIE

2.0 的

核心点是引

入了持续预

训练学习，解

决了灾难性

遗忘的问题

。其实个人认

为更重

要的

一点是它引

入了大量的

预训练任务

，包括单词感

知、结构感知

和语义感知

3 类。

5.8.1 ERNIE-B

1．ERNIE-B 的动机

ERNIE-B 和

ERNIE-T 都认为 BERT

在数

据上的随机

掩码是存在

非常严重的

问题的，如示

例 2。

其中“Harry Potter”和“J.K.

Rowling”是

两个实体，如

果我们用掩

码替换的是

这两个实体

中的一

个单

词，那么模型

还是很好预

测的。但是如

果我们用掩

码替换的是

整个实体，如

“J.K. Rowling”，

那么我们可

以根据图书

名字预测它

的作者，这时

候模型才真

正学到了知

识。

示例 2：Harry Potter is

a series of fantasy

novels written by J.K.

Rowling.

同理

，对于一个词

组或者短语

，如果我们只

是用掩码替

换了词组的

一部分，那么

模型也很容

易

根据词组

的剩余部分

学到被替换

为掩码的部

分。所以，作者

认为如果替

换为掩码，就

以词组或者

实

体为单位

来替换，这样

才能保证模

型学到真正

的语言知识

。

最后，ERNIE-B 使用的

网络结构就

是 BERT 的多头注

意力的

Transformer，这里

不赘述。

2．ERNIE-B 的掩

码机制

ERNIE-B

的 3 种

掩码机制如

图 5.28

所示，它由

BERT 的随机掩码

再加前面我

们介绍的短

语级

别掩码

和实体级别

掩码组成。

图

5.28

ERNIE-B 的 3 种掩码机

制

基本级别

掩码（basic-level masking）：这里采

用了和 BERT 完全

相同的掩码

机制，即随机

掩码，

在使用

中文语料时

，这里使用的

是字符级别

的掩码。在这

个阶段并没

有加入更高

级别的语义

知识。

短语级

别掩码（phrase-level masking）：在这

个阶段，首先

使用语法分

析工具得到

一个句子中

的短语，例如

图 5.28

中的“a series of”，然后

随机掩码掉

一部分，并使

用剩下的内

容对该短语

进行

预测。在

这个阶段，词

嵌入中加入

了短语信息

。

实体级别掩

码（entity-level masking）：在这个阶

段，将句子中

的某些实体

替换为掩码

，这样模

型就

有了学习更

高级别的语

义信息的能

力。

3．对话语言

模型

得益于

百度贴吧巨

大的数据量

，ERNIE-B 使用了海量

的对话内容

，因此在 ERNIE-B 中使

用了

对话语

言模型（dialogue language model，DLM）。作者

认为一组对

话可能有多

种形式，例如

QRQ、

1

参见 Yu Sun、Shuohuan Wang、Yukun

Li 等人的

论文“ERNIE 2.0: A

Continual Pre-Training Framework for

Language 

Understanding”。

222

5.8　ERNIE（百度）和

ERNIE 2.0

QRR、QQR

等（Q 表示 query，R 表示

response）。为了处理这

种多样性，ERNIE-B

给

输入嵌入加

入了对话嵌

入（dialogue embedding）特征。另外

在 ERNIE-B 中，对话语

言模型是可

以和

MLM 兼

容的

，如图 5.29

所示。

图

5.29 ERNIE-B 的对话语言

模型

5.8.2 ERNIE 2.0

1．ERNIE

2.0 的动机

对于一个句

子乃至一篇

文章，语言模

型是一个最

常见的无监

督学习任务

，但是能够从

中挖掘出

的

无监督学习

任务远远不

止语言模型

一种，而更丰

富的种类意

味着模型能

够学习到更

丰富的知识

。

因此，ERNIE

2.0 从中提

取了 3 个感知

级别，共

8 小类

任务。这么做

也非常好理

解，就像我们

在做

一张语

文试卷时，只

会一种题型

并不能拿高

分，只有会试

卷中的所有

题型时我们

才真正能拿

高分。

在训练

一个能处理

多个任务的

模型时，一个

非常棘手的

问题叫作灾

难性遗忘。灾

难性遗忘产

生

的原因是

当我们顺序

学习一系列

任务时，我们

以一个新任

务的目标去

优化模型，模

型的参数会

根据

新任务

的目标进行

调整，进而导

致模型在之

前老任务上

的效果变差

。ERNIE 2.0 提出解决灾

难性遗

忘的

策略是使用

持续学习。持

续学习的特

点是能够在

学习新任务

的同时保证

模型在老任

务上的准确

率不会变，如

图 5.30 所示。

图

5.30 ERNIE 2.0 的

持续学习和

预训练任务

223

第 5 章

模型预

训练

224

2．预训练

任务

ERNIE

2.0 有 3 个感

知级别的预

训练任务，分

别如下。

z 单词

感知预训练

任务（word-aware pre-training task）：学习单

词级别的知

识。

z 结构感知

预训练任务

（structure-aware pre-training task）：学习句子级

别的知识。

z 语

义感知预训

练任务（semantic-aware pre-training task）：学习

语义级别的

知识。

（1）单词感

知预训练任

务。

z 知识掩码

任务（knowledge masking

task）：这里使

用的是 ERNIE-B 的掩

码方式，如 5.8.1

节

所介绍的。

z 大

写预测任务

（capitalization prediction

task）：在英语中，一

个大写的单

词往往表示

这个词

在句

子中拥有特

殊的含义，例

如该词是一

个地名等。因

此 ERNIE 2.0

加入了一

个预测这个

单词是否需

要大写的任

务。因为引入

了这个任务

，输入模型的

单词都变成

了小写，这也

有

助于模型

的收敛。论文

中并没有介

绍这个任务

是如何处理

中文数据的

，因为对中文

来说并

没有

大小写的概

念。

z

标志 - 文档

关系预测任

务（token-document relation

prediction task）：这个任务

用来预测在

一篇文章的

一个片段中

出现的标志

是否也在这

篇文章的另

外一个片段

中出现了。作

者认为

这种

在一篇文章

中反复出现

的词往往是

这个文章的

关键词，也就

是说该模型

拥有了提取

关

键词这种

复杂任务的

语义理解能

力。

（2）结构感知

预训练任务

。

z 句子重排序

任务（sentence reordering

task）：在这个

任务中，文章

中的一段会

被切分成

n = 1,

2,…, m 个

片段，然后我

们将其随机

打乱，每种片

段数有 n!

个可

能，然后预测

它的排

列顺

序。在这里该

任务被当作

一个 k 分类任

务，其中

。

z 句子

距离任务（sentence distance

task）：预

测两个句子

的距离。这里

包括 3 类：“0”表示

两个句子在

一篇文章中

的距离很近

，“1”表示两个句

子在同一篇

文章中但是

距离比较远

，

“2”表示两个句

子不在同一

篇文章中。

（3）语

义感知预训

练任务。

z 篇章

句子关系任

务（discourse relation

task）：这是一个

无监督任务

，它根据文章

中的一些

关

键词来提取

两个句子之

间的语义或

者修辞关系

。例如“but”切分的

两个句子通

常具有

转折

关系。

z

相关性

计算任务（IR relevance task）：这

个任务的数

据集是根据

百度搜索引

擎得到的，它

用来判断一

条查询和一

篇文章的题

目的相关性

程度。它共有

3 类：“0”表示强相

关，即当

用户

查询后会单

击这篇文章

的题目；“1”表示

弱相关，即会

查询到这篇

文章，但是用

户不

会单击

该文章的题

目；“2”表示不相

关，这里使用

的是一条随

机采样的样

本。

3．持续多任

务学习

如图

5.30

所示，ERNIE 2.0 的持续

多任务学习

分成两个阶

段：

（1）在大量的

数据上进行

无监督预训

练；

（2）使用持续

多任务学习

增进式地更

新 ERNIE 2.0。

为什么这

里要引入持

续多任务学

习呢？试想一

下，当我们训

练一个由诸

多任务组成

的模型时，

我

们有两个问

题需要解决

：首先，如何能

够保证早期

学习的任务

不会被灾难

性遗忘；其次

如何高效

地

训练这些任

务。第一个问

题的解决方

案是在训练

新任务时，加

入老任务和

它一起训练

即可。第二

个

问题的解决

方案是在每

个训练阶段

，我们自动为

每个任务分

配 N

个训练步

。图 5.31 对 ERNIE

2.0

5.8　ERNIE（百度）和

ERNIE 2.0

的持续多任

务学习和多

任务学习以

及其他的持

续学习进行

了对比。

图 5.31 的

左侧是百度

提出的持续

多任务学习

框架，即每次

有新任务进

来时，它就在

原来训练

结

果的基础上

同时训练老

任务和新任

务。但是这样

等到后期任

务比较多的

时候，效率不

高。这就用

到

了我们刚刚

介绍的每个

任务训练 N 轮

的策略。

图 5.31 的

中间是多任

务学习框架

，它的思想是

每次有新任

务过来时，都

重新训练所

有任务，

这个

方法的缺点

是效果过于

差。

图 5.31 的右侧

是常见的持

续学习框架

，它的思想就

是对于每次

输入的数据

，依次对其进

行训

练，这个

方法有非常

严重的灾难

性遗忘的问

题。

图 5.31　（左）ERNIE 2.0

的持

续多任务学

习、（中）多任务

学习、（右）持续

学习

4．ERNIE 2.0 的网络

结构

和 ERNIE-B 相同

，ERNIE 2.0

使用的也是

由 Transformer 构成的网

络结构。但是

为了适应它

的

持续多任

务学习，它对

网络结构做

了几点修改

，如图

5.32 所示。首

先在输入阶

段，ERNIE 2.0 添加

了任

务嵌入（task embedding），对一

个一直可能

有任务进来

的模型来说

，任务嵌入的

作用就是根

据

它们的任

务编号对其

进行编码，然

后和 BERT

中采用

的 3 个嵌入相

加，作为编码

器的输入。

图

5.32

ERNIE 2.0 的网络结构

说到编码器

，由于 ERNIE

2.0 是可以

训练多个任

务的，因此它

需要多种不

同的输入头

。在 ERNIE

2.0 中，它采用

的是编码器

部分网络共

享，在编码器

之后不同任

务使用独立

的网络结构

的方式。

225

第

5 章

模型预训练

5.8.3 小结

从算法

角度来看，百

度的 ERNIE 系列并

没有太大的

创新点，其提

升准确率的

主要原因在

于百

度本身

拥有的强大

的中文数据

资源和工具

。借助这些中

文数据资源

和工具，ERNIE-B

和 ERNIE 2.0

中

提出的几个

无监督任务

还是很有参

考价值的。这

也促使我们

思考如何利

用现有的资

源来构造更

丰

富的任务

。

百度的 ERNIE 系列

和

OpenAI 的 GPT 系列把

词向量训练

任务推向了

两个极端，一

个是挖掘丰

富的任务，另

一个是堆积

超大的模型

和大量的数

据，无论哪个

极端都是需

要巨大的财

力支持的。但

是我们从中

也看出了词

向量任务还

有很高的上

限，何时能推

出一个泛化

能力足以解

决诸多场景

问题

的训练

模型，我们拭

目以待！

226

第三

篇 模型优化

“在实际的深

度学习场景

中，我们几乎

总是

会发现

最好的拟合

模型是一个

适当正则化

的大

型模型

。”

—Ian J.

Goodfellow

随着计算机

硬件性能的

不断提升，现

在神经网络

的参数数量

甚至达到了

以亿计的程

度。它除了

给

模型带来难

以置信的拟

合能力，也使

得模型非常

容易对某个

数据集过拟

合，为了缓解

过拟合的问

题，提升模型

的泛化能力

，我们需要对

模型进行正

则化。在深度

学习中有很

多非常有效

的缓解过拟

合的策略，例

如从数据角

度出发，我们

可以对数据

进行扩充；从

模型角度出

发，我们可以

给权重添

加

正则操作，例

如

L1 正则和 L2 正

则；从训练过

程角度出发

，我们可以对

模型进行早

停等。而在本

章中，我们将

介绍两类常

以网络层的

形式添加到

模型中的结

构：一类是

Dropout，另

一类是归一

化。

Dropout 是当发生

过拟合之后

，第一个被考

虑使用的网

络结构。在训

练时，Dropout 通过将

一

些节点替

换为掩码来

减轻节点之

间的耦合性

，从而实现正

则的效果。对

于 Dropout 的一些改

进，我

们也在

这里进行简

单的介绍，例

如针对

CNN 所做

的一些改进

，包括 Spatial Dropout、DropBlock

和

Max-pooling Dropout，针对

RNN 所做的一系

列改进，包括

只有一组掩

码的

RNNDrop 和 Recurrent

Dropout，也可

以将 Dropout 作用到

LSTM 的各个门上

。

归 一 化（normalization） 是

深

度 学 习 中

发

展 得 比 较

快

的 一 系 列

算

法， 批 归 一

化

（batch 

normalization，BN）是被使用得

最多的归一

化方法，它是

用不同样本

的同一个通

道的特征进

行归一

化的

。BN

是无法用在

序列模型中

的，因为它无

法处理同一

个批次中数

据长度不一

致的场景。在

这

个场景中

，层归一化（layer normalization，LN）是

被使用得最

多的策略，它

是在同一个

样本的不同

通道上进行

归一化的，这

样便避免了

对不同长度

的数据进行

统计量计算

。而在图像生

成这类任务

中

对图像的

细节要求比

较高，这时候

一般使用实

例归一化（instance

normalization，IN），它

是在不同

样

本、不同通道

上做归一化

的。组归一化

（group normalization，GN）是介于 LN

和 IN 之

间的一种

方

案，它将通道

分成若干组

分别进行归

一化的计算

。BN、LN、IN

和 GN 的异同如

图 6.1

所示，从

左

到右依次是

BN、LN、IN 和 GN。

图 6.1 BN、LN、IN 和

GN 的异

同

上面介绍

的 4

种归一化

都是在特征

上进行的，权

值归一化（weight normalization，WN）则

是

在权值矩

阵上进行归

一化的，它更

适用于噪声

敏感的环境

，如生成任务

、强化学习等

。自适配归一

第 6

章  模型优

化方法

第

6 章

模型优化方

法

230

化（switchable normalization，SN）1 是 BN、LN

和 IN 加

权之后的结

果，它能够自

适应地匹配

各种不

同类

型的任务。

6.1　Dropout

在

本节中，先验

知识包括：



LSTM（4.1 节

）。

Dropout 是深度学习

中被广泛应

用于解决模

型过拟合问

题的策略，相

信你对

Dropout 的计

算方

式和工

作原理已了

如指掌。这篇

论文将更深

入地探讨 Dropout

背

后的数学原

理，通过理解

Dropout

的数学原理

，我们可以推

导出几个设

置丢失率的

小技巧，通过

这一节你也

将对 Dropout 的底层

原

理有更深

刻的了解。同

时我们也将

对 Dropout 的几个改

进方法进行

探讨，主要是

讨论它在 CNN

和

RNN 中的应用以

及它的若干

个经典的变

种。

6.1.1 什么是

Dropout

没

有添加 Dropout 的网

络是需要对

网络的全部

节点进行学

习的，而添加

了

Dropout 的网络只

需

要对其中

没有被替换

为掩码的节

点进行训练

，如图 6.2

所示。Dropout 能

够有效解决

模型的过拟

合

问题，从而

使得训练更

深、更宽的网

络成为可能

。

图

6.2　普通的两

层 MLP 和添加

Dropout 之

后的网络结

构

在 Dropout

出现之

前，正则化是

主要的用来

解决模型过

拟合的策略

，如 L1 正则和 L2

正

则。但

是它们

并没有完全

解决模型的

过拟合问题

，原因就是网

络中存在共

适应（co-adaption）问题。

所

谓共适应，是

指网络中的

一些节点会

比另外一些

节点有更强

的表征能力

。这时，随着网

络的

不断训

练，具有更强

表征能力的

节点被不断

强化，而表征

能力更弱的

节点则被不

断弱化，直到

对网

络的贡

献可以忽略

不计。这时候

网络中只有

部分节点被

训练，浪费了

网络的宽度

和深度，进而

导致

模型的

效果提升受

到限制。

1　参见

Ping

Luo、Jiamin Ren、Zhanglin Peng 的论文“Differentiable

learning-to-normalize via switchable normalization”。

6.1　Dropout

231

而

Dropout 解

决了共适应

问题，使得训

练更宽的网

络成为可能

。

6.1.2 Dropout

的数学原理

图 6.3 所示的是

一个单层线

性网络，它的

输出是输入

的加权和，

表

示为式（6.1）。这里

我们只考虑

最简单的线

性激活函数

。Dropout

的推导过程

也适用于非

线性的激活

函数，只是线

性的激活函

数推导

起来

更加简单。

（6.1）

对

于图

6.3 所示的

无 Dropout 的网络，它

的误差

EN 可以

表示为式（6.2），其

中 t 是目标值

。

（6.2）

式（6.2）之所以使

用 w′ 是为了找

到无

Dropout 的网络

和之后要介

绍的添加了

Dropout 的网络

之间

的关系，其中

w′

= pw，p 为概率值。那

么式（6.2）可以表

示为式（6.3）。

（6.3）

它关

于 wi 的偏导数

为式（6.4）。

（6.4）

当我们

向图 6.3 所示的

网络中添加

Dropout 之后，它的误

差

ED 表示为式

（6.5）。

（6.5）

δ

～ Bernoulli(p) 是丢失率，它

服从伯努利

分布，即它有

p 的概率值为

1，有

1 − p 的概率值

为

0。那么式（6.5）关

于 wi 的导数表

示为式（6.6）。

（6.6）

因为

δi 服从伯努利

分布，我们对

其求期望，如

式（6.7）所示。

（6.7）

对比

式（6.6）和式（6.7）我们

可以看出，在

w′

= pw 的前提下，带

有 Dropout

的网络的

梯度的

期望

等价于带有

正则的普通

网络。换句话

说，Dropout 起到了正

则的作用，正

则项为 。

图 6.3　单

层线性网络

第 6

章  模型优

化方法

232

6.1.3 Dropout 是一

个正则网络

通过上面的

分析我们知

道，最小化含

有 Dropout

的网络的

损失等价于

最小化带有

正则项的普

通

网络，如式

（6.8）所示。

（6.8）

也就是

说，当我们对

式（6.8）的

wi 求偏导

数时，会得到

与式（6.4）的带有

Dropout 的网络对

wi

求

偏导数相同

的结果。因此

可以得到使

用 Dropout 的几个技

巧如下。

z

当丢

失率为 0.5 时，Dropout 会

有最强的正

则效果。因为

p(1

− p) 在 p

= 0.5 时取得最

大值。

z

丢失率

的选择策略

：在比较深的

网络中，使用

0.5 的丢失率是

比较好的选

择，因为这时

Dropout 能取到最好

的正则效果

。在比较浅的

网络中，丢失

率应该低于

0.2，因为过大的

丢

失率会导

致丢失过多

的输入数据

，对模型的影

响比较大；不

建议使用大

于

0.5 的丢失率

，

因为它在丢

失过多节点

的情况下并

不会取得更

好的正则效

果。

z

在测试时

需要使用丢

失率对 w 进行

缩放：基于前

面 w′

= pw 的假设，我

们得知无 Dropout

的

网络的权值

相当于对含

有 Dropout 的网络权

值缩放了 p

倍

。在含有 Dropout 的网

络中，

测试时

不会丢弃节

点，这相当于

它是一个普

通网络，因此

也需要进行

p

倍的缩放，如

式

（6.9）所示。

（6.9）

关于

Dropout

的理论依据

，业内还有很

多讨论，除了

上面的正则

和缓解共适

应的作用，还

有以

下比较

有代表性的

讨论。

z 模型集

成：Hinton

等人认为

由于 Dropout 在训练

过程中会随

机丢弃节点

，因此会在训

练过

程中产

生大量不同

的网络，而最

终的网络相

当于这些随

机网络的模

型集成。

z 贝叶

斯理论：在机

器学习中，贝

叶斯理论是

指根据一些

先验条件，在

给定数据集

的情况下

确

定参数的后

验分布。一些

作者认为，使

用 Dropout

训练的方

法可以使用

确定近似值

的贝

叶斯模

型来解释 1

。这

么说可能不

好理解，我们

用更简单的

方式来说明

，一个神经网

络往往

有几

百万个甚至

更多的节点

，在它的基础

上丢弃节点

后产生的子

网络的数量

是无限大的

数

字。如果我

们使用装袋

（bagging）算法训练这

些模型，需要

训练海量的

参数独立的

模型，

然后对

这些模型取

均值。Dropout 不同于

装袋算法的

一点是

Dropout 的参

数是共享的

，而正

是这种

参数共享使

得 Dropout

可以在有

限的时间和

硬件条件下

实现对无限

大量级的模

型的

训练。虽

然很多模型

并没有完整

地参与到训

练流程中，但

是它的子网

络参与了某

个训练步，

正

是这个被训

练的子网络

先验，导致了

剩余的子网

络也可以有

很好的参数

设定。

笔记

如

果要使用 SELU 激

活函数对自

归一化网络

（self-normalization network）2

进行归一

化

，则应使用 Alpha Dropout，因

为

Dropout 会破坏自

归一化。Alpha Dropout 是

Dropout

的

一个变种，它

的特点是保

留了输入的

均值和标准

差。

6.1.4 CNN

的 Dropout

不同于

MLP 的特征图是

一个特征向

量，CNN

的特征图

是一个由宽

、高、通道数组

成的三维矩

1　参见 Sida I.

Wang、Christopher D. Manning 的论文

“Fast

dropout training”。

2　参见

Günter Klambauer、Thomas Unterthiner、Andreas Mayr

等人的

论文“Self-Normalizing Neural Networks”。

6.1

Dropout

233

阵。按照

传统的 Dropout

的理

论，它丢弃的

应该是特征

图上的若干

像素。但是这

个方法在 CNN 中

并不是十分

奏效，一个重

要的原因便

是临近像素

之间的相似

性。因为它们

不仅有非常

接近的输入

值，而且拥有

相近的邻居

、相似的感受

野和相同的

卷积核，所以

直接丢掉一

个像素点而

丢失的信息

可以很容易

地通过其周

围的像素弥

补回来，因此

我们需要针

对 CNN

的特点单

独设计 Dropout。

在 CNN

中

，我们可以以

通道为单位

来随机丢弃

，这样可以增

加其他通道

的建模能力

并缓解通

道

之间的共适

应问题，这个

策略叫作空

间（spatial）Dropout1

。我们也可

以随机丢弃

特征图中的

一大

块区域

，来避免临近

像素的互相

弥补，这个方

法叫作

DropBlock。还有

一个常见的

策略叫作最

大池

化（max pooling）Dropout2

，它的

计算方式是

在执行最大

池化之前，将

窗口内的像

素随机替换

为掩码，

这样

使得窗口内

较小的值也

有机会影响

网络的效果

。空间 Dropout、DropBlock 和最大

池化 Dropout

的可视

化如图 6.4 所示

。

图

6.4　空间 Dropout、DropBlock 和最

大池化

Dropout 的可

视化

6.1.5 RNN

的 Dropout

和 CNN

一

样，传统的 Dropout 并

不能直接用

在 RNN

上，因为每

个时间片的

Dropout 会限

制 RNN

保留

长期记忆的

能力，所以一

些专门针对

RNN 的 Dropout 被提了出

来。针对

RNN 上的

Dropout 的研究主要

集中在 LSTM

上。RNNDrop3 提

出我们可以

在 RNN 的循环开

始之前生成

一组掩

码，这

组掩码作用

到 LSTM 的单元状

态（cell state）上，然后在

时间片的循

环中保持这

组掩码的值

不变，如式（6.10）所

示。

（6.10）

Recurrent Dropout4 则提出也

可以将掩码

作用到更新

单元状态的

地方，同样它

的掩码值也

保持不

变，如

式（6.11）所示。

（6.11）

Yarin Gal

等人

5 提出 Dropout 也可以

作用到

LSTM 的各

个门上，如式

（6.12）所示：

1　参见

Golnaz Ghiasi、Tsung-Yi Lin、Quoc V.

Le 的

论文“DropBlock: A regularization

method for convolutional networks”。

2　参见 Haibing Wu、Xiaodong

Gu 的

论文“Towards dropout training

for convolutional neural networks”。

3　参见 Taesup Moon、Heeyoul

Choi、Hoshik Lee 等

人的论文“Rnndrop: A

novel dropout for RNNS

in ASR”。

4　参

见

Stanislau Semeniuta、Aliaksei Severyn、Erhardt Barth

的论文“Recurrent Dropout without Memory

Loss”。

5　参

见 Yarin

Gal、Zoubin Ghahramani 的论文“A Theoretically

Grounded Application of Dropout

in Recurrent Neural Networks”。

第

6 章  模型优化

方法

234

（6.12）

其中，Zx 和

Zh

是作用到输

入数据和隐

层节点状态

的两个掩码

，它们在整个

时间步内保

持不变；i、f、

g 是 LSTM

的

3 个门。

6.1.6 Dropout

的变体

1．高斯 Dropout

在传统

的 Dropout

中，每个节

点以 1 − p

的概率

被替换为掩

码。反映到式

（6.5）中，它表示为

使用权值乘

δ，δ ～ Bernoulli(p) 服从伯努利

分布。式（6.5）相当

于给每个权

值一个伯努

利的门，如

图

6.5（a）所示。

图 6.5 Dropout

和高

斯 Dropout 的异同

高

斯

Dropout 是指以高

斯分布为每

一个权值生

成一个门，如

图 6.5（b）所示。在使

用高斯

Dropout

时，因

为激活值保

持不变，所以

高斯 Dropout 在测试

时不需要对

权值进行缩

放。在高斯

Dropout

中

，所有节点都

参与训练，这

样对提升训

练速度也有

帮助。在高斯

Dropout 中，每个节点

可以看作乘

p(1 − p)，这相当于增

熵，而

Dropout 丢弃节

点的策略相

当于减熵。在

Srivastava 等人的

论文

1

中，他们指出

增熵是比减

熵更好的策

略，因此高斯

Dropout 会有更好的

效果。

2．DropConnect

DropConnect

的思想

也很简单，它

不是随机地

将隐层节点

的输出置 0，而

是将节点中

每个与其相

连

的输入权

值以一定的

概率置 0，Dropout

和 DropConnect 一

个是输出，一

个是输入，表

示为式（6.13）：

（6.13）

其中

，M 是二值掩码

矩阵，它里面

的每一个元

素服从伯努

利分布，W 是权

值矩阵，v 是特

征向量，

a 是激

活函数。Dropout 可以

看作对计算

结果进行掩

码，而 DropConnect

可以看

作对输入权

值进行

掩码

，如图 6.6 所示。

在

测试时，使用

所有可能的

掩码的结果

求均值的策

略是不现实

的，因为所有

掩码的情况

共有

2|M| 2 种，因此

DropConnect

通常采用高

斯采样的方

式来拟合全

部枚举的结

果。高斯分布

的均值方差

1　参见 Nitish Srivastava、Geoffrey

Hinton、Alex Krizhevsky 等人的

论文“Dropout: A

Simple Way to Prevent

Neural Networks 

from

Overfitting”。

2　其中 |M|

表

示 M 中元素的

个数

6.1

Dropout

235

和概率

p 有关：均值

EM(u) = pWv，方

差 。采样的结

果经过激活

函数

a 之后再

进行求均值

的操作。DropConnect 的测

试过程表示

为算法 1。

图 6.6 Dropout 和

DropConnect

的异同

算法

1 DropConnect 的测试过程

输入：样本x、参

数θ、采样的数

量Z

1: 提取特征

：v←g(x; Wg )

2: 归一化统计

量计算：μ←EM(u)σ2

←VM(u)

3: for all z

= 1∶Z do//遍历

Z个样本

4: for all i

= 1∶d do

5: 从1维

高斯分布中

采样

 6:

ri,z←a(ui,z)

 7: end

for

 8: end

for

 9: 将结果

传递到下一

层

10: return预测结果

u

3．StandOut

在

Dropout 中，每个节

点以相同概

率 p 的伯努利

分布被丢弃

，StantOut

提出丢弃的

概率 p 应该

是

自适应的，它

的值取决于

权值，一般权

值越大，节点

被丢弃的概

率越高。这样

低权值的节

点也可

以逐

渐学习到有

用的特征，从

而缓解共适

应的问题。在

训练时，StandOut 的节

点被丢弃的

概率表示

为

式（6.14）：

mi

～ Bernoulli[g(Ws x)] （6.14）

其中，Ws 是网

络权值，g 是激

活函数，也可

以是一个网

络，如深度置

信网络。实验

结果表明深

度置

信网络

可以近似为

权重的仿射

函数，例如我

们可以采用

sigmoid

激活函数。在

测试的时候

，我们也

需要

对权值进行

缩放。

如图 6.7

所

示，在 StandOut 中，一个

节点被替换

为掩码的概

率取决于它

的权值，权值

越高它被

替

换为掩码的

概率越高，这

样就避免了

网络过分依

赖少数节点

的情况。

4．蒙特

卡洛 Dropout

蒙特卡

洛方法本质

上通过有限

次的采样来

拟合一个测

试结果。这里

要介绍的蒙

特卡洛 Dropout

第 6 章

模型优化方

法

（MCDropout）1 可以应用

到任何使用

Dropout 训练的网络

中，在训练时

MCDropout 和原始的

Dropout

保

持相同，但是

在测试时它

继续保留 Dropout 的

丢弃操作，通

过随机采样

大量不同的

测试结果来

产

生真实的

结果，得到预

测结果的均

值和方差。因

为 MCDropout 的多次预

测是可以并

行执行的，所

以并不会耗

费太长的时

间。

图

6.7 StandOut 示意

论

文中

MCDropout 的理论

证明非常复

杂，这里我们

大致阐述一

下它的思想

。MCDropout 的提出主要

是因为

作者

认为

softmax 的值并

不能反映样

本分类的可

靠程度。

根据

我们对 softmax

输出

向量的观察

，经过 Softmax 计算

后

的最大值往

往是一个非

常接近

0.99 的值

，这个值作为

模型的置信

度是非常不

可靠的。MCDropout 通过

在不同

的模

型上的采样

来对同一个

数据进行预

测，那么根据

多

次采样的

结果便可以

得到一个比

softmax 得到的更可

靠的

置信度

。恰好 Dropout

是一个

天然的不同

模型的生成

器，

所以在测

试的时候要

保留 Dropout，如图 6.8

所

示。

图 6.8　蒙特卡

洛

Dropout（有动图）

6.1.7 小

结

在这篇论

文中我们对

Dropout

背后的数学

原理进行了

深入的分析

，从而理解了

“Dropout 是一

个正则

方法”的背后

原理，通过对

Dropout 的数学分析

，我们还得出

了使用

Dropout 的几

个小技巧：

z 丢

失率设置为

0.5

时会取得最

好的正则化

效果；

z 不建议

使用大于 0.5

的

丢失率。

在 CNN 和

RNN

中，由于数据

的特征和 MLP 不

同，因此需要

针对性地设

计 Dropout，我们在

这

里讨论了 CNN 中

的空间 Dropout、DropBlock

等。而

RNN 的 Dropout 需要在每

个时间片中

保持

相同的

掩码以保证

模型捕捉长

期依赖的能

力，它们的不

同点是作用

到 LSTM 内部的不

同门还是直

接作用到输

出结果。

1

参见

Yarin Gal 和 Zoubin

Ghahramani 的论文“Dropout as a

Bayesian Approximation: Representing Model

Uncertainty in 

Deep

Learning”。

236

6.2　BN

237

6.2　BN

BN1

是

在深度学习

中缓解过拟

合时选择的

诸多技巧中

使用频率非

常高的一个

经典算法，它

不仅

能够有

效地解决梯

度爆炸的问

题，而且加入

BN 层的网络往

往更加稳定

且 BN

还起到了

一定的正则

化的作用。在

这篇论文中

，我们将详细

介绍 BN 的技术

细节及其背

后的原理。

在

提出

BN 的论文

中，作者认为

BN 有用的原因

是 BN

解决了普

通网络的内

部协变量偏

移

（internel covariate shift，ICS）的问题。所

谓

ICS 是指网络

各层的分布

不一致，网络

需要适应这

种不

一致从

而增加了学

习的难度。而

在 Santurkar

等人的论

文 2 中，他们通

过实验验证

了 BN

其实和 ICS

的

关系并不大

，其有用的原

因是使损失

平面更加平

滑，并给出了

其结论的数

学证明。那么

，孰对孰

错呢

？我们详细说

明。

6.2.1 BN 详解

1．内部

协变量偏移

BN

是基于小批

次 SGD 的，小批次

SGD 是介于单样

本

SGD 和全样本

SGD 的折中方案

，其优

点是比

全样本

SGD 有更

小的硬件需

求，比单样本

SGD 有更快的收

敛速度和更

好的并行能

力。SGD

的缺点是

对参数比较

敏感，较大的

学习率和不

合适的初始

化值均有可

能导致训练

过程中发生

梯度消

失或

者梯度爆炸

，BN 则有效地解

决了这个问

题。

在 Sergey

Ioffe 等人的

论文 1 中，他们

认为

BN 的主要

贡献是缓解

了 ICS 的问题，论

文中对

ICS

的定

义是：the change in

the distribution of network

activations due to the

change in network parameters

during training（在训练

过程中由网

络参数变化

导致网络激

活的分布产

生的变化）。作

者认为 ICS 是

导

致网络收敛

慢的罪魁祸

首，因为模型

需要学习在

训练过程中

会不断变化

的隐层特征

的分布。作者

提出 BN 的动机

是试图在训

练过程中将

每一层的隐

层节点的特

征的分布固

定下来，这样

就可以避免

ICS 的问题了。

在

深度学习训

练中，白化（whiten）是

加速收敛的

一个小技巧

，所谓白化是

指使图像像

素点

变化服

从均值为 0、方

差为 1

的正态

分布。我们知

道在深度学

习中，第 i 层的

输出会直接

作为第

i

+ 1 层的

输入，那么我

们能不能对

神经网络每

一层的输入

都做一次白

化呢？其实 BN

就

是这么

做的

。

2．梯度饱和

我

们知道

sigmoid 激活

函数和 tanh 激活

函数存在梯

度饱和的区

域，其原因是

激活函数的

输入值

过大

或者过小，导

致得到的激

活函数的梯

度值非常接

近于 0，使得网

络的收敛速

度减慢。传统

的方

法是使

用不存在梯

度饱和区域

的激活函数

，如 ReLU

等。BN 也可以

解决梯度饱

和的问题，它

的策

略是在

调用激活函

数之前将 Wx

+ b 的

值归一化到

梯度值比较

大的区域。假

设激活函数

为 g，BN

应

在激活

函数之前使

用，如式（6.15）所示

。

z =

g[BN(Wx + b)] （6.15）

3．BN 的训练过程

如果按照传

统白化的方

法，整个网络

的每个隐层

节点都会使

用整个训练

集进行归一

化统计量的

1　参见 Sergey

Ioffe、Christian Szegedy 的论文

“Batch Normalization:

Accelerating Deep Network Training

by Reducing Internal

Covariate Shift”。

2　参见

Shibani Santurkar、Dimitris Tsipras、Andrew Ilyas

等人的

论文“How Does Batch Normalization

Help Optimization?”。

第 6

章  模

型优化方法

238

计算，但是这

种过程无疑

是非常耗时

的。BN的训练过

程是以批次

为单位的。神

经网络的BN如

图6.9

所示，假设

一个批次有

m 个样本 B =

{x1,x2,…,xm}，每个

样本有 d 个特

征，那么这个

批次的每个

样本

第

k 个特

征的归一化

后的值如式

（6.16）所示：

（6.16）

其中，E(x

(k)

) 和

Var(x

(k)

) 分别表示在

第 k 个特征上

这个批次中

所有样本的

均值和方差

。

图 6.9　神经网络

的 BN

这种表示

会对模型的

收敛有帮助

，但是也可能

破坏已经学

习到的特征

。为了解决这

个问题，

BN 添加

了两个可以

学习的变量

β 和

γ，用于控制

网络表达直

接映射，也就

是还原 BN 之前

学习到

的特

征，如式（6.17）所示

。

（6.17）

当 并且 时，y

(k)

 = x

(k)

，也

就是说经过

BN 操作的网络

容量是不小

于没

有经过

BN

操作的网络

容量的。

综上

所述，BN 可以看

作一个以 γ

和

β 为参数的、从

x1,…,m 到 y1,…,m

的映射，表

示为式（6.18）。

通过

这个方式，BN 将

每一个隐层

节点的分布

都调整为正

态分布，从而

缓解了 ICS

的问

题。

（6.18）

BN 的伪代码

如算法

2 所示

。

算法 2

BN

输入：一

个小批次B = {x1,…,m}的

值x

BN的参数γ、β

 1: //小

批次均值

2: //小

批次方差

 3:

//归

一化

 4: //缩放和

移位

5: return ：

在训练

时，我们需要

计算

BN 的反向

传播过程，感

兴趣的读者

可以自行推

导，这里直接

给出结

论（L 表

示损失函数

），如式（6.19）所示。

6.2　BN

239

（6.19）

通

过式（6.19）我们可

以看出 BN 是处

处可导的，因

此可以直接

以层的形式

加入神经网

络中。

4．BN

的测试

过程

在训练

的时候，我们

采用 SGD 算法可

以获得该批

次中样本的

均值和方差

。但是在测试

的时候，

数据

都是以单个

样本的形式

输入网络的

。在计算 BN 层的

输出的时候

，我们需要获

取的均值和

方差

是通过

训练集统计

得到的。具体

地讲，我们会

从训练集中

随机取多个

批次的数据

集，每个批次

的样

本数是

m，测试的时候

使用的均值

和方差是这

些批次的均

值，如式（6.20）所示

。

（6.20）

上面的过程

需要在训练

完再进行一

次训练集的

采样，非常耗

时。更多的开

源框架是在

训练

的时候

，顺便就把采

集到的样本

的均值和方

差保留了下

来。在

Keras 中，这个

变量叫作滑

动平

均（moving average），对应

的均值叫作

滑动均值（moving

mean），方

差叫作滑动

方差（moving 

variance）。它们均

使用 moving_average_update()

进行更

新。在测试的

时候则使用

滑动均值

和

滑动方差代

替上面的 E(x) 和

Var(x)，这样我们得

到的统计量

也更具有全

局特征。滑动

均值和滑动

方差的更新

如式（6.21）所示：

（6.21）

其

中，Emoving(x) 表 示

滑 动

均 值，Esample(x) 表

示 采

样 均 值，

方 差

定 义 类

似。m 表

示遗忘因子

（momentum），默认值是 0.99。滑

动均值和滑

动方差，以及

可学习参数

β、γ 均是对输入

特征的线性

操作，因此可

以将这两个

操作合并起

来，如式（6.22）所示

。

（6.22）

5．CNN 中的 BN

BN 除了可

以应用在 MLP 上

，其在

CNN 中的表

现也非常好

，但是其在 RNN 上

的表现并不

好，

具体原因

后面解释，这

里主要介绍

BN 在 CNN 中的使用

方法。CNN

和 MLP 的不

同点是 CNN

中每

个样本的隐

层节点的输

出是三维（宽

度、高度、通道

数）的，而 MLP 的是

一维的。当我

们在 CNN

中使用

BN 时，归一化统

计量的计算

是以通道为

单位的，如图

6.10 所示。

第

6 章  模

型优化方法

图

6.10 CNN 的 BN

示意

在

图 6.10 中，假设一

个批次有

m 个

样本，特征图

的尺寸是 p ×

q，通

道数是 d。在 CNN 中

，

BN 的操作是以

特征图为单

位的，因此一

个 BN 要统计的

数据的个数

为

m × p ×

q，每个特征

图使用

一组

γ 和 β。

6.2.2 BN 的背后原

理

1．BN

与 ICS 无关

在

2018

年，MIT 的一篇论

文（即 Santurkar 等人的

论文）否定了

BN

背后的原理

是其缓解了

ICS 问题。在这篇

论文中，作者

通过两个实

验验证了 ICS 和

BN

的关系非常

小的观点。

第

一个实验验

证了 ICS 和网络

性能的关系

并不大，在这

个实验中作

者向使用了

BN

的网络中加

入

了随机噪

声，目的是使

这个网络的

ICS 问题更加严

重。实验结果

表明，虽然加

入了随机噪

声的使用

BN

的

网络的 ICS 问题

更加严重，但

是它的性能

是要优于没

有使用 BN

的普

通网络的，如

图 6.11 所示。

图

6.11　普

通网络的 ICS、使

用 BN

的网络的

ICS、加入噪声的

使用 BN 的网络

的 ICS

实验数据

第二个实验

验证了 BN 并不

会减小 ICS，有时

候甚至还能

增大

ICS。在这个

实验中，作者

对

ICS 的定义如

下。

240

6.2　BN

241

定义

6.1　内部

协变量偏移

（ICS）

假设 L

是损失

值， 是在 k 个层

中在时刻

t 的

参数值，(x

(t)

,

y

(t)

) 是在

t

时刻的输入

特

征和标签

值，ICS 定义为在

时刻 t，第

i 个隐

层节点的两

个变量的距

离 ，如式（6.23）

所示

。

（6.23）

两个变量的

区别在于 W1,…,Wi−1 是

t

时刻的还是

t + 1 时刻的，其中

Gt,i

表示更新梯

度时使用的

参数，G′

t,i 表示使

用这批样本

更新后的参

数。在上面提

到的欧氏距

离中，值越接

近 0

说明 ICS 越小

。

另外一个相

似度指标是

余弦夹角，值

越接近于

1 说

明 ICS 越小。图

6.12 的

实验结果（25 层

的深层

神经

网络）表明

BN 和

ICS 的关系并不

是很大。

图

6.12　普

通网络和带

BN 的网络在两

个 ICS

指标上的

实验结果

2．BN 与

损失平面

通

过上面两个

实验，作者认

为

BN 和 ICS 的关系

不大，那么

BN 为

什么效果好

呢？作者认为

BN

的作用是平

滑了损失平

面，关于损失

平面的介绍

，参考Hao Li等人的

论文1

，这篇论

文介绍了损

失平

面的概

念，并指出残

差网络和 DenseNet 均

起到了平滑

损失平面的

作用，因此它

们具有较快

的收敛

速度

。

作者证明了

BN 处理之后的

损失函数满

足利普希茨

（Lipschitz）连续，即损失

函数的梯度

小于

一个常

量，因此网络

的损失平面

不会振荡得

过于剧烈，如

式（6.24）所示。

||f(x1) − f(x2)|| ≤

L||x1 − x2|| （6.24）

而且

损失函数的

梯度也满足

利普希茨连

续，这里叫作

β- 平滑，即斜率

的斜率也不

会超过一个

常量，如式（6.25）所

示。

（6.25）

作者认为

当两个常量

的值均比较

小的时候，损

失平面就可

以看作平滑

的。图

6.13 所示的

是没

有加入

跳跃连接的

网络和加入

跳跃连接的

网络（残差网

络）的损失平

面的可视化

，作者认为 BN

和

残差网络对

损失平面平

滑的效果类

似。

通过上面

的分析，我们

知道 BN 收敛快

的原因是

BN 产

生了更光滑

的损失平面

。其实类似于

BN 的能平滑损

失平面的策

略均能起到

加速收敛的

效果，作者在

论文中尝试

了 L1

正则或 L2 正

则的

策略（即

通过取特征

的

L1 正则或 L2 正

则的均值的

方式进行归

一化）。实验结

果表明

lp-norm 均取

得了和 BN 类似

的效果。

1　参见

Hao Li、Zheng Xu、Gavin

Taylor 等人的论文

“Visualizing the Loss

Landscape of Neural Nets”。

第 6 章

模型优

化方法

242

图 6.13

损

失平面的可

视化

3．BN 的数学

定理

作者对

于自己的猜

想给出了

4 个

定理，并在附

录中给出了

证明，这一部

分的数学证

明过于烦

琐

，我们只列出

了 4

个重要的

定理，有需要

的读者自行

查看证明过

程。

定理 6.1 BN

使神

经网络满足

利普希茨连

续

设 为使用

BN 的网络的损

失函数，L

为普

通网络的损

失函数，它们

满足式（6.26）。

（6.26）

在绝

大多数场景

中，σ 作为不可

控的项往往

值是要大于

γ

的，因此证明

了 BN 可以使神

经网络

满足

利普希茨连

续。

定理 6.2 BN 使损

失函数的梯

度满足利普

希茨连续

假

设 ， 是黑塞（Hessian）矩

阵，如式（6.27）所示

。

（6.27）

同理，定理 6.2 证

明了 BN

使神经

网络的损失

函数的梯度

也满足利普

希茨连续。

由

于 BN 可以还原

为直接映射

，因此普通神

经网络的最

优损失平面

也一定存在

于带

BN 的网

络

中。

定理

6.3 BN 可以

降低损失函

数梯度的上

界

设

带 BN 的网

络的损失为

， 与之等价的

无

BN 的网络的

损失为 L， 它们

满足如果

， ，我

们可以推出

式（6.28）。

（6.28）

6.3

LN

243

定理 6.4

BN 对参

数的初始化

更加不敏感

假设 W* 和

分别

是普通神经

网络和带 BN 的

神经网络的

局部最优解

的权值，对于

任意的初

始

化

W0，我们有式

（6.29）。

（6.29）

6.2.3 小结

BN 是深度

学习调参中

非常好用的

策略之一（另

外一个是 Dropout），当

你的模型发

生梯度消

失

/

爆炸或者损

失值振荡比

较剧烈的时

候，在网络中

加入 BN 往往能

取得非常好

的效果。BN 也有

一些不是非

常适用的场

景，在遇见这

些场景时要

谨慎地使用

BN

：

z 受制于硬件

条件，每个批

次的尺寸比

较小，这时候

谨慎使用 BN

；

z 在

类似于 RNN

的动

态网络中谨

慎使用 BN ；

z

训练

数据集和测

试数据集方

差较大的时

候谨慎使用

BN。

在 Ioffe 等人的论

文中，他们认

为

BN 有用的原

因是缓解了

ICS 的问题，而在

Santurkar 等人的

论文

中则对其进

行了否定。他

们结论的得

出非常依赖

他们自己给

出的 ICS 的数学

定义，这个定

义不

能说不

对，但是似乎

不够精确，BN

真

的和 ICS 没有一

点关系吗？我

觉得不一定

。

6.3

LN

在本节中，先

验知识包括

：

 BN（6.2

节）。

在 6.2 节的最

后，我们指出

BN

并不适用于

RNN 等动态网络

和批次的尺

寸较小的场

景。LN1 有效地

解

决了

BN 的这两

个问题。LN 和 BN

的

不同点是其

归一化的维

度是互相垂

直的，如图 6.14 所

示。在图

6.14

中 N 表

示样本轴，C 表

示通道轴，F

是

每个通道的

特征数。BN 如图

6.14（b）所示，它取不

同样本

的同

一个通道的

特征进行归

一化；LN 则如图

6.14（a）所示，它取同

一个样本的

不同通道进

行归一化。

图

6.14 LN 和 BN

对比示意

1　参见 Jimmy Lei

Ba、Jamie Ryan Kiros、Geoffrey E.

Hinton 的论文

“Layer Normalization”。

第

6 章  模型优

化方法

244

6.3.1 BN 的问

题

1．BN 与批次大

小

如图 6.14（b）所示

，BN

是按照样本

数计算归一

化统计量的

。

当样本数很

少时，例如只

有 4 个，这

4 个样

本的均值和

方差便不能

反映全局的

统计分布信

息，所以基于

少量样本的

BN 的效果会变

得很

差。在一

些场景中，例

如硬件资源

受限、在线学

习等场景，BN

是

非

常不适用

的。

2．BN 与

RNN

RNN 可以展

开成一个隐

层共享参数

的 MLP，随着时间

片的增

多，展

开后的 MLP 的层

数也在增多

，最终层数由

输入数据的

时间片

的数

量决定，所以

RNN

是一个动态

的网络。在一

个批次中，通

常各

个样本

的长度都是

不同的，当统

计到比较靠

后的时间片

时，例如图

6.15 中

z>4

时，只有一个

样本还有数

据，基于这个

样本的统计

信息

不能反

映全局分布

，所以这时 BN 的

效果并不好

。另外如果在

测试时

我们

遇到了长度

大于任何一

个训练样本

的测试样本

，我们无法找

到

保存的归

一化统计量

，所以 BN 无法运

行。

6.3.2 LN 详解

1．MLP

中的

LN

通过上面的

分析我们知

道 BN 的两个缺

点的产生原

因均是计算

归一化统计

量时计算的

样本数太

少

。LN 是一个独立

于批次的算

法，无论样本

数多少都不

会影响参与

LN 计算的数据

量，所以它解

决 BN

的两个问

题。LN 的做法如

图 6.14（a）所示：根据

样本的特征

数进行归一

化。

先看

MLP 中的

LN。设 H 是一层中

隐层节点的

数量，l

是 MLP 的层

数，我们可以

计算 LN

的

归一

化统计量 μ 和

σ，如式（6.30）所示。

（6.30）

注

意，上面统计

量的计算和

样本数量是

没有关系的

，它的数量只

取决于隐层

节点的数量

，所以

只要隐

层节点的数

量足够多，我

们就能保证

LN 的归一化统

计量足够具

有代表性。通

过

μl

和 σl

可

以得

到归一化后

的值 ，如式（6.31）所

示：

（6.31）

其中，ϵ（论文

中忽略了这

个参数）是一

个很小的小

数，用于防止

除数为

0。在 LN 中

我们也需要

一组参数来

保证归一化

操作不会破

坏之前的信

息，在 LN

中这组

参数叫作增

益 g 和偏置 b（等

同于

BN 中的 γ 和

β）。假设激活函

数为

f，最终 LN 的

输出如式（6.32）所

示。

（6.32）

合并式（6.31）、式

（6.32）并忽略参数

l，得到式（6.33）。

图 6.15 RNN

中

使用 BN 会

导致

批次的尺寸

过小的问题

6.3

LN

245

（6.33）

2．RNN

中的 LN

在 RNN

中，我

们可以非常

简单地在每

个时间片中

使用 LN，而且在

任何时间片

中我们都能

保

证归一化

统计量统计

的是 H

个节点

的信息。对于

RNN 中时刻 t 的节

点，其输入是

t

− 1 时刻的隐层

状态 ht−1

和 t 时刻

的输入数据

xt

，可以表示为

式（6.34）。

at

 = Whhht−1

+ Wxhxt （6.34）

接着我们

便可以在

at

上

采取和 MLP 中的

LN

完全相同的

归一化过程

，如式（6.35）所示。

（6.35）

3．LN 与

ICS

和损失平面

平滑

LN 能缓解

ICS 问题吗？当然

可以，至少

LN 将

每个训练样

本都归一化

到了相同的

分布上。而

在

BN 的论文中介

绍过几乎所

有的归一化

方法都能起

到平滑损失

平面的作用

。所以从原理

上讲，

LN 能加速

收敛速度。

6.3.3 对

照实验

这里

我们设置了

一组对照实

验来对比普

通网络、BN 和 LN 在

MLP

和 RNN 上的表现

。这里使

用的

框架是

Keras。

1．MLP 上的

归一化

这里

使用的是

MNIST 数

据集，但是归

一化操作只

添加到了后

面的 MLP 部分。LN

的

使用方法

见

如下代码：

# pip

install keras-layer-normalization

from keras_layer_normalization

import LayerNormalization

# 构

建CNN中的LN

model_ln = Sequential()

model_ln.add(Conv2D(input_shape

= (28,28,1), filters=6, kernel_size=(5,5),

padding='valid', activation='tanh'))

model_ln.add(MaxPool2D(pool_size=(2,2), strides=2))

model_ln.add(Conv2D(input_shape=(14,14,6), filters=16, kernel_size=(5,5),

padding='valid', activation='tanh'))

model_ln.add(MaxPool2D(pool_size=(2,2), strides=2))

model_ln.add(Flatten())

model_ln.add(Dense(120, activation='tanh'))

model_ln.add(LayerNormalization())

# 添加

LN运算

model_ln.add(Dense(84, activation='tanh'))

model_ln.add(LayerNormalization())

model_ln.add(Dense(10, activation='softmax'))

另外两

个对照实验

也使用了这

个网络结构

，不同点在于

归一化部分

。图

6.16（a）是批次大

小

第 6 章

模型

优化方法

为

128 时得到的收

敛曲线，从中

我们可以看

出 BN

和 LN 均能取

得加速收敛

的效果，且 BN

的

效果

要优于

LN。图 6.16（b）是批次大

小为 8

时得到

的收敛曲线

，这时 BN 反而会

减慢收敛速

度，验证

了我

们上面的结

论，对比之下

LN

要轻微地优

于无归一化

的网络，说明

了 LN 在小尺寸

批次上的有

效性。得到图

6.16 运行结果的

完整代码见

随书资料。

图

6.16　批次大小为

128 时和批次大

小为 8

时损失

收敛曲线示

意

2．LSTM 上的归一

化

另外一组

对照实验是

基于

imdb 的二分

类任务，使用

了 GloVe 作为词嵌

入。这里设置

了无

LN

的 LSTM 和带

LN

的 LSTM 作为实验

对照。带 LN

的 LSTM 源

码参考随书

资料，构建其

网络结构

的

代码如下：

from lstm_ln import LSTM_LN

model_ln = Sequential()

model_ln.add(Embedding(max_features,100))

model_ln.add(LSTM_LN(128))

model_ln.add(Dense(1, activation='sigmoid'))

model_ln.summary()

从

图 6.17 所示的实

验结果中我

们可以看出

LN 对于

RNN 系列动

态网络的收

敛速度的提

升是略

有帮

助的。LN 的优点

主要体现在

如下两个方

面。

图 6.17　训练集

损失值和验

证集准确率

示意

246

6.4　WN

247

z

LN 得到的

模型更稳定

。

z LN

有正则化的

作用，得到的

模型更不容

易过拟合。

至

于论文中所

说的加速收

敛的效果，我

们从实验结

果上看不到

明显的加速

效果。

3．CNN 上的归

一化

我们也

尝试了将 LN 添

加到 CNN

之后，实

验结果发现

LN 破坏了卷积

学习到的特

征，模型无

法

收敛，所以在

CNN 之后使用

BN 是

一个更好的

选择。

6.3.4 小结

LN 是

和 BN 非常近似

的一种归一

化方法，不同

的是

BN 取的是

不同样本的

同一个特征

，而 LN

取的是同

一个样本的

不同特征。在

BN

和 LN 都能使用

的场景中，BN 的

效果一般优

于

LN，原因是

基

于不同样本

、同一特征得

到的归一化

特征更不容

易损失信息

。

但是有些场

景是不能使

用 BN

的，例如批

次较小或者

在 RNN 中，这时候

可以选择使

用 LN，使

用 LN 得到

的模型更稳

定且能起到

正则化的作

用。RNN 能应用到

小批次和

RNN 中

是因为 LN 的归

一化统计量

的计算和批

次是没有关

系的。

6.4　WN

在本节

中，先验知识

包括：



BN（6.2 节）。

之前

介绍的 BN

和 LN 都

在数据的层

面上进行归

一化，而这篇

论文介绍的

WN1 在权值的维

度上

进行归

一化。WN 的做法

是将权值向

量 w 在其欧氏

范数和其方

向上解耦成

了参数向量

v

和参数标量

g，然后使用 SGD 分

别优化这两

个参数。

WN

也是

和样本量无

关的，所以可

以应用在批

次较小的网

络以及 RNN 等动

态网络中；另

外

BN

使用基于

小批次的归

一化统计量

代替全局统

计量，相当于

在梯度计算

中引入了噪

声。而 WN 没

有这

个问题，所以

在生成模型

、强化学习等

噪声敏感的

环境中

WN 的效

果也要优于

BN。WN 没有

额外参

数，这样更节

约显存。同时

，WN

的计算效率

优于要计算

归一化统计

量的 BN。

6.4.1 WN

的计算

神经网络的

一个节点计

算可以表示

为式（6.36）：

（6.36）

其中，w 是

一个

k 维的特

征向量，y 是该

神经节点的

输出，所以是

一个标量。在

得到损失值

后，我

们会根

据损失函数

的值使用

SGD 等

优化策略更

新 w 和

b。WN 提出的

归一化策略

是将 w 分解为

一

个参数向

量 v 和一个参

数标量 g，分解

方法如式（6.37）所

示，如图

6.18 所示

。

（6.37）

1

参见 Tim Salimans、Diederik P.

Kingma 的论文

“Weight Normalization: A

Simple Reparameterization to Accelerate

Training 

of Deep

Neural Networks”。

第 6

章  模型优

化方法

248

式（6.37）中

||v|| 表示 v 的欧氏

范数。当

v = w 且

g = ||w|| 时

，

WN 的网络还原

为普通的神

经网络，所以

WN 网络的容量

是要

大于普

通神经网络

的。

当我们将

g 固定为 ||w|| 时，我

们只优化

v，这

时候相当于

只

优化 w 的方

向而保留其

范数。当

v 固定

为 w 时，这时候

相当于

只优

化 w 的范数，而

保留其方向

，这样为我们

优化权值提

供了

更多可

以选择的空

间，且解耦方

向与范数的

策略也能加

速

WN

网络收敛

。在优化 g 时，我

们一般通过

优化

g 的 log 级参

数

s 来

完成，即

g =

es

。v和g的更新值

可以通过SGD计

算得到式（6.38）：

（6.38）

其

中，L

为损失函

数， 为 w 在

L 下的

梯度值，从式

（6.37）中我们可以

看出 WN 并没有

引入新

的参

数。

6.4.2 WN 的原理

6.4.1 节

的式（6.38）也可以

写作式（6.39）：

（6.39）

推导

方式如式（6.40）所

示：

（6.40）

倒数第二

步的推导是

因为 v 是

w 的方

向向量。

式（6.40）反

映了 WN

两个重

要特征：

z 表明

WN 会对权值梯

度进行

的缩

放；

z 表明 WN

会将

梯度投影到

一个远离 的

方向。

这两个

特征都会加

速模型的收

敛。对于具体

原因，论文的

说法比较复

杂，其核心思

想有两点：

z

由

于 w 垂直于 Mw，因

此

非常接近

垂直参数方

向 w，这样对于

矫正梯度更

新方向是非

常有效的；

图

6.18　权值向量的

分解可视化

6.4

WN

249

z v

和梯度更新

值中的噪声

量成正比，而

v 和更新量成

反比，所以当

更新值中噪

声较多时，

更

新值会变小

，这说明 WN

有自

稳定（self-stablize）的作用

。这个特点使

得我们可以

在

WN 中使用比

较大的学习

率。

另一个角

度是从新权

值的协方差

矩阵出发的

，假设

w 的协方

差矩阵是 C，那

么 v

的协方差

矩

阵 ，当去掉

D 中的特征值

后我们发现

新的

D 非常趋

近于单位矩

阵，这说明了

w

是 C

的主特征

向量（dominant eigen vector），说明 WN

有

助于提升收

敛速度。

6.4.3 BN 和

WN 的

关系

假设 t

= vx，μ[t] 和

σ[t] 分别为

t 的均

值和方差，BN 可

以表示为式

（6.41）。

（6.41）

当网络只有

一层且输入

样本服从均

值为 0、方差为

1 的独立分布

时，我们有 μ[t]

= 0 且

σ[t] =

||v||，此时 WN 和 BN

等价

。

6.4.4 WN 的参数初始

化

由于 WN 不像

BN 有规范化特

征尺度的作

用，因此

WN 的初

始化需要慎

重。作者建议

的初始

化策

略是：

z

v 使用均

值为 0、标准差

为 0.05

的正态分

布进行初始

化；

z g 和偏置

b 使

用第一批训

练样本的统

计量进行初

始化，如式（6.42）所

示。

（6.42）

由于使用

了样本进行

初始化，因此

这种初始化

方法不适用

于

RNN 等动态网

络。

6.4.5 均值

BN

基于

WN 的动机，论文

提出了均值

BN，其是一个只

进行减均值

而不进行除

方差的 BN，动机

是

BN 的除方差

操作会引入

额外的噪声

，实验结果表

明 WN +

均值 BN 虽然

比标准 BN

收敛

得慢，

但它们

在测试集上

的精度要高

于 BN。

6.4.6

小结

和目

前主流归一

化方法不同

的是，WN 的归一

化操作作用

在了权值矩

阵上。从其计

算方法上来

看，WN 完全不像

是归一化方

法，更像是基

于矩阵分解

的优化策略

，它带来了

4 个

好处：

z 更快的

收敛速度；

z 更

强的学习率

健壮性；

z 可以

应用在

RNN 等动

态网络中；

z 对

噪声更不敏

感，更适用于

GAN、强化学习等

场景中。

说 WN 不

像是归一化

方法的原因

是它并没有

对得到的特

征范围进行

约束的功能

，所以 WN

依

旧对

参数的初始

值非常敏感

，这也是 WN 的一

个比较严重

的问题。

第 6 章

模型优化方

法

250

6.5　IN

在本节中

，先验知识包

括：

 BN（6.2 节）； 

LN（6.3 节）。

对图

像风格迁移

、图片生成等

注重每个像

素细节的任

务来说，每个

样本的每个

像素点的信

息都是非

常

重要的，于是

像

BN 这种对每

个批次的所

有样本都做

归一化的算

法就不太适

用了，因为 BN 计

算归一化

统

计量时考虑

了一个批次

中所有图片

的内容，从而

造成了每个

像素独特细

节的丢失。同

理，LN 这类需要

考虑一个样

本所有通道

的算法可能

忽略了不同

通道的差异

，也不太适用

于图像风格

迁移这类应

用。

所以这篇

论文提出了

IN1

，一种更适合

对单个像素

有更高要求

的场景（如图

像风格迁移

、GAN

等）的归一化

算法。IN 的算法

非常简单，计

算归一化统

计量时考虑

单个样本、单

个通道的所

有元素。

IN、BN 和

LN 的

不同从图 6.1 中

可以非常明

显地看出。

6.5.1 IST 中

的 IN

在 Gatys 等人的

图像风格迁

移（image style

transfer，IST）2 算法中，他

们提出的策

略是通过

L-BFGS 算

法优化生成

图片、风格图

片和内容图

片在

VGG-19 上生成

的特征图的

均方误差。这

种策

略由于

特征图的像

素点数量过

多导致优化

起来非常消

耗时间和内

存。IN 的作者

Ulyanov 等

人同在

2016 年提

出了

Texture Network（见图 6.19）3

。

图

6.19 Texture Networks 的结构

图 6.19 中

的生成器网

络（generator network）是一个由

卷积操作构

成的全

CNN，在原

始的

Texture Network 中，生成

器使用的操

作包括卷积

、池化、上采样

和

BN。但是作者

发现当训练

生成

器网络

时，使用的样

本数越少（如

16 个），得到的效

果越好。但是

我们知道 BN

并

不适用于样

本数

非常少

的环境中，因

此作者提出

了 IN，一种不受

限于批次大

小的算法，专

门用于 Texture

Network

中的

生成器网络

。

6.5.2 IN

与 BN 对比

BN

的详

细算法我们

已经分析过

，这里重复一

下它的计算

方式，如式（6.43）所

示。

1　参见 Dmitry

Ulyanov、Andrea Vedaldi、Victor Lempitsky 的论

文“Instance

Normalization: The Missing Ingredient

for Fast Stylization”。

2

参见 Leon A. Gatys、Alexander

S. Ecker、Matthias Bethge 的论

文“Image

Style Transfer Using Convolutional

Neural Networks”。

3　参见

Dmitry Ulyanov、Vadim Lebedev、Andrea Vedaldi

等人

的论文“Texture Networks: Feed-forward Synthesis

of Textures 

and

Stylized Images”。

6.5　IN

251

（6.43）

正如

我们之前所

分析的，IN 在计

算归一化统

计量时并没

有像

BN 那样跨

样本、单通道

，也没

有像 LN

那

样单样本、跨

通道。它是取

单通道、单样

本上的数据

进行计算的

，如图 6.19（c）所示。

所

以对比 BN

的公

式，它只需要

去掉批次维

的求和即可

，如式（6.44）所示。

（6.44）

对

于 BN

中的可学

习参数 β 和 γ，从

IN

的 TensorFlow 源码中我

们可以看出

这两个参数

是要使

用的

。但是我们也

可以通过将

其值置为

False 来

停用它们，这

一点和其他

归一化方法

在 TensorFlow

中的实现

是相同的。

6.5.3 TensorFlow 中

的 IN

IN 在 TensorFlow 中的函

数声明如下

：

def instance_norm(inputs,

 center=True,

scale=True,

 epsilon=1e-6,

activation_fn=None,

 param_initializers=None,

reuse=None,

 variables_collections=None,

outputs_collections=None,

 trainable=True,

data_format=DATA_FORMAT_NHWC,

 scope=None)

其中的

center 和 scale 便

分别对应

BN 中

的参数 β 和

γ。归

一化统计量

是通过 nn.moments() 函

数

计算的，决定

如何从输入

取值的是

axes 参

数，对应源码

中的 moments_axes 参数。

# 计

算moments (示例激活

)

mean,

variance = nn.moments(inputs, moments_axes,

keep_dims=True)

下面我们提

取源码中的

核心部分，并

通过注释的

方法对其进

行解释（假设

输入的张量

是按批次

数

、高、宽、通道数

的顺序排列

的）：

inputs_rank

= inputs.shape.ndims # 取张量的

维度，这里值

是4

reduction_axis = inputs_rank -

1 # 取通道维

的位置，值为

3

第

6 章  模型优

化方法

252

moments_axes = list(range(inputs_rank))

# 初始

化moments_axes链表，值为

[0,1,2,3]

del moments_axes[reduction_axis]

# 删除第三个

值（通道维），moments_axes变

为[0,1,2]

del moments_axes[0]

# 删除第一

个值（批次维

），moments_axes变为[1,2]

6.5.4 小结

IN 本

身是一个非

常简单的算

法，尤其适用

于批次较小

且单独考虑

每个像素点

的场景，因为

其计算

归一

化统计量时

没有混合批

次和通道的

数据。

另外需

要注意的一

点是在图像

这类应用中

，每个通道上

的值是比较

大的，因此能

够取得比较

合适

的归一

化统计量。但

是在两个场

景中建议不

要使用 IN。

z MLP

或者

RNN 中：因为在 MLP 或

者

RNN 中，每个通

道上只有一

个数据，这时

自然不能

使

用 IN。

z 特征图比

较小时：因为

此时 IN 的采样

数据非常少

，得到的归一

化统计量将

不具有代表

性。

6.6　GN

在本节中

，先验知识包

括：



BN（6.2 节）；  LN（6.3

节）；

 IN（6.5 节）。

GN1 是

何恺明团队

提出的一种

归一化策略

，它是介于 LN 和

IN

之间的一种

折中方案，如

图 6.1

最右所示

。它通过将通

道数据分成

几组计算归

一化统计量

，GN 也是和批次

大小无关的

算法，所以

可

以用在批次

比较小的环

境中。作者在

论文中指出

GN 比 LN 和

IN 的效果

要好。

6.6.1 GN

算法

和

之前所有介

绍过的归一

化算法相同

，GN 也根据该层

的输入数据

计算均值和

方差，然后使

用

这两个值

更新输入数

据，如式（6.45）所示

。

（6.45）

之前介绍的

所有归一化

方法均可以

使用式（6.45）进行

概括，区别是

Si 是如何取得

的。对

BN

来说，Si 是

从不同批次

的同一个通

道上取所有

的值：Si

 =

{k|kC = iC}。而对 LN

来

说，Si 是从

同一

个批次的不

同通道上取

所有的值：Si

= {k|kN = iN}。对

IN

来说，Si 既不跨

批次，也不跨

通道：

1　参见

Yuxin Wu、Kaiming He 的

论文“Group

Normalization”。

6.6　GN

253

Si

 = {k|kN

= iN,kC = iC}。GN

将通道

分成若干组

，只使用组内

的数据计算

均值和方差

。通常组数 G 是

一个超参数

，在 TensorFlow

中的默认

值是 32，如式（6.46）所

示。

（6.46）

我们可以

看出，当

GN 的组

数为 1 时，GN

和 LN 等

价；当 GN

的组数

为通道数时

，GN 和 IN

等价。GN

和其

他算法一样

，也可以添加

参数 γ 和 β

来还

原之前学习

到的特征。

6.6.2 GN 的

源码

论文中

给出了基于

TensorFlow 的 GN 源码：

def GroupNorm(x, gamma, beta,

G, eps=1e−5):

 #

x: 输入

特征，形状是

[N,C,H,W]

 #

gamma, beta: 缩放偏移, 形

状是[1,C,1,1]

# G: GN的组数

N,

C, H, W =

x.shape

 x =

tf.reshape(x, [N, G, C

// G, H, W])

mean, var = tf.nn.moments(x,

[2, 3, 4], keep

dims=True)

 x =

(x − mean) /

tf.sqrt(var + eps)

x = tf.reshape(x, [N,

C, H, W])

return x * gamma

+ beta

第 6

行代码向

张量中添加

一个“组”的维

度，形成一个

五维张量。第

7 行的 axes 的值为

[2,3,4]

表明计算归

一化统计量

时既不会跨

批次，也不会

跨组。

6.6.3 GN 的原理

在使用深度

学习之前，传

统的尺度不

变特征变换

（scale-invariant

feature transform，SIFT）、方

向梯度直

方图（histogram of

oriented gradient，HOG）等算法

均有按组统

计特征的特

性，它们一般

将同一个种

类的特征归

为一组，然后

进行组归一

化。在深度学

习中，每个通

道的特征图

也可以看作

结构化的特

征向量。如果

一个特征图

的卷积数足

够多，那么必

然有一些通

道的特征是

类似的，因此

我们可以将

这些类似的

特征进行归

一化处理。作

者认为，GN 比 LN

效

果好的原因

是 GN 比 LN

的

限制

更少，因为 LN 假

设了一个层

的所有通道

的数据共享

一个均值和

方差，而

IN 则丢

失了探索通

道之间依赖

性的能力。

6.6.4 小

结

作为一种

介于 IN 和 LN

之间

的归一化策

略，GN 的效果反

而优于另外

两个算法，这

令我非常困

惑。虽然作者

也尝试给出

解释，但总是

感觉这个解

释有些过于

主观，有根据

结果推导原

因的嫌疑。

另

外我也做了

一些归一化

方法的对照

实验，实验结

果并不如作

者所说的那

么理想。所以

我们在设计

网络时，如果

批次比较大

，BN 仍旧是最优

的选择。但是

如果批次比

较小，也许通

过对照实验

选出

最好的

归一化策略

是最优的选

择。

第 6 章

模型

优化方法

254

6.7　SN

在

本节中，先验

知识包括：

 BN（6.2 节

）；

 LN（6.3 节）；



IN（6.5 节）。

在本章

的前几节中

，我们介绍了

BN、LN、IN 和

GN 的算法细

节及适用的

任务。虽然这

些归

一化方

法往往能提

升模型的性

能，但是当你

接收一个任

务时，具体选

择哪个归一

化方法有时

候仍然

需要

人工决定，这

往往需要大

量的对照实

验或者依靠

开发者丰富

的经验。本文

提出了

SN1

，它的

算

法核心在

于提出了一

个可微的归

一化层，可以

让模型根据

数据来学习

每一层该选

择的归一化

方法，

抑或是

3

个归一化方

法的加权和

，如图 6.20 所示。所

以 SN

是一个与

任务无关的

归一化方法

，不管

是 LN 适用

的

RNN 还是 IN 适用

的图像风格

迁移，均能使

用

SN。作者在实

验中直接将

SN 用到了

分类

、检测、分割、IST、LSTM 等

各个方向的

任务中，均取

得了非常好

的效果。

图 6.20 SN 是

LN、BN

和 IN 的加权和

6.7.1 SN

详解

1．回顾

SN 实

现了对

BN、LN 和 IN 的

统一。以

CNN 为例

，假设一个 4D 特

征图的尺寸

为

(N,C,W,H)，

hncij 和 分别是

归一化前后

的像素点的

值，其中

n ∈ [1, N]，c

∈ [1, C]，i ∈

[1, H]，j ∈ [1,

W]，μ

和 σ 分

别是均值和

方差，上面所

介绍的所有

归一化方法

均可以表示

为式（6.47）：

（6.47）

其中，β 和

γ 分别是位移

变量和缩放

变量，ϵ

是一个

非常小的数

，用于防止除

数为 0。式（6.47）概

括

了 BN、LN

和 IN 这 3

种归

一化的计算

式，唯一不同

的是计算 μ 和

σ 时统计的像

素点不同。我

们

可以将 μ 和

σ 表示为式（6.48）：

（6.48）

其

中，k ∈ {in,

ln, bn}。IN 统计的是

单个批次、单

个通道的所

有像素点，如

图 6.20

绿色部分

所

1　参见 Ping

Luo、Jiamin Ren、Zhanglin Peng 的论

文“Differentiable

learning-to-normalize via switchable normalization”。

6.7　SN

255

示。BN

统计的

是单个通道

上所有像素

点，如图 6.20 红色

部分所示。LN 统

计的是单个

批次上

的所

有像素点，如

图 6.20 黄色部分

所示。它们依

次可以表示

为 ，

， 。

2．SN 算法介绍

SN

算法用于为

3 组不同的 μk 和

σk

分别学习 3 个

、总共 6

个标量

值（wk 和 wk′）， 的计算

使

用的是它

们的加权和

，如式（6.49）所示：

（6.49）

其

中，Ω =

{in, ln, bn}。在计算 μ1n、σ1n

和

μbn、σbn 时，我们可以

使用 μin、σin 作为中

间变量以减

少计

算量，如

式（6.50）至式（6.52）所示

。

（6.50）

（6.51）

（6.52）

wk 是通过 softmax 计算

得到的激活

函数，如式（6.53）所

示：

（6.53）

其中，{λin, λbn, λ1n}

是需

要优化的，可

以通过反向

传播调整它

们的值。同理

，我们也可以

计算 w′ 对

应的

参数值

{λ′in, λ′bn, λ′1n}。

从上

面的分析中

我们可以看

出，SN

只增加了

6 个参数 。假设

原始网

络的

参数集为

Θ，带

有 SN 的网络的

损失函数可

以表示为 L(Θ,Φ)，它

可以通过反

向传播联合

优化

Θ

和 Φ。对 SN

的

反向推导感

兴趣的读者

参考论文附

件 H。

3．测试

在

BN 的

测试过程中

，为了计算其

归一化统计

量，传统的 BN 方

法是在训练

过程中利用

滑动平

均的

方法得到均

值和方差。在

SN 的 BN 部分，它使

用的是一种

叫作批平均

（batch

average）的方法，

它分

成两步：

（1）固定

网络中的 SN

层

，从训练集中

随机抽取若

干个批次的

样本，将样本

输入网络中

；

（2）计算这些批

次在特定 SN 层

的

μ 和 σ 的平均

值，它们将会

作为测试阶

段的均值和

方差。

实验结

果表明，在 SN 中

批平均的效

果略微优于

滑动平均。

第

6

章  模型优化

方法

6.7.2

SN 的优点

1．SN 的普遍适用

性

SN

通过根据

不同的任务

调整不同归

一化策略的

权值来直接

应用到不同

的任务中，如

图 6.21

所示。

图

6.21 SN 在

不同任务上

不同归一化

策略的权值

比重的可视

化

从图

6.21 中我

们可以看出

LSTM（NAS 的训练）和图

像风格迁移

都学到了最

适合它们本

身的

归一化

策略。

2．SN 与批次

大小

SN 也能根

据批次大小

自动调整不

同归一化策

略的比重，如

果批次大小

的值比较小

，SN

学到的

BN 的权

值就会很小

，反之 BN

的权值

就会很大，如

图 6.22 所示。

图

6.22 SN 在

不同批次大

小下的权值

分布可视化

6.7.3 小结

这篇论

文介绍了统

一 BN、LN 和 IN

这 3 种归

一化策略的

SN，SN 具有以下

3 个

优点。

z 健壮性

：无论批次大

小如何，SN

均能

取得非常好

的效果。

z 通用

性：SN 可以直接

应用到各种

类型的应用

中，简化了人

工选择归一

化策略的过

程。

z 多样性：由

于网络的不

同层在网络

中起着不同

的作用，SN 能够

为每层学到

不同的归一

化

策略，这种

自适应的归

一化策略往

往要优于人

工设定的单

一方案的归

一化策略。

256

59631-深

度学习高手

笔记 卷1：基础

算法-改邮箱

.indd 1-2,4-5

2022/10/19 9:35:23
