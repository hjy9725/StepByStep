作者简介

李福林 一个在

IT领域摸爬滚打十多年的

老程序员、培训师，精

通多

种IT技术，具有软件设计师

职称。分享了多部AI技术教

程，受到

了读者的广泛赞

誉。现任职于阳狮集团，担

任算法工程师职位。教学

风格追求化繁为简，务实

而不空谈，课程设计思路

清晰，课程演绎说

理透彻

，对AI领域技术有自己独到

的见解。

内容简介

本书综

合性讲解HuggingFace社区提供的工

具集datasets和

transformers，书中包括最基础

的工具集的用例演示，具

体的项目实

战，以及预训

练模型的底层设计思路

和实现原理的介绍。通过

本书的

学习，读者可以快

速掌握HuggingFace工具集的使用方

法，掌握自然

语言处理项

目的一般研发流程，并能

研发自己的自然语言处

理项目。

本书分为3篇共14章

：工具集基础用例演示篇

（第1～6章），详

细讲解HuggingFace工具集的

基本使用方法；中文项目

实战篇（第7

～12章），通过几个实

战项目演示使用HuggingFace工具集

研发自然

语言处理项目

的一般流程；预训练模型

底层原理篇（第13、14章），

详细阐

述了预训练模型的设计

思路和计算原理。

本书将

使用最简单浅显的语言

，带领读者快速了解HuggingFace

工具

集的使用方法。通过本书

实战项目的学习，读者可

以掌握一般的

自然语言

处理项目的研发流程。通

过本书预训练模型底层

原理的学

习，读者能够知

其然也知其所以然，做到

融会贯通。

本书适合有PyTorch编

程基础的读者阅读，也适

合作为对自然语

言处理

感兴趣的读者的参考图

书。

前言

PREFACE

自然语言处理一

直作为人工智能领域内

的重要难题，历史上无数

的科学家付出了巨大的

心血对其进行研究。著名

的图灵测试本质上也

是

一个自然语言处理任务

。

在深度学习成为主流后

，自然语言处理确立了主

要的研究方向，

尤其是在

谷歌提出了Transformer和BERT模型以后

，基于预训练模型

的方法

，已成为自然语言处理研

究的主要方向。

随着自然

语言处理研究的大跨步

前进，问题也随之而来，首

要的

就是数据集格式缺

乏统一规范，往往更换一

个数据源，就要做复杂的

数据适配工作，从工程角

度来讲，这增加了项目的

实施风险，作为工

程人员

有时会想，要是能有一个

数据中心，它能把数据都

管理起来，

提供统一的数

据接口就好了。

与数据集

相应，预训练模型也缺乏

统一的规范，它们往往由

不同

的实验室提供，每个

实验室提供的下载方法

都不同，下载之后的使用

方法也各有区别，如果能

把这些模型的下载方式

和使用方式统一，就

能极

大地方便研究，也能降低

项目实施的风险。

基于以

上诉求，HuggingFace社区提供了两套

工具集datasets和

transformers，分别用于数据

集管理和模型管理。基于

HuggingFace

工具集研发能极大地简

化代码，把研发人员从细

节的海洋中拯救出

来，把

更多的精力集中在业务

本身上。

此外，由于数据集

和模型都统一了接口，所

以在更换时也非常方

便

，避免了项目和具体的数

据集、模型的强耦合，从而

降低了项目实

施的风险

。

综上所述，HuggingFace值得所有自然

语言处理研发人员学习

。

本书将使用最简单浅显

的语言，快速地讲解HuggingFace工具

集的使

用方法，并通过几

个实例来演示使用HuggingFace工具

集研发自然语

言处理项

目的过程。

通过本书的学

习，读者能够快速地掌握

HuggingFace工具集的使

用方法，并且

能够使用HuggingFace研发自己的自

然语言处理项目。

本书主

要内容

第1章介绍HuggingFace提出的

标准研发流程和提供的

工具集。

第2章介绍编码工

具，包括编码工具的工作

过程的示意，以及编

码工

具的用例。

第3章介绍数据

集工具，包括数据集仓库

和数据集的基本操作。

第

4章介绍评价指标，包括评

价指标的加载和使用方

法。

第5章介绍管道工具，并

演示使用管道工具完成

一些常见的自然

语言处

理任务。

第6章介绍训练工

具，并演示使用训练工具

完成一个情感分类任

务

。

第7章演示第1个实战任务

，完成一个中文情感分类

任务。

第8章演示第2个实战

任务，完成一个中文填空

任务。

第9章演示第3个实战

任务，完成一个中文句子

关系推断任务。

第10章演示

第4个实战任务，完成一个

中文命名实体识别任务

。

第11章演示使用TensorFlow框架完成

中文命名实体识别任务

。

第12章演示使用自动模型

完成一个情感分类任务

，并阅读源代码

深入了解

自动模型的工作原理。

第

13章演示手动实现Transformer模型，并

完成两个实验性质的

翻

译任务。

第14章演示手动实

现BERT模型，并演示BERT模型的训

练过程。

阅读建议

本书是

一本对HuggingFace工具集的综合性

讲解图书，既有基础

知识

，也有实战示例，还包括底

层原理的讲解。

本书尽量

以最简洁的语言书写，每

个章节之间的内容尽量

独立，

读者可以跳跃阅读

而没有障碍。

作为一本实

战性书籍，读者要掌握本

书的知识，务必结合代码

调

试，本书的代码也尽量

以最简洁的形式书写，使

读者阅读不感吃力。

每个

代码块即是一个单元测

试，读者可以对每个程序

的每个代码块按

从上到

下的顺序测试，从一个个

小知识点聚沙成塔，融会

贯通。

HuggingFace支持使用PyTorch、TensorFlow等深度学

习框架

进行计算，本书会

以PyTorch为主进行讲解。对于使

用TensorFlow的

读者也不用担心，会

有单独的一章讲解如何

使用TensorFlow实现一个

具体的例

子。项目之间有很多的共

同点，只要学会了一个例

子，其他

的都可以触类旁

通。

本书源代码

扫描付费

二维码，可获取本书源代

码。

本书源代码在以下环

境中测试通过，为避免不

必要的异常调试，

请尽量

选择一致的版本。

Python 3.6

transformers 4.18

datasets 2.3

PyTorch 1.10

致谢

感

谢我的好友L，在我写作的

过程中始终鼓励、鞭策我

，使我有勇

气和动力完成

本书的写作。

在本书的编

写过程中，我虽已竭尽所

能为读者呈现最好的内

容，

但疏漏之处在所难免

，敬请读者批评指正。

李福

林

2023年1月

工具集基础用例

演示篇

第1章　HuggingFace简介

HuggingFace是一个

开源社区，提供了开源的

AI研发框架、工具

集、可在线

加载的数据集仓库和预

训练模型仓库。

1.前HuggingFace时代的

弊端

在前HuggingFace时代，AI系统的研

发没有统一的标准，往往

凭

借研发人员各自的喜

好随意设计研发的流程

，缺乏统一的规范，设计

的

质量取决于研发人员个

人的经验水平。这增加了

项目实施的风险，

因为独

立设计的研发流程往往

没有经历过完整的工程

验证，不一定如

设想般可

行。

另一方面，研发流程设

计由研发人员个人设计

还有一个弊端：项

目和研

发人员个人形成了强绑

定，容易造成“祖传代码”问

题。在项

目交接时难度大

，后续人员需要完整地学

习前人的个人习惯，成本

较

大，导致很难让后续的

研发人员介入。

2.HuggingFace标准研发

流程

由于以上问题的存

在，HuggingFace提出了一套可以依照

的标准

研发流程，按照该

框架实施工程，能够在一

定程度上规避以上提出

的

问题，降低了项目实施

的风险及项目和研发人

员的耦合度，让后续的

研

发人员能够更容易地介

入，即把HuggingFace的标准研发流程

变成

所有研发人员的公

共知识，不需要额外地学

习。

HuggingFace把AI项目的研发大致分

为以下几部分，如图1-1所

示

。

图1-1 HuggingFace标准研发流程

HuggingFace能处理

文字、语音和图像数据，由

于本书的主题是

自然语

言处理，所以主要关注文

字类任务。

图1-1是一个粗略

的流程，现在稍微细化这

个流程，看一看各个

步骤

中更具体的内容，针对自

然语言处理任务细化的

HuggingFace标

准研发流程，如图1-2所示

。

图1-2 针对自然语言处理任

务细化的HuggingFace标准研发流程

可以看出，HuggingFace的标准研发流

程和传统的一般项目研

发

流程很相似，所以HuggingFace的学

习成本较低，值得所有研

发人员

学习掌握。

3.HuggingFace工具集

针对流程中的各个节点

，HuggingFace都提供了很多工具类，能

够帮助研发人员快速地

实施。HuggingFace提供的工具集如图

1-3所

示。

图1-3 各个步骤HuggingFace提供的

工具集

从图1-3可以看出，HuggingFace提

供的工具集基本囊括了

标准

流程中的各个步骤

，使用HuggingFace工具集能够极大地

简化代码复

杂度，让研发

人员能把更多的精力集

中在具体的业务问题上

，而不是

陷入琐碎的细节

中。

我们常说这世上不存

在“银弹”，针对具体的项目

，需要有各自

的优化点，正

所谓没有最好的，只有最

合适的，所以在研发具体

的项

目时需要灵活应对

，但依然应该尽量遵守标

准研发流程。

4.HuggingFace社区活跃度

HuggingFace的官方主页网址为https://huggingface.co，访问

后可以通过导航访问HuggingFace主

GitHub仓库，截至本书写作时

间

，已经获得了68059颗星。

包括Meta、Google、Microsoft、Amazon在

内的超过5000家

组织机构在

为HuggingFace开源社区贡献代码、数

据集和模型。

HuggingFace的模型仓库

已经共享了超过60000个模型

，数据集

仓库已经共享了

超过8000个数据集，基于开源

共享的精神，这些资源

的

使用都是完全免费的。

HuggingFace代

码库也在快速更新中，HuggingFace开

始时以

自然语言处理任

务为重点，所以HuggingFace大多数的

模型和数据集

也是自然

语言处理方向的，但图像

和语音的功能模型正在

快速更新

中，相信未来逐

渐会把图像和语音的功

能完善并标准化，如同自

然语

言处理一样。

第2章　使

用编码工具

2.1 编码工具简

介

HuggingFace提供了一套统一的编

码API，由每个模型各自提交

实现。由于统一了API，所以调

用者能快速地使用不同

模型的编码工

具。

在学习

HuggingFace的编码工具之前，先看一

个示例的编码过

程，以理

解编码工具的工作过程

。

2.2 编码工具工作流示意

1.定

义字典

文字是一个抽象

的概念，不是计算机擅长

处理的数据单元，计算

机

擅长处理的是数字运算

，所以需要把抽象的文字

转换为数字，让计

算机能

够做数学运算。

为了把抽

象的文字数字化，需要一

个字典把文字或者词对

应到某

个数字。一个示意

的字典如下：

#字典

vocab = {

'<SOS>': 0,

'<EOS>': 1,

'the': 2,

'quick': 3,

'brown': 4,

'fox': 5,

'jumps': 6,

'over': 7,

'a': 8,

'lazy': 9,

'dog': 10,

}

注意：这

只是一个示意的字典，所

以只有11个词，在实际项目

中

的字典可能会有成千

上万个词。

2.句子预处理

在

句子被分词之前，一般会

对句子进行一些特殊的

操作，例如把

太长的句子

截短，或在句子中添加首

尾标识符等。

在示例字典

中，我们注意到除了一般

的词之外，还有一些特殊

符

号，例如<SOS>和<EOS>，它们分别代

表一个句子的开头和结

束。

把这两个特殊符号添

加到句子上，代码如下：

#简

单编码

sent = 'the quick

brown fox jumps over a lazy

dog'

sent = '<SOS> ' +

sent + ' <EOS>'

print(sent)

运行结果如下：

<SOS> the quick brown fox jumps

over a lazy dog<EOS>

3.分

词

现在句子准备好了，接

下来需要把句子分成一

个一个的词。对于

中文来

讲，这是个复杂的问题，但

是对于英文来讲这个问

题比较容易

解决，因为英

文有自然的分词方式，即

以空格来分词，代码如下

：

#英文分词

words = sent.split()

print(words)

运行结果如下

：

['<SOS>', 'the', 'quick', 'brown',

'fox', 'jumps', 'over', 'a',

'lazy', 'dog',

'<EOS>']

可以看到，这个英文的句

子已经分成了比较理想

的一个一个的单

词。

对于

中文来讲，分词的问题比

较复杂，因为中文所有的

字是连在

一起写的，不存

在一个自然的分隔符号

。有很多成熟的工具能够

做中

文分词，例如jieba分词、LTP分

词等，但是在本书中不会

使用这些工

具，因为HuggingFace的编

码工具已经包括了分词

这一步工作，由各

个模型

自行实现，对于调用者来

讲这些工作是透明的，不

需要关心具

体的实现细

节。

4.编码

句子已按要求添

加了首尾标识符，并且分

割成了一个一个的单

词

，现在需要把这些抽象的

单词映射为数字。因为已

经定义好了字

典，所以使

用字典就可以把每个单

词分别地映射为数字，代

码如下：

#编码为数字

encode = [vocab[i] for

i in words]

print(encode)

运行

结果如下：

[0,

2, 3, 4, 5, 6, 7,

8, 9, 10, 1]

以上是一个示

例的编码的工作流程，经

历了定义字典、句子预处

理、分词、编码4个步骤，见表

2-1。

表2-1

编码工作的流程示意

2.3 使用编码工具

经过以上

示例，可以知道编码的过

程中要经历哪些工作步

骤了。

现在就来看一看如

何使用HuggingFace提供的编码工具

。

1.加载编码工具

首先需要

加载一个编码工具，这里

使用bert-base-chinese的实

现，代码如下：

#第

2章/加载编码工具

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(

pretrained_model_name_or_path='bert-base-chinese',

cache_dir=None,

force_download=False,

)

参 数 pretrained_model_name_or_path='bert-base￾chinese'指

定要加载的编码工具，大

多数模型会把自己提交

的编码工

具命名为和模

型一样的名字。

模型和它

的编码工具通常是成对

使用的，不会出现张冠李

戴的情

况，建议调用者也

遵从习惯，成对使用。

参数

cache_dir用于指定编码工具的缓

存路径，这里指定为

None（默认

值），也可以指定想要的缓

存路径。

参数force_download为True时表明无

论是否已经有本地缓存

，

都强制执行下载工作。建

议设置为False。

2.准备实验数据

现在有了一个编码工具

，让我们来准备一些句子

，以测试编码工

具，代码如

下：

#第2章/准备实验数据

sents = [

'你

站在桥上看风景',

'看风景

的人在楼上看你',

'明月装

饰了你的窗子',

'你装饰了

别人的梦',

]

这是一些中文

的句子，后面会用这几个

句子做一些实验。

3.基本的

编码函数

首先从一个基

本的编码方法开始，代码

如下：

#第2章/基本的编码函

数

out = tokenizer.encode(

text=sents[0],

text_pair=sents[1],

#当句子长度大于max_length时截

断

truncation=True,

#一律补PAD，直到max_length长度

padding='max_length',

add_special_tokens=True,

max_length=25,

return_tensors=None,

)

print(out)

print(tokenizer.decode(out))

这里

调用了编码工具的encode()函数

，这是最基本的编码函数

，

一次编码一个或者一对

句子，在这个例子中，编码

了一对句子。

不是每个编

码工具都有编码一对句

子的功能，具体取决于不

同模

型的实现。在BERT中一般

会编码一对句子，这和BERT的

训练方式有

关系，具体可

参见第14章。

(1)参数text和text_pair分别为

两个句子，如果只想编码

一个句

子，则可让text_pair传None。

(2)参数

truncation=True表明当句子长度大于max_length时

，

截断句子。

(3) 参 数 padding=

'max_length' 表 明 当 句

子 长

度 不 足

max_length时，在句子的

后面补充PAD，直到max_length长度。

(4)参数

add_special_tokens=True表明需要在句子中添加

特殊符

号。

(5)参数max_length=25定义了max_length的

长度。

(6)参数return_tensors=None表明返回的数

据类型为list格式，

也 可 以 赋

值为

tf 、 pt 、 np ，

分 别 表 示 TensorFlow 、

PyTorch 、

NumPy数据格式

。

运行结果如下：

 [101,

872, 4991, 1762, 3441, 677, 4692,

7599, 3250, 102,

4692, 7599, 3250,

4638, 782, 1762, 3517, 677, 4692,

872, 102, 0, 0, 0, 0]

[CLS] 你 站 在 桥

上 看

风 景 [SEP] 看 风 景

的 人 在

楼 上 看

你

[SEP] [PAD] [PAD]

[PAD] [PAD]

可以看到编码

的输出为一个数字的list，这

里使用了编码工具的

decode()函

数把这个list还原为分词前

的句子。这样就可以看出

编码

工具对句子做了哪

些预处理工作。

从输出可

以看出，编码工具把两个

句子前后拼接在一起，中

间使

用[SEP]符号分隔，在整个

句子的头部添加符号[CLS]，在

整个句子的

尾部添加符

号[SEP]，因为句子的长度不足

max_length，所以补充了4

个[PAD]。

另外从空

格的情况也能看出，编码

工具把每个字作为一个

词。因

为每个字之间都有

空格，表明它们是不同的

词，所以在BERT的实现

中，中文

分词处理比较简单，就是

把每个字都作为一个词

来处理。

4.进阶的编码函数

完成了上面最基础的编

码函数，现在来看一个稍

微复杂的编码函

数，代码

如下：

#第2章/进阶的编码函

数

out = tokenizer.encode_plus(

text=sents[0],

text_pair=sents[1],

#当句子长度大于max_length时截

断

truncation=True,

#一律补零，直到max_length长度

padding='max_length',

max_length=25,

add_special_tokens=True,

#可

取值tf、pt、np，默认为返回list

return_tensors=None,

#返回token_type_ids

return_token_type_ids=True,

#返

回attention_mask

return_attention_mask=True,

#返回special_tokens_mask

特殊符号标识

return_special_tokens_mask=True,

#返

回length 标识长度

return_length=True,

)

#input_ids 编码后的词

#token_type_ids 第1个句子和特殊符号的

位置是0，第2个句子的位置

是1

#special_tokens_mask 特殊符号的位置是1，其

他位置是0

#attention_mask

PAD的位置是0，其他

位置是1

#length 返回句子长度

for k, v

in out.items():

print(k, ':', v)

tokenizer.decode(out['input_ids'])

和

之前不同，这里调用了encode_plus()函

数，这是一个进阶版

的编

码函数，它会返回更加复

杂的编码结果。和encode()函数一

样，

encode_plus()函数也可以编码一个

句子或者一对句子，在这

个例子

中，编码了一对句

子。

参 数

return_token_type_ids 、 return_attention_mask 、

return_special_tokens_mask、return_length表明需要返回相

应的

编码结果，如果指定

为False，则不会返回对应的内

容。

运行结果如下：

 input_ids : [101, 872,

4991, 1762, 3441, 677, 4692, 7599,

3250, 102, 4692,

7599, 3250, 4638,

782, 1762, 3517, 677, 4692, 872,

102, 0, 0,

0, 0]

token_type_ids

: [0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 1,

1, 1, 1, 1,

1, 1,

1, 1,

1, 1, 0, 0,

0, 0]

special_tokens_mask : [1, 0,

0, 0, 0, 0, 0, 0,

0, 1, 0, 0, 0,

0,

0, 0, 0, 0,

0, 0,

1, 1, 1, 1, 1]

attention_mask

: [1, 1, 1, 1, 1,

1, 1, 1, 1, 1, 1,

1, 1, 1, 1,

1, 1,

1, 1,

1, 1, 0, 0,

0, 0]

length : 25

'[CLS]

你 站 在

桥 上 看 风

景 [SEP] 看 风 景 的

人

在 楼 上

看 你 [SEP]

[PAD] [PAD]

[PAD] [PAD]'

首先看最后

一行，这里把编码结果中

的input_ids还原为文字形

式，可以

看到经过预处理的原文

本。预处理的内容和encode()函数

一

致。

这次编码的结果和

encode()函数不一样的地方在于

这次返回的不

是一个简

单的list，而是4个list和1个数字，见

表2-2。

表2-2 进阶的编码函数结

果

续表

接下来对编码的

结果分别进行说明。

(1)输出

input_ids：编码后的词，也就是encode()函数

的输出。

(2)输出token_type_ids：因为编码的

是两个句子，这个list用于

表

明编码结果中哪些位置

是第1个句子，哪些位置是

第2个句子。具体

表现为，第

2个句子的位置是1，其他位

置是0。

(3)输出special_tokens_mask：用于表明编码

结果中哪些位置是

特殊

符号，具体表现为，特殊符

号的位置是1，其他位置是

0。

(4)输出attention_mask ： 用于 表 明

编 码结果

中 哪些 位 置 是

PAD。具体表现

为，PAD的位置是0，其他位置是

1。

(5)输出length：表明编码后句子的

长度。

5.批量的编码函数

以

上介绍的函数，都是一次

编码一对或者一个句子

，在实际工程

中需要处理

的数据往往是成千上万

的，为了提高效率，可以使

用

batch_encode_plus

()函数批量地进行数据

处理，代码如下：

#第2章/批量

编码成对的句子

out = tokenizer.batch_encode_plus(

#编码成

对的句子

batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2],

sents[3])],

add_special_tokens=True,

#当句子长度大

于max_length时截断

truncation=True,

#一律补零，直到

max_length长度

padding='max_length',

max_length=25,

#可取值tf、pt、np，默认为返回

list

return_tensors=None,

#返回token_type_ids

return_token_type_ids=True,

#返回attention_mask

return_attention_mask=True,

#返回special_tokens_mask 特殊符号

标识

return_special_tokens_mask=True,

#返回offsets_mapping 标识每个词的

起止位置，这个参数只能

BertTokenizerFast使用

#return_offsets_mapping=True,

#返回length 标识长度

return_length=True,

)

#input_ids 编码

后的词

#token_type_ids 第1个句子和特殊

符号的位置是0，第2个句子

的位置是1

#special_tokens_mask 特殊符号的位

置是1，其他位置是0

#attention_mask PAD的位置

是0，其他位置是1

#length 返回句子

长度

for k, v in out.items():

print(k,

':', v)

tokenizer.decode(out['input_ids'][0])

参数batch_text_or_text_pairs用于编码一批

句子，示例中为成对

的句

子，如果需要编码的是一

个一个的句子，则修改为

如下的形式即

可。

batch_text_or_text_pairs=[sents[0], sents[1]]

运行结

果如下：

 input_ids :

[[101, 872, 4991, 1762, 3441, 677,

4692, 7599,

3250, 102, 4692,

7599,

3250, 4638, 782, 1762, 3517, 677,

4692, 872, 102, 0, 0,

0,

0], [101, 21128,

21129, 749, 872,

4638, 21130, 102, 872, 21129, 749,

1166, 782,

4638, 3457, 102,

0,

0, 0, 0, 0, 0, 0,

0, 0]]

token_type_ids : [[0, 0,

0, 0, 0, 0, 0, 0,

0, 0, 1, 1, 1, 1,

1,

1, 1, 1, 1,

1,

1, 0, 0, 0, 0], [0,

0, 0, 0, 0, 0, 0,

0, 1, 1, 1, 1, 1,

1,

1, 1, 0, 0, 0,

0,

0, 0, 0, 0, 0]]

special_tokens_mask : [[1, 0, 0, 0,

0, 0, 0, 0, 0, 1,

0, 0, 0,

0, 0, 0,

0,

0, 0, 0, 1, 1,

1, 1, 1], [1, 0, 0,

0, 0, 0, 0, 1, 0,

0, 0, 0,

0, 0, 0,

1, 1, 1,

1, 1, 1,

1, 1, 1, 1]]

length :

[21, 16]

attention_mask : [[1, 1,

1, 1, 1, 1, 1, 1,

1, 1, 1, 1, 1, 1,

1,

1, 1, 1, 1,

1,

1, 0, 0, 0, 0], [1,

1, 1, 1, 1, 1, 1,

1, 1, 1, 1, 1, 1,

1,

1, 1, 0, 0, 0,

0,

0, 0, 0, 0, 0]]

'[CLS] 你 站 在 桥 上

看 风

景 [SEP] 看 风 景

的 人 在 楼 上

看

你

[SEP] [PAD] [PAD]

[PAD] [PAD]'

可以看到，这里的输出

都是二维的list了，表明这是

一个批量的编

码。这个函

数在后续章节中会多次

用到。

6.对字典的操作

到这

里，已经掌握了编码工具

的基本使用，接下来看一

看如何操

作编码工具中

的字典。首先查看字典，代

码如下：

#第2章/获取字典

vocab

= tokenizer.get_vocab()

type(vocab), len(vocab), '明

月' in

vocab

运行后输出如下：

(dict, 21128, False)

可以

看到，字典本身是个dict类型

的数据。在BERT的字典中，共

有

21128个词，并且“明月”这个词并

不存在于字典中。

既然“明

月”并不存在于字典中，可

以把这个新词添加到字

典

中，代码如下：

#第2章/添加

新词

tokenizer.add_tokens(new_tokens=['明月', '装饰',

'窗子'])

这里

添加了3个新词，分别为“明

月”“装饰”和“窗子”。也

可以添

加新的符号，代码如下：

#第

2章/添加新符号

tokenizer.add_special_tokens({'eos_token': '[EOS]'})

接下来试

试用添加了新词的字典

编码句子，代码如下：

#第2章

/编码新添加的词

out=tokenizer.encode(

text='明月装

饰了你的窗子[EOS]',

text_pair=None,

#当句子长

度大于max_length时截断

truncation=True,

#一律补PAD，直

到max_length长度

padding='max_length',

add_special_tokens=True,

max_length=10,

return_tensors=None,

)

print(out)

tokenizer.decode(out)

输出如下：

[101, 21128,

21129, 749, 872, 4638, 21130, 21131,

102, 0]

'[CLS] 明月 装

饰 了

你 的 窗子 [EOS] [SEP] [PAD]'

可以看到

，“明月”已经被识别为一个

词，而不是两个词，新的

特

殊符号[EOS]也被正确识别。

2.4 小

结

本章讲解了编码的工

作流程，分为定义字典、句

子预处理、分

词、编码等步

骤；使用了HuggingFace编码工具的基

本编码函数和批

量编码

函数，并对编码结果进行

了解读；查看了HuggingFace编码工

具

的字典，并且能向字典添

加新词。

第3章　使用数据集

工具

3.1 数据集工具介绍

在

以往的自然语言处理任

务中会花费大量的时间

在数据处理上，

针对不同

的数据集往往需要不同

的处理过程，各个数据集

的格式差异

大，处理起来

复杂又容易出错。针对以

上问题，HuggingFace提供了

统一的数

据集处理工具，让开发者

在处理各种不同的数据

集时可以通

过统一的API处

理，大大降低了数据处理

的工作量。

登

录 HuggingFace 官 网 ， 单

击

顶 部 的 Datasets ， 即

可 看 到

HuggingFace提供的

数据集，如图3-1所示。

图3-1 HuggingFace数据

集页面

在该界面左侧可

以根据不同的任务类型

、语言、体积、使用许可

来筛

选数据集，右侧为具体的

数据集列表，其中有经典

的glue、

super_glue数据集，问答数据集squad，情

感分类数据集imdb，纯

文本数

据集wikitext。

单击具体的某个数

据集，进入数据集的详情

页面，可以看到数据

集的

概要信息。以glue数据集为例

，在详情页可以看到glue的各

个数

据子集的概要内容

，每个数据子集的下方可

能会有作者写的说明信

息，如图3-2所示。

图3-2 数据集详

情页面

不要担心，你不需

要熟悉所有的数据集，这

些数据集大多是英文

的

，本书重点关注中文的数

据集。出于简单起见，本书

只会使用几个

简单的数

据集来完成后续的实战

任务，具体可参看接下来

的代码演

示。

3.2 使用数据集

工具

3.2.1 数据集加载和保存

1.在线加载数据集

使用HuggingFace数

据集工具加载数据往往

只需一行代码，以加

载名

为seamew/ChnSentiCorp数据集为例，代码如下

：

#第3章/加载数据集

from datasets import load_dataset

dataset = load_dataset(path='seamew/ChnSentiCorp')

dataset

注意：由

于HuggingFace把数据集存储在谷歌

云盘上，在国内加

载时可

能会遇到网络问题，所以

本书的配套资源中已经

提供了保存好

的 数 据 文

件 ， 使

用 load_from_disk() 函 数 加 载

即 可 。 关

于

load_from_disk()函数可参见本章“从本

地磁盘加载数据集”一节

。

可以看到，要加载一个数

据集是很简单的，使用load_dataset()

函

数，把数据集的名字作为

参数传入即可。运行结果

如下：

DatasetDict({

train: Dataset({

features: ['text',

'label'],

num_rows: 9600

})

validation: Dataset({

features: ['text', 'label'],

num_rows: 0

})

test: Dataset({

features: ['text', 'label'],

num_rows:

1200

})

})

可以看到seamew/ChnSentiCorp共分为3部

分，分别为train、

validation和test，分别代表训

练集、验证集和测试集，并

且每条数据

有两个字段

，即text和label，分别代表文本和标

签。

还可以看到3部分分别

的数据量，其中验证集的

数据量为0条，说

明虽然作

者切分出了验证集这部

分，但并没有向其中分配

数据，这是

一个空的部分

。

加载数据集的load_dataset()函数还有

一些其他参数，通过下面

这个例子说明，代码如下

：

#第3章/加载glue数据集

load_dataset(path='glue',

name='sst2', split='train')

这里加

载了经典的glue数据集，熟悉

glue的读者可能已经知道

glue分

了很多数据子集，可以以

参数name指定要加载的数据

子集，

在上面的例子中加

载了sst2数据子集。

还可以使

用参数split直接指定要加载

的数据部分，在上面的例

子

中加载了数据的train部分

。

运行结果如下：

Dataset({

features: ['sentence', 'label',

'idx'],

num_rows: 67349

})

可以看到

，glue的sst2数据子集的train部分有67 349条

数据，每

条数据都具有sentence、label和

idx字段。

2.将数据集保存到本

地磁盘

加载了数据集后

，可以使用save_to_disk()函数将数据集

保存到

本地磁盘，代码如

下：

#第3章/将数据集保存到

磁盘

dataset.save_to_disk(

dataset_dict_path='./data/ChnSentiCorp')

3.从本地磁盘加载数

据集

保存到磁盘以后可

以使用load_from_disk()函数加载数据集

，代

码如下：

#第3章/从磁盘加

载数据集

from

datasets import load_from_disk

dataset = load_from_disk('./data/ChnSentiCorp')

3.2.2 数据集基本操

作

1.取出数据部分

为了便

于做后续的实验，这里取

出数据集的train部分，代码如

下：

#使用train数据子集做后续

的实验

dataset

= dataset['train']

2.查看数据内容

可

以查看部分数据样例，代

码如下：

#第3章/查看数据样

例

for

i in [12, 17, 20, 26,

56]:

print(dataset[i])

运行结果如下：

 {'text': '轻便，方

便携带，性能也不错，能满

足平时的工作需要，对出

差人

员来讲非常不错',

'label': 1}

{'text': '很

好的地理位置，一塌糊涂

的服务，萧条的酒店。', 'label':

0}

{'text': '非常

不错，服务很好，位于市中

心区，交通方便，不过价格

也高！',

'label': 1}

{'text':

'跟住招待所没什么

太大区别。绝对不会再住

第2次的酒店！',

'label': 0}

{'text': '价格太高，性

价比不够好。我觉得今后

还是去其他酒店比较好

。',

'label':

0}

到这里，可以看出数据是

什么内容了，这是一份购

物和消费评论

数据，字段

text表示消费者的评论，字段

label表明这是一段好评还是

差评。

3.数据排序

可以使用

sort()函数让数据按照某个字

段排序，代码如下：

#第3章/排

序数据

#数据中的label是无序

的

print(dataset['label'][:10])

#让数据按照label排序

sorted_dataset = dataset.sort('label')

print(sorted_dataset['label'][:10])

print(sorted_dataset['label'][-10:])

运行

结果如下：

[1, 1, 0,

0, 1, 0, 0, 0, 1,

1]

[0, 0, 0, 0, 0,

0, 0, 0, 0, 0]

[1,

1, 1, 1, 1, 1, 1,

1, 1, 1]

可以看到，初始

数据是乱序的，使用sort()函数

后，数据按照

label排列为有序

的了。

4.打乱数据

和sort()函数相

对应，可以使用shuffle()函数再次

打乱数据，代码

如下：

#第3章

/打乱数据顺序

shuffled_dataset=sorted_dataset.shuffle(seed=42)

shuffled_dataset['label'][:10]

运行结果

如下：

[0, 1, 0, 0, 1, 0,

1, 0, 1, 0]

可以看到，数据再次

被打乱为无序。

5.数据抽样

可以使用select()函数从数据集

中选择某些数据，代码如

下：

#第3章/从数据集中选择

某些数据

dataset.select([0, 10, 20, 30, 40,

50])

运行结果如下

：

Dataset({

features: ['text', 'label'],

num_rows: 6

})

选择出的数据会再次组

装成一个数据子集，使用

这种方法可以实

现数据

抽样。

6.数据过滤

使用filter()函数

可以按照自定义的规则

过滤数据，代码如下：

#第3章

/过滤数据

def f(data):

return data['text'].startswith('非常不错')

dataset.filter(f)

filter()函数

接受一个函数作为参数

，在该函数中确定过滤数

据的

条件，在上面的例子

中数据过滤的条件是评

价以“非常不错”开头，

运行

结果如下：

Dataset({

features:

['text', 'label'],

num_rows: 13

})

可以看到，满足

评价以“非常不错”开头的

数据共有13条。

7.训练测试集

拆分

可以使用train_test_split()函数将数

据集切分为训练集和测

试

集，代码如下：

#第3章/切分

训练集和测试集

dataset.train_test_split(test_size=0.1)

参数test_size表

明测试集占数据总体的

比例，例子中占10%，可

知训练

集占90%，运行结果如下：

DatasetDict({

train: Dataset({

features: ['text',

'label'],

num_rows: 8640

})

test: Dataset({

features: ['text', 'label'],

num_rows: 960

})

})

可以

看到，数据集被切分为train和

test两部分，并且两部分数据

量的比例满足9:1。

8.数据分桶

可以使用shared ()函数把数据均

匀地分为n部分，代码如下

：

#第3章/数据分桶

dataset.shard(num_shards=4,

index=0)

(1)参数num_shards表明

要把数据均匀地分为几

部分，例子中分

为4部分。

(2)参

数index表明要取出第几份数

据，例子中为取出第0份。

运

行结果如下：

Dataset({

features: ['text', 'label'],

num_rows: 2400

})

因为原数据

集数量为9600条，均匀地分为

4份后每一份是2400

条，和上面

的输出一致。

9.重命名字段

使用rename_column()函数可以重命名字

段，代码如下：

#第3章/字段重

命名

dataset.rename_column('text', 'text_rename')

运行结果如下：

Dataset({

features: ['text_rename', 'label'],

num_rows:

9600

})

原始

字段text现在已经被重命名

为text_rename。

10.删除字段

使用remove_columns()函数可

以删除字段，代码如下：

#第

3章/删除字段

dataset.remove_columns(['text'])

运行结果如

下：

Dataset({

features: ['label'],

num_rows:

9600

})

可以看到字段text现在已

经被删除。

11.映射函数

有时

希望对数据集总体做一

些修改，可以使用map()函数遍

历数

据，并且对每条数据

都进行修改，代码如下：

#第

3章/应用函数

def f(data):

data['text'] = 'My

sentence: ' + data['text']

return data

maped_datatset = dataset.map(f)

print(dataset['text'][20])

print(maped_datatset['text'][20])

map()函数是很强

大的一个函数，map()函数以一

个函数作为入

参，在该函

数中确定要对数据进行

的修改，可以是对数据本

身的修

改，例如例子中的

代码就是对text字段增加了

一个前缀，也可以进行

增

加字段、删除字段、修改数

据格式等操作，运行结果

如下：

非常不错，服务很好

，位于市中心区，交通方便

，不过价格也高！

My sentence:

非常不错

，服务很好，位于市中心区

，交通方便，不过价格也高

！

经过map()函数的映射后text字段

多了一个前缀，而原始数

据则

没有。

12.使用批处理加

速

在使用过滤和映射这

类需要使用一个函数遍

历数据集的方法时，

可以

使用批处理减少函数调

用的次数，从而达到加速

处理的目的。在

默认情况

下是不使用批处理的，由

于每条数据都需要调用

一次函数，

所以函数调用

的次数等于数据集中数

据的条数，如果数据的数

量很

多，则需要调用很多

次函数。使用批处理函数

，能够一批一批地处理

数

据，让函数调用的次数大

大减少，代码如下：

#第3章/使

用批处理加速

def

f(data):

text=data['text']

text=['My sentence: ' +

i for i in text]

data['text']=text

return data

maped_datatset=dataset.map(function=f,

batched=True,

batch_size=1000,

num_proc=4)

print(dataset['text'][20])

print(maped_datatset['text'][20])

在这段代

码中，调用了数据集的map()函

数，对数据进行了映射

操

作，但这次除了数据处理

函数之外，还额外传入了

很多参数，下面

对这些参

数进行讲解。

(1)参数batched=True和batch_size=1000：表示

以1000条数据

为一个批次进

行一次处理，这将把函数

执行的次数削减约1000倍，提

高了运行效率，但同时对

内存会提出更高的要求

，读者需要结合自己

的运

算设备调节合适的值，通

常来讲，1000是个合适的值。

(2)参

数num_proc=4：表示在4条线程上执行

该任务，同样是和

性能相

关的参数，读者可以结合

自己的运算设备调节该

值，一般设置

为CPU核心数量

。

当使用批处理处理数据

时，每次传入处理函数的

就不是一条数据

了，而是

一个批次的数据。在上面

的例子中，一个批次为1000条

数

据，在编写处理函数时

需要注意，以上代码的运

行结果如下：

非常不错，服

务很好，位于市中心区，交

通方便，不过价格也高！

My sentence: 非

常不错，服务很好，位于市

中心区，交通方便，不过价

格也高！

可以看到，数据处

理的结果和使用单条数

据映射时的结果一致，

使

用批处理仅仅是性能上

的考量，不会影响数据处

理的结果。

13.设置数据格式

使用set_format()函数修改数据格式

，代码如下：

 #第3章/设置数据

格式

dataset.set_format(type='torch',

columns=['label'],

output_all_columns=

True)

dataset[20]

(1)参数type表明要修改为

的数据类型，常用的取值

有numpy、

torch、tensorflow、pandas等。

(2)参数columns表明要修改格

式的字段。

(3)参数output_all_columns表明是否

要保留其他字段，设置为

True表明要保留。

运行结果如

下：

 {'label': tensor(1),

'text': '非常不错，服务很好，位

于市中心区，

交通方便，不

过价格

也高！'}

字段label已经被

修改为PyTorch的Tensor格式。

3.2.3

将数据集

保存为其他格式

1.将数据

保存为CSV格式

可以把数据

集保存为CSV格式，便于分享

，同时数据集工具也有

加

载CSV格式数据的方法，代码

如下：

#第3章/导出为CSV格式

dataset

= load_dataset(path='seamew/ChnSentiCorp',

split='train')

dataset.to_csv(path_or_buf='./data/ChnSentiCorp.csv')

#加

载CSV格式数据

csv_dataset

= load_dataset(path='csv',

data_files='./data/ChnSentiCorp.csv',

split='train')

csv_dataset[20]

运行结果如

下：

{'Unnamed: 0': 20, 'text': '非常不错，服务很好，位

于市中心区，交

通方便，不

过价格也

高！', 'label': 1}

可以看到，保

存为CSV格式后再加载，多了

一个Unnamed字

段，在这一列中实

际保存的是数据的序号

，这和保存的CSV文件内容

有

关系。如果不想要这一列

，则可以直接到CSV文件去删

除第1列，删

除时可以使用

数据集的删除列功能，在

此不再赘述。

2.保存数据为

JSON格式

除了可以保存为CSV格

式外，也可以保存为JSON格式

，方法和

CSV格式大同小异，代

码如下：

#第3章/导出为JSON格式

dataset=load_dataset(path='seamew/ChnSentiCorp', split='train')

dataset.to_json(path_or_buf='./data/ChnSentiCorp.json')

#加载JSON格式数据

json_dataset=load_dataset(path='json',

data_files='./data/ChnSentiCorp.json',

split='train')

json_dataset[20]

运行结果

如下：

{'text': '非常不错，服务很好

，位于市中心区，交通方便

，不过价格也高！',

'label': 1}

可以看到

，保存为JSON格式并不存在多

列的问题。

3.3 小结

本章讲解

了HuggingFace数据集工具的使用，包

括数据的加载、

保存、查看

、排序、抽样、过滤、拆分、映射

、列重命名等操作。

第4章　使

用评价指标工具

4.1 评价指

标工具介绍

在训练和测

试一个模型时往往需要

计算不同的评价指标，如

正确

率、查准率、查全率、F1值

等，具体需要的指标往往

和处理的数据

集、任务类

型有关。HuggingFace提供了统一的评

价指标工具，能够

将具体

的计算过程隐藏，调用者

只需提供计算结果，由评

价指标工具

给出评价指

标。

4.2 使用评价指标工具

1.列

出可用的评价指标

使用

list_metrics()函数可获取可用的评价

指标列表，代码如下：

#第4章

/列出可用的评价指标

from datasets import list_metrics

metrics_list =

list_metrics()

len(metrics_list), metrics_list[:5]

运

行结果如下：

(51, ['accuracy',

'bertscore', 'bleu', 'bleurt', 'cer'])

可以看到，共

有51个可用的评价指标，为

了节省篇幅，这里只打

印

前5个。

2.加载一个评价指标

使用load_metric()函数加载一个评价

指标。评价指标往往和对

应

的数据集配套使用，此

处以glue数据集的mrpc子集为例

，代码如

下：

#第4章/加载一个

评价指标

from datasets

import load_metric

metric = load_metric(path='glue', config_name='mrpc')

可以看到，加载

一个评价指标和加载一

个数据集一样简单。将对

应数据集和子集的名字

输入load_metric()函数即可得到对应

的评价指

标，但并不是每

个数据集都有对应的评

价指标，在实际使用时以

满足

需要为准则选择合

适的评价指标即可。

3.获取

评价指标的使用说明

评

价指标的inputs_description属性为一段文

本，描述了评价指

标的使

用方法，不同的评价指标

需要的输入往往是不同

的，代码如

下：

print(metric.inputs_description)

该输出的内

容很长，包括了对此评价

指标的介绍，要求输入格

式

的说明，输出指标的说

明，以及部分示例代码，此

处截选部分内容如

下：

>>> glue_metric=datasets.load_metric('glue', 'mrpc') 

#'mrpc' or

'qqp'

>>> references=[0, 1]

>>> predictions=[0,

1]

>>> results=glue_metric.compute(predictions=predictions,

references=

references)

>>>

print(results)

{'accuracy': 1.0, 'f1': 1.0}

这

是一段示例代码，其中很

清晰地给出了此评价指

标的使用方

法。

4.计算评价

指标

按照上面的示例代

码，可以实际地计算此评

价指标，代码如下：

#第4章/计

算一个评价指标

predictions=[0, 1,

0]

references=[0, 1, 1]

metric.compute(predictions=predictions, references=references)

运行结

果如下：

{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666}

可以看到，这个评

价指标的计算输出包括

了正确率和F1值。

4.3 小结

本章

讲解了HuggingFace评价指标工具的

使用，在实际使用时评

价

指标工具往往和训练工

具一起使用，能够随着训

练步骤进行，同时

监控评

价指标，以确定模型确实

正向着一个理想的目标

进步。

第5章

使用管道工具

5.1 管道工具介绍

HuggingFace有一个巨

大的模型库，其中一些是

已经非常成熟的

经典模

型，这些模型即使不进行

任何训练也能直接得出

比较好的预测

结果，也就

是常说的Zero Shot

Learning。

使用管道工具

时，调用者需要做的只是

告诉管道工具要进行的

任

务类型，管道工具会自

动分配合适的模型，直接

给出预测结果，如果

这个

预测结果对于调用者已

经可以满足需求，则不再

需要再训练。

管道工具的

API非常简洁，隐藏了大量复

杂的底层代码，即使是

非

专业人员也能轻松使用

。

5.2 使用管道工具

5.2.1 常见任务

演示

1.文本分类

使用管道

工具处理文本分类任务

，代码如下：

#第5章/文本分类

from transformers import pipeline

classifier =

pipeline("sentiment-analysis")

result = classifier("I hate you")[0]

print(result)

result = classifier("I love you")[0]

print(result)

可以看到，使用管道工具

的代码非常简洁，把任务

类型输入

pipeline()函数中，返回值

即为能执行具体预测任

务的classifier对

象，如果向具体的

句子输入该对象，则会返

回具体的预测结果。示例

代码中预测了I hate you和I

love you两句话

的情感分类，运行结果如

下：

{'label': 'NEGATIVE', 'score': 0.9991129040718079}

{'label': 'POSITIVE', 'score': 0.9998656511306763}

从运行结果可以看到

，I hate

you和I love you两句话的情感分类

结

果分别为NEGATIVE和POSITIVE，并且分数都

高于0.99，可见模型

对预测结

果的信心很强。

2.阅读理解

使用管道工具处理阅读

理解任务，代码如下：

#第5章

/阅读理解

from transformers import pipeline

question_answerer=pipeline("question-answering")

context=r"""

Extractive Question Answering is the

task of extracting an

answer from

a text

given a question. An

example of a

question answering dataset

is the SQuAD dataset, which is

entirely based on

that task. If

you would like to fine-tune

a

model on a SQuAD task, you

may leverage the

examples/PyTorch/question￾answering/run_squad.py script.

"""

result=question_answerer(

question="What is extractive question answering?",

context=context,

)

print(result)

result=question_answerer(

question="What is

a good example of a question

answering

dataset?",

context=context,

)

print(result)

在

这 段 代 码 中

， 首

先 以 question-answering 为 参 数

调 用 了

pipeline()函

数，得到了question_answerer对象。context是一段文

本，也是模型需要阅读理

解的目标，把context和关于context的一

个

问题同时输入question_answerer对象中

，即可得到相应的答案。

注

意：问题的答案必须在context中

出现过，因为模型的计算

过

程是从context中找出问题的

答案，所以如果问题的答

案不在context

中，则模型不可能

找到答案。

运行结果如下

：

 {'score': 0.6177279949188232,

'start': 34, 'end': 95,

'answer': 'the

task of

extracting an answer from

a text given a question'}

{'score':

0.5152303576469421, 'start': 148, 'end': 161,

'answer':

'SQuAD

dataset'}

在示例代码中问了关于

context的两个问题，所以此处得

到了两

个答案。

第1个问题

翻译成中文是“什么是抽

取式问答？”，模型给出的

答

案翻译成中文是“从给定

文本中提取答案的任务

”。

第2个问题翻译成中文是

“问答数据集的一个好例

子是什

么？”，模型给出的答

案翻译成中文是“SQuAD数据集

”。

3.完形填空

使用管道工具

处理完形填空任务，代码

如下：

 #第5章/完形填空

from transformers import pipeline

unmasker=pipeline("fill-mask")

from

pprint import pprint

sentence='HuggingFace is creating

a <mask> that the community

uses

to solve

NLP tasks.'

unmasker(sentence)

在这

段代码中，sentence是一个句子，其

中某些词被<mask>符

号替代了

，表明这是需要让模型填

空的空位，运行结果如下

：

 [{'score': 0.17927466332912445,

'token': 3944,

'token_str': ' tool',

'sequence': 'HuggingFace is

creating a tool that the community

uses to solve

NLP tasks.'},

{'score':

0.11349395662546158,

'token': 7208,

'token_str': ' framework',

'sequence': 'HuggingFace is creating a framework

that the

community uses

to solve

NLP tasks.'},

{'score': 0.05243551731109619,

'token': 5560,

'token_str': ' library',

'sequence': 'HuggingFace is

creating a library that the

community

uses to

solve NLP tasks.'},

{'score':

0.034935347735881805,

'token': 8503,

'token_str': ' database',

'sequence': 'HuggingFace is creating a database

that the

community uses to

solve

NLP tasks.'},

{'score': 0.02860259637236595,

'token': 17715,

'token_str': ' prototype',

'sequence': 'HuggingFace is

creating a prototype that the

community

uses

to solve NLP tasks.'}]

原问题翻译成中文是“HuggingFace正

在创建一个社区用户，用

于解决NLP任务的____。”，模型按照

信心从高到低给出了5个

答案，

翻译成中文分别是

“工具”“框架”“资料库”“数据库

”“原

型”。

4.文本生成

使用管道

工具处理文本生成任务

，代码如下：

#第5章/文本生成

from transformers

import pipeline

text_generator=pipeline("text-generation")

text_generator("As far as

I am concerned, I will",

max_length=50,

do_sample=False)

在 这 段 代 码

中 ， 得 到 了 text_generator

对

象 后 ， 直 接 调

用

text_generator对象，入参

为一个句子的开头，让text_generator接

着往下续写，参数max_length=50表明要

续写的长度，运行结果如

下：

 [{'generated_text': 'As far

as I am concerned, I will

be

the first to admit

that

I am not a fan of

the idea of a "free market."

I think

that the idea of

a

free market is a bit

of a stretch. I think that

the idea'}]

这段文本翻译成中文

后为就我而言，我将是第

1个承认我不支持

“自由市

场”理念的人，我认为自由

市场的想法有点牵强。我

认为这

个想法……

5.命名实体

识别

命名实体识别任务

为找出一段文本中的人

名、地名、组织机构名

等。使

用管道工具处理命名实

体识别任务，代码如下：

 #第

5章/命名实体识别

from transformers

import pipeline

ner_pipe=pipeline("ner")

sequence = """Hugging

Face Inc. is a company based

in New York

City. Its

headquarters

are in DUMBO,

therefore very close

to the Manhattan Bridge which is

visible

from the

window."""

for entity

in ner_pipe(sequence):

print(entity)

运行结

果如下：

 {'entity':

'I-ORG', 'score': 0.99957865, 'index': 1,

'word':

'Hu', 'start':

0, 'end': 2}

{'entity':

'I-ORG', 'score': 0.9909764, 'index': 2, 'word':

'##gging',

'start': 2, 'end': 7}

{'entity':

'I-ORG', 'score': 0.9982224, 'index': 3, 'word':

'Face', 'start':

8, 'end': 12}

{'entity':

'I-ORG', 'score': 0.9994879, 'index': 4, 'word':

'Inc', 'start':

13, 'end': 16}

{'entity':

'I-LOC', 'score': 0.9994344, 'index': 11, 'word':

'New', 'start':

40, 'end': 43}

{'entity':

'I-LOC', 'score': 0.99931955, 'index': 12, 'word':

'York', 'start':

44, 'end': 48}

{'entity':

'I-LOC', 'score': 0.9993794, 'index': 13, 'word':

'City', 'start':

49, 'end': 53}

{'entity':

'I-LOC', 'score': 0.98625815, 'index': 19, 'word':

'D', 'start':

79, 'end': 80}

{'entity':

'I-LOC', 'score': 0.95142674, 'index': 20, 'word':

'##UM', 'start':

80, 'end': 82}

{'entity':

'I-LOC', 'score': 0.93365884, 'index': 21, 'word':

'##BO', 'start':

82, 'end': 84}

{'entity':

'I-LOC', 'score': 0.9761654, 'index': 28, 'word':

'Manhattan',

'start': 114, 'end': 123}

{'entity':

'I-LOC', 'score': 0.9914629, 'index': 29, 'word':

'Bridge',

'start': 124, 'end': 130}

可以看到，模型识

别中的原文中的组织机

构名为Hugging

Face

Inc，地名为New York City、DUMBO、Manhattan Bridge。

6.文本摘要

使用管道工具处理文本

摘要任务，代码如下：

示例

代码中的ARTICLE是一个很长的

文本，使用文本总结工具

对

这段长文本进行摘要

，并设定摘要内容的长度

为30～130个词，运行

结果如下：

 [{'summary_text': '

Liana Barrientos, 39, is charged with

two counts of

"offering a false

instrument for filing in the first

degree" In

total, she has

been

married 10 times, with nine of

her marriages occurring

between 1999 and

2002 .

At one time, she

was married to eight men at

once, prosecutors

say .'}]

摘

要翻译成中文为现年39岁

的莉安娜·巴连托斯被控

两项“提供虚

假文书申请

一级学位”的罪名。她共结

过10次婚，其中9次发生在

1999—2002年

。检察官表示，她曾一度与

8名男性同时结婚。

由于原

文太长，这里不便于给出

中文翻译，读者可以自行

检查该

摘要和原文的内

容是否契合。

7.翻译

使用管

道工具处理翻译任务，代

码如下：

#第5章/翻译

from transformers import pipeline

translator=pipeline("translation_en_to_de")

sentence="Hugging

Face is a technology company based

in New

York and Paris"

translator(sentence,

max_length=40)

在 这 段

代 码 中

， 首 先 以 参 数

translation_en_to_de 调 用

了

pipeline()函数，得到了translator。从该参数

可以看出，这是一个从

英

文翻译到德文的管道工

具。

注意：由于默认的翻译

任务底层调用的是t5-base模型

，该模型

只支持由英文翻

译为德文、法文、罗马尼亚

文，如果需要支持其他语

言，则需要替换模型，具体

可参见本章“替换模型执

行中译英任务”

和“替换模

型执行英译中任务”两节

。

运行结果如下：

 [{'translation_text': 'Hugging

Face ist ein

Technologieunternehmen mit Sitz

in New York und Paris.'}]

模型给出

的德文翻译成中文是“Hugging

Face是

一家总部位于

纽约和巴

黎的科技公司。”这和英文

原文的意思基本一致。

5.2.2 替

换模型执行任务

1.替换模

型执行中译英任务

管理

工具会根据不同的任务

自动分配一个模型，如果

该模型不是

调用者想使

用的，则可以指定管道工

具使用的模型。此处以翻

译任务

为例，代码如下：

#第

5章/替换模型执行中译英

任务

from transformers import

pipeline, AutoTokenizer,

AutoModelForSeq2SeqLM

#要使用该模型，需要

安装sentencepiece

!pip install

sentencepiece

tokenizer=AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt￾zh-en")

model=AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus￾mt-zh-en")

translator=pipeline(task="translation_zh_to_en",

model=model,

tokenizer=tokenizer)

sentence="我叫萨拉，我住在伦

敦。"

translator(sentence, max_length=20)

在这段代码中，同样执

行翻译任务，不过执行了

默认的翻译任务

工具不

支持的中译英任务，为了

支持中译英这个任务，需

要替换默认

的模型，代码

中加载了一个模型和其

对应的编码工具，再把模

型和编

码工具作为参数

输入pipeline()函数中，得到替换了

模型的翻译管道工

具。最

后执行一个中译英任务

，运行结果如下：

[{'translation_text': 'My name is

Sarah, and I live in

London.'}]

从运行结

果来看，翻译的效果还是

比较理想的。

2.替换模型执

行英译中任务

根据上述

中译英管道工具的例子

，此处再举一例英译中任

务，代

码如下：

#第5章/替换模

型执行英译中任务

from

transformers import pipeline, AutoTokenizer,

AutoModelForSeq2SeqLM

#要使

用该模型，需要安装sentencepiece

!pip install sentencepiece

tokenizer=AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt￾en-zh")

model=AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus￾mt-en-zh")

translator=pipeline(task="translation_en_to_zh",

model=model,

tokenizer=tokenizer)

sentence="My name is Sarah

and I live in London"

translator(sentence,

max_length=20)

代码

内容和中译英任务大同

小异，只是替换了模型的

名字，以及

管道工具的翻

译方向，运行结果如下：

[{'translation_text': '我

叫萨拉，我住伦敦'}]

从运行

结果来看，翻译的效果还

是比较理想的。

5.3 小结

本章

讲解了HuggingFace管道工具的使用

，管道工具的使用非常

简

单，同时也能实现非常强

大的功能，如果对预测的

结果要求不高，

则可以免

于再训练的烦琐步骤。管

道工具是HuggingFace提供的非常

实

用的工具。

第6章　使用训练

工具

6.1 训练工具介绍

HuggingFace提供

了巨大的模型库，虽然其

中的很多模型性能表

现

出色，但这些模型往往是

在广义的数据集上训练

的，缺乏针对特定

数据集

的优化，所以在获得一个

合适的模型之后，往往还

要针对具体

任务的特定

数据集进行二次训练，这

就是所谓的迁移学习。

使

用迁移学习的好处很多

，例如节约了碳排放，保护

了珍贵的地

球；迁移学习

的训练难度低，要求的数

据集数量少，对计算资源

的要

求也低。

HuggingFace提供了训练

工具，统一了模型的再训

练过程，使调

用者无须了

解具体模型的计算过程

，只需针对具体的任务准

备好数据

集，便可以再训

练模型。

在本章中将使用

一个情感分类任务的例

子来再训练一个模型，以

此来讲解HuggingFace训练工具的使

用方法。

6.2 使用训练工具

6.2.1

准

备数据集

1.加载编码工具

首先加载一个编码工具

，由于编码工具和模型往

往是成对使用

的，所以此

处使用hfl/rbt3编码工具，因为要

再训练的模型是hfl/rbt3

模型，代

码如下：

#第6章/加载tokenizer

from

transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('hfl/rbt3')

加载了

编码工具之后不妨试算

一下，观察一下输出，代码

如下：

#第6章/试编码句子

tokenizer.batch_encode_plus(

['明

月装饰了你的窗子', '你装

饰了别人的梦'],

truncation=True,

)

运行结果

如下：

 {'input_ids': [[101, 3209,

3299, 6163, 7652, 749, 872,

4638,

4970, 2094, 102],

[101, 872, 6163,

7652, 749, 1166, 782, 4638, 3457,

102]],

'token_type_ids': [[0,

0, 0, 0,

0, 0, 0, 0, 0, 0,

0], [0, 0, 0, 0, 0,

0, 0, 0, 0, 0]],

'attention_mask':

[[1, 1, 1, 1, 1, 1,

1, 1, 1, 1, 1], [1,

1, 1, 1, 1, 1, 1,

1, 1,

1]]}

2.准备数据集

加载数

据集，使用该数据集来再

训练模型，代码如下：

#第6章

/从磁盘加载数据集

from datasets import load_from_disk

dataset =

load_from_disk('./data/ChnSentiCorp')

#缩小

数据规模，便于测试

dataset['train'] =

dataset['train'].shuffle().select(range(2000))

dataset['test']

= dataset['test'].shuffle().select(range(100))

dataset

在这

段代码中，对数据集进行

了采样，目的有以下两方

面：一是

便于测试；二是模

拟再训练集的体量较小

的情况，以验证即使是小

的

数据集，也能通过迁移

学习得到一个较好的训

练结果。运行结果如

下：

DatasetDict({

train: Dataset({

features: ['text',

'label'],

num_rows: 2000

})

validation: Dataset({

features: ['text', 'label'],

num_rows: 0

})

test: Dataset({

features: ['text', 'label'],

num_rows:

100

})

})

可

见训练集的数量仅有2000条

，测试集的数量有100条。

现在

的数据集还是文本数据

，使用编码工具把这些抽

象的文字编

码成计算机

善于处理的数字，代码如

下：

#第6章/编码

def f(data):

return tokenizer.batch_encode_plus(data['text'],

truncation=True)

dataset=dataset.map(f,

batched=True,

batch_size=1000,

num_proc=4,

remove_columns=['text'])

dataset

在这段代码

中，使用了批量处理的技

巧，能够加快计算的速度

。

(1)参数batched=True：表明使用批处理来

处理数据，而不是一

条一

条地处理。

(2)参数batch_size=1000：表明每个

批次中有1000条数据。

(3)参数num_proc=4：表

明使用4个线程进行操作

。

(4)参数remove_columns=['text']：表明映射结束后删

除数据集

中的text字段。

运行

结果如下：

DatasetDict({

train: Dataset({

features:

['label', 'input_ids', 'token_type_ids',

'attention_mask'],

num_rows: 2000

})

validation: Dataset({

features: ['text', 'label'],

num_rows: 0

})

test: Dataset({

features:

['label', 'input_ids', 'token_type_ids',

'attention_mask'],

num_rows: 100

})

})

可以看到，原本

数据集中的text字段已经被

移除，但多了

input_ids、token_type_ids、attention_mask字段，这些字

段是编

码工具编码的结

果，这和前面观察到的编

码器试算的结果一致。

由

于模型对句子的长度有

限制，不能处理长度超过

512个词的句

子，所以需要把

数据集中长度超过512个词

的句子过滤掉，代码如

下

：

#第6章/移除太长的句子

def f(data):

return

[len(i)<=512 for i in data['input_ids']]

dataset=dataset.filter(f,

batched=True, batch_size=1000,

num_proc=4)

dataset

此

处依然使用了批处理的

技巧来加快计算，各参数

的意义和之前

编码时的

意义相同，运行结果如下

：

DatasetDict({

train: Dataset({

features: ['label', 'input_ids',

'token_type_ids',

'attention_mask'],

num_rows: 1973

})

validation:

Dataset({

features: ['text', 'label'],

num_rows: 0

})

test: Dataset({

features: ['label', 'input_ids',

'token_type_ids',

'attention_mask'],

num_rows: 100

})

})

可以看到，训练集中有7条

数据被移除，而测试集中

没有被移除

数据。

注意：对

于数据长度超过模型限

制有很多处理方法，此处

只演示

了最简单的丢弃

法。也可以把超出长度的

部分截断，留下符合模型

长

度要求的数据，截断数

据时可以截断数据的尾

部，也可以截断数据的

头

部，当截断数据时，编码结

果中的input_ids、token_type_ids、

attention_mask要一起截断，因为

它们是一一对应的关系

。

6.2.2 定义模型和训练工具

1.加

载预训练模型

数据集准

备好了，现在就可以加载

要再训练的模型了，代码

如

下：

#第6章/加载模型

from transformers import AutoModelForSequenceClassification

import

torch

model=AutoModelForSequenceClassification.from_pretrained('hfl/r

bt3',

num_labels=2)

#统计

模型参数量

sum([i.nelement()

for i in model.parameters()]) / 10000

如前所述，此

处加载的模型应该和编

码工具配对使用，所以此

处

加载的模型为hfl/rbt3模型，该

模型由哈尔滨工业大学

讯飞联合实验

室(HFL)分享到

HuggingFace模型库，这是一个基于中

文文本数据训

练的BERT模型

。后续将使用准备好的数

据集对该模型进行再训

练，在

代码的最后一行统

计了该模型的参数量，以

大致衡量一个模型的体

量

大小。该模型的参数量

约为3800万个，这是一个较小

的模型。

加载了模型之后

，不妨对模型进行一次试

算，以观察模型的输

出，代

码如下：

#第6章/模型试算

#模

拟一批数据

data =

{

'input_ids': torch.ones(4, 10, dtype=torch.long),

'token_type_ids':

torch.ones(4, 10, dtype=torch.long),

'attention_mask': torch.ones(4, 10,

dtype=torch.long),

'labels': torch.ones(4, dtype=torch.long)

}

#模型试算

out = model(**data)

out['loss'], out['logits'].shape

这

里模拟了一个批次的数

据对模型进行试算，运行

结果如下：

(tensor(0.3597, grad_fn=<NllLossBackward0>), torch.Size([4,

2]))

模型的输出主

要包括两部分，一部分是

loss，另一部分是logits。

对于不同的

模型，输出的内容也会不

一样，但一般会包括loss，所以

在使用HuggingFace模型时不需要自

行计算loss，而是由模型自行

封

装，这方便了模型的再

训练。

2.定义评价函数

为了

便于在训练过程中观察

模型的性能变化，需要定

义一个评价

指标函数。对

于情感分类任务往往关

注正确率指标，所以此处

加载正

确率评价函数，代

码如下：

#第6章/加载评价指

标

from datasets import load_metric

metric =

load_metric('accuracy')

由于模型计算的输出

和评价指标要求的输入

还有差别，所以需要

定义

一个转换函数，把模型计

算的输出转换成评价指

标可以计算的数

据类型

，这个函数就是在训练过

程中真正要用到的评价

函数，代码如

下：

#第6章/定义

评价函数

import numpy as np

from transformers.trainer_utils

import EvalPrediction

def compute_metrics(eval_pred):

logits, labels

= eval_pred

logits = logits.argmax(axis=1)

return

metric.compute(predictions=logits, references=labels)

#模拟输出

eval_pred = EvalPrediction(

predictions=np.array([[0, 1], [2, 3], [4, 5],

[6, 7]]),

label_ids=np.array([1, 1, 0, 1]),

)

compute_metrics(eval_pred)

在这

段代码中，不仅定义了评

价函数，还对该函数进行

了试算，

运行结果如下：

{'accuracy': 0.75}

可

见这个评价指标计算的

输出为正确率，在训练的

过程中可以观

察到模型

的正确率变化。

3.定义训练

超参数

在开始训练之前

，需要定义好超参数，HuggingFace 使用

TrainingArguments对象来封装超参数，代码

如下：

#第6章/定义训练参数

from

transformers import TrainingArguments

#定义训练参数

args =

TrainingArguments(

#定义临时

数据保存路径

output_dir='./output_dir',

#定义测试

执行的策略，可取值为no、epoch、steps

evaluation_strategy='steps',

#定

义每隔多少个step执行一次

测试

eval_steps=30,

#定义模型保存策略

，可取值为no、epoch、steps

save_strategy='steps',

#定义每隔多少

个step保存一次

save_steps=30,

#定义共训练

几个轮次

num_train_epochs=1,

#定义学习率

learning_rate=1e-4,

#加

入参数权重衰减，防止过

拟合

weight_decay=1e-2,

#定义测试和训练时

的批次大小

per_device_eval_batch_size=16,

per_device_train_batch_size=16,

#定义是否要

使用GPU训练

no_CUDA=True,

)

TrainingArguments对象中可以封

装的超参数很多，但除了

output_dir之外其他的超参数均有

默认值，在上面的示例代

码中只给

出了常用的参

数，对于初学者建议从这

些简单的参数开始调试

，完整

的参数列表可参照

HuggingFace官方文档。

4.定义训练器

完

成了上面的准备工作，现

在可以定义训练器，代码

如下：

#第6章/定义训练器

from

transformers import Trainer

from transformers.data.data_collator import

DataCollatorWithPadding

#定

义训练器

trainer = Trainer(

model=model,

args=args,

train_dataset=dataset['train'],

eval_dataset=dataset['test'],

compute_metrics=compute_metrics,

data_collator=DataCollatorWithPadding(tokenizer),

)

定义训练器时

需要传递要训练的模型

、超参数对象、训练和验证

数据集、评价函数，以及数

据整理函数。

5.数据整理函

数介绍

数 据 整 理

函 数 使

用 了 由 HuggingFace

提 供 的

DataCollatorWithPadding对象，它能

把一个批次中长短不一

的句子

补充成统一的长

度，长度取决于这个批次

中最长的句子有多长，所

有

数据的长度一致后即

可转换成矩阵，模型期待

的数据类型也是矩阵，

所

以经过数据整理函数的

处理之后，数据即被整理

成模型可以直接计

算的

矩阵格式。可以通过下面

的例子验证，代码如下：

#第

6章/测试数据整理函数

data_collator = DataCollatorWithPadding(tokenizer)

#获

取一批数据

data = dataset['train'][:5]

#输出这些句

子的长度

for

i in data['input_ids']:

print(len(i))

#调用数据整理

函数

data

= data_collator(data)

#查看整理后的数据

for k, v in

data.items():

print(k, v.shape)

运行结果如下：

62

34

185

101

40

input_ids torch.Size([5, 185])

token_type_ids torch.Size([5, 185])

attention_mask torch.Size([5, 185])

labels torch.Size([5])

在这段代

码中，首先初始化了一个

DataCollatorWithPadding对

象作为数据整理函数

，然后从训练集中获取了

5条数据作为一批数

据，从

输出可以看出这些句子

有长有短，之后使用数据

整理函数处理

这批数据

，得到的结果再输出形状

，可以看到这些数据已经

被整理成

统一的长度，长

度取决于这批句子中最

长的句子，并且被转换为

矩阵

形式。

通过如下代码

可以查看数据整理函数

是如何对句子进行补长

的，

代码如下：

tokenizer.decode(data['input_ids'][0])

运行结果如

下：

'[CLS] 1. 综合配置不错； 2 键盘触

摸板手感不错； 3.

液晶屏看

电影的效

果不错，就是上

下可视

角小了； 4. 质量轻便

，因为是全塑料的外壳 [SEP]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD]'

可

以看到，数据整理函数是

通过对句子的尾部补充

PAD来对句子

补长的。

6.2.3

训练和

测试

1.训练模型

在开始训

练之前，不妨直接对模型

进行一次测试，先定下训

练前

的基准，在训练结束

后再对比这里得到的基

准，以验证训练的有效

性

，代码如下：

#评价模型

trainer.evaluate()

运行

结果如下：

{'eval_loss': 0.8067871928215027,

'eval_accuracy': 0.48484848484848486,

'eval_runtime': 12.1022,

'eval_samples_per_second': 8.18,

'eval_steps_per_second': 0.578}

可见模型在训

练之前，有48%的正确率。由于

使用的训练集为二

分类

数据集，所以48%的正确率近

乎于瞎猜。这符合预期，因

为模型

还没有训练，接下

来对模型进行训练，期待

它能超过此处得到的成

绩。

对模型进行训练，代码

如下：

#第6章/训练

trainer.train()

运行训练

，会输出如下日志信息：

***** Running training *****

Num

examples = 1979

Num Epochs =

1

Instantaneous batch size per device

= 16

Total train batch size

(w. parallel, distributed &

accumulation) =

16

Gradient Accumulation steps = 1

Total optimization steps = 124

从

该日志中的Total

optimization steps = 124可知，本次训

练

共有124个steps，由于定义超参

数时指定了每30个steps执行一

次测

试，并保存模型参数

，所以当训练结束时，期待

有4次测试的结果，

并且有

4个保存的模型参数。

训练

的时间取决于计算资源

的大小，使用一颗Intel酷睿10代

i5训

练的时间约20min，在训练的

过程中会逐步输出一张

表格以便于观察

各项指

标，内容见表6-1。

表6-1 消息格式

观察该表，由于在超参数

中设定了每30个steps执行一次

测试，

而每次测试产生一

次测试结果，表现在表格

中为一行数据，由于在本

次训练任务中共有124个steps，会

执行4次测试，所以这张表

有4行数

据。下面对表格各

列的内容分别进行介绍

。

(1)列Step：表示测试执行时的steps。

(2)列

Training Loss：表示训练loss，在本次任务中

未记录。

(3)列Validation

Loss：表示在验证集

上测试得出的loss。

(4)列Accuracy：表示在

验证集上测试得出的正

确率，也就是评

价函数计

算的输出。

理解了表格内

容之后，可以观察到随着

训练步数的增多，正确率

在不断上升，当训练到120步

时，已经达到了94%的正确率

，这相比

训练前得到的正

确率48%有了很大提升，证明

训练是有效的。

由于在超

参数设置了每30个steps保存一

次模型参数，所以可以

到

设定的output_dir文件夹检查模型

参数是否已经保存。

在output_dir文

件夹中可以找到4个文件

夹，即checkpoint-30、

checkpoint-60、checkpoint-90、checkpoint-120，分别是对应步

数保

存的检查点，每个文件夹

中都有一个PyTorch_model.bin文件，

这个文

件就是模型的参数。

如果

在训练的过程中由于各

种原因导致训练中断，或

者希望从某

个 检 查 点 重

新 训

练 模 型 ， 则 可

以 使 用

训 练 器 的

train() 函 数 的

resume_from_checkpoint参数设

定检查点，从该检查点重

新训练，

代码如下：

#第6章/从

某个存档文件继续训练

trainer.train(resume_from_checkpoint='./output_dir/checkpoint-

90')

继续训练和从头训练的

输出是一致的，只是继续

训练会跳过前90

个steps，所以上

面的代码只会训练124−90=34个steps，继

续训练

同样会保存检查

点，所以上面的代码会覆

盖检查点checkpoint-120。

在训练结束后

，不妨再执行一次测试，以

测试模型的性能，代码

如

下：

#第6章/评价模型

trainer.evaluate()

运行结

果如下：

{'eval_loss': 0.1946406215429306,

'eval_accuracy': 0.94,

'eval_runtime': 12.1149,

'eval_samples_per_second': 8.254,

'eval_steps_per_second': 0.578,

'epoch': 1.0}

可以看到，模型最

终的性能为正确率94%。从训

练过程的表格来

看，模型

显然还能继续进步，还没

有达到收敛，但本章的主

题为介绍

训练器的使用

方法，所以达到该成绩已

经足够。读者可以自行加

强训

练力度，包括增加数

据量和增加训练轮数，以

此让模型达到更好的性

能。

2.模型的保存和加载

训

练得到满意的模型之后

，可以手动将该模型的参

数保存到磁盘

上，以备以

后需要时加载，代码如下

：

#第6章/手动保存模型参数

trainer.save_model(output_dir='./output_dir/save_model')

加载模型参数的方法如

下：

 #第6章/手动加载模型参

数

import torch

model.load_state_dict(torch.load('./output_dir/save_model/PyTor

ch_model.

bin'))

3.使用模型预测

最后介

绍使用模型进行预测的

方法，代码如下：

#第6章/测试

model.eval()

for

i, data in enumerate(trainer.get_eval_dataloader()):

break

out

= model(**data)

out = out['logits'].argmax(dim=1)

for

i in range(8):

print(tokenizer.decode(data['input_ids'][i],

skip_special_tokens =True))

print('label=', data['labels'][i].item())

print('predict=', out[i].item())

在这段代码中，首先把模

型切换到运行模式，然后

从测试数据集

中获取1个

批次的数据用于预测，之

后把这批数据输入模型

进行计

算，得出的结果即

为模型预测的结果，最后

输出前4句的结果，并与

真

实的label进行比较，运行结果

如下：

从测试结果可以看

到一些错误，但大部分的

预测是正确的。

6.3 小结

本章

通过一个情感分类任务

讲解了HuggingFace训练工具的使用

方法，介绍了一般的数据

集处理方法，在训练过程

中结合评价函数观

察模

型的性能变化，并且介绍

了模型的保存和加载及

预测的方法。

中文项目实

战篇

第7章　实战任务1：中文

情感分类

7.1 任务简介

分类

任务是大多数机器学习

任务中最基础的，对于自

然语言处理

也不例外。本

章将讲解一个情感分类

的自然语言处理任务。

7.2 数

据集介绍

本章所使用的

数据集依然是ChnSentiCorp数据集，这

是一个情

感分类数据集

，每条数据中包括一句购

物评价，以及一个标识，表

明

这条评价是一条好评

还是一条差评。在ChnSentiCorp数据集

中，被

评价的商品包括书

籍、酒店、计算机配件等。对

于人类来讲，即使不

给予

标识，也能通过评价内容

大致判断出这是一条好

评还是一条差

评；对于神

经网络，也将通过这个任

务来验证它的有效性。

ChnSentiCorp数

据集中的部分数据样例

见表7-1，通过该表读者

可对

ChnSentiCorp数据集有直观的认识。

表

7-1 ChnSentiCorp数据集数据样例

续表

7.3 模

型架构

在BERT、GPT、Transformers模型被提出之

前，被广泛使用的

自然语

言处理网络结构是RNN。RNN的主

要功能是能把自然语言

的

句子抽取成特征向量

，有了特征向量之后再接

入全连接神经网络做分

类或者回归就水到渠成

了。从这个角度来讲，RNN把一

个自然语言处

理的任务

转换成了全连接神经网

络任务。对于类似于RNN这样

能够把

抽 象 数

据 类 型 转

换 成 具

体 的 特 征 向 量

的

网 络 层 ， 被 统

称 为

backbone，中文一

般译为特征抽取层。

自从

BERT、GPT、Transformers模型被提出之后，它们被

广泛

应用于任务中的backbone层

，也就是特征抽取层，在本

章的情感分

类任务中也

将使用BERT中文模型作为backbone层

。

相对于backbone的网络，后续的处

理神经网络被称为下游

任务

模型，它往往会对backbone输

出的特征向量进行再计

算，得到业务

上需要的计

算结果，这往往是分类或

者回归的结果。整合backbone

和下

游任务模型的架构如图

7-1所示。

图7-1 使用backbone的网络计算

过程

从图7-1可以看出，网络

的计算过程是先把一句

自然语言输入

backbone网络中进

行特征抽取，特征是一个

向量，再把特征向量输

入

下游任务模型中进行计

算，得出最终业务需要的

结果。

对于应用了预训练

的backbone的网络，训练时可以选

择继续训

练backbone层，也可以不

训练backbone层，因为backbone的参数

量往

往非常巨大。如果要对backbone进

行再训练，则往往会消耗

掉

更多的计算资源；如果

不对backbone进行再训练而模型

的性能已经

达到业务需

求，也可以选择节省这些

计算资源，在本章中将演

示这种

训练方法。

7.4 实现代

码

7.4.1

准备数据集

1.使用编码

工具

首先需要加载编码

工具，编码工具能够把抽

象的文字转换成数

字，便

于神经网络的后续处理

，代码如下：

#第7章/加载编码

工具

from

transformers import BertTokenizer

token = BertTokenizer.from_pretrained('bert-base-chinese')

token

这里加载的编码工

具为bert-base-chinese，编码工具和预训练

模 型 往 往 是

成 对 使 用 的

， 后

续 将 使 用 同 名

的 预 训

练 模 型 作

为

backbone，运行结果如

下：

 PreTrainedTokenizer(name_or_path='bert-base-chinese',

vocab_size=21128,

model_max_len=512,

is_fast=False, padding_side='right',

truncation_side='right',

special_tokens={'unk_token': '[UNK]', 'sep_token':

'[SEP]',

'pad_token': '[PAD]',

'cls_token': '[CLS]', 'mask_token':

'[MASK]'})

从输出可以看出，bert-base-chinese模型

的字典中有21128个

词，编码器

编码句子的最大长度为

512个词，并且能看到bert-base￾chinese模型所

使用的一些特殊符号。

加

载编码工具之后，不妨进

行一次试算，以便更清晰

地观察到编

码工具的输

入和输出，代码如下：

#第7章

/试编码句子

out = token.batch_encode_plus(

batch_text_or_text_pairs=['从明天起，做

一个幸福的人。', '喂马，劈柴

，

周游世界。'],

truncation=True,

padding='max_length',

max_length=17,

return_tensors='pt',

return_length=True)

#查看编码输出

for

k, v in out.items():

print(k, v.shape)

#把编码还原为句子

print(token.decode(out['input_ids'][0]))

在这

段代码中，让编码工具试

编码了两个句子，编码工

具工作的

方法和编码时

各个参数的含义已在“编

码工具”一章有详细解释

，此

处不再赘述，如果读者

对这部分的内容不理解

，则可参考“编码工

具”一章

。

从上面的代码中的参数

max_length=17的说明可以看出，经过编

码之后的句子一定是确

定的17个词的长度。如果超

出，则会被截断；

如果不足

，则会被补充PAD。运行结果如

下：

input_ids torch.Size([2, 17])

token_type_ids

torch.Size([2, 17])

length torch.Size([2])

attention_mask torch.Size([2,

17])

[CLS] 从 明 天 起，做

一 个 幸 福

的 人。 [SEP]

[PAD] [PAD]

可 以 看 到

， 编 码 的 结

果 确

实 都 是 确 定 的

长 度

， 即 参 数 中

max_length=17个词的长度。编

码结果见表7-2。

表7-2 编码结果

示意

从表7-2可以看出，编码

工具首先对原句子进行

了分词，把一条

完整的句

子切割成了一个一个的

词，对于不同的编码工具

，分词的结

果不一定一致

。在bert-base-chinese这个具体的编码工具

中，则是

以字为词，即把每

个字都作为一个词进行

处理。

这些编码的结果对

于预训练模型的计算十

分重要，后续将使用编

码

器把所有的句子编码，便

于输入预训练模型进行

计算。

2.定义数据集

本次任

务为情感分类任务，所以

需要一个情感分类数据

集进行模

型的训练和测

试，此处加载ChnSentiCorp数据集，代码

如下：

#第7章/定义数据集

import torch

from datasets import

load_from_disk

class Dataset(torch.utils.data.Dataset):

def __init__(self, split):

self.dataset = load_from_disk('./data/ChnSentiCorp')[split]

def __len__(self):

return

len(self.dataset)

def __getitem__(self, i):

text =

self.dataset[i]['text']

label = self.dataset[i]['label']

return text,

label

dataset = Dataset('train')

len(dataset), dataset[20]

在

这段代码中，加载了ChnSentiCorp数据

集，并使用PyTorch的

Dataset对象进行封

装，在__getitem__()函数中定义了每条

数据，包

括text和label两个字段，最

后初始化训练数据集，并

查看训练数据集

的长度

和一条数据样例。运行结

果如下：

(9600, ('非常不错，服务很

好，位于市中心区，交通方

便，不过价格也高！',

1))

可见训

练数据集包括9600条数据，每

条数据包括一条评论文

本和

一个标识，表明这是

一条好评还是差评。值得

注意的是，此处的数据

依

然是文本数据，还没有被

编码器编码。

3.定义计算设

备

对于大多数的神经网

络计算来讲，在CUDA计算平台

上进行计算

比在CPU上要快

。由于本章使用PyTorch框架进行

计算，而PyTorch

支持使用NVIDIA的CUDA计算

平台，所以如果环境中存

在CUDA计算

设备，则可使用CUDA计

算设备进行计算，这可以

极大地加速模型的

训练

和测试过程。代码如下：

#第

7章/定义计算设备

device

= 'cpu'

if torch.cuda.is_available():

device =

'CUDA'

device

这段代

码判断了环境中是否存

在支持CUDA的计算设备，这可

能

是一块GPU，也可能是一块

TPU，如果没有找到任何CUDA设备

，则

使用CPU进行计算，运行结

果如下：

'CUDA'

很幸运，在笔者的

环境中存在CUDA设备，所以可

以使用该设备

加速训练

的过程，如果读者的环境

中没有该设备也不用担

心，使用

CPU也可以计算，只是

时间可能稍长。

4.定义数据

整理函数

之前在定义数

据集时可以看到，数据集

中的每条数据依然是抽

象

的文本数据，还没有经

过编码工具的编码，而预

训练模型需要编码之

后

的数据才能计算，所以需

要一个把文本句子编码

的过程。

另一方面，在训练

模型时数据集往往很大

，如果一条一条地处理

效

率太低，现实中往往一批

一批地处理数据，能够更

快速地处理数

据，同时从

梯度下降角度来讲，批数

据的梯度方差小（相对于

一条数

据来讲），能让模型

更稳定地更新参数。

综上

所述，需要定义一个数据

整理函数，它具有批量编

码一批文

本数据的功能

。代码如下：

#第7章/数据整理

函数

def collate_fn(data):

sents =

[i[0] for i in data]

labels

= [i[1] for i in data]

#编码

data = token.batch_encode_plus(batch_text_or_text_pairs=sents,

truncation=True,

padding='max_length',

max_length=500,

return_tensors='pt',

return_length=True)

#input_ids：编码之后的数

字

#attention_mask：补零的位置是0, 其他位

置是1

input_ids = data['input_ids']

attention_mask = data['attention_mask']

token_type_ids = data['token_type_ids']

labels = torch.LongTensor(labels)

#把数据移动到计算

设备上

input_ids = input_ids.to(device)

attention_mask =

attention_mask.to(device)

token_type_ids = token_type_ids.to(device)

labels =

labels.to(device)

return input_ids, attention_mask, token_type_ids, labels

在这段代码中，入

参的data表示一批数据，取出

其中的句子和标

识，分别

为两个list，代码中命名为sents和

labels。

使用编码工具编码这一

批句子，在参数中将编码

后的结果指定为

确定的

500个词，超过500个词的句子将

被截断，而不足500个词的句

子将被补充PAD，直到500个词。

在

编 码

时 ， 通 过 参 数

return_tensors='pt' 让 编 码

的 结 果

为

PyTorch的Tensor格式，这免去

了后续转换数据格式的

麻烦。

之后取出编码的结

果，并把labels也转换为PyTorch的Tensor格

式

，再把它们都移动到之前

定义好的计算设备上，最

后把这些数据全

部返回

，至此数据整理函数的工

作完毕。

定义好了数据整

理函数，不妨假定一批数

据，让数据整理函数进

行

试算，以观察数据整理函

数的输入和输出，代码如

下：

#第7章/数据整理函数试

算

#模拟一批数据

data = [

('你站在

桥上看风景', 1),

('看风景的人

在楼上看你', 0),

('明月装饰了

你的窗子', 1),

('你装饰了别人

的梦', 0),

]

#试算

input_ids, attention_mask,

token_type_ids, labels =

collate_fn(data)

input_ids.shape, attention_mask.shape,

token_type_ids.shape,

labels

在这段代码中

先虚拟了一批数据，这批

数据中包括4个句子，输

入

数据整理函数后，运行结

果如下：

(torch.Size([4, 500]),

torch.Size([4, 500]),

torch.Size([4, 500]),

tensor([1, 0,

1, 0], device='CUDA:0'))

可见编码之后的

结果都是确定的500个词，并

且每个结果都被移

动到

可用的计算设备上，这方

便了后续的计算。

5.定义数

据集加载器

定义了数据

集和数据整理函数之后

，可以定义数据集加载器

，它

能使用数据整理函数

来成批地处理数据集中

的数据，代码如下：

#第7章/数

据集加载器

loader = torch.utils.data.DataLoader(dataset=dataset,

batch_size=16,

collate_fn=collate_fn,

shuffle=True,

drop_last=True)

len(loader)

在这段代码

中，使用PyTorch提供的工具类定

义数据集加载器，

下面对

数据集加载器的各个参

数进行说明。

(1)参数dataset=dataset：表示要

加载的数据集，此处使用

了之

前定义好的训练数

据集，所以此处的加载器

为训练数据集加载器，区

别于测试数据集加载器

。

(2)参数batch_size=16：表示每个批次中包

括16条数据。

(3)参数collate_fn=collate_fn：表示要使

用的数据整理函数，这

里

使用了之前定义好的数

据整理函数。

(4)参数shuffle=True：表示打

乱各个批次之间的顺序

，让数据更

加随机。

(5)参数drop_last=True：表

示当剩余的数据不足16条

时，丢弃

这些尾数。

在代码

的最后还输出了这个加

载器一共有多少个批次

，运行结果

如下：

600

可见训练

数据集加载器一共有600个

批次。

定义好了数据集加

载器之后，可以查看一批

数据样例，代码如

下：

#第7章

/查看数据样例

for

i, (input_ids, attention_mask, token_type_ids,

labels) in

enumerate(loader):

break

input_ids.shape, attention_mask.shape, token_type_ids.shape,

labels

运行结果

如下：

(torch.Size([16, 500]),

torch.Size([16, 500]),

torch.Size([16,

500]),

tensor([0, 0, 1, 0, 0,

0, 1, 1, 0, 0, 1,

1, 0, 1, 1, 1],

device='CUDA:0'))

这个结果其实就是

数据整理函数的计算结

果，只是句子的数量更

多

。

7.4.2 定义模型

1.加载预训练模

型

完成以上准备工作，现

在数据的结构已经准备

好，可以输入模型

进行计

算了，即可以加载预训练

模型了，代码如下：

#第7章/加

载预训练模型

from transformers import BertModel

pretrained = BertModel.from_pretrained('bert-base-chinese')

#统计参数

量

sum(i.numel() for

i in pretrained.parameters()) / 10000

此处加载的模型为bert-base-chinese模

型，和编码工具的名字

一

致，注意模型和其编码工

具往往配套使用。对于本

章中的中文情感

分类任

务而言，这个模型不是唯

一的选择，如果想试试其

他的模型，

则应选择一个

支持中文的模型。

在代码

的最后，输出了模型的参

数量，运行结果如下：

10226.7648

可见

bert-base-chinese模型的参数量约为1亿个

。这个模型的体

量是比较

大的。

由于bert-base-chinese模型的体量较

大，如果要训练它，对计

算

资源的要求较高，而对于

本次的任务（二分类任务

）来讲，则可以

选择不训练

它，只是作为一个特征提

取器。这样便避免了训练

这个笨

重的模型，节约了

计算的资源和时间，而要

做到这一点，需要冻结

bert-base-chinese模

型的参数，不计算它的梯

度，进而不更新它的

参数

，代码如下：

#第7章/不训练预

训练模型，不需要计算梯

度

for param in pretrained.parameters():

param.requires_grad_(False)

通过这段代码即可冻

结bert-base-chinese模型的参数。

定义好预

训练模型之后，可以进行

一次试算，观察模型的输

入和

输出，代码如下：

#第7章

/预训练模型试算

#设定计

算设备

pretrained.to(device)

#模型试算

out = pretrained(input_ids=input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids)

out.last_hidden_state.shape

在这段

代码中，首先把预训练模

型移动到计算设备上，如

果模型

和数据不在同一

个设备上，则无法计算。对

于笔者的运行环境来讲

，

它们都会被移动到一个

CUDA设备上。

之后把之前得到

的样例数据输入预训练

模型中，得到的计算结果

为一个BaseModelOutputWithPoolingAndCrossAttentions对象，

其中包括last_hidden_state和

pooler_output两个字段，此处只关心

last_hidden_state字

段，取出该字段并输出其

形状，运行结果如下：

torch.Size([16, 500, 768])

样例

数据为16句话的编码结果

，从预训练模型的计算结

果可以看

出，这也是16句话

的结果，每句话包括500个词

，每个词被抽成一个

768维的

向量。到此为止，通过预训

练模型成功地把16句话转

换为一

个特征向量矩阵

，可以接入下游任务模型

做分类或者回归任务。

2.定

义下游任务模型

完成以

上工作，现在可以定义下

游任务模型了。下游任务

模型的

任务是对backbone抽取的

特征进行进一步计算，得

到符合业务需求

的计算

结果。对于本章的任务来

讲，需要计算一个二分类

的结果，和

数据集中真实

的label保持一致，代码如下：

#第

7章/定义下游任务模型

class Model(torch.nn.Module):

def

__init__(self):

super().__init__()

self.fc = torch.nn.Linear(768, 2)

def forward(self, input_ids, attention_mask, token_type_ids):

#使

用预训练模型抽取数据

特征

with torch.no_grad():

out = pretrained(input_ids=input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids)

#对抽取的特征只取

第1个字的结果做分类即

可

out = self.fc(out.last_hidden_state[:, 0])

out = out.Softmax(dim=1)

return out

model

= Model()

#设定计算设备

model.to(device)

#试算

model(input_ids=input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids).shape

在

这段代码中，定义了下游

任务模型，该模型只包括

一个全连接

的线性神经

网络，权重矩阵为768×2，所以它

能够把一个768维度的

向量

转换到二维空间中。

下游

任务模型的计算过程为

，获取了一批数据之后，使

用

backbone将这批数据抽取成特

征矩阵，抽取的特征矩阵

的形状应该

是16×500×768，这在之前

预训练模型的试算中已

经看到。这3个维

度分别代

表了16句话、500个词、768维度的特

征向量。

之后下游任务模

型丢弃了499个词的特征，只

取得第1个词（索引

为0）的特

征向量，对应编码结果中

的[CLS]，把特征向量矩阵变成

了

16×768。相当于把每句话变成

了一个768维度的向量。

注意

：之所以只取了第0个词的

特征做后续的判断计算

，这和预

训练模型BERT的训练

方法有关系，具体可见“手

动实现BERT”章。

之后再使用自

己的全连接线性神经网

络把16×768特征矩阵转换

到16×2，即

为要求的二分类结果。

在

代码的最后对该模型进

行试算，运行结果如下：

torch.Size([16,

2])

可

见，这就是要求的16句话的

二分类的结果。

7.4.3 训练和测

试

1.训练

模型定义之后，接

下来就可以对模型进行

训练了，代码如下：

在这段

代码中，首先定义了优化

器、loss计算函数、学习率调节

器，其中优化器使用了HuggingFace提

供的AdamW优化器，这是传

统的

Adam优化器的改进版本，在自

然语言处理任务中，该优

化器往

往能取得比Adam优化

器更好的成绩，并且计算

效率更高。

学习率调节器

也使用了HuggingFace提供的线性学

习率调节器，

它能在训练

的过程中，让学习率缓慢

地下降，而不是使用始终

如一的

学习率，因为在训

练的后期阶段，需要更小

的学习率来微调参数，这

有利于loss下降到更低的点

。

由于本章的任务为分类

任务，所以使用的loss计算函

数为

CrossEntropyLoss，即交叉熵计算函数

。

之后把下游任务模型切

换到训练模式，即可开始

训练。训练的过

程为不断

地从数据集加载器中获

取一批一批的数据，让模

型进行计

算，用模型计算

的结果和真实的labels计算loss，根

据loss计算模型中

所有参数

的梯度，并执行梯度下降

优化参数。

最后，每优化10次

模型参数，就计算一次当

前模型预测结果的正

确

率，并输出模型的loss和优化

器的学习率，最终训练完

毕后，输出

的观察数据见

表7-3。

从表7-3可以看出，在训练

到大约200个steps时，模型已经能

够

达到大约85%的正确率，并

且能够观察到loss是随着训

练的进程在不

断地下降

，学习率也如预期的一样

，也在缓慢地下降。

表7-3 训练

过程输出

2.测试

对训练好

的模型进行测试，以验证

训练的有效性，代码如下

：

在这段代码中，首先定义

了测试数据集和加载器

，并取出5个批

次的数据让

模型进行预测，最后统计

正确率并输出，运行结果

如下：

0.875

最终模型取得了87.5%的

正确率，这个正确率虽然

不是很高，但

验证了下游

任务模型，即使在不训练

backbone的情况下也能达到一

定

的成绩，如果这个程序已

经能满足业务要求，则可

以免去对

backbone的训练。

7.5

小结

本

章通过一个情感分类的

例子讲解了使用BERT预训练

模型抽取文

本特征数据

的方法，使用BERT作用backbone，相对于

传统的RNN而

言计算量会大

一些，但BERT抽取的信息更完

整，更容易被下游任务模

型总结出统计规律，所以

在使用BERT作为backbone时可以适当

地减

少下游任务模型的

训练量。此外，由于使用的

BERT模型是预训练的，

所以可

以不对其进行训练，这大

大节约了计算量，同时也

能取得不错

的效果。

第8章

实战任务2：中文填空

8.1 任务

简介

人类在阅读一个句

子时，即使挖掉句子中的

一两个词，往往也能

根据

上下文猜出被挖掉的是

什么词，这被称作填空任

务，例如如下是

一道典型

的填空题：

“外观很漂亮，特

别____合女孩子使用。”

人类很

容易就能猜出横线处应

该填写“适”字，这样才能符

合上

下文的语义，而人类

是通过从小到大每天的

听、说、读、写交流获得

这样

的普遍性知识的。自然语

言虽然复杂，但却有着明

显的统计规

律，而神经网

络最擅长的就是找出统

计规律，所以本章将尝试

使用预

训练神经网络完

成填空任务。

8.2 数据集介绍

本章所使用的数据集依

然是ChnSentiCorp数据集，这是一个情

感分类数据集，每条数据

中包括一句购物评价，以

及一个标识，由于

本章的

任务为填空任务，所以只

需文本就可以了，不需要

分类标识。

在数据处理的

过程中，会把每句话的第

15个词挖掉，也就是替换

成

特殊符号[MASK]，并且每句话会

被截断成固定的30个词的

长度，

神经网络的任务就

是根据每句话的上下文

，把第15个词预测出来。

本次

任务的部分数据样例见

表8-1，通过该表读者可对本

次任务

数据集有直观的

认识。

表8-1 ChnSentiCorp数据集数据样例

续表

8.3

模型架构

在本次任

务中，依然将一个预训练

的BERT模型当作backbone网

络层使用

，使用该backbone来抽取文本数据

特征，后续接入下游任

务

模型来把抽取的数据特

征还原为任务需要的答

案。由于填空任务的

答案

可能是词表中的任何一

个词，所以这可以视为一

个多分类任务，

分类的数

目为整个词表的词数量

。

本 次 任 务 的 计

算 流 程 如

图 8-1 所

示 ， 首 先 把 文

本 数 据

输 入

backbone网络抽取数据特征

，再把数据特征输入下游

任务模型进行

计算，下游

任务模型将把数据特征

投影到全体词表空间，即

可得出最

终的预测词。

图

8-1 使用backbone的网络计算过程

在

本次任务中，将忽略对backbone的

训练，只是将backbone当

作一个数

据特征抽取层使用。在训

练过程中，只训练下游任

务模型，

这将节约宝贵的

计算资源，但会降低预测

正确率。如果读者对预测

正

确率有较高要求，则可

以连同backbone共同参与训练，能

有效地提

高预测正确率

，但需要更多的训练时间

和训练数据。

8.4 实现代码

8.4.1 准

备数据集

1.使用编码工具

首先需要加载编码工具

，编码工具能够把抽象的

文字转换成数

字，便于神

经网络的后续处理，本章

使用的编码工具依然是

bert￾base-chinese编码工具，这个编码工具

在“实战任务1：中文情感分

类”一章中已经详细介绍

了，此处不再赘述，仅给出

代码，代码如

下：

#第8章/加载

编码工具

from transformers

import BertTokenizer

token = BertTokenizer.from_pretrained('bert-base-chinese')

token

运行结果如下

：

 PreTrainedTokenizer(name_or_path='bert-base-chinese',

vocab_size=21128,

model_max_len=512, is_fast=False,

padding_side='right',

truncation_side='right',

special_tokens={'unk_token':'[UNK]', 'sep_token':'[SEP]',

'pad_token':'[PAD]',

'cls_token':

'[CLS]', 'mask_token': '[MASK]'})

加载编码工具之后，不妨

进行一次试算，以便更清

晰地观察编码

工具的输

入和输出，代码如下：

#第8章

/试编码句子

out = token.batch_encode_plus(

batch_text_or_text_pairs=['轻轻地我走

了，正如我轻轻地来。', '我轻

轻地

招手，作

别西天的云

彩。'],

truncation=True,

padding='max_length',

max_length=18,

return_tensors='pt',

return_length=True)

#查看编码输出

for k, v in

out.items():

print(k, v.shape)

#把编码

还原为句子

print(token.decode(out['input_ids'][0]))

在这段代码

中，让编码工具试编码了

两个句子，运行结果如下

：

input_ids torch.Size([2, 18])

token_type_ids torch.Size([2, 18])

length torch.Size([2])

attention_mask torch.Size([2, 18])

[CLS]

轻 轻 地 我 走 了，正

如 我 轻

轻 地 来。 [SEP]

[PAD]

编码工具工作的

方法和编码时各个参数

的含义及编码结果在“编

码工具”一章已有详细解

读，此处不再赘述，如果读

者对编码结果还

不理解

，则可以参考“编码工具”一

章。

2.定义数据集

在本次任

务中，依然将使用ChnSentiCorp数据集

，但需要对数

据集进行一

些操作，将它变成一个填

空任务数据集。在开始处

理之

前，首先需要加载数

据集，代码如下：

#第8章/加载

数据集

from datasets import load_from_disk

dataset = load_from_disk('./data/ChnSentiCorp')

dataset

在这段代码中，加

载了ChnSentiCorp数据集，运行结果如

下：

DatasetDict({

train: Dataset({

features: ['text', 'label'],

num_rows:

9600

})

validation: Dataset({

features: ['text',

'label'],

num_rows: 0

})

test: Dataset({

features: ['text', 'label'],

num_rows: 1200

})

})

可见训练数据集包括

9600条数据，每条数据中包括

两个字段，分

别为text和label。由于

本章要做的任务是填空

任务，所以并不需要

label字段

，后续将把这个字段丢弃

，并建立真正需要的label。

有了

文本数据之后，接下来需

要对这些文本数据进行

编码，便于

后续的处理，代

码如下：

#第8章/编码数据，同

时删除多余的字段

def f(data):

return

token.batch_encode_plus(batch_text_or_text_pairs=data['text'],

truncation=True,

padding='max_length',

max_length=30,

return_length=True)

dataset = dataset.map(function=f,

batched=True,

batch_size=1000,

num_proc=4,

remove_columns=['text', 'label'])

dataset

在这

段代码中，使用了之前加

载的编码工具，对数据集

中的text

字段进行了编码，编

码的结果同之前编码器

的试算结果一致。

(1)参数truncation=True和

max_length=30意味着编码结果的长

度

不会长于30个词，超出30个词

的部分会被截断。

(2)参数padding='max_length'表

明不足30个词的句子会被

补充

PAD，直到达到30个词的长

度。

(3)参数return_length=True会让编码结果中

多出一个length字

段，表明这段

数据的长度，由于PAD不会被

计算在长度内，所以

length一定

小于或等于30，这个字段方

便了后续的数据过滤。

在

数据集上调用map()函数时使

用了批处理加速，每1000条数

据

为一个批次调用一次

编码函数，关于数据集的

批处理加速在“数据

集”一

章已经详细介绍，如果读

者对此感到困惑，则可以

参考“数据

集”一章。

调 用 map() 函

数 时

还 指 定 了 参 数

remove_columns=['text',

'label']：表示

丢弃原数据中的text和label数据

，只需编码的结果。

以上代

码的运行结果如下：

DatasetDict({

train: Dataset({

features: ['input_ids', 'token_type_ids', 'length',

'attention_mask'],

num_rows:

9600

})

validation: Dataset({

features: [],

num_rows: 0

})

test: Dataset({

features:

['input_ids', 'token_type_ids', 'length',

'attention_mask'],

num_rows: 1200

})

})

由于

编码结果和原句子是一

一对应的关系，并不会导

致数据的增

加或者减少

，所以数据的数量没有变

化，但是每条数据的字段

都变化

了，原本的text和label字段

被丢弃，取而代之的是编

码器编码的结

果。

在编码

的过程中，把所有长于30个

词的句子都截断了，现在

所有

的句子的长度都小

于或等于30个词了。接下来

要把所有小于30个词的

句

子丢弃，确保所有输入模

型训练的句子都刚好30个

词，由于在编码

过程中让

编码器返回了每句话的

长度，所以很容易完成这

个过滤，代

码如下：

#第8章/过

滤掉太短的句子

def f(data):

return [i >= 30

for i in data['length']]

dataset =

dataset.filter(function=f, batched=True,

batch_size=1000,

num_proc=4)

dataset

在这段

代码中，以每句话的长度

来过滤数据，把长度少于

30个词

的句子丢弃，在filter()函数

中使用的各个参数的意

思和map()中的相

同，运行结果

如下：

DatasetDict({

train: Dataset({

features:

['input_ids', 'token_type_ids', 'length',

'attention_mask'],

num_rows: 9286

})

validation: Dataset({

features: [],

num_rows:

0

})

test: Dataset({

features: ['input_ids',

'token_type_ids', 'length',

'attention_mask'],

num_rows: 1157

})

})

可以看到在训练集

中少了314条数据，在测试集

中少了43条，这

个数据损失

的量在可接受的范围内

。以此为代价，现在所有数

据的长

度都是30个词了，这

方便了后续的数据处理

工作。

3.定义计算设备

关于

计算设备在“实战任务1：中

文情感分类”一章中已经

详细

介绍，此处不再赘述

，仅给出代码，代码如下：

#第

8章/定义计算设备

device = 'cpu'

if

torch.cuda.is_available():

device = 'CUDA'

device

运行结

果如下：

'CUDA'

4.定义数据整理函

数

本次的任务为填空任

务，现在的数据中每句话

都是由30个词组成

的，所以

可以把每句话的第15个词

挖出作为label，也就是网络模

型预

测的目标，为了防止

网络直接从原句子中读

取答案，把每句话的第15

个

词替换为[MASK]。相当于在需要

网络模型填答案的位置

画横线，

同时擦除正确答

案。网络模型需要根据[MASK]的

上下文把[MASK]处

原本的词预

测出来。

上述工作将在数

据整理函数中完成，数据

整理函数还有把多条数

据合并为一个批次的功

能。使用批量数据训练不

仅能提高数据处理的

速

度，节约训练、测试的时间

，还能让loss的梯度更平稳，让

模型参

数更稳定地更新

。

在本章中使用的数据整

理函数的代码如下：

#第8章

/数据整理函数

def collate_fn(data):

#取出编码

结果

input_ids =

[i['input_ids'] for i in data]

attention_mask

= [i['attention_mask'] for i in data]

token_type_ids = [i['token_type_ids'] for i in

data]

#转换为Tensor格式

input_ids = torch.LongTensor(input_ids)

attention_mask

= torch.LongTensor(attention_mask)

token_type_ids = torch.LongTensor(token_type_ids)

#把第15个

词替换为MASK

labels = input_ids[:, 15].reshape(-1).clone()

input_ids[:, 15]

= token.get_vocab()[token.mask_token]

#移动到计算设

备

input_ids = input_ids.to(device)

attention_mask = attention_mask.to(device)

token_type_ids = token_type_ids.to(device)

labels = labels.to(device)

return input_ids, attention_mask,

token_type_ids, labels、

在这段代码中，入参的

data表示一批数据，其中的内

容为编码工

具编码的结

果。

由于编码时并未指定

返回PyTorch的Tensor格式数据，所以在

数

据整理函数中把数据

整理为Tensor格式，整理成Tensor格式

后，数据

的表现形式为b×30的

矩阵，其中b表示batch size，这是由数

据集加

载器确定的批次

大小。

接下来把input_ids矩阵中的

第15个字克隆一份，定义为

labels，

也就是网络模型要预测

的目标，并把input_ids矩阵中的第

15个字替换

为[MASK]，相当于从题

目中擦除答案，画上横线

。

接下来把3个矩阵移动到

之前定义好的计算设备

上，方便后续的

模型计算

。

定义好了数据整理函数

，不妨假定一批数据，让数

据整理函数进

行试算，以

观察数据整理函数的输

入和输出，代码如下：

#第8章

/数据整理函数试算

#模拟

一批数据

data = [{

'input_ids': [

101,

2769, 3221, 3791, 6427, 1159, 2110,

5442, 117, 2110, 749,

8409,

702,

6440, 3198, 4638, 1159, 5277, 4408,

119, 1728, 711, 2769,

3221,

5439,

2399, 782, 117, 3791, 102

],

'token_type_ids': [0] * 30,

'attention_mask': [1]

* 30

}, {

'input_ids': [

101, 679, 7231, 8024, 2376, 3301,

1351, 6848, 4638, 8024, 3301,

1351,

3683, 6772, 4007, 2692, 8024, 2218,

3221, 100, 2970, 1366,

2208, 749,

8024, 5445, 684, 1059, 3221, 102

],

'token_type_ids': [0] * 30,

'attention_mask':

[1] * 30

}]

#试算

input_ids,

attention_mask, token_type_ids, labels =

collate_fn(data)

#把编码还

原为句子

print(token.decode(input_ids[0]))

print(token.decode(labels[0]))

input_ids.shape, attention_mask.shape, token_type_ids.shape,

labels

在这段代码中

先虚拟了一批数据，这批

数据中包括两个句子，输

入数据整理函数后，运行

结果如下：

 [CLS] 我 是 法

语 初 学

者，学 了 78 个

课 时 [MASK] 初 级 班.

因

为 我 是 老 年 人，

法 [SEP] 的

(torch.Size([2, 30]),

torch.Size([2,

30]),

torch.Size([2, 30]),

tensor([4638, 2692], device='CUDA:0'))

可以

看到第一句话的[MASK]处应该

填写“的”字，这也比较符

合

自然语义。此外可以看到

编码之后的结果都是确

定的30个词，并且

每个结果

都被移动到了可用的计

算设备上，这方便了后续

的计算。

5.定义数据集加载

器

关于数据集加载器在

第7章中已经详细介绍，此

处不再赘述，仅

给出代码

，代码如下：

#第8章/定义数据

集加载器

loader = torch.utils.data.DataLoader(dataset=dataset['train'],

batch_size=16,

collate_fn=collate_fn,

shuffle=True,

drop_last=True)

len(loader)

运行结果如下

：

580

可见训练数据集加载器

一共加载了580个批次。

定义

好了数据集加载器之后

，可以查看一批数据样例

，代码如

下：

#第8章/查看数据

样例

for i, (input_ids,

attention_mask, token_type_ids,

labels) in enumerate(loader):

break

print(token.decode(input_ids[0]))

print(token.decode(labels[0]))

input_ids.shape, attention_mask.shape, token_type_ids.shape,

labels

运行结果如下：

 [CLS] 位 于

友 谊

路 金 融 街，找 不 到

吃

饭 [MASK] 地 方。

酒 店

刚 刚 装 修 好

，有

点

[SEP] 的

(torch.Size([16, 30]),

torch.Size([16, 30]),

torch.Size([16, 30]),

tensor([4638, 6230, 511, 7313,

3221, 7315, 6820, 6858, 7564,

3211,

1690,

3315, 3300, 172, 6821, 1126],

device='CUDA:0'))

这段代码把一批

数据中的第1条还原为了

文本形式，便于人类观

察

，可以看到这段文本的[MASK]处

应该填写“的”字，这比较符

合

自然语义。

样例数据的

结果其实就是数据整理

函数的计算结果，只是句

子的

数量更多。

8.4.2 定义模型

1.加载预训练模型

关于预

训练模型在第7章中已经

详细介绍，此处不再赘述

，仅给

出代码，代码如下：

#第

8章/加载预训练模型

from

transformers import BertModel

pretrained = BertModel.from_pretrained('bert-base-chinese')

#统计

参数量

sum(i.numel() for i in pretrained.parameters())

/ 10000

在代码的最后，输

出了模型的参数量，运行

结果如下：

10226.7648

可见bert-base-chinese模型的参

数量约为1亿个，在本次任

务中

选择不训练它，代码

如下：

#第8章/不训练预训练

模型，不需要计算梯度

for param in pretrained.parameters():

param.requires_grad_(False)

定

义好预训练模型之后，可

以进行一次试算，代码如

下：

#第8章/预训练模型试算

#设定计算设备

pretrained.to(device)

#模型试算

out = pretrained(input_ids=input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids)

out.last_hidden_state.shape

运行结果如下：

torch.Size([16, 30,

768])

此处输入

的数据就是之前看到的

样例数据，从预训练模型

的计算

结果可以看出，这

也是16句话的结果，每句话

包括30个词，每个词被

抽成

了一个768维的向量。到此为

止，通过预训练模型成功

地把16句

话转换为一个特

征向量矩阵，可以接入下

游任务模型做分类或者

回归

任务。

2.定义下游任务

模型

完成以上工作后，现

在可以定义下游任务模

型了，下游任务模型

的任

务是对backbone抽取的特征进行

下一步计算，得到符合业

务需

求的计算结果，对于

本章的任务来讲，需要计

算一个多分类的结果，

类

别的数目等于整个词表

的词数量，模型理想的计

算结果为数据集中

的label字

段，代码如下：

在这段代码

中，定义了下游任务模型

，该模型只包括一个全连

接

的线性神经网络，权重

矩阵为768×21128，所以它能够把一

个768维

度的向量转换到21128维

空间中。21128这个数字来自编

码器的字典

空间，它是编

码器所认识的字的数量

，所以可以理解为下游任

务模型

可以把backbone抽取的数

据特征还原为字典中的

任何一个字。

下

游 任 务 模

型 的 计

算 过 程 为 ， 获

取 一

批 数 据 之 后

， 使 用

backbone将这批

数据抽取成特征矩阵，抽

取的特征矩阵的形状应

该

是16×30×768，这在之前预训练模

型的试算中已经看到。这

3个维度

分别代表了16句话

、30个词、768维度的特征向量。

在

本次的填空任务中，填空

处固定出现在每句话的

第15个词的位

置，所以只取

出每句话的第15个词的特

征，再尝试把这个词的特

征投

影到全体词表空间

中，即还原为词典中的某

个词。

在投影到全体词表

空间中时，由于768×21128是一个很

大的矩

阵，如果直接计算

，则很容易导致过拟合，所

以对backbone抽取的

数据特征要

接入一个DropOut网络，把其中的

数据以一定的概率置为

0，防止网络的过拟合。

在代

码的最后对该模型进行

了试算，运行结果如下：

torch.Size([16, 21128])

可

见，预测结果为16句的填空

结果，如果在该结果上再

套用

Softmax()函数，则为在全体词

表中每个词的概率。

注意

：在此处的计算后不建议

再套用Softmax作为激活函数，因

为分类的结果比较多，导

致每个类别分到的概率

都非常低，套用

Softmax后大多数

类别的概率将非常接近

0，这在计算参数梯度时会

出

现问题，也就是出现了

梯度消失的情况，这不利

于模型的训练和收

敛，所

以不建议在计算过程中

套用Softmax。

8.4.3 训练和测试

1.训练

模

型定义之后，接下来就可

以对模型进行训练了，代

码如下：

在这段代码中，首

先定义了优化器、loss计算函

数、学习率调节

器，其中优

化器使用了HuggingFace提供的AdamW优化

器，这是传

统的Adam优化器的

改进版本，在自然语言处

理任务中，该优化器往

往

能取得比Adam优化器更好的

成绩，并且计算效率更高

。

由于本次的下游任务模

型中包含了一个比较大

的权重矩阵参数，

形状为

768×21128，它很可能导致过拟合，所

以在优化过程中加入权

重参数二范数衰减，使用

AdamW参数要做到这一点非常

简单，在定

义时AdamW指定参数

weight_decay即可，在本章代码中这个

参数等

于1.0，权重衰减的生

效原理如下：

从 式 (8-1)

可 以 看

出 ， 权 重

衰 减 即 为 在 loss

的 基

础 上 加 上

weight_decay倍的权重的二

范数，二范数的计算公式

如下：

从式(8-2)可以看出，二范

数衡量了一组数绝对值

的大小，在loss

中加入权重矩

阵的二范数后能够约束

权重矩阵中数字偏离0的

绝对

值，能够防止绝对值

太大的权重出现，进而防

止模型的过拟合。

学习率

调节器也使用了HuggingFace提供的

线性学习率调节器，

它能

在训练的过程中，让学习

率缓慢地下降，而不是使

用始终如一的

学习率，因

为在训练的后期，需要更

小的学习率来微调参数

，这有利

于loss下降到更低的

点。

由于本章的任务为分

类任务，所以使用的loss计算

函数为

CrossEntropyLoss，即交叉熵计算函

数。

之后把下游任务模型

切换到训练模式，即可开

始训练。训练的过

程为不

断地从数据集加载器中

获取一批一批的数据，让

模型进行计

算，用模型计

算的结果和真实的labels计算

loss，根据loss计算模型中

所有参

数的梯度，并执行梯度下

降优化参数。

最后，每优化

50次模型参数，就计算一次

当前模型预测结果的正

确率，并输出模型的loss和优

化器的学习率，最终训练

完毕后，输出

的观察数据

见表8-2。

表8-2 训练过程输出

续

表

从表8-2可以看出，在全量

数据训练了5个epochs，模型的预

测

正确率在缓慢地上升

，并且能够观察到loss随着训

练的进程在不断地

下降

，学习率也如预期在缓慢

地下降。

2.测试

最后，对训练

好的模型进行测试，以验

证训练的有效性，代码如

下：

在这段代码中，首先定

义了测试数据集和加载

器，并取出5个批

次的数据

让模型进行预测，最后统

计正确率并输出，运行结

果如下：

0.5645833333333333

最终模型取得了

约56.5%正确率的成绩，这个正

确率看起来不

高，但是需

要注意这是一个21128分类的

任务，所以能取得56.5%的

正确

率验证了下游任务模型

，在即使不训练backbone的情况下

也能

取得一定的成绩。如

果连同backbone模型一起训练，则

可以进一步

提高预测的

正确率，感兴趣的读者可

以自行实验。

8.5 小结

本章通

过一个填空的例子讲解

了使用BERT预训练模型抽取

文本特

征数据的方法，事

实上填空任务也是BERT模型

本身在训练时的一个子

任务，所以使用BERT模型在做

填空任务时效果往往较

好，在处理不同

的任务时

，应该选择合适的预训练

模型。

填空任务本身可以

被视为一个多分类任务

，但由于全体词表空间

的

数量比较大，往往有上万

个词，所以是个类别特别

多的多分类任

务，这导致

在输出时很容易过拟合

，本章演示了使用DropOut层来随

机断开部分网络权重和

使用权重参数衰减这两

种方式来缓解过拟合。

分

类的类别太多也容易出

现梯度消失的问题，所以

在下游任务的

输出时不

能使用Softmax函数激活，需要格

外注意。

第9章

实战任务3：中

文句子关系推断

9.1 任务简

介

本章将使用神经网络

判断两个句子是否是连

续的关系，以人类的

角度

来讲，阅读两个句子，很容

易就能判断出这两个句

子是相连的，

还是无关的

，所以在本章中，将尝试让

神经网络来完成这个任

务。

本章依然使用BERT模型作

为backbone，使用BETR预训练模型来

抽

取两个句子的文本特征

，并在文本特征的基础上

做出判断，得出两

个句子

是相连的，还是无关的结

果。BERT模型在本身的训练过

程中，

有一个子任务用于

判断两个句子的关系，所

以使用BERT完成这个任务

非

常合适，本章依然不会对

BERT模型本身进行训练，只是

将BERT模

型作为backbone层使用。完成

本任务，只需训练下游任

务模型。

9.2 数据集介绍

出于

简单起见，本章所使用的

数据集依然是ChnSentiCorp数据

集，对

于本章的任务而言，不需

要数据集中的label字段，只需

文本数

据，在后续的数据

处理过程中，将把文本数

据整理成需要的句子对

的

形式，并且每一对句子

都有一个标识，用于表明

这两个句子是相连的

还

是无关的关系，见表9-1。

表9-1 句

子关系推断数据集样例

续表

9.3 模型架构

与情感分

类和填空任务不同，在这

两个任务中，输入网络模

型的

都是一个一个的句

子，在句子关系推断任务

中，输入网络模型的是一

对一对的句子。本次任务

的计算流程如图9-1所示。

图

9-1 使用backbone的网络计算过程

从

图9-1可以看出，网络的计算

过程是先把两句话同时

输入

backbone网络中进行特征抽

取，特征是一个向量，再把

特征向量输

入下游任务

模型中进行计算，得出两

句话是相连或无关的结

果。

在本章中依然不会训

练backbone层，如果读者对最终预

测的性

能不满足，则可以

通过连同backbone一起训练的方

式提高性能，不

过这需要

更强的计算力，更多的训

练数据，在本章中不涉及

这些内

容。

9.4 实现代码

9.4.1 准备

数据集

1.使用编码工具

首

先需要加载编码工具。编

码工具能够把抽象的文

字转换成数

字，便于神经

网络的后续处理。本章使

用的编码工具依然是bert￾base-chinese编

码工具，这个编码工具在

第7章中已经详细介绍过

，

此处不再赘述，仅给出代

码，代码如下：

#第9章/加载编

码工具

from transformers import BertTokenizer

token

= BertTokenizer.from_pretrained('bert-base-chinese')

token

运行结果如下：

 PreTrainedTokenizer(name_or_path='bert-base-chinese',

vocab_size=21128,

model_max_len=512, is_fast=False, padding_side='right',

truncation_side='right',

special_tokens={'unk_token':

'[UNK]', 'sep_token': '[SEP]',

'pad_token': '[PAD]',

'cls_token':

'[CLS]', 'mask_token': '[MASK]'})

加

载编码工具之后不妨进

行一次试算，以更清晰地

观察编码工具

的输入和

输出，代码如下：

#第9章/试编

码句子

out = token.batch_encode_plus(

batch_text_or_text_pairs=[('不是一切大树，', '都

被风暴折断。'),

('不是一切种

子，',

'都找不到生根的土壤

。')],

truncation=True,

padding='max_length',

max_length=18,

return_tensors='pt',

return_length=True,

)

#查看编码输出

for k, v in

out.items():

print(k, v.shape)

#把编码还

原为句子

print(token.decode(out['input_ids'][0]))

与情感分类和

填空任务不同，这里编码

的是句子对，运行结果如

下：

input_ids torch.Size([2, 18])

token_type_ids torch.Size([2, 18])

length torch.Size([2])

attention_mask torch.Size([2, 18])

[CLS]

不是一切大树， [SEP] 都被风

暴折断。 [SEP] [PAD]

可

以 看 到 ， 编 码

的

结 果 都 是 确 定

的 长 度 ， 为

参 数

中 的

max_length=18个词的长度。编

码结果见表9-2。

表9-2 编码结果

示意

编码工具工作的方

法和编码时各个参数的

含义及编码结果在“编

码

工具”一章已有详细解读

，此处不再赘述，如果读者

对编码结果还

不理解，则

可以参考“编码工具”一章

。

2.定义数据集

定 义 本

次 任

务 所 需 要 的

数 据 集 ， 如 前

所

述 ， 依 然 使 用

ChnSentiCorp数据集中

的文本数据制作，代码如

下：

#第9章/定义数据集

import torch

from datasets

import load_from_disk

import random

class Dataset(torch.utils.data.Dataset):

def __init__(self, split):

dataset = load_from_disk('./data/ChnSentiCorp')[split]

def f(data):

return len(data['text']) > 40

self.dataset = dataset.filter(f)

def __len__(self):

return

len(self.dataset)

def __getitem__(self, i):

text =

self.dataset[i]['text']

#将一

句话切分为前半句和后

半句

sentence1 = text[:20]

sentence2

= text[20:40]

#随机整数，取值为0和

1

label = random.randint(0,

1)

#有一半概率把后半句替

换为无关的句子

if label == 1:

j = random.randint(0, len(self.dataset) - 1)

sentence2 = self.dataset[j]['text'][20:40]

return sentence1, sentence2,

label

dataset = Dataset('train')

sentence1, sentence2,

label = dataset[7]

len(dataset), sentence1, sentence2,

label

在这段

代码中，加载了ChnSentiCorp数据集，并

使用了PyTorch

的Dataset对象进行了封

装，由于本次任务是要判

断两句话是否存在

相连

的关系，如果假设定义每

句话的长度为20个字，则原

句子最短不

能少于40个字

，否则不能被切割成两句

话。

所以在__init__()函数中加载了

ChnSentiCorp数据集后对数据集

进行

过滤，丢弃了数字少于40个

字的句子。

在__getitem__()函数中把原

句切割成了各20个字的两

句话，并且

有一半的概率

把后半句替换为无关的

句子，这样就形成了本次

任务中

需要的数据结构

，即每条数据中包括两句

话，并且这两句话分别有

50%的概率是相连和无关的

关系。

最后初始化训练数

据集，并查看训练数据集

的长度和一条数据样

例

，运行结果如下：

(8001, '地理位置

佳，在市中心。酒店服务好

、早餐品', '种丰富。我住的商

务数码房计算机宽带

速

度满意', 0)

可见，训练数据集

包括8001条数据，每条数据包

括两句话和一个

标识，标

识表明这两句话是相连

还是无关的关系。值得注

意的是，此

处的数据依然

是文本数据，还没有被编

码器编码。

3.定义计算设备

关于计算设备在第7章中

已经详细介绍，此处不再

赘述，仅给出

代码，代码如

下：

#第9章/定义计算设备

device

= 'cpu'

if torch.cuda.is_available():

device =

'CUDA'

device

运

行结果如下：

'CUDA'

4.定义数据整

理函数

定义一个数据整

理函数，它具有批量编码

一批文本数据的功能，

代

码如下：

#第9章/数据整理函

数

def collate_fn(data):

sents =

[i[:2] for i in data]

labels

= [i[2] for i in data]

#编码

data = token.batch_encode_plus(batch_text_or_text_pairs=sents,

truncation=True,

padding='max_length',

max_length=45,

return_tensors='pt',

return_length=True,

add_special_tokens=True)

#input_ids：编码之后的数字

#attention_mask：补零的位置是0, 其他位置

是1

#token_type_ids：第1个句子和特殊符号

的位置是0, 第2个句子的位

置是1

input_ids = data['input_ids'].to(device)

attention_mask

= data['attention_mask'].to(device)

token_type_ids = data['token_type_ids'].to(device)

labels

= torch.LongTensor(labels).to(device)

return input_ids, attention_mask, token_type_ids,

labels

在这段代码中，入参

的data表示一批数据，取出其

中的句子对和

标识，分别

为两个list，其中句子对的list中

为一个一个tuple，每个

tuple中包括

两个句子，即一对句子。

在

制作数据集时已经明确

两个句子各有20个字，但在

经过编码时

每个字并不

一定会被编码成一个词

，此外在编码时还要往句

子中插入

一些特殊符号

，如标识句子开始的[CLS]，标识

一个句子结束的

[SEP]，所以编

码的结果并不能确定为

40个词，因此在编码时需要

留

下一定的容差，让编码

结果中能囊括两个句子

的所有信息，如果有多

余

的位置，则可以以[PAD]填充。

综

上所述，使用编码工具编

码这一批句子对时，在参

数中指定了

编码后的结

果为确定的45个词，超过45个

词的句子将被截断，而不

足

45个词的句子将被补充

PAD，直到45个词。

在 编 码 时 ，

通 过

参 数 return_tensors='pt' 让 编

码 的 结 果 为

PyTorch的

Tensor格式，这免去了后续转换

数据格式的麻烦。

之后取

出编码的结果，并把labels也转

换为PyTorch的Tensor格

式，再把它们都

移动到之前定义好的计

算设备上，最后把这些数

据全

部返回，数据整理函

数的工作完毕。

定义好了

数据整理函数，不妨假定

一批数据，让数据整理函

数进

行试算，以观察数据

整理函数的输入和输出

，代码如下：

#第9章/数据整理

函数试算

#模拟一批数据

data = [('酒店还是非常的不错，我

预定的是套间，服务', '非常

好，随叫随到，

结账非常快

。',

0),

('外观很漂亮，性价比感觉

还不错，功能简', '单，适合出

差携带。蓝牙摄像头都有

了。',

0),

('《穆斯林的葬礼》我已闻

名很久，只是一直没', '怎能

享受4星的服务，连空调都

不能

用的。', 1)]

#试算

input_ids, attention_mask, token_type_ids,

labels =

collate_fn(data)

#把编码还

原为句子

print(token.decode(input_ids[0]))

input_ids.shape,

attention_mask.shape, token_type_ids.shape,

labels

在这段代码中

先虚拟了一批数据，在这

批数据中包括3对句子，

输

入数据整理函数后，运行

结果如下：

[CLS] 酒店还是非常

的不错，我预定的是套间

，服务 [SEP] 非常好，随叫随

到，结

账非常快。 [SEP]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

Out[7]:

(torch.Size([3, 45]),

torch.Size([3, 45]),

torch.Size([3, 45]),

tensor([0, 0, 1], device='CUDA:0'))

可见，编码之后

的结果都是确定的45个词

，并且每个结果都被移

动

到了可用的计算设备上

，这方便了后续的计算。

5.定

义数据集加载器

关于数

据集加载器在第7章中已

经详细介绍过，此处不再

赘述，

仅给出代码，代码如

下：

#第9章/数据集加载器

loader = torch.utils.data.DataLoader(dataset=dataset,

batch_size=8,

collate_fn=collate_fn,

shuffle=True,

drop_last=True)

len(loader)

运

行结果如下：

1000

可见，训练数

据集加载器一共有1000个批

次。

定义好了数据加载器

之后，可以查看一批数据

样例，代码如下：

#第9章/查看

数据样例

for i, (input_ids, attention_mask, token_type_ids,

labels) in enumerate(loader):

break

input_ids.shape, attention_mask.shape,

token_type_ids.shape,

labels

运行结果如下

：

(torch.Size([8, 45]),

torch.Size([8,

45]),

torch.Size([8, 45]),

tensor([0, 1, 0,

0, 1, 0, 0, 0], device='CUDA:0'))

这个结果其实就是数据

整理函数的计算结果，只

是句子的数量更

多。

9.4.2 定义

模型

1.加载预训练模型

关

于预训练模型在第7章中

已经详细介绍过，此处不

再赘述，仅

给出代码，代码

如下：

#第9章/加载预训练模

型

from transformers import BertModel

pretrained = BertModel.from_pretrained('bert-base-chinese')

#统计参数量

sum(i.numel() for

i in pretrained.parameters()) / 10000

在代码的

最后，输出了模型的参数

量，运行结果如下：

10226.7648

可见，bert-base-chinese模

型的参数量约为1亿个，在

本次任务

中选择不训练

它，代码如下：

#第9章/不训练

预训练模型，不需要计算

梯度

for param

in pretrained.parameters():

param.requires_grad_(False)

定义好预训练模型

之后，可以进行一次试算

，代码如下：

#第9章/预训练模

型试算

#设定计算设备

pretrained.to(device)

#模

型试算

out = pretrained(input_ids=input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids)

out.last_hidden_state.shape

运行结果如下：

torch.Size([8, 45, 768])

样

例数据为8句话的编码结

果，从预训练模型的计算

结果可以看

出，这也是8句

话的结果，每句话包括45个

词，每个词被抽成了一个

768维的向量。到此为止，通过

预训练模型成功地把8句

话转换为一个

特征向量

矩阵，可以接入下游任务

模型做分类或者回归任

务。

2.定义下游任务模型

完

成以上工作后，现在可以

定义下游任务模型了，对

于本章的任

务来讲，需要

计算一个二分类的结果

，并且需要和数据集中真

实的

label保持一致，代码如下

：

在这段代码中，定义了下

游任务模型，该模型只包

括一个全连接

的线性神

经网络，权重矩阵为768×2，所以

它能够把一个768维度的

向

量转换到二维空间中。

下

游 任

务 模 型 的 计 算

过 程

为 ， 获 取 一

批 数 据 之 后 ，

使

用

backbone将这批数据抽取成特

征矩阵，抽取的特征矩阵

的形状应该

是16×45×768，这个在之

前预训练模型的试算中

已经看到。这3个维

度分别

代表了16句话、45个词、768维度的

特征向量。

之后下游任务

模型丢弃了44个词的特征

，只取得了第1个词（索

引为

0）的特征向量，对应了编码

结果中的[CLS]，把特征向量矩

阵变

成了16×768。相当于把每句

话变成了一个768维度的向

量。

注意：之所以只取了第

1个词的特征做后续的判

断计算，这和预

训练模型

BERT的训练方法有关系，具体

可见第14章。

之后再使用自

己的全连接线性神经网

络把16×768特征矩阵转换

到16×2，即

为要求的二分类结果。

在

代码的最后对该模型进

行试算，运行结果如下：

torch.Size([16, 2])

可

见，这就是要求的16句话的

二分类的结果。

9.4.3 训练和测

试

1.训练

模型定义之后，接

下来就可以对模型进行

训练了，代码如下：

在这段

代码中，首先定义了优化

器、loss计算函数、学习率调节

器。这3个工具在第7章已经

详细介绍过，此处不再赘

述。

最后，每优化10次模型参

数，就计算一次当前模型

预测结果的正

确率，并输

出模型的loss和优化器的学

习率，最终训练完毕后，输

出

的观察数据见表9-3。

表9-3

训

练过程输出

从表9-3可以看

出，模型收敛的速度很快

，这得益于从BERT预训

练模型

得到的数据特征。对于下

游任务模型只是非常简

单的一层全连

接神经网

络，所以训练的难度很低

。能够观察到学习率也如

预期，即

在缓慢地下降。

2.测

试

最后，对训练好的模型

进行测试，以验证训练的

有效性，代码如

下：

在这段

代码中，首先定义了测试

数据集和加载器，并取出

5个批

次的数据让模型进

行预测，最后统计正确率

并输出，运行结果如下：

0.89375

最

终模型取得了约89.4%正确率

的成绩，这验证了下游任

务模

型，即使在不训练backbone的

情况下也取得一定的成

绩。

9.5 小结

本章通过中文句

子关系推断任务讲解了

如何使用预训练的BERT模

型

抽取句子对的数据特征

。句子关系推断也是BERT模型

本身在训练时

的一个子

任务，所以使用BERT模型能很

有效地解决句子关系推

断任

务。

第10章　实战任务4：中

文命名实体识别

10.1 任务简

介

标记分类是一个自然

语言理解任务，一般可以

分为Named

Entity

Recognition(NER)和Part-of-Speech(PoS)两类。其中，NER类任务

指命名实体识别，NER任务是

要识别出自然语句中的

人物、地点、组

织结构名等

命名实体；另一类任务PoS指

词性标注，PoS任务是要识

别

出自然语句中的动词、名

词、标点符号等。NER任务和PoS任

务在

神经网络模型中计

算的方法几乎相同，本章

将以NER为例进行讲解。

对于

命名实体识别任务来讲

，每个字对应一个标记，标

识这个字

是否属于某个

命名实体，以及处于命名

实体的哪一部分，所以在

命名

实体识别数据集中

，文本数据和标签数据是

严格的一一对应关系，见

表10-1。

表10-1 命名实体识别数据

示例

从表10-1可以看出文本

中的每个字都有标签与

之对应，标识每个

字是否

属于一个命名实体，如果

是一个命名实体的一部

分，则标出属

于该命名实

体的开头，还是中间和结

尾部分。以表中的数据来

看，这

句话中共有两个命

名实体，分别为“厦门”和“金

门”，两个命名实

体均为地

点名。

从表10-1就能很直观地

看出网络模型的计算目

标，即通过文本计

算出标

签。

10.2

数据集介绍

本章所使

用的数据集是people_daily_ner数据集，这

是一个中文

的命名实体

识别数据集，people_daily_ner数据集中的

部分数据样例

见表10-2，通过

该表读者可对people_daily_ner数据集有

直观的认

识。

表10-2

people_daily_ner数据集数

据样例

续表

从表10-1中的标

签可以对照表10-3。

表10-3 people_daily_ner数据集

标签对照表

下面对表10-3中

的各个name分别进行介绍。

(1)O：表

示不属于一个命名实体

。

(2)B-PER：表示人名的开始。

(3)I-PER：表示人

名的中间和结尾部分。

(4)B-ORG：表

示组织机构名的开始。

(5)I-ORG：表

示组织机构名的中间和

结尾部分。

(6)B-LOC：表示地名的开

始。

(7)I-LOC：表示地名的中间和结

尾部分。

通过以上讲解可

以得知，在看到标签中存

在1,2,2串时，表示这是

一个三

个字的人名。同理，1,2是一个

两个字的人名，3,4,4,4是一个四

个字的组织机构名，而0,2和

1,6这样的组合不可能出现

。

10.3 模型架构

从数据集的介

绍可以看出，输入文字的

数量和标签是严格的一

一

对应关系，所以这是一

个典型的N to N任务。可以通过

以下思路达到

该计算结

果，使用一个预训练模型

从文本中抽取数据特征

，再对每个

字的数据特征

做分类任务，最终即可得

到和原文一一对应的标

签序

列。按照该思路，可画

出本次任务的计算流程

图，如图10-1所示。

与之前所做

的3个中文实战任务不同

，本章将连同预训练模型

一

起训练，以提高最终的

预测正确率。在之前的3个

中文实战任务中，

使用的

预训练模型是bert-base-chinese模型，这个

模型的体量比较

大，有大

约1亿个参数，考虑计算量

的问题，本章将使用一个

体量较

小的hfl/rbt3模型，该模型

的参数量约3800万个，更小的

体量方便再

训练。

图10-1 命名

实体识别任务计算过程

10.4 实现代码

10.4.1 准备数据集

1.使

用编码工具

与以往的任

务相同，本章依然从加载

一个编码工具开始，不同

点

在于本章将加载hfl/rbt3编码

器，原因在于后续要使用

hfl/rbt3预训练

模型，从而避免使

用笨重的bert-base-chinese模型。

hfl/rbt3编码器的

编码结果基本同bert-base-chinese编码器

相

同，使用hfl/rbt3编码基本不需

要任何学习过程，此处首

先加载该编

码器，代码如

下：

#第10章/加载编码器

from transformers import AutoTokenizer

tokenizer

= AutoTokenizer.from_pretrained('hfl/rbt3')

tokenizer

运行

结果如下：

 PreTrainedTokenizerFast(name_or_path='hfl/rbt3',

vocab_size=21128,

model_max_len=1000000000000000019884624838656, is_fast=True,

padding_side=

'right', truncation_side='right',

special_tokens={'unk_token':

'[UNK]', 'sep_

token': '[SEP]', 'pad_token':

'[PAD]', 'cls_token': '[CLS]',

'mask_token':

'[MASK]'})

从输出中可以

看出，hfl/rbt3编码使用的特殊符

号基本和bert￾base-chinese编码器相同。

加

载编码工具之后不妨进

行一次试算，以更清晰地

观察编码工具

的输入和

输出，代码如下：

在这段代

码中，让编码工具试编码

了两个句子，与以往的编

码函

数不同，在这个例子

中，输入编码器的不是完

整的句子，而是已经被

分

割成一个一个字的句子

，通过参数is_split_into_words=True告诉

编码器输

入的句子是已经分好词

的，不需要再进行分词工

作了。

之所以需要这样做

的原因在于，在编码器编

码句子时字和编码结

果

并不一定是一一对应的

关系，虽然BERT系列的编码器

一般是以字为

词的，但依

然有可能忽略某些字，或

者标点符号，从而导致编

码结果

的数量和原句子

的字数量不一致，在以往

的任务中这点并不是特

别重

要，但是在命名实体

识别任务中却不能允许

这样的情况发生，因为在

命名实体识别任务中，原

句子中的每个字和标签

是严格的一一对应关

系

，如果原句子编码之后和

标签不能一一对应，就会

导致无法进行后

续计算

，所以需要通过参数is_split_into_words=True来让

编码器跳

过分词步骤，而

分词这个步骤在编码前

手动完成，从而确保分词

的结

果和标签是严格的

一一对应关系。

从上面的

参数说明可以看出，经过

编码之后的句子一定是

确定的

20个词的长度。如果

超出，则会被截断，如果不

足，则会被补充

PAD，运行结果

如下：

[CLS] 海钓比赛地点在厦

门与金门之间的海域。 [SEP]

[CLS] 这

座依山傍水的博物馆由

国内一流的设计 [SEP]

input_ids tensor([[ 101, 3862, 7157, 3683,

6612, 1765, 4157,

1762, 1336, 7305,

680, 7032,

7305, 722, 7313, 4638,

3862, 1818, 511, 102],

[ 101,

6821, 2429, 898, 2255, 988, 3717,

4638, 1300, 4289,

7667, 4507,

1744,

1079, 671, 3837, 4638, 6392, 6369,

102]])

token_type_ids tensor([[0, 0, 0, 0,

0, 0, 0, 0, 0, 0,

0, 0, 0,

0, 0, 0,

0,

0, 0, 0],

[0, 0,

0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0,

0, 0, 0, 0, 0, 0]])

attention_mask tensor([[1, 1, 1, 1, 1,

1, 1, 1, 1, 1, 1,

1, 1,

1, 1, 1, 1,

1, 1, 1],

[1, 1, 1,

1, 1, 1, 1, 1, 1,

1, 1, 1, 1, 1, 1,

1, 1, 1, 1, 1]])

编码时

的其他参数和编码结果

在“编码工具”一章已有详

细解

读，此处不再赘述，如

果读者对编码结果还不

理解，则可以参考“编

码工

具”一章。

2.定义数据集

如前

所述，本次任务需要使用

的数据集为people_daily_ner，定

义数据集

的代码如下：

在这段代码

中，给出了两种加载数据

集的方法，分别为在线加

载

和离线加载，读者可以

根据自己的网络环境选

择其中的一种方法，离

线

加载所需要的数据文件

可在本书的配套资源中

找到。

加载数据集之后可

以查看数据集的标签数

量和各个标签的名字，

相

应的结果已经被写在注

释中，读者可以自行运行

并查看。

在people_daily_ner数据集中，每条

数据包括两个字段，即

tokens和

ner_tags，分别代表句子和标签，在

__getitem__()函数中把

这两个字段取

出并返回即可。

在代码的

最后初始化训练数据集

，并查看训练数据集的长

度和一

条数据样例，运行

结果如下：

 ['海', '钓',

'比', '赛', '地', '点

', '在', '厦',

'门', '与',

'金', '门', '之',

'间',

'的', '海

', '域', '。']

[0, 0,

0, 0, 0, 0, 0, 5,

6, 0, 5, 6, 0, 0,

0, 0, 0, 0]

20865

可见，训练数据集包括

20

865条数据，每条数据包括一

条分好词

的文本和一个

标签列表。值得注意的是

，此处的数据依然是文本

数

据，还没有被编码器编

码。

3.定义计算设备

关于计

算设备在“第7章 实战任务

1：中文情感分类”中已经详

细介绍过，此处不再赘述

，仅给出代码，代码如下：

#第

10章/定义计算设备

device = 'cpu'

if torch.cuda.is_available():

device = 'CUDA'

device

运行结

果如下：

'CUDA'

由于本次任务需

要对预训练模型进行再

训练，计算量会大于以往

的任务，最好能在CUDA设备上

运行本任务，在CPU上可能会

消耗很

多时间。

4.定义数据

整理函数

与以往的任务

一样，在本次任务中，数据

的处理依然是以批为单

位的，而不是一条一条地

进行处理，所以需要一个

数据整理函数，把

一批数

据整理成需要的格式，具

体实现如下：

在这段代码

中，入参的data表示一批数据

，取出其中的句子和标

签

，分别为两个list。

使用编码工

具编码这一批句子，在参

数指定了编码后的结果

最长

为512个词，超过512个词的

句子将被截断。

在一批句

子中有的句子长，有的句

子短，为了便于网络处理

，需

要把这些数据整理成

矩阵的形式，要求这些句

子有相同的长度，参数

padding=True会

对这批句子补充PAD，使它们

具有同样的长度，具

体长

度取决于这一个批次中

最长的句子有多长，该过

程如图10-2所

示。

图10-2 动态补充

PAD示意

在 编

码 时 ， 通 过 参

数

return_tensors='pt' 让 编 码 的 结

果 为

PyTorch的Tensor格式

，从而免去了后续转换数

据格式的麻烦。

参数is_split_into_words=True告知

编码器这些句子是已经

分词

完毕的，不需要再次

执行分词工作，原因在本

章开头已经介绍过，此

处

不再赘述。

完成文本的编

码之后，需要对labels进行填充

。和文本一样，

labels也是长短不

一的，labels和对应的文本长度

一致，为了把labels

也转换成便

于处理的矩阵，需要对labels进

行填充，让所有的labels的

长度

一致。具体的做法是在所

有labels的开头插入一个标签

7，对应文

本开头会被插入

的[CLS]标签，之后在labels的尾部也

填充7，直到

labels的长度达到当

前批次中最长的句子的

长度。经过以上操作之

后

，当前批次中所有的labels的长

度都一致，即可转换为矩

阵，便于

后续的计算。

最后

把所有的矩阵都转移到

之前定义好的计算设备

上，方便后续

的模型计算

。

定义好了数据整理函数

，不妨假定一批数据，让数

据整理函数进

行试算，以

观察数据整理函数的输

入和输出，代码如下：

在这

段代码中先虚拟了一批

数据，这批数据中包括两

个句子，输

入数据整理函

数后，运行结果如下：

input_ids torch.Size([2, 37])

token_type_ids

torch.Size([2, 37])

attention_mask torch.Size([2, 37])

labels

torch.Size([2, 37])

从编

码结果可以看出，当前批

次中最长的句子有36个词

。

5.定义数据集加载器

关于

数据集加载器在第7章中

已经详细介绍过，此处不

再赘述，

仅给出代码，代码

如下：

#第10章/数据集加载器

loader = torch.utils.data.DataLoader(dataset=dataset,

batch_size=16,

collate_fn=collate_fn,

shuffle=True,

drop_last=True)

len(loader)

运行结果如下：

1304

可见训练

数据集加载器一共执行

了1304个批次。

定义好了数据

集加载器之后，可以查看

一批数据样例，代码如

下

：

#第10章/查看数据样例

for i, (inputs, labels)

in enumerate(loader):

break

print(tokenizer.decode(inputs['input_ids'][0]))

print(labels[0])

for

k, v in inputs.items():

print(k, v.shape)

运行

结果如下：

 [CLS] 按 照 欧

洲 经 货

联 盟 的 进

程，他 将 是 最 后

一 任

局 长。 [SEP] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD] [PAD] [PAD] [PAD] [PAD]

[PAD]

[PAD] [PAD] [PAD]

tensor([7, 0, 0,

3, 4, 4, 4, 4, 4,

0, 0, 0, 0, 0, 0,

0, 0, 0,

0, 0, 0,

0, 0, 7,

7, 7, 7,

7, 7, 7, 7, 7, 7,

7, 7, 7, 7, 7, 7,

7, 7, 7, 7, 7, 7,

7, 7, 7,

7, 7, 7,

7, 7, 7], device='CUDA:0')

input_ids torch.Size([16,

54])

token_type_ids torch.Size([16, 54])

attention_mask torch.Size([16,

54])

这个结果其实

就是数据整理函数的计

算结果，只是句子的数量

更

多。

10.4.2 定义模型

1.加载预训

练模型

如上所述，本章将

使用hfl/rbt3模型作为预训练模

型，代码如

下：

#第10章/加载预

训练模型

from transformers import

AutoModel

pretrained = AutoModel.from_pretrained('hfl/rbt3')

pretrained.to(device)

#统计参数量

print(sum(i.numel() for i in pretrained.parameters()) /

10000)

和

以往任务中加载预训练

模型的方法几乎相同，仅

仅在加载函数

中修改模

型的名字即可。在代码的

最后，输出了模型的参数

量，运行

结果如下：

3847.68

定义好

预训练模型之后，可以进

行一次试算，观察模型的

输入和

输出，代码如下：

#第

10章/模型试算

#[b, lens] -> [b,

lens, 768]

pretrained(**inputs).last_hidden_state.shape

运行结果如

下：

torch.Size([16, 54,

768])

样例数据为16句话的编

码结果，从预训练模型的

计算结果可以看

出，这也

是16句话的结果，每句话包

括54个词，每个词被抽成了

一个

768维的向量。到此为止

，通过预训练模型成功地

把16句话转换为一

个特征

向量矩阵，可以接入下游

任务模型做分类或者回

归任务。

2.定义下游任务模

型

完成以上工作后，现在

可以定义下游任务模型

了。与以往的任务

不同，本

章将对预训练模型进行

再训练，并且本章将使用

两段式训

练，所以要求下

游任务模型能够切换微

调(Fine Tuning)模式。

什么是两段式训

练？两段式训练是一种训

练技巧，指先单独对下

游

任务模型进行一定的训

练，待下游任务模型掌握

了一定的知识以

后，再连

同预训练模型和下游任

务模型一起进行训练的

模式。

可以把这个过程想

象为一条流水线上的两

个工作，上游的是熟练

工

，下游的是生疏工人。一开

始生疏的工人没有任何

知识，当生产出

错时，我们

就会要求生疏的工人改

进工作方法，而不会怀疑

熟练工的

工作方法。

在这

个阶段如果要求熟练工

人改进，则反而会导致他

怀疑以往积

累的知识是

否是正确的，他会为了配

合糟糕的生疏工人而错

误地修改

自己的生产方

法，这显然并不是我们想

要的。

所以应该先训练生

疏工人，把生疏工人训练

成一个半熟练的工

人，此

时生产的正确率已经难

以上升，再让两个工人共

同训练，以优

化生产的正

确率，这就是两段式训练

的思想。

综上所述，为了支

持两段式训练，需要下游

任务模型能够切换微

调

模式，所谓的微调模式即

连同预训练模型和下游

任务模型一起训练

的模

式，反之，则为单独训练下

游任务模型，具体实现代

码如下：

这段代码定义并

初始化了下游任务模型

，在下游任务模型的

__init__()函数

中有两个重要的变量，即

tuning和pretrained，其中

tuning为布尔型变量，取

值为True和False，它表明了当前模

型是否

处于微调模式，默

认值为False，即非微调模式。pretrained代

表了预

训练模型，当处于

微调模式时预训练模型

应该属于当前模型的一

部

分，反之则不属于，默认

为None，即预训练模型不属于

当前模型的

一部分。

在__init__()函

数中还定义了下游任务

模型的两个网络层，即是

循

环神经网络层和全连

接神经网络层，分别命名

为rnn和fc，其中循环神

经网络

的实现为GRU网络。

forward()函数定义

了下游任务模型的计算

过程，首先判断当前模

型

是否处于微调模式，如果

处于微调模式，则使用内

部的预训练模

型，否则使

用外部的预训练模型，并

且不计算预训练模型的

梯度。得

到预训练模型抽

取的文本特征后，把文本

特征输入循环神经网络

进一

步抽取特征，最后把

特征数据输入全连接神

经网络做分类即可。

为什

么需要循环神经网络层

？这是一个想当然的想法

，因为标签

列表也可以看

作一句话，这句“话”也符合

一定的统计规律，例如人

名的中间部分(I-PER)一定出现

在人名的开头(B-PER)之后，所以

把预

训练模型抽取的文

本特征也当作一个序列

数据进行处理，输入循环

神

经网络再次抽取特征

，最后做分类计算，期望可

以得到更好的结果。

读者

也可以尝试移除，或者增

加其他的层，来提高模型

预测的正确

率，深度学习

任务中往往有很多这样

的尝试性实验。

一般的PyTorch模

型定义__init__()函数和forward()函数就可

以

了，但是在上面的模型

中还定义了fine_tuning()函数，这个函

数就是

要切换下游任务

模型的微调模型，入参为

一个布尔值，取值为True和

False。如

前所述，当切换到微调模

式时，把预训练模型作为

下游任务

模型的一部分

，并且解冻预训练模型的

参数，让它们随着训练更

新、

优化，并且把预训练模

型切换到训练模型。

反之

，不处于微调模式时要冻

结预训练模型的参数，不

让它们随

着训练更新，并

且预训练模型不属于下

游任务模型的一部分，要

把预

训练模型切换到运

行模式。

在代码的最后对

下游任务模型进行了试

算，入参即为之前看到的

数据样例，运行结果如下

：

torch.Size([16, 54, 8])

从结果可以看出，运算的

结果为16句话，54个词，每个词

为8分

类结果。

10.4.3 训练和测试

1.两个工具函数

为了便于

后续的训练和测试，需要

定义两个工具函数，第1个

函

数的功能是对计算结

果和labels变形，并且移除PAD，需要

这个函数

的原因是因为

在一批数据中，往往会有

很多PAD，对这些PAD去计算

它们

的命名实体是没有意义

的，显然它们不可能是任

何的命名实体，

为了不让

模型去研究这些PAD是什么

东西，直接从计算结果中

移除这

些PAD，以防止模型做

无用功。实现代码如下：

#第

10章/对计算结果和labels变形，并

且移除PAD

def

reshape_and_remove_pad(outs, labels, attention_mask):

#变形，便于计算loss

#[b, lens,

8] -> [b*lens, 8]

outs =

outs.reshape(-1, 8)

#[b, lens] -> [b*lens]

labels = labels.reshape(-1)

#忽

略对PAD的计算结果

#[b, lens]

-> [b*lens - pad]

select =

attention_mask.reshape(-1) == 1

outs = outs[select]

labels = labels[select]

return outs, labels

reshape_and_remove_pad(torch.randn(2, 3, 8), torch.ones(2, 3),

torch.ones(2,

3))

在这段

代码中，首先把模型的预

测结果和labels都从多句话合

并

成一句话，合并的方式

就是简单地进行头尾相

接，这样能够方便后续

计

算loss。

移 除

PAD 时 使 用 编 码

结 果

中 的 attention_mask ，

attention_mask

标 记 了 一 个 句

子 中

哪 些 位 置 是

PAD ，

attention_mask 中 只 有

0 和 1 ， 其

中 0

表 示 是 PAD 的 位

置 ， 使 用

attention_mask可

以很轻松地过滤掉结果

中的PAD。

在代码的最后使用

一批虚拟的数据试算该

函数，运行结果如下：

(tensor([[ 0.0291, 0.5538, -0.6427, 0.5524, -0.3672,

1.1282,

1.3546, 1.3098],

[-1.6091, -0.6178, -1.8915,

-0.6785, 1.8442, 0.1800, -1.1797,

0.9228],

[-1.3673,

0.1874, -0.0652, 1.4556, 1.4159, 1.8392, 0.5031,

0.9490],

[-0.0035, 1.4326, 0.2621, 1.3923, 0.7450,

-2.0021, -2.8821,

0.0661],

[ 0.1377, -1.2215,

-2.0415, -1.1509, 0.1217, -0.5679, 1.2549,

1.0358],

[-0.4724, 0.2421, -0.2521, 2.6841, 1.3514, 0.5778,

0.2485,

-0.4031]]),

tensor([1., 1., 1., 1.,

1., 1.]))

虚拟

数据中的2×3×8矩阵表示2句话

、3个词、每个词8分类的预

测

结果，第1个2×3矩阵表示真实

的labels，第2个2×3的矩阵表示

attention_mask，因为

全为1，所以全部保留，没有

PAD。最后计算

的结果也确实

全部保留了预测结果和

labels，并且预测结果和labels被

变形

成一句话，和预期一致。

第

2个函数用于计算预测结

果中预测正确了多少个

，以及一共有

多少个预测

结果，代码如下：

#第10章/获取

正确数量和总数

def get_correct_and_total_count(labels,

outs):

#[b*lens, 8] -> [b*lens]

outs

= outs.argmax(dim=1)

correct = (outs ==

labels).sum().item()

total = len(labels)

#计算除

了0以外元素的正确率，因

为0太多了，所以正确率很

容易虚高

select

= labels != 0

outs =

outs[select]

labels = labels[select]

correct_content =

(outs == labels).sum().item()

total_content = len(labels)

return correct, total, correct_content, total_content

get_correct_and_total_count(torch.ones(16),

torch.randn(16, 8))

这个函数的入

参已经过上一个函数的

处理，所以预测结果和

labels都

已经是一句话了，而不是

多句话。

在函数实现中，一

共计算了两对正确数量

和总数，它们的区别是

一

套计算了0这个标签，另一

套则排除了0个标签。

之所

以需要计算两套，是因为

在labels中各个标签的分布并

不是

均匀的，0这个标签的

数量特别多，如果在计算

正确率时包括0这个标

签

，则正确率很容易虚高。因

为模型只要猜标签都是

0就可以取得很

高的正确

率，为了排除标签0特别高

，而导致的正确率虚高的

问题，

此处需要计算另一

套正确数量和总数，即排

除标签0后的正确数量和

总数。

在代码的最后虚拟

了数据对函数进行试算

，运行结果如下：

(2, 16, 2, 16)

因为虚拟

的labels全部是1，并没有出现标

签0的情况，所以统计

得出

的两套正确数量和总数

相等。

2.训练

经过以上准备

工作后，现在可以定义训

练函数了，代码如下：

训练

函数接受一个参数epochs，表示

要使用全量数据训练几

个轮

次，由于是两段式训

练，在两个阶段分别进行

训练的轮次可能不一

样

，所以需要这个参数。

与以

往的任务不同，由于采用

了两段式训练，所以会根

据模型是

否处于微调模

式选择不同的Learning Rate，在非微调

模式时选择较

大的Learning Rate，以快

速训练下游任务模型；在

微调模式时则选

择较小

的Learning Rate，以精细地调节模型参

数，帮助模型优化到

更优

的性能。

之后定义了优化

器、loss计算函数、学习率调节

器。这3个工具在

“实战任务

1：中文情感分类”一章中已

经详细介绍过，此处不再

赘

述。

需要注意的是，优化

器优化的参数表为下游

任务模型的所有参

数，因

为下游任务模型存在微

调模式的问题，在非微调

模式下，预训

练模型并不

属于下游任务模型的一

部分，所以优化器优化的

参数数量

会比较少，仅包

含下游任务模型本身的

参数。而在微调模式下，预

训

练模型属于下游任务

模型的一部分，所以优化

器优化的参数表也会包

括预训练模型，这也是为

什么要在切换微调模式

时，设置下游任务模

型的

pretrained属性的原因。

接下来把下

游任务模型切换到训练

模式，并且在全量训练数

据上

遍历epochs个轮次，对模型

进行训练。训练过程如下

所述：

(1)从数据集加载器中

获取一个批次的数据。

(2)让

模型计算预测结果。

(3)使用

工具函数对预测结果和

labels进行变形，移除预测结果

和

labels中的PAD。

(4)计算loss并执行梯度

下降优化模型参数。

(5)每隔

一定的steps，输出一次模型当

前的各项数据，便于观

察

。

(6)每训练完一个epoch，将模型的

参数保存到磁盘。

3.两段式

训练

完成以上工作之后

，就可以进行两段式训练

的第1步了，代码如

下：

#第10章

/两段式训练第1步，训练下

游任务模型

model.fine_tuning(False)

print(sum(p.numel() for p in model.parameters())

/ 10000)

train(1)

在这段代码

中，首先把下游任务模型

切换到非微调模式，之后

输

出了模型的参数量，由

于预训练模型并不属于

下游任务模型的一部

分

，所以此处期待的参数量

应该稍小，最后在全量数

据上训练1个轮

次，运行结

果如下：

354.9704

可以看到在非微

调模式下，下游任务模型

的参数量为354万。训

练过程

的输出见表10-4。

表10-4 第一阶段

训练输出

续表

从表10-4可以

看出，随着训练步骤的增

多，loss收敛得很快，并

且正确

率已经很高，即达到了85%，但

排除labels中的0之后，正确率

却

只有25%，可见正确率是虚高

的。

接下来可以进行两段

式训练的第二阶段，代码

如下：

#第10章/两段式训练第

2步，同时训练下游任务模

型和预训练模型

model.fine_tuning(True)

print(sum(p.numel() for p in model.parameters())

/ 10000)

train(5)

在这段

代码中，把下游任务模型

切换到微调模式，这意味

着预训

练模型将被一起

训练。代码中输出了当前

下游任务模型的参数量

，由

于预训练模型已经属

于下游任务模型的一部

分，因此此处的参数量期

望会比较大，最后在全量

数据上执行5个轮次的训

练，运行结果如

下：

4202.6504

可见切

换到微调模式后，下游任

务模型的参数量增加到

4200万

个，由于采用了较小的

预训练模型，所以这个参

数量的规模依然较

小，即

使在一颗CPU上训练这个任

务，时间也应该在可接受

的范围

内。训练过程的输

出见表10-5。

表10-5 第二阶段训练

输出

续表

从表10-5可以看出

，在本次的训练中不仅总

体正确率上升了，排

除标

签0之后的正确率也上升

了。

4.测试

最后，对训练好的

模型进行测试，以验证训

练的有效性，代码如

下：

在

这段代码中，首先从磁盘

加载了训练完毕的模型

，然后把模型

切换到运行

模式，再把模型移动到定

义好的计算设备上。

完成

模型的加载之后，定义测

试数据集和加载器，并取

出5个批

次的数据让模型

进行预测，最后统计两个

正确率并输出，两个正确

率

之间的区别是一个统

计了标签0，另一个则没有

，运行结果如下：

0

1

2

3

4

0.9879000658286574 0.9409127954360228

经过5个批

次的测试之后，最终模型

取得了98.8%和94.1%的正确

率的成

绩，两个正确率之间的差

距还是比较大的。考虑到

这是一个8

分类的任务，当

前的正确率已经验证了

模型的有效性。

5.预测

验证

了模型的有效性之后，可

以进行一些预测，以更直

观地观察

模型的预测结

果，代码如下：

在这段代码

中执行了以下工作：

(1)加载

了训练完毕的模型，并切

换到运行模式，再移动到

定义好

的计算设备上。(2)定

义了测试数据集加载器

，然后从数据集加载器中

取出了一批数据。

(3)对这批

数据进行预测。

(4)对原句子

进行一些处理，以更符合

人类的阅读习惯。

(5)输出labels和

预测结果，以观察两者的

异同。

由于输出的结果较

长，考虑到篇幅此处只给

出部分结果，以下是

几个

例子：

[CLS]长篇小说《放逐》出版

青年作家刘方炜的长篇

小说《放逐》日前由中国电

影出版

社出版。[SEP]

[CLS]7··············刘1方2炜2············中

3国4电4影4出4版4社

4···[SEP]7

[CLS]7··············刘1方2炜2············中

3国4电4影4出4版4社

4···[SEP]7

==========================

输出中的

第1行为原文，中间一行为

labels，即网络计算的目标，

第3行

为网络预测的结果。从这

个例子中看，网络预测的

结果和原

labels完全一致，没有

任何错误，成功捕捉到了

组织机构名“中国电

影出

版社”和人名“刘方炜”。

接下

来再看三个例子，输出如

下：

 [CLS]老人临走时，一再向房

东表示感谢并激动地说

：[UNK]西柏坡，和我的故

乡一样

亲切美好！

[UNK][SEP]

[CLS]7······················西5柏6坡6··············[SEP]7

[CLS]7······················西5柏6坡

6··············[SEP]7

==========================

[CLS]水南流，至五门堰及斗山

一带拐若干个荒滩大弯

，人称龙摆尾，每年发大水

都要

甩开大片。

[SEP]

[CLS]7·····五5门6堰6·斗

5山6·····························[SEP]7

[CLS]7·····五5门6堰6·斗5山6·····························[SEP]7

==========================

[CLS]两个月后

少女平静地离去，她的身

边簇拥着俊平的朋友们

，枕边还放着俊平为她

捎

去的书。

[SEP]

[CLS]7···················俊1平2··········俊1平2·······[SEP]7

[CLS]7···················俊1平2··········俊

1平2·······[SEP]7

==========================

可见预测的结果和labels完

全一致，没有任何错误，接

下来再看

几个错误的例

子，输出如下：

 [CLS]为使农民尽

快富起来，和万春还帮助

农民架桥，组建20多支农运

车队，每

支队伍全年收入

六

七万元。[SEP]

[CLS]7··········和1万2春2································[SEP]7

[CLS]7···········万1春2································[SEP]7

==========================

[CLS]大

连女子足球队今天在香

港举行的首届[UNK]连港杯[UNK]女

子足球赛中，以

3∶0击败东道

主中国香港队，夺得冠军

。[SEP]

[CLS]7大3连4女4子4足4球4队4···香5港6······连

5港5··················

中

3国4香4港4队4······[SEP]7

[CLS]7大3连4女4子

4足4球4队4···香5港6······连5港6···············东

3道4·中

3国4香4港4队4······[SEP]7

==========================

[CLS]一些标志性的

宏伟建筑，如国家大剧院

，将在广场西侧兴建。[SEP]

[CLS]7············国5家

6大6剧6院6···广5场6·····[SEP]7

[CLS]7············国3家4大4剧4院

4··········[SEP]7

==========================

在第1个例子中，人名“和万

春”被错认成了“万春”。

在第

2个例子中，原文中的“东道

主”并不是一个命名实体

，但

却被错误地识别为组

织机构名“东道”。

在第3个例

子中，地名“广场”没有被识

别出来。

以上是一些典型

的错误。

10.5 小结

本章通过命

名实体识别任务介绍了

预训练模型的再训练过

程，并

且介绍了两段式训

练的原理以及操作方法

，演示了完整的训练过程

。

通过本章的学习，希望读

者能掌握预训练模型的

再训方法，并能通过

两段

式训练的技巧更稳定地

训练模型。

第11章　使用TensorFlow训练

11.1 任务简介

在前面的章节

中，演示了4个中文任务，这

些任务都是使用

PyTorch

计 算 的

， HuggingFace 支 持

多 个 深 度 学 习

框 架

， 包 括

PyTorch和TensorFlow。有些读者可能对

使用TensorFlow计算感兴

趣，本章将

使用TensorFlow框架再次实现中文

命名实体识别任务，以

演

示在TensorFlow中使用HuggingFace的方法。

HuggingFace支持

2.3以上版本的TensorFlow，在运行本章

代码

前，需要确保TensorFlow版本符

合要求。

11.2 数据集介绍

本章

使用的数据集依然是people_daily_ner数

据集，该数据集在

第10章已

经详细介绍过，此处不再

重复介绍，只给出数据示

例，见表

11-1，如读者对该数据

集不了解，则可参考第10章

。

表11-1 命名实体识别数据示

例

从表11-1就能很直观地看

出网络模型的计算目标

，即通过文本计

算出标签

。

11.3 模型架构

使用TensorFlow实现该任

务和使用PyTorch实现的计算流

程完全

一致，计算流程如

图10-1所示。

在使用PyTorch实现该任

务时使用了两段式训练

的技巧，在

TensorFlow框架中依然将

使用该技巧，以演示在TensorFlow框

架中

实现两段式训练的

方法。

11.4 实现代码

11.4.1 准备数据

集

1.使用编码工具

HuggingFace提供的

编码工具支持多个深度

学习框架，包括

PyTorch和TensorFlow，在更换

计算框架时，编码工具的

部分几乎不

需要修改，加

载编码工具的代码如下

：

#第11章/加载编码器

from transformers

import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('hfl/rbt3')

tokenizer

运行结

果如下：

PreTrainedTokenizerFast(name_or_path='hfl/rbt3',

vocab_size=21128,

model_max_len=1000000000000000019884624838656, is_fast=True,

padding_side=

'right', truncation_side='right', special_tokens={'unk_token':

'[UNK]',

'sep_token': '[SEP]',

'pad_token': '[PAD]', 'cls_token':

'[CLS]', 'mask_token':

'[MASK]'})

这部分代码和使

用的计算框架无关，所以

和使用PyTorch时的代

码完全一

致。

加载编码工具之后，可

以进行一次试算，以观察

输入和输出，代

码如下：

由

于编码工具同时支持PyTorch和

TensorFlow，所以此处的代码

也几乎

是一样的，唯一的修改点

是batch_encode_plus()函数的参数

return_tensors='tf'，在PyTorch框架中

该参数的值为'pt'，在使用

TensorFlow框

架时应修改为'tf'。

运行结果

如下：

[CLS] 海钓比赛地点在厦

门与金门之间的海域。 [SEP]

[CLS] 这

座依山傍水的博物馆由

国内一流的设计 [SEP]

input_ids tf.Tensor(

[[

101 3862 7157 3683 6612 1765

4157 1762 1336 7305 680 7032

7305 722

7313 4638 3862 1818

511 102]

[ 101 6821 2429

898 2255 988 3717 4638 1300

4289 7667 4507

1744 1079

671

3837 4638 6392 6369 102]], shape=(2,

20), dtype=int32)

token_type_ids tf.Tensor(

[[0 0

0 0 0 0 0 0

0 0 0 0 0 0

0 0 0 0 0 0]

[0 0 0 0 0 0

0 0 0 0 0 0

0 0 0 0 0 0

0 0]], shape=(2, 20),

dtype=int32)

attention_mask

tf.Tensor(

[[1 1 1 1 1

1 1 1 1 1 1

1 1 1 1 1 1

1 1 1]

[1 1 1

1 1 1 1 1 1

1 1 1 1 1 1

1 1 1 1 1]], shape=(2,

20),

dtype=int32)

可以看

到编码的结果已经是TensorFlow的

Tensor格式。

2.定义数据集

如前所

述，本次任务需要使用的

数据集为people_daily_ner，加

载数据集的

函数的代码如下：

#第11章/获

取数据集

from datasets import load_dataset, load_from_disk

def get_dataset(split):

#在线加载数据

集

#dataset = load_dataset(path='people_daily_ner',

split=split)

#离线加载数据集

dataset =

load_from_disk(dataset_path='./data/people_daily_ner')[split]

#打乱

顺序

dataset.shuffle()

#dataset.features['ner_tags'].feature.num_classes

#7

#dataset.features['ner_tags'].feature.names

#['O', 'B-PER',

'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']

return

dataset

dataset = get_dataset('train')

dataset

在这段代码中，给出

了两种加载数据集的方

法，分别为在线加载

和离

线加载，读者可以根据自

己的网络环境选中其中

一种方法，离线

加载所需

要的数据文件可在本书

的配套资源中找到。

由于

在本章代码中需要多次

加载数据集，所以把加载

数据集封装

成一个函数

，便于后续的调用，调用该

函数时传入需要加载的

数据部

分即可，数据部分

包括训练集和测试集，参

数值分别为train和test。

在代码的

最后加载了训练数据集

，运行结果如下：

Dataset({

features: ['id', 'tokens', 'ner_tags'],

num_rows:

20865

})

可见训练

数据集包括20 865条数据，每条

数据包括一条分好词的

文本和一个标签列表。值

得注意的是此处的数据

依然是文本数据，还

没有

被编码器编码。

3.定义数据

加载函数

与在PyTorch框架中不

同，TensorFlow没有特别好的数据遍

历工

具，可以自定义一个

数据遍历函数，代码如下

：

数据加载函数的任务是

取出数据集中的一批数

据，并把这批数据

编 码 成

适

合 模 型 计 算 的

格 式 。 在

这 段 代

码 中 首 先 根 据

序

号 和

batch_size计算出遍历的起点

和终点，再使用起点和终

点从数据集中

取出这一

段数据，即此次要处理的

一批数据。

如果数据加载

函数发现遍历已经越界

，则会返回None值，表明

这一轮

次的数据遍历已经结束

。

得到一批要处理的数据

以后，使用编码工具编码

这一批句子，在

参数指定

了编码后，结果最长为512个

词，超过512个词的句子将被

截

断。

在一批句子中有的

句子长，有的句子短，为了

便于网络处理，需

要把这

些数据整理成矩阵的形

式，要求这些句子有相同

的长度，参数

padding=True会对这批句

子补充PAD，使其成同样的长

度，具体长

度取决于这一

个批次中最长的句子有

多长，该过程如图11-1所示。

图

11-1 动态补充PAD示意

在 编 码

时

， 通 过 参 数 return_tensors='tf'

让 编 码 的 结 果

为

TensorFlow的Tensor格式，免去了后续转

换数据格式的麻烦。

和使

用PyTorch实现时数据整理函数

一样，这里也需要对labels

进行

填充，在一批数据中labels的长

短是不一的，为了把labels也转

换

成便于处理的矩阵，需

要对labels进行填充，让所有的

labels的长度一

致。具体的做法

是在所有labels的开头插入一

个标签7，对应文本开头

会

被插入的[CLS]标签，之后在labels的

尾部也填充7，直到labels的长

度

达到当前批次中最长的

句子的长度。经过以上操

作之后，当前批次

中所有

的labels的长度都一致，这样就

可以转换为矩阵便于后

续的计

算了。

定义好了数

据加载函数，可以取一批

样例数据，查看数据样例

的

格式，代码如下：

#第11章/查

看数据样例

inputs, labels = get_batch_data(dataset, 0, 16)

for k, v in inputs.items():

print(k,

v.shape)

print('labels', labels.shape)

运行结果如

下：

input_ids(16, 140)

token_type_ids (16, 140)

attention_mask (16, 140)

labels(16, 140)

从结果可以看出，这批

数据中最长的数据为140个

词，包括labels

在内的结果中有

4个矩阵。

11.4.2 定义模型

1.加载预

训练模型

完成以上工作

以后就可以加载预训练

模型了，代码如下：

#第11章/加

载预训练模型

from transformers import

TFAutoModel

pretrained = TFAutoModel.from_pretrained('hfl/rbt3')

#查看模型

概述

pretrained.summary()

和 PyTorch 不 同 ， 使

用 TensorFlow 计 算 时

需 要

使 用

TFAutoModel类来加载模型

，在代码的最后，输出了模

型的概述，运

行结果如下

：

Model: "tf_bert_model"

_______________________________________________________________

__

Layer (type) Output Shape

Param #

===============================================================

==

bert (TFBertMainLayer)

multiple 38476800

===============================================================

==

Total params:

38,476,800

Trainable params: 38,476,800

Non-trainable params:

0

_______________________________________________________________

__

可见模型的参数量约3800万

个，和使用PyTorch实现时大致相

同。

定义好预训练模型之

后，可以进行一次试算，计

算方法和使用

PyTorch实现时相

同，代码如下：

#第11章/模型试

算

#[b, lens] -> [b, lens,

768]

pretrained(**inputs).last_hidden_state.shape

运行结果如下：

TensorShape([16, 140, 768])

计算后

输出的结果也和使用PyTorch实

现时相同，也是16句话的

结

果，每句话包括140个词，每个

词被抽成了一个768维的向

量。到此

为止，通过预训练

模型成功地把16句话转换

为了一个特征向量矩阵

，

可以接入下游任务模型

，进行后续的计算。

2.定义下

游任务模型

下游任务模

型的计算过程和使用PyTorch实

现时相同，只是修改

为使

用TensorFlow计算，同样需要支持两

段式训练，代码如下：

运行

结果如下：

TensorShape([16, 140, 8])

从结果可以看

出，运算的结果为16句话、140个

词、每个词为8分

类结果。

11.4.3 训

练和测试

1.两个工具函数

和使用PyTorch实现时相同，此处

也需要定义两个工具函

数，第1

个函数的功能是对

计算结果和labels变形，并且移

除PAD，实现代码

如下：

#第11章/对

计算结果和labels变形，并且移

除PAD

def reshape_and_remove_pad(outs, labels, attention_mask):

#变形，便于计算loss

#[b, lens, 8] -> [b*lens, 8]

#[b, lens] -> [b*lens]

outs =

tf.reshape(outs, [-1, 8])

labels = tf.reshape(labels,

[-1])

#忽略对

PAD的计算结果

#[b, lens] -> [b*lens

- pad]

select = tf.reshape(attention_mask, [-1])

== 1

outs = outs[select]

labels

= labels[select]

return outs, labels

reshape_and_remove_pad(tf.random.normal([2,

3, 8]), tf.ones([2,

3]),

tf.ones([2, 3]))

在代码的最

后使用一批虚拟的数据

试算该函数，运行结果如

下：

(<tf.Tensor: shape=(6, 8), dtype=float32, NumPy=

array([[-0.8518044, 0.56981546, 1.7722402, 1.5570363, 

-0.5452776,

-0.05904967, 0.6430304, -0.5592008],

[-0.10965751, 0.31557927, -1.1976087,

0.11825781, 

-0.89963585,

-1.1651767, -1.7429291, -1.4400107],

[1.4001974, -1.3210682, -0.37927464, -0.14094475,

-1.3921576,

-0.12169897,

0.11071096, -0.521887],

[-2.4660468, 0.41077474, -0.06646279, -1.8674058,

0.23685668,

-1.4304556, 0.2736403, 0.40887165],

[-0.47869956, 0.6307642,

1.0175115, 0.6412728, 0.9174518,

-1.6071075, 0.8128216, -0.12776785],

[0.9170497, 0.62383527, 0.4977573, -0.0440811,

0.39723176,

1.4127846,

-0.50897956, 1.7356095]], dtype=float32)>,

<tf.Tensor: shape=(6,), dtype=float32,

NumPy=array([1., 1., 1.,

1., 1., 1.],

dtype=float32)>)

第2个函数用于计算预

测结果中预测正确了多

少个，以及一共有

多少个

预测结果，代码如下：

#第11章

/获取正确数量和总数

def get_correct_and_total_count(outs,

labels):

#[b*lens, 8] -> [b*lens]

outs

= tf.argmax(outs, axis=1, output_type=tf.int32)

correct =

tf.cast(outs == labels, dtype=tf.int32)

correct =

int(tf.reduce_sum(correct))

total = len(labels)

#计

算除了0以外元素的正确

率，因为0太多了，所以正确

率很容易虚高

select

= labels != 0

outs =

outs[select]

labels = labels[select]

correct_content =

tf.cast(outs == labels, dtype=tf.int32)

correct_content =

int(tf.reduce_sum(correct_content))

total_content = len(labels)

return correct,

total, correct_content, total_content

get_correct_and_total_count(tf.random.normal([16, 8]),

tf.ones([16],

dtype=tf.int32))

和使用PyTorch实

现时一样，在这个函数中

，一共计算了两对正

确数

量和总数，它们的区别是

一套计算了0这个标签，另

一套则排除

了0这个标签

。因为在labels中各个标签的分

布并不是均匀的，0这个

标

签的数量特别多，如果在

计算正确率时包括0这个

标签，则正确率

很容易虚

高，因此需要在排除0这个

标签以后额外计算一套

正确数量

和总数。

在代码

的最后虚拟了数据对函

数进行试算，运行结果如

下：

(2, 16, 2, 16)

因为虚拟的labels全部是1，并

没有出现标签0的情况，所

以统计

得出的两套正确

数量和总数相等。

2.训练

经

过以上准备工作后，现在

可以定义训练函数了，代

码如下：

和使用PyTorch实现时一

样，本章也将使用两段式

训练，在两个

阶段的训练

轮次可能不一样，所以需

要epochs这个参数。

HuggingFace提供了工具

函数create_optimizer()，用于创建

TensorFlow的优化器

和Learning Rate衰减器，下面对这个工

具函数

的各个参数分别

进行介绍。

(1)参数init_lr：初始的Learning Rate，在

代码中会根据模型的微

调模式选择不同的初始

Learning

Rate，如果处于微调模式，则使

用

更小的Learning Rate，防止模型出现

灾难性遗忘。

(2)参数num_warmup_steps：Learning Rate预热步

数，表明在

开始训练后，多

少个steps之内不衰减Learning

Rate，而是提

高

Learning Rate以更快地训练模型。

(3)参

数num_train_steps：表明一共将训练多少

个steps，在这些

steps之后Learning Rate将会被衰

减为0。

创建完了优化器和

Learning Rate衰减器，就开始遍历数据

，在

训练数据集上遍历epochs个

轮次，每次使用数据加载

函数获取一批数

据，如果

获取的数据为None，则说明此

次遍历已经结束。

把每一

批数据输入模型进行计

算，得到计算结果以后使

用

tf.losses.categorical_crossentropy()函数计算交叉熵loss，下面

对该

函数的各个参数分

别进行介绍。

(1)参数y_true：即labels，但此

处需要的是One Hot的格式，使

用

tf.one_hot()函数把labels转换为One Hot格式传入

即可。

(2)参数y_pred：即模型计算的

结果。

(3)参数from_logits：由于计算结果

经过了激活函数Softmax的

计算

，所以并不是logits的，此处传入

False。

(4)参数axis：表明要计算交叉熵

的维度，由于计算的结果

维度为

[字，分类]，所以传入

分类所在的索引1即可。

tf.losses.categorical_crossentropy()函

数计算的loss为N个字的

交叉

熵，使用tf.reduce_mean()函数求平均值即

为最终的loss。

得到loss以后可以

根据loss求得模型中各个参

数的梯度，最后使

用优化

器根据梯度优化参数即

可。

每训练完一个epoch，把模型

的参数保存到磁盘上，以

便于后续

调用。

3.两段式训

练

做完以上工作之后，就

可以进行两段式训练的

第1步了，代码如

下：

#第11章/两

段式训练第1步，训练下游

任务模型

model.fine_tuning(False)

print(sum([int(tf.size(i)) for i

in model.trainable_variables])

/ 10000)

train(1)

在这段代码中

，首先把下游任务模型切

换到非微调模式，之后输

出模型的参数量，由于预

训练模型并不属于下游

任务模型的一部分，

所以

此处期待的参数量应该

稍小，最后在全量数据上

训练一个轮次，

运行结果

如下：

354.9704

可以看到在非微调

模式下，下游任务模型的

参数量约为354万。

训练过程

的输出见表11-2。

表11-2

第一阶段

训练输出

续表

从表11-2可以

看出，随着训练步骤的增

多，loss收敛得很快，并

且正确

率已经很高，达到了96%，在排

除labels中的0之后，正确率为

80%左

右。

接下来可以进行两段

式训练的第二阶段，代码

如下：

#第11章/两段式训练第

2步，同时训练下游任务模

型和预训练模型

model.fine_tuning(True)

print(sum([int(tf.size(i)) for i in

model.trainable_variables])

/ 10000)

train(2)

在这段

代码中，把下游任务模型

切换到微调模式，这意味

着预训

练模型将被一起

训练，代码中输出了当前

下游任务模型的参数量

，由

于预训练模型已经属

于下游任务模型的一部

分，所以此处的参数量期

望会比较大，最后在全量

数据上执行两个轮次的

训练，运行结果如

下：

4202.6504

可见

切换到微调模式后，下游

任务模型的参数量增加

到约4200万

个，由于采用了较

小的预训练模型，所以这

个参数量的规模依然较

小，即使在一颗CPU上训练这

个任务，时间也应该在可

接受的范围

内。训练过程

的输出见表11-3。

表11-3 第二阶段

训练输出

续表

从表11-3可以

看出，在本次的训练中不

仅总体正确率上升了，排

除标签0之后的正确率也

上升了。

4.测试

最后，对训练

好的模型进行测试，以验

证训练的有效性，代码如

下：

在这段代码中，首先从

磁盘加载训练完毕的模

型参数。获取了测

试数据

集，并加载了5个批次，每个

批次有128条数据让模型进

行预

测，最后统计两个正

确率并输出，运行结果如

下：

0

1

2

3

4

0.9899993506071822 0.9566824060767809

经过5个批次的测试之

后，最终模型取得了约99.0%和

95.7%的正

确率。

5.预测

验证了模

型的有效性之后，可以进

行一些预测，以更直观地

观察

模型的预测结果，代

码如下：

这段代码的实现

和使用PyTorch实现时的思路完

全一致，只是修

改为使用

TensorFlow进行计算，故代码内容不

再详细解释。

由于输出的

结果较长，考虑到篇幅此

处只给出部分结果，参看

以

下几个例子：

 [CLS]可一想到

自己这个大老爷们得让

妻子养活，王建新闷在心

里的苦水直往嗓

子眼上

冒。[SEP]

[CLS]7···················王1建2新2···············[SEP]7

[CLS]7···················王1建2新2···············[SEP]7

==========================

[CLS]本报北

京5月10日讯亚洲山地车锦

标赛男、女越野赛的上届

冠军今天在这里双双

失

利；中国

的马燕萍和日本

的户漳井俊介都以绝对

的优势夺金。[SEP]

[CLS]7··北5京6······亚5洲6···························中

3国4·马1燕2萍2·日3本4·户1漳2井2俊

2介2··········[SEP]7

[CLS]7··北5京6······亚5洲6···························中

5国6·马1燕2萍

2·日3本6·户1漳2井2俊2介2··········[SEP]7

==========================

输出中

的第1行为原文，中间一行

为labels，即网络计算的目标，

第

3行为网络预测的结果，从

这个例子中看，网络预测

的结果和原

labels完全一致，没

有任何错误，成功地捕捉

到了人名“王建新”

“马燕萍

”“户漳井俊介”和地名“北京

”“亚洲”“中国”。

接下来再看以

下两个例子：

[CLS]天津儿童医

院儿科研究所研究的[UNK]人

类微小病毒b19外壳蛋白基

因vp2的克

隆与表达

[UNK]课题获

得成功，日前通过专家评

审。[SEP]

[CLS]7天3津4儿4童4医4院4儿4科4研

4究4所

4·············································[SEP]7

[CLS]7天3津4儿4童4医4院4儿4科

4研4究4所

4·············································[SEP]7

==========================

[CLS]△部队作家艾奇的

报告文学新著《金陵桂冠

》近日由江苏文艺出版社

出版。

[SEP]

[CLS]7·····艾1奇2········金5陵6······江3苏4文4艺

4出4版4社

4···[SEP]7

[CLS]7·····艾1奇2········金5陵6······江3苏4文

4艺4出4版4社

4···[SEP]7

==========================

成功捕捉到了

组织机构名“天津儿童医

院儿科研究所”“江苏文

艺

出版社”，以及人名“艾奇”和

地名“金陵”。

接下来再看以

下3个错误的例子：

 [CLS]本报讯

6月20日，红双喜中国乒乓球

俱乐部甲级联赛大战7场

，掀起一个小

高潮。[SEP]

[CLS]7·········红3双4喜

4中5国6·······················[SEP]7

[CLS]7·········红3双4·中5国6·······················[SEP]7

==========================

[CLS]昨天，他们

对喀麦隆的比赛，则受到

此间舆论的好评。[SEP]

[CLS]7······喀3麦4隆

4···············[SEP]7

[CLS]7······喀5麦6隆6···············[SEP]7

==========================

[CLS]我们常常一早从

桥儿沟鲁艺出发，通过飞

机场，过延河到文化俱乐

部；往往演出

到深夜才又

经过

飞机场，踏着寂静和

曲折的山路返回鲁艺。[SEP]

[CLS]7·······桥

5 儿 6 沟

6 鲁 3 艺 

4··········延5

河

6···································鲁3艺4·[SEP]7

[CLS]7·······桥5 儿

6 沟 6

鲁 4 艺 4··········延5河4··化

4··部

4·····························鲁5艺4·[SEP]7

==========================

在

第1个例子中，组织机构名

“红双喜”被捕捉成了“红双

”，

少了一个字。

在第2个例子

中，地名“喀麦隆”在数据集

中被错误地标记为组

织

机构名，但其实应该是地

名，所以这是一个数据集

本身的错误，而

在网络的

计算结果中纠正了这个

错误。可见网络不仅有高

正确率，而

且有纠正数据

错误的能力。

在第3个例子

中网络捕捉到了地名“桥

儿沟”“延河”和组织机

构名

“鲁艺”，但也错误捕捉到了

单个字的“化”和“部”。

以上是

一些典型的错误。

11.5 小结

本

章使用TensorFlow框架再次实现了

命名实体识别任务，通过

这

个例子演示在TensorFlow框架下

使用HuggingFace的方法。

第12章　使用自

动模型

12.1 任务简介

通过前

面的几个实战任务，相信

读者已经发现使用HuggingFace

训练

NLP模型的一般形式，大体上

可以分为以下几个步骤

：

(1)准备数据集。

(2)加载预训练

模型。

(3)定义下游任务模型

。

(4)执行训练和测试。

其中预

训练模型一般起将文本

特征抽取为向量的作用

，在下游任

务模型中使用

抽取好的特征向量执行

分类及回归等任务。

前面

几个任务是通过手动定

义的方式获得下游任务

模型的，针对

一些常见的

任务，HuggingFace提供了预定义的下

游任务模型，包括

以下任

务类型：

(1)预测下一个词。

(2)文

本填空。

(3)问答任务。

(4)文本摘

要。

(5)文本分类。

(6)命名实体识

别。

(7)翻译。

以上是针对文本

常见的任务，事实上HuggingFace不仅

支持处理

文本数据，还能

处理声频和图像数据，但

暂时让我们聚焦在文本

任务

上。

在本章中将会以

文本分类任务为例演示

HuggingFace预定义的下

游任务模型

的使用方法。

使用预定义

的下游任务能够给我们

提供一种思路，通过阅读

预定

义模型的源代码，可

以查看HuggingFace在实现特定的下

游任务时是

如何定义模

型的，进而可以照猫画虎

，定义自己的模型。

12.2 数据集

介绍

本章所使用的数据

集依然是ChnSentiCorp数据集，在前面

的几

个章节中已经反复

使用过此数据集，相信读

者已经很熟悉这个数据

集

了，此处不再赘述。本次

任务的部分数据样例见

表12-1，通过该表读

者可对本

次任务数据集有直观的

认识。

表12-1 ChnSentiCorp数据集数据样例

12.3 模型架构

和以往的任务

不同，本章不再手动定义

下游任务模型，而是使用

HuggingFace预定义的文本分类任务

模型。但是在该模型内部

，依然

包括预训练模型和

下游任务模型两部分，只

是HuggingFace通过API

的方式对调用者

隐藏了具体的细节，但作

为调用者应该做到心中

有

数，认识到该模型仍然

是一个两段式的模型结

构。

为了体现自动模型的

封装性，图12-1中并没有画出

自动模型内部

的细节。

图

12-1

使用自动模型的计算过

程

12.4 实现代码

12.4.1 准备数据集

1.使用编码工具

和以往所

有的任务一样，在准备数

据集的过程中依然需要

用到编

码工具，代码如下

：

#第12章/加载编码工具

from transformers import BertTokenizer

token = BertTokenizer.from_pretrained('bert-base-chinese')

token

运行

结果如下：

PreTrainedTokenizer(name_or_path='bert-base-chinese',

vocab_size=21128,

model_max_len=512, is_fast=False, padding_side='right',

truncation_side='right',

special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]',

'pad_token': '[PAD]',

'cls_token': '[CLS]', 'mask_token': '[MASK]'})

2.定义数据集

在

本

章 中 ， 不 再 把

HuggingFace 数 据 集 封

装 成

PyTorch 的

Dataset对象，而是直接使

用HuggingFace的数据集对象，代码如

下：

#第12章/加载数据集

from datasets

import load_from_disk

dataset = load_from_disk('./data/ChnSentiCorp')

dataset

运行

结果如下：

DatasetDict({

train: Dataset({

features: ['text',

'label'],

num_rows: 9600

})

validation: Dataset({

features: ['text', 'label'],

num_rows: 0

})

test: Dataset({

features: ['text', 'label'],

num_rows:

1200

})

})

3.定义计算设备

定义本次任务中要使用

的计算设备，代码如下：

#第

12章/定义计算设备

device

= 'cpu'

if torch.cuda.is_available():

device =

'CUDA'

device

运行结

果如下：

'CUDA'

4.定义数据整理函

数

在本章中使用的数据

整理函数的代码如下：

#第

12章/数据整理函数

def collate_fn(data):

sents = [i['text']

for i in data]

labels =

[i['label'] for i in data]

#编码

data = token.batch_encode_plus(batch_text_or_text_pairs=sents,

truncation=True,

padding=True,

max_length=512,

return_tensors='pt')

#转

移到计算设备

for k, v in

data.items():

data[k] = v.to(device)

data['labels'] =

torch.LongTensor(labels).to(device)

return data

5.定义数据

集加载器

数据集加载器

代码如下：

#第12章/数据集加

载器

loader = torch.utils.data.DataLoader(dataset=dataset['train'],

batch_size=16,

collate_fn=collate_fn,

shuffle=True,

drop_last=True)

len(loader)

运行结果如下：

600

定义

好了数据集加载器之后

，可以查看一批数据样例

，代码如

下：

#第12章/查看数据

样例

for i, data in enumerate(loader):

break

for k, v in data.items():

print(k, v.shape)

运行结果如下：

input_ids torch.Size([16, 235])

token_type_ids torch.Size([16, 235])

attention_mask torch.Size([16, 235])

labels torch.Size([16])

12.4.2 加载

自动模型

针 对

文 本 分 类

任 务 ，

使 用 HuggingFace 提 供 的

AutoModelForSequenceClassification工具类

加载自动模型，代码

如下

：

#第12章/加载预训练模型

from transformers import

AutoModelForSequenceClassification

#加

载模型

model =

AutoModelForSequenceClassification.from_pretrained('bert-base￾chinese',

num_labels=2)

#设定计算设备

model.to(device)

#统

计参数量

print(sum(i.numel() for i

in model.parameters()) / 10000)

AutoModelForSequenceClassification工具类有两个

主要的参

数，分别为要使

用的backbone网络名称和分类的

类别数量，在代码

的最后

输出了模型的参数量，运

行结果如下：

10226.9186

可见bert-base-chinese模型的

参数量约为1亿个。

如前所

述，在自动模型中其实依

然是backbone网络，后续再接

下游

任务模型，可以通过输出

模型本身查看模型的结

构，代码如下：

model

运行结果如

下：

BertForSequenceClassification(

(bert): BertModel(

...

)

(DropOut): DropOut(p=0.1, inplace=False)

(classifier): Linear(in_features=768, out_features=2,

bias=True)

)

由于输出的内容很长

，此处省略了backbone网络的内部

细节，

可以看到自动模型

的内部使用的backbone网络是BERT模

型，另外还

有DropOut层和Linear层，很显

然其中的Linear层是用来做二

分类

的。

加载好模型后，可

以进行一次试算，观察模

型的输入和输出，代

码如

下：

#模型试算

out = model(**data)

out['loss'], out['logits'].shape

运行结果如

下：

(tensor(0.7723, grad_fn=<NllLossBackward0>), torch.Size([16,

2]))

可以看到输出的内容

包括loss和分类的结果，其中

loss只有在入

参中包括labels时才

会有值，显然模型需要有

labels才能计算loss。如

果读者去翻

看自动模型的内部代码

，则可以发现，自动模型计

算的是

交叉熵损失。

12.4.3

训练

和测试

1.训练

由于本章使

用的是自动模型，所以我

们跳过了定义下游任务

模型

的步骤，现在可以训

练模型了，代码如下：

由于

自动模型自身包括计算

loss的功能，所以在训练函数

中不需

要手动计算loss，直接

使用自动模型计算出来

的loss执行梯度下降即

可，十

分方便。

训练过程的输出

见表12-2。

表12-2 训练过程的输出

续表

从表12-2可以看出，模型

的预测正确率在缓慢上

升，并且能够观

察到loss随着

训练的进程在不断地下

降，学习率也如预期在缓

慢地下

降。

2.测试

最后，对训

练好的模型进行测试，以

验证训练的有效性，代码

如

下：

运行结果如下：

0.89375

最终

模型取得了约89.4%正确率的

成绩。

12.5 深入自动模型源代

码

看完以上的例子，也许

有的读者会对自动模型

内部的实现感兴

趣，这里

简要介绍HuggingFace内部的代码运

行流程，以大致了解使

用

自动模型时HuggingFace是如何实现

的。

1.加载配置文件过程

首

先来看加载配置文件的

过程，代码如下：

 #第12章/加载

预训练模型

from transformers

import AutoModelForSequenceClassification

#加载模型

model =

AutoModelForSequenceClassification.from_pretrained('bert-base￾chinese',

num_labels=2)

当

执 行 这 段 代

码 时 ， 调 用 了

transformers/models/auto/auto_factory.py

文 件 中 的

_BaseAutoModelClass类的from_pretrained()函数。进入

该函数

后，首先根据模型

的名字，在线加载了该模

型的配置文件，关键代码

如下：

config, kwargs = AutoConfig.from_pretrained(

pretrained_model_name_or_path,

return_unused_kwargs=True,

trust_remote_code=trust_remote_code,

**kwargs)

返回结果中的kwargs不重

要，需要重点关注config对象，如

果打

印该对象，则内容如

下：

BertConfig {

"_name_or_path": "bert-base-chinese",

"architectures": [

"BertForMaskedLM"

],

"attention_probs_DropOut_prob": 0.1,

"classifier_DropOut": null,

"directionality": "bidi",

"hidden_act": "gelu",

"hidden_DropOut_prob": 0.1,

"hidden_size": 768,

"initializer_range": 0.02,

"intermediate_size": 3072,

"layer_norm_eps": 1e-12,

"max_position_embeddings": 512,

"model_type": "bert",

"num_attention_heads": 12,

"num_hidden_layers": 12,

"pad_token_id": 0,

"pooler_fc_size": 768,

"pooler_num_attention_heads": 12,

"pooler_num_fc_layers": 3,

"pooler_size_per_head": 128,

"pooler_type": "first_token_transform",

"position_embedding_type": "absolute",

"transformers_version": "4.18.0",

"type_vocab_size": 2,

"use_cache": true,

"vocab_size": 21128

}

从该对象可以看出，该

对象内部存储了初始化

模型时所需要的所

有参

数，主要参数如下。

(1)_name_or_path=bert-base-chinese：定义了

模型的名字，

也就是checkpoint。

(2)attention_probs_DropOut_prob=0.1：注意

力层DropOut

的比例。

(3)hidden_act=gelu：隐藏层的激

活函数。

(4)hidden_DropOut_prob=0.1：隐藏层DropOut的比例。

(5)hidden_size=768：隐

藏层神经元的数量。

(6)layer_norm_eps=1e-12：标准

化层的eps参数。

(7)max_position_embeddings=512：句子的最大

长度。

(8)model_type=bert：模型类型。

(9)num_attention_heads=12：注意力层

的头数量。

(10)num_hidden_layers=12：隐藏层层数。

(11)pad_token_id=0：PAD的

编号。

(12)pooler_fc_size=768：池化层的神经元数

量。

(13)pooler_num_attention_heads=12

： 池 化 层 的 注

意 力 头

数

。

(14)pooler_num_fc_layers=3：池化层的全连接神经网

络层数。

(15)vocab_size=21128：字典的大小。

2.深入

加载配置文件过程

有些

读者可能会对该配置文

件的加载过程感兴趣，HuggingFace

是

如何根据一个模型的名

字加载到它对应的配置

文件的呢？

如 果 继

续 深 入

该 函 数 ，

则 可 以 追 踪 到

transformers/configuration_utils.py文

件中的PretrainedConfig类

的_get_config_dict()函数，该函数

中的关键代码如下：

config_file = hf_bucket_url(pretrained_model_name_or_path,

filename=configuration_file,

revision=revision, mirror=None)

在 这

段 代 码

中 ， 调 用 了 hf_bucket_url()

函 数 ， 入

参 中 的

pretrained_model_name_or_path 即 为 模 型 的

名 字

，

configuration_file的值等于config.json。

hf_bucket_url()函数做的事情

很简单，使用了一个字符

串模板，

把模型的名字和

configuration_file的值填入，获得配置文件

的http地

址，关键代码如下：

return HUGGINGFACE_CO_PREFIX.format(model_id=model_id,

revision=revision,

filename=filename)

这

段 代

码 中 的 HUGGINGFACE_CO_PREFIX 为 常

量 ， 值 为

https://huggingface.co/{model_id}/resolve/{reversion}/{filenam

e}。很显然，这是一个字符串

模板，只要把其中的model_id、

reversion、filename替换

即可获得配置文件的http地

址。

model_id即模型的名字，reversion的值没

有定义，默认使用

main，filename的值为

config.json，所以全部替换完成后的

配置文

件 的 http 地

址 为 https://huggingface.co/bert￾basechinese/resolve/main/config.json，如果

在浏览器中访问该地

址

，则可得到配置文件的内

容，如图12-2所示。

至此，对于配

置文件的加载过程，相信

读者已经理解，把模型的

名字填入一个http地址模板

中，即可获得配置文件的

http加载地址。

按照这个理论

，把模板中的模型名字替

换为其他的模型名字，即

可加载其他模型的配置

文件。使用模型roberta-base实验一次

，模板

替 换 后 的 访 问

地 址

为

https://huggingface.co/robertabase/resolve/main/config.json。

在浏览器中访问的结

果如图12-3所示。

从图12-3可以看

出，访问结果成功地加载

了roberta-base模型的

配置文件。

3.初始

化模型过程

加载完配置

文件，下一步就是根据配

置文件初始化模型了，这

一

步的关键代码依然在

transformers/models/auto/auto_factory.py文

件的_BaseAutoModelClass类的from_pretrained()函数中。在加

载

完配置文件得到config对象

后，使用该对象初始化了

模型，关键代码

如下：

model_class = _get_model_class(config, cls._model_mapping)

return

model_class.from_pretrained(pretrained_model_name_or_path,

*model_args, config=config, **kwargs)

图12-2 访

问配置文件http地址的结果

图12-3 访问roberta-base模型的配置文件

执行这段代码中的第1行

后，获得变量model_class，它的值等于

<class

'transformers.models.bert.modeling_bert.BertForSequenceClass

ification'>，这就是要初始化的模型

类，下一步调用了该模型

的

from_pretrained函数，并把模型的名字

传入，跟踪该步可以到达

transformers/models/bert/modeling_bert.py 文 件 的

BertForSequenceClassification 类 的 __init__() 函 数

。 该 类 继

承 自

PyTorch的模型对象，所以它

也是一个PyTorch模型。

__init__()函数中的

关键代码如下：

 self.num_labels = config.num_labels

self.config

= config

self.bert = BertModel(config)

classifier_DropOut

= (

config.classifier_DropOut if config.classifier_DropOut is

not

None else

config.hidden_DropOut_prob

)

self.DropOut

= nn.DropOut(classifier_DropOut)

self.classifier = nn.Linear(config.hidden_size,

config.num_labels)

从这段代

码可以看出，该模型中包

括一个BERT模型和一个全连

接

神经网络。很显然该模

型的计算过程就是使用

BERT模型抽取文本的特

征向

量，再把特征向量输入全

连接神经网络进行分类

计算。

以上推测可以通过

阅读该模型的forward()函数进行

验证，关键代

码如下：

outputs

= self.bert(

input_ids,

attention_mask=attention_mask,

token_type_ids=token_type_ids,

position_ids=position_ids,

head_mask=head_mask,

inputs_embeds=inputs_embeds,

output_attentions=output_attentions,

output_hidden_states=output_hidden_states,

return_dict=return_dict,

)

pooled_output = outputs[1]

pooled_output = self.DropOut(pooled_output)

logits = self.classifier(pooled_output)

在这

段代码中，首先使用BERT模型

抽取了文本的特征向量

，再在

特征向量上计算DropOut和

分类，这和之前预想的计

算过程完全一

致。

HuggingFace的模型

还有计算loss的功能，loss的计算

同样是在

forward()函数中，计算loss的

关键代码如下：

loss_fct = CrossEntropyLoss()

loss

= loss_fct(logits.view(-1, self.num_labels),

labels.view(-1))

可以看到

，在文本分类任务中比较

简单，计算CrossEntropyLoss

即可。

4.加载预训

练参数

至此，模型初始化

完毕了，但是此时的模型

还只是一个框架而

已，没

有加载预训练参数，模型

中所有的参数还没有被

训练，接下来

就要加载预

训练参数，填入模型中。

这

项 工

作 是 在 transformers/modeling_utils.py 文 件

的

PreTrainedModel类的

from_pretrained()函数中完成的，关键代码

如下：

archive_file = hf_bucket_url(

pretrained_model_name_or_path,

filename=filename,

revision=revision,

mirror=mirror,

)

函数hf_bucket_url()在之前加载模

型配置时已经介绍了，它

的功

能是对http模板中的各

个占位符进行替换，得到

可访问的http地址。

上次调用

该函数是要获得模型配

置文件的访问地址，而这

次是要获得

模型参数文

件的访问地址。

参数中的

pretrained_model_name_or_path很显然就是模型的

名字

，如果filename的值等于PyTorch_model.bin且revision的值等

于None，则在替换时默认使用

main。

执 行

完 成 后 便 可 得

到 模

型 配 置 文 件

的 访 问 地 址

https://huggingface.co/bert-base-

chinese/resolve/main/PyTorch_model.bin。

由于模型的参数文件往

往比较大，如果每次都在

线加载，则比较

浪费资源

，所以在首次加载后会被

缓存在本地磁盘，并且在

加载该在

线文件前会先

检查本地缓存，如果之前

已经被缓存，则使用本地

缓存

即可，不需要再次在

线加载，以节约资源；反之

，如果没有缓存，则

需要在

线加载参数文件，并缓存

到本地磁盘。执行该过程

的关键代码

如下：

resolved_archive_file = cached_path(

archive_file,

cache_dir=cache_dir,

force_download=force_download,

proxies=proxies,

resume_download=resume_download,

local_files_only=local_files_only,

use_auth_token=use_auth_token,

user_agent=user_agent,

)

执行该

函数，可能会使用本地缓

存或在线加载参数文件

，无论是

哪 种 情 况 ，

执 行 完

成 后 ， 都

会 得 到 本 地 的

缓

存 路 径 ， 值

为

/root/.cache/huggingface/transformers/58592490276d9ed1e

8e

33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197

bfe5d6a318c2833172d6757ccc7e4

9f692cb949a6fabf560cee81508。

这是一个本

地的磁盘路径，接下来就

可以加载该文件中的参

数

了，关键代码如下：

state_dict = load_state_dict(resolved_archive_file)

这里

使用了之前得到的缓存

路径，加载为PyTorch的配置文件

，

接下来需要把参数文件

填入模型中，关键代码如

下：

model, missing_keys, unexpected_keys, mismatched_keys,

error_msgs =

cls.

_load_pretrained_model(

model,

state_dict,

resolved_archive_file,

pretrained_model_name_or_path,

ignore_mismatched_sizes=ignore_mismatched_sizes,

sharded_metadata=sharded_metadata,

_fast_init=_fast_init,

)

至此，就得到了一个填

好了预训练参数的模型

，它使用一个BERT

模型作为backbone，并

添加了一个全连接神经

网络作为下游任务模

型

，能够完成文本的分类任

务。

12.6 小结

本章通过一个文

本分类的例子演示了HuggingFace自

动模型的使

用方法，使用

HuggingFace自动模型不需要手动计

算loss，也不需要

手动定义下

游任务模型，使用起来十

分方便。对于进阶读者，还

可以

通过阅读自动模型

的代码了解HuggingFace是如何实现

不同任务的下

游任务模

型的，进而提高自身的建

模能力。

预训练模型底层

原理篇

第13章　手动实现Transformer

13.1

Transformer架

构

完成了上面的实验，有

些读者可能会对BERT的内部

是如何计算的

感兴趣，为

什么BERT能够很好地抽取文

本特征呢？要讲清楚BERT的

工

作原理，需要先理解BERT的前

身Transformer。BERT模型的构建

使用了Transformer的

部分组件，如果理解了Transformer，则

能很

轻松地理解BERT。所以在

本章中，将讲解Transformer模型的设

计思

路和计算方法。

Transformer最初

的设想是作为文本翻译

模型使用的，在本章中

将

延续它设计的初衷，将使

用Transformer实现一个简单的翻译

任

务。

在正式开始本章的

任务之前，先从架构层面

看一看Transformer

的设计思路。让我

们从图13-1开始。

图13-1 黑盒结构

当我们对Transformer一无所知时，我

们不理解它内部的计算

过

程，它对我们来讲是个

黑盒结构，我们输入一句

话，它会输出一句

话，而输

入和输出之间刚好形成

了原文和译文的关系，如

图13-2所

示，这就是广义上的

翻译任务。

现在把Transformer这个黑

盒打开一点点，会看到它

内部有一个

编码器和一

个解码器，很显然，编码器

负责读取原文，从原文中

抽取

特征后交给解码器

生成译文。

现在把编码器

和解码器都再打开一点

点，看一看它们的内部构

造，如图13-3所示。

从图13-3可以看

出，编码器和解码器的内

部都是多层结构，图中

画

出的是3层，实际情况中可

能多于这个数字。编码器

在计算时，多

层编码器是

前后串行的结构，最后一

层抽取的文本特征作为

最终的文

本特征。解码器

同样是前后串行的结构

，每次的计算输入除了前

一层

的计算输出，还包括

了编码器抽取的文本特

征。

如果要把上面的计算

过程类比成人类思考的

过程，则可以设想这

样一

个场景，一个人看到了一

句中文，他的任务是把这

句中文翻译成

英文，他大

体上需要分两步来完成

这项任务，首先需要把中

文读到大

脑中，读的过程

往往不是一次完成的，人

类在做这件事情时往往

依靠

潜意识，所以很难意

识到读的过程需要很多

次，同样一句话，第1次

读和

第2次读往往有不同的感

觉，这就相当于Transformer中的多层

编

码器。在读取文本后，人

类需要组织语言把这句

话翻译成英文，翻译

的过

程同样需要多次“改稿”，最

终人类在大脑中完成翻

译工作，组

织了一句满意

的译文，相当于Transformer中的多层

解码器。

图13-2

编码器和解码

器结构

图13-3 编码器和解码

器展开图

现在更加深入

一点点，打开每层编码器

和解码器，看一看它们的

内部构造，如图13-4所示。

图13-4 编

码器和解码器的内部结

构

从图13-4中可以看出，编码

器层的计算包括两步，分

别是自注意

力层计算和

全连接层计算。再看一看

解码器层的计算，会发现

解码器

层的计算和编码

器层的计算过程很相似

，只是多了一层，即编解码

注

意力层计算，这些层的

计算过程我们稍后都会

详细讲解。

13.2 注意力

13.2.1 为什么

需要注意力

在讲解自注

意力的计算过程之前，先

介绍为什么需要自注意

力，

以及自注意力的计算

起什么作用。Transformer的设计初衷

是完成翻

译任务，在翻译

任务中，最重要的难点是

找出词与词之间的对应

关

系，如图13-5所示。

从图13-5可以

看出，原文和译文之间的

词有对应关系，需要注意

的是图13-5仅为示意，并非真

实的对应关系。事实上在

真实的注意力

计算中原

文的所有词和译文的所

有词是完全连接的，此处

以原文中的

一个词fox单独

举例，如图13-6所示。

图13-5 自注意

力求词与词之间的对应

关系

图13-6

一个词的自注意

力权重

从图13-6可以看出，原

文中的fox是和译文中的所

有词求注意力

权重的，只

是权重值有大有小，图中

以连线的颜色深浅来体

现，可以

看到原文中的fox和

译文中的“狐狸”注意力权

重最大，这告诉了

Transformer模型原

文中的fox应该翻译为译文

中的“狐狸”。

如果把原文中

的所有词的注意力权重

都计算出来，并且隐藏权

重

低的对应关系，就得到

了图13-5，有了这样的对应关

系能帮助

Transformer更好地捕捉到

翻译任务中的词对应关

系，进而提高翻译

的质量

。

13.2.2 注意力的计算过程

现

在

已 经 看 过 Transformer 的

内 部 结 构 了

， 大

概 知 道 了

Transformer的计算过程

，但是还不理解什么是自

注意力的计算。为

了讲解

清楚自注意力的计算过

程，设想一下，现在有一句

话，这句话

中有两个词，分

别是a和b，为了便于后续的

计算，这句话已经被分词

且转换成了向量形式，如

图13-7所示。

按照Transformer的计算过程

，要翻译这句话首先要把

这句话输

入第1层编码器

进行计算，而第1层编码器

的第1步计算就是要对这

句

话计算自注意力。

在计

算自注意力之前，首先需

要对这两个词的向量进

行投影，如

何做到这一点

呢？很简单，使用一个矩阵

和词向量相乘即可，此处

给

这个矩阵起名为WQ，意思

是Weight of Queries，如图13-8所示。

图13-7 词向量形

式的一句话

图13-8

使用WQ矩阵

投影词向量得到Queries向量

从

图13-8可以看出，词向量本身

是1×4向量，WQ矩阵的形状是

4×3，两

者相乘之后等于1×3向量，即

图中的Queries。

与生成Queries的过程相

同，如果再多两个矩阵，就

可以再多生

成两个词向

量。此处把这两个矩阵分

别称为WK(Weight of

Keys)和

WV(Weight of Values)。现在根据一组

词向量，通过这3个矩阵，投

影得到三组词向量，分别

是Queries、Keys、Values，后续将简称为

Q、K、V，如图13-9所

示。

图13-9

生成Queries、Keys和Values词向量

得到

Q、K、V词向量以后，就可以开始

计算自注意力了，计算过

程如图13-10所示。

从图13-10可以看

出，自注意力是按照每个

词分别计算的，先来

看a这

个词的计算过程。

(1)当前词

的Q和每个词的K相乘，在这

个简单的例子当中只有

两

个词，意味着也只有两

组Q、K、V，所以此处要进行的计

算有

q1×k1=112，q1×k2=96。计算的结果仅仅是

示例。

(2)上一步的计算结果

除以词向量编码维度的

平方根，这里假设词

向量

编码的维度是64，64的平方根

是8，所以应该是112÷8=14，

96÷8=12。

(3)对上一步

计算的结果再计算Softmax，Softmax(14,12)=

[0.88,

0.12]。

图13-10 自

注意力的计算过程

(4) 上 一

步

计 算 的 结 果 和

每 个 词

的 V 相 乘

， 所 以 应 该 是

v1=0.88×v1，v2=0.12×v2。

(5)上一

步计算的结果求和，即为

当前词的注意力分数，所

以a这

个词的注意力分数

为z1=v1+v2。

以上描述的是词a，针对

词b，计算的过程是一样的

。

13.2.3 注意力计算的矩阵形式

从上面的描述可以看出

，每个词的注意力分数的

计算过程是相互

独立的

，词和词之间没有前后相

互依赖性，所以可以并行

计算，这也

是Transformer抽取文本特

征的效率高于RNN的原因。考

虑到并行性

这一点，可以

把以上计算流程转换成

矩阵计算，如图13-11所示。

图13-11 自

注意力的矩阵计算形式

按照之前的描述，使用WQ、WK、WV三

个矩阵把词向量投影，

得

到了Q、K、V词向量。再使用Q、K、V词向

量计算得到一组注意力

分数。

13.2.4 多头注意力

当只有

一组WQ、WK、WV矩阵时，只能计算一

组注意力分数，

称为单头

注意力。

现在设想一下，如

果有多组WQ、WK、WV矩阵，就可以针

对一

句话计算多组注意

力分数，称为多头注意力

。

与单头注意力相比，多头

注意力往往能抽取更丰

富的文本特征信

息。两者

的对比如图13-12所示。

图13-12 单头

注意力对比多头注意力

现在看一个多头注意力

的完整计算过程，如图13-13所

示。

图13-13 多头注意力的完整

计算过程

从图13-13可以看出

，多头注意力使用了多组

WQ、WK、WV矩

阵，投影得到了多组Q、K、V词

向量，再计算出多组Z。

此时

出现了一个问题，如何把

多组Z整合成一个注意力

分数呢？

自然的想法是把

多个Z左右拼合在一起，但

这会造成多头注意力的

头

数越多，Z的数量越多，最

后出现拼合得到的矩阵

越“宽”的问题。

所以在多头

注意力计算的最后，会使

用一个“又高又窄”的矩阵

和“又宽又扁”的Z矩阵相乘

，得到“不胖不瘦”的Z矩阵，即

最后的

注意力分数矩阵

。

13.3 位置编码

13.3.1 为什么需要位

置编码

看完了上面的计

算过程，相信仔细的读者

已经发现了一个问题，

之

前提到Transformer当中每个词的注

意力分数是单独计算的

，不依

赖其他的词，所以所

有词的注意力分数可以

并行计算，这提高了

Transformer计算

的效率，但是也造成了每

个词出现在句子的任何

位

置，计算出来的注意力

分数都一样的问题。为了

更清晰地说明这个问

题

，通过一个例子来说明，如

图13-14所示。

在图13-14中，两句话所

使用的词语完全一样，只

是组成句子的

顺序不同

，而且两句话的意思也一

样，在这样的情况下两句

话计算出

相同的注意力

分数是没有问题的，因为

两句话的意思相同，可以

使用

同一组注意力分数

来表示这两句话。这意味

着这两句话的翻译结果

将

相同，但是在有些情况

下交换词的顺序会导致

一句话的意思改变，例

如

图13-15中的例子。

图13-14

交换词序

意思不变的句子

图13-15 交换

词序句子的意思改变

在

如图13-15所示的例子中，两句

话依然使用了完全同样

的词，

只是组成句子的顺

序不同，和图13-14中的情况不

同，这次两句话的

意思由

于词序的不同而改变了

。此时两句话计算出同样

的注意力分数

将出现问

题，显然这两句话不应该

翻译出同样的译文。

综上

所述，Transformer不同于RNN，在计算每个

词的注意力分

数时不考

虑词的位置信息，所以需

要在词的编码中加入位

置信息，以

让处于不同位

置的词的编码有所不同

，相互区分。

13.3.2 位置编码计算

过程

为了做到这一点，Transformer的

做法是在词向量编码中

加入一

个位置编码的信

息，如图13-16所示。

图13-16 在词向量

编码中加入位置编码信

息

从图13-16可以看出，位置编

码信息是一个形状和词

向量编码一

样的向量，最

终的词向量编码等于原

始的词向量和位置编码

信息相

加。

位置编码矩阵

的计算公式如式(13-1)和式(13-2)所

示。

式(13-1)和式(13-2)中的pos表示词的

位置，i表示词向量编码的

位置，d_model表示词向量编码的

位置。

从式(13-1)和式(13-2)可以看出

，位置编码矩阵的尺寸可

以扩展

到无穷大，结合实

际来讲，位置编码矩阵的

行数不能少于句子中词

的

数量，位置编码矩阵的

列数应该等于词向量编

码的维度，实际计算可

参

照图13-17。

图13-17 词向量和编码矩

阵的对应关系

在图13-17中假

设一句话有4个词，将每个

词编码成5维的向量，

则这

句话的编码矩阵和图中

所示相同，是一个4×5的矩阵

，即每个词

对应矩阵中的

一行，每维度的词向量编

码对应矩阵中的一列。相

对应

的位置编码矩阵也

是同样的形状，两个矩阵

的形状相同，可以执行相

加操作，相加之后就是需

要的最终编码矩阵了。

位

置编码矩阵的偶数列使

用式(13-1)计算，奇数列使用式

(13-2)

计算，如果把位置编码的

光谱画出，则将如图13-18所示

。

从图13-18可以看出，位置编码

矩阵的每列都是一个周

期函数，

数值会从大变小

，再从小变大，周期变化，并

且越靠前的列震荡的周

期越短，越往后的列震荡

的周期越长，越趋向于稳

定。

图13-18

位置编码矩阵的光

谱

位置编码矩阵在Transformer当中

是一个常量，一次计算完

成

后，后续不会再有任何

更新，所以位置编码矩阵

本身并不是一个可学

习

的参数，这和Transformer的一些延伸

模型相区别，例如在BERT和

GPT2当

中就把位置编码矩阵当

作可学习的参数，会随着

模型的训练而

不断变化

。

13.4 MASK

13.4.1 PAD MASK

在自然语言处理任务中

，为了提高效率，往往成批

地处理句子，

要提高计算

效率，就要把批次中的句

子组合成矩阵进行计算

，但是在

一个批次中，句子

往往有长有短，为了把长

短不一的句子补充成同

样

的长度，就需要对短的

句子补充PAD，这些PAD本身没有

任何意义，

仅仅是为了让

短的句子加长，以和批次

中长的句子组合成矩阵

，如图

13-19所示。

图13-19

对批次中的

句子补充PAD，直到等长

在Transformer当

中计算时，为了忽略这些

没有意义的PAD，就

需要使用

MASK遮挡这些PAD，如果不这样做

，模型则可能会花很多

时

间去研究PAD是什么，为了防

止模型做这样的无用功

，所以需要

PAD MASK，如图13-20所示。

图13-20 PAD MASK

在

图13-20中，虚拟了两句话，这两

句话在句尾各有1个PAD，现

在

我们假设竖着的句子（列

）为原文，横着的句子（行）为

译文，在

计算原文对译文

的注意力分数时，对译文

当中是PAD的词位置使用

MASK遮

罩，这意味着这些位置的

注意力分数是0。

也就是说

，原文中的词1、词2、PAD只会计算

针对译文中的词

1、词2的注

意力分数，而不会计算针

对译文PAD的注意力。

13.4.2 上三角

MASK

除

了 PAD MASK 之 外 ，

Transformer 当 中 还 有 一

个

上 三 角

MASK，如图13-21所示。

图13-21 上三

角MASK

需要上三角MASK的原因是

在Transformer的解码器中，需要根

据

当前词解码出下一个词

，为了加速Transformer的训练，将会使

用

强制教学的方法，所以

在解码阶段，会把正确的

译文输入解码器中，

解码

器其实是在有正确答案

的情况下做题的，如果解

码器只是不断地

照抄答

案，则它能很轻易地取得

极高的分数，但很显然它

并没有学到

任何知识，这

会导致它在实际预测时

的准确率极低，这显然并

不是我

们想要的。

所以为

了防止解码器照抄答案

，需要使用上三角MASK对正确

答

案进行部分遮挡，这样

解码器在预测第2个词时

，只能看到第1个词的

答案

，第1个词以后的答案是不

可见的，这强制了解码器

必须自己预

测第2个词的

答案，在解码器给出了答

案以后，再根据第2个词的

答案

预测第3个词，以此类

推。

从上面的讲述能看出

来，解码器是一个词一个

词地依序预测的，

后一个

词的预测依赖于前一个

词的预测结果，这导致解

码的错误容易

累计，在前

一个词预测错误的情况

下，后续所有的词都会预

测错误，

从而导致解码的

训练效率太低，为了提高

训练的效率，在解码器每

预

测一个词后，无论错误

与否，都强制使用正确答

案预测下一个词，这

被称

为强制教学。强制教学确

保了解码器的错误不会

累计，无论前一

个词的预

测是否正确，都能使用正

确的词预测下一个词，从

而提高解

码器的训练效

率。

图13-21演示的是译文中没

有PAD的情况，如果译文中有

PAD，

则还需要对上三角MASK叠加

PAD MASK，如图13-22所示。

图13-22

上三角MASK叠加

PAD MASK

13.5 Transformer计算流程

13.5.1 编码器

讲解完

了上面Transformer当中的一些计算

细节之后，现在来从

整体

上看一下Transformer的计算流程，首

先看编码器的计算过程

，

如图13-23所示。

图13-23 编码器计算

流程

从图13-23可以看出，编码

器的计算流程如下：

(1)文本

编码成词向量之后和位

置编码矩阵相加，得到最

终编码向

量x1、x2。

(2)输入编码器

后，计算每个词的自注意

力分数，得到z1、z2。

(3)注意力分数

z1、z2和最终编码向量x1、x2相加，这

一步的目

的是做短接，防

止梯度消失。

(4)短接之后的

结果计算批量标准化，把

数值稳定为均值0，标准

差

为1的标准正态分布，计算

得到的结果重新赋值为

z1、z2。

(5)对z1和z2分别计算线性输出

。

(6)线性输出和z1、z2再次做短接

，并再次做批量标准化后

输出。

13.5.2 整体计算流程

以上

是编码器的计算流程，接

下来结合解码器看Transformer的

整

体计算流程，如图13-24所示。

图

13-24 Transformer整体计算流程

从图13-24可以

看出，在Transformer中编码器往往是

多层的，

每层编码器之间

是串行的关系，后一层编

码器的输入是前一层编

码器

的输出，最后一层编

码器的输出即为整体编

码器的输出，将作为每层

解码器的入参的一部分

。

现在再看一下解码器的

计算流程，解码器的计算

流程和编码器很

类似，只

是多了一层编解码注意

力层的计算，这一层也需

要用到编码

器输出的部

分，这一层的计算细节此

处不展开，稍后在代码中

将会看

得更加清楚。

和编

码器一样，解码器同样有

多层，每层之间是串行的

关系，最

后一层的输出再

经过线性层的计算，最后

使用Softmax()函数激活，

即为Transformer模型

的最终输出。

13.5.3 解码器解码

过程详细讲解

解码器的

解码过程是一个词一个

词地依序预测的，如图13-25所

示。

图13-25 解码器解码第2个词

在图13-25中，编码器的计算已

经完成，解码器需要根据

编码器

的计算结果解码

译文，解码器的计算往往

是从第2个词开始的，而不

会从第1个词开始，因为第

1个词往往是特殊符号，是

一个常量。现在

假设解码

器的输入包括第1个词[SOS]和

编码器的计算结果，解码

器将

解码出词A。

下一步解

码器要预测第3个词，解码

器的输入将包括前两个

词

[SOS]和A及编码器的计算结

果，解码器将解码出词B，如

图13-26所

示。

图13-26 解码器解码第

3个词

重复以上过程，直到

解码完成，如图13-27所示。

图13-27 解

码器解码完成

13.5.4

总体架构

最后给出谷歌官方的Transformer总

体架构图，仅供参考，如图

13-28所示。

图13-28 Transformer总体架构

13.6 简单翻

译任务

13.6.1

任务介绍

讲解完

了以上理论知识，现在将

手动实现一个Transformer模

型，完成

一个简单的翻译任务，以

更直观地理解Transformer的计算

过

程。

考 虑

到 自 然 语 言 的

复

杂 性 ， 而 且 本

章 的 目 标 是

要 理

解

Transformer的计算过程，所以

不会涉及太复杂的任务

，而是完成一

个尽量简单

的任务，以更清晰地观察

到Transformer的计算过程。

接下来介

绍本次任务中原文和译

文的生成策略，原文的生

成策略

如图13-29所示。

在本次

任务中，原文的生成策略

是从一个有限词表中按

一定的概

率随机采样，生

成随机长度的原文。这并

不是自然语言，但可以模

拟

一句自然语言而且足

够简单，便于我们观察Transformer的

计算过

程。

译文的生成策

略如图13-30所示。

图13-29 原文的生

成策略

图13-30 译文的生成策

略

出于简单起见，译文的

生成策略也简单清晰，译

文中第1个词双

写，剩下的

词和原文的词一一对应

，如果是数字，则取9以内的

互补

数，如果是字母，则取

大写，并且顺序是原文的

整体逆序。首字母双

写的

目的一方面是为了增加

对应复杂度，让这个任务

不至于过分简

单，另一方

面是为了让译文比原文

多一位，这会给后续的计

算提供方

便。

和大多数的

NLP任务一样，在本次任务中

也会对文本进行预处

理

，会对每一句文本添加首

尾标识符，此外由于文本

是随机长度的，

所以文本

是长短不一的，为了更方

便地处理这些文本，我们

会把文本

都补充到固定

的长度，如图13-31所示。

图13-31 文本

数据预处理

出了方便叙

述，后将使用x和y表示原文

和译文。

13.6.2 定义数据集

首先

把本次任务要使用的字

典定义出来，代码如下：

#第

13章/定义字典

vocab_x = '<SOS>,<EOS>,

<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,

f,g,h,

j,k,l,z,x,c,v,b,n,m'

vocab_x = {word: i for

i, word in

enumerate(vocab_x.split(','))}

vocab_xr =

[k for k, v in vocab_x.items()]

vocab_y = {k.upper(): v for k,

v in vocab_x.items()}

vocab_yr = [k

for k, v in vocab_y.items()]

print('vocab_x=',

vocab_x)

print('vocab_y=', vocab_y)

在这段代码

中分别定义了x和y的字典

，运行结果如下：

 vocab_x=

{'<SOS>': 0, '<EOS>': 1, '<PAD>': 2,

'0': 3, '1':

4, '2': 5,

'3':

6, '4': 7, '5': 8,

'6': 9, '7': 10, '8': 11,

'9': 12, 'q': 13,

'w': 14,

'e': 15,

'r': 16, 't': 17,

'y': 18, 'u': 19, 'i': 20,

'o': 21, 'p': 22,

'a': 23,

's': 24,

'd': 25, 'f': 26,

'g': 27, 'h': 28, 'j': 29,

'k': 30, 'l': 31,

'z': 32,

'x': 33,

'c': 34, 'v': 35,

'b': 36, 'n': 37, 'm': 38}

vocab_y= {'<SOS>': 0, '<EOS>': 1, '<PAD>':

2, '0': 3, '1': 4,

'2':

5, '3':

6, '4': 7, '5':

8, '6': 9, '7': 10, '8':

11, '9': 12, 'Q': 13,

'W':

14, 'E': 15,

'R': 16, 'T':

17, 'Y': 18, 'U': 19, 'I':

20, 'O': 21, 'P': 22,

'A':

23, 'S': 24,

'D': 25, 'F':

26, 'G': 27, 'H': 28, 'J':

29, 'K': 30, 'L': 31,

'Z':

32, 'X': 33,

'C': 34, 'V':

35, 'B': 36, 'N': 37, 'M':

38}

vocab_x和vocab_y分别代

表了x和y的字典，字典内容

就是简单

的每个词和某

个数字的对应关系，包括

特殊符号。为了方便以后

打印

预测结果，还构建了

两个逆字典，也就是通过

数字找到对应的原文。

接

下来定义生成数据的函

数，代码如下：

这个函数每

调用一次就按照之前所

说的策略生成一对x、y，运行

结果如下：

(tensor([ 0, 38, 36, 35, 36,

30, 34, 5, 37, 34, 31,

38, 28,

35, 37, 38, 24,

34, 28, 30, 35, 33, 27,

34, 25, 36, 12, 22, 37,

24, 26, 27, 31, 

8,

28, 19,24, 30, 27, 23, 24,

1, 2, 2, 2, 2, 2,

2, 2, 

2]),

tensor([ 0,

24, 24, 23, 27, 30, 24,

19, 28, 7, 31, 27, 26,

24,

37, 22, 3,

36, 25,

34, 27, 33, 35, 30, 28,

34, 24, 38, 37, 35, 28,

38, 31,

34,

37, 10, 34,

30, 36, 35, 36, 38, 1,

2, 2, 2, 2, 2, 2,

2, 

2]))

现在来看一些

x和y的例子，为了便于观察

，已经反编码成了文字

形

式，见表13-1。

表13-1

数据样例

续表

接下来可以定义数据集

及数据集加载器，代码如

下：

#第13章/定义数据集和加

载器

#定义数据集

class Dataset(torch.utils.data.Dataset):

def __init__(self):

super(Dataset, self).__init__()

def __len__(self):

return 100000

def __getitem__(self, i):

return

get_data()

#数据集

加载器

loader = torch.utils.data.DataLoader(dataset=Dataset(),

batch_size=8,

drop_last=True,

shuffle=True,

collate_fn=None)

#查看数据样例

for i,

(x, y) in enumerate(loader):

break

x.shape,

y.shape

在

数据集中，数据的总量定

义为10万条，事实上由于数

据是随机

生成的，其实数

据有无穷多条，但PyTorch在定义

数据集时需要有一

个明

确的数量，所以此处定义

为10万条。每次获取数据时

，就调用定

义好的生成数

据函数，生成一对x、y即可。

数

据集加载器定义了每个

批次中包括8对x和y。

在代码

的最后获取了一批x、y，并输

出了形状，运行结果如下

：

(torch.Size([8, 50]), torch.Size([8, 51]))

可以观察到y的长度比x多

一位，这是故意为之的设

计，以便于后

续的计算。

13.6.3 定

义MASK函数

接下来定义两个

MASK函数，先定义PAD MASK函数，代码如

下：

#第13章/定义mask_pad函数

def mask_pad(data):

#b句话，每

句话50个词，这里是还没embed的

#data = [b, 50]

#判断每个词是不是<PAD>

mask = data == vocab_x['<PAD>']

#[b, 50] -> [b, 1, 1,

50]

mask = mask.reshape(-1, 1, 1,

50)

#在计

算注意力时，计算50个词和

50个词相互之间的注意力

，所以是个50*50的矩阵

#是PAD的列

为True, 意味着任何词对PAD的注

意力都是0

#但是PAD本身对其

他词的注意力并不是0

#所

以是PAD的行不为True

#复制n次

#[b, 1, 1, 50] ->

[b, 1, 50, 50]

mask =

mask.expand(-1, 1, 50, 50)

return mask

mask_pad(x[:1])

运

行结果如下：

tensor([[[[False, False, False, …,

False, True, True],

[False, False, False,

…, False, True, True],

[False, False,

False, …, False, True, True],

...,

[False, False, False, …, False, True,

True],

[False, False, False, …, False,

True, True],

[False, False, False, …,

False, True, True]]]])

在这段代码

中，根据输入的句子中的

每个词是否是PAD，选择是

否

MASK某一列，最终的输出形状

是b×1×50×50，其中b表示一个批

次数

据的数量，50表示句子的词

数量，在本次任务中，每个

句子的长

度都是固定的

50。

接下来定义上三角MASK，代码

如下：

#第13章/定义mask_tril函数

def mask_tril(data):

#b句话

，每句话50个词，这里是还没

embed的

#data = [b, 50]

#50*50的矩阵表示每个词对

其他词是否可见

#上三角

矩阵，不包括对角线，意味

着对每个词而言它只能

看到它自己和它之前的

词，

而看不到

#之后的词

#[1, 50, 50]

"""

[[0, 1, 1, 1, 1],

[0,

0, 1, 1, 1],

[0, 0,

0, 1, 1],

[0, 0, 0,

0, 1],

[0, 0, 0, 0,

0]]"""

tril = 1 - torch.tril(torch.ones(1,

50, 50, dtype=torch.long))

#判

断y当中每个词是不是PAD, 如

果是PAD, 则不可见

#[b, 50]

mask = data ==

vocab_y['<PAD>']

#变形+转型

，为了之后的计算

#[b, 1, 50]

mask

= mask.unsqueeze(1).long()

#mask和tril求并

集

#[b, 1, 50]

+ [1, 50, 50] -> [b,

50, 50]

mask = mask +

tril

#转布尔型

mask = mask >

0

#转布尔型，增

加一个维度，便于后续的

计算

mask = (mask ==

1).unsqueeze(dim=1)

return mask

mask_tril(x[:1])

运行结果如下：

tensor([[[[False,

True, True, …, True, True, True],

[False, False, True, …, True, True,

True],

[False, False, False, …, True,

True, True],

...,

[False, False, False,

…, False, True, True],

[False, False,

False, …, False, True, True],

[False,

False, False, …, False, True, True]]]])

在这

段代码中，首先生成了一

个上三角MASK，之后以输入文

本

中的每个词是否是PAD来

生成PAD MASK，最后把两个MASK合并。

最

终输出的形状和PAD MASK函数相

同，也是b×1×50×50。

13.6.4

定义Transformer工具子层

接

下来定义注意力计算层

，代码如下：

#第13章/定义注意

力计算函数

def attention(Q, K,

V, mask):

#b句话，每句话

50个词，每个词编码成32维向

量，4个头，每个头分到8维向

量

#Q、K、V = [b,

4, 50, 8]

#[b, 4, 50,

8] * [b, 4, 8, 50]

-> [b, 4, 50, 50]

#Q、K矩阵相乘，求每个词相

对其他所有词的注意力

score

= torch.matmul(Q, K.permute(0, 1, 3, 2))

#除以每个头维数的平方

根，做数值缩放

score /= 8**0.5

#mask遮盖， mask是True的

地方都被替换成-inf，这样在

计算Softmax时-inf会

被压缩到0

#mask = [b, 1, 50,

50]

score = score.masked_fill_(mask, -float('inf'))

score

= torch.Softmax(score, dim=-1)

#以注

意力分数乘以V得到最终

的注意力结果

#[b, 4,

50, 50] * [b, 4, 50,

8] -> [b, 4, 50, 8]

score = torch.matmul(score, V)

#每个头计

算的结果合一

#[b,

4, 50, 8] -> [b, 50,

32]

score = score.permute(0, 2, 1,

3).reshape(-1, 50, 32)

return score

attention(torch.randn(8,

4, 50, 8), torch.randn(8, 4, 50,

8),

torch.randn(8, 4, 50, 8), torch.zeros(8,

1, 50, 50)).shape

运行结果

如下：

torch.Size([8, 50,

32])

该处的计算过程如

本章开头部分所述，完全

是理论部分的实现，

只是

把其中的部分数字替换

成了实际情况中的数字

。

需要注意的是，在该函数

中计算的已经是多头注

意力，为了计算

简便，这里

把多组Q、K、V组成了一个矩阵

输入注意力函数中，再在

函数中拆分成多组Q、K、V，最后

通过矩阵计算的形式计

算多头注意

力。

接下来要

定义多头注意力计算层

，在该层中需要使用批量

标准化

层，在PyTorch当中主要提

供了两种批量标准化的

网络层，分别是

BatchNorm和LayerNorm，其中BatchNorm按

照处理的数据维度分

为

BatchNorm1d、BatchNorm2d、BatchNorm3d。由于本次的任

务 是

自 然

语 言 处 理 任

务 ， 属 于 一 维

的

数 据 ， 所 以 应

该 使 用

BatchNorm1d。

BatchNorm1d和

LayerNorm之间的区别，在于BatchNorm1d

是取不

同样本做标准化，而LayerNorm是取

不同通道做标准化，可通

过如下代码验证。

#第13章/BatchNorm1d和

LayerNorm的对比

#标准化之后，均值

是0, 标准差是1

#BN是取不同样

本做标准化

#LN是取不同通

道做标准化

#affine=True,elementwise_affine=True：指定标准化

后再计算一个线性映射

norm

= torch.nn.BatchNorm1d(num_features=4, affine=True)

print(norm(torch.arange(32, dtype=torch.float32).reshape(2, 4,

4)))

norm = torch.nn.LayerNorm(normalized_shape=4,

elementwise_affine=True)

print(norm(torch.arange(32,

dtype=torch.float32).reshape(2, 4,

4)))

运行结果如下：

tensor([[[-1.1761, -1.0523,

-0.9285, -0.8047],

[-1.1761, -1.0523, -0.9285, -0.8047],

[-1.1761, -1.0523, -0.9285, -0.8047],

[-1.1761, -1.0523,

-0.9285, -0.8047]],

[[ 0.8047, 0.9285, 1.0523,

1.1761],

[ 0.8047, 0.9285, 1.0523, 1.1761],

[ 0.8047, 0.9285, 1.0523, 1.1761],

[

0.8047, 0.9285, 1.0523, 1.1761]]],

grad_fn=<NativeBatchNormBackward0>)

tensor([[[-1.3416,

-0.4472, 0.4472, 1.3416],

[-1.3416, -0.4472, 0.4472,

1.3416],

[-1.3416, -0.4472, 0.4472, 1.3416],

[-1.3416,

-0.4472, 0.4472, 1.3416]],

[[-1.3416, -0.4472, 0.4472,

1.3416],

[-1.3416, -0.4472, 0.4472, 1.3416],

[-1.3416,

-0.4472, 0.4472, 1.3416],

[-1.3416, -0.4472, 0.4472,

1.3416]]],

grad_fn=<NativeLayerNormBackward0>)

从结果很

显然能够看出，两个标准

化层的计算输出虽然都

是标准

的正态分布，但是

BatchNorm1d计算后的数据两个样本

的均值都不

是0，前一个样

本的均值显然小于0，后一

个样本的均值显然大于

0。

相比较之下，LayerNorm计算后的两

个样本均值都在0附近，对

于本次的任务而言，选择

使用LayerNorm更适合。

明确了要使

用的标准化层实现以后

，接下来就可以定义多头

注意

力计算层了，代码如

下：

运行结果如下：

torch.Size([8, 50, 32])

和理论

部分一致，这里使用多组

WQ、WK、WV矩阵对词向量进

行投影

，得到多组Q、K、V向量，只是为了

便于计算，这里把多组

WQ、WK、WV矩

阵进行了合并，使用矩阵

运算也能提高计算的效

率。

在这段代码中，首先对

词向量进行了标准化计

算，这和论文的实

现不一

致，在Transformer原始论文中的计算

顺序是先计算自注意

力

，再进行短接，然后进行标

准化计算。此处把标准化

的计算提前

了，这样做的

原因是因为经过了广泛

的实验论证，从实际效果

来看标

准化前置能更好

地保证数值的稳定性，能

帮助模型更好地收敛，所

以

此处选择标准化前置

的计算方法，这是一种对

Transformer原有模型

的改进。

接下来

定义位置编码层，代码如

下：

运行结果如下：

torch.Size([8, 50, 32])

在这段

代码中包括一个内嵌函

数get_pe()，这个函数的实现完全

是式(13-1)和式(13-2)的实现，使用该

函数计算出位置编码矩

阵，位

置编码矩阵的尺寸

是50×32，因为在本次任务中，文

本的长度是50个

词，每个词

编码成32维的向量。

位 置 编

码 矩 阵 本

身 是 一 个 不 更

新

的 常 量 ， 所 以

使 用

register_buffer()函数

定义为常量。

位置编码层

的计算过程和理论保持

一致，先把每个词编码成

普通

的词向量，再和位置

编码矩阵相加作为最终

的词向量编码。

接下来定

义全连接输出层，代码如

下：

运行结果如下：

torch.Size([8, 50, 32])

这里同

样使用了标准化层前置

的计算方法。

13.6.5

定义Transformer模型

做

完以上准备工作，现在可

以定义编码器层和解码

器层了。先看

编码器，代码

如下：

运行结果如下：

torch.Size([8, 50,

32])

在这

段代码中，定义了编码器

层和编码器，编码器由3层

编码器

层组成，和理论部

分一致，3层编码器是前后

串联的关系。

编码器层本

身的计算是用x同时作为

Q、K、V向量计算自注意

力，计算

得到的注意力分数再输

入全连接输出层计算输

出。

接下来看解码器的实

现，代码如下：

运行结果如

下：

torch.Size([8, 50, 32])

解码器和编码器的计

算过程大致相同，第1步是

以y同时作为Q

、K、V向量计算自

注意力。

接下来就是解码

器和编码器计算的不同

点，多了一层编解码注意

力层的计算。这一层的计

算也是多头注意力的计

算，只是入参的Q向

量替换

成了上一步计算得到的

y的自注意力分数，K和V向量

则是使用

从编码器那里

获得的x的注意力分数。

最

后把编解码注意力层计

算得到的注意力分数输

入全连接输出层

计算输

出。

有了编码器和解码器

就可以定义Transformer主模型了，代

码如

下：

运行结果如下：

torch.Size([8, 50, 39])

在

主模型中，初始化了两个

位置编码层，分别用来编

码x和y，计

算的流程如下：

(1)获

取一批x和y之后，对x计算PAD MASK，对

y计算上三角

MASK。

(2)对x和y分别编

码。

(3)把x输入编码器计算输

出。

(4)把编码器的输出和y同

时输入解码器计算输出

。

(5)将解码器的输出输入全

连接输出层计算输出。

从

上面的叙述可以看出，Transformer主

模型的计算需要同时输

入x和y。原因是我们要使用

强制教学的方法训练Transformer模

型，

所以在计算时需要同

时输入x和y。

但是在预测时

只有x数据，没有y数据，所以

需要定义一个额外的

预

测函数，这个函数不使用

强制教学，所以不需要y，而

是使用

Transformer本身的能力预测

句子，代码如下：

运行结果

如下：

tensor([[ 0, 19, 19,

19, 3, 17, 30, 37, 19,

37, 37, 37, 37,

19, 3,

3, 19,

19, 37, 37, 37,

37, 37, 17, 17, 17, 36,

25, 3, 17, 17, 17, 33,

37,

7, 7, 7, 7, 17,

3, 7, 7, 7, 32, 3,

3, 3, 3, 3, 3]])

如理论部分所描述

，在预测函数中，Transformer模型将一

个词

一个词地预测输出

，每预测一个词，就作为下

一个词预测的输入使

用

。

13.6.6 训练和测试

现在Transformer模型已

经定义完毕，并且也有了

预测函数，现

在可以开始

训练Transformer模型了，代码如下：

在

这段代码中，定义了学习

率衰减器，每训练3个轮次

，则学习

率减半，但是由于

本次要训练的任务复杂

度太低，所以只需训练1个

轮次就可以了，没有机会

应用到学习率衰减。

在每

次计算时，把y的最后一个

词切除，因为在Transformer中计

算时

，根据y的前一个词预测下

一个词，所以不需要最后

一个词。这也

是在设计数

据时，故意让y多一个词的

原因，这样在切除一个词

以后，

长度刚好和x的长度

相等。

而在计算loss和正确率

时，需要把y的第1个词切除

，因为

Transformer根据y的前一个词预

测下一个词，所以Transformer并没

有

预测y当中的第1个词，而在

设计数据时，y当中的第1个

词是确定的

<SOS>，这个词也确

实没有预测的必要。

该过

程如图13-32所示。

图13-32

y和预测结

果的对应关系

训练过程

的输出见表13-2。

表13-2 训练过程

的输出

续表

从表13-2可以看

出，预测的正确率上升得

很快。训练结束后，可

以使

用模型预测，代码如下：

#第

13章/测试

def test():

for i,

(x, y) in enumerate(loader):

break

for

i in range(8):

print(i)

print(''.join([vocab_xr[i] for

i in x[i].tolist()]))

print(''.join([vocab_yr[i] for i

in y[i].tolist()]))

print(''.join(

[vocab_yr[i] for i

in predict(x[i].unsqueeze(0))[0].tolist()]))

test()

运行结果如下：

从

结果可以看出，Transformer在这个简

单的翻译任务中表现良

好，预测结果和真实的y相

差较小。

13.7

两数相加任务

13.7.1 任

务介绍

有些读者可能觉

得上面的例子太过于简

单了，x和y之间的关系为

一

一对应关系，缺乏相互影

响，在本节将介绍一个更

高难度的任务，

在这个任

务中，我们将尝试使用Transformer计

算加法，先来看一些

数据

样例，见表13-3。

表13-3 加法数据样

例

在表13-3中，x是被字母a分隔

的两串数字，这两串数字

相加之后

等于y。两串数字

的长度是随机的，可以把

x看作一句话，把y看作x的

译

文，这是个相对复杂的对

应关系，不再是简单的一

一对应。

13.7.2 实现代码

要尝试

完成该任务，只需重新定

义数据生成函数，并且把

训练的

轮数修改为10次。新

的数据生成函数的代码

如下：

#第13章/两数相加测试

#使用这份数据时可把训

练次数改为10

def

get_data():

#定义词集合

words = ['0', '1', '2',

'3', '4', '5', '6', '7', '8',

'9']

#定义每个词被选中的概

率

p = np.array([1, 2,

3, 4, 5, 6, 7, 8,

9, 10])

p = p /

p.sum()

#随机选n个词

n = random.randint(10, 20)

s1 = np.random.choice(words, size=n, replace=True, p=p)

#采样的结

果就是s1

s1 = s1.tolist()

#以同样的方法，再

采出s2

n

= random.randint(10, 20)

s2 = np.random.choice(words,

size=n, replace=True, p=p)

s2 = s2.tolist()

#y等于s1和s2数值的和

y = int(''.join(s1)) + int(''.join(s2))

y = list(str(y))

#x由

s1和s2字符连接而成

x =

s1 + ['a'] + s2

#加上首

尾符号

x = ['<SOS>'] + x +

['<EOS>']

y = ['<SOS>'] + y

+ ['<EOS>']

#补PAD, 直到固定长度

x = x

+ ['<PAD>'] * 50

y =

y + ['<PAD>'] * 51

x

= x[:50]

y = y[:51]

#编码成数据

x = [vocab_x[i] for i in

x]

y = [vocab_y[i] for i

in y]

#转Tensor

x = torch.LongTensor(x)

y = torch.LongTensor(y)

return x, y

get_data()

运行结果

如下：

(tensor([ 0, 32, 35,

33, 9, 21, 7, 23, 35,

26, 23, 27, 36, 

7,

12, 32, 11, 30,

35, 24,

26, 35, 32, 38, 30, 28,

31, 33, 29, 30, 35, 35,

22, 10,

16, 30,

37, 7,

37, 34, 11, 22, 38, 26,

30, 1, 2, 2, 2, 2]),

tensor([ 0, 30, 30, 26, 38,

22, 4, 34, 37, 8, 37,

30, 16, 5,

22, 35, 35,

30,

29, 33, 31, 28, 30,

38, 32, 35, 26, 24, 35,

30, 4, 32, 3, 8,

36,

27,

23, 26, 35, 23, 8,

21, 6, 33, 35, 32, 1,

2, 2, 2, 2]))

13.7.3 训练和测试

接下来

把训练的轮数修改为10次

，这样就可以训练了，训练

过程

的输出见表13-4。

表13-4 两数

相加训练过程的输出

续

表

训练完成后执行一次

测试，查看模型的预测效

果，运行结果如

下：

可以看

到在这个更加复杂的任

务中，Transformer依然很好地完

成了

任务，虽然有一些错误，但

在可容忍的范围内。

13.8 小结

本章详细介绍了Transformer的模型

设计思路和计算过程，并

且

通过两个实例使用Transformer执

行了两个翻译任务。

在Transformer被

提出之前，普遍使用的文

本特征抽取层是

RNN，RNN的缺点

是能表达的文本复杂度

很有限，尤其针对长文本

的处理能力更差，虽然在

LSTM和GRU模型被提出后RNN的这个

缺点

在很大程度上被弥

补了，但依然没有得到彻

底解决。

RNN还有个缺点，即它

的计算过程是串联的，必

须先算第1个词

才能算第

2个词，在文本长度较长的

情况下RNN的计算效率较低

。

Transformer使用注意力模型抽取文

本特征，很好地解决了RNN

的

两个缺点，Transformer的注意力模型

就是要找出词与词之间

的相

互对应关系，所以对

长文本有较好的处理能

力，Transformer的计算

过程是可并行

的，效率比RNN要高很多。

但是

Transformer也有缺点，它的缺点就是

相比RNN而言太复杂

了，RNN是个

非常简单漂亮的模型，就

算是对RNN一无所知的人也

能 在

很 短 的 时 间 内

理 解

RNN 的 计 算 过

程 和 原 理 ， 相

比

之 下

Transformer就复杂得多，学习的

难度也较大。

BERT模型是基于

Transformer的改进模型，理解了Transformer

就能

很好地理解BERT。

第14章

手动实

现BERT

14.1 BERT架构

学习了Transformer模型之后

，现在来研究BERT模型，如前所

述，BERT是基于Transformer模型的改进模

型，与Transformer不

同，BERT的设计并不是

为了完成特定的具体任

务，BERT的设计初衷

就是要作

为一个通用的backbone使用，即提

取文本的特征向量，有

了

特征向量后就可以接入

各种各样的下游任务，包

括翻译任务、分类

任务、回

归任务等。

先来看BERT模型的

架构，如图14-1所示。

图14-1 BERT模型架

构

下面对图14-1中的计算流

程进行解释。

(1)输入层：BERT每次

计算时输入两句话，而不

是一句话，这一

点和Transformer模型

不同。

(2)数据预处理：包括移

除不能识别的字符、将所

有字母小写、多

余的空格

等。由于输入的句子为两

句，在数据预处理时需要

把两个句

子组合成一个

句子，便于后续的计算。

(3)随

机将一些词替换为MASK：BERT模型

的训练过程包括两个

子

任务，其中一个即为预测

被遮掩的词的原本的词

，所以在计算之

前，需要把

句子中的一些词替换为

MASK交给BERT预测。

(4)编码句子：把句

子编码成向量，和Transformer一样，BERT同

样也有位置编码层，以让

处于不同位置的相同的

词有不同的向量表

示。

(5)编

码器：此处的编码器即为

Transformer中的编码器，BERT使

用了Transformer中的

编码器来抽取文本特征

。

(6)预测两个句子的关系：BERT的

计算包括两个子任务，预

测两

个句子的关系为其

中一个子任务，BERT要计算出

输入的两个句子的关

系

，这一般是二分类任务。

(7)预

测MASK词：这是BERT的另外一个子

任务，要预测出句子

中的

MASK原本的词。

以上就是BERT模型

计算过程的一个概览，为

了训练BERT模型设

计了两个

子任务，在这两个子任务

训练的过程中，训练BERT抽取

文本

特征向量的能力，如

果把图14-1中的最后一层剪

掉，留下的就是一个

能够

抽取句子向量的BERT模型。

14.2 数

据集处理

14.2.1

数据处理过程

概述

和Transformer不同，BERT每次处理的

并不是一个句子，而是一

对句子，这两个句子表达

的意思可能相同，也可能

不同，BERT的一个

子任务就是

要判断两个句子的意思

是否相同，接下来介绍BERT训

练数

据的一般处理过程

。

在数据预处理流程中，需

要移除所有的标点符号

、将所有字母小

写、将数字

替换为特殊符号，此步骤

的大致示意见表14-1。

表14-1 数据

预处理示意

处理好两个

句子后，就可以把两个句

子组合成一个句子了，两

个

句子中间使用特殊符

号分隔，组合后的句子首

尾也需要添加特殊符

号

，为了满足特定的长度，句

子可能需要补充一些PAD，组

合后的句

子使用事先准

备好的字典转换为数字

，该过程见表14-2。

表14-2 组合句子

并编码示意

BERT有两个子任

务，其中一个是要预测句

子中的MASK原本的

词，因此需

要在句子中将一些词替

换为MASK，该过程见表14-3。

表14-3 随机

用MASK遮掩部分词示意

在表

14-3中有两个MASK，MASK出现的位置是

随机的，MASK

不会替换特殊符

号，只会替换词，事实上遮

掩不只是简单地替换为

MASK，还会有其他的变化，这在

后续看代码时再详述。

接

下来要对句子进行编码

，编码的计算流程见表14-4。

表

14-4 句子编码示意

从表14-4可以

看出，一个句子可以编码

为3个编码，分别为词向

量

编码、片段编码和位置编

码，最终的编码为这3个编

码相加，下面

对这3种编码

分别进行介绍。

(1)词向量编

码：即简单地把词投影到

N维向量空间中去，投影矩

阵一般是随机初始化的

。

(2)片段编码：标识了句子中

哪一段属于句子1，哪一段

属于句子

2，哪一段属于PAD。

(3)位

置编码：和句子的具体内

容无关，只和位置有关，一

般初始

化为随机矩阵。和

Transformer不同，Transformer中的位置编码矩

阵

是常量，是不会更新的，但

是在BERT当中，位置编码矩阵

是个可学

习的参数，在训

练的过程中会不断地变

化。

经过编码以后，句子已

经被向量化，接下来就可

以被输入编码器

网络进

行计算，抽取文本的特征

向量，此处的编码器即Transformer

的

编码器。

抽取文本特征向

量以后，即可输入两个下

游任务网络计算输出，

这

两个输出是BERT的两个子任

务，分别为预测两个句子

的意思是否相

同和预测

被遮掩的词的原本的词

。

接下来是代码实现部分

，通过手动构建BERT模型，以帮

助读者更

深入地理解BERT的

设计思路和计算流程。

14.2.2

数

据集介绍

首 先 来 看 数

据

文 件 ， 在 本 章

中 ， 将 使 用 微

软

提 供 的 MSR

Paraphrase数据集进行训

练，在本书的配套资源中

可以找到数据文件

msr_paraphrase.csv。该数

据文件中的部分数据样

例见表14-5。

表14-5 MSR Paraphrase数据集示例

MSR Paraphrase数

据集共5801行，5列，其中两列ID对

于训练

BERT模型没有用处，只

需关注第1列和另外两列

String。

两列String在同一行显然为两

个句子，这两个句子可能

表达的是

同样的意思，也

可能是不同的意思，第1列

的数字即标识了这两个

句

子的意思是否相同。

从

表14-5可以看出，MSR Paraphrase数据集中的

句子是比较杂

乱的，有很

多特殊符号、数字、大小写

混写的情况，在数据预处

理过

程中逐一修正这些

问题。

14.2.3 数据处理实现代码

1.字词处理

首先读取数据

文件，代码如下：

#第14章/读取

数据文件

import

pandas as pd

data = pd.read_csv('data/msr_paraphrase.csv',

sep='\t')

data

运行结果见表

14-6。

表14-6 处理之前的MSR Paraphrase数据集

两

列ID对于本章的任务没有

用处，移除这两列，代码如

下：

#第14章/删除无用的两列

数据

data.pop('#1 ID')

data.pop('#2 ID')

data

重命名列名，代码如

下：

#第14章/重命名列

columns = list(data.columns)

columns[0] = 'same'

columns[1] = 's1'

columns[2] = 's2'

data.columns = columns

data

文本中

有很多特殊符号<QUOTE>，在本章

的任务中，出于简单

起见

，不考虑标点符号，所以可

以移除这个符号，代码如

下：

#第14章/删除文本中的<QUOTE>符

号

data['s1'] =

data['s1'].str.replace('<QUOTE>', ' ')

data['s2'] = data['s2'].str.replace('<QUOTE>',

' ')

data

删除所有标点符号，代

码如下：

#第14章/删除标点符

号

data['s1']

= data['s1'].str.replace('[^\w\s]', ' ')

data['s2'] =

data['s2'].str.replace('[^\w\s]', ' ')

data

文本中有一些特殊字

符需要替换为常规的字

符，代码如下：

#第14章/替换特

殊字符

data['s1'] = data['s1'].str.replace('â', 'a')

data['s2'] =

data['s2'].str.replace('â', 'a')

data['s1'] = data['s1'].str.replace('Â', 'A')

data['s2'] = data['s2'].str.replace('Â', 'A')

data['s1'] =

data['s1'].str.replace('Ã', 'A')

data['s2'] = data['s2'].str.replace('Ã', 'A')

data['s1'] = data['s1'].str.replace(' ', ' ')

data['s2'] = data['s2'].str.replace(' ', ' ')

data['s1'] = data['s1'].str.replace('μ', 'u')

data['s2'] =

data['s2'].str.replace('μ', 'u')

data['s1'] = data['s1'].str.replace('³', '

')

data['s2'] = data['s2'].str.replace('³', ' ')

data['s1'] = data['s1'].str.replace('½', ' ')

data['s2']

= data['s2'].str.replace('½', ' ')

data

经过以上处理以

后，文档中有很多连续的

空格，需要将连续的空

格

合并为1个空格，代码如下

：

#第14章/合并连续的空格

data['s1'] = data['s1'].str.replace('\s{2,}', '

')

data['s2'] = data['s2'].str.replace('\s{2,}', ' ')

data

文

档中有些数字和字母连

在一起，例如“12th”“1990s”等，

需要把它

们拆分开，代码如下：

#第14章

/拆分数字和字母连写的

词

data['s1'] =

data['s1'].str.replace('(\d)([a-zA-Z])', '\\1

\\2')

data['s2'] = data['s2'].str.replace('(\d)([a-zA-Z])',

'\\1

\\2')

data['s1'] = data['s1'].str.replace('([a-zA-Z])(\d)', '\\1

\\2')

data['s2'] = data['s2'].str.replace('([a-zA-Z])(\d)', '\\1

\\2')

data

文本中大小写是混写

的，出于简单考虑，把所有

的大写字母转换

为小写

字母，并移除每个句子首

尾的空格，代码如下：

#第14章

/删除首尾空格并小写所

有字母

data['s1'] =

data['s1'].str.strip()

data['s2'] = data['s2'].str.strip()

data['s1'] =

data['s1'].str.lower()

data['s2'] = data['s2'].str.lower()

data

文本中有很多数

字，如果每个数字都作为

一个词处理，则字典的

量

将不可控，并且数字太过

于抽象，神经网络不太可

能捕捉到每个数

字的词

向量表示，所以将所有的

数字替换为特殊符号，代

码如下：

#第14章/替换数字为

符号

data['s1'] = data['s1'].str.replace('\d+',

'<NUM>')

data['s2'] = data['s2'].str.replace('\d+', '<NUM>')

data

运行结果见表14-7。

表14-7 文

本处理完毕的MSR Paraphrase数据集

续

表

2.合并句子

到此，文本的

处理已经完毕，接下来需

要把两个句子组合为一

个

句子，首先要对第1个句

子添加首尾符号，代码如

下：

#第14章/为s1添加首尾符号

def f(sent):

return '<SOS>

' + sent + ' <EOS>'

data['s1'] = data['s1'].apply(f)

data

由于第2个句子会接在第

1个句子的后面，所以第1个

句子的结尾

符号即为第

2个句子的开头符号，由此

第2个句子不需要添加开

头符

号，只需添加结尾符

号，代码如下：

#第14章/为s2添加

结尾符号

def f(sent):

return sent

+ ' <EOS>'

data['s2'] = data['s2'].apply(f)

data

在组合两个句

子之后，需要先计算出两

个句子的长度，后续在

BERT中

计算片段编码时需要用

到，代码如下：

#第14章/分别求

出s1和s2的长度

def f(sent):

return len(sent.split(' '))

data['s1_lens'] = data['s1'].apply(f)

data['s2_lens'] = data['s2'].apply(f)

data

接下来需要

求出两个句子相加之后

的最大长度，以确定每个

句子

需要补充PAD的长度，代

码如下：

#第14章/求s1+s2后的最大

长度

max_lens = max(data['s1_lens'] + data['s2_lens'])

max_lens

运行结果如下：

72

可见

两个句子相加，最大长度

为72个单词，对于不足72个单

词的

句子，需要补充PAD，让所

有句子的长度保持一致

，便于后续的计

算。

在补充

PAD之前，首先需要计算出每

个句子要补充PAD的长度，

代

码如下：

#第14章/求出每个句

子需要补充PAD的长度

data['pad_lens'] = max_lens

- data['s1_lens'] - data['s2_lens']

data

至此

，就可以合并两个句子了

，代码如下：

#第14章/合并s1和s2

data['sent'] = data['s1'] + '

' + data['s2']

data.pop('s1')

data.pop('s2')

data

现

在对每个句子补充PAD，代码

如下：

#第14章/为不足最大长

度的句子补充PAD

def f(row):

pad =

' '.join(['<PAD>'] * row['pad_lens'])

row['sent'] =

row['sent'] + ' ' + pad

return row

data = data.apply(f, axis=1)

data

运行结果

见表14-8。

表14-8 合并句子完毕的

MSR Paraphrase数据集

3.构建字典并编码

至此，句子的合并已经完

毕，接下来需要构建字典

，代码如下：

#第14章/构建字典

def build_vocab():

vocab = {

'<PAD>':

0,

'<SOS>': 1,

'<EOS>': 2,

'<NUM>':

3,

'<UNK>': 4,

'<MASK>': 5,

'<Symbol6>':

6,

'<Symbol7>': 7,

'<Symbol8>': 8,

'<Symbol9>':

9,

'<Symbol10>': 10,

}

for i

in range(len(data)):

for word in data.iloc[i]['sent'].split('

'):

if word not in vocab:

vocab[word] = len(vocab)

return vocab

vocab

= build_vocab()

len(vocab), vocab['the']

输出的结果如下：

(14789,

18)

构建字

典之前，首先定义了10个特

殊符号，有些特殊符号是

预留

的，防止以后可能需

要添加新的特殊符号的

情况，普通词的序号从11

开

始。

构建字典的过程需要

遍历所有句子的所有词

，如果发现新词，则

添加入

字典，序号相应地增加1。

从

结果可以看出，使用MSR Paraphrase数据

集编出的字典共

14789个词，包

括10个特殊符号，单词the的序

号为18。

有了字典之后，可以

使用字典把所有的单词

转换为数字，代码如

下：

#第

14章/使用字典编码文本

def f(sent):

sent = [str(vocab[word]) for

word in sent.split()]

sent = ','.join(sent)

return sent

data['sent'] = data['sent'].apply(f)

data

运

行结果见表14-9。

表14-9 处理完毕

的MSR Paraphrase数据集

4.保存数据文件

至此数据已经处理完毕

，可以保存为CSV文件，便于后

续输入

BERT当中计算，代码如

下：

#第14章/保存为CSV文件

data.to_csv('data/msr_paraphrase_data.csv', index=False)

字典

也保存为CSV文件，后续需要

对数据解码，代码如下：

 #第

14章/保存字典

pd.DataFrame(vocab.items(), columns=['word',

'token']).to_csv('data/msr_

paraphrase_vocab.csv',

index=False)

保存的数据

文件内容见表14-10。

表14-10 保存的

数据文件

续表

保存的字

典文件内容见表14-11。

表14-11 保存

的字典文件

14.3 PyTorch提供的Transformer工具

层介绍

BERT使用了Transformer的编码器

，在之前的章节我们手动

构建

了Transformer模型，手动构建的

模型的代码量大，过程比

较复杂。

本章的主题是BERT模

型，所以我们尽量剥离Transformer内

部的实现

细节，更多地关

注BERT的计算过程。

在PyTorch当中提

供了Transformer的一些工具层，能够

帮助我

们快速地构建Transformer模

型，忽略Transformer实现的具体细

节

，接下来将详细介绍这些

工具层。

1.定义测试数据

首

先需要虚拟一些数据，以

进行后续的实验，代码如

下：

#第14章/虚拟数据

import torch

#假设有

两句话，8个词

x = torch.ones(2,

8)

#两句话中各

有一些PAD

x[0, 6:] = 0

x[1, 7:] = 0

x

运行结果如下：

tensor([[1., 1., 1., 1., 1., 1.,

0., 0.],

[1., 1., 1., 1.,

1., 1., 1., 0.]])

在

这段代码中，虚拟了两句

话，每句话包括8个词，每句

话的末

尾都有一些PAD，后续

将使用这两句话进行一

些实验。

2.各个MASK的含义解释

在Transformer当中有几种MASK，用来遮挡

数据中的某些不需

要关

注的位置。

第1个MASK是key_padding_mask，它的作

用是遮挡数据中的

PAD位置

，防止Transformer把注意力浪费在PAD上

，显然PAD是没

有 承

载 任 何 信

息 的 ，

所 以 应 该 忽 略

语 句

中 的 PAD ， 定

义

key_padding_mask的代码如下：

#第

14章/定义key_padding_mask

#key_padding_mask的定义方式，就是

x中是pad的为True，否则是False

key_padding_mask =

x == 0

key_padding_mask

运行结

果如下：

tensor([[False,

False, False, False, False, False, True,

True],

[False, False, False, False, False,

False, False, True]])

key_padding_mask的定义是根据语

句中每个位置是否是PAD来

确定的，如果是PAD，则是True，在计

算注意力时会被忽略，否

则是

False，会被正常地计算注

意力。

第2个MASK是encode_attn_mask，它定义了是

否要忽略输入语

句内某

些词与词之间的注意力

，一般来讲不需要忽略输

入语句中的注

意力，所以

将encode_attn_mask定义为全False的矩阵即可

，代码如

下：

#第14章/定义encode_attn_mask

#在encode阶

段不需要定义encode_attn_mask

#定义为None或

者全False都可以

encode_attn_mask = torch.ones(8, 8) == 0

encode_attn_mask

运行结果如

下：

tensor([[False, False, False, False,

False, False, False,

False],

[False, False,

False, False, False, False, False, False],

[False, False, False, False, False, False,

False, False],

[False, False, False, False,

False, False, False, False],

[False, False,

False, False, False, False, False, False],

[False, False, False, False, False, False,

False, False],

[False, False, False, False,

False, False, False, False],

[False, False,

False, False, False, False, False, False]])

可以看到，encode_attn_mask是个全False的矩

阵，由于全

False 也 是 PyTorch 的

Transformer 工 具 层

的 默 认

值 ， 所 以

encode_attn_mask也可以定

义为None，两者是等价的。

第3个

MASK是decode_attn_mask，它定义了是否要忽略

输出语

句内某些词与词

之间的注意力，一般来讲

在解码输出语句时，应该

遮

挡正确答案，防止模型

直接照抄正确答案，导致

模型的成绩虚高，代

码如

下：

#第14章/定义decode_attn_mask

#在decode阶段需要

定义decode_attn_mask

#decode_attn_mask的定义方式是对角

线以上为True的上三角矩阵

decode_attn_mask

= torch.tril(torch.ones(8, 8)) == 0

decode_attn_mask

运行结果如下：

tensor([[False, True, True, True, True,

True, True, 

True],

[False, False,

True, True, True, True, True, True],

[False, False, False, True, True, True,

True, True],

[False, False, False, False,

True, True, True, True],

[False, False,

False, False, False, True, True, True],

[False, False, False, False, False, False,

True, True],

[False, False, False, False,

False, False, False, True],

[False, False,

False, False, False, False, False, False]])

可以看到

decode_attn_mask是一个8×8的上三角矩阵，对

角线

以上的位置全为True，其

他位置为False，这个MASK表达的含

义是，

在解码第2个词时，只

能看到第1个词，看不到以

后的词，在解码第3

个词时

，只能看到第1个和第2个词

，看不到以后的词，以此类

推，这

样就避免了解码器

直接从题目中照抄答案

。

3.编码数据

到目前为止，3个

MASK就定义好了，接下来可以

对x编码，把每

个词编码成

词向量，代码如下：

#第14章/编

码x

x = x.unsqueeze(2)

x = x.expand(-1, -1, 12)

x,

x.shape

运行结果如下：

(tensor([[[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[0., 0., 0., 0.,

0., 0., 0., 0., 0., 0.,

0., 0.],

[0., 0., 0., 0.,

0., 0., 0., 0., 0., 0.,

0., 0.]],

[[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[1., 1., 1., 1.,

1., 1., 1., 1., 1., 1.,

1., 1.],

[0., 0., 0., 0.,

0., 0., 0., 0., 0., 0.,

0., 0.]]]),

torch.Size([2, 8, 12]))

可以看

到，x中的每个词都被编码

成了一个12维的向量。x的维

度

被转换为2×8×12，表示2句话、每

句话8个词、每个词用12维的

向量

表示。

4.多头注意力计

算函数

在介绍PyTorch的Transformer工具层

之前，首先来看PyTorch提

供的多

头注意力计算函数，在计

算多头注意力时需要做

两次线性变

换，一次是对

入参的Q、K、V矩阵分别做线性

变换，另一次是计算完

成

以后，对注意力分数做线

性变换，两次线性变换分

别需要两组

weight和bias参数，这里

先把它们定义出来，代码

如下：

 #第14章/定义multi_head_attention_forward()所需要的

参数

#in_proj就是Q、K、V线性变换的参

数

in_proj_weight

= torch.nn.Parameter(torch.randn(3 * 12, 12))

in_proj_bias

= torch.nn.Parameter(torch.zeros((3 * 12)))

#out_proj就是输出时做线性变

换的参数

out_proj_weight

= torch.nn.Parameter(torch.randn(12, 12))

out_proj_bias = torch.nn.Parameter(torch.zeros(12))

in_proj_weight.shape, in_proj_bias.shape,

out_proj_weight.shape,

out_proj_bias.shape

运行结果如下

：

(torch.Size([36,

12]),

torch.Size([36]),

torch.Size([12, 12]),

torch.Size([12]))

定义好了两组线性变换

的参数以后就可以调用

多头注意力计算函

数了

，代码如下：

#第14章/使用工具

函数计算多头注意力

data = {

#因

为不是batch_first的，所以需要进行

变形

'query': x.permute(1, 0, 2),

'key': x.permute(1,

0, 2),

'value': x.permute(1, 0, 2),

'embed_dim_to_check': 12,

'num_heads': 2,

'in_proj_weight': in_proj_weight,

'in_proj_bias': in_proj_bias,

'bias_k': None,

'bias_v': None,

'add_zero_attn': False,

'DropOut_p': 0.2,

'out_proj_weight': out_proj_weight,

'out_proj_bias': out_proj_bias,

'key_padding_mask': key_padding_mask,

'attn_mask': encode_attn_mask,

}

score, attn =

torch.nn.functional.multi_head_attention_forward(**data)

score.shape,

attn, attn.shape

运行结果如下：

(torch.Size([8, 2, 12]),

tensor([[[0.2083, 0.2083, 0.2083, 0.2083, 0.1042, 0.2083,

0.0000, 0.0000],

[0.1042, 0.1042, 0.2083, 0.2083,

0.2083, 0.0000, 0.0000,

0.0000],

[0.2083, 0.2083,

0.1042, 0.2083, 0.2083, 0.1042, 0.0000,

0.0000],

[0.2083, 0.2083, 0.1042, 0.2083, 0.2083, 0.2083,

0.0000,

0.0000],

[0.1042, 0.1042, 0.1042, 0.2083,

0.1042, 0.2083, 0.0000,

0.0000],

[0.2083, 0.1042,

0.1042, 0.2083, 0.2083, 0.2083, 0.0000,

0.0000],

[0.2083, 0.2083, 0.2083, 0.1042, 0.2083, 0.2083,

0.0000,

0.0000],

[0.1042, 0.2083, 0.1042, 0.1042,

0.2083, 0.2083, 0.0000,

0.0000]],

[[0.0893, 0.0893,

0.1786, 0.1786, 0.1786, 0.1786, 0.1786,

0.0000],

[0.0893, 0.1786, 0.1786, 0.1786, 0.0893, 0.1786,

0.0893,

0.0000],

[0.1786, 0.0000, 0.1786, 0.0893,

0.1786, 0.0893, 0.1786,

0.0000],

[0.1786, 0.0893,

0.1786, 0.1786, 0.1786, 0.1786, 0.1786,

0.0000],

[0.1786, 0.0893, 0.0893, 0.1786, 0.1786, 0.0000,

0.1786,

0.0000],

[0.1786, 0.1786, 0.1786, 0.0893,

0.0893, 0.0893, 0.1786,

0.0000],

[0.1786, 0.1786,

0.1786, 0.0893, 0.0893, 0.0893, 0.1786,

0.0000],

[0.0893, 0.1786, 0.0893, 0.1786, 0.1786, 0.1786,

0.1786,

0.0000]]],

grad_fn=<DivBackward0>),

torch.Size([2, 8, 8]))

多头

注意力计算函数需要的

入参比较多，下面分别进

行介绍。

(1)query、key、value：分别是计算注意

力的Q、K、V矩阵，在

上面的例子

中都使用x计算，也就是说

，我们计算的是自注意力

。

(2)embed_dim_to_check：词向量编码的维度。

(3)num_heads：多头

注意力的头数，这个数字

必须可以整除词

向量编

码的维度。

(4)in_proj_weight、in_proj_bias：对Q、K、V矩阵做线性

变换

所使用的参数。

(5)bias_k、bias_v：是否

要对K和V矩阵单独添加bias，一

般设置

为None即可。

(6)add_zero_attn：如果设置

为True，则会在Q、K的注意力结

果

中单独加一列0，一般设置

为默认值False即可。

(7)DropOut_p：运行过程

中所使用的DropOut概率。

(8)out_proj_weight、out_proj_bias：对注意

力分数做线性变换

所使

用的参数。

(9)key_padding_mask：是否要忽略语

句中的某些位置，一般只

需忽略PAD的位置。

(10)attn_mask：是否要忽

略每个词之间的注意力

，在编码器中

一般只用全

False的矩阵，在解码器中一般

使用对角线以上全True的矩

阵。

从输出结果可以看出

，在注意力矩阵中，所有词

对PAD的注意力

都是0，这正是

我们所期望的，注意力分

数是一个8×2×12的矩阵。

5.多头注

意力层

完成了比较复杂

的多头注意力计算函数

，接下来看一下封装程度

更高、使用更方便的多头

注意力层，代码如下：

#第14章

/使用多头注意力工具层

multihead_attention =

torch.nn.MultiheadAttention(embed_dim=12,

num_heads=2,

DropOut=0.2,

batch_first=True)

data =

{

'query': x,

'key': x,

'value':

x,

'key_padding_mask': key_padding_mask,

'attn_mask': encode_attn_mask,

}

score, attn = multihead_attention(**data)

score.shape, attn,

attn.shape

运行结果如下：

(torch.Size([2, 8, 12]),

tensor([[[0.2083,

0.2083, 0.2083, 0.2083, 0.2083, 0.1042,

0.0000,

0.0000],

[0.2083, 0.1042, 0.2083, 0.2083, 0.0000,

0.2083, 0.0000,

0.0000],

[0.2083, 0.2083, 0.2083,

0.1042, 0.1042, 0.2083, 0.0000,

0.0000],

[0.2083,

0.2083, 0.0000, 0.2083, 0.2083, 0.2083, 0.0000,

0.0000],

[0.2083, 0.1042, 0.1042, 0.0000, 0.2083,

0.2083, 0.0000,

0.0000],

[0.2083, 0.1042, 0.2083,

0.2083, 0.1042, 0.2083, 0.0000,

0.0000],

[0.2083,

0.1042, 0.2083, 0.2083, 0.2083, 0.2083, 0.0000,

0.0000],

[0.2083, 0.2083, 0.2083, 0.1042, 0.2083,

0.2083, 0.0000,

0.0000]],

[[0.1786, 0.0893, 0.0893,

0.1786, 0.0893, 0.1786, 0.1786,

0.0000],

[0.1786,

0.1786, 0.1786, 0.1786, 0.1786, 0.1786, 0.1786,

0.0000],

[0.1786, 0.0893, 0.0893, 0.0893, 0.0893,

0.1786, 0.0893,

0.0000],

[0.0893, 0.0893, 0.0893,

0.0893, 0.0893, 0.0893, 0.1786,

0.0000],

[0.1786,

0.1786, 0.0893, 0.1786, 0.1786, 0.1786, 0.1786,

0.0000],

[0.1786, 0.0893, 0.0893, 0.1786, 0.1786,

0.1786, 0.1786,

0.0000],

[0.1786, 0.0893, 0.1786,

0.1786, 0.1786, 0.0000, 0.0000,

0.0000],

[0.1786,

0.1786, 0.1786, 0.1786, 0.1786, 0.1786, 0.1786,

0.0000]]],

grad_fn=<DivBackward0>),

torch.Size([2, 8, 8]))

多头注意

力层初始化的参数和运

算参数大多在多头注意

力计算函

数中出现过，它

们表示的意思也相同。

参

数batch_first=True表示输入的语句Batch Size在第

一维度，

这样输入和输出

的形状都和x的定义一致

，不需要再做额外的变形

。

由于多头注意力层是一

个神经网络层，它封装了

输入和输出的线

性计算

的参数，所以不需要再额

外指定。

从输出的注意力

矩阵来看，也同样忽略了

对语句中所有PAD的注

意力

。

6.编码器层

接下来是编码

器层，代码如下：

#第14章/使用

单层编码器工具层

encoder_layer

= torch.nn.TransformerEncoderLayer(

d_model=12,

nhead=2,

dim_feedforward=24,

DropOut=0.2,

activation=torch.nn.functional.ReLU,

batch_first=True,

norm_first=True)

data = {

'src': x,

'src_mask': encode_attn_mask,

'src_key_padding_mask': key_padding_mask,

}

out = encoder_layer(**data)

out.shape

运行

结果如下：

torch.Size([2, 8, 12])

编码器层初始

化时的参数列表如下。

(1)d_mode：词

向量编码的维度。

(2)nhead：多头注

意力的头数，这个数字必

须可以整除词向量编

码

的维度。

(3)dim_feedforward：在内部计算线性

变换时，投影空间的维

度

。

(4)DropOut：内部计算时DropOut的概率。

(5)activation：内部

计算时使用的激活函数

。

(6)batch_first：输入语句的第一维度是

否是batch_size。

(7)norm_first：PyTorch的Transformer工具层同样支持

标准化

层前置的计算方

法，通过该参数指定即可

。

编码器层计算时的参数

列表如下。

(1)src：已经被编码的

输入语句。

(2)src_mask ：

定 义 是 否 要 忽

略

词 与 词 之 间 的

注 意 力

， 即

encode_attn_mask。

(3)src_key_padding_mask：定义语句中哪些位置

是PAD，以忽

略对PAD的注意力，即

key_padding_mask。

在Transformer模型中，多个编码器层

串联在一起就成了编码

器，在PyTorch当中提供了编码器

层，代码如下：

#第14章/使用编

码器工具层

encoder = torch.nn.TransformerEncoder(

encoder_layer=encoder_layer,

num_layers=3,

norm=torch.nn.LayerNorm(normalized_shape=12))

data = {

'src': x,

'mask': encode_attn_mask,

'src_key_padding_mask': key_padding_mask,

}

out = encoder(**data)

out.shape

运行结果如

下：

torch.Size([2, 8, 12])

编码器初始化时的参

数列表如下。

(1)encoder_layer：要使用的编

码器层。

(2)num_layers：使用几层的编码

器层串联。

(3)norm：要使用的标准

化层实现。

编码器计算时

的参数列表和编码器层

的参数列表相同。

7.解码器

层

接下来看解码器层，虽

然在BERT模型当中不会用到

Transformer

的解码器，但出于内容的

完整性，会把PyTorch提供的Transformer

工具

层都进行介绍。使用解码

器层的示例代码如下：

#第

14章/使用单层解码器工具

层

decoder_layer = torch.nn.TransformerDecoderLayer(

d_model=12,

nhead=2,

dim_feedforward=24,

DropOut=0.2,

activation=torch.nn.functional.ReLU,

batch_first=True,

norm_first=True)

data

= {

'tgt': x,

'memory': x,

'tgt_mask': decode_attn_mask,

'memory_mask': encode_attn_mask,

'tgt_key_padding_mask': key_padding_mask,

'memory_key_padding_mask': key_padding_mask,

}

out = decoder_layer(**data)

out.shape

运行结果如下：

torch.Size([2, 8, 12])

解码器

初始化时的参数列表和

编码器层的相同，表达的

意思也都

相同。

解码器计

算时的参数列表如下。

(1)tgt：解

码输出的目标语句，即target。

(2)memory：编

码器的编码结果，也就是

解码器解码时的根据数

据。

(3)tgt_mask ：

定 义 是 否 要 忽

略 词 与

词 之 间 的

注 意 力 ， 即

decode_attn_mask。

(4)memory_mask：定义

是否要忽略memory内的部分词

与词之

间的注意力，一般

不需要忽略。

(5)tgt_key_padding_mask：定义target内哪些

位置是PAD，以

忽略对PAD的注意

力。

(6)memory_key_padding_mask：定义memory内哪些位置是

PAD，以

忽略对PAD的注意力。

和编码

器一样，同样存在解码器

，使用的示例代码如下：

#第

14章/使用编码器工具层

decoder = torch.nn.TransformerDecoder(

decoder_layer=decoder_layer,

num_layers=3,

norm=torch.nn.LayerNorm(normalized_shape=12))

data = {

'tgt':

x,

'memory': x,

'tgt_mask': decode_attn_mask,

'memory_mask':

encode_attn_mask,

'tgt_key_padding_mask': key_padding_mask,

'memory_key_padding_mask': key_padding_mask,

}

out = decoder(**data)

out.shape

运

行结果如下：

torch.Size([2,

8, 12])

解码器初始

化时的参数列表和编码

器的相同，表达的意思也

都相

同。

解码器计算时的

参数列表和解码器层的

相同，表达的意思也都相

同。

8.完整的Transformer模型

最后，PyTorch提供

了完整的Transformer模型，使用的代

码如

下：

运行结果如下：

torch.Size([2, 8, 12])

Transformer模

型初始化时的参数列表

很多在前面已经介绍过

，

这里只介绍几个特殊的

参数。

(1)custom_encoder：要使用的编码器，如

果指定为None，则

会使用默认

的编码器层堆叠num_encoder_layers层组成

编码器。

(2)custom_decoder：要使用的解码器

，如果指定为None，则

会使用默

认的解码器层堆叠num_decoder_layers层组

成解码器。

Transformer模型计算时的

参数基本在前面已经看

到过，这里不

再赘述。

14.4 手动

实现BERT模型

做完了前期的

准备工作，掌握了必要的

储备知识以后，就可以开

始着手实现BERT模型了。

14.4.1

准备

数据集

1.读取字典

首先读

取字典，代码如下：

#第14章/读

取字典

import pandas

as pd

vocab = pd.read_csv('data/msr_paraphrase_vocab.csv',

index_col='word')

vocab_r = pd.read_csv('data/msr_paraphrase_vocab.csv',

index_col='token')

vocab, vocab_r

运行结果如下：

( token

word

<PAD> 0

<SOS> 1

<EOS> 2

<NUM> 3

<UNK> 4

... ...

eastbound 14784

clouds 14785

repave 14786

complained 14787

dominate 14788

[14789 rows x 1

columns],

word

token

0 <PAD>

1

<SOS>

2 <EOS>

3 <NUM>

4

<UNK>

... ...

14784 eastbound

14785

clouds

14786 repave

14787 complained

14788

dominate

[14789 rows x 1 columns])

同

一份字典被读取了两次

，分别为词到索引的字典

和索引到词的

字典，在后

续的计算中这两份字典

都有用处。

2.读取数据集

接

下来把本次任务中要用

到的数据集定义出来，代

码如下：

#第14章/定义数据集

import torch

class MsrDataset(torch.utils.data.Dataset):

def __init__(self):

data =

pd.read_csv('data/msr_paraphrase_data.csv')

self.data = data

def __len__(self):

return len(self.data)

def __getitem__(self, i):

return

self.data.iloc[i]

dataset = MsrDataset()

len(dataset), dataset[0]

运行结果如下：

(5801,

same 1

s1_lens 16

s2_lens 17

pad_lens 39

sent 1,11,12,13,14,15,16,17,18,19,20,21,22,13,23,2,...

Name: 0, dtype: object)

由于前期

数据的处理工作已经完

成，所以这里所需要做的

工作就

很少了，只需读取

处理好的数据。从输出来

看，共有5801条数据，每

条数据

中包括5个字段。

3.定义数据

整理函数

接下来需要定

义数据整理函数，代码如

下：

#第14章/定义数据整理函

数

import numpy

as np

def collate_fn(data):

#取出数据

same

= [i['same'] for i in data]

sent = [i['sent'] for i in

data]

s1_lens = [i['s1_lens'] for i

in data]

s2_lens = [i['s2_lens'] for

i in data]

pad_lens = [i['pad_lens']

for i in data]

seg =

[]

for i in range(len(sent)):

#seg的形状和sent一

样，但是内容不一样

#补PAD的

位置是0，s1的位置是1，s2的位置

是2

seg.append([1] * s1_lens[i] + [2]

* s2_lens[i] + [0] *

pad_lens[i])

#sent由字符型转换为list

sent = [np.array(i.split(','), dtype=np.int) for

i in sent]

same = torch.LongTensor(same)

sent = torch.LongTensor(sent)

seg = torch.LongTensor(seg)

return same, sent, seg

collate_fn([dataset[0], dataset[1]])

运行

结果如下：

(tensor([1,0]),

tensor([[1,11,12,13,14,15,16,17,18,19,20,21,22,13,23,2,24,25,

26,27,28,18,19,11,12,13,14,20,21,22,13,23,2,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

[1,29,30,31,32,33,34,18,35,25,36,37,3,38,3,3,39,2,

29,40,31,32,37,3,38,3,41,42,43,44,25,36,38,3,3,39,

37,3,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]),

tensor([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,

2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,

2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))

在数据整理函

数中，需要把一批数据整

理为矩阵格式，并且要根

据每条数据生成对应的

seg数据，seg表示Segment，它体现了在一

条数据中哪些位置属于

第一句话，哪些位置属于

第二句话，以及哪些

位置

是PAD，在后续BERT中的计算需要

用到seg数据。

4.定义数据集加

载器

现在可以定义数据

集加载器了，代码如下：

#第

14章/定义数据集加载器

loader = torch.utils.data.DataLoader(dataset=dataset,

batch_size=32,

shuffle=True,

drop_last=True,

collate_fn=collate_fn)

len(loader)

运

行结果如下：

181

可见，共有181个

批次的数据，这个数据量

太小，不足以训练一

个具

有普遍理解力的BERT模型，不

过在本章中仅对BERT计算过

程进

行示例，使用该数据

集已经足够。

5.查看数据样

例

接下来可以查看数据

的样例，代码如下：

#第14章/查

看数据样例

for i, (same, sent, seg)

in enumerate(loader):

break

same, sent.shape, seg.shape,

sent[0], seg[0]

运行结果如

下：

(tensor([1,0,0,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,

0,1,1,1,1,1,0,1]),

torch.Size([32,72]),

torch.Size([32,72]),

tensor([1,1024,88,590,908,359,10694,18,188,37,

69,2305,20,744,1024,880,3339,13538,38,620,

1234,2,1024,500,820,18,188,37,69,1339,

802,20,3339,13538,38,620,880,787,13539,2,

0,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,

0,0]),

tensor([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,

2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,

0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]))

可以看到same中的数据取

值只有0和1两种情况，标识

了一条数

据中两句话表

达的意思是否相同。

sent表示

Sentence，即句子数据。

seg表示Segment，即段信

息。

14.4.2 定义辅助函数

1.定义随

机替换函数

接下来需要

定义random_replace()函数，该函数的作用

是能够随

机地将一条数

据中的某些词替换为MASK，也

就是给BERT模型出题，

BERT需要预

测出这些MASK原本的词，代码

如下：

运行结果如下：

tensor([

5, 7257, 5, ..., 5, 5,

5])

从代

码实现能够看出，被输入

random_replace()函数的所有句子

会被遍

历每个词，每个词都有15%的

概率被替换，而替换也不

仅有替

换为MASK这一种情况

。

在被判定为当前词要替

换后，该词有80%的概率被替

换为

MASK，有10%的概率被替换为

一个随机词，有10%的概率不

替换为

任何词。

使用一个

矩阵记录下每个词是否

被操作过，这里的操作包

括什么

也不做。

以上过程

可以总结为图14-2。

图14-2 random_replace()函数替

换词流程

2.定义MASK函数

在BERT中

需要用到MASK，这里先定义获

取MASK的函数，方便

后续的调

用，代码如下：

#第14章/定义获

取MASK的函数

def get_mask(seg):

#key_padding_mask的定义方式为

句子中PAD的位置为True，否则为

False

key_padding_mask = seg == 0

#在encode阶段不需要定义encode_attn_mask

#定义

为None或者全False都可以

encode_attn_mask = torch.ones(72, 72)

== -1

return key_padding_mask, encode_attn_mask

key_padding_mask,

encode_attn_mask = get_mask(seg)

key_padding_mask.shape, encode_attn_mask.shape,

key_padding_mask[

0], encode_attn_mask

运行结

果如下：

(torch.Size([32,72]),

torch.Size([72,72]),

tensor([False,False,False,False,False,False,False,False,False,F

alse,

False,False,False,False,False,False,False,False,False,False,

False,False,False,False,False,False,False,False,False,False,

False,False,False,False,False,False,False,False,False,False,

False,False,False,False,False,False,False,False,False,True,

True,True,True,True,True,True,True,True,True,True,

True,True,True,True,True,True,True,True,True,True,

True,True]),

tensor([[False,False,False,…,False,False,False],

[False,False,False,…,False,False,False],

[False,False,False,…,False,False,False],

...,

[False,False,False,…,False,False,False],

[False,False,False,…,False,False,False],

[False,False,False,…,False,False,False]]))

14.4.3 定义BERT模型

做完以

上准备工作，现在就可以

定义BERT模型了，代码如下：

运

行结果如下：

(torch.Size([32, 2]), torch.Size([32, 72, 14789]))

可以看到，在

BERT模型中使用了3个编码层

，分别是一般的词编

码、Segment编

码和位置编码，最终的编

码为这3个编码的累加，这

和本章开头时所描述的

BERT模型的架构一致。

编码之

后的句子会被送入编码

器抽取文本特征，这里使

用的编码

器是由PyTorch提供的

Transformer编码器。

BERT在训练阶段有两

个子任务，分别为预测两

句话的意思是否一

致，以

及被遮掩的词的原本的

词。把编码器抽取的文本

特征分别输入

两个线性

神经网络，并且以此计算

这两个输出。

值得注意的

是，在计算same的输出时使用

的不是全量的文本特征

信息，而是只使用了第1个

词的特征信息，每条数据

的第1个词必然是

特殊符

号<SOS>，这原本是没有意义的

词，但由于注意力计算的

原

因，可以认为在这个词

上也包括了整句话的信

息，所以使用该词直接

计

算same的输出是可行的。

这也

是为什么在之前的章节

中使用BERT模型抽取文本特

征，再做

分类预测时只使

用第1个词的特征做分类

的原因。

14.4.4 训练和测试

1.训练

定义好了模型，现在可以

进行训练了，代码如下：

在

这段代码中，每次获取一

批数据并随机遮掩其中

的部分词，再

让BERT模型预测

这些被遮掩的词的原本

的词，在这个过程中不断

训练

BERT对自然语言的理解

能力。

由于BERT有两个子任务

，所以会计算出两份loss，最终

的loss对

这两份loss加权求和即

可。

训练过程的输出见表

14-12，从输出的情况可以看出

，loss是在

不断下降的，两份正

确率也在不断地提高。

表

14-12

训练过程输出

续表

续表

BERT模型的训练需要大数据

量和大计算力，由于数据

量太少，模

型已经被训练

得过拟合了。作为一个示

例程序，主要的目的是演

示

BERT的计算流程。

2.测试

训练

结束后，可以对模型进行

测试，以验证训练的有效

性。

为了便于测试，定义两

个工具函数，第1个是能够

把Tensor转换

为字符串的工具

函数，代码如下：

#第14章/定义

工具函数，把Tensor转换为字符

串

def tensor_to_str(tensor):

#转换为list格式

tensor = tensor.tolist()

#过滤掉PAD

tensor

= [i for i in tensor

if i != vocab.loc['<PAD>'].token]

#转

换为词

tensor

= [vocab_r.loc[i].word for i in tensor]

#转换为字符串

return ' '.join(tensor)

tensor_to_str(sent[0])

运

行结果如下：

'<SOS> among three major candidates schwarzenegger

is

wining the battle for

independents

and crossover voters <EOS> schwarzenegger picks

up

more independents

and crossover voters

than bustamante <EOS>'

这段代码比

较简单，就是把Tensor中的各个

数字使用字典转换为

词

即可。

第2个工具函数是打

印预测结果，代码如下：

#第

14章/定义工具函数，打印预

测结果

def print_predict(same, pred_same, replace_sent, sent,

pred_sent, replace):

#输出same预测结果

same = same[0].item()

pred_same = pred_same.argmax(dim=1)[0].item()

print('same=', same, 'pred_same=',

pred_same)

print()

#输

出句子替换词的预测结

果

replace_sent = tensor_to_str(replace_sent[0])

sent = tensor_to_str(sent[0][replace[0]])

pred_sent = tensor_to_str(pred_sent.argmax(dim=2)[0]

[replace[0]])

print('replace_sent=', replace_sent)

print()

print('sent=', sent)

print()

print('pred_sent=', pred_sent)

print()

print('-------------------------------------')

print_predict(same,

torch.randn(32, 2), replace_sent, sent,

torch.randn(32, 72,

100), replace)

运行结果如下：

 same= 0

pred_same= 1

replace_sent= <SOS> among three

major candidates schwarzenegger

is wining the

battle for independents and crossover <MASK><EOS>

schwarzenegger picks up more

independents <MASK>

crossover <MASK> than bustamante <EOS>

sent=

voters and voters

pred_sent= before hanging

distorting

-------------------------------------

这段代

码同样比较简单，即输出

真实的same和预测的same，并

输出

原句子，以及输出真实的

被遮掩的词和预测的被

遮掩的词。

定义好上面两

个工具函数以后，就可以

进行测试了，代码如下：

运

行结果如下：

从输出结果

可以看出，在本次测试中

模型对数据的拟合能力

很

强，对same的预测正确率达

到了100%，对被遮掩的词的预

测正确率

也达到了87%以上

，但如前所述，这其实是一

个过拟合的结果，实际

训

练BERT时需要更大的数据量

来缓解过拟合。

14.5 小结

本章

介绍了BERT模型的设计思路

，并通过一个示例程序演

示了

BERT模型的计算过程，通

过本章的学习，读者应该

能够理解BERT模

型设计的思

路和计算的原理。

由于训

练数据量较少，模型被训

练得过拟合了，要缓解过

拟合，

可以通过增加数据

量的方法实现，不过这会

进一步增加计算的负担

，

完整的BERT模型的训练需要

大数据量和大算力，所以

无论是出于保护

环境的

角度，还是降低项目风险

的角度，都建议使用预训

练的BERT模

型。

图书推荐

续表
