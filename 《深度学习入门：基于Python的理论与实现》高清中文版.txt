හਁᇇ๦्ก

图灵社区的

电子书没有

采用专有客

户

端，您可以

在任意设备

上，用自己喜

欢的浏览器

和PDF阅读器进

行阅读。

但您

购买的电子

书仅供您个

人使用，

未经

授权，不得进

行传播。 

我们

愿意相信读

者具有这样

的良知和

觉

悟，与我们共

同保护知识

产权。 

如果购

买者有侵权

行为，我们可

能对

该用户

实施包括但

不限于关闭

该帐号

等维

权措施，并可

能追究法律

责任。

图灵程

序 设 计

丛 书

深度学习入

门

基于Python的理

论与实现

Deep

Learning from Scratch

[日

］斋藤康毅

著

陆宇杰 译

Beijing・Boston・Farnham・Sebastopol・Tokyo

O’Reilly

Japan, Inc. 授

权人民邮电

出版社出版

人 民

邮 电 出

版 社

北　　京

内

容

提 要

本书

是深度学习

真正意义上

的入门书，深

入浅出地剖

析了深度学

习的原理和

相关技术。

书

中使用Python

3，尽量

不依赖外部

库或工具，带

领读者从零

创建一个经

典的深度学

习网

络，使读

者在此过程

中逐步理解

深度学习。书

中不仅介绍

了深度学习

和神经网络

的概念、

特征

等基础知识

，对误差反向

传播法、卷积

神经网络等

也有深入讲

解，此外还介

绍了学

习相

关的实用技

巧，自动驾驶

、图像生成、强

化学习等方

面的应用，以

及为什么加

深层

可以提

高识别精度

等“为什么”的

问题。

本书适

合深度学习

初学者阅读

，也可作为高

校教材使用

。

◆ 著

［日］ 斋藤康

毅

 译

陆宇杰

责任编辑

杜

晓静

 执行编

辑 刘香娣

责

任印制 周昇

亮

◆ 人民邮电

出版社出版

发行

北京市

丰台区成寿

寺路11号

 邮编

100164 电子邮件

315@ptpress.com.cn

网

址 http://www.ptpress.com.cn

北京

印刷

◆

开本：880×1230 1/32

 印张：9.625

字

数：300千字 2018年7月

第 1 版

印数：1 - 4 000册

2018年7月北京第

1次印刷

著作

权合同登记

号 图字：01-2017-0526号

定

价：59.00元

读者服

务热线：(010)51095186转600

印

装质量热线

：(010)81055316

反盗版热线

：(010)81055315

广告经营许

可证：京东工

商广登字20170147号

深度学习入

门

: 基于Python的理

论与实现 / (日

)

斋藤康毅著

; 陆宇杰译. -- 北

京

: 人民邮电

出版社, 

2018.7

（图灵

程序设计丛

书）

 ISBN

978-7-115-48558-8

 Ⅰ. ①深…

Ⅱ. ①斋… ②陆… Ⅲ.

①软

件工具－程

序

设计 Ⅳ. ①TP311.561

中国版

本图书馆CIP数

据核字(2018)第112509号

图书在版编

目（ＣＩＰ）数据

版 权

声

明

Copyright © 2016

Koki Saitoh, O’Reilly Japan,

Inc. 

Posts and

Telecommunications Press, 2018.

Authorized

translation of the Japanese

edition of “Deep Learning

from 

Scratch” ©

2016 O’Reilly Japan, Inc.

This translation is published

and sold by

permission of O’Reilly Japan,

Inc., the owner of

all rights to publish

and sell the

same.

日文原

版由O’Reilly Japan, Inc.出版，2016。

简

体中文版由

人民邮电出

版社出版，2018。日

文原版的翻

译得到O’Reilly 

Japan, Inc.的授

权。此简体中

文版的出版

和销售得到

出版权和销

售权的所有

者——O’Reilly

Japan, Inc.的许可。

版

权所有，未得

书面许可，本

书的任何部

分和全部不

得以任何形

式重制。

O’Reilly

Media, Inc.介绍

O’Reilly Media 通过图书、杂

志、在线服务

、调查研究和

会议等方式

传播创新知

识。

自 1978 年开始

，O’Reilly 一直都是前

沿发展的见

证者和推动

者。超级极客

们正在开

创

着未来，而我

们关注真正

重要的技术

趋势——通过放

大那些“细微

的信号”来刺

激

社会对新

科技的应用

。作为技术社

区中活跃的

参与者，O’Reilly 的发

展充满了对

创新

的倡导

、创造和发扬

光大。

O’Reilly 为软件

开发人员带

来革命性的

“动物书”；创建

第一个商业

网站（GNN）；组

织了

影响深远的

开放源代码

峰会，以至于

开源软件运

动以此命名

；创立了 Make

杂志

，

从而成为 DIY 革

命的主要先

锋；公司一如

既往地通过

多种形式缔

结信息与人

的纽带。

O’Reilly 的会

议和峰会集

聚了众多超

级极客和高

瞻远瞩的商

业领袖，共同

描绘出开创

新产业的革

命性思想。作

为技术人士

获取信息的

选择，O’Reilly 现在还

将先锋专家

的

知识传递

给普通的计

算机用户。无

论是通过书

籍出版、在线

服务或者面

授课程，每一

项

O’Reilly 的产品都

反映了公司

不可动摇的

理念——信息是

激发创新的

力量。

业界评

论

“O’Reilly

Radar 博客有口

皆碑。”

——Wired

“O’Reilly

凭借一

系列（真希望

当初我也想

到了）非凡想

法建立了数

百万美元的

业务。”

——Business 2.0

“O’Reilly

Conference 是聚集

关键思想领

袖的绝对典

范。”

——CRN

“一本

O’Reilly 的书

就代表一个

有用、有前途

、需要学习的

主题。”

——Irish Times

“Tim 是位特

立独行的商

人，他不光放

眼于最长远

、最广阔的视

野，并且切实

地按照

Yogi Berra

的建

议去做了：‘如

果你在路上

遇到岔路口

，走小路（岔路

）。’回顾过去，

Tim 似

乎每一次都

选择了小路

，而且有几次

都是一闪即

逝的机会，尽

管大路也不

错。”

——Linux

Journal

目录

译者

序······················································· xiii

前言························································· xv

第1章

Python入门· ···········································

1

1.1 Python是什么

· ·········································

1

1.2 Python的安装· ·········································

2

1.2.1 Python版本

· ·····································

2

1.2.2　使用的外部

库· ····································

2

1.2.3 Anaconda发行版· ·································

3

1.3 Python解

释器· ·········································

4

1.3.1　算术计

算········································· 4

1.3.2　数据类型

········································· 5

1.3.3

变量············································· 5

1.3.4　列表·············································

6

1.3.5　字

典············································· 7

1.3.6　布尔型··········································· 7

1.3.7

if语

句· ·········································· 8

1.3.8

for 语句·········································· 8

1.3.9

函数

············································· 9

1.4 Python脚本文件·

······································· 9

目

录 vi

1.4.1　保存为文

件·······································

9

1.4.2　类· ············································

10

1.5 NumPy· ··············································

11

1.5.1　导入NumPy· ····································

11

1.5.2　生

成NumPy数组································· 12

1.5.3 NumPy 的算

术运算······························· 12

1.5.4 NumPy的N维

数组· ······························ 13

1.5.5　广播············································ 14

1.5.6

访

问元素········································ 15

1.6 Matplotlib·············································

16

1.6.1　绘制

简单图形· ···································

16

1.6.2 pyplot的

功能· ···································

17

1.6.3　显示图

像········································ 18

1.7 小结·················································· 19

第2章

感知机················································

21

2.1 感知

机是什么· ·········································

21

2.2 简

单逻辑电路

· ·········································

23

2.2.1　与门············································ 23

2.2.2　与非门

和或门· ··································· 23

2.3 感知

机的实现· ········································· 25

2.3.1　简

单的实现······································ 25

2.3.2

导

入权重和偏

置· ································· 26

2.3.3

使用权重

和偏置的实

现· ··························· 26

2.4

感知机的

局限性· ······································· 28

2.4.1

异或

门·········································· 28

2.4.2　线性和非

线性·

··································· 30

2.5 多层感

知机············································

31

2.5.1　已有门

电路的组合

· ·······························

31

目录  vii

2.5.2　异或门

的实现· ··································· 33

2.6 从与

非门到计算

机· ····································· 35

2.7 小结·················································· 36

第3章

神经网络··············································

37

3.1 从

感知机到神

经网络· ···································

37

3.1.1　神经

网络的例子

· ·································

37

3.1.2　复习感知机

······································ 38

3.1.3　激活函数登

场· ··································· 40

3.2 激活函数

·············································· 42

3.2.1

sigmoid函数· ···································· 42

3.2.2

阶跃函

数的实现· ································· 43

3.2.3

阶

跃函数的图

形· ································· 44

3.2.4

sigmoid函数的实

现· ······························ 45

3.2.5

sigmoid函数和阶

跃函数的比

较······················ 46

3.2.6　非线性函

数······································

48

3.2.7 ReLU函数· ·····································

49

3.3 多维

数组的运算

· ·······································

50

3.3.1　多维数组········································ 50

3.3.2　矩

阵乘法········································ 51

3.3.3

神经

网络的内积

· ································· 55

3.4

3层神经网络

的实现· ···································· 56

3.4.1

符号

确认········································ 57

3.4.2　各层间

信号传递的

实现·

··························· 58

3.4.3　代码实

现小结·

··································· 62

3.5 输出

层的设计·

········································· 63

3.5.1　恒

等函数和softmax函

数·

·························· 64

3.5.2　实现softmax函数

时的注意事

项·

···················· 66

3.5.3 softmax函数的特

征·

······························ 67

目录 viii

3.5.4　输出

层的神经元

数量·

····························· 68

3.6 手写数

字识别·

········································· 69

3.6.1 MNIST数据

集·

·································· 70

3.6.2　神经网络

的推理处理

·

····························· 73

3.6.3　批处理··········································

75

3.7 小结

·················································· 79

第4章　神经网

络的学习· ······································· 81

4.1 从

数据中学习

· ········································· 81

4.1.1　数据驱动········································ 82

4.1.2

训

练数据和测

试数据· ····························· 84

4.2

损失

函数·············································· 85

4.2.1　均方误

差········································

85

4.2.2　交叉熵误

差······································ 87

4.2.3 mini-batch学习· ································· 88

4.2.4 mini-batch版交

叉熵误差的

实现· ···················· 91

4.2.5　为何要

设定损失函

数· ····························· 92

4.3 数值微分

·············································· 94

4.3.1

导数············································ 94

4.3.2　数值微

分的例子·

································· 96

4.3.3　偏

导数··········································

98

4.4 梯度··················································100

4.4.1

梯

度法··········································102

4.4.2　神经网

络的梯度· ·································106

4.5 学

习算法的实

现· ·······································109

4.5.1

2层神经网

络的类·································110

4.5.2 mini-batch的实

现· ·······························114

4.5.3　基于测试

数据的评价

· ·····························116

4.6

小结··················································118

目录  ix

第

5章　误差反向

传播法· ·······································121

5.1

计算

图················································121

5.1.1　用计算图

求解· ···································122

5.1.2　局部计

算········································124

5.1.3　为何用计

算图解题·

·······························125

5.2 链

式法则··············································126

5.2.1

计算

图的反向传

播· ·······························127

5.2.2　什么是链

式法则·

·································127

5.2.3　链式

法则和计算

图· ·······························129

5.3 反向传播

··············································130

5.3.1　加法节点的

反向传播·

·····························130

5.3.2　乘

法节点的反

向传播· ·····························132

5.3.3　苹果

的例子······································133

5.4 简单

层的实现·

·········································135

5.4.1　乘

法层的实现

· ···································135

5.4.2　加法层的实

现· ···································137

5.5

激活函数

层的实现· ·····································139

5.5.1 ReLU层

·

·······································139

5.5.2 Sigmoid层·······································141

5.6

Affine/Softmax层的实现

·································144

5.6.1 Affine层· ·······································144

5.6.2　批版本的

Affine层· ·······························148

5.6.3

Softmax-with-Loss 层· ····························150

5.7

误差反

向传播法的

实现· ·································154

5.7.1　神经网

络学习的全

貌图·

···························154

5.7.2　对应误

差反向传播

法的神经网

络的实现· ··············155

5.7.3　误

差反向传播

法的梯度确

认·························158

5.7.4　使用误差

反向传播法

的学习·························159

5.8 小结

··················································161

目录 x

第6章　与

学习相关的

技巧·

·····································163

6.1 参数的

更新············································163

6.1.1

探险家

的故事· ···································164

6.1.2 SGD·

··········································164

6.1.3 SGD的缺

点· ····································166

6.1.4 Momentum······································168

6.1.5 AdaGrad········································170

6.1.6 Adam· ·········································172

6.1.7

使用哪种

更新方法呢

· ·····························174

6.1.8　基于MNIST数据集

的更新方法

的比较················175

6.2 权重

的初始值· ·········································176

6.2.1

可

以将权重初

始值设为0吗

· ························176

6.2.2　隐藏层的激

活值的分布

·

···························177

6.2.3 ReLU的权重初始

值·······························181

6.2.4

基于MNIST数据

集的权重初

始值的比较

· ·············183

6.3 Batch

Normalization· ···································184

6.3.1 Batch

Normalization的算法· ·······················184

6.3.2 Batch

Normalization的评

估· ·······················186

6.4 正则化················································188

6.4.1　过

拟合··········································189

6.4.2　权值衰

减········································191

6.4.3 Dropout· ·······································192

6.5

超参数的

验证· ·········································195

6.5.1　验证数

据········································195

6.5.2　超参数的

最优化· ·································196

6.5.3

超参

数最优化的

实现· ·····························198

6.6 小结··················································200

目

录  xi

第7章

卷积

神经网络· ·········································201

7.1 整

体结构··············································201

7.2 卷积

层················································202

7.2.1　全连接层

存在的问题

·

·····························203

7.2.2　卷积运算········································203

7.2.3

填

充············································206

7.2.4　步幅············································207

7.2.5

3维数

据的卷积运

算· ······························209

7.2.6　结合方块

思考·

···································211

7.2.7　批处理

··········································213

7.3

池化层················································214

7.4 卷积

层和池化层

的实现· ·································216

7.4.1 4维数

组· ·······································216

7.4.2

基于im2col的展

开· ·······························217

7.4.3　卷积层的

实现·

···································219

7.4.4　池化层

的实现· ···································222

7.5 CNN的实

现· ··········································224

7.6

CNN的可视化

· ········································228

7.6.1　第1层权重的

可视化·······························228

7.6.2　基于

分层结构的

信息提取· ·························230

7.7

具

有代表性的

CNN·····································231

7.7.1 LeNet· ·········································231

7.7.2 AlexNet·········································232

7.8 小结··················································233

第8章　深

度学习··············································235

8.1 加深

网络··············································235

8.1.1　向更深

的网络出发

· ·······························235

8.1.2

进一步提高

识别精度· ·····························238

目

录 xii

8.1.3　加深层的

动机·

···································240

8.2 深度学

习的小历史

· ·····································242

8.2.1 ImageNet· ······································243

8.2.2

VGG· ··········································244

8.2.3 GoogLeNet·

·····································245

8.2.4 ResNet· ········································246

8.3 深度学习的

高速化· ·····································248

8.3.1

需要

努力解决的

问题· ·····························248

8.3.2　基于GPU的

高速化·

······························249

8.3.3　分布

式学习······································250

8.3.4

运算

精度的位数

缩减· ·····························252

8.4 深度学

习的应用案

例·

···································253

8.4.1　物体检测

········································253

8.4.2

图像分割········································255

8.4.3　图

像标题的生

成· ·································256

8.5 深度学习

的未来· ·······································258

8.5.1

图像

风格变换· ···································258

8.5.2　图

像的生成······································259

8.5.3　自

动驾驶········································261

8.5.4 Deep

Q-Network（强化

学习）· ·······················262

8.6 小结··················································264

附

录A Softmax-with-Loss层的计算

图· ···························267

A.1

正向传播

· ············································268

A.2 反向传播·

············································270

A.3 小

结· ················································277

参考文献

·····················································279

译者序

深度

学习的浪潮

已经汹涌澎

湃了一段时

间了，市面上

相关的图书

也已经

出版

了很多。其中

，既有知名学

者伊恩·古德

费洛（Ian

Goodfellow）等人撰

写的系统介

绍深度学习

基本理论的

《深度学习》，也

有各种介绍

深度学习框

架的使用方

法的入门书

。你可能会问

，现在再出一

本关于深度

学习的书，是

不是“为时已

晚”？其实并非

如此，因为本

书考察深度

学习的角度

非常独特，

它

的出版可以

说是“千呼万

唤始出来”。

本

书最大的特

点是“剖解”了

深度学习的

底层技术。正

如美国物理

学家

理查德

·费曼（Richard

Phillips Feynman）所说：“What I cannot

create, I 

do

not understand.”只

有创造一个

东西，才算真

正弄懂了一

个问题。本书

就

是教你如

何创建深度

学习模型的

一本书。并且

，本书不使用

任何现有的

深度

学习框

架，尽可能仅

使用最基本

的数学知识

和Python库，从零讲

解深度学

习

核心问题的

数学原理，从

零创建一个

经典的深度

学习网络。

本

书的日文版

曾一度占据

了东京大学

校内书店（本

乡校区）理工

类图书

的畅

销书榜首。各

类读者阅读

本书，均可有

所受益。对于

非AI方向的技

术

人员，本书

将大大降低

入门深度学

习的门槛；对

于在校的大

学生、研究生

，

本书不失为

学习深度学

习的一本好

教材；即便是

对于在工作

中已经熟练

使用

框架开

发各类深度

学习模型的

读者，也可以

从本书中获

得新的体会

。

本书从开始

翻译到出版

，前前后后历

时一年之久

。译者翻译时

力求忠于

原

文，表达简练

。为了保证翻

译质量，每翻

译完一章后

，译者都会放

置一段

译者

序 xiv

时间，再重

新检查一遍

。图灵公司的

专业编辑们

又进一步对

译稿进行了

全面

细致的

校对，提出了

许多宝贵意

见，在此表示

感谢。但是，由

于译者才疏

学浅，

书中难

免存在一些

错误或疏漏

，恳请读者批

评指正，以便

我们在重印

时改正。

最后

，希望本书的

出版能为国

内的AI技术社

区添砖加瓦

！

陆宇杰

2018年2月

上海

前言

科

幻电影般的

世界已经变

成了现实—人

工智能战胜

过日本将棋

、国际

象棋的

冠军，最近甚

至又打败了

围棋冠军；智

能手机不仅

可以理解人

们说的话，

还

能在视频通

话中进行实

时的“机器翻

译”；配备了摄

像头的“自动

防撞的车”

保

护着人们的

生命安全，自

动驾驶技术

的实用化也

为期不远。环

顾我们的四

周，原来被认

为只有人类

才能做到的

事情，现在人

工智能都能

毫无差错地

完

成，甚至试

图超越人类

。因为人工智

能的发展，我

们所处的世

界正在逐渐

变

成一个崭

新的世界。

在

这个发展速

度惊人的世

界背后，深度

学习技术在

发挥着重要

作用。对

于深

度学习，世界

各地的研究

人员不吝褒

奖之辞，称赞

其为革新性

技术，甚

至有

人认为它是

几十年才有

一次的突破

。实际上，深度

学习这个词

经常出现

在

报纸和杂志

中，备受关注

，就连一般大

众也都有所

耳闻。

本书就

是一本以深

度学习为主

题的书，目的

是让读者尽

可能深入地

理解

深度学

习的技术。因

此，本书提出

了“从零开始

”这个概念。

本

书的特点是

通过实现深

度学习的过

程，来逼近深

度学习的本

质。通过

实现

深度学习的

程序，尽可能

无遗漏地介

绍深度学习

相关的技术

。另外，本

书还

提供了实际

可运行的程

序，供读者自

己进行各种

各样的实验

。

为了实现深

度学习，我们

需要经历很

多考验，花费

很长时间，但

是相应

地也

能学到和发

现很多东西

。而且，实现深

度学习的过

程是一个有

趣的、令

前言

xvi

人兴奋的过

程。希望读者

通过这一过

程可以熟悉

深度学习中

使用的技术

，并

能从中感

受到快乐。

目

前，深度学习

活跃在世界

上各个地方

。在几乎人手

一部的智能

手机中、

开启

自动驾驶的

汽车中、为Web服

务提供动力

的服务器中

，深度学习都

在

发挥着作

用。此时此刻

，就在很多人

没有注意到

的地方，深度

学习正在默

默

地发挥着

其功能。今后

，深度学习势

必将更加活

跃。为了让读

者理解深度

学

习的相关

技术，感受到

深度学习的

魅力，笔者写

下了本书。

本

书的理念

本

书是一本讲

解深度学习

的书，将从最

基础的内容

开始讲起，逐

一介绍

理解

深度学习所

需的知识。书

中尽可能用

平实的语言

来介绍深度

学习的概念

、

特征、工作原

理等内容。不

过，本书并不

是只介绍技

术的概要，而

是旨在让

读

者更深入地

理解深度学

习。这是本书

的特色之一

。

那么，怎么才

能更深入地

理解深度学

习呢？在笔者

看来，最好的

办法就

是亲

自实现。从零

开始编写可

实际运行的

程序，一边看

源代码，一边

思考。

笔者坚

信，这种做法

对正确理解

深度学习（以

及那些看上

去很高级的

技术）

是很重

要的。这里用

了“从零开始

”一词，表示我

们将尽可能

地不依赖外

部

的现成品

（库、工具等）。也

就是说，本书

的目标是，尽

量不使用内

容不明的

黑

盒，而是从自

己能理解的

最基础的知

识出发，一步

一步地实现

最先进的深

度学习技术

。并通过这一

实现过程，使

读者加深对

深度学习的

理解。

如果把

本书比作一

本关于汽车

的书，那么本

书并不会教

你怎么开车

，其

着眼点不

是汽车的驾

驶方法，而是

要让读者理

解汽车的原

理。为了让读

者理

解汽车

的结构，必须

打开汽车的

引擎盖，把零

件一个一个

地拿在手里

观察，

并尝试

操作它们。之

后，用尽可能

简单的形式

提取汽车的

本质，并组装

汽车

模型。本

书的目标是

，通过制造汽

车模型的过

程，让读者感

受到自己可

以实

际制造

出汽车，并在

这一过程中

熟悉汽车相

关的技术。

为

了实现深度

学习，本书使

用了Python这一编

程语言。Python非常

受

欢迎，初学

者也能轻松

使用。Python尤其适

合用来制作

样品（原型），使

用

前言

xvii

Python可以

立刻尝试突

然想到的东

西，一边观察

结果，一边进

行各种各样

的实验。本书

将在讲解深

度学习理论

的同时，使用

Python实现程序，进

行

各种实验

。

在光看数学

式和理论说

明无法理解

的情况下，可

以尝试阅读

源代码

并运

行，很多时候

思路都会变

得清晰起来

。对数学式感

到困惑时，

就

阅读源代码

来理解技术

的流程，这样

的事情相信

很多人都经

历过。

本书通

过实际实现

（落实到代码

）来理解深度

学习，是一本

强调“工程”

的

书。书中会出

现很多数学

式，但同时也

会有很多程

序员视角的

源代码。

本书

面向的读者

本书旨在让

读者通过实

际动手操作

来深入理解

深度学习。为

了明确本书

的读者对象

，这里将本书

涉及的内容

列举如下。

• 使

用Python，尽可能少

地使用外部

库，从零开始

实现深度学

习的程序。

•

为

了让Python的初学

者也能理解

，介绍Python的使用

方法。

• 提供实

际可运行的

Python源代码，同时

提供可以让

读者亲自实

验的

学习环

境。

• 从简单的

机器学习问

题开始，最终

实现一个能

高精度地识

别图像的系

统。

• 以简明易

懂的方式讲

解深度学习

和神经网络

的理论。

• 对于

误差反向传

播法、卷积运

算等乍一看

很复杂的技

术，使读者能

够

在实现层

面上理解。

•

介

绍一些学习

深度学习时

有用的实践

技巧，如确定

学习率的方

法、权

重的初

始值等。

• 介绍

最近流行的

Batch

Normalization、Dropout、Adam等，并进行

实

现。

• 讨论为什

么深度学习

表现优异、为

什么加深层

能提高识别

精度、为什

么

隐藏层很重

要等问题。

• 介

绍自动驾驶

、图像生成、强

化学习等深

度学习的应

用案例。

前言

xviii

本书不面向

的读者

明确

本书不适合

什么样的读

者也很重要

。为此，这里将

本书不会涉

及的

内容列

举如下。

• 不介

绍深度学习

相关的最新

研究进展。

• 不

介绍Caffe、TensorFlow、Chainer等深度

学习框架的

使用方法。

• 不

介绍深度学

习的详细理

论，特别是神

经网络相关

的详细理论

。

• 不详细介绍

用于提高识

别精度的参

数调优相关

的内容。

• 不会

为了实现深

度学习的高

速化而进行

GPU相关的实现

。

• 本书以图像

识别为主题

，不涉及自然

语言处理或

者语音识别

的例子。

综上

，本书不涉及

最新研究和

理论细节。但

是，读完本书

之后，读者

应

该有能力进

一步去阅读

最新的论文

或者神经网

络相关的理

论方面的技

术书。

本书以

图像识别为

主题，主要学

习使用深度

学习进行图

像识别时

所

需的技术。自

然语言处理

或者语音识

别等不是本

书的讨论对

象。

本书的阅

读方法

学习

新知识时，只

听别人讲解

的话，有时会

无法理解，或

者会立刻忘

记。

正如“不闻

不若闻之，闻

之不若见之

，见之不若知

之，知之不若

行之”A ，在

学习

新东西时，没

有什么比实

践更重要了

。本书在介绍

某个主题时

，都细心

地准

备了一个可

以实践的场

所——能够作为

程序运行的

源代码。

本书

会提供Python源代

码，读者可以

自己动手实

际运行这些

源代码。

在阅

读源代码的

同时，可以尝

试去实现一

些自己想到

的东西，以确

保真正

A 出自

荀子《儒效篇

》。

前言

xix

理解了

。另外，读者也

可以使用本

书的源代码

，尝试进行各

种实验，反复

试错。

本书将

沿着“理论说

明”和“Python实现”两

个路线前进

。因此，建议

读

者准备好编

程环境。本书

可以使用Windows、Mac、Linux中

的任何一个

系统。关于Python的

安装和使用

方法将在第

1章介绍。另外

，本书中用到

的程序可以

从以下网址

下载。

http://www.ituring.com.cn/book/1921

让我们

开始吧

通过

前面的介绍

，希望读者了

解本书大概

要讲的内容

，产生继续阅

读的

兴趣。

最

近出现了很

多深度学习

相关的库，任

何人都可以

方便地使用

。实际上，

使用

这些库的话

，可以轻松地

运行深度学

习的程序。那

么，为什么我

们还要

特意

花时间从零

开始实现深

度学习呢？一

个理由就是

，在制作东西

的过程中

可

以学到很多

。

在制作东西

的过程中，会

进行各种各

样的实验，有

时也会卡住

，抱着脑

袋想

为什么会这

样。这种费时

的工作对深

刻理解技术

而言是宝贵

的财富。像

这

样认真花费

时间获得的

知识在使用

现有的库、阅

读最新的文

章、创建原创

的系统时都

大有用处。而

且最重要的

是，制作本身

就是一件快

乐的事情。（还

需要快乐以

外的其他什

么理由吗？）

既

然一切都准

备好了，下面

就让我们踏

上实现深度

学习的旅途

吧！

表述规则

本书在表述

上采用如下

规则。

粗体字

（Bold）

用来表示新

引入的术语

、强调的要点

以及关键短

语。

前言 xx

等宽

字（Constant Width）

用来表示

下面这些信

息：程序代码

、命令、序列、组

成元素、语句

选项、

分支、变

量、属性、键值

、函数、类型、类

、命名空间、方

法、模块、属性

、

参数、值、对象

、事件、事件处

理器、XML标签、HTML标

签、宏、文件

的

内容、来自命

令行的输出

等。若在其他

地方引用了

以上这些内

容（如变量、

函

数、关键字等

），也会使用该

格式标记。

等

宽粗体字（Constant Width Bold）

用

来表示用户

输入的命令

或文本信息

。在强调代码

的作用时也

会使用该

格

式标记。

等宽

斜体字（Constant Width

Italic）

用来

表示必须根

据用户环境

替换的字符

串。

用来表示

提示、启发以

及某些值得

深究的内容

的补充信息

。

表示程序库

中存在的

bug或

时常会发生

的问题等警

告信息，引

起

读者对该处

内容的注意

。

读者意见与

咨询

虽然笔

者已经尽最

大努力对本

书的内容进

行了验证与

确认，但仍不

免在

某些地

方出现错误

或者容易引

起误解的表

达等，给读者

的理解带来

困扰。如

果读

者遇到这些

问题，请及时

告知，我们在

本书重印时

会将其改正

，在此先

表示

不胜感激。与

此同时，也希

望读者能够

为本书将来

的修订提出

中肯的建

议

。本书编辑部

的联系方式

如下。

前言  xxi

株

式会社O’Reilly

Japan 

电子

邮件 japan@oreilly.co.jp

本书的

主页地址如

下。

http://www.ituring.com.cn/book/1583

http://www.oreilly.co.jp/books/9784873117584（日语）

https://github.com/oreilly-japan/deep-learning-from-scratch

关于

O’Reilly的其他信息

，可以访问下

面的O’Reilly主页查

看。

http://www.oreilly.com/（英语）

http://www.oreilly.co.jp/（日语

）

致谢

首先，笔

者要感谢推

动了深度学

习相关技术

（机器学习、计

算机科学等

）

发展的研究

人员和工程

师。本书的完

成离不开他

们的研究工

作。其次，笔者

还要感谢在

图书或网站

上公开有用

信息的各位

同仁。其中，斯

坦福大学的

CS231n [5]公开课慷慨

提供了很多

有用的技术

和信息，笔者

从中学到了

很多东西。

在

本书执笔过

程中，曾受到

下列人士的

帮助：teamLab公司的

加藤哲朗、

喜

多慎弥、飞永

由夏、中野皓

太、中村将达

、林辉大、山本

辽；Top Studio

公司的武

藤健志、增子

萌；Flickfit公司的野

村宪司；得克

萨斯大学奥

斯汀

分校JSPS海

外特别研究

员丹野秀崇

。他们阅读了

本书原稿，提

出了很多宝

贵的建议，在

此深表谢意

。另外，需要说

明的是，本书

中存在的不

足或错误

均

是笔者的责

任。

最后，还要

感谢O’Reilly Japan的宮川

直树，在从本

书的构想到

完成的

大约

一年半的时

间里，宫川先

生一直支持

着笔者。非常

感谢！

2016年9月1日

斋藤康毅

第

1章

Python入门

Python这一

编程语言已

经问世20多年

了，在这期间

，Python不仅完成

了

自身的进化

，还获得了大

量的用户。现

在，Python作为最具

人气的编程

语言，

受到了

许多人的喜

爱。

接下来我

们将使用Python实

现深度学习

系统。不过在

这之前，本章

将简

单地介

绍一下Python，看一

下它的使用

方法。已经掌

握了Python、NumPy、

Matplotlib等知识

的读者，可以

跳过本章，直

接阅读后面

的章节。

1.1 Python是什

么

Python是一个简

单、易读、易记

的编程语言

，而且是开源

的，可以免

费

地自由使用

。Python可以用类似

英语的语法

编写程序，编

译起来也不

费

力，因此我

们可以很轻

松地使用Python。特

别是对首次

接触编程的

人士来说，

Python是

最合适不过

的语言。事实

上，很多高校

和大专院校

的计算机课

程

均采用Python作

为入门语言

。

此外，使用Python不

仅可以写出

可读性高的

代码，还可以

写出性能高

（处

理速度快

）的代码。在需

要处理大规

模数据或者

要求快速响

应的情况下

，使

用Python可以稳

妥地完成。因

此，Python不仅受到

初学者的喜

爱，同时也

受

到专业人士

的喜爱。实际

上，Google、Microsoft、Facebook等战斗在

IT

行业最前沿

的企业也经

常使用Python。

  第

1章

Python入门 2

再者，在

科学领域，特

别是在机器

学习、数据科

学领域，Python也被

大量使用。Python除

了高性能之

外，凭借着 NumPy、SciPy等

优秀的数

值

计算、统计分

析库，在数据

科学领域占

有不可动摇

的地位。深度

学习的

框架

中也有很多

使用Python的场景

，比如Caffe、TensorFlow、Chainer、

Theano等著名

的深度学习

框架都提供

了Python接口。因此

，学习Python

对使用

深度学习框

架大有益处

。

综上，Python是最适

合数据科学

领域的编程

语言。而且，Python具

有

受众广的

优秀品质，从

初学者到专

业人士都在

使用。因此，为

了完成本书

的

从零开始

实现深度学

习的目标，Python可

以说是最合

适的工具。

1.2

Python的

安装

下面，我

们首先将Python安

装到当前环

境（电脑）上。这

里说明一下

安

装时需要

注意的一些

地方。

1.2.1

Python版本

Python有

Python 2.x和Python 3.x两个版本

。如果我们调

查一下目前

Python的使用情况

，会发现除了

最新的版本

3.x以外，旧的版

本2.x仍在被

大

量使用。因此

，在安装Python时，需

要慎重选择

安装Python的哪个

版

本。这是因

为两个版本

之间没有兼

容性（严格地

讲，是没有“向

后兼容性”），

也

就是说，会发

生用Python 3.x写的代

码不能被Python

2.x执

行的情况。

本

书中使用Python 3.x，只

安装了Python 2.x的读

者建议另外

安装一下

Python 3.x。

1.2.2　使

用的外部库

本书的目标

是从零开始

实现深度学

习。因此，除了

NumPy库和Matplotlib

库之外

，我们极力避

免使用外部

库。之所以使

用这两个库

，是因为它们

可以

有效地

促进深度学

习的实现。

1.2

Python的

安装 3

NumPy是用于

数值计算的

库，提供了很

多高级的数

学算法和便

利的数

组（矩

阵）操作方法

。本书中将使

用这些便利

的方法来有

效地促进深

度学习

的实

现。

Matplotlib是用来画

图的库。使用

Matplotlib能将实验结

果可视化，并

在视觉上确

认深度学习

运行期间的

数据。

本书将

使用下列编

程语言和库

。

•

Python 3.x（2016年 8月时的最

新版本是 3.5）

• NumPy

• Matplotlib

下

面将为需要

安装Python的读者

介绍一下Python的

安装方法。已

经安

装了Python的

读者，请跳过

这一部分内

容。

1.2.3　Anaconda发行版

Python的

安装方法有

很多种，本书

推荐使用Anaconda这

个发行版。发

行版集成了

必要的库，使

用户可以一

次性完成安

装。Anaconda是一个侧

重

于数据分

析的发行版

，前面说的NumPy、Matplotlib等

有助于数据

分析的

库都

包含在其中

A。

如前所述，本

书将使用Python

3.x版

本，因此Anaconda发行

版也要安

装

3.x的版本。请读

者从官方网

站下载与自

己的操作系

统相应的发

行版，然

后安

装。

A

Anaconda作为一个

针对数据分

析的发行版

，包含了许多

有用的库，而

本书中实际

上只会使用

其中的

NumPy库和

Matplotlib库。因此，如果

想保持轻量

级的开发环

境，单独安装

这两个库也

是可以的。

——译

者注

第 1章　Python入

门 4

1.3 Python解释器

完

成Python的安装后

，要先确认一

下Python的版本。打

开终端（Windows

中的

命令行窗口

），输入python

--version命令，该

命令会输出

已经安装的

Python的版本信息

。



$ python --version

Python 3.4.1 :: Anaconda 2.1.0 (x86_64)

如上所示，显示了Python 3.4.1（根据实际安装的版本，版本号可能不同），

说明已正确安装了Python 3.x。接着输入python，启动Python解释器。

$



python

Python 3.4.1 |Anaconda

2.1.0 (x86_64)| (default, Sep

10 2014, 17:24:09)

[GCC

4.2.1 (Apple Inc. build

5577)] on darwin

Type

"help", "copyright", "credits" or

"license" for more information.

>>>

Python解释器也被

称为“对话模

式”，用户能够

以和Python对话的

方式

进行编

程。比如，当用

户询问“1 +

2等于

几？”的时候，Python解

释器会回

答

“3”，所谓对话模

式，就是指这

样的交互。现

在，我们实际

输入一下看

看。

>>> 1

+ 2

3

Python解释器可

以像这样进

行对话式（交

互式）的编程

。下面，我们使

用这个对话

模式，来看几

个简单的Python编

程的例子。

1.3.1　算

术计算

加法

或乘法等算

术计算，可按

如下方式进

行。

>>>

1 - 2

-1

>>> 4 * 5

20

1.3  Python解释器

5

>>> 7 /

5

1.4

>>> 3

** 2

9

*表

示乘法，/表示

除法，**表示乘

方（3**2是

3 的 2 次方

）。另外，在

Python 2.x中，整

数除以整数

的结果是整

数，比如，7 ÷ 5的结

果是1。但在

Python 3.x中

，整数除以整

数的结果是

小数（浮点数

）。

1.3.2　数据类型

编

程中有数据

类型（data type）这一概

念。数据类型

表示数据的

性质，

有整数

、小数、字符串

等类型。Python中的

type()函数可以用

来查看数据

类型。

>>>

type(10)

<class 'int'>

>>>

type(2.718)

<class 'float'>

>>>

type("hello")

<class 'str'>

根据上

面的结果可

知，10是int类型（整

型），2.718是float类型（浮

点型），

"hello"是str（字符

串）类型。另外

，“类型”和“类”这

两个词有时

用作相同

的

意思。这里，对

于输出结果

<class 'int'>，可以将其解

释成“10是int类（类

型）”。

1.3.3

变量

可以

使用x或y等字

母定义变量

（variable）。此外，可以使

用变量进行

计算，

也可以

对变量赋值

。

>>>

x = 10 #

初始化

>>> print(x) #

输出

x 

10

>>>

x = 100 #

赋值

>>> print(x)

100

第 1章　Python入

门

6

>>> y =

3.14

>>> x *

y

314.0

>>> type(x

* y)

<class 'float'>

Python是属于“动

态类型语言

”的编程语言

，所谓动态，是

指变量的类

型是根据情

况自动决定

的。在上面的

例子中，用户

并没有明确

指出“x的类

型

是int（整型）”，是Python根

据x被初始化

为10，从而判断

出x的类型为

int的。此外，我们

也可以看到

，整数和小数

相乘的结果

是小数（数据

类型的

自动

转换）。另外，“#”是

注释的意思

，它后面的文

字会被Python忽略

。

1.3.4

列表

除了单

一的数值，还

可以用列表

（数组）汇总数

据。

>>> a

= [1, 2, 3,

4, 5] # 生成列表

>>>

print(a) # 输出列表的

内容

[1,

2, 3, 4, 5]

>>> len(a) # 获取列

表的长度

5

>>> a[0] #

访

问第一个元

素的值

1

>>> a[4]

5

>>> a[4] =

99 # 赋值

>>> print(a)

[1, 2, 3, 4,

99]

元素的访问

是通过a[0]这样

的方式进行

的。[]中的数字

称为索引（下

标），

索引从0开

始（索引0对应

第一个元素

）。此外，Python的列表

提供了切片

（slicing）这一便捷的

标记法。使用

切片不仅可

以访问某个

值，还可以访

问列

表的子

列表（部分列

表）。

>>> print(a)

[1, 2,

3, 4, 99]

>>>

a[0:2] # 获取索引

为0到2（不包括

2！）的元素

[1,

2]

>>> a[1:] #

获取

从索引为1的

元素到最后

一个元素

[2, 3, 4,

99]

1.3 Python解

释器

7

>>> a[:3] #

获取从

第一个元素

到索引为3（不

包括3！）的元素

[1, 2, 3]

>>>

a[:-1] # 获取从第一

个元素到最

后一个元素

的前一个元

素之间的元

素

[1,

2, 3, 4]

>>>

a[:-2] # 获取从第

一个元素到

最后一个元

素的前二个

元素之间的

元素

[1,

2, 3]

进行列

表的切片时

，需要写成a[0:2]这

样的形式。a[0:2]用

于取出从索

引为0的元素

到索引为2的

元素的前一

个元素之间

的元素。另外

，索引−1对

应最

后一个元素

，−2对应最后一

个元素的前

一个元素。

1.3.5　字

典

列表根据

索引，按照0, 1,

2, ...的

顺序存储值

，而字典则以

键值对的形

式存储数据

。字典就像《新

华字典》那样

，将单词和它

的含义对应

着存储起来

。

>>> me

= {'height':180} # 生成字典

>>> me['height'] # 访

问元素

180

>>> me['weight'] =

70 # 添加

新元素

>>>

print(me)

{'height': 180, 'weight':

70}

1.3.6　布尔

型

Python中有bool型。bool型

取True或False中的一

个值。针对bool型

的

运算符包

括and、or和not（针对数

值的运算符

有+、-、*、/等，根据不

同的

数据类

型使用不同

的运算符）。

>>> hungry

= True # 饿

了？

>>> sleepy = False

# 困了？

>>> type(hungry)

<class 'bool'>

>>> not

hungry

False

>>> hungry

and sleepy # 饿并

且困

False

>>> hungry or

sleepy # 饿或者

困

True

第 1章　Python入门

8

1.3.7　if语句

根据不

同的条件选

择不同的处

理分支时可

以使用if/else语句

。

>>>

hungry = True

>>>

if hungry:

... print("I'm

hungry")

...

I'm hungry

>>> hungry = False

>>> if hungry:

...

print("I'm hungry") # 使用空白字

符进行缩进

...

else:

... print("I'm not

hungry")

... print("I'm sleepy")

...

I'm not hungry

I'm sleepy

Python中的空白字

符具有重要

的意义。上面

的if语句中，if hungry:下

面

的语句开

头有4个空白

字符。它是缩

进的意思，表

示当前面的

条件（if hungry）

成立时

，此处的代码

会被执行。这

个缩进也可

以用tab表示，Python中

推荐

使用空

白字符。

Python使用

空白字符表

示缩进。一般

而言，每缩进

一次，使用 4

个

空白字符。

1.3.8

for 语

句

进行循环

处理时可以

使用for语句。

>>>

for i in [1,

2, 3]:

... print(i)

...

1

2

3

这

是输出列表

[1, 2, 3]中的元素的

例子。使用for ���

in ��� :语

句结构，

1.4

Python脚本

文件  9

可以按

顺序访问列

表等数据集

合中的各个

元素。

1.3.9　函数

可

以将一连串

的处理定义

成函数（function）。

>>>

def hello():

... print("Hello

World!")

...

>>> hello()

Hello World!

此外

，函数可以取

参数。

>>>

def hello(object):

... print("Hello

" + object +

"!")

...

>>> hello("cat")

Hello cat!

另外，字

符串的拼接

可以使用+。

关

闭Python解释器时

，Linux或Mac

OS X的情况下

输入Ctrl-D（按住

Ctrl，再

按D键）；Windows的情况

下输入Ctrl-Z，然后

按Enter键。

1.4

Python脚本文

件

到目前为

止，我们看到

的都是基于

Python解释器的例

子。Python解释

器能

够以对话模

式执行程序

，非常便于进

行简单的实

验。但是，想进

行一

连串的

处理时，因为

每次都需要

输入程序，所

以不太方便

。这时，可以将

Python程序保存为

文件，然后（集

中地）运行这

个文件。下面

，我们来看一

个Python脚本文件

的例子。

1.4.1　保存

为文件

打开

文本编辑器

，新建一个hungry.py的

文件。hungry.py只包含

下面一

行语

句。

第 1章　Python入门

10

print("I'm hungry!")

接着，打开终

端（Windows中的命令

行窗口），移至

hungry.py所在的位置

。

然后，将hungry.py文件

名作为参数

，运行python命令。这

里假设hungry.

py在 ~/deep-learning-from-scratch/ch01目

录下（在本书

提供的源代

码中，

hungry.py文件位

于ch01目录下）。



$ cd ~/deep-learning-from-scratch/ch01 # 移动目录

$



python hungry.py

I'm hungry!

这样，使用python hungry.py命

令就可以执

行这个Python程序

了。

1.4.2　类

前面我

们了解了int和

str等数据类型

（通过type()函数可

以查看对象

的

类型）。这些

数据类型是

“内置”的数据

类型，是Python中一

开始就有的

数

据类型。现

在，我们来定

义新的类。如

果用户自己

定义类的话

，就可以自己

创建数据类

型。此外，也可

以定义原创

的方法（类的

函数）和属性

。

Python中使用class关键

字来定义类

，类要遵循下

述格式（模板

）。

class 类名：

 def

__init__(self, 参数, …): #

构

造函数

 ...

def 方法

名1(self, 参数, …):

# 方法

1

 ...

def 方法名2(self, 参数

, …):

# 方法2

 ...

这里有

一个特殊的

__init__方法，这是进

行初始化的

方法，也称为

构造

函数（constructor）,只

在生成类的

实例时被调

用一次。此外

，在方法的第

一

个参数中

明确地写入

表示自身（自

身的实例）的

self是Python的一个特

点（学

过其他

编程语言的

人可能会觉

得这种写self的

方式有一点

奇怪）。

下面我

们通过一个

简单的例子

来创建一个

类。这里将下

面的程序保

存为

man.py。

1.5 NumPy

11

class Man:

def __init__(self, name):

self.name = name

print("Initialized!")

 def hello(self):

print("Hello " + self.name

+ "!")

 def

goodbye(self):

 print("Good-bye "

+ self.name + "!")

m = Man("David")

m.hello()

m.goodbye()

从终端

运行man.py。



python man.py

Initialized!

Hello

David!

Good-bye David!

这里我们定

义了一个新

类Man。上面的例

子中，类Man生成

了实例（对象

）m。

类Man的构造函

数（初始化方

法）会接收参

数name，然后用这

个参数初始

化实例变量

self.name。实例变量是

存储在各个

实例中的变

量。Python中可

以像

self.name这样，通过在

self后面添加属

性名来生成

或访问实例

变量。

1.5 NumPy

在深度

学习的实现

中，经常出现

数组和矩阵

的计算。NumPy的数

组类

（numpy.array）中提供

了很多便捷

的方法，在实

现深度学习

时，我们将使

用这

些方法

。本节我们来

简单介绍一

下后面会用

到的NumPy。

1.5.1

导入NumPy

NumPy是

外部库。这里

所说的“外部

”是指不包含

在标准版Python中

。

因此，我们首

先要导入NumPy库

。

第 1章　Python入门 12

>>> import numpy as

np

Python 中

使用 import语句来

导入库。这里

的

import numpy as np，直

译的话

就是“将numpy作为

np导入”的意思

。通过写成这

样的形式，之

后

NumPy相关的方

法均可通过

np来调用。

1.5.2　生成

NumPy数组

要生成

NumPy数组，需要使

用np.array()方法。np.array()接收

Python

列表作为参

数，生成NumPy数组

（numpy.ndarray）。

>>> x

= np.array([1.0, 2.0, 3.0])

>>> print(x)

[ 1.

2. 3.]

>>> type(x)

<class 'numpy.ndarray'>

1.5.3　NumPy

的算术运算

下面是NumPy数组

的算术运算

的例子。

>>> x =

np.array([1.0, 2.0, 3.0])

>>>

y = np.array([2.0, 4.0,

6.0])

>>> x +

y # 对应

元素的加法

array([ 3.,

6., 9.])

>>> x

- y

array([ -1.,

-2., -3.])

>>> x

* y # element-wise

product

array([ 2., 8.,

18.])

>>> x /

y

array([ 0.5, 0.5,

0.5])

这里需要注

意的是，数组

x和数组y的元

素个数是相

同的（两者均

是元素

个数

为3的一维数

组）。当x和y的元

素个数相同

时，可以对各

个元素进行

算

术运算。如

果元素个数

不同，程序就

会报错，所以

元素个数保

持一致非常

重要。

另外，“对

应元素的”的

英文是element-wise，比如

“对应元素的

乘法”就是

element-wise product。

NumPy数

组不仅可以

进行element-wise运算，也

可以和单一

的数值（标量

）

1.5 NumPy  13

组合起来进

行运算。此时

，需要在NumPy数组

的各个元素

和标量之间

进行运算。

这

个功能也被

称为广播（详

见后文）。

>>> x

= np.array([1.0, 2.0, 3.0])

>>> x / 2.0

array([ 0.5, 1. ,

1.5])

1.5.4　NumPy的N维

数组

NumPy不仅可

以生成一维

数组（排成一

列的数组），也

可以生成多

维数组。

比如

，可以生成如

下的二维数

组（矩阵）。

>>> A =

np.array([[1, 2], [3, 4]])

>>> print(A)

[[1 2]

[3 4]]

>>> A.shape

(2, 2)

>>> A.dtype

dtype('int64')

这里

生成了一个

2 × 2的矩阵A。另外

，矩阵A的形状

可以通过shape查

看，

矩阵元素

的数据类型

可以通过dtype查

看。下面，我们

来看一下矩

阵的算术运

算。

>>> B =

np.array([[3, 0],[0, 6]])

>>>

A + B

array([[

4, 2],

 [

3, 10]])

>>> A

* B

array([[ 3,

0],

 [ 0,

24]])

和数组的

算术运算一

样，矩阵的算

术运算也可

以在相同形

状的矩阵间

以

对应元素

的方式进行

。并且，也可以

通过标量（单

一数值）对矩

阵进行算术

运算。

这也是

基于广播的

功能。

>>> print(A)

[[1 2]

[3 4]]

第 1章　Python入

门 14

>>> A * 10

array([[ 10, 20],

[ 30, 40]])

NumPy数组（np.array）可以

生成N维数组

，即可以生成

一维数组、

二

维数组、三维

数组等任意

维数的数组

。数学上将一

维数组称为

向量，

将二维

数组称为矩

阵。另外，可以

将一般化之

后的向量或

矩阵等统

称

为张量（tensor）。本书

基本上将二

维数组称为

“矩阵”，将三维

数

组及三维

以上的数组

称为“张量”或

“多维数组”。

1.5.5　广

播

NumPy中，形状不

同的数组之

间也可以进

行运算。之前

的例子中，在

2×2的矩阵A和标

量10之间进行

了乘法运算

。在这个过程

中，如图1-1所示

，

标量10被扩展

成了2

× 2的形状

，然后再与矩

阵A进行乘法

运算。这个巧

妙

的功能称

为广播（broadcast）。

图1-1

广

播的例子：标

量10被当作2 × 2的

矩阵

1

* = 2

3

4

10 10

10

10

1

* =

2

3 4

10

10 20

30 40

我们通

过下面这个

运算再来看

一个广播的

例子。

>>> A =

np.array([[1, 2], [3, 4]])

>>> B = np.array([10,

20])

>>> A *

B

array([[ 10, 40],

[ 30, 80]])

在这个

运算中，如图

1-2所示，一维数

组B被“巧妙地

”变成了和二

位数

组A相同

的形状，然后

再以对应元

素的方式进

行运算。

综上

，因为NumPy有广播

功能，所以不

同形状的数

组之间也可

以顺利

地进

行运算。

1.5

NumPy  15

图1-2

广

播的例子2

1

* =

2

3 4

10

20

10 20

1

* = 2

3

4

10 20 10

40

30 80

1.5.6

访

问元素

元素

的索引从0开

始。对各个元

素的访问可

按如下方式

进行。

>>> X

= np.array([[51, 55], [14,

19], [0, 4]])

>>>

print(X)

[[51 55]

[14 19]

 [

0 4]]

>>> X[0]

# 第0行

array([51, 55])

>>> X[0][1] # (0,1)的

元素

55

也可以

使用for语句访

问各个元素

。

>>> for

row in X:

...

print(row)

...

[51 55]

[14 19]

[0 4]

除了前面介

绍的索引操

作，NumPy还可以使

用数组访问

各个元素。

>>> X =

X.flatten() # 将

X转换为一维

数组

>>>

print(X)

[51 55 14

19 0 4]

>>>

X[np.array([0, 2, 4])] #

获取索

引为0、2、4的元素

array([51, 14, 0])

运用这个标

记法，可以获

取满足一定

条件的元素

。例如，要从X中

抽出

大于15的

元素，可以写

成如下形式

。

  第

1章　Python入门 16

>>>

X > 15

array([

True, True, False, True,

False, False], dtype=bool)

>>>

X[X>15]

array([51, 55, 19])

对

NumPy数组使用不

等号运算符

等（上例中是

X > 15）,结果会得到

一个

布尔型

的数组。上例

中就是使用

这个布尔型

数组取出了

数组的各个

元素（取

出True对

应的元素）。

Python等

动态类型语

言一般比C和

C++等静态类型

语言（编译型

语言）

运算速

度慢。实际上

，如果是运算

量大的处理

对象，用 C/C++写程

序更好。为此

，当

Python中追求性

能时，人们会

用 C/C++来实现

处

理的内容。Python则

承担“中间人

”的角色，负责

调用那些用

C/

C++写的程序。NumPy中

，主要的处理

也都是通过

C或C++实现的。

因

此，我们可以

在不损失性

能的情况下

，使用 Python便利的

语法。

1.6 Matplotlib

在深度

学习的实验

中，图形的绘

制和数据的

可视化非常

重要。Matplotlib

是用于

绘制图形的

库，使用Matplotlib可以

轻松地绘制

图形和实现

数据的可

视

化。这里，我们

来介绍一下

图形的绘制

方法和图像

的显示方法

。

1.6.1

绘制简单图

形

可以使用

matplotlib的pyplot模块绘制

图形。话不多

说，我们来看

一个

绘制sin函

数曲线的例

子。

import

numpy as np

import

matplotlib.pyplot as plt

#

生成数据

x = np.arange(0, 6,

0.1) # 以0.1为单位，生

成0到6的数据

y =

np.sin(x)

# 绘制图形

1.6

Matplotlib  17

plt.plot(x,

y)

plt.show()

这

里使用NumPy的arange方

法生成了[0, 0.1,

0.2, ���, 5.8, 5.9]的

数据，将其设

为x。对x的各个

元素，应用NumPy的

sin函数np.sin()，将x、

y的数

据传给plt.plot方法

，然后绘制图

形。最后，通过

plt.show()显示图形。

运

行上述代码

后，就会显示

图1-3所示的图

形。

图1-3 sin函数的

图形

1.0

0.5

0 1

2 3 4 5

6

0.0

−0.5

−1.0

1.6.2　pyplot的功能

在刚才的sin函

数的图形中

，我们尝试追

加cos函数的图

形，并尝试使

用

pyplot的添加标

题和x轴标签

名等其他功

能。

import

numpy as np

import

matplotlib.pyplot as plt

#

生成数据

x = np.arange(0, 6,

0.1) # 以0.1为单位，生

成0到6的数据

y1 =

np.sin(x)

  第

1章　Python入门 18

y2

= np.cos(x)

# 绘

制图形

plt.plot(x, y1, label="sin")

plt.plot(x,

y2, linestyle = "--",

label="cos") # 用虚

线绘制

plt.xlabel("x")

# x轴标

签

plt.ylabel("y") #

y轴标签

plt.title('sin & cos')

# 标

题

plt.legend()

plt.show()

结果如图

1-4所示，我们看

到图的标题

、轴的标签名

都被标出来

了。

图1-4 sin函数和

cos函数的图形

1.0 sin

& cos

0.5

0.0

−0.5

−1.0

0 1

3

x 2 4

5 6

cos

sin

1.6.3　显示图像

pyplot 中

还提供了用

于显示图像

的方法

imshow()。另外

，可以使用

matplotlib.image模

块的imread()方法读

入图像。下面

我们来看一

个例子。

import matplotlib.pyplot

as plt

from matplotlib.image

import imread

y

1.7

小结

19

img =

imread('lena.png') # 读入图像（设

定合适的路

径！）

plt.imshow(img)

plt.show()

运行上述

代码后，会显

示图1-5所示的

图像。

图1-5　显示

图像

0

50

100

150

200

250

0 50

100 150 200 250

这里，我

们假定图像

lena.png在当前目录

下。读者根据

自己的环境

，可

能需要变

更文件名或

文件路径。另

外，本书提供

的源代码中

，在dataset目

录下有

样本图像lena.png。比

如，在通过Python解

释器从ch01目录

运行上

述代

码的情况下

，将图像的路

径'lena.png'改为'../dataset/lena.png'，即

可

正确运行。

1.7 小

结

本章重点

介绍了实现

深度学习（神

经网络）所需

的编程知识

，以为学习

深

度学习做好

准备。从下一

章开始，我们

将通过使用

Python实际运行代

码，

逐步了解

深度学习。

第

1章　Python入门 20

本章

只介绍了关

于Python的最低限

度的知识，想

进一步了解

Python的

读者，可以

参考下面这

些图书。首先

推荐《Python语言及

其应用》[1]

一书

。

这是一本详

细介绍从Python编

程的基础到

应用的实践

性的入门书

。关于

NumPy，《利用Python进

行数据分析

》[2]

一书中进行

了简单易懂

的总结。此

外

，“Scipy Lecture Notes”[3]

这个网站上

也有以科学

计算为主题

的NumPy

和Matplotlib的详细

介绍，有兴趣

的读者可以

参考。

下面，我

们来总结一

下本章所学

的内容，如下

所示。

本章所

学的内容

• Python是

一种简单易

记的编程语

言。

• Python是开源的

，可以自由使

用。

• 本书中将

使用Python 3.x实现深

度学习。

•

本书

中将使用NumPy和

Matplotlib这两种外部

库。

• Python有“解释器

”和“脚本文件

”两种运行模

式。

•

Python能够将一

系列处理集

成为函数或

类等模块。

• NumPy中

有很多用于

操作多维数

组的便捷方

法。

第2章

感知

机

本章将介

绍感知机A

（perceptron）这

一算法。感知

机是由美国

学者Frank

Rosenblatt在1957年提

出来的。为何

我们现在还

要学习这一

很久以前就

有

的算法呢

？因为感知机

也是作为神

经网络（深度

学习）的起源

的算法。因此

，

学习感知机

的构造也就

是学习通向

神经网络和

深度学习的

一种重要思

想。

本章我们

将简单介绍

一下感知机

，并用感知机

解决一些简

单的问题。希

望读者通过

这个过程能

熟悉感知机

。

2.1 感知机是什

么

感知机接

收多个输入

信号，输出一

个信号。这里

所说的“信号

”可以想

象成

电流或河流

那样具备“流

动性”的东西

。像电流流过

导线，向前方

输送

电子一

样，感知机的

信号也会形

成流，向前方

输送信息。但

是，和实际的

电

流不同的

是，感知机的

信号只有“流

/不流”（1/0）两种取

值。在本书中

，0

对应“不传递

信号”，1对应“传

递信号”。

图2-1是

一个接收两

个输入信号

的感知机的

例子。x1、x2是输入

信号，

y是输出

信号，w1、w2是权重

（w是weight的首字母

）。图中的○称为

“神

经元”或者

“节点”。输入信

号被送往神

经元时，会被

分别乘以固

定的权重

A 严

格地讲，本章

中所说的感

知机应该称

为“人工神经

元”或“朴素感

知机”，但是因

为很多基本

的处

理都是

共通的，所以

这里就简单

地称为“感知

机”。

  第

2章　感知

机 22

（w1x1、w2x2）。神经元会

计算传送过

来的信号的

总和，只有当

这个总和超

过

了某个界

限值时，才会

输出1。这也称

为“神经元被

激活”。这里将

这个界

限值

称为阈值，用

符号θ表示。

图

2-1　有两个输入

的感知机

x1

x2

w1

w2

y

感

知机的运行

原理只有这

些！把上述内

容用数学式

来表示，就是

式（2.1）。

（2.1）

感知机的

多个输入信

号都有各自

固有的权重

，这些权重发

挥着控制各

个

信号的重

要性的作用

。也就是说，权

重越大，对应

该权重的信

号的重要性

就

越高。

权重

相当于电流

里所说的电

阻。电阻是决

定电流流动

难度的参数

，

电阻越低，通

过的电流就

越大。而感知

机的权重则

是值越大，通

过

的信号就

越大。不管是

电阻还是权

重，在控制信

号流动难度

（或者流

动容

易度）这一点

上的作用都

是一样的。

2.2

简

单逻辑电路

23

2.2 简单逻辑电

路

2.2.1

与门

现在

让我们考虑

用感知机来

解决简单的

问题。这里首

先以逻辑电

路为题

材来

思考一下与

门（AND gate）。与门是有

两个输入和

一个输出的

门电路。图2-2

这

种输入信号

和输出信号

的对应表称

为“真值表”。如

图2-2所示，与门

仅在

两个输

入均为1时输

出1，其他时候

则输出0。

x1 x2

y

1

1

1

1 1

0

0

0 0

0

0

0

图2-2　与

门的真值表

下面考虑用

感知机来表

示这个与门

。需要做的就

是确定能满

足图2-2的

真值

表的w1、w2、θ的值。那

么，设定什么

样的值才能

制作出满足

图2-2的

条件的

感知机呢？

实

际上，满足图

2-2的条件的参

数的选择方

法有无数多

个。比如，当

(w1, w2,

θ) = (0.5, 0.5,

0.7) 时

，可以满足图

2-2 的条件。此外

，当 (w1,

w2, θ)

为(0.5, 0.5,

0.8)或者(1.0, 1.0, 1.0)时

，同样也满足

与门的条件

。设定这样的

参数后，仅当

x1和x2同时为1时

，信号的加权

总和才会超

过给定的阈

值θ。

2.2.2

与非门和

或门

接着，我

们再来考虑

一下与非门

（NAND gate）。NAND是Not AND的

第 2章　感

知机

24

意思，与

非门就是颠

倒了与门的

输出。用真值

表表示的话

，如图2-3所示，

仅

当x1和x2同时为

1时输出0，其他

时候则输出

1。那么与非门

的参数又可

以是什么样

的组合呢？

图

2-3

与非门的真

值表

1

1 1

1 1

1

1

0 0

0

0

0

x1 x2 y

要表示

与非门，可以

用(w1, w2, θ) =

(−0.5, −0.5, −0.7)这样的组

合（其

他的组

合也是无限

存在的）。实际

上，只要把实

现与门的参

数值的符号

取反，

就可以

实现与非门

。

接下来看一

下图2-4所示的

或门。或门是

“只要有一个

输入信号是

1，输

出就为1”的

逻辑电路。那

么我们来思

考一下，应该

为这个或门

设定什么样

的参数呢？

图

2-4

或门的真值

表

x1 x2 y

1 1

1 1

1 1 1

0

0

0

0 0

2.3 感知机的

实现  25

这里决

定感知机参

数的并不是

计算机，而是

我们人。我们

看着真值

表

这种“训练数

据”，人工考虑

（想到）了参数

的值。而机器

学习的课

题

就是将这个

决定参数值

的工作交由

计算机自动

进行。学习是

确定

合适的

参数的过程

，而人要做的

是思考感知

机的构造（模

型），并把

训练

数据交给计

算机。

如上所

示，我们已经

知道使用感

知机可以表

示与门、与非

门、或门的逻

辑电路。这里

重要的一点

是：与门、与非

门、或门的感

知机构造是

一样的。

实际

上，3个门电路

只有参数的

值（权重和阈

值）不同。也就

是说，相同构

造

的感知机

，只需通过适

当地调整参

数的值，就可

以像“变色龙

演员”表演不

同的角色一

样，变身为与

门、与非门、或

门。

2.3 感知机的

实现

2.3.1　简单的

实现

现在，我

们用Python来实现

刚才的逻辑

电路。这里，先

定义一个接

收

参数x1和x2的

AND函数。

def AND(x1,

x2):

 w1, w2,

theta = 0.5, 0.5,

0.7

 tmp =

x1*w1 + x2*w2

if tmp <= theta:

return 0

 elif

tmp > theta:

return 1

在函数

内初始化参

数w1、w2、theta，当输入的

加权总和超

过阈值时返

回1，

否则返回

0。我们来确认

一下输出结

果是否如图

2-2所示。

AND(0, 0) # 输出0

AND(1, 0) # 输

出0

AND(0, 1) # 输出0

AND(1, 1) # 输出

1

果然和我们

预想的输出

一样！这样我

们就实现了

与门。按照同

样的步骤，

  第

2章

感知机 26

也

可以实现与

非门和或门

，不过让我们

来对它们的

实现稍作修

改。

2.3.2

导入权重

和偏置

刚才

的与门的实

现比较直接

、容易理解，但

是考虑到以

后的事情，我

们

将其修改

为另外一种

实现形式。在

此之前，首先

把式（2.1）的θ换成

−b，于

是就可以

用式（2.2）来表示

感知机的行

为。

（2.2）

式（2.1）和式（2.2）虽

然有一个符

号不同，但表

达的内容是

完全相同的

。

此处，b称为偏

置，w1和w2称为权

重。如式（2.2）所示

，感知机会计

算输入

信号

和权重的乘

积，然后加上

偏置，如果这

个值大于0则

输出1，否则输

出0。

下面，我们

使用NumPy，按式（2.2）的

方式实现感

知机。在这个

过程中，我

们

用Python的解释器

逐一确认结

果。

>>> import

numpy as np

>>>

x = np.array([0, 1])

# 输入

>>> w

= np.array([0.5, 0.5]) #

权重

>>> b = -0.7

# 偏置

>>> w*x

array([ 0. , 0.5])

>>> np.sum(w*x)

0.5

>>>

np.sum(w*x) + b

-0.19999999999999996

# 大约为

-0.2（由浮点小数

造成的运算

误差）

如上例

所示，在NumPy数组

的乘法运算

中，当两个数

组的元素个

数相同时，

各

个元素分别

相乘，因此w*x的

结果就是它

们的各个元

素分别相乘

（[0,

1] * 

[0.5,

0.5] => [0, 0.5]）。之后，np.sum(w*x)再计算

相乘后的各

个元素的总

和。

最后再把

偏置加到这

个加权总和

上，就完成了

式（2.2）的计算。

2.3.3　使

用权重和偏

置的实现

使

用权重和偏

置，可以像下

面这样实现

与门。

2.3 感知机

的实现  27

def AND(x1, x2):

x = np.array([x1, x2])

w = np.array([0.5, 0.5])

b = -0.7

tmp = np.sum(w*x) +

b

 if tmp

<= 0:

 return

0

 else:

return 1

这里

把−θ命名为偏

置b，但是请注

意，偏置和权

重w1、w2的作用是

不

一样的。具

体地说，w1和w2是

控制输入信

号的重要性

的参数，而偏

置是调

整神

经元被激活

的容易程度

（输出信号为

1的程度）的参

数。比如，若b为

−0.1，则只要输入

信号的加权

总和超过0.1，神

经元就会被

激活。但是如

果b

为−20.0，则输入

信号的加权

总和必须超

过20.0，神经元才

会被激活。像

这样，

偏置的

值决定了神

经元被激活

的容易程度

。另外，这里我

们将w1和w2称为

权重，

将b称为

偏置，但是根

据上下文，有

时也会将b、w1、w2这

些参数统称

为权重。

偏置

这个术语，有

“穿木屐”A 的效

果，即在没有

任何输入时

（输入为

0时），给

输出穿上多

高的木屐（加

上多大的值

）的意思。实际

上，在

式(2.2)

的b + w1x1 +

w2x2的

计算中，当输

入x1和x2为 0时，只

输出

偏置的

值。

A

接着，我们

继续实现与

非门和或门

。

def NAND(x1, x2):

x = np.array([x1, x2])

w = np.array([-0.5, -0.5])

# 仅权重和偏

置与AND不同！

 b

= 0.7

 tmp

= np.sum(w*x) + b

if tmp <= 0:

return 0

 else:

return 1

def OR(x1,

x2):

A 因

为木屐的底

比较厚，穿上

它后，整个人

也会显得更

高。——译者注

第

2章　感知机 28

x = np.array([x1, x2])

w = np.array([0.5, 0.5])

# 仅

权重和偏置

与AND不同！

 b

= -0.2

 tmp

= np.sum(w*x) + b

if tmp <= 0:

return 0

 else:

return 1

我们

在2.2节介绍过

，与门、与非门

、或门是具有

相同构造的

感知机，

区别

只在于权重

参数的值。因

此，在与非门

和或门的实

现中，仅设置

权重和

偏置

的值这一点

和与门的实

现不同。

2.4 感知

机的局限性

到这里我们

已经知道，使

用感知机可

以实现与门

、与非门、或门

三种逻

辑电

路。现在我们

来考虑一下

异或门（XOR

gate）。

2.4.1　异或

门

异或门也

被称为逻辑

异或电路。如

图2-5所示，仅当

x1或x2中的一方

为

1时，才会输

出1（“异或”是拒

绝其他的意

思）。那么，要用

感知机实现

这个

异或门

的话，应该设

定什么样的

权重参数呢

？

x1 x2

y

1 1

11

11

0

0

0

0

0 0

图2-5

异或门的

真值表

2.4 感知

机的局限性

29

实际上，用前

面介绍的感

知机是无法

实现这个异

或门的。为什

么用感知

机

可以实现与

门、或门，却无

法实现异或

门呢？下面我

们尝试通过

画图来思

考

其中的原因

。

首先，我们试

着将或门的

动作形象化

。或门的情况

下，当权重参

数

(b, w1, w2) =

(−0.5, 1.0, 1.0)时，可满足

图2-4的真值表

条件。此时，感

知机

可用下

面的式（2.3）表示

。

（2.3）

式（2.3）表示的感

知机会生成

由直线−0.5 + x1

+ x2 = 0分割

开的两个空

间。其中一个

空间输出1，另

一个空间输

出0，如图2-6所示

。

x2

x1

0 1

1

图2-6　感知机的

可视化：灰色

区域是感知

机输出0的区

域，这个区域

与或门的性

质一致

或门

在(x1,

x2) = (0, 0)时输出0，在

(x1,

x2)为(0, 1)、(1, 0)、(1, 1)时输

出1。图

2-6中，○表示0，△表示

1。如果想制作

或门，需要用

直线将图2-6

  第

2章

感知机 30

中

的○和△分开。实

际上，刚才的

那条直线就

将这4个点正

确地分开了

。

那么，换成异

或门的话会

如何呢？能否

像或门那样

，用一条直线

作出分

割图

2-7中的○和△的空

间呢？

x2

x1

0

1

1

图2-7　○和△表

示异或门的

输出。可否通

过一条直线

作出分割○和

△的空间呢？

想

要用一条直

线将图2-7中的

○和△分开，无论

如何都做不

到。事实上，

用

一条直线是

无法将○和△分

开的。

2.4.2　线性和

非线性

图2-7中

的○和△无法用

一条直线分

开，但是如果

将“直线”这个

限制条

件去

掉，就可以实

现了。比如，我

们可以像图

2-8那样，作出分

开○和△的空间

。

感知机的局

限性就在于

它只能表示

由一条直线

分割的空间

。图2-8这样弯

曲

的曲线无法

用感知机表

示。另外，由图

2-8这样的曲线

分割而成的

空间称为

非

线性空间，由

直线分割而

成的空间称

为线性空间

。线性、非线性

这两个术

语

在机器学习

领域很常见

，可以将其想

象成图2-6和图

2-8所示的直线

和曲线。

2.5 多层

感知机

31

x2

x1

0

1

1

图2-8　使

用曲线可以

分开○和△

2.5 多层

感知机

感知

机不能表示

异或门让人

深感遗憾，但

也无需悲观

。实际上，感知

机

的绝妙之

处在于它可

以“叠加层”（通

过叠加层来

表示异或门

是本节的要

点）。

这里，我们

暂且不考虑

叠加层具体

是指什么，先

从其他视角

来思考一下

异或

门的问

题。

2.5.1　已有门电

路的组合

异

或门的制作

方法有很多

，其中之一就

是组合我们

前面做好的

与门、与

非门

、或门进行配

置。这里，与门

、与非门、或门

用图2-9中的符

号表示。另外

，

图2-9中与非门

前端的○表示

反转输出的

意思。

那么，请

思考一下，要

实现异或门

的话，需要如

何配置与门

、与非门和

或

门呢？这里给

大家一个提

示，用与门、与

非门、或门代

替图2-10中的各

个

“？”，就可以实

现异或门。

第

2章　感知机 32

AND

NAND OR

图

2-9　与门、与非门

、或门的符号

图2-10

将与门、与

非门、或门代

入到“？”中，就可

以实现异或

门！

x2

x1

y

2.4节讲到的

感知机的局

限性，严格地

讲，应该是“单

层感知机无

法

表示异或

门”或者“单层

感知机无法

分离非线性

空间”。接下来

，我

们将看到

通过组合感

知机（叠加层

）就可以实现

异或门。

异或

门可以通过

图2-11所示的配

置来实现。这

里，x1和x2表示输

入信号，

y表示

输出信号。x1和

x2是与非门和

或门的输入

，而与非门和

或门的输出

则

是与门的

输入。

图2-11　通过

组合与门、与

非门、或门实

现异或门

x1

s1

x2

s2

y

现

在，我们来确

认一下图2-11的

配置是否真

正实现了异

或门。这里，把

s1作为与非门

的输出，把s2作

为或门的输

出，填入真值

表中。结果如

图2-12

所示，观察

x1、x2、y，可以发现确

实符合异或

门的输出。

2.5

多

层感知机  33

x1

0

1

0

1

0

0

1

1

1

1

1

0

0

1

1

1

0

1

1

0

x2 s1 s2 y

图

2-12　异或门的真

值表

2.5.2　异或门

的实现

下面

我们试着用

Python来实现图2-11所

示的异或门

。使用之前定

义的

AND函数、NAND函

数、OR函数，可以

像下面这样

（轻松地）实现

。

def XOR(x1,

x2):

 s1 =

NAND(x1, x2)

 s2

= OR(x1, x2)

y = AND(s1, s2)

return y

这个XOR函数会

输出预期的

结果。

XOR(0,

0) # 输出0

XOR(1,

0) # 输

出1

XOR(0,

1) # 输出1

XOR(1,

1) # 输出

0

这样，异或门

的实现就完

成了。下面我

们试着用感

知机的表示

方法（明

确地

显示神经元

）来表示这个

异或门，结果

如图2-13所示。

如

图2-13所示，异或

门是一种多

层结构的神

经网络。这里

，将最左边的

一列称为第

0层，中间的一

列称为第1层

，最右边的一

列称为第2层

。

图2-13所示的感

知机与前面

介绍的与门

、或门的感知

机（图2-1）形状不

同。实际上，与

门、或门是单

层感知机，而

异或门是2层

感知机。叠加

了多

层的感

知机也称为

多层感知机

（multi-layered

perceptron）。

  第

2章　感知机

34

图2-13　用感知机

表示异或门

x1

s1

x2 s2

y

第2层 第0层 第

1层

图

2-13中的感

知机总共由

3层构成，但是

因为拥有权

重的层实质

上只有 2层（第

0层和第 1层之

间，第 1层和第

2层之间），所以

称

为“2层感知

机”。不过，有的

文献认为图

2-13的感知机是

由 3层

构成的

，因而将其称

为“3层感知机

”。

在图2-13所示的

2层感知机中

，先在第0层和

第1层的神经

元之间进行

信号的传送

和接收，然后

在第1层和第

2层之间进行

信号的传送

和接收，具

体

如下所示。

1.第

0层的两个神

经元接收输

入信号，并将

信号发送至

第1层的神经

元。

2.第1层的神

经元将信号

发送至第2层

的神经元，第

2层的神经元

输出y。

这种2层

感知机的运

行过程可以

比作流水线

的组装作业

。第1段（第1层）

的

工人对传送

过来的零件

进行加工，完

成后再传送

给第2段（第2层

）的工人。

第2层

的工人对第

1层的工人传

过来的零件

进行加工，完

成这个零件

后出货

（输出

）。

像这样，在异

或门的感知

机中，工人之

间不断进行

零件的传送

。通过这

样的

结构（2层结构

），感知机得以

实现异或门

。这可以解释

为“单层感知

机

无法表示

的东西，通过

增加一层就

可以解决”。也

就是说，通过

叠加层（加深

层），感知机能

进行更加灵

活的表示。

2.6

从

与非门到计

算机 35

2.6 从与非

门到计算机

多层感知机

可以实现比

之前见到的

电路更复杂

的电路。比如

，进行加法

运

算的加法器

也可以用感

知机实现。此

外，将二进制

转换为十进

制的编码器

、

满足某些条

件就输出1的

电路（用于等

价检验的电

路）等也可以

用感知机表

示。

实际上，使

用感知机甚

至可以表示

计算机！

计算

机是处理信

息的机器。向

计算机中输

入一些信息

后，它会按照

某种

既定的

方法进行处

理，然后输出

结果。所谓“按

照某种既定

的方法进行

处理”

是指，计

算机和感知

机一样，也有

输入和输出

，会按照某个

既定的规则

进行

计算。

人

们一般会认

为计算机内

部进行的处

理非常复杂

，而令人惊讶

的是，实

际上

只需要通过

与非门的组

合，就能再现

计算机进行

的处理。这一

令人吃惊

的

事实说明了

什么呢？说明

使用感知机

也可以表示

计算机。前面

也介绍了，

与

非门可以使

用感知机实

现。也就是说

，如果通过组

合与非门可

以实现计算

机的话，那么

通过组合感

知机也可以

表示计算机

（感知机的组

合可以通过

叠

加了多层

的单层感知

机来表示）。

说

到仅通过与

非门的组合

就能实现计

算机，大家也

许一下子很

难相信。

建议

有兴趣的读

者看一下《计

算机系统要

素：从零开始

构建现代计

算机》。这本书

以深入理解

计算机为主

题，论述了通

过 NAND构建可

运

行俄罗斯方

块的计算机

的过程。此书

能让读者真

实体会到，通

过

简单的 NAND元

件就可以实

现计算机这

样复杂的系

统。

综上，多层

感知机能够

进行复杂的

表示，甚至可

以构建计算

机。那么，

什么

构造的感知

机才能表示

计算机呢？层

级多深才可

以构建计算

机呢？

理论上

可以说2层感

知机就能构

建计算机。这

是因为，已有

研究证明，

2层

感知机（严格

地说是激活

函数使用了

非线性的sigmoid函

数的感知机

，具

体请参照

下一章）可以

表示任意函

数。但是，使用

2层感知机的

构造，通过

第

2章　感知机 36

设

定合适的权

重来构建计

算机是一件

非常累人的

事情。实际上

，在用与非门

等低层的元

件构建计算

机的情况下

，分阶段地制

作所需的零

件（模块）会比

较自然，即先

实现与门和

或门，然后实

现半加器和

全加器，接着

实现算数逻

辑单元（ALU），然后

实现CPU。因此，通

过感知机表

示计算机时

，使用叠

加了

多层的构造

来实现是比

较自然的流

程。

本书中不

会实际来实

现计算机，但

是希望读者

能够记住，感

知机通过叠

加层能够进

行非线性的

表示，理论上

还可以表示

计算机进行

的处理。

2.7 小结

本章我们学

习了感知机

。感知机是一

种非常简单

的算法，大家

应该很快

就

能理解它的

构造。感知机

是下一章要

学习的神经

网络的基础

，因此本章的

内容非常重

要。

本章所学

的内容

• 感知

机是具有输

入和输出的

算法。给定一

个输入后，将

输出一个既

定的值。

• 感知

机将权重和

偏置设定为

参数。

• 使用感

知机可以表

示与门和或

门等逻辑电

路。

• 异或门无

法通过单层

感知机来表

示。

• 使用2层感

知机可以表

示异或门。

• 单

层感知机只

能表示线性

空间，而多层

感知机可以

表示非线性

空间。

• 多层感

知机（在理论

上）可以表示

计算机。

第3章

神经网络

上

一章我们学

习了感知机

。关于感知机

，既有好消息

，也有坏消息

。好

消息是，即

便对于复杂

的函数，感知

机也隐含着

能够表示它

的可能性。上

一

章已经介

绍过，即便是

计算机进行

的复杂处理

，感知机（理论

上）也可以将

其表示出来

。坏消息是，设

定权重的工

作，即确定合

适的、能符合

预期的输

入

与输出的权

重，现在还是

由人工进行

的。上一章中

，我们结合与

门、或门

的真

值表人工决

定了合适的

权重。

神经网

络的出现就

是为了解决

刚才的坏消

息。具体地讲

，神经网络的

一

个重要性

质是它可以

自动地从数

据中学习到

合适的权重

参数。本章中

，我们

会先介

绍神经网络

的概要，然后

重点关注神

经网络进行

识别时的处

理。在下

一章

中，我们将了

解如何从数

据中学习权

重参数。

3.1 从感

知机到神经

网络

神经网

络和上一章

介绍的感知

机有很多共

同点。这里，我

们主要以两

者

的差异为

中心，来介绍

神经网络的

结构。

3.1.1　神经网

络的例子

用

图来表示神

经网络的话

，如图3-1所示。我

们把最左边

的一列称为

输入层，最右

边的一列称

为输出层，中

间的一列称

为中间层。中

间层有时

  第

3章

神经网络

38

也称为隐藏

层。“隐藏”一词

的意思是，隐

藏层的神经

元（和输入层

、输出

层不同

）肉眼看不见

。另外，本书中

把输入层到

输出层依次

称为第0层、第

1层、第2层（层号

之所以从0开

始，是为了方

便后面基于

Python进行实现）。

图

3-1中，第0层对应

输入层，第1层

对应中间层

，第2层对应输

出层。

输出层

输入层

中间

层

图3-1　神经网

络的例子

图

3-1中的网络一

共由 3层神经

元构成，但实

质上只有 2层

神经

元有权

重，因此将其

称为“2层网络

”。请注意，有的

书也会根据

构成网络的

层数，把图

3-1的

网络称为“3层

网络”。本书将

根据

实质上

拥有权重的

层数（输入层

、隐藏层、输出

层的总数减

去 1

后的数量

）来表示网络

的名称。

只看

图3-1的话，神经

网络的形状

类似上一章

的感知机。实

际上，就神

经

元的连接方

式而言，与上

一章的感知

机并没有任

何差异。那么

，神经网络

中

信号是如何

传递的呢？

3.1.2

复

习感知机

在

观察神经网

络中信号的

传递方法之

前，我们先复

习一下感知

机。现在

3.1 从感

知机到神经

网络

39

来思考

一下图3-2中的

网络结构。

x1

w1

x2

w2

y

图

3-2

复习感知机

图3-2中的感知

机接收x1和x2两

个输入信号

，输出y。如果用

数学式来

表

示图3-2中的感

知机，则如式

（3.1）所示。

（3.1）

b是被称

为偏置的参

数，用于控制

神经元被激

活的容易程

度；而w1和w2

是表

示各个信号

的权重的参

数，用于控制

各个信号的

重要性。

顺便

提一下，在图

3-2的网络中，偏

置b并没有被

画出来。如果

要明确

地表

示出b，可以像

图3-3那样做。图

3-3中添加了权

重为b的输入

信号1。这

个感

知机将x1、x2、1三个

信号作为神

经元的输入

，将其和各自

的权重相乘

后，

传送至下

一个神经元

。在下一个神

经元中，计算

这些加权信

号的总和。如

果

这个总和

超过0，则输出

1，否则输出0。另

外，由于偏置

的输入信号

一直是1，

所以

为了区别于

其他神经元

，我们在图中

把这个神经

元整个涂成

灰色。

现在将

式（3.1）改写成更

加简洁的形

式。为了简化

式（3.1），我们用一

个

函数来表

示这种分情

况的动作（超

过0则输出1，否

则输出0）。引入

新函数

h(x)，将式

（3.1）改写成下面

的式（3.2）和式（3.3）。

y =

h(b + w1x1 +

w2x2) （3.2）

第

3章　神经网络

40

x1 w1

x2

w2

y

1

b

图3-3　明确表示

出偏置

（3.3）

式（3.2）中

，输入信号的

总和会被函

数h(x)转换，转换

后的值就是

输出y。

然后，式

（3.3）所表示的函

数h(x)，在输入超

过0时返回1，否

则返回0。因此

，

式（3.1）和式（3.2）、式（3.3）做

的是相同的

事情。

3.1.3

激活函

数登场

刚才

登场的h（x）函数

会将输入信

号的总和转

换为输出信

号，这种函数

一般称为激

活函数（activation function）。如“激

活”一词所示

，激活函数的

作用在于决

定如何来激

活输入信号

的总和。

现在

来进一步改

写式（3.2）。式（3.2）分两

个阶段进行

处理，先计算

输入

信号的

加权总和，然

后用激活函

数转换这一

总和。因此，如

果将式（3.2）写

得

详细一点，则

可以分成下

面两个式子

。

a =

b + w1x1 +

w2x2 （3.4）

y =

h(a) （3.5）

首先，式（3.4）计算

加权输入信

号和偏置的

总和，记为a。然

后，式（3.5）

用h()函数

将a转换为输

出y。

3.1 从感知机

到神经网络

41

之前的神经

元都是用一

个○表示的，如

果要在图中

明确表示出

式（3.4）

和式（3.5），则可

以像图3-4这样

做。

x1

w1

x2

w2

y

1

b

a

图3-4　明确显

示激活函数

的计算过程

h()

如图3-4所示，表

示神经元的

○中明确显示

了激活函数

的计算过程

，即

信号的加

权总和为节

点a，然后节点

a被激活函数

h()转换成节点

y。本书中，“神

经

元”和“节点”两

个术语的含

义相同。这里

，我们称a和y为

“节点”，其实

它

和之前所说

的“神经元”含

义相同。

通常

如图3-5的左图

所示，神经元

用一个○表示

。本书中，在可

以明确

神经

网络的动作

的情况下，将

在图中明确

显示激活函

数的计算过

程，如图3-5

的右

图所示。

图3-5 左

图是一般的

神经元的图

，右图是在神

经元内部明

确显示激活

函数的计算

过程的图（a

表

示输入信号

的总和，h()表示

激活函数，y表

示输出）

y

h()

a

第 3章

神经网络 42

下

面，我们将仔

细介绍激活

函数。激活函

数是连接感

知机和神经

网络的

桥梁

。A

本书在使用

“感知机”一词

时，没有严格

统一它所指

的算法。一

般

而言，“朴素感

知机”是指单

层网络，指的

是激活函数

使用了阶

跃

函数A 的模型

。“多层感知机

”是指神经网

络，即使用 sigmoid

函

数（后述）等平

滑的激活函

数的多层网

络。

3.2 激活函数

式（3.3）表示的激

活函数以阈

值为界，一旦

输入超过阈

值，就切换输

出。

这样的函

数称为“阶跃

函数”。因此，可

以说感知机

中使用了阶

跃函数作为

激活函数。也

就是说，在激

活函数的众

多候选函数

中，感知机使

用了阶跃函

数。

那么，如果

感知机使用

其他函数作

为激活函数

的话会怎么

样呢？实际上

，如

果将激活

函数从阶跃

函数换成其

他函数，就可

以进入神经

网络的世界

了。下

面我们

就来介绍一

下神经网络

使用的激活

函数。

3.2.1　sigmoid函数

神

经网络中经

常使用的一

个激活函数

就是式（3.6）表示

的sigmoid函数

（sigmoid function）。

（3.6）

式（3.6）中

的exp(−x)表示e

−x

的意

思。e是纳皮尔

常数2.7182 ...。式（3.6）

表示

的sigmoid函数看上

去有些复杂

，但它也仅仅

是个函数而

已。而函数就

是

给定某个

输入后，会返

回某个输出

的转换器。比

如，向sigmoid函数输

入1.0或2.0

后，就会

有某个值被

输出，类似h(1.0) =

0.731 ...、h(2.0) = 0.880

...这

样。

神经网络

中用sigmoid函数作

为激活函数

，进行信号的

转换，转换后

的

A 阶跃函数

是指一旦输

入超过阈值

，就切换输出

的函数。

3.2 激活

函数  43

信号被

传送给下一

个神经元。实

际上，上一章

介绍的感知

机和接下来

要介绍

的神

经网络的主

要区别就在

于这个激活

函数。其他方

面，比如神经

元的多层

连

接的构造、信

号的传递方

法等，基本上

和感知机是

一样的。下面

，让我们

通过

和阶跃函数

的比较来详

细学习作为

激活函数的

sigmoid函数。

3.2.2　阶跃函

数的实现

这

里我们试着

用Python画出阶跃

函数的图（从

视觉上确认

函数的形状

对

理解函数

而言很重要

）。阶跃函数如

式（3.3）所示，当输

入超过0时，输

出1，

否则输出

0。可以像下面

这样简单地

实现阶跃函

数。

def step_function(x):

if x > 0:

return 1

 else:

return 0

这个实现

简单、易于理

解，但是参数

x只能接受实

数（浮点数）。也

就是

说，允许

形如step_function(3.0)的调用

，但不允许参

数取NumPy数组，例

如step_function(np.array([1.0,

2.0]))。为了便于

后面的操作

，我们把它修

改为支持NumPy数

组的实现。为

此，可以考虑

下述实现。

def step_function(x):

y = x >

0

 return y.astype(np.int)

上

述函数的内

容只有两行

。由于使用了

NumPy中的“技巧”，可

能会有

点难

理解。下面我

们通过Python解释

器的例子来

看一下这里

用了什么技

巧。

下面这个

例子中准备

了NumPy数组x，并对

这个NumPy数组进

行了不等号

运算。

>>>

import numpy as np

>>> x = np.array([-1.0,

1.0, 2.0])

>>> x

array([-1., 1., 2.])

>>>

y = x >

0

  第

3章　神

经网络 44

>>>

y

array([False, True, True],

dtype=bool)

对NumPy数

组进行不等

号运算后，数

组的各个元

素都会进行

不等号运算

，

生成一个布

尔型数组。这

里，数组x中大

于0的元素被

转换为True，小于

等

于0的元素

被转换为False，从

而生成一个

新的数组y。

数

组y是一个布

尔型数组，但

是我们想要

的阶跃函数

是会输出int型

的0

或1的函数

。因此，需要把

数组y的元素

类型从布尔

型转换为int型

。

>>> y

= y.astype(np.int)

>>> y

array([0, 1, 1])

如上所示，可

以用astype()方法转

换NumPy数组的类

型。astype()方

法通过

参数指定期

望的类型，这

个例子中是

np.int型。Python中将布尔

型

转换为int型

后，True会转换为

1，False会转换为0。以

上就是阶跃

函数的

实现

中所用到的

NumPy的“技巧”。

3.2.3

阶跃

函数的图形

下面我们就

用图来表示

上面定义的

阶跃函数，为

此需要使用

matplotlib库。

import numpy as

np

import matplotlib.pylab as

plt

def step_function(x):

return np.array(x > 0,

dtype=np.int)

x = np.arange(-5.0,

5.0, 0.1)

y =

step_function(x)

plt.plot(x, y)

plt.ylim(-0.1,

1.1) # 指定y轴的

范围

plt.show()

np.arange(-5.0, 5.0, 0.1)在−5.0到5.0的

范围内，以0.1为

单位，生成

NumPy数

组（[-5.0,

-4.9, ���, 4.9]）。step_function()以该NumPy数组

为

参数，对数

组的各个元

素执行阶跃

函数运算，并

以数组形式

返回运算结

果。

对数组x、y进

行绘图，结果

如图3-6所示。

3.2 激

活函数

45

图3-6　阶

跃函数的图

形

1.0

0.8

0.6

0.4

0.2

0.0

−6 −4 −2

0 2 4 6

如图3-6所示

，阶跃函数以

0为界，输出从

0切换为1（或者

从1切换为0）。

它

的值呈阶梯

式变化，所以

称为阶跃函

数。

3.2.4　sigmoid函数的实

现

下面，我们

来实现sigmoid函数

。用Python可以像下

面这样写出

式（3.6）

表示的sigmoid函

数。

def sigmoid(x):

return 1 / (1

+ np.exp(-x))

这里，np.exp(-x)对应

exp(−x)。这个实现没

有什么特别

难的地方，但

是要注意参

数x为NumPy数组时

，结果也能被

正确计算。实

际上，如果在

这个sigmoid函数中

输入一个NumPy数

组，则结果如

下所示。

>>>

x = np.array([-1.0, 1.0,

2.0])

>>> sigmoid(x)

array([

0.26894142, 0.73105858, 0.88079708])

第 3章

神经网络 46

之

所以sigmoid函数的

实现能支持

NumPy数组，秘密就

在于NumPy的

广播

功能（1.5.5节）。根据

NumPy 的广播功能

，如果在标量

和NumPy数组

之间

进行运算，则

标量会和NumPy数

组的各个元

素进行运算

。这里来看一

个具体的例

子。

>>>

t = np.array([1.0, 2.0,

3.0])

>>> 1.0 +

t

array([ 2., 3.,

4.])

>>> 1.0 /

t

array([ 1. ,

0.5 , 0.33333333])

在这个例

子中，标量（例

子中是1.0）和NumPy数

组之间进行

了数值运

算

（+、/等）。结果，标量

和NumPy数组的各

个元素进行

了运算，运算

结

果以NumPy数组

的形式被输

出。刚才的sigmoid函

数的实现也

是如此，因

为

np.exp(-x)会生成NumPy数组

，所以1 /

(1 + np.exp(-x))的运算

将会在

NumPy数组

的各个元素

间进行。

下面

我们把sigmoid函数

画在图上。画

图的代码和

刚才的阶跃

函数的代

码

几乎是一样

的，唯一不同

的地方是把

输出y的函数

换成了sigmoid函数

。

x =

np.arange(-5.0, 5.0, 0.1)

y

= sigmoid(x)

plt.plot(x, y)

plt.ylim(-0.1, 1.1) # 指定y轴的范

围

plt.show()

运行上面

的代码，可以

得到图3-7。

3.2.5　sigmoid函数

和阶跃函数

的比较

现在

我们来比较

一下sigmoid 函数和

阶跃函数，如

图3-8所示。两者

的

不同点在

哪里呢？又有

哪些共同点

呢？我们通过

观察图3-8来思

考一下。

观察

图3-8，首先注意

到的是“平滑

性”的不同。sigmoid函

数是一条平

滑的曲线，输

出随着输入

发生连续性

的变化。而阶

跃函数以0为

界，输出发

生

急剧性的变

化。sigmoid函数的平

滑性对神经

网络的学习

具有重要意

义。

3.2 激活函数

47

图3-7 sigmoid函数的图

形

1.0

0.8

0.6

0.4

0.2

0.0

−6 −4 −2 0

2 4 6

图3-8

阶跃函

数与sigmoid函数（虚

线是阶跃函

数）

1.0

0.8

0.6

0.4

0.2

0.0

−6

−4 −2 0 2

4 6

第 3章　神经

网络 48

另一个

不同点是，相

对于阶跃函

数只能返回

0或1，sigmoid函数可以

返

回0.731 ...、0.880 ...等实数

（这一点和刚

才的平滑性

有关）。也就是

说，感

知机中

神经元之间

流动的是0或

1的二元信号

，而神经网络

中流动的是

连续

的实数

值信号。

如果

把这两个函

数与水联系

起来，则阶跃

函数可以比

作“竹筒敲石

”A，

sigmoid函数可以比

作“水车”。阶跃

函数就像竹

筒敲石一样

，只做是否传

送

水（0或1）两个

动作，而sigmoid函数

就像水车一

样，根据流过

来的水量相

应

地调整传

送出去的水

量。

接着说一

下阶跃函数

和sigmoid函数的共

同性质。阶跃

函数和sigmoid

函数

虽然在平滑

性上有差异

，但是如果从

宏观视角看

图3-8，可以发现

它们

具有相

似的形状。实

际上，两者的

结构均是“输

入小时，输出

接近0（为0）；

随着

输入增大，输

出向1靠近（变

成1）”。也就是说

，当输入信号

为重要信息

时，

阶跃函数

和sigmoid函数都会

输出较大的

值；当输入信

号为不重要

的信息时，

两

者都输出较

小的值。还有

一个共同点

是，不管输入

信号有多小

，或者有多

大

，输出信号的

值都在0到1之

间。

3.2.6　非线性函

数

阶跃函数

和sigmoid函数还有

其他共同点

，就是两者均

为非线性函

数。

sigmoid函数是一

条曲线，阶跃

函数是一条

像阶梯一样

的折线，两者

都属于

非线

性的函数。

在

介绍激活函

数时，经常会

看到“非线性

函数”和“线性

函数”等术语

。

函数本来是

输入某个值

后会返回一

个值的转换

器。向这个转

换器输

入某

个值后，输出

值是输入值

的常数倍的

函数称为线

性函数（用数

学

式表示为

h(x) = cx。c为常数）。因此

，线性函数是

一条笔直的

直线。

而非线

性函数，顾名

思义，指的是

不像线性函

数那样呈现

出一条直

线

的函数。

A 竹筒

敲石是日本

的一种庭院

设施。支点架

起竹筒，一端

下方置石，另

一端切口上

翘。在切口上

滴水，

水积多

后该端下垂

，水流出，另一

端翘起，之后

又因重力而

落下，击石发

出响声。——译者

注

3.2 激活函数

49

神经网络的

激活函数必

须使用非线

性函数。换句

话说，激活函

数不能使

用

线性函数。为

什么不能使

用线性函数

呢？因为使用

线性函数的

话，加深神

经

网络的层数

就没有意义

了。

线性函数

的问题在于

，不管如何加

深层数，总是

存在与之等

效的“无

隐藏

层的神经网

络”。为了具体

地（稍微直观

地）理解这一

点，我们来思

考下面这个

简单的例子

。这里我们考

虑把线性函

数 h(x) = cx

作为激活

函数，把y(x) = h(h(h(x)))的运

算对应3层神

经网络A。这个

运算会进行

y(x) =

c × c ×

c × x的乘法运算

，但是同样的

处理可以由

y(x) =

ax（注意，

a = c

3

）这一次

乘法运算（即

没有隐藏层

的神经网络

）来表示。如本

例所示，

使用

线性函数时

，无法发挥多

层网络带来

的优势。因此

，为了发挥叠

加层所

带来

的优势，激活

函数必须使

用非线性函

数。

3.2.7　ReLU函数

到目

前为止，我们

介绍了作为

激活函数的

阶跃函数和

sigmoid函数。在

神经

网络发展的

历史上，sigmoid函数

很早就开始

被使用了，而

最近则主要

使用ReLU（Rectified

Linear Unit）函数。

ReLU函

数在输入大

于0时，直接输

出该值；在输

入小于等于

0时，输

出0（图3-9）。

ReLU函

数可以表示

为下面的式

(3.7)。

（3.7）

如图 3-9

和式（3.7）所

示，ReLU 函数是一

个非常简单

的函数。因此

，

ReLU函数的实现

也很简单，可

以写成如下

形式。

def

relu(x):

 return np.maximum(0,

x)

A 该对应

只是一个近

似，实际的神

经网络运算

比这个例子

要复杂，但不

影响后面的

结论成立。

——译

者注

第 3章　神

经网络

50

图3-9 ReLU函

数

5

4

3

2

1

−1

0

−6 −4

−2 0 2 4

6

这里使用

了NumPy的maximum函数。maximum函

数会从输入

的数值中选

择较大的那

个值进行输

出。

本章剩余

部分的内容

仍将使用sigmoid函

数作为激活

函数，但在本

书的

后半部

分，则将主要

使用ReLU函数。

3.3 多

维数组的运

算

如果掌握

了NumPy多维数组

的运算，就可

以高效地实

现神经网络

。因此，

本节将

介绍NumPy多维数

组的运算，然

后再进行神

经网络的实

现。

3.3.1　多维数组

简单地讲，多

维数组就是

“数字的集合

”，数字排成一

列的集合、排

成

长方形的

集合、排成三

维状或者（更

加一般化的

）N维状的集合

都称为多维

数

组。下面我

们就用NumPy来生

成多维数组

，先从前面介

绍过的一维

数组开始。

3.3 多

维数组的运

算  51

>>> import numpy as

np

>>> A =

np.array([1, 2, 3, 4])

>>> print(A)

[1 2

3 4]

>>> np.ndim(A)

1

>>> A.shape

(4,)

>>> A.shape[0]

4

如上所示

，数组的维数

可以通过np.dim()函

数获得。此外

，数组的形状

可以通过实

例变量shape获得

。在上面的例

子中，A是一维

数组，由4个元

素

构成。注意

，这里的A.shape的结

果是个元组

（tuple）。这是因为一

维数组的

情

况下也要返

回和多维数

组的情况下

一致的结果

。例如，二维数

组时返回的

是元组(4,3)，三维

数组时返回

的是元组(4,3,2)，因

此一维数组

时也同样以

元组的形式

返回结果。下

面我们来生

成一个二维

数组。

>>> B

= np.array([[1,2], [3,4], [5,6]])

>>> print(B)

[[1 2]

[3 4]

 [5

6]]

>>> np.ndim(B)

2

>>> B.shape

(3, 2)

这里生

成了一个3 × 2的

数组B。3 ×

2的数组

表示第一个

维度有3个元

素，

第二个维

度有2个元素

。另外，第一个

维度对应第

0维，第二个维

度对应第

1维

（Python的索引从0开

始）。二维数组

也称为矩阵

（matrix）。如图3-10所示，

数

组的横向排

列称为行（row），纵

向排列称为

列（column）。

3.3.2　矩阵乘法

下面，我们来

介绍矩阵（二

维数组）的乘

积。比如2 × 2的矩

阵，其乘积

可

以像图3-11这样

进行计算（按

图中顺序进

行计算是规

定好了的）。

  第

3章

神经网络

52

21

43

6

列

行

5

图3-10

横向

排列称为行

，纵向排列称

为列

图3-11　矩阵

的乘积的计

算方法

21

43

A

=

B

65

87

2219

5043

1 × 5 +

2 × 7

3

× 5 + 4

× 7

如本

例所示，矩阵

的乘积是通

过左边矩阵

的行（横向）和

右边矩阵的

列（纵

向）以对

应元素的方

式相乘后再

求和而得到

的。并且，运算

的结果保存

为新

的多维

数组的元素

。比如，A的第1行

和B的第1列的

乘积结果是

新数组的

第

1行第1列的元

素，A的第2行和

B的第1列的结

果是新数组

的第2行第1

列

的元素。另外

，在本书的数

学标记中，矩

阵将用黑斜

体表示（比如

，矩阵

A），以区别

于单个元素

的标量（比如

，a或b）。这个运算

在Python中可以用

如下代码实

现。

>>> A = np.array([[1,2],

[3,4]])

3.3 多维数组

的运算

53

>>> A.shape

(2,

2)

>>> B =

np.array([[5,6], [7,8]])

>>> B.shape

(2, 2)

>>> np.dot(A,

B)

array([[19, 22],

[43, 50]])

这 里

，A

和 B 都 是

2 × 2 的

矩

阵，它 们 的 乘

积

可 以 通 过

NumPy

的

np.dot()函数计算

（乘积也称为

点积）。np.dot()接收两

个NumPy数组作为

参

数，并返回

数组的乘积

。这里要注意

的是，np.dot(A, B)和np.dot(B,

A)的

值

可能不一样

。和一般的运

算（+或*等）不同

，矩阵的乘积

运算中，操作

数（A、

B）的顺序不

同，结果也会

不同。

这里介

绍的是计算

2

× 2形状的矩阵

的乘积的例

子，其他形状

的矩阵的

乘

积也可以用

相同的方法

来计算。比如

，2 ×

3的矩阵和3 × 2 的

矩阵的乘积

可按如下形

式用Python来实现

。

>>> A = np.array([[1,2,3],

[4,5,6]])

>>> A.shape

(2,

3)

>>> B =

np.array([[1,2], [3,4], [5,6]])

>>>

B.shape

(3, 2)

>>>

np.dot(A, B)

array([[22, 28],

[49, 64]])

2 ×

3的矩阵A和3 × 2的

矩阵B的乘积

可按以上方

式实现。这里

需要

注意的

是矩阵的形

状（shape）。具体地讲

，矩阵A的第1维

的元素个数

（列数）

必须和

矩阵B的第0维

的元素个数

（行数）相等。在

上面的例子

中，矩阵A

的形

状是2 × 3，矩阵B的

形状是3

× 2，矩阵

A的第1维的元

素个数（3）和

矩

阵B的第0维的

元素个数（3）相

等。如果这两

个值不相等

，则无法计算

矩

阵的乘积

。比如，如果用

Python计算2

× 3 的矩阵

A和2 ×

2的矩阵C的

乘

积，则会输

出如下错误

。

>>> C

= np.array([[1,2], [3,4]])

>>>

C.shape

  第

3章　神经网

络 54

(2,

2)

>>> A.shape

(2,

3)

>>> np.dot(A, C)

Traceback (most recent call

last):

 File "<stdin>",

line 1, in <module>

ValueError: shapes (2,3) and

(2,2) not aligned: 3

(dim 1) != 2

(dim 0)

这个错误

的意思是，矩

阵A的第1维和

矩阵C的第0维

的元素个数

不一

致（维度

的索引从0开

始）。也就是说

，在多维数组

的乘积运算

中，必须使两

个矩阵中的

对应维度的

元素个数一

致，这一点很

重要。我们通

过图3-12再来

确

认一下。

图3-12　在

矩阵的乘积

运算中，对应

维度的元素

个数要保持

一致

A

B C =

3

× 2 2 ×

4 3 × 4

形状：

保

持一致

图3-12中

，3 ×

2的矩阵A和2 × 4 的

矩阵B的乘积

运算生成了

3

× 4的

矩阵C。如图

所示，矩阵A和

矩阵B的对应

维度的元素

个数必须保

持一致。

此外

，还有一点很

重要，就是运

算结果的矩

阵C的形状是

由矩阵A的行

数

和矩阵B的

列数构成的

。

另外，当A是二

维矩阵、B是一

维数组时，如

图3-13所示，对应

维度

的元素

个数要保持

一致的原则

依然成立。

可

按如下方式

用Python实现图3-13的

例子。

>>> A = np.array([[1,2],

[3, 4], [5,6]])

>>>

A.shape

(3, 2)

>>>

B = np.array([7,8])

>>>

B.shape

(2,)

>>> np.dot(A,

B)

array([23, 53, 83])

3.3 多维数

组的运算  55

图

3-13 A是二维矩阵

、B是一维数组

时，也要保持

对应维度的

元素个数一

致

2 3

× 2 3 形状：

保持

一致

B C =

A

3.3.3　神经网

络的内积

下

面我们使用

NumPy矩阵来实现

神经网络。这

里我们以图

3-14中的简

单神

经网络为对

象。这个神经

网络省略了

偏置和激活

函数，只有权

重。

X W Y

=

2 2 ×

3 3

1

2

3

5

4

6

)( 1 3 5

2 4 6

一致

x1

x2

y 1

y 2

y 3

图3-14　通

过矩阵的乘

积进行神经

网络的运算

实现该神经

网络时，要注

意X、W、Y的形状，特

别是X和W的对

应

维度的元

素个数是否

一致，这一点

很重要。

>>>

X = np.array([1, 2])

>>> X.shape

(2,)

>>>

W = np.array([[1, 3,

5], [2, 4, 6]])

>>> print(W)

[[1 3

5]

 [2 4

6]]

>>> W.shape

第 3章

神经网络 56

(2,

3)

>>> Y =

np.dot(X, W)

>>> print(Y)

[ 5 11 17]

如

上所示，使用

np.dot（多维数组的

点积），可以一

次性计算出

Y 的结果。

这意

味着，即便Y 的

元素个数为

100或1000，也可以通

过一次运算

就计算出

结

果！如果不使

用np.dot，就必须单

独计算Y 的每

一个元素（或

者说必须使

用for语句），非常

麻烦。因此，通

过矩阵的乘

积一次性完

成计算的技

巧，在

实现的

层面上可以

说是非常重

要的。

3.4

3层神经

网络的实现

现在我们来

进行神经网

络的实现。这

里我们以图

3-15的3层神经网

络为

对象，实

现从输入到

输出的（前向

）处理。在代码

实现方面，使

用上一节介

绍的NumPy多维数

组。巧妙地使

用NumPy数组，可以

用很少的代

码完成

神经

网络的前向

处理。

图3-15

3层神

经网络：输入

层（第0层）有2个

神经元，第1个

隐藏层（第1层

）有3个神经元

，

第2个隐藏层

（第2层）有2个神

经元，输出层

（第3层）有2个神

经元

x1

x2

y 1

y 2

3.4  3层神经

网络的实现

57

3.4.1　符号确认

在

介绍神经网

络中的处理

之前，我们先

导入 、

等符号

。这些符

号可

能看上去有

些复杂，不过

因为只在本

节使用，稍微

读一下就跳

过去也问

题

不大。

本节的

重点是神经

网络的运算

可以作为矩

阵运算打包

进行。因为

神

经网络各层

的运算是通

过矩阵的乘

法运算打包

进行的（从宏

观

视角来考

虑），所以即便

忘了（未记忆

）具体的符号

规则，也不影

响理解后面

的内容。

我们

先从定义符

号开始。请看

图3-16。图3-16中只突

出显示了从

输入层

神经

元x2到后一层

的神经元

的

权重。

如图3-16所

示，权重和隐

藏层的神经

元的右上角

有一个“(1)”，它表

示

权重和神

经元的层号

（即第1层的权

重、第1层的神

经元）。此外，权

重的右

下角

有两个数字

，它们是后一

层的神经元

和前一层的

神经元的索

引号。比如，

表

示前一层的

第2个神经元

x2到后一层的

第1个神经元

的权重。权

重

右下角按照

“后一层的索

引号、前一层

的索引号”的

顺序排列。

w

1

2

(1)

第

1层的权重

后

一层的第1个

神经元

前一

层的第2个神

经元

x1

x2

图3-16

权重

的符号

  第

3章

神经网络 58

3.4.2　各

层间信号传

递的实现

现

在看一下从

输入层到第

1层的第1个神

经元的信号

传递过程，如

图3-17

所示。

1

x1

x2

y 1

y

2

图3-17　从

输入层到第

1层的信号传

递

图3-17中增加

了表示偏置

的神经元“1”。请

注意，偏置的

右下角的索

引

号只有一

个。这是因为

前一层的偏

置神经元（神

经元“1”）只有一

个A 。

为了确认

前面的内容

，现在用数学

式表示 。

通过

加权信号和

偏

置的和按

如下方式进

行计算。

（3.8）

A

任何

前一层的偏

置神经元“1”都

只有一个。偏

置权重的数

量取决于后

一层的神经

元的数量（不

包括

后一层

的偏置神经

元“1”）。——译者注

3.4 3层

神经网络的

实现

59

此外，如

果使用矩阵

的乘法运算

，则可以将第

1层的加权和

表示成下面

的式（3.9）。

A(1) =

XW(1) + B(1) （3.9）

其中，A(1)

、X、B(1)

、W(1)

如

下所示。

下面

我们用NumPy多维

数组来实现

式（3.9），这里将输

入信号、权重

、

偏置设置成

任意值。

X =

np.array([1.0, 0.5])

W1 =

np.array([[0.1, 0.3, 0.5], [0.2,

0.4, 0.6]])

B1 =

np.array([0.1, 0.2, 0.3])

print(W1.shape)

# (2, 3)

print(X.shape)

# (2,)

print(B1.shape) #

(3,)

A1 = np.dot(X,

W1) + B1

这个

运算和上一

节进行的运

算是一样的

。W1是2

× 3的数组，X是

元素个

数为

2的一维数组

。这里，W1和X的对

应维度的元

素个数也保

持了一致。

接

下来，我们观

察第1层中激

活函数的计

算过程。如果

把这个计算

过程

用图来

表示的话，则

如图3-18所示。

如

图3-18所示，隐藏

层的加权和

（加权信号和

偏置的总和

）用a表示，被

激

活函数转换

后的信号用

z表示。此外，图

中h()表示激活

函数，这里我

们

使用的是

sigmoid函数。用Python来实

现，代码如下

所示。

Z1 = sigmoid(A1)

print(A1)

# [0.3, 0.7, 1.1]

print(Z1) # [0.57444252, 0.66818777,

0.75026011]

  第

3章　神

经网络 60

图3-18

从

输入层到第

1层的信号传

递

1

1

1

h()

h()

h()

x1

x2

y1

y2

这个sigmoid()函数

就是之前定

义的那个函

数。它会接收

NumPy数组，

并返回

元素个数相

同的NumPy数组。

下

面，我们来实

现第1层到第

2层的信号传

递（图3-19）。

W2 =

np.array([[0.1, 0.4], [0.2, 0.5],

[0.3, 0.6]])

B2 =

np.array([0.1, 0.2])

print(Z1.shape) #

(3,)

print(W2.shape) # (3,

2)

print(B2.shape) # (2,)

A2 = np.dot(Z1, W2)

+ B2

Z2 =

sigmoid(A2)

除了第

1层的输出（Z1）变

成了第2层的

输入这一点

以外，这个实

现和刚

才的

代码完全相

同。由此可知

，通过使用NumPy数

组，可以将层

到层的信

号

传递过程简

单地写出来

。

3.4 3层神经网络

的实现  61

图3-19　第

1层到第2层的

信号传递

h()

h()

1

1

1

x1

x2

y1

y2

最

后是第2层到

输出层的信

号传递（图3-20）。输

出层的实现

也和之前的

实现基本相

同。不过，最后

的激活函数

和之前的隐

藏层有所不

同。

def identity_function(x):

 return

x

W3 = np.array([[0.1,

0.3], [0.2, 0.4]])

B3

= np.array([0.1, 0.2])

A3

= np.dot(Z2, W3) +

B3

Y = identity_function(A3)

# 或者Y = A3

这里

我们定义了

identity_function()函数（也称为

“恒等函数”），并

将

其作为输

出层的激活

函数。恒等函

数会将输入

按原样输出

，因此，这个例

子

中没有必

要特意定义

identity_function()。这里这样实

现只是为了

和之前的

流

程保持统一

。另外，图3-20中，输

出层的激活

函数用σ()表示

，不同于隐

藏

层的激活函

数h()（σ读作sigma）。

  第

3章

神经网络 62

图

3-20　从第2层到输

出层的信号

传递

1

1

1

x1

x2

y1

y2

σ()

σ()

输出层

所用的激活

函数，要根据

求解问题的

性质决定。一

般地，回

归问

题可以使用

恒等函数，二

元分类问题

可以使用 sigmoid函

数，

多元分类

问题可以使

用 softmax函数。关于

输出层的激

活函数，我

们

将在下一节

详细介绍。

3.4.3

代

码实现小结

至此，我们已

经介绍完了

3层神经网络

的实现。现在

我们把之前

的代码

实现

全部整理一

下。这里，我们

按照神经网

络的实现惯

例，只把权重

记为大

写字

母W1，其他的（偏

置或中间结

果等）都用小

写字母表示

。

def

init_network():

 network =

{}

 network['W1'] =

np.array([[0.1, 0.3, 0.5], [0.2,

0.4, 0.6]])

 network['b1']

= np.array([0.1, 0.2, 0.3])

network['W2'] = np.array([[0.1, 0.4],

[0.2, 0.5], [0.3, 0.6]])

network['b2'] = np.array([0.1, 0.2])

network['W3'] = np.array([[0.1, 0.3],

[0.2, 0.4]])

3.5 输出层的设

计

63

 network['b3'] =

np.array([0.1, 0.2])

 return

network

def forward(network, x):

W1, W2, W3 =

network['W1'], network['W2'], network['W3']

b1, b2, b3 =

network['b1'], network['b2'], network['b3']

a1 = np.dot(x, W1)

+ b1

 z1

= sigmoid(a1)

 a2

= np.dot(z1, W2) +

b2

 z2 =

sigmoid(a2)

 a3 =

np.dot(z2, W3) + b3

y = identity_function(a3)

return y

network =

init_network()

x = np.array([1.0,

0.5])

y = forward(network,

x)

print(y) # [

0.31682708 0.69627909]

这里定义

了init_network()和forward()函数。init_network()函

数会进

行权

重和偏置的

初始化，并将

它们保存在

字典变量network中

。这个字典变

量network中保存了

每一层所需

的参数（权重

和偏置）。forward()函数

中则封

装了

将输入信号

转换为输出

信号的处理

过程。

另外，这

里出现了forward（前

向）一词，它表

示的是从输

入到输出方

向

的传递处

理。后面在进

行神经网络

的训练时，我

们将介绍后

向（backward，

从输出到

输入方向）的

处理。

至此，神

经网络的前

向处理的实

现就完成了

。通过巧妙地

使用NumPy

多维数

组，我们高效

地实现了神

经网络。

3.5 输出

层的设计

神

经网络可以

用在分类问

题和回归问

题上，不过需

要根据情况

改变输出

层

的激活函数

。一般而言，回

归问题用恒

等函数，分类

问题用softmax函数

。

第 3章　神经网

络 64

机器学习

的问题大致

可以分为分

类问题和回

归问题。分类

问题是数

据

属于哪一个

类别的问题

。比如，区分图

像中的人是

男性还是女

性

的问题就

是分类问题

。而回归问题

是根据某个

输入预测一

个（连续的）

数

值的问题。比

如，根据一个

人的图像预

测这个人的

体重的问题

就

是回归问

题（类似“57.4kg”这样

的预测）。

3.5.1　恒等

函数和 softmax函数

恒等函数会

将输入按原

样输出，对于

输入的信息

，不加以任何

改动地直

接

输出。因此，在

输出层使用

恒等函数时

，输入信号会

原封不动地

被输出。

另外

，将恒等函数

的处理过程

用之前的神

经网络图来

表示的话，则

如图3-21

所示。和

前面介绍的

隐藏层的激

活函数一样

，恒等函数进

行的转换处

理可以

用一

根箭头来表

示。

a1 y1

a2 y2

a3 y3

σ()

σ()

σ()

图3-21　恒等函

数

分类问题

中使用的softmax函

数可以用下

面的式（3.10）表示

。

（3.10）

exp(x)是表示e

x

的指

数函数（e是纳

皮尔常数2.7182

...）。式

（3.10）表示

假设输

出层共有n个

神经元，计算

第k个神经元

的输出yk。如式

（3.10）所示，

softmax函数的

分子是输入

信号ak的指数

函数，分母是

所有输入信

号的指数

函

数的和。

3.5 输出

层的设计  65

用

图表示softmax函数

的话，如图3-22所

示。图3-22中，softmax函数

的输出通过

箭头与所有

的输入信号

相连。这是因

为，从式（3.10）可以

看出，

输出层

的各个神经

元都受到所

有输入信号

的影响。

a1 y1

a2 y2

a3 y3

σ()

图3-22 softmax函

数

现在我们

来实现softmax函数

。在这个过程

中，我们将使

用Python解释

器逐

一确认结果

。

>>> a =

np.array([0.3, 2.9, 4.0])

>>>

>>> exp_a = np.exp(a)

# 指数函数

>>> print(exp_a)

[ 1.34985881 18.17414537 54.59815003]

>>>

>>> sum_exp_a =

np.sum(exp_a) # 指

数函数的和

>>> print(sum_exp_a)

74.1221542102

>>>

>>> y

= exp_a / sum_exp_a

>>> print(y)

[ 0.01821127

0.24519181 0.73659691]

这个Python实现是

完全依照式

（3.10）进行的，所以

不需要特别

的解释。

考虑

到后面还要

使用softmax函数，这

里我们把它

定义成如下

的Python函数。

def softmax(a):

 exp_a

= np.exp(a)

 sum_exp_a

= np.sum(exp_a)

 y

= exp_a / sum_exp_a

return y

第 3章

神经网络 66

3.5.2

实

现 softmax函数时的

注意事项

上

面的softmax函数的

实现虽然正

确描述了式

（3.10），但在计算机

的运算

上有

一定的缺陷

。这个缺陷就

是溢出问题

。softmax函数的实现

中要进行指

数函数的运

算，但是此时

指数函数的

值很容易变

得非常大。比

如，e

10的值

会超

过20000，e

100会变成一

个后面有40多

个0的超大值

，e

1000的结果会返

回

一个表示

无穷大的inf。如

果在这些超

大值之间进

行除法运算

，结果会出现

“不

确定”的情

况。

计算机处

理“数”时，数值

必须在 4字节

或

8字节的有

限数据宽度

内。

这意味着

数存在有效

位数，也就是

说，可以表示

的数值范围

是有

限的。因

此，会出现超

大值无法表

示的问题。这

个问题称为

溢出，

在进行

计算机的运

算时必须（常

常）注意。

softmax函数

的实现可以

像式（3.11）这样进

行改进。

（3.11）

首先

，式（3.11）在分子和

分母上都乘

上C这个任意

的常数（因为

同时对

分母

和分子乘以

相同的常数

，所以计算结

果不变）。然后

，把这个C移动

到

指数函数

（exp）中，记为log C。最后

，把log C替换为另

一个符号C 

。

式

（3.11）说明，在进行

softmax的指数函数

的运算时，加

上（或者减去

）

某个常数并

不会改变运

算的结果。这

里的C 

可以使

用任何值，但

是为了防

止

溢出，一般会

使用输入信

号中的最大

值。我们来看

一个具体的

例子。

3.5 输出层

的设计

67

>>> a =

np.array([1010, 1000, 990])

>>>

np.exp(a) / np.sum(np.exp(a)) #

softmax函数

的运算

array([ nan, nan,

nan]) # 没有

被正确计算

>>>

>>>

c = np.max(a) #

1010

>>> a -

c

array([ 0, -10,

-20])

>>>

>>> np.exp(a

- c) / np.sum(np.exp(a

- c))

array([ 9.99954600e-01,

4.53978686e-05, 2.06106005e-09])

如该例所示

，通过减去输

入信号中的

最大值（上例

中的c），我们发

现原

本为nan（not

a number，不

确定）的地方

，现在被正确

计算了。综上

，我们

可以像

下面这样实

现softmax函数。

def

softmax(a):

 c =

np.max(a)

 exp_a =

np.exp(a - c) #

溢出

对策

 sum_exp_a =

np.sum(exp_a)

 y =

exp_a / sum_exp_a

return y

3.5.3　softmax函数的

特征

使用softmax()函

数，可以按如

下方式计算

神经网络的

输出。

>>> a =

np.array([0.3, 2.9, 4.0])

>>>

y = softmax(a)

>>>

print(y)

[ 0.01821127 0.24519181

0.73659691]

>>> np.sum(y)

1.0

如上所

示，softmax函数的输

出是0.0到1.0之间

的实数。并且

，softmax

函数的输出

值的总和是

1。输出总和为

1是softmax函数的一

个重要性质

。正

因为有了

这个性质，我

们才可以把

softmax函数的输出

解释为“概率

”。

比如，上面的

例子可以解

释成y[0]的概率

是0.018（1.8

%），y[1]的概率

是

0.245（24.5 %），y[2]的概率是0.737（73.7 %）。从

概率的结果

来看，可以

说

“因为第2个元

素的概率最

高，所以答案

是第2个类别

”。而且，还可以

回

  第

3章　神经

网络 68

答“有74

%的

概率是第2个

类别，有25 %的概

率是第1个类

别，有1 %的概

率

是第0个类别

”。也就是说，通

过使用softmax函数

，我们可以用

概率的（统

计

的）方法处理

问题。

这里需

要注意的是

，即便使用了

softmax函数，各个元

素之间的大

小关

系也不

会改变。这是

因为指数函

数（y =

exp(x)）是单调递

增函数。实际

上，

上例中a的

各元素的大

小关系和y的

各元素的大

小关系并没

有改变。比如

，a

的最大值是

第2个元素，y的

最大值也仍

是第2个元素

。

一般而言，神

经网络只把

输出值最大

的神经元所

对应的类别

作为识别结

果。

并且，即便

使用softmax函数，输

出值最大的

神经元的位

置也不会变

。因此，

神经网

络在进行分

类时，输出层

的softmax函数可以

省略。在实际

的问题中，

由

于指数函数

的运算需要

一定的计算

机运算量，因

此输出层的

softmax函数

一般会

被省略。AB

求解

机器学习问

题的步骤可

以分为“学习

”A 和“推理”两个

阶段。首

先，在

学习阶段进

行模型的学

习B，然后，在推

理阶段，用学

到的

模型对

未知的数据

进行推理（分

类）。如前所述

，推理阶段一

般会省

略输

出层的 softmax函数

。在输出层使

用 softmax函数是因

为它和

神经

网络的学习

有关系（详细

内容请参考

下一章）。

3.5.4　输出

层的神经元

数量

输出层

的神经元数

量需要根据

待解决的问

题来决定。对

于分类问题

，输

出层的神

经元数量一

般设定为类

别的数量。比

如，对于某个

输入图像，预

测

是图中的

数字0到9中的

哪一个的问

题（10类别分类

问题），可以像

图3-23这样，

将输

出层的神经

元设定为10个

。

如图3-23所示，在

这个例子中

，输出层的神

经元从上往

下依次对应

数字

0,

1, ..., 9。此外，图

中输出层的

神经元的值

用不同的灰

度表示。这个

例子

A“学习”也

称为“训练”，为

了强调算法

从数据中学

习模型，本书

使用“学习”一

词。——译者注

B 这

里的“学习”是

指使用训练

数据、自动调

整参数的过

程，具体请参

考第4章。——译者

注

3.6

手写数字

识别  69

中神经

元y2颜色最深

，输出的值最

大。这表明这

个神经网络

预测的是y2对

应

的类别，也

就是“2”。

图3-23　输出

层的神经元

对应各个数

字

=

“ 0 ”

=

“ 2 ”

=

“ 1 ”

=

“ 9 ”

输入层

输

出层

某种运

算

y0

y1

y2

y9

3.6 手写数字

识别

介绍完

神经网络的

结构之后，现

在我们来试

着解决实际

问题。这里我

们

来进行手

写数字图像

的分类。假设

学习已经全

部结束，我们

使用学习到

的参

数，先实

现神经网络

的“推理处理

”。这个推理处

理也称为神

经网络的前

向

传播（forward

propagation）。

和求

解机器学习

问题的步骤

（分成学习和

推理两个阶

段进行）一样

，

使用神经网

络解决问题

时，也需要首

先使用训练

数据（学习数

据）进

行权重

参数的学习

；进行推理时

，使用刚才学

习到的参数

，对输入

数据

进行分类。

  第

3章

神经网络

70

3.6.1　MNIST数据集

这里

使用的数据

集是MNIST手写数

字图像集。MNIST是

机器学习领

域

最有名的

数据集之一

，被应用于从

简单的实验

到发表的论

文研究等各

种场合。

实际

上，在阅读图

像识别或机

器学习的论

文时，MNIST数据集

经常作为实

验用的数据

出现。

MNIST数据集

是由0到9的数

字图像构成

的（图3-24）。训练图

像有6万张，

测

试图像有1万

张，这些图像

可以用于学

习和推理。MNIST数

据集的一般

使用方法是

，先用训练图

像进行学习

，再用学习到

的模型度量

能在多大程

度

上对测试

图像进行正

确的分类。

图

3-24 MNIST图像数据集

的例子

MNIST的图

像数据是28像

素

× 28像素的灰

度图像（1通道

），各个像素

的

取值在0到255之

间。每个图像

数据都相应

地标有“7”“2”“1”等标

签。

本书提供

了便利的Python脚

本mnist.py，该脚本支

持从下载MNIST数

据

集到将这

些数据转换

成NumPy数组等处

理（mnist.py在dataset目录下

）。使用

mnist.py时，当前

目录必须是

ch01、ch02、ch03、…、ch08目录中的一

个。使

用mnist.py中的

load_mnist()函数，就可以

按下述方式

轻松读入MNIST数

据。

import

sys, os

sys.path.append(os.pardir) #

为了导入

父目录中的

文件而进行

的设定

from dataset.mnist import

load_mnist

# 第一

次调用会花

费几分钟 ……

(x_train, t_train), (x_test, t_test)

= load_mnist(flatten=True,

normalize=False)

#

输

出各个数据

的形状

print(x_train.shape) # (60000,

784)

3.6 手写

数字识别

71

print(t_train.shape) # (60000,)

print(x_test.shape) # (10000, 784)

print(t_test.shape) # (10000,)

首

先，为了导入

父目录中的

文件，进行相

应的设定A。然

后，导入

dataset/mnist.py中的

load_mnist函数。最后，使

用 load_mnist函数，读入

MNIST数据集。第一

次调用load_mnist函数

时，因为要下

载MNIST数据集，

所

以需要接入

网络。第2次及

以后的调用

只需读入保

存在本地的

文件（pickle

文件）即

可，因此处理

所需的时间

非常短。

用来

读入MNIST图像的

文件在本书

提供的源代

码的dataset目

录下

。并且，我们假

定了这个MNIST数

据集只能从

ch01、ch02、

ch03、…、ch08目录中使用

，因此，使用时

需要从父目

录（dataset

目录）中导

入文件，为此

需要添加sys.path.append(os.pardir)语

句。

load_mnist函数以“(训

练图像 ,训练

标签 )，(测试图

像，测试标签

)”的

形式返回

读入的MNIST数据

。此外，还可以

像load_mnist(normalize=True,

flatten=True, one_hot_label=False) 这 样，设

置

3 个 参 数。第

1 个

参数

normalize设置是

否将输入图

像正规化为

0.0～1.0的值。如果将

该参数设置

为False，则输入图

像的像素会

保持原来的

0～255。第2个参数flatten设

置

是否展开

输入图像（变

成一维数组

）。如果将该参

数设置为False，则

输入图

像为

1 × 28 ×

28的三维数组

；若设置为True，则

输入图像会

保存为由784个

元素构成的

一维数组。第

3个参数one_hot_label设置

是否将标签

保存为one￾hot表示

（one-hot representation）。one-hot表示是仅正

确解标签为

1，其余

皆为0的

数组，就像[0,0,1,0,0,0,0,0,0,0]这

样。当one_hot_label为False时，

只

是像7、2这样简

单保存正确

解标签；当one_hot_label为

True时，标签则

保

存为one-hot表示。

A 观

察本书源代

码可知，上述

代码在mnist_show.py文件

中。mnist_show.py文件的当

前目录是ch03，

但

包含load_mnist()函数的

mnist.py文件在dataset目录

下。因此，mnist_show.py文件

不能跨目

录

直接导入mnist.py文

件。sys.path.append(os.pardir)语句实际

上是把父目

录deep-learning￾from-scratch加入到sys.path（Python的

搜索模块的

路径集）中，从

而可以导入

deep-learning￾from-scratch下的任何目

录（包括dataset目录

）中的任何文

件。——译者注

  第

3章

神经网络

72

Python有 pickle这个便利

的功能。这个

功能可以将

程序运行中

的对

象保存

为文件。如果

加载保存过

的

pickle文件，可以

立刻复原之

前

程序运行

中的对象。用

于读入MNIST数据

集的load_mnist()函数内

部也使用了

pickle功能（在第 2次

及以后读入

时）。利用 pickle功能

，

可以高效地

完成MNIST数据的

准备工作。

现

在，我们试着

显示MNIST图像，同

时也确认一

下数据。图像

的显示

使用

PIL（Python Image

Library）模块。执行下

述代码后，训

练图像的第

一

张就会显

示出来，如图

3-25所示（源代码

在ch03/mnist_show.py中）。

import sys,

os

sys.path.append(os.pardir)

import numpy

as np

from dataset.mnist

import load_mnist

from PIL

import Image

def img_show(img):

pil_img = Image.fromarray(np.uint8(img))

pil_img.show()

(x_train, t_train), (x_test,

t_test) = load_mnist(flatten=True,

normalize=False)

img = x_train[0]

label

= t_train[0]

print(label) #

5

print(img.shape) # (784,)

img = img.reshape(28, 28)

# 把图像

的形状变成

原来的尺寸

print(img.shape) # (28,

28)

img_show(img)

这里需要注

意的是，flatten=True时读

入的图像是

以一列（一维

）NumPy

数组的形式

保存的。因此

，显示图像时

，需要把它变

为原来的28像

素

× 28

像素的形

状。可以通过

reshape()方法的参数

指定期望的

形状，更改NumPy

数

组的形状。此

外，还需要把

保存为NumPy数组

的图像数据

转换为PIL用

的

数据对象，这

个转换处理

由Image.fromarray()来完成。

3.6 手

写数字识别

73

图3-25

显示MNIST图像

3.6.2　神经网络的

推理处理

下

面，我们对这

个MNIST数据集实

现神经网络

的推理处理

。神经网络

的

输入层有784个

神经元，输出

层有10个神经

元。输入层的

784这个数字来

源于图像大

小的28

× 28 = 784，输出层

的10这个数字

来源于10类别

分类（数

字0到

9，共10类别）。此外

，这个神经网

络有2个隐藏

层，第1个隐藏

层有

50个神经

元，第2个隐藏

层有100个神经

元。这个50和100可

以设置为任

何值。

下面我

们先定义get_data()、init_network()、predict()这

3个函数（代码

在

ch03/neuralnet_mnist.py中）。

def get_data():

 (x_train,

t_train), (x_test, t_test) =

\

 load_mnist(normalize=True, flatten=True,

one_hot_label=False)

 return x_test,

t_test

def init_network():

with open("sample_weight.pkl", 'rb') as

f:

 network =

pickle.load(f)

 return network

def predict(network, x):

W1, W2, W3 =

network['W1'], network['W2'], network['W3']

b1, b2, b3 =

network['b1'], network['b2'], network['b3']

第 3章　神

经网络 74

a1 = np.dot(x, W1)

+ b1

 z1

= sigmoid(a1)

 a2

= np.dot(z1, W2) +

b2

 z2 =

sigmoid(a2)

 a3 =

np.dot(z2, W3) + b3

y = softmax(a3)

return y

init_network()会读

入保存在pickle文

件sample_weight.pkl中的学习

到的

权重参

数A。这个文件

中以字典变

量的形式保

存了权重和

偏置参数。剩

余的2

个函数

，和前面介绍

的代码实现

基本相同，无

需再解释。现

在，我们用这

3

个函数来实

现神经网络

的推理处理

。然后，评价它

的识别精度

（accuracy），

即能在多大

程度上正确

分类。

x,

t = get_data()

network

= init_network()

accuracy_cnt =

0

for i in

range(len(x)):

 y =

predict(network, x[i])

 p

= np.argmax(y) # 获取概

率最高的元

素的索引

if p == t[i]:

accuracy_cnt += 1

print("Accuracy:"

+ str(float(accuracy_cnt) / len(x)))

首

先获得MNIST数据

集，生成网络

。接着，用for语句

逐一取出保

存

在x中的图

像数据，用predict()函

数进行分类

。predict()函数以NumPy数

组

的形式输出

各个标签对

应的概率。比

如输出[0.1, 0.3,

0.2, ..., 0.04]的

数

组，该数组表

示“0”的概率为

0.1，“1”的概率为0.3，等

等。然后，我们

取出这个概

率列表中的

最大值的索

引（第几个元

素的概率最

高），作为预测

结

果。可以用

np.argmax(x)函数取出数

组中的最大

值的索引，np.argmax(x)将

获取被赋给

参数x的数组

中的最大值

元素的索引

。最后，比较神

经网络所预

测的答案和

正确解标签

，将回答正确

的概率作为

识别精度。

A 因

为之前我们

假设学习已

经完成，所以

学习到的参

数被保存下

来。假设保存

在sample_weight.pkl

文件中，在

推理阶段，我

们直接加载

这些已经学

习到的参数

。——译者注

3.6 手写

数字识别  75

执

行上面的代

码后，会显示

“Accuracy:0.9352”。这表示有93.52 %的

数

据被正确

分类了。目前

我们的目标

是运行学习

到的神经网

络，所以不讨

论识

别精度

本身，不过以

后我们会花

精力在神经

网络的结构

和学习方法

上，思考

如何

进一步提高

这个精度。实

际上，我们打

算把精度提

高到99 %以上。

另

外，在这个例

子中，我们把

load_mnist函数的参数

normalize设置成了

True。将

normalize设置成True后，函

数内部会进

行转换，将图

像的各个像

素值除以255，使

得数据的值

在0.0～1.0的范围内

。像这样把数

据限定到某

个范围内的

处理称为正

规化（normalization）。此外，对

神经网络的

输入数据

进

行某种既定

的转换称为

预处理（pre-processing）。这里

，作为对输入

图像的

一种

预处理，我们

进行了正规

化。

预处理在

神经网络（深

度学习）中非

常实用，其有

效性已在提

高识别

性能

和学习的效

率等众多实

验中得到证

明。在刚才的

例子中，作为

一种预处理

，我们将各个

像素值除以

255，进行了简单

的正规化。

实

际上，很多预

处理都会考

虑到数据的

整体分布。比

如，利用数据

整体的均值

或标准差，移

动数据，使数

据整体以 0为

中心分布，或

者进行正规

化，把数据的

延展控制在

一定范围内

。除此之外，还

有

将数据整

体的分布形

状均匀化的

方法，即数据

白化（whitening）等。

3.6.3

批处

理

以上就是

处理MNIST数据集

的神经网络

的实现，现在

我们来关注

输入

数据和

权重参数的

“形状”。再看一

下刚才的代

码实现。

下面

我们使用Python解

释器，输出刚

才的神经网

络的各层的

权重的形状

。

>>> x, _ =

get_data()

>>> network =

init_network()

>>> W1, W2,

W3 = network['W1'], network['W2'],

network['W3']

>>>

>>> x.shape

(10000, 784)

>>> x[0].shape

(784,)

>>> W1.shape

第 3章　神经网

络 76

(784, 50)

>>> W2.shape

(50, 100)

>>> W3.shape

(100, 10)

我们通过

上述结果来

确认一下多

维数组的对

应维度的元

素个数是否

一致

（省略了

偏置）。用图表

示的话，如图

3-26所示。可以发

现，多维数组

的对应

维度

的元素个数

确实是一致

的。此外，我们

还可以确认

最终的结果

是输出了

元

素个数为10 的

一维数组。

图

3-26

数组形状的

变化

X W1 Y

→

784 × 50

784 10 形状：

一

致

W2 W3

50 ×

100 100 × 10

一致 一致

从整体的处

理流程来看

，图3-26中，输入一

个由784个元素

（原本是一

个

28 ×

28的二维数组

）构成的一维

数组后，输出

一个有10个元

素的一维数

组。

这是只输

入一张图像

数据时的处

理流程。

现在

我们来考虑

打包输入多

张图像的情

形。比如，我们

想用predict()

函数一

次性打包处

理100张图像。为

此，可以把x的

形状改为100

× 784，将

100张图像打包

作为输入数

据。用图表示

的话，如图3-27所

示。

图3-27　批处理

中数组形状

的变化

100 × 10

X

W1 Y →

100

× 784 784 ×

50 形状

：

W2 W3

50 × 100 100

× 10

如图 3-27

所示，输

入数据的形

状为 100 × 784，输出数

据的形状为

100

× 10。这表示输入

的100张图像的

结果被一次

性输出了。比

如，x[0]和

3.6 手写数

字识别

77

y[0]中保

存了第0张图

像及其推理

结果，x[1]和y[1]中保

存了第1张图

像及

其推理

结果，等等。

这

种打包式的

输入数据称

为批（batch）。批有“捆

”的意思，图像

就如同

纸币

一样扎成一

捆。

批处理对

计算机的运

算大有利处

，可以大幅缩

短每张图像

的处理时

间

。那么为什么

批处理可以

缩短处理时

间呢？这是因

为大多数处

理

数值计算

的库都进行

了能够高效

处理大型数

组运算的最

优化。并且，

在

神经网络的

运算中，当数

据传送成为

瓶颈时，批处

理可以减轻

数

据总线的

负荷（严格地

讲，相对于数

据读入，可以

将更多的时

间用在

计算

上）。也就是说

，批处理一次

性计算大型

数组要比分

开逐步计算

各个小型数

组速度更快

。

下面我们进

行基于批处

理的代码实

现。这里用粗

体显示与之

前的实现的

不同之处。

x, t = get_data()

network = init_network()

batch_size

= 100 # 批

数量

accuracy_cnt = 0

for

i in range(0, len(x),

batch_size):

 x_batch =

x[i:i+batch_size]

 y_batch =

predict(network, x_batch)

 p

= np.argmax(y_batch, axis=1)

accuracy_cnt += np.sum(p ==

t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt)

/ len(x)))

我们来

逐个解释粗

体的代码部

分。首先是range()函

数。range()函数若

指

定为range(start,

end)，则会生

成一个由start到

end-1之间的整数

构成的

列表

。若像range(start, end, step)这样指

定3个整数，则

生成的列表

中的

下一个

元素会增加

step指定的值。我

们来看一个

例子。

>>> list( range(0,

10) )

[0, 1,

2, 3, 4, 5,

6, 7, 8, 9]

第 3章　神

经网络

78

>>> list( range(0,

10, 3) )

[0,

3, 6, 9]

在range()函

数生成的列

表的基础上

，通过x[i:i+batch_size]从输入

数

据中抽出

批数据。x[i:i+batch_n]会取

出从第i个到

第i+batch_n个之间的

数据。

本例中

是像x[0:100]、x[100:200]……这样，从

头开始以100为

单位将数据

提

取为批数

据。

然后，通过

argmax()获取值最大

的元素的索

引。不过这里

需要注意的

是，

我们给定

了参数axis=1。这指

定了在100 × 10的数

组中，沿着第

1维方向（以

第

1维为轴）找到

值最大的元

素的索引（第

0维对应第1个

维度）A。这里也

来

看一个例

子。

>>> x =

np.array([[0.1, 0.8, 0.1], [0.3,

0.1, 0.6],

... [0.2,

0.5, 0.3], [0.8, 0.1,

0.1]])

>>> y =

np.argmax(x, axis=1)

>>> print(y)

[1 2 1 0]

最后，我们

比较一下以

批为单位进

行分类的结

果和实际的

答案。为此，

需

要在NumPy数组之

间使用比较

运算符（==）生成

由True/False构成的布

尔

型数组，并

计算True的个数

。我们通过下

面的例子进

行确认。

>>>

y = np.array([1, 2,

1, 0])

>>> t

= np.array([1, 2, 0,

0])

>>> print(y==t)

[True

True False True]

>>>

np.sum(y==t)

3

至此

，基于批处理

的代码实现

就介绍完了

。使用批处理

，可以实现高

速

且高效的

运算。下一章

介绍神经网

络的学习时

，我们将把图

像数据作为

打包

的批数

据进行学习

，届时也将进

行和这里的

批处理一样

的代码实现

。

A 矩阵的第0维

是列方向，第

1维是行方向

。——译者注

3.7

小结

79

3.7 小结

本章介

绍了神经网

络的前向传

播。本章介绍

的神经网络

和上一章的

感知

机在信

号的按层传

递这一点上

是相同的，但

是，向下一个

神经元发送

信号时，

改变

信号的激活

函数有很大

差异。神经网

络中使用的

是平滑变化

的sigmoid

函数，而感

知机中使用

的是信号急

剧变化的阶

跃函数。这个

差异对于神

经网

络的学

习非常重要

，我们将在下

一章介绍。

本

章所学的内

容

• 神经网络

中的激活函

数使用平滑

变化的sigmoid函数

或ReLU函数。

• 通过

巧妙地使用

NumPy多维数组，可

以高效地实

现神经网络

。

• 机器学习的

问题大体上

可以分为回

归问题和分

类问题。

• 关于

输出层的激

活函数，回归

问题中一般

用恒等函数

，分类问题中

一般用softmax函数

。

• 分类问题中

，输出层的神

经元的数量

设置为要分

类的类别数

。

• 输入数据的

集合称为批

。通过以批为

单位进行推

理处理，能够

实现

高速的

运算。

第4章

神

经网络的学

习

本章的主

题是神经网

络的学习。这

里所说的“学

习”是指从训

练数据中

自

动获取最优

权重参数的

过程。本章中

，为了使神经

网络能进行

学习，将导

入

损失函数这

一指标。而学

习的目的就

是以该损失

函数为基准

，找出能使它

的值达到最

小的权重参

数。为了找出

尽可能小的

损失函数的

值，本章我们

将

介绍利用

了函数斜率

的梯度法。

4.1 从

数据中学习

神经网络的

特征就是可

以从数据中

学习。所谓“从

数据中学习

”，是指

可以由

数据自动决

定权重参数

的值。这是非

常了不起的

事情！因为如

果所有

的参

数都需要人

工决定的话

，工作量就太

大了。在第2章

介绍的感知

机的例

子中

，我们对照着

真值表，人工

设定了参数

的值，但是那

时的参数只

有3个。

而在实

际的神经网

络中，参数的

数量成千上

万，在层数更

深的深度学

习中，

参数的

数量甚至可

以上亿，想要

人工决定这

些参数的值

是不可能的

。本章将

介绍

神经网络的

学习，即利用

数据决定参

数值的方法

，并用Python实现对

MNIST手写数字数

据集的学习

。

对于线性可

分问题，第 2章

的感知机是

可以利用数

据自动学习

的。

根据“感知

机收敛定理

”，通过有限次

数的学习，线

性可分问题

是可

解的。但

是，非线性可

分问题则无

法通过（自动

）学习来解决

。

  第

4章　神经网

络的学习 82

4.1.1

数

据驱动

数据

是机器学习

的命根子。从

数据中寻找

答案、从数据

中发现模式

、根

据数据讲

故事……这些机

器学习所做

的事情，如果

没有数据的

话，就无从谈

起。因此，数据

是机器学习

的核心。这种

数据驱动的

方法，也可以

说脱离了

过

往以人为中

心的方法。

通

常要解决某

个问题，特别

是需要发现

某种模式时

，人们一般会

综合考

虑各

种因素后再

给出回答。“这

个问题好像

有这样的规

律性？”“不对，可

能

原因在别

的地方。”——类似

这样，人们以

自己的经验

和直觉为线

索，通过反

复

试验推进工

作。而机器学

习的方法则

极力避免人

为介入，尝试

从收集到的

数据中发现

答案（模式）。神

经网络或深

度学习则比

以往的机器

学习方法更

能

避免人为

介入。

现在我

们来思考一

个具体的问

题，比如如何

实现数字“5”的

识别。数字

5是

图4-1所示的手

写图像，我们

的目标是实

现能区别是

否是5的程序

。这个

问题看

起来很简单

，大家能想到

什么样的算

法呢？

图4-1　手写

数字5的例子

：写法因人而

异，五花八门

如果让我们

自己来设计

一个能将5正

确分类的程

序，就会意外

地发现这

是

一个很难的

问题。人可以

简单地识别

出5，但却很难

明确说出是

基于何种

规

律而识别出

了5。此外，从图

4-1中也可以看

到，每个人都

有不同的写

字习惯，

要发

现其中的规

律是一件非

常难的工作

。

因此，与其绞

尽脑汁，从零

开始想出一

个可以识别

5的算法，不如

考虑

通过有

效利用数据

来解决这个

问题。一种方

案是，先从图

像中提取特

征量，

4.1

从数据

中学习  83

再用

机器学习技

术学习这些

特征量的模

式。这里所说

的“特征量”是

指可以

从输

入数据（输入

图像）中准确

地提取本质

数据（重要的

数据）的转换

器。图

像的特

征量通常表

示为向量的

形式。在计算

机视觉领域

，常用的特征

量包括

SIFT、SURF和HOG等

。使用这些特

征量将图像

数据转换为

向量，然后对

转换后的向

量使用机器

学习中的SVM、KNN等

分类器进行

学习。

机器学

习的方法中

，由机器从收

集到的数据

中找出规律

性。与从零开

始

想出算法

相比，这种方

法可以更高

效地解决问

题，也能减轻

人的负担。但

是

需要注意

的是，将图像

转换为向量

时使用的特

征量仍是由

人设计的。对

于不

同的问

题，必须使用

合适的特征

量（必须设计

专门的特征

量），才能得到

好的

结果。比

如，为了区分

狗的脸部，人

们需要考虑

与用于识别

5的特征量不

同

的其他特

征量。也就是

说，即使使用

特征量和机

器学习的方

法，也需要针

对

不同的问

题人工考虑

合适的特征

量。

到这里，我

们介绍了两

种针对机器

学习任务的

方法。将这两

种方法用图

来表示，如图

4-2所示。图中还

展示了神经

网络（深度学

习）的方法，可

以看

出该方

法不存在人

为介入。

如图

4-2所示，神经网

络直接学习

图像本身。在

第2个方法，即

利用特

征量

和机器学习

的方法中，特

征量仍是由

人工设计的

，而在神经网

络中，连

图像

中包含的重

要特征量也

都是由机器

来学习的。

图

4-2

从人工设计

规则转变为

由机器从数

据中学习：没

有人为介入

的方块用灰

色表示

人想

到的算法 答

案

答案

答案

人想到的特

征量

（SIFT、HOG等）

神经

网络

（深度学

习）

机器学习

（SVM、KNN等）

  第

4章　神经

网络的学习

84

深 度

学 习 有

时 也

称 为 端

到 端

机 器 学

习（end-to-end machine

learning）。这里所说

的端到端是

指从一端到

另一端的意

思，也就是

从

原始数据（输

入）中获得目

标结果（输出

）的意思。

神经

网络的优点

是对所有的

问题都可以

用同样的流

程来解决。比

如，不

管要求

解的问题是

识别5，还是识

别狗，抑或是

识别人脸，神

经网络都是

通

过不断地

学习所提供

的数据，尝试

发现待求解

的问题的模

式。也就是说

，与

待处理的

问题无关，神

经网络可以

将数据直接

作为原始数

据，进行“端对

端”

的学习。

4.1.2

训

练数据和测

试数据

本章

主要介绍神

经网络的学

习，不过在这

之前，我们先

来介绍一下

机器

学习中

有关数据处

理的一些注

意事项。

机器

学习中，一般

将数据分为

训练数据和

测试数据两

部分来进行

学习和

实验

等。首先，使用

训练数据进

行学习，寻找

最优的参数

；然后，使用测

试

数据评价

训练得到的

模型的实际

能力。为什么

需要将数据

分为训练数

据和测

试数

据呢？因为我

们追求的是

模型的泛化

能力。为了正

确评价模型

的泛化能

力

，就必须划分

训练数据和

测试数据。另

外，训练数据

也可以称为

监督数据。

泛

化能力是指

处理未被观

察过的数据

（不包含在训

练数据中的

数据）的

能力

。获得泛化能

力是机器学

习的最终目

标。比如，在识

别手写数字

的问题

中，泛

化能力可能

会被用在自

动读取明信

片的邮政编

码的系统上

。此时，手

写数

字识别就必

须具备较高

的识别“某个

人”写的字的

能力。注意这

里不是“特

定

的某个人写

的特定的文

字”，而是“任意

一个人写的

任意文字”。如

果系统

只能

正确识别已

有的训练数

据，那有可能

是只学习到

了训练数据

中的个人的

习惯写法。

因

此，仅仅用一

个数据集去

学习和评价

参数，是无法

进行正确评

价的。

这样会

导致可以顺

利地处理某

个数据集，但

无法处理其

他数据集的

情况。顺

便说

一下，只对某

个数据集过

度拟合的状

态称为过拟

合（over fitting）。避免

过拟

合也是机器

学习的一个

重要课题。

4.2

损

失函数  85

4.2

损失

函数

如果有

人问你现在

有多幸福，你

会如何回答

呢？一般的人

可能会给出

诸

如“还可以

吧”或者“不是

那么幸福”等

笼统的回答

。如果有人回

答“我现在

的

幸福指数是

10.23”的话，可能会

把人吓一跳

吧。因为他用

一个数值指

标来

评判自

己的幸福程

度。

这里的幸

福指数只是

打个比方，实

际上神经网

络的学习也

在做同样的

事

情。神经网

络的学习通

过某个指标

表示现在的

状态。然后，以

这个指标为

基

准，寻找最

优权重参数

。和刚刚那位

以幸福指数

为指引寻找

“最优人生”的

人一样，神经

网络以某个

指标为线索

寻找最优权

重参数。神经

网络的学习

中

所用的指

标称为损失

函数（loss function）。这个损

失函数可以

使用任意函

数，

但一般用

均方误差和

交叉熵误差

等。

损失函数

是表示神经

网络性能的

“恶劣程度”的

指标，即当前

的

神经网络

对监督数据

在多大程度

上不拟合，在

多大程度上

不一致。

以“性

能的恶劣程

度”为指标可

能会使人感

到不太自然

，但是如

果给

损失函数乘

上一个负值

，就可以解释

为“在多大程

度上不坏”，

即

“性能有多好

”。并且，“使性能

的恶劣程度

达到最小”和

“使性

能的优

良程度达到

最大”是等价

的，不管是用

“恶劣程度”还

是“优

良程度

”，做的事情本

质上都是一

样的。

4.2.1　均方误

差

可以用作

损失函数的

函数有很多

，其中最有名

的是均方误

差（mean squared 

error）。均方误差

如下式所示

。

（4.1）

这里，yk是表示

神经网络的

输出，tk表示监

督数据，k表示

数据的维数

。

第 4章　神经网

络的学习 86

比

如，在3.6节手写

数字识别的

例子中，yk、tk是由

如下10个元素

构成的数据

。

>>> y =

[0.1, 0.05, 0.6, 0.0,

0.05, 0.1, 0.0, 0.1,

0.0, 0.0]

>>> t

= [0, 0, 1,

0, 0, 0, 0,

0, 0, 0]

数组元素的

索引从第一

个开始依次

对应数字“0”“1”“2”……

这

里，神

经网络

的输出y是softmax函

数的输出。由

于softmax函数的输

出可以理解

为

概率，因此

上例表示“0”的

概率是0.1，“1”的概

率是0.05，“2”的概率

是0.6

等。t是监督

数据，将正确

解标签设为

1，其他均设为

0。这里，标签“2”为

1，

表示正确解

是“2”。将正确解

标签表示为

1，其他标签表

示为0的表示

方法称

为one-hot表

示。

如式（4.1）所示

，均方误差会

计算神经网

络的输出和

正确解监督

数据的

各个

元素之差的

平方，再求总

和。现在，我们

用Python来实现这

个均方误差

，

实现方式如

下所示。

def mean_squared_error(y, t):

return 0.5 * np.sum((y-t)**2)

这里

，参数y和t是NumPy数

组。代码实现

完全遵照式

（4.1），因此不

再具

体说明。现在

，我们使用这

个函数，来实

际地计算一

下。

>>> #

设“2”为正确

解

>>> t =

[0, 0, 1, 0,

0, 0, 0, 0,

0, 0]

>>>

>>>

# 例1：“2”的概率

最高的情况

（0.6）

>>> y

= [0.1, 0.05, 0.6,

0.0, 0.05, 0.1, 0.0,

0.1, 0.0, 0.0]

>>>

mean_squared_error(np.array(y), np.array(t))

0.097500000000000031

>>>

>>> # 例2：“7”的概率最

高的情况（0.6）

>>>

y = [0.1, 0.05,

0.1, 0.0, 0.05, 0.1,

0.0, 0.6, 0.0, 0.0]

>>> mean_squared_error(np.array(y), np.array(t))

0.59750000000000003

这

里举了两个

例子。第一个

例子中，正确

解是“2”，神经网

络的输出的

最大

值是“2”；第

二个例子中

，正确解是“2”，神

经网络的输

出的最大值

是“7”。如

实验结

果所示，我们

发现第一个

例子的损失

函数的值更

小，和监督数

据之间的

误

差较小。也就

是说，均方误

差显示第一

个例子的输

出结果与监

督数据更加

吻合。

4.2 损失函

数  87

4.2.2　交叉熵误

差

除了均方

误差之外，交

叉熵误差（cross entropy

error）也

经常被用作

损

失函数。交

叉熵误差如

下式所示。

（4.2）

这

里，log表示以e为

底数的自然

对数（log

e

）。yk是神经

网络的输出

，tk是

正确解标

签。并且，tk中只

有正确解标

签的索引为

1，其他均为0（one-hot表

示）。

因此，式（4.2）实

际上只计算

对应正确解

标签的输出

的自然对数

。比如，假设

正

确解标签的

索引是“2”，与之

对应的神经

网络的输出

是0.6，则交叉熵

误差

是−log 0.6 =

0.51；若“2”对

应的输出是

0.1，则交叉熵误

差为−log 0.1 = 2.30。

也就是

说，交叉熵误

差的值是由

正确解标签

所对应的输

出结果决定

的。

自然对数

的图像如图

4-3所示。

图4-3　自然

对数y

= log x的图像



第 4章　神经网

络的学习 88

如

图4-3所示，x等于

1时，y为0；随着x向

0靠近，y逐渐变

小。因此，

正确

解标签对应

的输出越大

，式（4.2）的值越接

近0；当输出为

1时，交叉熵

误

差为0。此外，如

果正确解标

签对应的输

出较小，则式

（4.2）的值较大。

下

面，我们来用

代码实现交

叉熵误差。

def cross_entropy_error(y, t):

delta = 1e-7

return -np.sum(t * np.log(y

+ delta))

这

里，参数y和t是

NumPy数组。函数内

部在计算np.log时

，加上了一

个

微小值delta。这是

因为，当出现

np.log(0)时，np.log(0)会变为负

无限大

的-inf，这

样一来就会

导致后续计

算无法进行

。作为保护性

对策，添加一

个

微小值可

以防止负无

限大的发生

。下面，我们使

用cross_entropy_error(y, t)

进行一些

简单的计算

。

>>> t = [0,

0, 1, 0, 0,

0, 0, 0, 0,

0]

>>> y =

[0.1, 0.05, 0.6, 0.0,

0.05, 0.1, 0.0, 0.1,

0.0, 0.0]

>>> cross_entropy_error(np.array(y),

np.array(t))

0.51082545709933802

>>>

>>>

y = [0.1, 0.05,

0.1, 0.0, 0.05, 0.1,

0.0, 0.6, 0.0, 0.0]

>>> cross_entropy_error(np.array(y), np.array(t))

2.3025840929945458

第一个例子

中，正确解标

签对应的输

出为0.6，此时的

交叉熵误差

大约

为0.51。第二

个例子中，正

确解标签对

应的输出为

0.1的低值，此时

的交叉

熵误

差大约为2.3。由

此可以看出

，这些结果与

我们前面讨

论的内容是

一致的。

4.2.3

mini-batch学习

机器学习使

用训练数据

进行学习。使

用训练数据

进行学习，严

格来说，

就是

针对训练数

据计算损失

函数的值，找

出使该值尽

可能小的参

数。因此，

计算

损失函数时

必须将所有

的训练数据

作为对象。也

就是说，如果

训练数据

有

100个的话，我们

就要把这100个

损失函数的

总和作为学

习的指标。

前

面介绍的损

失函数的例

子中考虑的

都是针对单

个数据的损

失函数。如

4.2 损

失函数

89

果要

求所有训练

数据的损失

函数的总和

，以交叉熵误

差为例，可以

写成下面

的

式（4.3）。

（4.3）

这里,假设

数据有N个，tnk表

示第n个数据

的第k个元素

的值（ynk是神

经

网络的输出

，tnk是监督数据

）。式子虽然看

起来有一些

复杂，其实只

是把

求单个

数据的损失

函数的式（4.2）扩

大到了N份数

据，不过最后

还要除以N

进

行正规化。通

过除以N，可以

求单个数据

的“平均损失

函数”。通过这

样的

平均化

，可以获得和

训练数据的

数量无关的

统一指标。比

如，即便训练

数据

有1000个或

10000个，也可以求

得单个数据

的平均损失

函数。

另外，MNIST数

据集的训练

数据有60000个，如

果以全部数

据为对象

求

损失函数的

和，则计算过

程需要花费

较长的时间

。再者，如果遇

到大数据，

数

据量会有几

百万、几千万

之多，这种情

况下以全部

数据为对象

计算损失函

数是不现实

的。因此，我们

从全部数据

中选出一部

分，作为全部

数据的“近

似

”。神经网络的

学习也是从

训练数据中

选出一批数

据（称为mini-batch,小

批

量），然后对每

个mini-batch进行学习

。比如，从60000个训

练数据中随

机

选择100笔，再

用这100笔数据

进行学习。这

种学习方式

称为mini-batch学习。

下

面我们来编

写从训练数

据中随机选

择指定个数

的数据的代

码，以进行

mini-batch学

习。在这之前

，先来看一下

用于读入MNIST数

据集的代码

。

import sys,

os

sys.path.append(os.pardir)

import numpy

as np

from dataset.mnist

import load_mnist

(x_train, t_train),

(x_test, t_test) = \

load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) #

(60000, 784)

print(t_train.shape) #

(60000, 10)

第3章介绍过

，load_mnist函数是用于

读入MNIST数据集

的函数。这个

函数在本书

提供的脚本

dataset/mnist.py中，它会读入

训练数据和

测试数据。

第

4章　神经网络

的学习 90

读入

数据时，通过

设定参数one_hot_label=True，可

以得到one-hot表示

（即

仅正确解

标签为1，其余

为0的数据结

构）。

读入上面

的MNIST数据后，训

练数据有60000个

，输入数据是

784维

（28 ×

28）的图像数

据，监督数据

是10维的数据

。因此，上面的

x_train、t_

train的形状分别

是(60000, 784)和(60000, 10)。

那么，如

何从这个训

练数据中随

机抽取10笔数

据呢？我们可

以使用

NumPy的np.random.choice()，写

成如下形式

。

train_size =

x_train.shape[0]

batch_size = 10

batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]

t_batch

= t_train[batch_mask]

使用np.random.choice()可以从

指定的数字

中随机选择

想要的数字

。比如，

np.random.choice(60000,

10)会从0到

59999之间随机选

择10个数字。如

下

面的实际

代码所示，我

们可以得到

一个包含被

选数据的索

引的数组。

>>> np.random.choice(60000,

10)

array([ 8013, 14666,

58210, 23832, 52091, 10153,

8107, 19410, 27260,

21411])

之

后，我们只需

指定这些随

机选出的索

引，取出mini-batch，然后

使用

这个mini-batch计

算损失函数

即可。

计算电

视收视率时

，并不会统计

所有家庭的

电视机，而是

仅以那些

被

选中的家庭

为统计对象

。比如，通过从

关东地区随

机选择

1000个

家

庭计算收视

率，可以近似

地求得关东

地区整体的

收视率。这 1000

个

家庭的收视

率，虽然严格

上不等于整

体的收视率

，但可以作为

整

体的一个

近似值。和收

视率一样，mini-batch的

损失函数也

是利用

一部

分样本数据

来近似地计

算整体。也就

是说，用随机

选择的小批

量数据（mini-batch）作为

全体训练数

据的近似值

。

4.2 损失函数

91

4.2.4　mini-batch版

交叉熵误差

的实现

如何

实现对应mini-batch的

交叉熵误差

呢？只要改良

一下之前实

现的对

应单

个数据的交

叉熵误差就

可以了。这里

，我们来实现

一个可以同

时处理单

个

数据和批量

数据（数据作

为batch集中输入

）两种情况的

函数。

def cross_entropy_error(y,

t):

 if y.ndim

== 1:

 t

= t.reshape(1, t.size)

y = y.reshape(1, y.size)

batch_size = y.shape[0]

return -np.sum(t * np.log(y

+ 1e-7)) / batch_size

这里，y是

神经网络的

输出，t是监督

数据。y的维度

为1时，即求单

个

数据的交

叉熵误差时

，需要改变数

据的形状。并

且，当输入为

mini-batch时，

要用batch的个

数进行正规

化，计算单个

数据的平均

交叉熵误差

。

此外，当监督

数据是标签

形式（非one-hot表示

，而是像“2”“7”这样

的

标签）时，交

叉熵误差可

通过如下代

码实现。

def cross_entropy_error(y, t):

if y.ndim == 1:

t = t.reshape(1, t.size)

y = y.reshape(1, y.size)

batch_size = y.shape[0]

return -np.sum(np.log(y[np.arange(batch_size), t] +

1e-7)) / batch_size

实现

的要点是，由

于one-hot表示中t为

0的元素的交

叉熵误差也

为0，因

此针对

这些元素的

计算可以忽

略。换言之，如

果可以获得

神经网络在

正确

解标签

处的输出，就

可以计算交

叉熵误差。因

此，t为one-hot表示时

通过

t *

np.log(y)计算的

地方，在t为标

签形式时，可

用np.log( y[np.arange 

(batch_size),

t] )实现相同

的处理（为了

便于观察，这

里省略了微

小值1e-7）。

作为参

考，简单介绍

一下np.log( y[np.arange(batch_size),

t] )。np.arange 

(batch_size)会生成

一个从0到batch_size-1的

数组。比如当

batch_size为5

时，np.arange(batch_size)会生成

一个NumPy 数组[0, 1, 2,

3, 4]。因

为

第 4章　神经

网络的学习

92

t中标签是以

[2,

7, 0, 9, 4]的形式存储

的，所以y[np.arange(batch_size),

t]能抽

出各个数据

的正确解标

签对应的神

经网络的输

出（在这个例

子中，

y[np.arange(batch_size), t] 会生成

NumPy

数 组 [y[0,2], y[1,7],

y[2,0], 

y[3,9], y[4,4]]）。

4.2.5　为何要

设定损失函

数

上面我们

讨论了损失

函数，可能有

人要问：“为什

么要导入损

失函数呢？”

以

数字识别任

务为例，我们

想获得的是

能提高识别

精度的参数

，特意再导入

一个损失函

数不是有些

重复劳动吗

？也就是说，既

然我们的目

标是获得使

识

别精度尽

可能高的神

经网络，那不

是应该把识

别精度作为

指标吗？

对于

这一疑问，我

们可以根据

“导数”在神经

网络学习中

的作用来回

答。

下一节中

会详细说到

，在神经网络

的学习中，寻

找最优参数

（权重和偏置

）时，

要寻找使

损失函数的

值尽可能小

的参数。为了

找到使损失

函数的值尽

可能小

的地

方，需要计算

参数的导数

（确切地讲是

梯度），然后以

这个导数为

指引，

逐步更

新参数的值

。

假设有一个

神经网络，现

在我们来关

注这个神经

网络中的某

一个权重参

数。此时，对该

权重参数的

损失函数求

导，表示的是

“如果稍微改

变这个权

重

参数的值，损

失函数的值

会如何变化

”。如果导数的

值为负，通过

使该权

重参

数向正方向

改变，可以减

小损失函数

的值；反过来

，如果导数的

值为正，

则通

过使该权重

参数向负方

向改变，可以

减小损失函

数的值。不过

，当导数

的值

为0时，无论权

重参数向哪

个方向变化

，损失函数的

值都不会改

变，此

时该权

重参数的更

新会停在此

处。

之所以不

能用识别精

度作为指标

，是因为这样

一来绝大多

数地方的导

数

都会变为

0，导致参数无

法更新。话说

得有点多了

，我们来总结

一下上面的

内容。

在进行

神经网络的

学习时，不能

将识别精度

作为指标。因

为如果以

识

别精度为指

标，则参数的

导数在绝大

多数地方都

会变为0。

为什

么用识别精

度作为指标

时，参数的导

数在绝大多

数地方都会

变成0

4.2 损失函

数

93

呢？为了回

答这个问题

，我们来思考

另一个具体

例子。假设某

个神经网络

正

确识别出

了100笔训练数

据中的32笔，此

时识别精度

为32 %。如果以识

别精

度为指

标，即使稍微

改变权重参

数的值，识别

精度也仍将

保持在32 %，不会

出现变化。也

就是说，仅仅

微调参数，是

无法改善识

别精度的。即

便识别精

度

有所改善，它

的值也不会

像32.0123 ...

%这样连续

变化，而是变

为33 %、

34 %这样的不

连续的、离散

的值。而如果

把损失函数

作为指标，则

当前损

失函

数的值可以

表示为0.92543 ... 这样

的值。并且，如

果稍微改变

一下参数

的

值，对应的损

失函数也会

像0.93432

... 这样发生

连续性的变

化。

识别精度

对微小的参

数变化基本

上没有什么

反应，即便有

反应，它的值

也是不连续

地、突然地变

化。作为激活

函数的阶跃

函数也有同

样的情况。出

于相同的原

因，如果使用

阶跃函数作

为激活函数

，神经网络的

学习将无法

进行。

如图4-4所

示，阶跃函数

的导数在绝

大多数地方

（除了0以外的

地方）均为0。

也

就是说，如果

使用了阶跃

函数，那么即

便将损失函

数作为指标

，参数的微

小

变化也会被

阶跃函数抹

杀，导致损失

函数的值不

会产生任何

变化。

阶跃函

数就像“竹筒

敲石”一样，只

在某个瞬间

产生变化。而

sigmoid函数，

如图4-4所

示，不仅函数

的输出（竖轴

的值）是连续

变化的，曲线

的斜率（导数

）

也是连续变

化的。也就是

说，sigmoid函数的导

数在任何地

方都不为0。这

对

神经网络

的学习非常

重要。得益于

这个斜率不

会为0的性质

，神经网络的

学

习得以正

确进行。

阶跃

函数

sigmoid函数

图

4-4 阶跃函数和

sigmoid函数：阶跃函

数的斜率在

绝大多数地

方都为0，而sigmoid函

数的斜率（切

线）不会为0

第

4章　神经网络

的学习 94

4.3

数值

微分

梯度法

使用梯度的

信息决定前

进的方向。本

节将介绍梯

度是什么、有

什

么性质等

内容。在这之

前，我们先来

介绍一下导

数。

4.3.1

导数

假如

你是全程马

拉松选手，在

开始的10分钟

内跑了2千米

。如果要计算

此时的奔跑

速度，则为2/10 = 0.2［千

米/分］。也就是

说，你以1分钟

前进0.2

千米的

速度（变化）奔

跑。

在这个马

拉松的例子

中，我们计算

了“奔跑的距

离”相对于“时

间”发生

了多

大变化。不过

，这个10分钟跑

2千米的计算

方式，严格地

讲，计算的是

10分钟内的平

均速度。而导

数表示的是

某个瞬间的

变化量。因此

，将10分

钟这一

时间段尽可

能地缩短，比

如计算前1分

钟奔跑的距

离、前1秒钟奔

跑

的距离、前

0.1秒钟奔跑的

距离……这样就

可以获得某

个瞬间的变

化量（某个

瞬

时速度）。

综上

，导数就是表

示某个瞬间

的变化量。它

可以定义成

下面的式子

。

（4.4）

式（4.4）表示的是

函数的导数

。左边的符号

表示f（x）关于x的

导

数，即f（x）相对

于x的变化程

度。式（4.4）表示的

导数的含义

是，x的“微小

变

化”将导致函

数f（x）的值在多

大程度上发

生变化。其中

，表示微小变

化的

h无限趋

近0，表示为

。

接

下来，我们参

考式（4.4），来实现

求函数的导

数的程序。如

果直接实

现

式（4.4）的话，向h中

赋入一个微

小值，就可以

计算出来了

。比如，下面

的

实现如何？

# 不

好的实现示

例

def numerical_diff(f,

x):

4.3 数值微分

95

h = 10e-50

return

(f(x+h) - f(x)) /

h

函数numerical_diff(f, x)的名称

来源于数值

微分A 的英文

numerical

differentiation。这个函数有

两个参数，即

“函数f”和“传给

函数f的参数

x”。

乍一看这个

实现没有问

题，但是实际

上这段代码

有两处需要

改进的地方

。

在上面的实

现中，因为想

把尽可能小

的值赋给h（可

以话，想让h无

限接

近0），所以

h使用了10e-50（有50个

连续的0的“0.00

... 1”）这

个微小值。但

是，这样反而

产生了舍入

误差（rounding error）。所谓舍

入误差，是指

因省

略小数

的精细部分

的数值（比如

，小数点第8位

以后的数值

）而造成最终

的计

算结果

上的误差。比

如，在Python中，舍入

误差可如下

表示。

>>> np.float32(1e-50)

0.0

如上所

示，如果用float32类

型（32位的浮点

数）来表示1e-50，就

会变成

0.0，无法

正确表示出

来。也就是说

，使用过小的

值会造成计

算机出现计

算

上的问题

。这是第一个

需要改进的

地方，即将微

小值h改为10−4

。使

用10−4

就可以得

到正确的结

果。

第二个需

要改进的地

方与函数f的

差分有关。虽

然上述实现

中计算了函

数f在x+h和x之间

的差分，但是

必须注意到

，这个计算从

一开始就有

误差。

如图4-5所

示，“真的导数

”对应函数在

x处的斜率（称

为切线），但上

述实现

中计

算的导数对

应的是(x

+ h)和x之

间的斜率。因

此，真的导数

（真的切线）

和

上述实现中

得到的导数

的值在严格

意义上并不

一致。这个差

异的出现是

因

为h不可能

无限接近0。

如

图4-5所示，数值

微分含有误

差。为了减小

这个误差，我

们可以计算

函数f在(x + h)和(x −

h)之

间的差分。因

为这种计算

方法以x为中

心，计

算它左

右两边的差

分，所以也称

为中心差分

（而(x + h)和x之间的

差分称为

前

向差分）。下面

，我们基于上

述两个要改

进的点来实

现数值微分

（数值梯度）。

A 所

谓数值微分

就是用数值

方法近似求

解函数的导

数的过程。——译

者注

第 4章　神

经网络的学

习 96

图4-5　真的导

数（真的切线

）和数值微分

（近似切线）的

值不同

真的

切线

近似切

线

y = f(x)

x

x + h

def

numerical_diff(f, x):

 h

= 1e-4 # 0.0001

return (f(x+h) - f(x-h))

/ (2*h)

如上所示

，利用微小的

差分求导数

的过程称为

数值微分（numerical

differentiation）。而

基于数学式

的推导求导

数的过程，则

用“解析

性”（analytic）一

词，称为“解析

性求解”或者

“解析性求导

”。比如，

y =

x2

的导数

，可以通过 解

析性地求解

出来。因此，当

x =

2时，

y的导数为

4。解析性求导

得到的导数

是不含误差

的“真的导数

”。

4.3.2　数值微分的

例子

现在我

们试着用上

述的数值微

分对简单函

数进行求导

。先来看一个

由下

式表示

的2次函数。

y =

0.01x2 + 0.1x （4.5）

4.3 数

值微分  97

用Python来

实现式（4.5），如下

所示。

def function_1(x):

return 0.01*x**2 + 0.1*x

接下来

，我们来绘制

这个函数的

图像。画图所

用的代码如

下，生成的图

像如图4-6所示

。

import numpy as

np

import matplotlib.pylab as

plt

x = np.arange(0.0,

20.0, 0.1) # 以0.1为单位，从

0到20的数组x

y = function_1(x)

plt.xlabel("x")

plt.ylabel("f(x)")

plt.plot(x, y)

plt.show()

图

4-6 f(x) = 0.01x2

+ 0.1x的图像

我们

来计算一下

这个函数在

x =

5和x = 10处的导数

。

第 4章　神经网

络的学习 98

>>> numerical_diff(function_1, 5)

0.1999999999990898

>>> numerical_diff(function_1, 10)

0.2999999999986347

这

里计算的导

数是f(x)相对于

x的变化量，对

应函数的斜

率。另外，

f(x) = 0.01x2

+ 0.1x 的解

析解是 。因

此

，在 x = 5

和

x = 10处，“真的

导数”分别为

0.2和0.3。和上面的

结果相比，我

们发现虽然

严格意义上

它们并不一

致，但误差非

常小。实际上

，误差小到基

本上可以认

为它们是相

等的。

现在，我

们用上面的

数值微分的

值作为斜率

，画一条直线

。结果如图4-7

所

示，可以确认

这些直线确

实对应函数

的切线（源代

码在ch04/gradient_1d.

py中）。

图4-7

x = 5、x =

10处

的切线：直线

的斜率使用

数值微分的

值

4.3.3　偏导数

接

下来，我们看

一下式(4.6)表示

的函数。虽然

它只是一个

计算参数的

平方和的简

单函数，但是

请注意和上

例不同的是

，这里有两个

变量。

（4.6）

这个式

子可以用Python来

实现，如下所

示。

def function_2(x):

f (x )

f

(x )

4.3

数值微分

99

 return

x[0]**2 + x[1]**2

# 或者return np.sum(x**2)

这里，我

们假定向参

数输入了一

个NumPy数组。函数

的内部实现

比较

简单，先

计算NumPy数组中

各个元素的

平方，再求它

们的和（np.sum(x**2)

也可

以实现同样

的处理）。我们

来画一下这

个函数的图

像。结果如图

4-8所示，

是一个

三维图像。

图

4-8

的图像

0

−3 −2

−1 0 1 2

3

f(x)

x1

x0

现在

我们来求式

（4.6）的导数。这里

需要注意的

是，式（4.6）有两个

变量，

所以有

必要区分对

哪个变量求

导数，即对x0和

x1两个变量中

的哪一个求

导数。

另外，我

们把这里讨

论的有多个

变量的函数

的导数称为

偏导数。用数

学式表

示的

话，可以写成

、

。

怎么求偏导

数呢？我们先

试着解一下

下面两个关

于偏导数的

问题。

第 4章　神

经网络的学

习 100

问题1：求x0 = 3, x1

= 4时

，关于x0的偏导

数 。

>>>

def function_tmp1(x0):

... return

x0*x0 + 4.0**2.0

...

>>> numerical_diff(function_tmp1, 3.0)

6.00000000000378

问题2：求x0 = 3, x1

= 4时

，关于x1的偏导

数 。

>>>

def function_tmp2(x1):

... return

3.0**2.0 + x1*x1

...

>>> numerical_diff(function_tmp2, 4.0)

7.999999999999119

在这些问

题中，我们定

义了一个只

有一个变量

的函数，并对

这个函数进

行了求导。例

如，问题1中，我

们定义了一

个固定x1 = 4的新

函数，然后对

只有变量x0的

函数应用了

求数值微分

的函数。从上

面的计算结

果可知，问题

1的答案是6.00000000000378，问

题2的答案是

7.999999999999119，和解析

解的

导数基本一

致。

像这样，偏

导数和单变

量的导数一

样，都是求某

个地方的斜

率。不过，

偏导

数需要将多

个变量中的

某一个变量

定为目标变

量，并将其他

变量固定为

某个值。在上

例的代码中

，为了将目标

变量以外的

变量固定到

某些特定的

值

上，我们定

义了新函数

。然后，对新定

义的函数应

用了之前的

求数值微分

的

函数，得到

偏导数。

4.4 梯度

在刚才的例

子中，我们按

变量分别计

算了x0和x1的偏

导数。现在，我

们希望一起

计算x0和x1的偏

导数。比如，我

们来考虑求

x0 = 3,

x1 = 4时(x0, x1)

的偏导数

。另外，像 这样

的由全部变

量的偏导数

汇总

而成的

向量称为梯

度（gradient）。梯度可以

像下面这样

来实现。

4.4

梯度

101

def numerical_gradient(f,

x):

 h =

1e-4 # 0.0001

grad = np.zeros_like(x) #

生成和x形状

相同的数组

for idx in

range(x.size):

 tmp_val =

x[idx]

 # f(x+h)的计算

x[idx] = tmp_val +

h

 fxh1 =

f(x)

 # f(x-h)的计

算

x[idx] = tmp_val -

h

 fxh2 =

f(x)

 grad[idx] =

(fxh1 - fxh2) /

(2*h)

 x[idx] =

tmp_val # 还原值

return grad

函

数numerical_gradient(f, x)的实现看

上去有些复

杂，但它执行

的处

理和求

单变量的数

值微分基本

没有区别。需

要补充说明

一下的是，np.zeros_

like(x)会

生成一个形

状和x相同、所

有元素都为

0的数组。

函数

numerical_gradient(f, x)中，参数f为函

数，x为NumPy数组，该

函数对NumPy数组

x的各个元素

求数值微分

。现在，我们用

这个函数实

际

计算一下

梯度。这里我

们求点(3, 4)、(0, 2)、(3, 0)处的

梯度。

>>> numerical_gradient(function_2, np.array([3.0, 4.0]))

array([ 6., 8.])A

>>>

numerical_gradient(function_2, np.array([0.0, 2.0]))

array([

0., 4.])

>>> numerical_gradient(function_2,

np.array([3.0, 0.0]))

array([ 6.,

0.])

像这样

，我们可以计

算(x0, x1)在各点处

的梯度。上例

中，点(3, 4)处的

梯

度是(6, 8)、点(0, 2)处的

梯度是(0, 4)、点(3,

0)处

的梯度是(6, 0)。这

个

梯度意味

着什么呢？为

了更好地理

解，我们把 的

梯度

画在图

上。不过，这里

我们画的是

元素值为负

梯度B 的向量

（源代码在ch04/

gradient_2d.py中

）。

A

实际上，虽然

求到的值是

[6.0000000000037801, 7.9999999999991189]，但实际输出

的是[6., 8.]。

这是因

为在输出NumPy数

组时，数值会

被改成“易读

”的形式。

B 后面

我们将会看

到，负梯度方

向是梯度法

中变量的更

新方向。——译者

注

第 4章　神经

网络的学习

102

如图4-9所示，

的

梯度呈现为

有向向量（箭

头）。观

察图4-9，我

们发现梯度

指向函数f(x0,x1)的

“最低处”（最小

值），就像指南

针

一样，所有

的箭头都指

向同一点。其

次，我们发现

离“最低处”越

远，箭头越大

。

图4-9

的梯度

虽

然图 4-9 中的梯

度指向了最

低处，但并非

任何时候都

这样。实际上

，

梯度会指向

各点处的函

数值降低的

方向。更严格

地讲，梯度指

示的方向

是

各点处的函

数值减小最

多的方向 A。这

是一个非常

重要的性质

，请一定

牢记

！

4.4.1　梯度法

机器

学习的主要

任务是在学

习时寻找最

优参数。同样

地，神经网络

也必

须在学

习时找到最

优参数（权重

和偏置）。这里

所说的最优

参数是指损

失函数

A 高等

数学告诉我

们，方向导数

= cos(θ) ×

梯度（θ是方向

导数的方向

与梯度方向

的夹角）。因此

，所

有的下降

方向中，梯度

方向下降最

多。——译者注

4.4 梯

度

103

取最小值

时的参数。但

是，一般而言

，损失函数很

复杂，参数空

间庞大，我

们

不知道它在

何处能取得

最小值。而通

过巧妙地使

用梯度来寻

找函数最小

值

（或者尽可

能小的值）的

方法就是梯

度法。

这里需

要注意的是

，梯度表示的

是各点处的

函数值减小

最多的方向

。因此，

无法保

证梯度所指

的方向就是

函数的最小

值或者真正

应该前进的

方向。实际

上

，在复杂的函

数中，梯度指

示的方向基

本上都不是

函数值最小

处。

函数的极

小值、最小值

以及被称为

鞍点（saddle

point）的地方

，

梯度为 0。极小

值是局部最

小值，也就是

限定在某个

范围内的最

小值。鞍点是

从某个方向

上看是极大

值，从另一个

方向上看则

是

极小值的

点。虽然梯度

法是要寻找

梯度为

0的地

方，但是那个

地

方不一定

就是最小值

（也有可能是

极小值或者

鞍点）。此外，当

函

数很复杂

且呈扁平状

时，学习可能

会进入一个

（几乎）平坦的

地区，

陷入被

称为“学习高

原”的无法前

进的停滞期

。

虽然梯度的

方向并不一

定指向最小

值，但沿着它

的方向能够

最大限度地

减小函数的

值。因此，在寻

找函数的最

小值（或者尽

可能小的值

）的位置的

任

务中，要以梯

度的信息为

线索，决定前

进的方向。

此

时梯度法就

派上用场了

。在梯度法中

，函数的取值

从当前位置

沿着梯

度方

向前进一定

距离，然后在

新的地方重

新求梯度，再

沿着新梯度

方向前进，

如

此反复，不断

地沿梯度方

向前进。像这

样，通过不断

地沿梯度方

向前进，

逐渐

减小函数值

的过程就是

梯度法（gradient method）。梯度

法是解决机

器

学习中最

优化问题的

常用方法，特

别是在神经

网络的学习

中经常被使

用。

根据目的

是寻找最小

值还是最大

值，梯度法的

叫法有所不

同。严格地讲

，

寻找最小值

的梯度法称

为梯度下降

法（gradient descent method），

寻找最大

值的梯度法

称为梯度上

升法（gradient ascent method）。但

是通

过反转损失

函数的符号

，求最小值的

问题和求最

大值的问题

会

变成相同

的问题，因此

“下降”还是“上

升”的差异本

质上并不重

要。

一般来说

，神经网络（深

度学习）中，梯

度法主要是

指梯度下降

法。

第 4章　神经

网络的学习

104

现在，我们尝

试用数学式

来表示梯度

法，如式（4.7）所示

。

（4.7）

式（4.7）的η表示更

新量，在神经

网络的学习

中，称为学习

率（learning 

rate）。学习率决

定在一次学

习中，应该学

习多少，以及

在多大程度

上更新参数

。

式（4.7）是表示更

新一次的式

子，这个步骤

会反复执行

。也就是说，每

一步都按式

（4.7）更新变量的

值，通过反复

执行此步骤

，逐渐减小函

数值。

虽然这

里只展示了

有两个变量

时的更新过

程，但是即便

增加变量的

数量，也

可以

通过类似的

式子（各个变

量的偏导数

）进行更新。

学

习率需要事

先确定为某

个值，比如0.01或

0.001。一般而言，这

个值

过大或

过小，都无法

抵达一个“好

的位置”。在神

经网络的学

习中，一般会

一边改变学

习率的值，一

边确认学习

是否正确进

行了。

下面，我

们用Python来实现

梯度下降法

。如下所示，这

个实现很简

单。

def gradient_descent(f,

init_x, lr=0.01, step_num=100):

x = init_x

for i in range(step_num):

grad = numerical_gradient(f, x)

x -= lr *

grad

 return x

参数f是要

进行最优化

的函数，init_x是初

始值，lr是学习

率learning 

rate，step_num是梯度法

的重复次数

。numerical_gradient(f,x)会求函数的

梯度，用该梯

度乘以学习

率得到的值

进行更新操

作，由step_num指定重

复的

次数。

使

用这个函数

可以求函数

的极小值，顺

利的话，还可

以求函数的

最小值。

下面

，我们就来尝

试解决下面

这个问题。

4.4 梯

度

105

问题：请用

梯度法求 的

最小值。

>>>

def function_2(x):

... return

x[0]**2 + x[1]**2

...

>>> init_x = np.array([-3.0,

4.0])

>>> gradient_descent(function_2, init_x=init_x,

lr=0.1, step_num=100)

array([ -6.11110793e-10,

8.14814391e-10])

这里

，设初始值为

(-3.0, 4.0)，开始使用梯

度法寻找最

小值。最终的

结

果是(-6.1e-10,

8.1e-10)，非常

接近(0，0)。实际上

，真的最小值

就是(0，0)，

所以说

通过梯度法

我们基本得

到了正确结

果。如果用图

来表示梯度

法的更新

过

程，则如图4-10所

示。可以发现

，原点处是最

低的地方，函

数的取值一

点点在向其

靠近。这个图

的源代码在

ch04/gradient_method.py 中（但ch04/

gradient_method.py不显示

表示等高线

的虚线）。

图4-10 的

梯度法的更

新过程：虚线

是函数的等

高线

第 4章　神

经网络的学

习 106

前面说过

，学习率过大

或者过小都

无法得到好

的结果。我们

来做个实验

验证一下。

# 学

习率过大的

例子：lr=10.0

>>>

init_x = np.array([-3.0, 4.0])

>>> gradient_descent(function_2, init_x=init_x, lr=10.0,

step_num=100)

array([ -2.58983747e+13, -1.29524862e+12])

# 学习率

过小的例子

：lr=1e-10

>>> init_x

= np.array([-3.0, 4.0])

>>>

gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)

array([-2.99999994, 3.99999992])

实验结果表

明，学习率过

大的话，会发

散成一个很

大的值；反过

来，学

习率过

小的话，基本

上没怎么更

新就结束了

。也就是说，设

定合适的学

习率

是一个

很重要的问

题。

像学习率

这样的参数

称为超参数

。这是一种和

神经网络的

参数（权重

和

偏置）性质不

同的参数。相

对于神经网

络的权重参

数是通过训

练

数据和学

习算法自动

获得的，学习

率这样的超

参数则是人

工设定的。

一

般来说，超参

数需要尝试

多个值，以便

找到一种可

以使学习顺

利

进行的设

定。

4.4.2　神经网络

的梯度

神经

网络的学习

也要求梯度

。这里所说的

梯度是指损

失函数关于

权重参

数的

梯度。比如，有

一个只有一

个形状为2 × 3的

权重W的神经

网络，损失

函

数用L表示。此

时，梯度可以

用 表示。用数

学式表示的

话，如下所示

。

（4.8）

的元素由各

个元素关于

W的偏导数构

成。比如，第1行

第1列的元

4.4 梯

度  107

素 表示当

w11稍微变化时

，损失函数L会

发生多大变

化。这里的重

点是，

的形状

和W相同。实际

上，式（4.8）中的W和

都是2 ×

3的形状

。

下面，我们以

一个简单的

神经网络为

例，来实现求

梯度的代码

。为此，

我们要

实现一个名

为simpleNet的类（源代

码在ch04/gradient_simplenet.py

中）。

import sys, os

sys.path.append(os.pardir)

import numpy as np

from common.functions import softmax,

cross_entropy_error

from common.gradient import

numerical_gradient

class simpleNet:

def __init__(self):

 self.W

= np.random.randn(2,3) # 用高

斯分布进行

初始化

def predict(self, x):

return np.dot(x, self.W)

def loss(self, x, t):

z = self.predict(x)

y = softmax(z)

loss = cross_entropy_error(y, t)

return loss

这里

使用了 common/functions.py中的

softmax和

cross_entropy_error方

法，以及

common/gradient.py中的numerical_gradient方法。simpleNet类

只有

一个实

例变量，即形

状为2×3的权重

参数。它有两

个方法，一个

是用于预

测

的predict(x)，另一个是

用于求损失

函数值的loss(x,t)。这

里参数x接收

输入数据，t接

收正确解标

签。现在我们

来试着用一

下这个simpleNet。

>>> net = simpleNet()

>>> print(net.W) # 权重

参数

[[ 0.47355232 0.9977393 0.84668094],

[ 0.85557411 0.03563661 0.69422093]])

>>>

>>> x =

np.array([0.6, 0.9])

>>> p

= net.predict(x)

>>> print(p)

第 4章　神

经网络的学

习

108

[ 1.05414809 0.63071653

1.1328074]

>>> np.argmax(p) #

最大值的

索引

2

>>>

>>>

t = np.array([0, 0,

1]) # 正确解

标签

>>>

net.loss(x, t)

0.92806853663411326

接下来

求梯度。和前

面一样，我们

使用numerical_gradient(f,

x)求梯

度

（这里定义的

函数f(W)的参数

W是一个伪参

数。因为numerical_gradient(f, 

x)会在

内部执行f(x),为

了与之兼容

而定义了f(W)）。

>>> def f(W):

...

return net.loss(x, t)

...

>>> dW = numerical_gradient(f,

net.W)

>>> print(dW)

[[

0.21924763 0.14356247 -0.36281009]

[ 0.32887144 0.2153437 -0.54421514]]

numerical_gradient(f, x) 的

参数f是函数

，x是传给函数

f的参数。因此

，

这里参数x取

net.W，并定义一个

计算损失函

数的新函数

f，然后把这个

新定

义的函

数传递给numerical_gradient(f, x)。

numerical_gradient(f, net.W)的

结果是dW，一个

形状为2

× 3的二

维数组。

观察

一下dW的内容

，例如，会发现

中的 的值大

约是0.2，这表示

如

果将w11增加

h，那么损失函

数的值会增

加0.2h。再如， 对应

的值大约

是

−0.5，这表示如果

将w23增加h，损失

函数的值将

减小0.5h。因此，从

减

小损失函

数值的观点

来看，w23应向正

方向更新，w11应

向负方向更

新。至于

更新

的程度，w23比w11的

贡献要大。

另

外，在上面的

代码中，定义

新函数时使

用了“def f(x):···”的形式

。

实际上，Python中如

果定义的是

简单的函数

，可以使用lambda表

示法。使

用lambda的

情况下，上述

代码可以如

下实现。

>>> f =

lambda w: net.loss(x, t)

>>> dW = numerical_gradient(f,

net.W)

4.5 学习

算法的实现

109

求出神经网

络的梯度后

，接下来只需

根据梯度法

，更新权重参

数即可。

在下

一节中，我们

会以2层神经

网络为例，实

现整个学习

过程。

为了对

应形状为多

维数组的权

重参数W，这里

使用的 numerical_

gradient()和之

前的实现稍

有不同。不过

，改动只是为

了对应多维

数组，所以改

动并不大。这

里省略了对

代码的说明

，想知道细节

的

读者请参

考源代码（common/gradient.py）。

4.5 学

习算法的实

现

关于神经

网络学习的

基础知识，到

这里就全部

介绍完了。“损

失函

数”“mini-batch”“梯度

”“梯度下降法

”等关键词已

经陆续登场

，这里我们

来

确认一下神

经网络的学

习步骤，顺便

复习一下这

些内容。神经

网络的学习

步骤如下所

示。

前提

神经

网络存在合

适的权重和

偏置，调整权

重和偏置以

便拟合训练

数据的

过程

称为“学习”。神

经网络的学

习分成下面

4个步骤。

步骤

1（mini-batch）

从训练数据

中随机选出

一部分数据

，这部分数据

称为mini-batch。我们

的

目标是减小

mini-batch的损失函数

的值。

步骤2（计

算梯度）

为了

减小mini-batch的损失

函数的值，需

要求出各个

权重参数的

梯度。

梯度表

示损失函数

的值减小最

多的方向。

步

骤3（更新参数

）

将权重参数

沿梯度方向

进行微小更

新。

第 4章　神经

网络的学习

110

步骤4（重复）

重

复步骤1、步骤

2、步骤3。

神经网

络的学习按

照上面4个步

骤进行。这个

方法通过梯

度下降法更

新

参数，不过

因为这里使

用的数据是

随机选择的

mini batch数据，所以又

称为

随机梯

度下降法（stochastic gradient descent）。“随

机”指的是“随

机选择的”

的

意思，因此，随

机梯度下降

法是“对随机

选择的数据

进行的梯度

下降法”。

深度

学习的很多

框架中，随机

梯度下降法

一般由一个

名为SGD的函数

来实现。

SGD来源

于随机梯度

下降法的英

文名称的首

字母。

下面，我

们来实现手

写数字识别

的神经网络

。这里以2层神

经网络（隐

藏

层为1层的网

络）为对象，使

用MNIST数据集进

行学习。

4.5.1　2层神

经网络的类

首先，我们将

这个2层神经

网络实现为

一个名为TwoLayerNet的

类，实现

过程

如下所示A 。源

代码在ch04/two_layer_net.py中。

import sys, os

sys.path.append(os.pardir)

from common.functions import *

from common.gradient import numerical_gradient

class TwoLayerNet:

 def

__init__(self, input_size, hidden_size, output_size,

weight_init_std=0.01):

 # 初

始化权重

self.params = {}

self.params['W1'] = weight_init_std *

\

 np.random.randn(input_size, hidden_size)

self.params['b1'] = np.zeros(hidden_size)

self.params['W2'] = weight_init_std *

\

 np.random.randn(hidden_size, output_size)

self.params['b2'] = np.zeros(output_size)

A

TwoLayerNet的

实现参考了

斯坦福大学

CS231n课程提供的

Python源代码。

4.5 学习

算法的实现

111

def predict(self, x):

W1, W2 = self.params['W1'],

self.params['W2']

 b1, b2

= self.params['b1'], self.params['b2']

a1 = np.dot(x, W1)

+ b1

 z1

= sigmoid(a1)

 a2

= np.dot(z1, W2) +

b2

 y =

softmax(a2)

 return y

# x:输入数据, t:监

督数据

def loss(self, x, t):

y = self.predict(x)

return cross_entropy_error(y, t)

def accuracy(self, x, t):

y = self.predict(x)

y = np.argmax(y, axis=1)

t = np.argmax(t, axis=1)

accuracy = np.sum(y ==

t) / float(x.shape[0])

return accuracy

 #

x:输入

数据, t:监督数

据

 def

numerical_gradient(self, x, t):

loss_W = lambda W:

self.loss(x, t)

 grads

= {}

 grads['W1']

= numerical_gradient(loss_W, self.params['W1'])

grads['b1'] = numerical_gradient(loss_W, self.params['b1'])

grads['W2'] = numerical_gradient(loss_W, self.params['W2'])

grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

return grads

虽然这个

类的实现稍

微有点长，但

是因为和上

一章的神经

网络的前向

处

理的实现

有许多共通

之处，所以并

没有太多新

东西。我们先

把这个类中

用到

的变量

和方法整理

一下。表4-1中只

罗列了重要

的变量，表4-2中

则罗列了所

有的方法。

  第

4章

神经网络

的学习 112

表 4-1

TwolayerNet类

中使用的变

量

变量 说明

params 保存神经网

络的参数的

字典型变量

（实例变量）。

params['W1']是

第1层的权重

，params['b1']是第1层的偏

置。

params['W2']是第2层的

权重，params['b2']是第2层

的偏置

grads 保存

梯度的字典

型变量（numerical_gradient()方法

的返回值）。

grads['W1']是

第1层权重的

梯度，grads['b1']是第1层

偏置的梯度

。

grads['W2']是第2层权重

的梯度，grads['b2']是第

2层偏置的梯

度

表 4-2

TwoLayerNet类的方

法

方法 说明

__init__(self, input_size,

hidden_size, output_size)

进行初始化

。

参数从头开

始依次表示

输入层的神

经元数、隐藏

层

的神经元

数、输出层的

神经元数

predict(self, x) 进

行识别（推理

）。

参数x是图像

数据

loss(self, x, t)

计算损

失函数的值

。

参数x是图像

数据，t是正确

解标签（后面

3个方法

的参

数也一样）

accuracy(self,

x, t) 计

算识别精度

numerical_gradient(self, x,

t) 计算权重参

数的梯度

gradient(self, x,

t) 计

算权重参数

的梯度。

numerical_gradient()的高

速版，将在下

一章实现

TwoLayerNet类

有params和grads两个字

典型实例变

量。params变量中保

存

了权重参

数，比如params['W1']以NumPy数

组的形式保

存了第1层的

权重参

数。此

外，第1层的偏

置可以通过

param['b1']进行访问。这

里来看一个

例子。

net =

TwoLayerNet(input_size=784, hidden_size=100, output_size=10)

net.params['W1'].shape

# (784, 100)

net.params['b1'].shape

# (100,)

net.params['W2'].shape #

(100, 10)

net.params['b2'].shape #

(10,)

4.5 学习算

法的实现

113

如

上所示，params变量

中保存了该

神经网络所

需的全部参

数。并且，

params变量

中保存的权

重参数会用

在推理处理

（前向处理）中

。顺便说一下

，

推理处理的

实现如下所

示。

x = np.random.rand(100, 784)

# 伪输入数

据（100笔）

y =

net.predict(x)

此外，与

params变量对应，grads变

量中保存了

各个参数的

梯度。如下所

示，

使用numerical_gradient()方法

计算梯度后

，梯度的信息

将保存在grads变

量中。

x

= np.random.rand(100, 784) #

伪输入

数据（100笔）

t = np.random.rand(100,

10) # 伪正

确解标签（100笔

）

grads

= net.numerical_gradient(x, t) #

计算梯度

grads['W1'].shape # (784,

100)

grads['b1'].shape # (100,)

grads['W2'].shape # (100, 10)

grads['b2'].shape # (10,)

接

着，我们来看

一下TwoLayerNet的方法

的实现。首先

是__init__(self,

input_size, hidden_size, output_size)方法，它是

类的初始化

方法（所谓初

始化方法，就

是生成TwoLayerNet实例

时被调用的

方法）。从第1个

参数开始，

依

次表示输入

层的神经元

数、隐藏层的

神经元数、输

出层的神经

元数。另外，

因

为进行手写

数字识别时

，输入图像的

大小是784（28 × 28），输出

为10个类别，

所

以指定参数

input_size=784、output_size=10，将隐藏层的

个数hidden_size

设置为

一个合适的

值即可。

此外

，这个初始化

方法会对权

重参数进行

初始化。如何

设置权重参

数

的初始值

这个问题是

关系到神经

网络能否成

功学习的重

要问题。后面

我

们会详细

讨论权重参

数的初始化

，这里只需要

知道，权重使

用符合高斯

分布的随机

数进行初始

化，偏置使用

0进行初始化

。predict(self,

x)和

accuracy(self, x, t)的实现和

上一章的神

经网络的推

理处理基本

一样。如

果仍

有不明白的

地方，请再回

顾一下上一

章的内容。另

外，loss(self, x, t)

第 4章　神经

网络的学习

114

是计算损失

函数值的方

法。这个方法

会基于predict()的结

果和正确解

标签，

计算交

叉熵误差。

剩

下的numerical_gradient(self, x, t)方法会

计算各个参

数的梯度。根

据数值微分

，计算各个参

数相对于损

失函数的梯

度。另外，gradient(self,

x, t)

是下

一章要实现

的方法，该方

法使用误差

反向传播法

高效地计算

梯度。

numerical_gradient(self,

x, t)基于数

值微分计算

参数的梯度

。下

一章，我们

会介绍一个

高速计算梯

度的方法，称

为误差反向

传播法。

用误

差反向传播

法求到的梯

度和数值微

分的结果基

本一致，但可

以

高速地进

行处理。使用

误差反向传

播法计算梯

度的gradient(self, 

x, t)方法会

在下一章实

现，不过考虑

到神经网络

的学习比较

花时间，

想节

约学习时间

的读者可以

替换掉这里

的numerical_gradient(self, 

x, t)，抢先使用

gradient(self,

x, t)！

4.5.2　mini-batch的实现

神经

网络的学习

的实现使用

的是前面介

绍过的mini-batch学习

。所谓

mini-batch学习，就

是从训练数

据中随机选

择一部分数

据（称为mini-batch），

再以

这些mini-batch为对象

，使用梯度法

更新参数的

过程。下面，我

们就以

TwoLayerNet类为

对象，使用MNIST数

据集进行学

习（源代码在

ch04/train_

neuralnet.py中）。

import numpy as

np

from dataset.mnist import

load_mnist

from two_layer_net import

TwoLayerNet

(x_train, t_train), (x_test,

t_test) = \ load_mnist(normalize=True,

one_hot_

laobel = True)

train_loss_list = []

#

超参数

iters_num = 10000

train_size = x_train.shape[0]

batch_size

= 100

learning_rate =

0.1

4.5  学

习算法的实

现

115

network = TwoLayerNet(input_size=784,

hidden_size=50, output_size=10)

for i

in range(iters_num):

 #

获取mini-batch

 batch_mask =

np.random.choice(train_size, batch_size)

 x_batch

= x_train[batch_mask]

 t_batch

= t_train[batch_mask]

 #

计算

梯度

 grad =

network.numerical_gradient(x_batch, t_batch)

 #

grad = network.gradient(x_batch, t_batch)

# 高速版

!

 #

更新参数

 for key

in ('W1', 'b1', 'W2',

'b2'):

 network.params[key] -=

learning_rate * grad[key]

# 记

录学习过程

loss =

network.loss(x_batch, t_batch)

 train_loss_list.append(loss)

这里，mini-batch的大小

为100，需要每次

从60000个训练数

据中随机

取

出100个数据（图

像数据和正

确解标签数

据）。然后，对这

个包含100笔数

据的mini-batch求梯度

，使用随机梯

度下降法（SGD）更

新参数。这里

，梯

度法的更

新次数（循环

的次数）为10000。每

更新一次，都

对训练数据

计算损

失函

数的值，并把

该值添加到

数组中。用图

像来表示这

个损失函数

的值的推

移

，如图4-11所示。

放

大显示

图4-11　损

失函数的推

移：左图是10000次

循环的推移

，右图是1000次循

环的推移

第

4章　神经网络

的学习 116

观察

图4-11，可以发现

随着学习的

进行，损失函

数的值在不

断减小。这

是

学习正常进

行的信号，表

示神经网络

的权重参数

在逐渐拟合

数据。也就是

说，神经网络

的确在学习

！通过反复地

向它浇灌（输

入）数据，神经

网络正

在逐

渐向最优参

数靠近。

4.5.3

基于

测试数据的

评价

根据图

4-11呈现的结果

，我们确认了

通过反复学

习可以使损

失函数的值

逐渐减小这

一事实。不过

这个损失函

数的值，严格

地讲是“对训

练数据的某

个mini-batch的损失函

数”的值。训练

数据的损失

函数值减小

，虽说是神经

网络的学习

正常进行的

一个信号，但

光看这个结

果还不能说

明该神经网

络在

其他数

据集上也一

定能有同等

程度的表现

。

神经网络的

学习中，必须

确认是否能

够正确识别

训练数据以

外的其他数

据，即确认是

否会发生过

拟合。过拟合

是指，虽然训

练数据中的

数字图像能

被正确辨别

，但是不在训

练数据中的

数字图像却

无法被识别

的现象。

神经

网络学习的

最初目标是

掌握泛化能

力，因此，要评

价神经网络

的泛

化能力

，就必须使用

不包含在训

练数据中的

数据。下面的

代码在进行

学习的

过程

中，会定期地

对训练数据

和测试数据

记录识别精

度。这里，每经

过一个

epoch，我们

都会记录下

训练数据和

测试数据的

识别精度。A

epoch是

一个单位。一

个 epoch表示学习

中所有训练

数据均被使

用过

一次时

的更新次数

。比如，对于 10000笔

训练数据，用

大小为

100

笔数

据的mini-batch进行学

习时，重复随

机梯度下降

法 100次，所

有的

训练数据就

都被“看过”了

A。此时，100次就是

一个

epoch。

为了正

确进行评价

，我们来稍稍

修改一下前

面的代码。与

前面的代码

不

同的地方

，我们用粗体

来表示。

A

实际

上，一般做法

是事先将所

有训练数据

随机打乱，然

后按指定的

批次大小，按

序生成mini-batch。

这样

每个mini-batch均有一

个索引号，比

如此例可以

是0, 1, 2,

... , 99，然后用索

引号可以遍

历所有

的mini-batch。遍

历一次所有

数据，就称为

一个epoch。请注意

，本节中的mini-batch每

次都是随机

选择的，所以

不一定每个

数据都会被

看到。——译者注

4.5

学习算法的

实现  117

import

numpy as np

from

dataset.mnist import load_mnist

from

two_layer_net import TwoLayerNet

(x_train,

t_train), (x_test, t_test) =

\ load_mnist(normalize=True, one_hot_

laobel

= True)

train_loss_list =

[]

train_acc_list = []

test_acc_list = []

#

平均每

个epoch的重复次

数

iter_per_epoch = max(train_size

/ batch_size, 1)

#

超参数

iters_num = 10000

batch_size = 100

learning_rate

= 0.1

network =

TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for

i in range(iters_num):

# 获

取mini-batch

 batch_mask

= np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

# 计算梯度

grad =

network.numerical_gradient(x_batch, t_batch)

 #

grad = network.gradient(x_batch, t_batch)

# 高速版!

 #

更新

参数

 for key

in ('W1', 'b1', 'W2',

'b2'):

 network.params[key] -=

learning_rate * grad[key]

loss = network.loss(x_batch, t_batch)

train_loss_list.append(loss)

 # 计算每

个epoch的识别精

度

if i % iter_per_epoch

== 0:

 train_acc

= network.accuracy(x_train, t_train)

test_acc = network.accuracy(x_test, t_test)

train_acc_list.append(train_acc)

 test_acc_list.append(test_acc)

print("train acc, test acc

| " + str(train_acc)

+ ", " +

str(test_acc))

在上面的

例子中，每经

过一个epoch，就对

所有的训练

数据和测试

数据

计算识

别精度，并记

录结果。之所

以要计算每

一个epoch的识别

精度，是因

为

如果在for语句

的循环中一

直计算识别

精度，会花费

太多时间。并

且，也

第 4章　神

经网络的学

习

118

没有必要

那么频繁地

记录识别精

度（只要从大

方向上大致

把握识别精

度的推

移就

可以了）。因此

，我们才会每

经过一个epoch就

记录一次训

练数据的识

别

精度。

把从

上面的代码

中得到的结

果用图表示

的话，如图4-12所

示。

图4-12　训练数

据和测试数

据的识别精

度的推移（横

轴的单位是

epoch）

图4-12中，实线表

示训练数据

的识别精度

，虚线表示测

试数据的识

别精

度。如图

所示，随着epoch的

前进（学习的

进行），我们发

现使用训练

数据和

测试

数据评价的

识别精度都

提高了，并且

，这两个识别

精度基本上

没有差异（两

条线基本重

叠在一起）。因

此，可以说这

次的学习中

没有发生过

拟合的现象

。

4.6 小结

本章中

，我们介绍了

神经网络的

学习。首先，为

了能顺利进

行神经网络

的学习，我们

导入了损失

函数这个指

标。以这个损

失函数为基

准，找出使它

4.6 小结  119

的值达

到最小的权

重参数，就是

神经网络学

习的目标。为

了找到尽可

能小的

损失

函数值，我们

介绍了使用

函数斜率的

梯度法。

本章

所学的内容

• 机器学习中

使用的数据

集分为训练

数据和测试

数据。

• 神经网

络用训练数

据进行学习

，并用测试数

据评价学习

到的模型的

泛化能力。

• 神

经网络的学

习以损失函

数为指标，更

新权重参数

，以使损失函

数

的值减小

。

• 利用某个给

定的微小值

的差分求导

数的过程，称

为数值微分

。

•

利用数值微

分，可以计算

权重参数的

梯度。

• 数值微

分虽然费时

间，但是实现

起来很简单

。下一章中要

实现的稍

微

复杂一些的

误差反向传

播法可以高

速地计算梯

度。

第5章

误差

反向传播法

上一章中，我

们介绍了神

经网络的学

习，并通过数

值微分计算

了神经网

络

的权重参数

的梯度（严格

来说，是损失

函数关于权

重参数的梯

度）。数值微

分

虽然简单，也

容易实现，但

缺点是计算

上比较费时

间。本章我们

将学习一

个

能够高效计

算权重参数

的梯度的方

法——误差反向

传播法。

要正

确理解误差

反向传播法

，我个人认为

有两种方法

：一种是基于

数学式；

另一

种是基于计

算图（computational graph）。前者是

比较常见的

方法，机器

学

习相关的图

书中多数都

是以数学式

为中心展开

论述的。因为

这种方法严

密

且简洁，所

以确实非常

合理，但如果

一上来就围

绕数学式进

行探讨，会忽

略

一些根本

的东西，止步

于式子的罗

列。因此，本章

希望大家通

过计算图，直

观地理解误

差反向传播

法。然后，再结

合实际的代

码加深理解

，相信大家一

定会有种“原

来如此！”的感

觉。

此外，通过

计算图来理

解误差反向

传播法这个

想法，参考了

Andrej

Karpathy的博客[4]和他

与Fei-Fei Li教授负责

的斯坦福大

学的深度学

习课程

CS231n [5]。

5.1 计算

图

计算图将

计算过程用

图形表示出

来。这里说的

图形是数据

结构图，通

过

多个节点和

边表示（连接

节点的直线

称为“边”）。为了

让大家熟悉

计算图，

第 5章

误差反向传

播法 122

本节先

用计算图解

一些简单的

问题。从这些

简单的问题

开始，逐步深

入，最

终抵达

误差反向传

播法。

5.1.1　用计算

图求解

现在

，我们尝试用

计算图解简

单的问题。下

面我们要看

的几个问题

都是

用心算

就能解开的

简单问题，这

里的目的只

是通过它们

让大家熟悉

计算图。

掌握

了计算图的

使用方法之

后，在后面即

将看到的复

杂计算中它

将发挥巨大

威力，所以本

节请一定学

会计算图的

使用方法。

问

题1：太郎在超

市买了2个100日

元一个的苹

果，消费税是

10%，请计

算支付

金额。

计算图

通过节点和

箭头表示计

算过程。节点

用○表示，○中是

计算的内

容

。将计算的中

间结果写在

箭头的上方

，表示各个节

点的计算结

果从左向右

传递。用计算

图解问题1，求

解过程如图

5-1所示。

×2

×1.1

100 200 220

图5-1　基于

计算图求解

的问题1的答

案

如图5-1所示

，开始时，苹果

的100日元流到

“× 2”节点，变成200日

元，

然后被传

递给下一个

节点。接着，这

个200日元流向

“× 1.1”节点，变成220

日

元。因此，从这

个计算图的

结果可知，答

案为220日元。

虽

然图5-1中把“×

2”“× 1.1”等

作为一个运

算整体用○括

起来了，不过

只用○表示乘

法运算“×”也是

可行的。此时

，如图5-2所示，可

以将“2”和

“1.1”分别

作为变量“苹

果的个数”和

“消费税”标在

○外面。

5.1

计算图

123

图5-2　基于计算

图求解的问

题1的答案：“苹

果的个数”和

“消费税”作为

变量标在○外

面

× ×

100 200

220

1.1

2 苹果的个

数

消费税

再

看下一题。

问

题2：太郎在超

市买了2个苹

果、3个橘子。其

中，苹果每个

100日元，

橘子每

个150日元。消费

税是10%，请计算

支付金额。

同

问题1，我们用

计算图来解

问题2，求解过

程如图5-3所示

。

图5-3　基于计算

图求解的问

题2的答案

×

+

100 200

2

650 715

150 450

1.1

3

×

×

苹

果的个数

橘

子的个数

消

费税

这个问

题中新增了

加法节点“+”，用

来合计苹果

和橘子的金

额。构建了

计

算图后，从左

向右进行计

算。就像电路

中的电流流

动一样，计算

结果从左

向

右传递。到达

最右边的计

算结果后，计

算过程就结

束了。从图5-3中

可知，

问题2的

答案为715日元

。

第 5章　误差反

向传播法 124

综

上，用计算图

解题的情况

下，需要按如

下流程进行

。

1.构建计算图

。

2.在计算图上

，从左向右进

行计算。

这里

的第2歩“从左

向右进行计

算”是一种正

方向上的传

播，简称为正

向传播（forward

propagation）。正向

传播是从计

算图出发点

到结束点的

传播。

既然有

正向传播这

个名称，当然

也可以考虑

反向（从图上

看的话，就是

从右向左）

的

传播。实际上

，这种传播称

为反向传播

（backward propagation）。反向传

播将

在接下来的

导数计算中

发挥重要作

用。

5.1.2　局部计算

计算图的特

征是可以通

过传递“局部

计算”获得最

终结果。“局部

”这个

词的意

思是“与自己

相关的某个

小范围”。局部

计算是指，无

论全局发生

了什么，

都能

只根据与自

己相关的信

息输出接下

来的结果。

我

们用一个具

体的例子来

说明局部计

算。比如，在超

市买了2个苹

果和

其他很

多东西。此时

，可以画出如

图5-4所示的计

算图。

图5-4

买了

2个苹果和其

他很多东西

的例子

×

+

100

200

2

1.1

苹果

的个数

消费

税

× 4620 4200

4000

其他很多

东西

复杂的

计算

如图5-4所

示，假设（经过

复杂的计算

）购买的其他

很多东西总

共花费

…

5.1  计算

图

125

4000日元。这里

的重点是，各

个节点处的

计算都是局

部计算。这意

味着，例

如苹

果和其他很

多东西的求

和运算（4000 +

200 → 4200）并不

关心4000这个

数

字是如何计

算而来的，只

要把两个数

字相加就可

以了。换言之

，各个节点

处

只需进行与

自己有关的

计算（在这个

例子中是对

输入的两个

数字进行加

法

运算），不用

考虑全局。

综

上，计算图可

以集中精力

于局部计算

。无论全局的

计算有多么

复杂，

各个步

骤所要做的

就是对象节

点的局部计

算。虽然局部

计算非常简

单，但是

通过

传递它的计

算结果，可以

获得全局的

复杂计算的

结果。

比如，组

装汽车是一

个复杂的工

作，通常需要

进行“流水线

”作业。

每个工

人（机器）所承

担的都是被

简化了的工

作，这个工作

的成果会

传

递给下一个

工人，直至汽

车组装完成

。计算图将复

杂的计算分

割

成简单的

局部计算，和

流水线作业

一样，将局部

计算的结果

传递给

下一

个节点。在将

复杂的计算

分解成简单

的计算这一

点上与汽车

的

组装有相

似之处。

5.1.3

为何

用计算图解

题

前面我们

用计算图解

答了两个问

题，那么计算

图到底有什

么优点呢？一

个优点就在

于前面所说

的局部计算

。无论全局是

多么复杂的

计算，都可以

通

过局部计

算使各个节

点致力于简

单的计算，从

而简化问题

。另一个优点

是，

利用计算

图可以将中

间的计算结

果全部保存

起来（比如，计

算进行到2个

苹

果时的金

额是200日元、加

上消费税之

前的金额650日

元等）。但是只

有这些

理由

可能还无法

令人信服。实

际上，使用计

算图最大的

原因是，可以

通过反

向传

播高效计算

导数。

在介绍

计算图的反

向传播时，我

们再来思考

一下问题1。问

题1中，我

们计

算了购买2个

苹果时加上

消费税最终

需要支付的

金额。这里，假

设我们

想知

道苹果价格

的上涨会在

多大程度上

影响最终的

支付金额，即

求“支付金

额

关于苹果的

价格的导数

”。设苹果的价

格为x，支付金

额为L，则相当

于求

。这个导

数的值表示

当苹果的价

格稍微上涨

时，支付金额

会增加多少

。

第 5章　误差反

向传播法

126

如

前所述，“支付

金额关于苹

果的价格的

导数”的值可

以通过计算

图的

反向传

播求出来。先

来看一下结

果，如图5-5所示

，可以通过计

算图的反向

传播求导数

（关于如何进

行反向传播

，接下来马上

会介绍）。

图5-5

基

于反向传播

的导数的传

递

× ×

100

2.2

200

1.1

220

1

1.1

2 苹果的个

数

消费税

如

图5-5所示，反向

传播使用与

正方向相反

的箭头（粗线

）表示。反向传

播传递“局部

导数”，将导数

的值写在箭

头的下方。在

这个例子中

，反向传

播从

右向左传递

导数的值（1 →

1.1 → 2.2）。从

这个结果中

可知，“支付金

额

关于苹果

的价格的导

数”的值是2.2。这

意味着，如果

苹果的价格

上涨1日元，

最

终的支付金

额会增加2.2日

元（严格地讲

，如果苹果的

价格增加某

个微小值，

则

最终的支付

金额将增加

那个微小值

的2.2倍）。

这里只

求了关于苹

果的价格的

导数，不过“支

付金额关于

消费税的导

数”“支付金额

关于苹果的

个数的导数

”等也都可以

用同样的方

式算出来。并

且，计算中途

求得的导数

的结果（中间

传递的导数

）可以被共享

，从而可以

高

效地计算多

个导数。综上

，计算图的优

点是，可以通

过正向传播

和反向传

播

高效地计算

各个变量的

导数值。

5.2 链式

法则

前面介

绍的计算图

的正向传播

将计算结果

正向（从左到

右）传递，其计

算过程是我

们日常接触

的计算过程

，所以感觉上

可能比较自

然。而反向传

播

5.2  链式法则

127

将局部导数

向正方向的

反方向（从右

到左）传递，一

开始可能会

让人感到困

惑。

传递这个

局部导数的

原理，是基于

链式法则（chain rule）的

。本节将介绍

链

式法则，并

阐明它是如

何对应计算

图上的反向

传播的。

5.2.1

计算

图的反向传

播

话不多说

，让我们先来

看一个使用

计算图的反

向传播的例

子。假设存在

y = f(x)的计算，这个

计算的反向

传播如图5-6所

示。

y x

f

E

E

图5-6　计算图

的反向传播

：沿着与正方

向相反的方

向，乘上局部

导数

如图所

示，反向传播

的计算顺序

是，将信号E乘

以节点的局

部导数

（ ），然后

将结果传递

给下一个节

点。这里所说

的局部导数

是指正向传

播

中y =

f(x)的导数

，也就是y关于

x的导数（ ）。比如

，假设y = f(x)

= x2

，

则局部

导数为

= 2x。把这

个局部导数

乘以上游传

过来的值（本

例中为E），

然后

传递给前面

的节点。

这就

是反向传播

的计算顺序

。通过这样的

计算，可以高

效地求出导

数的

值，这是

反向传播的

要点。那么这

是如何实现

的呢？我们可

以从链式法

则的

原理进

行解释。下面

我们就来介

绍链式法则

。

5.2.2　什么是链式

法则

介绍链

式法则时，我

们需要先从

复合函数说

起。复合函数

是由多个函

数

构成的函

数。比如，z = (x

+ y)

2

是由

式（5.1）所示的两

个式子构成

的。

z = t

2

t = x +

y 

（5.1）

第 5章　误差

反向传播法

128

链式法则是

关于复合函

数的导数的

性质，定义如

下。

如果某个

函数由复合

函数表示，则

该复合函数

的导数可以

用构成复

合

函数的各个

函数的导数

的乘积表示

。

这就是链式

法则的原理

，乍一看可能

比较难理解

，但实际上它

是一个

非常

简单的性质

。以式（5.1）为例，

（z关

于x的导数）可

以用 （z关于t

的

导数）和 （t关于

x的导数）的乘

积表示。用数

学式表示的

话，可以写成

式（5.2）。

（5.2）

式（5.2）中的∂ t正

好可以像下

面这样“互相

抵消”，所以记

起来很简单

。

现在我们使

用链式法则

，试着求式（5.2）的

导数

。为此，我

们要先求

式

（5.1）中的局部导

数（偏导数）。

（5.3）

如

式（5.3）所示，

等于

2t， 等于1。这是基

于导数公式

的解析解。

然

后，最后要计

算的 可由式

（5.3）求得的导数

的乘积计算

出来。

（5.4）

5.2  链式法

则

129

5.2.3　链式法则

和计算图

现

在我们尝试

将式（5.4）的链式

法则的计算

用计算图表

示出来。如果

用

“**2”节点表示

平方运算的

话，则计算图

如图5-7所示。

+ **2

y

x

z t

图

5-7

式（5.4）的计算图

：沿着与正方

向相反的方

向，乘上局部

导数后传递

如图所示，计

算图的反向

传播从右到

左传播信号

。反向传播的

计算顺序

是

，先将节点的

输入信号乘

以节点的局

部导数（偏导

数），然后再传

递给下一

个

节点。比如，反

向传播时，“**2”节

点的输入是

，将其乘以局

部导数 （因

为

正向传播时

输入是t、输出

是z，所以这个

节点的局部

导数是 ），然后

传

递给下一

个节点。另外

，图5-7中反向传

播最开始的

信号 在前面

的数学

式中

没有出现，这

是因为 ，所以

在刚才的式

子中被省略

了。

图 5-7

中需要

注意的是最

左边的反向

传播的结果

。根据链式法

则，

成立，对应

“z关于x的导数

”。也就是说，反

向传播

是基

于链式法则

的。

把式（5.3）的结

果代入到图

5-7

中，结果如图

5-8 所示， 的结果

为

2(x

+ y)。

第 5章

误差

反向传播法

130

图5-8　根据计算

图的反向传

播的结果， 等

于2(x

+ y)

y

x

z t

1

+

**2

5.3 反向传播

上一节介绍

了计算图的

反向传播是

基于链式法

则成立的。本

节将以“+”

和“×”等

运算为例，介

绍反向传播

的结构。

5.3.1　加法

节点的反向

传播

首先来

考虑加法节

点的反向传

播。这里以z =

x + y为

对象，观察它

的

反向传播

。z

= x + y的导数可由

下式（解析性

地）计算出来

。

（5.5）

如式（5.5）所示， 和

同时都等于

1。因此，用计算

图表示的话

，如

图5-9所示。

在

图5-9中，反向传

播将从上游

传过来的导

数（本例中是

）乘以1，然

后传

向下游。也就

是说，因为加

法节点的反

向传播只乘

以1，所以输入

的值

会原封

不动地流向

下一个节点

。

5.3

反向传播 131

图

5-9 加法节点的

反向传播：左

图是正向传

播，右图是反

向传播。如右

图的反向传

播所示，

加法

节点的反向

传播将上游

的值原封不

动地输出到

下游

+ +

y

x

z

另外，本

例中把从上

游传过来的

导数的值设

为 。这是因为

，如图5-10

所示，我

们假定了一

个最终输出

值为L的大型

计算图。z = x +

y的计

算位于

这个

大型计算图

的某个地方

，从上游会传

来 的值，并向

下游传递 和

。

图5-10 加法节点

存在于某个

最后输出的

计算的一部

分中。反向传

播时，从最右

边的输

出出

发，局部导数

从节点向节

点反方向传

播

+

y

x

z 某种计算

某种计算

某

种计算

L

第 5章

误差反向传

播法

132

现在来

看一个加法

的反向传播

的具体例子

。假设有“10 + 5=15”这一

计

算，反向传

播时，从上游

会传来值1.3。用

计算图表示

的话，如图5-11所

示。

图5-11　加法节

点的反向传

播的具体例

子

+

5

10

15

1.3

1.3

1.3

+

因为加法

节点的反向

传播只是将

输入信号输

出到下一个

节点，所以如

图

5-11所示，反向

传播将1.3向下

一个节点传

递。

5.3.2　乘法节点

的反向传播

接下来，我们

看一下乘法

节点的反向

传播。这里我

们考虑z =

xy。这个

式子的导数

用式（5.6）表示。

（5.6）

根

据式（5.6），可以像

图5-12那样画计

算图。

乘法的

反向传播会

将上游的值

乘以正向传

播时的输入

信号的“翻转

值”

后传递给

下游。翻转值

表示一种翻

转关系，如图

5-12所示，正向传

播时信号

是

x的话，反向传

播时则是y；正

向传播时信

号是y的话，反

向传播时则

是x。

现在我们

来看一个具

体的例子。比

如，假设有“10 ×

5 = 50”这

一计算，

反向

传播时，从上

游会传来值

1.3。用计算图表

示的话，如图

5-13所示。

5.3  反向传

播 133

图5-13　乘法节

点的反向传

播的具体例

子

× ×

5

10

50

1.3

13

6.5

×

y

x

z ×

图5-12

乘法的

反向传播：左

图是正向传

播，右图是反

向传播

因为

乘法的反向

传播会乘以

输入信号的

翻转值，所以

各自可按1.3 × 5

= 

6.5、1.3 ×

10 = 13计

算。另外，加法

的反向传播

只是将上游

的值传给下

游，

并不需要

正向传播的

输入信号。但

是，乘法的反

向传播需要

正向传播时

的输

入信号

值。因此，实现

乘法节点的

反向传播时

，要保存正向

传播的输入

信号。

5.3.3　苹果的

例子

再来思

考一下本章

最开始举的

购买苹果的

例子（2个苹果

和消费税）。这

里要解的问

题是苹果的

价格、苹果的

个数、消费税

这3个变量各

自如何影响

最终支付的

金额。这个问

题相当于求

“支付金额关

于苹果的价

格的导数”“支



第 5章　误差反

向传播法 134

付

金额关于苹

果的个数的

导数”“支付金

额关于消费

税的导数”。用

计算图的

反

向传播来解

的话，求解过

程如图5-14所示

。

图5-14　购买苹果

的反向传播

的例子

× ×

100

2.2

200

1.1

200

110

220

1

1.1

2

苹果

的个数

消费

税

如前所述

，乘法节点的

反向传播会

将输入信号

翻转后传给

下游。从图5-14

的

结果可知，苹

果的价格的

导数是2.2，苹果

的个数的导

数是110，消费税

的

导数是200。这

可以解释为

，如果消费税

和苹果的价

格增加相同

的值，则消

费

税将对最终

价格产生200倍

大小的影响

，苹果的价格

将产生2.2倍大

小的影响。

不

过，因为这个

例子中消费

税和苹果的

价格的量纲

不同，所以才

形成了这样

的结果（消费

税的1是100%，苹果

的价格的1是

1日元）。

最后作

为练习，请大

家来试着解

一下“购买苹

果和橘子”的

反向传播。

在

图5-15中的方块

中填入数字

，求各个变量

的导数（答案

在若干页后

）。

图5-15　购买苹果

和橘子的反

向传播的例

子：在方块中

填入数字，完

成反向传播

×

+

100 200

2

650

715

150 450

1.1

3

×

×

苹果的个数

橘子的个数

消费税

5.4  简单

层的实现 135

5.4 简

单层的实现

本节将用Python实

现前面的购

买苹果的例

子。这里，我们

把要实现

的

计算图的乘

法节点称为

“乘法层”（MulLayer），加法

节点称为“加

法层”

（AddLayer）。

下一节

，我们将把构

建神经网络

的“层”实现为

一个类。这里

所

说的“层”是

神经网络中

功能的单位

。比如，负责 sigmoid函

数的

Sigmoid、负责矩

阵乘积的Affine等

，都以层为单

位进行实现

。因此，

这里也

以层为单位

来实现乘法

节点和加法

节点。

5.4.1　乘法层

的实现

层的

实现中有两

个共通的方

法（接口）forward()和backward()。forward()

对

应正向传播

，backward()对应反向传

播。

现在来实

现乘法层。乘

法层作为MulLayer类

，其实现过程

如下所示（源

代码在ch05/layer_naive.py中）。

class MulLayer:

def __init__(self):

 self.x

= None

 self.y

= None

 def

forward(self, x, y):

self.x = x

self.y = y

out = x *

y

 return out

def backward(self, dout):

dx = dout *

self.y # 翻

转x和y

dy = dout *

self.x

 return dx,

dy

  第

5章　误

差反向传播

法 136

__init__()中会初始

化实例变量

x和y，它们用于

保存正向传

播时的输入

值。

forward()接收x和y两

个参数，将它

们相乘后输

出。backward()将从上游

传

来的导数

（dout）乘以正向传

播的翻转值

，然后传给下

游。

上面就是

MulLayer的实现。现在

我们使用MulLayer实

现前面的购

买苹果

的例

子（2个苹果和

消费税）。上一

节中我们使

用计算图的

正向传播和

反向传播，

像

图5-16这样进行

了计算。

图5-16　购

买2个苹果

×

×

100

2.2

200

1.1

200

110

220

1

1.1

2 苹

果的个数

消

费税

使用这

个乘法层的

话，图5-16的正向

传播可以像

下面这样实

现（源代码

在

ch05/buy_apple.py中）。

apple

= 100

apple_num =

2

tax = 1.1

# layer

mul_apple_layer =

MulLayer()

mul_tax_layer = MulLayer()

# forward

apple_price =

mul_apple_layer.forward(apple, apple_num)

price =

mul_tax_layer.forward(apple_price, tax)

print(price) #

220

此外，关于

各个变量的

导数可由backward()求

出。

5.4

简单层的

实现 137

# backward

dprice = 1

dapple_price,

dtax = mul_tax_layer.backward(dprice)

dapple,

dapple_num = mul_apple_layer.backward(dapple_price)

print(dapple,

dapple_num, dtax) # 2.2

110 200

这里，调

用backward()的顺序与

调用forward()的顺序

相反。此外，要

注

意backward()的参数

中需要输入

“关于正向传

播时的输出

变量的导数

”。比如，

mul_apple_layer乘法层

在正向传播

时会输出apple_price，在

反向传播时

，则

会将apple_price的导

数dapple_price设为参数

。另外，这个程

序的运行结

果

和图5-16是一

致的。

5.4.2

加法层

的实现

接下

来，我们实现

加法节点的

加法层，如下

所示。

class AddLayer:

def __init__(self):

 pass

def forward(self, x, y):

out = x +

y

 return out

def backward(self, dout):

dx = dout *

1

 dy =

dout * 1

return dx, dy

加法层

不需要特意

进行初始化

，所以__init__()中什么

也不运行（pass

语

句表示“什么

也不运行”）。加

法层的forward()接收

x和y两个参数

，将它

们相加

后输出。backward()将上

游传来的导

数（dout）原封不动

地传递给下

游。

现在，我们

使用加法层

和乘法层，实

现图5-17所示的

购买2个苹果

和3

个橘子的

例子。

第 5章　误

差反向传播

法

138

图5-17　购买2个

苹果和3个橘

子

×

+

100 200

2

650 715

150 450

1.1

3

×

×

苹果的个

数

橘子的个

数

消费税

2.2

3.3

1.1

1.1

110

650

165

1.1

1

用

Python实现图5-17的计

算图的过程

如下所示（源

代码在ch05/buy_

apple_orange.py中）。

apple =

100

apple_num = 2

orange = 150

orange_num

= 3

tax =

1.1

# layer

mul_apple_layer

= MulLayer()

mul_orange_layer =

MulLayer()

add_apple_orange_layer = AddLayer()

mul_tax_layer = MulLayer()

#

forward

apple_price = mul_apple_layer.forward(apple,

apple_num) #(1)

orange_price =

mul_orange_layer.forward(orange, orange_num) #(2)

all_price

= add_apple_orange_layer.forward(apple_price, orange_price) #(3)

price = mul_tax_layer.forward(all_price, tax)

#(4)

# backward

dprice

= 1

dall_price, dtax

= mul_tax_layer.backward(dprice) #(4)

dapple_price,

dorange_price = add_apple_orange_layer.backward(dall_price) #(3)

dorange, dorange_num = mul_orange_layer.backward(dorange_price)

#(2)

dapple, dapple_num =

mul_apple_layer.backward(dapple_price) #(1)

print(price) #

715

print(dapple_num, dapple, dorange,

dorange_num, dtax) # 110

2.2 3.3 165 650

5.5  激

活函数层的

实现 139

这个实

现稍微有一

点长，但是每

一条命令都

很简单。首先

，生成必要的

层，以合适的

顺序调用正

向传播的forward()方

法。然后，用与

正向传播相

反的顺序调

用反向传播

的backward()方法，就可

以求出想要

的导数。

综上

，计算图中层

的实现（这里

是加法层和

乘法层）非常

简单，使用这

些层可以进

行复杂的导

数计算。下面

，我们来实现

神经网络中

使用的层。

5.5 激

活函数层的

实现

现在，我

们将计算图

的思路应用

到神经网络

中。这里，我们

把构成神经

网络的层实

现为一个类

。先来实现激

活函数的ReLU层

和Sigmoid层。

5.5.1　ReLU层

激活

函数ReLU（Rectified

Linear Unit）由下式

（5.7）表示。

（5.7）

通过式

（5.7），可以求出y关

于x的导数，如

式（5.8）所示。

（5.8）

在式

（5.8）中，如果正向

传播时的输

入x大于0，则反

向传播会将

上游的

值原

封不动地传

给下游。反过

来，如果正向

传播时的x小

于等于0，则反

向

传播中传

给下游的信

号将停在此

处。用计算图

表示的话，如

图5-18所示。

现在

我们来实现

ReLU层。在神经网

络的层的实

现中，一般假

定forward()

和backward()的参数

是NumPy数组。另外

，实现ReLU层的源

代码在common/

layers.py中。

第

5章

误差反向

传播法 140

图5-18 ReLU层

的计算图

relu relu

y y

x

0

x

x

> 0 时

x

0 时

class Relu:

def __init__(self):

 self.mask

= None

 def

forward(self, x):

 self.mask

= (x <= 0)

out = x.copy()

out[self.mask] = 0

return out

 def

backward(self, dout):

 dout[self.mask]

= 0

 dx

= dout

 return

dx

Relu类有实例

变量mask。这个变

量mask是由True/False构成

的NumPy数

组，它会

把正向传播

时的输入x的

元素中小于

等于0的地方

保存为True，其

他

地方（大于0的

元素）保存为

False。如下例所示

，mask变量保存了

由True/

False构成的NumPy数

组。

>>> x =

np.array( [[1.0, -0.5], [-2.0,

3.0]] )

>>> print(x)

[[ 1. -0.5]

[-2. 3. ]]

>>>

mask = (x <=

0)

>>> print(mask)

[[False

True]

 [ True

False]]

5.5  激活函数

层的实现

141

如

图5-18所示，如果

正向传播时

的输入值小

于等于0，则反

向传播的值

为0。

因此，反向

传播中会使

用正向传播

时保存的mask，将

从上游传来

的dout的

mask中的元

素为True的地方

设为0。

ReLU层的作

用就像电路

中的开关一

样。正向传播

时，有电流通

过

的话，就将

开关设为 ON；没

有电流通过

的话，就将开

关设为 OFF。

反向

传播时，开关

为ON的话，电流

会直接通过

；开关为OFF的话

，

则不会有电

流通过。

5.5.2　Sigmoid层

接

下来，我们来

实现sigmoid函数。sigmoid函

数由式（5.9）表示

。

（5.9）

用计算图表

示式（5.9）的话，则

如图5-19所示。

图

5-19

sigmoid层的计算图

（仅正向传播

）

1 −1

y

1

× + /

exp

x −x exp(−x)

1+exp(−x) 1+exp(−x)

图5-19中，除了“×”和

“+”节点外，还出

现了新的“exp”和

“/”节点。

“exp”节点会

进行y

= exp(x)的计算

，“/”节点会进行

的计算。

如图

5-19所示，式（5.9）的计

算由局部计

算的传播构

成。下面我们

就来

进行图

5-19的计算图的

反向传播。这

里，作为总结

，我们来依次

看一下反向

传播的流程

。

第 5章　误差反

向传播法 142

步

骤1

“/”节点表示

，它的导数可

以解析性地

表示为下式

。

（5.10）

根据式（5.10），反向

传播时，会将

上游的值乘

以−y

2

（正向传播

的输出的

平

方乘以−1后的

值）后，再传给

下游。计算图

如下所示。

×

exp

x −x exp(−x)

1+exp(−x)

1 −1

y

+ /

步

骤2

“+”节点将上

游的值原封

不动地传给

下游。计算图

如下所示。

x −x exp(−x) 1+exp(−x)

y

1 −1

×

exp + /

步

骤3

“exp”节点表示

y = exp(x)，它的导数由

下式表示。

（5.11）

计

算图中，上游

的值乘以正

向传播时的

输出（这个例

子中是exp(−x)）后，

再

传给下游。

× ＋

/ exp

x −x

exp(−x) 1+exp(−x) y

1

−1

exp(−x)

5.5

激

活函数层的

实现 143

步骤4

“×”节

点将正向传

播时的值翻

转后做乘法

运算。因此，这

里要乘以−1。

× ＋ / exp

x −x exp(−x) 1+exp(−x)

y

exp(−x) exp(−x)

1

−1

图

5-20 Sigmoid层的计算图

根据上述内

容，图5-20的计算

图可以进行

Sigmoid层的反向传

播。从图5-20

的结

果可知，反向

传播的输出

为

，这个值会

传播给下游

的节点。

这里

要注意， 这个

值只根据正

向传播时的

输入x和输出

y就可

以算出

来。因此，图5-20的

计算图可以

画成图5-21的集

约化的“sigmoid”节点

。

sigmoid

y x

exp(−x)

图5-21 Sigmoid层的计算

图（简洁版）

图

5-20的计算图和

简洁版的图

5-21的计算图的

计算结果是

相同的，但是

，

简洁版的计

算图可以省

略反向传播

中的计算过

程，因此计算

效率更高。此

外，

通过对节

点进行集约

化，可以不用

在意Sigmoid层中琐

碎的细节，而

只需要

专注

它的输入和

输出，这一点

也很重要。

另

外， 可以进一

步整理如下

。

（5.12）

第 5章　误差反

向传播法

144

因

此，图5-21所表示

的Sigmoid层的反向

传播，只根据

正向传播的

输出

就能计

算出来。

sigmoid

y x

(1 −

y)

图5-22 Sigmoid层

的计算图：可

以根据正向

传播的输出

y计算反向传

播

现在，我们

用Python实现Sigmoid层。参

考图5-22，可以像

下面这样实

现（实现的代

码在common/layers.py中）。

class Sigmoid:

 def

__init__(self):

 self.out =

None

 def forward(self,

x):

 out =

1 / (1 +

np.exp(-x))

 self.out =

out

 return out

def backward(self, dout):

dx = dout *

(1.0 - self.out) *

self.out

 return dx

这个

实现中，正向

传播时将输

出保存在了

实例变量out中

。然后，反向

传

播时，使用该

变量out进行计

算。

5.6 Affine/Softmax层的实现

5.6.1

Affine层

神经网络

的正向传播

中，为了计算

加权信号的

总和，使用了

矩阵的乘

积

运算（NumPy中是np.dot()，具

体请参照3.3节

）。比如，还记得

我们用

Python进行

了下面的实

现吗？

5.6  Affine/Softmax层的实

现 145

>>> X = np.random.rand(2)

# 输入

>>> W

= np.random.rand(2,3) # 权重

>>>

B = np.random.rand(3) #

偏置

>>>

>>> X.shape

# (2,)

>>> W.shape

# (2, 3)

>>>

B.shape # (3,)

>>>

>>> Y = np.dot(X,

W) + B

这里，X、W、B

分

别是形状为

(2,)、(2, 3)、(3,)的多维数组

。这样一

来，神

经元的加权

和可以用Y =

np.dot(X, W) + B计

算出来。然后

，Y

经过

激活函

数转换后，传

递给下一层

。这就是神经

网络正向传

播的流程。此

外，

我们来复

习一下，矩阵

的乘积运算

的要点是使

对应维度的

元素个数一

致。比

如，如下

面的图5-23所示

，X和W

的乘积必

须使对应维

度的元素个

数一致。

另外

，这里矩阵的

形状用(2, 3)这样

的括号表示

（为了和NumPy的shape属

性的输出一

致）。

X

= · W O

(3,) (2,) (2, 3)

保持一致

图5-23　矩阵的乘

积运算中对

应维度的元

素个数要保

持一致

神经

网络的正向

传播中进行

的矩阵的乘

积运算在几

何学领域被

称为“仿

射变

换”A。因此，这里

将进行仿射

变换的处理

实现为“Affine层”。

A

现

在将这里进

行的求矩阵

的乘积与偏

置的和的运

算用计算图

表示出来。

将

乘积运算用

“dot”节点表示的

话，则np.dot(X, W)

+ B的运算

可用图5-24

所示

的计算图表

示出来。另外

，在各个变量

的上方标记

了它们的形

状（比如，

计算

图上显示了

X的形状为(2,)，X·W的

形状为(3,)等）。

A 几

何中，仿射变

换包括一次

线性变换和

一次平移，分

别对应神经

网络的加权

和运算与加

偏置运算。

——译

者注

第

5章　误

差反向传播

法 146

图5-24

Affine层的计

算图（注意变

量是矩阵，各

个变量的上

方标记了该

变量的形状

）

dot

X W

Y

W

B

(3,)

(3,)

(3,)

(2, 3)

X

(2,)

+

图5-24是比较简

单的计算图

，不过要注意

X、W、B是矩阵（多维

数组）。

之前我

们见到的计

算图中各个

节点间流动

的是标量，而

这个例子中

各个节点

间

传播的是矩

阵。

现在我们

来考虑图5-24的

计算图的反

向传播。以矩

阵为对象的

反向传播，

按

矩阵的各个

元素进行计

算时，步骤和

以标量为对

象的计算图

相同。实际写

一下的话，可

以得到下式

（这里省略了

式（5.13）的推导过

程）。

（5.13）

式（5.13）中WT的T表

示转置。转置

操作会把W的

元素(i, j)换成元

素

(j,

i)。用数学式

表示的话，可

以写成下面

这样。

（5.14）

5.6

Affine/Softmax层的实

现  147

如式（5.14）所示

，如果W的形状

是(2,

3)，WT的形状就

是(3, 2)。

现在，我们

根据式（5.13），尝试

写出计算图

的反向传播

，如图5-25所示。

(3,)

(2, 1) (1, 3)

(2,)

(2, 3)

(3,

2) dot ＋

X

X・W Y

W

B

(3,) (3,)

(3,)

(2,)

(2, 3)

1

2

1

2

(3,) (3,)

(3,)

图

5-25 Affi ne层的反向传

播：注意变量

是多维数组

。反向传播时

各个变量的

下方标记了

该变量的形

状

我们看一

下图2-25的计算

图中各个变

量的形状。尤

其要注意，X和

形状相同，W和

形状相同。从

下面的数学

式可以很明

确地看出X和

形状相同。

（5.15）

为

什么要注意

矩阵的形状

呢？因为矩阵

的乘积运算

要求对应维

度的元素

个

数保持一致

，通过确认一

致性，就可以

导出式（5.13）。比如

，

的形状是

(3,)，W的

形状是(2, 3)时，思

考 和WT的乘积

，使得

的形状

为(2,)

（图5-26）。这样一

来，就会自然

而然地推导

出式（5.13）。

第 5章　误

差反向传播

法 148

1

X

(2,)

X・W

(3,)

W

(2, 3)

dot

(3,)

1

(3,)

(3, 2) (2,)

图5-26

矩阵的

乘积（“dot”节点）的

反向传播可

以通过组建

使矩阵对应

维度的元素

个数一

致的

乘积运算而

推导出来

5.6.2　批

版本的Affine层

前

面介绍的Affi ne层

的输入X是以

单个数据为

对象的。现在

我们考虑N

个

数据一起进

行正向传播

的情况，也就

是批版本的

Affi ne层。

先给出批

版本的Affi ne层的

计算图，如图

5-27所示。

dot ＋

W

(2, 3)

X

(N, 2)

X・W

(N,

3)

Y

(N, 3)

B

(3,)

1

2

3

(N, 3) (N,

3)

(N, 2) (N,

3)

1

2

3

(3, 2)

(2, 3)

(2, N) (N, 3)

(3) (N, 3)

的第一

个轴（第0轴）方

向上的和

图

5-27　批版本的Affi ne层

的计算图

5.6

Affine/Softmax层

的实现 149

与刚

刚不同的是

，现在输入X的

形状是(N, 2)。之后

就和前面一

样，在

计算图

上进行单纯

的矩阵计算

。反向传播时

，如果注意矩

阵的形状，就

可以

和前面

一样推导出

和 。

加上偏置

时，需要特别

注意。正向传

播时，偏置被

加到X·W的各个

数据上。比如

，N

= 2（数据为2个）时

，偏置会被分

别加到这2个

数据（各自

的

计算结果）上

，具体的例子

如下所示。

>>>

X_dot_W = np.array([[0, 0,

0], [10, 10, 10]])

>>> B = np.array([1,

2, 3])

>>>

>>>

X_dot_W

array([[ 0, 0,

0],

 [ 10,

10, 10]])

>>> X_dot_W

+ B

array([[ 1,

2, 3],

 [11,

12, 13]])

正

向传播时，偏

置会被加到

每一个数据

（第1个、第2个……）上

。因此，

反向传

播时，各个数

据的反向传

播的值需要

汇总为偏置

的元素。用代

码表示

的话

，如下所示。

>>> dY =

np.array([[1, 2, 3,], [4,

5, 6]])

>>> dY

array([[1, 2, 3],

[4, 5, 6]])

>>>

>>> dB = np.sum(dY,

axis=0)

>>> dB

array([5,

7, 9])

这

个例子中，假

定数据有2个

（N =

2）。偏置的反向

传播会对这

2个数据

的导

数按元素进

行求和。因此

，这里使用了

np.sum()对第0轴（以数

据为单

位的

轴，axis=0）方向上的

元素进行求

和。

综上所述

，Affine的实现如下

所示。另外，common/layers.py中

的Affine

的实现考

虑了输入数

据为张量（四

维数据）的情

况，与这里介

绍的稍有差

别。

  第

5章　误差

反向传播法

150

class Affine:

def __init__(self, W, b):

self.W = W

self.b = b

self.x = None

self.dW = None

self.db = None

def forward(self, x):

self.x = x

out = np.dot(x, self.W)

+ self.b

 return

out

 def backward(self,

dout):

 dx =

np.dot(dout, self.W.T)

 self.dW

= np.dot(self.x.T, dout)

self.db = np.sum(dout, axis=0)

return dx

5.6.3　Softmax-with-Loss

层

最后介绍

一下输出层

的softmax函数。前面

我们提到过

，softmax函数

会将输

入值正规化

之后再输出

。比如手写数

字识别时，Softmax层

的输出如

图

5-28所示。

Softmax

得分 概

率

Affine

ReLU

5.3

10.1

0.3

Affine ReLU

0.01

0.008

0.00005

0.991

0.00004

Affine

图5-28 输入图

像通过Affi ne层和

ReLU层进行转换

，10个输入通过

Softmax层进行正

规

化。在这个例

子中，“0”的得分

是5.3，这个值经

过Softmax层转换为

0.008

（0.8%）；“2”的得分是10.1，被

转换为0.991（99.1%）

在图

5-28中，Softmax层将输入

值正规化（将

输出值的和

调整为1）之后

再输出。另外

，因为手写数

字识别要进

行10类分类，所

以向Softmax层的输

入也有10个。

・・・

・・・

・・・・・・

・・・

・・・

・・・・・・

・・・

・・・

・・・

5.6

Affine/Softmax层

的实现 151

神经

网络中进行

的处理有推

理（inference）和学习两

个阶段。神经

网

络的推理

通常不使用

Softmax层。比如，用图

5-28的网络进行

推理时，

会将

最后一个 Affine层

的输出作为

识别结果。神

经网络中未

被正规

化的

输出结果（图

5-28中 Softmax层前面的

Affine层的输出）有

时

被称为“得

分”。也就是说

，当神经网络

的推理只需

要给出一个

答案

的情况

下，因为此时

只对得分最

大值感兴趣

，所以不需要

Softmax层。

不过，神经

网络的学习

阶段则需要

Softmax层。

下面来实

现Softmax层。考虑到

这里也包含

作为损失函

数的交叉熵

误

差（cross entropy error），所以称

为“Softmax-with-Loss层”。Softmax-with￾Loss层（Softmax函数

和交叉熵误

差）的计算图

如图5-29所示。

×

×

×

×

log

log

log

×

×

exp

exp

exp

+

+

/ S

L

Softmax 层

Cross Entropy Error

层

×

a1

a2

a3

y1−t1

y2−t2

y3−t3

exp(a1)

exp(a2)

exp(a3)

S

1

S

1

S

1

S

1

exp(a1)

exp(a1)

t1 −

exp(a2)

exp(a2)

t2 −

exp(a3)

exp(a3)

t3 −

−t3S

S

1

S

1

S

1

t1

t2

t3

y1

y2

y3

y1

t1 −

y2

t2 −

y3

t3

−

log y1

log

y2

log y3

−t1

−t2

−t3

t1 log

y1 + t2 log

y2 + t3 log

y3

t1 log y1

t2 log y2

t3

log y3

−1

−1

−1

−1

−1

1

图5-29 Softmax-with-Loss层的计

算图

可以看

到，Softmax-with-Loss层有些复

杂。这里只给

出了最终结

果，

对Softmax-with-Loss层的导

出过程感兴

趣的读者，请

参照附录A。

图

5-29的计算图可

以简化成图

5-30。

图5-30的计算图

中，softmax函数记为

Softmax层，交叉熵误

差记为

Cross Entropy

Error层。这

里假设要进

行3类分类，从

前面的层接

收3个输

入（得

分）。如图5-30所示

，Softmax层将输入（a1, a2, a3）正

规化，输出（y1,

y2, y3）。Cross Entropy Error层

接收Softmax的输出

（y1,

y2, y3）和教师标签

（t1, 

t2,

t3），从这些数据

中输出损失

L。

−t2S −t1S

第

5章　误差反

向传播法 152

图

5-30

“简易版”的Softmax-with-Loss层

的计算图

Softmax

L

1

Cross

Entropy

Error

a1

a2

a3

y1−t

1

y2−t 2

y3−t

3

t 1

y1

t 2

y2

t

3

y3

图

5-30中要注意的

是反向传播

的结果。Softmax层的

反向传播得

到了

（y1

− t1, y2 −

t2, y3 − t3）这样“漂

亮”的结果。由

于（y1,

y2, y3）是Softmax层的

输

出，（t1, t2,

t3）是监督数

据，所以（y1 − t1, y2

− t2, y3 −

t3）是Softmax层

的输

出和教

师标签的差

分。神经网络

的反向传播

会把这个差

分表示的误

差传递给

前

面的层，这是

神经网络学

习中的重要

性质。

神经网

络学习的目

的就是通过

调整权重参

数，使神经网

络的输出（Softmax

的

输出）接近教

师标签。因此

，必须将神经

网络的输出

与教师标签

的误差高

效

地传递给前

面的层。刚刚

的（y1 − t1,

y2 − t2, y3

− t3）正是Softmax层的

输出

与教师

标签的差，直

截了当地表

示了当前神

经网络的输

出与教师标

签的误差。

这

里考虑一个

具体的例子

，比如思考教

师标签是（0,

1, 0），Softmax层

的输出是(0.3, 0.2, 0.5)的

情形。因为正

确解标签处

的概率是0.2（20%），这

个

时候的神

经网络未能

进行正确的

识别。此时，Softmax层

的反向传播

传递的

是(0.3, −0.8, 0.5)这

样一个大的

误差。因为这

个大的误差

会向前面的

层传播，

所以

Softmax层前面的层

会从这个大

的误差中学

习到“大”的内

容。

5.6  Affine/Softmax层的实现

153

使用交叉熵

误差作为 softmax函

数的损失函

数后，反向传

播得到

（y1 −

t1, y2 − t2,

y3 − t3）这样

“漂亮”的结果

。实际上，这样

“漂亮”

的结果

并不是偶然

的，而是为了

得到这样的

结果，特意设

计了交叉

熵

误差函数。回

归问题中输

出层使用“恒

等函数”，损失

函数使用

“平

方和误差”，也

是出于同样

的理由（3.5节）。也

就是说，使用

“平

方和误差

”作为“恒等函

数”的损失函

数，反向传播

才能得到（y1 −

t1, y2 − t2,

y3 − t3）这

样“漂亮”的结

果。

再举一个

例子，比如思

考教师标签

是(0,

1, 0)，Softmax层的输出

是(0.01, 

0.99,

0)的情形（这

个神经网络

识别得相当

准确）。此时Softmax层

的反向传播

传递的是(0.01, −0.01, 0)这

样一个小的

误差。这个小

的误差也会

向前面的层

传播，因为误

差很小，所以

Softmax层前面的层

学到的内容

也很“小”。

现在

来进行Softmax-with-Loss层的

实现，实现过

程如下所示

。

class SoftmaxWithLoss:

 def

__init__(self):

 self.loss =

None # 损失

self.y = None #

softmax的输出

self.t = None

# 监督数据（one-hot vector）

def forward(self, x, t):

self.t = t

self.y = softmax(x)

self.loss = cross_entropy_error(self.y, self.t)

return self.loss

 def

backward(self, dout=1):

 batch_size

= self.t.shape[0]

 dx

= (self.y - self.t)

/ batch_size

 return

dx

这

个实现利用

了3.5.2节和4.2.4节中

实现的softmax()和cross_entropy_

error()函

数。因此，这里

的实现非常

简单。请注意

反向传播时

，将要传播

的

值除以批的

大小（batch_size）后，传递

给前面的层

的是单个数

据的误差。

第

5章　误差反向

传播法 154

5.7

误差

反向传播法

的实现

通过

像组装乐高

积木一样组

装上一节中

实现的层，可

以构建神经

网络。

本节我

们将通过组

装已经实现

的层来构建

神经网络。

5.7.1

神

经网络学习

的全貌图

在

进行具体的

实现之前，我

们再来确认

一下神经网

络学习的全

貌图。神

经网

络学习的步

骤如下所示

。

前提

神经网

络中有合适

的权重和偏

置，调整权重

和偏置以便

拟合训练数

据的

过程称

为学习。神经

网络的学习

分为下面4个

步骤。

步骤1（mini-batch）

从

训练数据中

随机选择一

部分数据。

步

骤2（计算梯度

）

计算损失函

数关于各个

权重参数的

梯度。

步骤3（更

新参数）

将权

重参数沿梯

度方向进行

微小的更新

。

步骤4（重复）

重

复步骤1、步骤

2、步骤3。

之前介

绍的误差反

向传播法会

在步骤2中出

现。上一章中

，我们利用数

值微分求得

了这个梯度

。数值微分虽

然实现简单

，但是计算要

耗费较多的

时

间。和需要

花费较多时

间的数值微

分不同，误差

反向传播法

可以快速高

效地

计算梯

度。

5.7  误差反向

传播法的实

现

155

5.7.2　对应误差

反向传播法

的神经网络

的实现

现在

来进行神经

网络的实现

。这里我们要

把2层神经网

络实现为TwoLayerNet。

首

先，将这个类

的实例变量

和方法整理

成表5-1和表5-2。

表

5-1　TwoLayerNet类的实例变

量

实例变量

说明

params 保存神

经网络的参

数的字典型

变量。

params['W1']是第1层

的权重，params['b1']是第

1层的偏置。

params['W2']是

第2层的权重

，params['b2']是第2层的偏

置

layers 保存神经

网络的层的

有序字典型

变量。

以layers['Affine1']、layers['ReLu1']、layers['Affine2']的形

式，

通过有序

字典保存各

个层

lastLayer 神经网

络的最后一

层。

本例中为

SoftmaxWithLoss层

表

5-2　TwoLayerNet类的方

法

方法 说明

__init__(self,

input_size,

hidden_size, output_size,

weight_init_std)

进行初始化

。

参数从头开

始依次是输

入层的神经

元数、隐藏层

的

神经元数

、输出层的神

经元数、初始

化权重时的

高

斯分布的

规模

predict(self, x) 进行识

别（推理）。

参数

x是图像数据

loss(self,

x, t) 计算损失函

数的值。

参数

X是图像数据

、t是正确解标

签

accuracy(self, x, t) 计算识别

精度

numerical_gradient(self, x, t) 通过数

值微分计算

关于权重参

数的梯度（同

上一章）

gradient(self, x, t) 通过

误差反向传

播法计算关

于权重参数

的梯度

这个

类的实现稍

微有一点长

，但是内容和

4.5节的学习算

法的实现有

很

多共通的

部分，不同点

主要在于这

里使用了层

。通过使用层

，获得识别结

果

的处理（predict()）和

计算梯度的

处理（gradient()）只需通

过层之间的

传递就

第

5章

误差反向传

播法 156

能完成

。下面是TwoLayerNet的代

码实现。

import

sys, os

sys.path.append(os.pardir)

import

numpy as np

from

common.layers import *

from

common.gradient import numerical_gradient

from

collections import OrderedDict

class

TwoLayerNet:

 def __init__(self,

input_size, hidden_size, output_size,

weight_init_std=0.01):

 # 初始

化权重

self.params = {}

self.params['W1'] = weight_init_std *

\

 np.random.randn(input_size, hidden_size)

self.params['b1'] = np.zeros(hidden_size)

self.params['W2'] = weight_init_std *

\

 np.random.randn(hidden_size, output_size)

self.params['b2'] = np.zeros(output_size)

# 生成

层

 self.layers

= OrderedDict()

 self.layers['Affine1']

= \

 Affine(self.params['W1'],

self.params['b1'])

 self.layers['Relu1'] =

Relu()

 self.layers['Affine2'] =

\

 Affine(self.params['W2'], self.params['b2'])

self.lastLayer = SoftmaxWithLoss()

def predict(self, x):

for layer in self.layers.values():

x = layer.forward(x)

return x

 #

x:输入数据

, t:监督数据

 def

loss(self, x, t):

y = self.predict(x)

return self.lastLayer.forward(y, t)

def accuracy(self, x, t):

y = self.predict(x)

y = np.argmax(y, axis=1)

if t.ndim != 1

: t = np.argmax(t,

axis=1)

5.7  误

差反向传播

法的实现

157

 accuracy =

np.sum(y == t) /

float(x.shape[0])

 return accuracy

# x:输

入数据, t:监督

数据

def numerical_gradient(self, x, t):

loss_W = lambda W:

self.loss(x, t)

 grads

= {}

 grads['W1']

= numerical_gradient(loss_W, self.params['W1'])

grads['b1'] = numerical_gradient(loss_W, self.params['b1'])

grads['W2'] = numerical_gradient(loss_W, self.params['W2'])

grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

return grads

 def

gradient(self, x, t):

# forward

 self.loss(x,

t)

 # backward

dout = 1

dout = self.lastLayer.backward(dout)

layers = list(self.layers.values())

layers.reverse()

 for layer

in layers:

 dout

= layer.backward(dout)

 #

设定

 grads =

{}

 grads['W1'] =

self.layers['Affine1'].dW

 grads['b1'] =

self.layers['Affine1'].db

 grads['W2'] =

self.layers['Affine2'].dW

 grads['b2'] =

self.layers['Affine2'].db

 return grads

请

注意这个实

现中的粗体

字代码部分

，尤其是将神

经网络的层

保存为

OrderedDict这一

点非常重要

。OrderedDict是有序字典

，“有序”是指它

可以

记住向

字典里添加

元素的顺序

。因此，神经网

络的正向传

播只需按照

添加元

素的

顺序调用各

层的forward()方法就

可以完成处

理，而反向传

播只需要按

照相反的顺

序调用各层

即可。因为Affine层

和ReLU层的内部

会正确处理

正

向传播和

反向传播，所

以这里要做

的事情仅仅

是以正确的

顺序连接各

层，再

按顺序

（或者逆序）调

用各层。

第 5章

误差反向传

播法

158

像这样

通过将神经

网络的组成

元素以层的

方式实现，可

以轻松地构

建神

经网络

。这个用层进

行模块化的

实现具有很

大优点。因为

想另外构建

一个神

经网

络（比如5层、10层

、20层……的大的神

经网络）时，只

需像组装乐

高

积木那样

添加必要的

层就可以了

。之后，通过各

个层内部实

现的正向传

播和

反向传

播，就可以正

确计算进行

识别处理或

学习所需的

梯度。

5.7.3　误差反

向传播法的

梯度确认

到

目前为止，我

们介绍了两

种求梯度的

方法。一种是

基于数值微

分的方

法，另

一种是解析

性地求解数

学式的方法

。后一种方法

通过使用误

差反向传

播

法，即使存在

大量的参数

，也可以高效

地计算梯度

。因此，后文将

不再使

用耗

费时间的数

值微分，而是

使用误差反

向传播法求

梯度。

数值微

分的计算很

耗费时间，而

且如果有误

差反向传播

法的（正确的

）

实现的话，就

没有必要使

用数值微分

的实现了。那

么数值微分

有什么用呢

？

实际上，在确

认误差反向

传播法的实

现是否正确

时，是需要用

到数值微分

的。

数值微分

的优点是实

现简单，因此

，一般情况下

不太容易出

错。而误差

反

向传播法的

实现很复杂

，容易出错。所

以，经常会比

较数值微分

的结果和

误

差反向传播

法的结果，以

确认误差反

向传播法的

实现是否正

确。确认数值

微分求出的

梯度结果和

误差反向传

播法求出的

结果是否一

致（严格地讲

，是

非常相近

）的操作称为

梯度确认（gradient check）。梯

度确认的代

码实现如下

所示（源代码

在ch05/gradient_check.py中）。

import sys, os

sys.path.append(os.pardir)

import numpy as np

from dataset.mnist import load_mnist

from two_layer_net import TwoLayerNet

# 读入数

据

(x_train, t_train),

(x_test, t_test) = \

load_mnist(normalize=True, one_

hot_label =

True)

network = TwoLayerNet(input_size=784,

hidden_size=50, output_size=10)

x_batch =

x_train[:3]

5.7  误差反向

传播法的实

现

159

t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)

grad_backprop = network.gradient(x_batch, t_batch)

# 求各个权

重的绝对误

差的平均值

for key in

grad_numerical.keys():

 diff =

np.average( np.abs(grad_backprop[key] - grad_numerical[key])

)

 print(key +

":" + str(diff))

和以前一样

，读入MNIST数据集

。然后，使用训

练数据的一

部分，确

认数

值微分求出

的梯度和误

差反向传播

法求出的梯

度的误差。这

里误差的计

算方法是求

各个权重参

数中对应元

素的差的绝

对值，并计算

其平均值。运

行

上面的代

码后，会输出

如下结果。

b1:9.70418809871e-13

W2:8.41139039497e-13

b2:1.1945999745e-10

W1:2.2232446644e-13

从

这个结果可

以看出，通过

数值微分和

误差反向传

播法求出的

梯度的差

非

常小。比如，第

1层的偏置的

误差是9.7e-13（0.00000000000097）。这样

一来，

我们就

知道了通过

误差反向传

播法求出的

梯度是正确

的，误差反向

传播法的

实

现没有错误

。

数值微分和

误差反向传

播法的计算

结果之间的

误差为 0是很

少见的。

这是

因为计算机

的计算精度

有限（比如，32位

浮点数）。受到

数值精

度的

限制，刚才的

误差一般不

会为 0，但是如

果实现正确

的话，可

以期

待这个误差

是一个接近

0的很小的值

。如果这个值

很大，就说

明

误差反向传

播法的实现

存在错误。

5.7.4　使

用误差反向

传播法的学

习

最后，我们

来看一下使

用了误差反

向传播法的

神经网络的

学习的实现

。

和之前的实

现相比，不同

之处仅在于

通过误差反

向传播法求

梯度这一点

。这

里只列出

了代码，省略

了说明（源代

码在ch05/train_neuralnet.py中）。

第 5章

误差反向传

播法

160

import sys, os

sys.path.append(os.pardir)

import numpy as

np

from dataset.mnist import

load_mnist

from two_layer_net import

TwoLayerNet

# 读入数

据

(x_train,

t_train), (x_test, t_test) =

\

 load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50,

output_size=10)

iters_num = 10000

train_size = x_train.shape[0]

batch_size

= 100

learning_rate =

0.1

train_loss_list = []

train_acc_list = []

test_acc_list

= []

iter_per_epoch =

max(train_size / batch_size, 1)

for i in range(iters_num):

batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

# 通过误差

反向传播法

求梯度

 grad

= network.gradient(x_batch, t_batch)

# 更新

for key

in ('W1', 'b1', 'W2',

'b2'):

 network.params[key] -=

learning_rate * grad[key]

loss = network.loss(x_batch, t_batch)

train_loss_list.append(loss)

 if i

% iter_per_epoch == 0:

train_acc = network.accuracy(x_train, t_train)

test_acc = network.accuracy(x_test, t_test)

train_acc_list.append(train_acc)

 test_acc_list.append(test_acc)

print(train_acc, test_acc)

5.8

小结 161

5.8 小结

本

章我们介绍

了将计算过

程可视化的

计算图，并使

用计算图，介

绍了神

经网

络中的误差

反向传播法

，并以层为单

位实现了神

经网络中的

处理。我们

学

过的层有ReLU层

、Softmax-with-Loss层、Affine层、Softmax层等，这

些层中实现

了forward和backward方法，通

过将数据正

向和反向地

传播，可

以高

效地计算权

重参数的梯

度。通过使用

层进行模块

化，神经网络

中可以自

由

地组装层，轻

松构建出自

己喜欢的网

络。

本章所学

的内容

• 通过

使用计算图

，可以直观地

把握计算过

程。

• 计算图的

节点是由局

部计算构成

的。局部计算

构成全局计

算。

• 计算图的

正向传播进

行一般的计

算。通过计算

图的反向传

播，可以

计算

各个节点的

导数。

• 通过将

神经网络的

组成元素实

现为层，可以

高效地计算

梯度（反向传

播法）。

•

通过比

较数值微分

和误差反向

传播法的结

果，可以确认

误差反向传

播法的实现

是否正确（梯

度确认）。

第6章

与学习相关

的技巧

本章

将介绍神经

网络的学习

中的一些重

要观点，主题

涉及寻找最

优权重

参数

的最优化方

法、权重参数

的初始值、超

参数的设定

方法等。此外

，为了

应对过

拟合，本章还

将介绍权值

衰减、Dropout等正则

化方法，并进

行实现。

最后

将对近年来

众多研究中

使用的Batch Normalization方法

进行简单的

介绍。

使用本

章介绍的方

法，可以高效

地进行神经

网络（深度学

习）的学习，提

高

识别精度

。让我们一起

往下看吧！

6.1 参

数的更新

神

经网络的学

习的目的是

找到使损失

函数的值尽

可能小的参

数。这是寻

找

最优参数的

问题，解决这

个问题的过

程称为最优

化（optimization）。遗憾的是

，

神经网络的

最优化问题

非常难。这是

因为参数空

间非常复杂

，无法轻易找

到

最优解（无

法使用那种

通过解数学

式一下子就

求得最小值

的方法）。而且

，在

深度神经

网络中，参数

的数量非常

庞大，导致最

优化问题更

加复杂。

在前

几章中，为了

找到最优参

数，我们将参

数的梯度（导

数）作为了线

索。

使用参数

的梯度，沿梯

度方向更新

参数，并重复

这个步骤多

次，从而逐渐

靠

近最优参

数，这个过程

称为随机梯

度下降法（stochastic gradient

descent），

简

称SGD。SGD是一个简

单的方法，不

过比起胡乱

地搜索参数

空间，也算是

“聪

明”的方法

。但是，根据不

同的问题，也

存在比SGD更加

聪明的方法

。本节

第 6章　与

学习相关的

技巧 164

我们将

指出SGD的缺点

，并介绍SGD以外

的其他最优

化方法。

6.1.1　探险

家的故事

进

入正题前，我

们先打一个

比方，来说明

关于最优化

我们所处的

状况。

有一个

性情古怪的

探险家。他在

广袤的干旱

地带旅行，坚

持寻找幽

深

的山谷。他的

目标是要到

达最深的谷

底（他称之为

“至深之地”）。这

也是他旅行

的目的。并且

，他给自己制

定了两个严

格的“规定”：一

个

是不看地

图；另一个是

把眼睛蒙上

。因此，他并不

知道最深的

谷底在这

个

广袤的大地

的何处，而且

什么也看不

见。在这么严

苛的条件下

，这位

探险家

如何前往“至

深之地”呢？他

要如何迈步

，才能迅速找

到“至深

之地

”呢？

寻找最优

参数时，我们

所处的状况

和这位探险

家一样，是一

个漆黑的世

界。我们必须

在没有地图

、不能睁眼的

情况下，在广

袤、复杂的地

形中寻找

“至

深之地”。大家

可以想象这

是一个多么

难的问题。

在

这么困难的

状况下，地面

的坡度显得

尤为重要。探

险家虽然看

不到周

围的

情况，但是能

够知道当前

所在位置的

坡度（通过脚

底感受地面

的倾斜状况

）。

于是，朝着当

前所在位置

的坡度最大

的方向前进

，就是SGD的策略

。勇敢

的探险

家心里可能

想着只要重

复这一策略

，总有一天可

以到达“至深

之地”。

6.1.2　SGD

让大家

感受了最优

化问题的难

度之后，我们

再来复习一

下SGD。用数

学式

可以将SGD写成

如下的式（6.1）。

（6.1）

这

里把需要更

新的权重参

数记为W，把损

失函数关于

W的梯度记为

。

η表示学习率

，实际上会取

0.01或0.001这些事先

决定好的值

。式子中的←

6.1

参

数的更新  165

表

示用右边的

值更新左边

的值。如式（6.1）所

示，SGD是朝着梯

度方向只前

进一定距离

的简单方法

。现在，我们将

SGD实现为一个

Python类（为方便

后

面使用，我们

将其实现为

一个名为SGD的

类）。

class SGD:

def __init__(self, lr=0.01):

self.lr = lr

def update(self, params, grads):

for key in params.keys():

params[key] -= self.lr *

grads[key]

这里，进行

初始化时的

参数lr表示learning rate（学

习率）。这个学

习率

会保存

为实例变量

。此外，代码段

中还定义了

update(params,

grads)方法，

这个方

法在SGD中会被

反复调用。参

数params和grads（与之前

的神经网络

的实现一样

）是字典型变

量，按params['W1']、grads['W1']的形式

，分别保

存了

权重参数和

它们的梯度

。

使用这个SGD类

，可以按如下

方式进行神

经网络的参

数的更新（下

面的

代码是

不能实际运

行的伪代码

）。

network = TwoLayerNet(...)

optimizer = SGD()

for

i in range(10000):

...

 x_batch, t_batch

= get_mini_batch(...) # mini-batch

grads = network.gradient(x_batch, t_batch)

params = network.params

optimizer.update(params, grads)

 ...

这里首次出

现的变量名

optimizer表示“进行最

优化的人”的

意思，这里

由

SGD承担这个角

色。参数的更

新由optimizer负责完

成。我们在这

里需要

做的

只是将参数

和梯度的信

息传给optimizer。

像这

样，通过单独

实现进行最

优化的类，功

能的模块化

变得更简单

。

比如，后面我

们马上会实

现另一个最

优化方法Momentum，它

同样会实现

成拥有update(params, grads)这个

共同方法的

形式。这样一

来，只需要将

第

6章　与学习

相关的技巧

166

optimizer =

SGD()这一语句换

成optimizer = Momentum()，就可以从

SGD切

换为Momentum。

很多

深度学习框

架都实现了

各种最优化

方法，并且提

供了可以简

单

切换这些

方法的构造

。比如 Lasagne深度学

习框架，在updates.py

这

个文件中以

函数的形式

集中实现了

最优化方法

。用户可以从

中选

择自己

想用的最优

化方法。

6.1.3　SGD的缺

点

虽然SGD简单

，并且容易实

现，但是在解

决某些问题

时可能没有

效率。

这里，在

指出SGD的缺点

之际，我们来

思考一下求

下面这个函

数的最小值

的问题。

（6.2）

如图

6-1所示，式（6.2）表示

的函数是向

x轴方向延伸

的“碗”状函数

。

实际上，式（6.2）的

等高线呈向

x轴方向延伸

的椭圆状。

图

6-1 的图形（左图

）和它的等高

线（右图）

120

100

80

60

40

20

0

0

10

10

10

10

5

5

5

5 x x

y

z

0 0

0 −5 −5

−5

−5 −10

−10

−10

−10

y

6.1 参数

的更新

167

现在

看一下式（6.2）表

示的函数的

梯度。如果用

图表示梯度

的话，则如

图

6-2所示。这个梯

度的特征是

，y轴方向上大

，x轴方向上小

。换句话说，

就

是y轴方向的

坡度大，而x轴

方向的坡度

小。这里需要

注意的是，虽

然式

（6.2）的最小

值在(x, y) = (0,

0)处，但是

图6-2中的梯度

在很多地方

并没有指

向

(0, 0)。

图6-2

的梯度

4

10 5

0 −5 −10

2

0

−2

−4

x

我

们来尝试对

图6-1这种形状

的函数应用

SGD。从(x, y) = (−7.0,

2.0)处

（初始值

）开始搜索，结

果如图6-3所示

。

在图6-3中，SGD呈“之

”字形移动。这

是一个相当

低效的路径

。也就是说，

SGD的

缺点是，如果

函数的形状

非均向（anisotropic），比如

呈延伸状，搜

索

的路径就

会非常低效

。因此，我们需

要比单纯朝

梯度方向前

进的SGD更聪

明

的方法。SGD低效

的根本原因

是，梯度的方

向并没有指

向最小值的

方向。

为了改

正SGD的缺点，下

面我们将介

绍Momentum、AdaGrad、Adam这3

种方法

来取代SGD。我们

会简单介绍

各个方法，并

用数学式和

Python进行实现。

y

  第

6章

与学习相

关的技巧 168

图

6-3　基于SGD的最优

化的更新路

径：呈“之”字形

朝最小值(0,

0)移

动，效率低

10

10 5

0 −5 −10

5

0

−5

−10

x

SGD

6.1.4　Momentum

Momentum是

“动量”的意思

，和物理有关

。用数学式表

示Momentum方

法，如下

所示。

（6.3）

（6.4）

和前面

的SGD一样，W表示

要更新的权

重参数，

表示

损失函数关

于W的梯度，η表

示学习率。这

里新出现了

一个变量v，对

应物理上的

速度。

式（6.3）表示

了物体在梯

度方向上受

力，在这个力

的作用下，物

体的速度增

加这一物理

法则。如图6-4所

示，Momentum方法给人

的感觉就像

是小球在

地

面上滚动。

y

6.1  参

数的更新

169

图

6-4 Momentum：小球在斜面

上滚动

式（6.3）中

有αv这一项。在

物体不受任

何力时，该项

承担使物体

逐渐减

速的

任务（α设定为

0.9之类的值），对

应物理上的

地面摩擦或

空气阻力。下

面是Momentum的代码

实现（源代码

在common/optimizer.py中）。

class Momentum:

def __init__(self, lr=0.01, momentum=0.9):

self.lr = lr

self.momentum = momentum

self.v = None

def update(self, params, grads):

if self.v is None:

self.v = {}

for key, val in

params.items():

 self.v[key] =

np.zeros_like(val)

 for key

in params.keys():

 self.v[key]

= self.momentum*self.v[key] - self.lr*grads[key]

params[key] += self.v[key]

实例变

量v会保存物

体的速度。初

始化时，v中什

么都不保存

，但当第

一次

调用update()时，v会以

字典型变量

的形式保存

与参数结构

相同的数据

。

剩余的代码

部分就是将

式（6.3）、式（6.4）写出来

，很简单。

现在

尝试使用Momentum解

决式（6.2）的最优

化问题，如图

6-5所示。

图6-5中，更

新路径就像

小球在碗中

滚动一样。和

SGD相比，我们发

现

“之”字形的

“程度”减轻了

。这是因为虽

然x轴方向上

受到的力非

常小，但

是一

直在同一方

向上受力，所

以朝同一个

方向会有一

定的加速。反

过来，虽

然y轴

方向上受到

的力很大，但

是因为交互

地受到正方

向和反方向

的力，它

们会

互相抵消，所

以y轴方向上

的速度不稳

定。因此，和SGD时

的情形相比

，

可以更快地

朝x轴方向靠

近，减弱“之”字

形的变动程

度。

  第

6章　与学

习相关的技

巧 170

图6-5

基于Momentum的

最优化的更

新路径

10

10 5

0 −5 −10

5

0

−5

−10

x

Momentum

6.1.5　AdaGrad

在神

经网络的学

习中，学习率

（数学式中记

为η）的值很重

要。学习率过

小，

会导致学

习花费过多

时间；反过来

，学习率过大

，则会导致学

习发散而不

能

正确进行

。

在关于学习

率的有效技

巧中，有一种

被称为学习

率衰减（learning rate

decay）的方

法，即随着学

习的进行，使

学习率逐渐

减小。实际上

，一开始“多”

学

，然后逐渐“少

”学的方法，在

神经网络的

学习中经常

被使用。

逐渐

减小学习率

的想法，相当

于将“全体”参

数的学习率

值一起降低

。

而AdaGrad

[6]进一步发

展了这个想

法，针对“一个

一个”的参数

，赋予其“定

制

”的值。

AdaGrad会为参

数的每个元

素适当地调

整学习率，与

此同时进行

学习

（AdaGrad的Ada来自

英文单词Adaptive，即

“适当的”的意

思）。下面，让

我

们用数学式

表示AdaGrad的更新

方法。

y

6.1 参数的

更新

171

（6.5）

（6.6）

和前面

的SGD一样，W表示

要更新的权

重参数，

表示

损失函数关

于W的梯度，η表

示学习率。这

里新出现了

变量h，如式(6.5)所

示，它保

存了

以前的所有

梯度值的平

方和（式（6.5）中的

表示对应矩

阵元素的乘

法）。

然后，在更

新参数时，通

过乘以 ，就可

以调整学习

的尺度。这意

味着，

参数的

元素中变动

较大（被大幅

更新）的元素

的学习率将

变小。也就是

说，

可以按参

数的元素进

行学习率衰

减，使变动大

的参数的学

习率逐渐减

小。

AdaGrad会记录过

去所有梯度

的平方和。因

此，学习越深

入，更新

的幅

度就越小。实

际上，如果无

止境地学习

，更新量就会

变为

0，

完全不

再更新。为了

改善这个问

题，可以使用

RMSProp [7]方法。

RMSProp方法并

不是将过去

所有的梯度

一视同仁地

相加，而是逐

渐

地遗忘过

去的梯度，在

做加法运算

时将新梯度

的信息更多

地反映出来

。

这种操作从

专业上讲，称

为“指数移动

平均”，呈指数

函数式地减

小

过去的梯

度的尺度。

现

在来实现

AdaGrad。AdaGrad 的

实 现 过

程 如

下 所 示（源

代

码 在

common/optimizer.py中）。

class

AdaGrad:

 def __init__(self,

lr=0.01):

 self.lr =

lr

 self.h =

None

 def update(self,

params, grads):

 if

self.h is None:

self.h = {}

for key, val in

params.items():

 self.h[key] =

np.zeros_like(val)

 for key

in params.keys():

 self.h[key]

+= grads[key] * grads[key]

params[key] -= self.lr *

grads[key] / (np.sqrt(self.h[key]) +

1e-7)

  第

6章

与学习相关

的技巧 172

这里

需要注意的

是，最后一行

加上了微小

值1e-7。这是为了

防止当

self.h[key]中有

0时，将0用作除

数的情况。在

很多深度学

习的框架中

，这

个微小值

也可以设定

为参数，但这

里我们用的

是1e-7这个固定

值。

现在，让我

们试着使用

AdaGrad解决式（6.2）的最

优化问题，结

果如图6-6

所示

。

图6-6

基于AdaGrad的最

优化的更新

路径

10

10 5

0 −5 −10

5

0

−5

−10

x

AdaGrad

由图6-6的

结果可知，函

数的取值高

效地向着最

小值移动。由

于y轴方

向上

的梯度较大

，因此刚开始

变动较大，但

是后面会根

据这个较大

的变动按

比

例进行调整

，减小更新的

步伐。因此，y轴

方向上的更

新程度被减

弱，“之”

字形的

变动程度有

所衰减。

6.1.6　Adam

Momentum参照

小球在碗中

滚动的物理

规则进行移

动，AdaGrad为参

数的

每个元素适

当地调整更

新步伐。如果

将这两个方

法融合在一

起会怎么样

y

6.1 参数的更新

173

呢？这就是Adam[8]方

法的基本思

路 A。

Adam是2015年提出

的新方法。它

的理论有些

复杂，直观地

讲，就是融

合

了Momentum和AdaGrad的方法

。通过组合前

面两个方法

的优点，有望

实现参数空

间的高效搜

索。此外，进行

超参数的“偏

置校正”也是

Adam的特征。

这里

不再进行过

多的说明，详

细内容请参

考原作者的

论文[8]。关于Python

的

实现，common/optimizer.py中将其

实现为了Adam类

，有兴趣的读

者可以参考

。

现在，我们试

着使用Adam解决

式（6.2）的最优化

问题，结果如

图6-7所示。

图6-7

基

于Adam的最优化

的更新路径

Adam

10

10 5

0 −5 −10

5

0

−5

−10

x

在图6-7中，基于

Adam的更新过程

就像小球在

碗中滚动一

样。虽然

Momentun也有

类似的移动

，但是相比之

下，Adam的小球左

右摇晃的程

度

有所减轻

。这得益于学

习的更新程

度被适当地

调整了。

A

这里

关于Adam方法的

说明只是一

个直观的说

明，并不完全

正确。详细内

容请参考原

作者的论文

。

y

第 6章　与学习

相关的技巧

174

Adam会设置

3个超

参数。一个是

学习率（论文

中以α出现），另

外两

个是一

次momentum系数β1和二

次momentum系数β2。根据

论文，

标准的

设定值是β1为

0.9，β2 为

0.999。设置了这

些值后，大多

数情

况下都

能顺利运行

。

6.1.7　使用哪种更

新方法呢

到

目前为止，我

们已经学习

了4种更新参

数的方法。这

里我们来比

较一

下这4种

方法（源代码

在ch06/optimizer_compare_naive.py中）。

如图6-8所

示，根据使用

的方法不同

，参数更新的

路径也不同

。只看这

个图

的话，AdaGrad似乎是

最好的，不过

也要注意，结

果会根据要

解决的问

题

而变。并且，很

显然，超参数

（学习率等）的

设定值不同

，结果也会发

生变化。

图6-8　最

优化方法的

比较：SGD、Momentum、AdaGrad、Adam

x

−10 −5 0 5

10

x

−10 −5

0 5 10

x

−10 −5 0 5

10

−10

−5

0

5

10

y

−10

−5

0

5

10

y

SGD Momentum

AdaGrad

Adam

−10

−5

0

5

10

y

x

−10 −5 0 5

10

−10

−5

0

5

10

y

6.1

参数的

更新  175

上面我

们介绍了SGD、Momentum、AdaGrad、Adam这

4种方法，那

么

用哪种方法

好呢？非常遗

憾，（目前）并不

存在能在所

有问题中都

表现良好

的

方法。这4种方

法各有各的

特点，都有各

自擅长解决

的问题和不

擅长解决

的

问题。

很多研

究中至今仍

在使用SGD。Momentum和AdaGrad也

是值得一试

的方法。最近

，很多研究人

员和技术人

员都喜欢用

Adam。本书将主要

使用

SGD或者Adam，读

者可以根据

自己的喜好

多多尝试。

6.1.8　基

于MNIST数据集的

更新方法的

比较

我

们 以

手 写 数

字 识

别 为 例，比

较

前 面 介 绍

的

SGD、Momentum、

AdaGrad、Adam这4种方法，并

确认不同的

方法在学习

进展上有多

大程度

的差

异。先来看一

下结果，如图

6-9所示（源代码

在ch06/optimizer_compare_

mnist.py中）。

图6-9 基于

MNIST数据集的4种

更新方法的

比较：横轴表

示学习的迭

代次数（iteration），

纵轴

表示损失函

数的值（loss）

1.0

0.8

0.6

0.4

0.2

0.0

0 500 1000

iterations

1500 2000

Adam

SGD

AdaGrad

Momentum

这个

实验以一个

5层神经网络

为对象，其中

每层有100个神

经元。激活

函

数使用的是

ReLU。 loss

第 6章　与学习

相关的技巧

176

从图6-9的结果

中可知，与SGD相

比，其他3种方

法学习得更

快，而且

速度

基本相同，仔

细看的话，AdaGrad的

学习进行得

稍微快一点

。这个实验

需

要注意的地

方是，实验结

果会随学习

率等超参数

、神经网络的

结构（几层

深

等）的不同而

发生变化。不

过，一般而言

，与SGD相比，其他

3种方法可

以

学习得更快

，有时最终的

识别精度也

更高。

6.2 权重的

初始值

在神

经网络的学

习中，权重的

初始值特别

重要。实际上

，设定什么样

的

权重初始

值，经常关系

到神经网络

的学习能否

成功。本节将

介绍权重初

始值

的推荐

值，并通过实

验确认神经

网络的学习

是否会快速

进行。

6.2.1　可以将

权重初始值

设为0吗

后面

我们会介绍

抑制过拟合

、提高泛化能

力的技巧——权

值衰减（weight

decay）。简单

地说，权值衰

减就是一种

以减小权重

参数的值为

目的进行学

习

的方法。通

过减小权重

参数的值来

抑制过拟合

的发生。

如果

想减小权重

的值，一开始

就将初始值

设为较小的

值才是正途

。实际上，

在这

之前的权重

初始值都是

像0.01

* np.random.randn(10, 100)这样，使用

由高斯分布

生成的值乘

以0.01后得到的

值（标准差为

0.01的高斯分布

）。

如果我们把

权重初始值

全部设为0以

减小权重的

值，会怎么样

呢？从结

论来

说，将权重初

始值设为0不

是一个好主

意。事实上，将

权重初始值

设为

0的话，将

无法正确进

行学习。

为什

么不能将权

重初始值设

为0呢？严格地

说，为什么不

能将权重初

始

值设成一

样的值呢？这

是因为在误

差反向传播

法中，所有的

权重值都会

进行

相同的

更新。比如，在

2层神经网络

中，假设第1层

和第2层的权

重为0。这

样一

来，正向传播

时，因为输入

层的权重为

0，所以第2层的

神经元全部

会

被传递相

同的值。第2层

的神经元中

全部输入相

同的值，这意

味着反向传

播

时第2层的

权重全部都

会进行相同

的更新（回忆

一下“乘法节

点的反向传

播”

6.2 权重的初

始值  177

的内容

）。因此，权重被

更新为相同

的值，并拥有

了对称的值

（重复的值）。

这

使得神经网

络拥有许多

不同的权重

的意义丧失

了。为了防止

“权重均一化

”

（严格地讲，是

为了瓦解权

重的对称结

构），必须随机

生成初始值

。

6.2.2

隐藏层的激

活值的分布

观察隐藏层

的激活值 A

（激

活函数的输

出数据）的分

布，可以获得

很多启

发。这

里，我们来做

一个简单的

实验，观察权

重初始值是

如何影响隐

藏层的

激活

值的分布的

。这里要做的

实验是，向一

个5层神经网

络（激活函数

使用

sigmoid函数）传

入随机生成

的输入数据

，用直方图绘

制各层激活

值的数据分

布。这个实验

参考了斯坦

福大学的课

程CS231n [5]。

进行实验

的源代码在

ch06/weight_init_activation_histogram.py中，下

面展示

部分代码。

import numpy as

np

import matplotlib.pyplot as

plt

def sigmoid(x):

return 1 / (1

+ np.exp(-x))

x =

np.random.randn(1000, 100) # 1000个

数据

node_num = 100 #

各隐藏

层的节点（神

经元）数

hidden_layer_size = 5

# 隐藏

层有5层

activations =

{} # 激活

值的结果保

存在这里

for

i in range(hidden_layer_size):

if i != 0:

x = activations[i-1]

w = np.random.randn(node_num, node_num)

* 1

 z

= np.dot(x, w)

a = sigmoid(z) #

sigmoid函

数

 activations[i] =

a

A 这里我们

将激活函数

的输出数据

称为“激活值

”，但是有的文

献中会将在

层之间流动

的数据也称

为“激

活值”。

第

6章　与学习相

关的技巧 178

这

里假设神经

网络有5层，每

层有100个神经

元。然后，用高

斯分布随

机

生成1000个数据

作为输入数

据，并把它们

传给5层神经

网络。激活函

数使

用sigmoid函数

，各层的激活

值的结果保

存在activations变量中

。这个代码

段

中需要注意

的是权重的

尺度。虽然这

次我们使用

的是标准差

为1的高斯分

布，但实验的

目的是通过

改变这个尺

度（标准差），观

察激活值的

分布如何变

化。现在，我们

将保存在activations中

的各层数据

画成直方图

。

# 绘制直方图

for i, a

in activations.items():

 plt.subplot(1,

len(activations), i+1)

 plt.title(str(i+1)

+ "-layer")

 plt.hist(a.flatten(),

30, range=(0,1))

plt.show()

运行这段代

码后，可以得

到图6-10的直方

图。

图6-10　使用标

准差为1的高

斯分布作为

权重初始值

时的各层激

活值的分布

40000

35000

30000

25000

20000

15000

10000

5000

1-layer 2-layer 3-layer

4-layer 5-layer

0

0.0

0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4

0.6 0.8 1.0 0.0

0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4

0.6 0.8 1.0 0.0

0.2 0.4 0.6 0.8

1.0

从图6-10可知，各

层的激活值

呈偏向0和1的

分布。这里使

用的sigmoid

函数是

S型函数，随着

输出不断地

靠近0（或者靠

近1），它的导数

的值逐渐接

近0。因此，偏向

0和1的数据分

布会造成反

向传播中梯

度的值不断

变小，最

后消

失。这个问题

称为梯度消

失（gradient

vanishing）。层次加深

的深度学习

中，梯度消失

的问题可能

会更加严重

。

下面，将权重

的标准差设

为0.01，进行相同

的实验。实验

的代码只需

要

把设定权

重初始值的

地方换成下

面的代码即

可。

6.2

权重的初

始值  179

#

w = np.random.randn(node_num, node_num)

* 1

w =

np.random.randn(node_num, node_num) * 0.01

来看一

下结果。使用

标准差为0.01的

高斯分布时

，各层的激活

值的分布

如

图6-11所示。

图6-11　使

用标准差为

0.01的高斯分布

作为权重初

始值时的各

层激活值的

分布

40000

35000

30000

25000

20000

15000

10000

5000

1-layer 2-layer 3-layer 4-layer

5-layer

0

0.0 0.2

0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2

0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2

0.4 0.6 0.8 1.0

这次呈

集中在0.5附近

的分布。因为

不像刚才的

例子那样偏

向0和1，所

以不

会发生梯度

消失的问题

。但是，激活值

的分布有所

偏向，说明在

表现力

上会

有很大问题

。为什么这么

说呢？因为如

果有多个神

经元都输出

几乎相同

的

值，那它们就

没有存在的

意义了。比如

，如果100个神经

元都输出几

乎相

同的值

，那么也可以

由1个神经元

来表达基本

相同的事情

。因此，激活值

在

分布上有

所偏向会出

现“表现力受

限”的问题。

各

层的激活值

的分布都要

求有适当的

广度。为什么

呢？因为通过

在各层间传

递多样性的

数据，神经网

络可以进行

高效的学习

。反

过来，如果

传递的是有

所偏向的数

据，就会出现

梯度消失或

者“表

现力受

限”的问题，导

致学习可能

无法顺利进

行。

接着，我们

尝试使用Xavier Glorot等

人的论文[9]中

推荐的权重

初始值（俗

称

“Xavier初始值”）。现在

，在一般的深

度学习框架

中，Xavier初始值已

被

作为标准

使用。比如，Caffe框

架中，通过在

设定权重初

始值时赋予

xavier参数，

就可以

使用Xavier初始值

。

Xavier的论文中，为

了使各层的

激活值呈现

出具有相同

广度的分布

，推

第 6章　与学

习相关的技

巧 180

导了合适

的权重尺度

。推导出的结

论是，如果前

一层的节点

数为n，则初始

值使用标准

差为 的分布

A

（图6-12）。

图6-12

Xavier初始值

：与前一层有

n个节点连接

时，初始值使

用标准差为

的分布

n 个节

点

使用标准

差为

的高斯

分布进行初

始化

使用Xavier初

始值后，前一

层的节点数

越多，要设定

为目标节点

的初始

值的

权重尺度就

越小。现在，我

们使用Xavier初始

值进行实验

。进行实验的

代码只需要

将设定权重

初始值的地

方换成如下

内容即可（因

为此处所有

层的

节点数

都是100，所以简

化了实现）。

node_num = 100 #

前

一层的节点

数

w = np.random.randn(node_num,

node_num) / np.sqrt(node_num)

使用Xavier初始

值后的结果

如图6-13所示。从

这个结果可

知，越是后

面

的层，图像变

得越歪斜，但

是呈现了比

之前更有广

度的分布。因

为各层间

传

递的数据有

适当的广度

，所以sigmoid函数的

表现力不受

限制，有望进

行

高效的学

习。

A

Xavier的论文中

提出的设定

值，不仅考虑

了前一层的

输入节点数

量，还考虑了

下一层的输

出节点数量

。

但是，Caffe等框架

的实现中进

行了简化，只

使用了这里

所说的前一

层的输入节

点进行计算

。

6.2 权重的初始

值

181

图6-13　使用Xavier初

始值作为权

重初始值时

的各层激活

值的分布

6000

5000

4000

3000

2000

1000

1-layer 2-layer 3-layer

4-layer 5-layer

0

0.0

0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4

0.6 0.8 1.0 0.0

0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4

0.6 0.8 1.0 0.0

0.2 0.4 0.6 0.8

1.0

图

6-13的分布中，后

面的层的分

布呈稍微歪

斜的形状。如

果用tanh

函数（双

曲线函数）代

替sigmoid函数，这个

稍微歪斜的

问题就能得

到改善。实际

上，使用tanh函数

后，会呈漂亮

的吊钟型分

布。tanh

函数和sigmoid函

数同是

S型曲

线函数，但tanh函

数是关于原

点(0, 0)

对称的 S型

曲线，而sigmoid函数

是关于(x,

y)=(0, 0.5)对称

的S型曲

线。众

所周知，用作

激活函数的

函数最好具

有关于原点

对称的性质

。

6.2.3

ReLU的权重初始

值

Xavier初始值是

以激活函数

是线性函数

为前提而推

导出来的。因

为

sigmoid函数和tanh函

数左右对称

，且中央附近

可以视作线

性函数，所以

适

合使用Xavier初

始值。但当激

活函数使用

ReLU时，一般推荐

使用ReLU专

用的

初始值，也就

是Kaiming He等人推荐

的初始值，也

称为“He初始值

”[10]。

当前一层的

节点数为n时

，He初始值使用

标准差为 的

高斯分布。当

Xavier初始值是

时

，（直观上）可以

解释为，因为

ReLU的负值区域

的值

为0，为了

使它更有广

度，所以需要

2倍的系数。

现

在来看一下

激活函数使

用ReLU时激活值

的分布。我们

给出了3个实

验的结果（图

6-14），依次是权重

初始值为标

准差是0.01的高

斯分布（下文

简

写为“std

= 0.01”）时、初

始值为Xavier初始

值时、初始值

为ReLU专用的

“He初

始值”时的结

果。

第 6章　与学

习相关的技

巧 182

图6-14　激活函

数使用ReLU时,不

同权重初始

值的激活值

分布的变化

权重初始值

为标准差是

0.01 的高斯分布

时

权重初始

值为

Xavier 初始值

时

权重初始

值为 He

初始值

时

0.0 0.2 0.4

0.6 0.8 1.0 0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0 0.2 0.4 0.6

0.8 1.0

2-layer

0.0

0.2 0.4 0.6 0.8

1.0

3-layer

0.0 0.2

0.4 0.6 0.8 1.0

4-layer

0.0 0.2 0.4

0.6 0.8 1.0

5-layer

0.0 0.2 0.4 0.6

0.8 1.0 0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0

0.2 0.4 0.6 0.8

1.0

2-layer

0.0 0.2

0.4 0.6 0.8 1.0

3-layer

0.0 0.2 0.4

0.6 0.8 1.0

4-layer

0.0 0.2 0.4 0.6

0.8 1.0

5-layer

0.0

0.2 0.4 0.6 0.8

1.0 0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0 0.2

0.4 0.6 0.8 1.0

2-layer

0.0 0.2 0.4

0.6 0.8 1.0

3-layer

0.0 0.2 0.4 0.6

0.8 1.0

4-layer

0.0

0.2 0.4 0.6 0.8

1.0

5-layer

观察实验

结果可知，当

“std =

0.01”时，各层的激

活值非常小

A。神经网

络上

传递的是非

常小的值，说

明逆向传播

时权重的梯

度也同样很

小。这是很

严

重的问题，实

际上学习基

本上没有进

展。

A

各层激活

值的分布平

均值如下。1层

: 0.0396，2层: 0.00290，3层: 0.000197，4层:

1.32e-5，5层: 9.46e-7。

6.2 权

重的初始值



183

接下来是初

始值为Xavier初始

值时的结果

。在这种情况

下，随着层的

加深，

偏向一

点点变大。实

际上，层加深

后，激活值的

偏向变大，学

习时会出现

梯

度消失的

问题。而当初

始值为He初始

值时，各层中

分布的广度

相同。由于

即

便层加深，数

据的广度也

能保持不变

，因此逆向传

播时，也会传

递合适的值

。

总结一下，当

激活函数使

用ReLU时，权重初

始值使用He初

始值，当

激活

函数为sigmoid或tanh等

S型曲线函数

时，初始值使

用Xavier初始值。

这

是目前的最

佳实践。

6.2.4　基于

MNIST数据集的权

重初始值的

比较

下面通

过实际的数

据，观察不同

的权重初始

值的赋值方

法会在多大

程度

上影响

神经网络的

学习。这里，我

们基于std

= 0.01、Xavier初始

值、He初

始值进

行实验（源代

码在ch06/weight_init_compare.py中）。先来

看一下结果

，

如图6-15所示。

2.5

2.0

1.5

1.0

0.5

0.0

0 500

iterations

1000 1500 2000

He

std = 0.01

Xavier

图

6-15 基于MNIST数据集

的权重初始

值的比较：横

轴是学习的

迭代次数（iterations），

纵

轴是损失函

数的值（loss）

loss

  第

6章

与学习相关

的技巧 184

这个

实验中，神经

网络有5层，每

层有100个神经

元，激活函数

使用的

是ReLU。从

图6-15的结果可

知，std

= 0.01时完全无

法进行学习

。这和刚

才观

察到的激活

值的分布一

样，是因为正

向传播中传

递的值很小

（集中在0

附近

的数据）。因此

，逆向传播时

求到的梯度

也很小，权重

几乎不进行

更新。

相反，当

权重初始值

为Xavier初始值和

He初始值时，学

习进行得很

顺利。

并且，我

们发现He初始

值时的学习

进度更快一

些。

综上，在神

经网络的学

习中，权重初

始值非常重

要。很多时候

权重初始

值

的设定关系

到神经网络

的学习能否

成功。权重初

始值的重要

性容易被忽

视，

而任何事

情的开始（初

始值）总是关

键的，因此在

结束本节之

际，再次强调

一下权重初

始值的重要

性。

6.3 Batch Normalization

在上一节

，我们观察了

各层的激活

值分布，并从

中了解到如

果设定了合

适的权重初

始值，则各层

的激活值分

布会有适当

的广度，从而

可以顺利地

进

行学习。那

么，为了使各

层拥有适当

的广度，“强制

性”地调整激

活值的分布

会怎样呢？实

际上，Batch Normalization[11]方法就

是基于这个

想法而产生

的。

6.3.1

Batch Normalization 的算法

Batch

Normalization（下

文简称 Batch Norm）是 2015

年

提出的方法

。

Batch Norm虽然是一个

问世不久的

新方法，但已

经被很多研

究人员和技

术

人员广泛

使用。实际上

，看一下机器

学习竞赛的

结果，就会发

现很多通过

使

用这个方

法而获得优

异结果的例

子。

为什么Batch Norm这

么惹人注目

呢？因为Batch Norm有以

下优点。

• 可以

使学习快速

进行（可以增

大学习率）。

• 不

那么依赖初

始值（对于初

始值不用那

么神经质）。

• 抑

制过拟合（降

低Dropout等的必要

性）。

6.3

Batch Normalization  185

考虑到深

度学习要花

费很多时间

，第一个优点

令人非常开

心。另外，后

两

点也可以帮

我们消除深

度学习的学

习中的很多

烦恼。

如前所

述，Batch Norm的思路是

调整各层的

激活值分布

使其拥有适

当

的广度。为

此，要向神经

网络中插入

对数据分布

进行正规化

的层，即Batch 

Normalization层（下

文简称Batch Norm层），如

图6-16所示。

图6-16　使

用了Batch Normalization的神经

网络的例子

（Batch Norm层的背景为

灰色）

Affine ReLU Batch

Norm

Affine ReLU Batch

Norm

Softmax Affine

Batch Norm，顾名思

义，以进行学

习时的mini-batch为单

位，按mini￾batch进行正

规化。具体而

言，就是进行

使数据分布

的均值为0、方

差为1的

正规

化。用数学式

表示的话，如

下所示。

（6.7）

这里

对mini-batch的m个输入

数据的集合

B =

{x1, x2, ... ,

xm}求均值

µB和方

差 。然后，对输

入数据进行

均值为0、方差

为1（合适的分

布）的

正规化

。式（6.7）中的ε是一

个微小值（比

如，10e-7等），它是为

了防止出现

除以0的情况

。

式（6.7）所做的是

将mini-batch的输入数

据{x1, x2, ... ,

xm}变换为均

值

  第

6章　与学

习相关的技

巧 186

为0、方差为

1的数据

，非常

简单。通过将

这个处理插

入到

激活函

数的前面（或

者后面）A，可以

减小数据分

布的偏向。

接

着，Batch Norm层会对正

规化后的数

据进行缩放

和平移的变

换，用

数学式

可以如下表

示。

（6.8）

这里，γ和β是

参数。一开始

γ =

1，β = 0，然后再通过

学习调整到

合

适的值。

上

面就是Batch Norm的算

法。这个算法

是神经网络

上的正向传

播。如

果使用

第5章介绍的

计算图，Batch Norm可以

表示为图6-17。

图

6-17 Batch Normalization的计算图（引

用自文献[13]）

x

(N,D) (N,D)

x −

* * +

ˆ2

dx

r (D, )

r dr

β (D,

)

β dβ

x−ε

x

1

out

dout

Batch Norm的

反向传播的

推导有些复

杂，这里我们

不进行介绍

。不过

如果使

用图6-17的计算

图来思考的

话，Batch Norm的反向传

播或许也能

比较

轻松地

推导出来。Frederik Kratzert 的

博客“Understanding the

backward 

pass through

Batch Normalization Layer”[13]里有详

细说明，感兴

趣的读者

可

以参考一下

。

6.3.2　Batch Normalization的评估

现在

我们使用Batch

Norm层

进行实验。首

先，使用MNIST数据

集，

A 文献[11]、文献

[12]等中有讨论

（做过实验）应

该把Batch Normalization插入到

激活函数的

前面还是

后

面。

6.3 Batch Normalization

187

观察使用

Batch Norm层和不使用

Batch Norm层时学习的

过程会如何

变化（源

代码

在ch06/batch_norm_test.py中），结果如

图6-18所示。

Training Accuracy 0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 5 10 15

20

epochs

Batch Normalization

Normal(without BatchNorm)

图6-18　基

于Batch

Norm的效果：使

用Batch Norm后，学习进

行得更快了

从图6-18的结果

可知，使用Batch Norm后

，学习进行得

更快了。接着

，

给予不同的

初始值尺度

，观察学习的

过程如何变

化。图6-19是权重

初始值的

标

准差为各种

不同的值时

的学习过程

图。

我们发现

，几乎所有的

情况下都是

使用Batch Norm时学习

进行得更快

。

同时也可以

发现，实际上

，在不使用Batch

Norm的

情况下，如果

不赋予一

个

尺度好的初

始值，学习将

完全无法进

行。

综上，通过

使用Batch Norm，可以推

动学习的进

行。并且，对权

重初

始值变

得健壮（“对初

始值健壮”表

示不那么依

赖初始值）。Batch Norm具

备

了如此优

良的性质，一

定能应用在

更多场合中

。 accuracy

第 6章　与学习

相关的技巧

188

图6-19 图中的实

线是使用了

Batch Norm时的结果，虚

线是没有使

用Batch Norm时

的结果

：图的标题处

标明了权重

初始值的标

准差

w:1.0 w:0.541169526546 w:0.292864456463

w:0.158483319246

w:0.0125935639088 w:0.0251188643151 w:0.0464158883361

w:0.0857695898591

w:0.00116591440118 w:0.00215443469003 w:0.00398107170553

w:0.0073564225446

w:0.0001 w:0.000184784979742 w:0.000341454887383

w:0.00063095734448

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.00

epochs

5 10 15

20

1.0

0.8

0.6

0.4

0.2

0

epochs

5 10 15 20

0

epochs

5 10

15 20 0

epochs

5 10 15 20

6.4 正则化

机器学习的

问题中，过拟

合是一个很

常见的问题

。过拟合指的

是只能拟

合

训练数据，但

不能很好地

拟合不包含

在训练数据

中的其他数

据的状态。机

器学习的目

标是提高泛

化能力，即便

是没有包含

在训练数据

里的未观测

数据，

也希望

模型可以进

行正确的识

别。我们可以

制作复杂的

、表现力强的

模型，

accuracy accuracy accuracy accuracy

6.4 正则化

189

但是相应地

，抑制过拟合

的技巧也很

重要。

6.4.1　过拟合

发生过拟合

的原因，主要

有以下两个

。

• 模型拥有大

量参数、表现

力强。

• 训练数

据少。

这里，我

们故意满足

这两个条件

，制造过拟合

现象。为此，要

从

MNIST数据集原

本的60000个训练

数据中只选

定300个，并且，为

了增加网

络

的复杂度，使

用7层网络（每

层有100个神经

元，激活函数

为ReLU）。

下面是用

于实验的部

分代码（对应

文件在ch06/overfit_weight_decay.py

中）。首

先是用于读

入数据的代

码。

(x_train,

t_train), (x_test, t_test) =

load_mnist(normalize=True)

# 为了再现

过拟合，减少

学习数据

x_train

= x_train[:300]

t_train =

t_train[:300]

接

着是进行训

练的代码。和

之前的代码

一样，按epoch分别

算出所有训

练数据和所

有测试数据

的识别精度

。

network =

MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100,

100, 100, 100], output_size=10)

optimizer = SGD(lr=0.01) #

用学习率为

0.01的SGD更新参数

max_epochs = 201

train_size

= x_train.shape[0]

batch_size =

100

train_loss_list = []

train_acc_list = []

test_acc_list

= []

iter_per_epoch =

max(train_size / batch_size, 1)

epoch_cnt = 0

for

i in range(1000000000):

batch_mask = np.random.choice(train_size, batch_size)

第 6章　与学习

相关的技巧

190

x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

grads = network.gradient(x_batch, t_batch)

optimizer.update(network.params, grads)

 if

i % iter_per_epoch ==

0:

 train_acc =

network.accuracy(x_train, t_train)

 test_acc

= network.accuracy(x_test, t_test)

train_acc_list.append(train_acc)

 test_acc_list.append(test_acc)

epoch_cnt += 1

if epoch_cnt >= max_epochs:

break

train_acc_list和test_acc_list中以epoch为单

位（看完了所

有训练数据

的单位）保存

识别精度。现

在，我们将这

些列表（train_acc_list、test_acc_

list）绘成

图，结果如图

6-20所示。

图6-20

训练

数据（train）和测试

数据（test）的识别

精度的变化

0.0

0

1.0

0.8

0.6

0.4

0.2

50

100 150 200

epochs

train

test

accuracy

6.4

正则化  191

过了

100

个 epoch 左右后，用

训练数据测

量到的识别

精度几乎都

为

100%。但是，对于

测试数据，离

100%的识别精度

还有较大的

差距。如此大

的识别精度

差距，是只拟

合了训练数

据的结果。从

图中可知，模

型对训练时

没有使用的

一般数据（测

试数据）拟合

得不是很好

。

6.4.2　权值衰减

权

值衰减是一

直以来经常

被使用的一

种抑制过拟

合的方法。该

方法通过

在

学习的过程

中对大的权

重进行惩罚

，来抑制过拟

合。很多过拟

合原本就是

因为权重参

数取值过大

才发生的。

复

习一下，神经

网络的学习

目的是减小

损失函数的

值。这时，例如

为

损失函数

加上权重的

平方范数（L2范

数）。这样一来

，就可以抑制

权重变大。

用

符号表示的

话，如果将权

重记为W，L2范数

的权值衰减

就是 ，然

后将

这个 加到损

失函数上。这

里，λ是控制正

则化强度的

超参数。λ

设置

得越大，对大

的权重施加

的惩罚就越

重。此外， 开头

的

是用于

将

的求导结果

变成λW的调整

用常量。

对于

所有权重，权

值衰减方法

都会为损失

函数加上 。因

此，在求权

重

梯度的计算

中，要为之前

的误差反向

传播法的结

果加上正则

化项的导数

λW。

L2范数相当于

各个元素的

平方和。用数

学式表示的

话，假设有权

重

W =

(w1, w2, ... ,

wn)，则 L2范数可

用 计算

出来

。除了

L2范数，还

有 L1范数、L∞范数

等。L1范数是各

个元

素的绝

对值之和，相

当于|w1| +

|w2| + ... +

|wn|。L∞范数也

称为

Max范数，相

当于各个元

素的绝对值

中最大的那

一个。L2范数、L1

范

数、L∞范数都可

以用作正则

化项，它们各

有各的特点

，不过这里

我

们要实现的

是比较常用

的

L2范数。

现在

我们来进行

实验。对于刚

刚进行的实

验，应用λ = 0.1的权

值衰减，

结果

如图6-21所示（对

应权值衰减

的网络在common/multi_layer_net.py中

，

用于实验的

代码在ch06/overfit_weight_decay.py中）。

第

6章　与学习相

关的技巧 192

0.0

1.0

0.8

0.6

0.4

0.2

0 50 100

150 200

epochs

train

test

图

6-21　使用了权值

衰减的训练

数据（train）和测试

数据（test）的识别

精度的变化

如图6-21所示，虽

然训练数据

的识别精度

和测试数据

的识别精度

之间有

差距

，但是与没有

使用权值衰

减的图6-20的结

果相比，差距

变小了。这说

明

过拟合受

到了抑制。此

外，还要注意

，训练数据的

识别精度没

有达到100%（1.0）。

6.4.3　Dropout

作为

抑制过拟合

的方法，前面

我们介绍了

为损失函数

加上权重的

L2范

数的权值

衰减方法。该

方法可以简

单地实现，在

某种程度上

能够抑制过

拟合。

但是，如

果网络的模

型变得很复

杂，只用权值

衰减就难以

应对了。在这

种情

况下，我

们经常会使

用Dropout [14]方法。

Dropout是一

种在学习的

过程中随机

删除神经元

的方法。训练

时，随机

选出

隐藏层的神

经元，然后将

其删除。被删

除的神经元

不再进行信

号的传递，

如

图6-22所示。训练

时，每传递一

次数据，就会

随机选择要

删除的神经

元。

然后，测试

时，虽然会传

递所有的神

经元信号，但

是对于各个

神经元的输

出，

要乘上训

练时的删除

比例后再输

出。 accuracy

6.4

正则化  193

图

6-22

Dropout的概念图（引

用自文献[14]）：左

边是一般的

神经网络，右

边是应用了

Dropout的网络。Dropout通过

随机选择并

删除神经元

，停止向前传

递信号

下面

我们来实现

Dropout。这里的实现

重视易理解

性。不过，因为

训练

时如果

进行恰当的

计算的话，正

向传播时单

纯地传递数

据就可以了

（不用乘

以删

除比例），所以

深度学习的

框架中进行

了这样的实

现。关于高效

的实现，

可以

参考Chainer中实现

的Dropout。

class Dropout:

def __init__(self, dropout_ratio=0.5):

self.dropout_ratio = dropout_ratio

self.mask = None

def forward(self, x, train_flg=True):

if train_flg:

 self.mask

= np.random.rand(*x.shape) > self.dropout_ratio

return x * self.mask

else:

 return x

* (1.0 - self.dropout_ratio)

def backward(self, dout):

return dout * self.mask

这里的要

点是，每次正

向传播时，self.mask中

都会以False的形

式保

存要删

除的神经元

。self.mask会随机生成

和x形状相同

的数组，并将

值比

dropout_ratio大的元

素设为True。反向

传播时的行

为和ReLU相同。也

就是说，

正向

传播时传递

了信号的神

经元，反向传

播时按原样

传递信号；正

向传播时

第

6章　与学习相

关的技巧 194

没

有传递信号

的神经元，反

向传播时信

号将停在那

里。

现在，我们

使用MNIST数据集

进行验证，以

确认Dropout的效果

。源代

码在ch06/overfit_dropout.py中

。另外，源代码

中使用了Trainer类

来简化实现

。

common/trainer.py中实现了

Trainer类

。这个类可以

负责前面所

进行的网络

的学习。详细

内容可以参

照common/trainer.py和ch06/

overfit_dropout.py。

Dropout的实验

和前面的实

验一样，使用

7层网络（每层

有100个神经元

，

激活函数为

ReLU），一个使用Dropout，另

一个不使用

Dropout，实验的结

果

如图6-23所示。

图

6-23　左边没有使

用Dropout，右边使用

了Dropout（dropout_rate=0.15）

0

50 100 150 200

250 300

epochs

0.0

0.2

0.4

0.6

0.8

1.0

train

test

train

test

0 50 100

150 200 250 300

epochs

0.0

0.2

0.4

0.6

0.8

1.0

图6-23中，通过

使用Dropout，训练数

据和测试数

据的识别精

度的差距

变

小了。并且，训

练数据也没

有到达100%的识

别精度。像这

样，通过使用

Dropout，即便是表现

力强的网络

，也可以抑制

过拟合。

机器

学习中经常

使用集成学

习。所谓集成

学习，就是让

多个模型单

独进行学习

，推理时再取

多个模型的

输出的平均

值。用神经网

络的

语境来

说，比如，准备

5个结构相同

（或者类似）的

网络，分别进

行

学习，测试

时，以这

5个网

络的输出的

平均值作为

答案。实验告

诉我们，

accuracy

accuracy

6.5

超参

数的验证  195

通

过进行集成

学习，神经网

络的识别精

度可以提高

好几个百分

点。

这个集成

学习与 Dropout有密

切的关系。这

是因为可以

将 Dropout

理解为，通

过在学习过

程中随机删

除神经元，从

而每一次都

让不同

的模

型进行学习

。并且，推理时

，通过对神经

元的输出乘

以删除比

例

（比如，0.5等），可以

取得模型的

平均值。也就

是说，可以理

解成，

Dropout将集成

学习的效果

（模拟地）通过

一个网络实

现了。

6.5

超参数

的验证

神经

网络中，除了

权重和偏置

等参数，超参

数（hyper-parameter）也经

常出

现。这里所说

的超参数是

指，比如各层

的神经元数

量、batch大小、参

数

更新时的学

习率或权值

衰减等。如果

这些超参数

没有设置合

适的值，模型

的性能就会

很差。虽然超

参数的取值

非常重要，但

是在决定超

参数的过程

中

一般会伴

随很多的试

错。本节将介

绍尽可能高

效地寻找超

参数的值的

方法。

6.5.1　验证数

据

之前我们

使用的数据

集分成了训

练数据和测

试数据，训练

数据用于学

习，

测试数据

用于评估泛

化能力。由此

，就可以评估

是否只过度

拟合了训练

数据

（是否发

生了过拟合

），以及泛化能

力如何等。

下

面我们要对

超参数设置

各种各样的

值以进行验

证。这里要注

意的是，

不能

使用测试数

据评估超参

数的性能。这

一点非常重

要，但也容易

被忽视。

为什

么不能用测

试数据评估

超参数的性

能呢？这是因

为如果使用

测试数

据调

整超参数，超

参数的值会

对测试数据

发生过拟合

。换句话说，用

测试数

据确

认超参数的

值的“好坏”，就

会导致超参

数的值被调

整为只拟合

测试数据。

这

样的话，可能

就会得到不

能拟合其他

数据、泛化能

力低的模型

。

因此，调整超

参数时，必须

使用超参数

专用的确认

数据。用于调

整超参

数的

数据，一般称

为验证数据

（validation data）。我们使用这

个验证数据

来

评估超参

数的好坏。

第

6章　与学习相

关的技巧 196

训

练数据用于

参数（权重和

偏置）的学习

，验证数据用

于超参数的

性

能评估。为

了确认泛化

能力，要在最

后使用（比较

理想的是只

用一次）

测试

数据。

根据不

同的数据集

，有的会事先

分成训练数

据、验证数据

、测试数据三

部分，有的只

分成训练数

据和测试数

据两部分，有

的则不进行

分割。在这种

情况下，用户

需要自行进

行分割。如果

是MNIST数据集，获

得验证数据

的

最简单的

方法就是从

训练数据中

事先分割20%作

为验证数据

，代码如下所

示。

(x_train, t_train), (x_test,

t_test) = load_mnist()

#

打乱训练

数据

x_train, t_train =

shuffle_dataset(x_train, t_train)

# 分割验

证数据

validation_rate = 0.20

validation_num

= int(x_train.shape[0] * validation_rate)

x_val = x_train[:validation_num]

t_val

= t_train[:validation_num]

x_train =

x_train[validation_num:]

t_train = t_train[validation_num:]

这里

，分割训练数

据前，先打乱

了输入数据

和教师标签

。这是因为数

据

集的数据

可能存在偏

向（比如，数据

从“0”到“10”按顺序

排列等）。这里

使

用的shuffle_dataset函数

利用了np.random.shuffle，在common/util.py中

有

它的实现

。

接下来，我们

使用验证数

据观察超参

数的最优化

方法。

6.5.2　超参数

的最优化

进

行超参数的

最优化时，逐

渐缩小超参

数的“好值”的

存在范围非

常重要。

所谓

逐渐缩小范

围，是指一开

始先大致设

定一个范围

，从这个范围

中随机选

出

一个超参数

（采样），用这个

采样到的值

进行识别精

度的评估；然

后，多次

重复

该操作，观察

识别精度的

结果，根据这

个结果缩小

超参数的“好

值”的范围。

通

过重复这一

操作，就可以

逐渐确定超

参数的合适

范围。

6.5 超参数

的验证  197

有报

告[15]显示，在进

行神经网络

的超参数的

最优化时，与

网格搜索

等

有规律的搜

索相比，随机

采样的搜索

方式效果更

好。这是因为

在

多个超参

数中，各个超

参数对最终

的识别精度

的影响程度

不同。

超参数

的范围只要

“大致地指定

”就可以了。所

谓“大致地指

定”，是指

像0.001（10−3

）到

1000（103

）这样，以“10的阶

乘”的尺度指

定范围（也表

述

为“用对数

尺度（log

scale）指定”）。

在

超参数的最

优化中，要注

意的是深度

学习需要很

长时间（比如

，几天

或几周

）。因此，在超参

数的搜索中

，需要尽早放

弃那些不符

合逻辑的超

参数。

于是，在

超参数的最

优化中，减少

学习的epoch，缩短

一次评估所

需的时间

是

一个不错的

办法。

以上就

是超参数的

最优化的内

容，简单归纳

一下，如下所

示。

步骤0

设定

超参数的范

围。

步骤1

从设

定的超参数

范围中随机

采样。

步骤2

使

用步骤1中采

样到的超参

数的值进行

学习，通过验

证数据评估

识别精

度（但

是要将epoch设置

得很小）。

步骤

3

重复步骤1和

步骤2（100次等），根

据它们的识

别精度的结

果，缩小超参

数的范围。

反

复进行上述

操作，不断缩

小超参数的

范围，在缩小

到一定程度

时，从

该范围

中选出一个

超参数的值

。这就是进行

超参数的最

优化的一种

方法。

  第

6章　与

学习相关的

技巧 198

这里介

绍的超参数

的最优化方

法是实践性

的方法。不过

，这个方

法与

其说是科学

方法，倒不如

说有些实践

者的经验的

感觉。在超

参

数的最优化

中，如果需要

更精炼的方

法，可以使用

贝叶斯最优

化（Bayesian optimization）。贝叶斯最

优化运用以

贝叶斯定理

为中

心的数

学理论，能够

更加严密、高

效地进行最

优化。详细内

容请

参 考 论

文“Practical Bayesian

Optimization of Machine Learning

Algorithms”[16]等。

6.5.3　超参数

最优化的实

现

现在，我们

使用MNIST数据集

进行超参数

的最优化。这

里我们将学

习

率和控制

权值衰减强

度的系数（下

文称为“权值

衰减系数”）这

两个超参数

的

搜索问题

作为对象。这

个问题的设

定和解决思

路参考了斯

坦福大学的

课程

“CS231n”。

如前所

述，通过从

0.001（10−3

）到

1000（103

）这样的对数

尺度的范围

中随机采样

进行超参数

的验证。这在

Python中可以写成

10 **

np.random.

uniform(-3, 3)。在该实验中

，权值衰减系

数的初始范

围为10−8

到10−4

，学

习

率的初始范

围为10−6

到10−2

。此时

，超参数的随

机采样的代

码如下所示

。

weight_decay = 10 **

np.random.uniform(-8, -4)

lr =

10 ** np.random.uniform(-6, -2)

像这样进行

随机采样后

，再使用那些

值进行学习

。之后，多次使

用各种

超参

数的值重复

进行学习，观

察合乎逻辑

的超参数在

哪里。这里省

略了具体

实

现，只列出了

结果。进行超

参数最优化

的源代码在

ch06/hyperparameter_

optimization.py中，请大家自

由参考。

下面

我们就以权

值衰减系数

为10−8

到10−4

、学习率

为10−6

到10−2

的范围

进行实验，结

果如图6-24所示

。

6.5 超参数的验

证

199

图6-24　实线是

验证数据的

识别精度，虚

线是训练数

据的识别精

度

Best-1

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

Best-6 Best-7 Best-8 Best-9

Best-10

Best-11 Best-12 Best-13

Best-14 Best-15

Best-16 Best-17

Best-18 Best-19 Best-20

Best-2

Best-3 Best-4 Best-5

图6-24中，按识

别精度从高

到低的顺序

排列了验证

数据的学习

的变化。

从图

中可知，直到

“Best-5”左右，学习进

行得都很顺

利。因此，我们

来

观察一下

“Best-5”之前的超参

数的值（学习

率和权值衰

减系数），结果

如下

所示。

Best-1

(val acc:0.83) | lr:0.0092,

weight decay:3.86e-07

Best-2 (val

acc:0.78) | lr:0.00956, weight

decay:6.04e-07

Best-3 (val acc:0.77)

| lr:0.00571, weight decay:1.27e-06

Best-4 (val acc:0.74) |

lr:0.00626, weight decay:1.43e-05

Best-5

(val acc:0.73) | lr:0.0052,

weight decay:8.97e-06

从

这个结果可

以看出，学习

率在0.001到0.01、权值

衰减系数在

10−8

到

10−6

之间时，学

习可以顺利

进行。像这样

，观察可以使

学习顺利进

行的超参

数

的范围，从而

缩小值的范

围。然后，在这

个缩小的范

围中重复相

同的操作。

第

6章　与学习相

关的技巧 200

这

样就能缩小

到合适的超

参数的存在

范围，然后在

某个阶段，选

择一个最终

的超参数的

值。

6.6 小结

本章

我们介绍了

神经网络的

学习中的几

个重要技巧

。参数的更新

方法、

权重初

始值的赋值

方法、Batch

Normalization、Dropout等，这些

都是现代

神

经网络中不

可或缺的技

术。另外，这里

介绍的技巧

，在最先进的

深度学习

中

也被频繁使

用。

本章所学

的内容

• 参 数

的 更

新 方 法

，除 了

SGD 之 外，还

有 Momentum、AdaGrad、

Adam等方法。

• 权

重初始值的

赋值方法对

进行正确的

学习非常重

要。

•

作为权重

初始值，Xavier初始

值、He初始值等

比较有效。

• 通

过使用Batch Normalization，可以

加速学习，并

且对初始值

变得

健壮。

• 抑

制过拟合的

正则化技术

有权值衰减

、Dropout等。

•

逐渐缩小

“好值”存在的

范围是搜索

超参数的一

个有效方法

。

第7章

卷积神

经网络

本章

的主题是卷

积神经网络

（Convolutional

Neural Network，CNN）。

CNN被用于图像

识别、语音识

别等各种场

合，在图像识

别的比赛中

，基于

深度学

习的方法几

乎都以CNN为基

础。本章将详

细介绍CNN的结

构，并用

Python实现

其处理内容

。

7.1 整体结构

首

先，来看一下

CNN的网络结构

，了解CNN的大致

框架。CNN和之

前

介绍的神经

网络一样，可

以像乐高积

木一样通过

组装层来构

建。不过，

CNN中新

出现了卷积

层（Convolution层）和池化

层（Pooling层）。卷积层

和

池化层将

在下一节详

细介绍，这里

我们先看一

下如何组装

层以构建CNN。

之

前介绍的神

经网络中，相

邻层的所有

神经元之间

都有连接，这

称为全

连接

（fully-connected）。另外，我们用

Affine层实现了全

连接层。如果

使用

这个Affine层

，一个5层的全

连接的神经

网络就可以

通过图7-1所示

的网络结

构

来实现。

如图

7-1所示，全连接

的神经网络

中，Affine层后面跟

着激活函数

ReLU

层（或者Sigmoid层）。这

里堆叠了4层

“Affine-ReLU”组合，然后第

5层是

Affine层，最后

由Softmax层输出最

终结果（概率

）。

第 7章　卷积神

经网络 202

图7-1　基

于全连接层

（Affi ne层）的网络的

例子

Softmax

Affine ReLU Affine ReLU

Affine ReLU Affine ReLU

Affine

那么，CNN会

是什么样的

结构呢？图7-2是

CNN的一个例子

。

图7-2　基于CNN的网

络的例子：新

增了Convolution层和Pooling层

（用灰色的方

块表示）

Conv Conv ReLU Pooling

ReLU Conv Pooling ReLU

Affine ReLU Softmax Affine

如 图

7-2 所 示，CNN

中新增

了 Convolution 层 和

Pooling 层。CNN 的

层的连接顺

序是“Convolution -

ReLU -（Pooling）”（Pooling层有时

会被省

略）。这

可以理解为

之前的“Affi ne

- ReLU”连接

被替换成了

“Convolution -

ReLU -（Pooling）”连接。

还需要

注意的是，在

图7-2的CNN中，靠近

输出的层中

使用了之前

的“Affi ne

- ReLU”组合。此外

，最后的输出

层中使用了

之前的“Affi ne -

Softmax”组合

。这些都是一

般的CNN中比较

常见的结构

。

7.2 卷积层

CNN中出

现了一些特

有的术语，比

如填充、步幅

等。此外，各层

中传

递的数

据是有形状

的数据（比如

，3维数据），这与

之前的全连

接网络不同

，

因此刚开始

学习CNN时可能

会感到难以

理解。本节我

们将花点时

间，认真

学习

一下CNN中使用

的卷积层的

结构。

7.2

卷积层

203

7.2.1　全连接层存

在的问题

之

前介绍的全

连接的神经

网络中使用

了全连接层

（Affine层）。在全连接

层中，相邻层

的神经元全

部连接在一

起，输出的数

量可以任意

决定。

全连接

层存在什么

问题呢？那就

是数据的形

状被“忽视”了

。比如，输

入数

据是图像时

，图像通常是

高、长、通道方

向上的3维形

状。但是，向全

连接层输入

时，需要将3维

数据拉平为

1维数据。实际

上，前面提到

的使用

了MNIST数

据集的例子

中，输入图像

就是1通道、高

28像素、长28像素

的（1, 28,

28）形状，但却

被排成1列，以

784个数据的形

式输入到最

开始的

Affine层。

图

像是3维形状

，这个形状中

应该含有重

要的空间信

息。比如，空间

上

邻近的像

素为相似的

值、RBG的各个通

道之间分别

有密切的关

联性、相距

较

远的像素之

间没有什么

关联等，3维形

状中可能隐

藏有值得提

取的本质模

式。但是，因为

全连接层会

忽视形状，将

全部的输入

数据作为相

同的神经元

（同一维度的

神经元）处理

，所以无法利

用与形状相

关的信息。

而

卷积层可以

保持形状不

变。当输入数

据是图像时

，卷积层会以

3维

数据的形

式接收输入

数据，并同样

以3维数据的

形式输出至

下一层。因此

，

在CNN中，可以（有

可能）正确理

解图像等具

有形状的数

据。

另外，CNN 中，有

时将卷积层

的输入输出

数据称为特

征图（feature 

map）。其中，卷

积层的输入

数据称为输

入特征图（input

feature map），输

出

数据称为

输出特征图

（output feature

map）。本书中将“输

入输出数据

”和“特

征图”作

为含义相同

的词使用。

7.2.2　卷

积运算

卷积

层进行的处

理就是卷积

运算。卷积运

算相当于图

像处理中的

“滤波

器运算

”。在介绍卷积

运算时，我们

来看一个具

体的例子（图

7-3）。

第 7章

卷积神

经网络 204

图7-3　卷

积运算的例

子：用“”符号表

示卷积运算

1

2 3 0

0

1 2 3

3

0 1

2 0

1

0 1

15

16

6 15

2

1 0 2

2

2 3 0 1

输入数据 滤

波器

如图7-3所

示，卷积运算

对输入数据

应用滤波器

。在这个例子

中，输入

数据

是有高长方

向的形状的

数据，滤波器

也一样，有高

长方向上的

维度。假

设用

（height, width）表示数据和

滤波器的形

状，则在本例

中，输入大小

是

(4, 4)，滤波器大

小是(3,

3)，输出大

小是(2, 2)。另外，有

的文献中也

会用“核”

这个

词来表示这

里所说的“滤

波器”。

现在来

解释一下图

7-3的卷积运算

的例子中都

进行了什么

样的计算。图

7-4

中展示了卷

积运算的计

算顺序。

对于

输入数据，卷

积运算以一

定间隔滑动

滤波器的窗

口并应用。这

里所

说的窗

口是指图7-4中

灰色的3 ×

3的部

分。如图7-4所示

，将各个位置

上滤

波器的

元素和输入

的对应元素

相乘，然后再

求和（有时将

这个计算称

为乘积

累加

运算）。然后，将

这个结果保

存到输出的

对应位置。将

这个过程在

所有

位置都

进行一遍，就

可以得到卷

积运算的输

出。

在全连接

的神经网络

中，除了权重

参数，还存在

偏置。CNN中，滤波

器的参数就

对应之前的

权重。并且，CNN中

也存在偏置

。图7-3的卷积运

算

的例子一

直展示到了

应用滤波器

的阶段。包含

偏置的卷积

运算的处理

流如图

7-5所示

。

如图7-5所示，向

应用了滤波

器的数据加

上了偏置。偏

置通常只有

1个

（1 × 1）（本例中，相

对于应用了

滤波器的4个

数据，偏置只

有1个），这个值

会被加到应

用了滤波器

的所有元素

上。

7.2

卷积层 205

图

7-4　卷积运算的

计算顺序

1 2 3 0

0 1 2 3

3 0 1

2

0 1

0 1

15

2

1 0

2

2

2 3

0 1

1 2

3

0 1 2

3 0 1

15

1 2 3 0

0 1 2 3

3 0 1

2

0 1

0 1

15 16

2

1

0 2

2

2

3 0 1

2

3 0

1 2

3

0 1

16

2

1 2 3

0

0 1 2

3

3 0 1

2 0 1

0

1

15 16

6

2

1 0 2

2

2 3 0

1

0 1 2

3 0 1 6

2 3 0

1

2 3 0

0

1 2 3

3

0 1

2 0

1

0 1

15

16

6 15

2

1 0 2

2

2 3 0 1

1 2 3

0

1 15 2

3

0 1

第

7章　卷积神经

网络

206

图7-5　卷积

运算的偏置

：向应用了滤

波器的元素

加上某个固

定值（偏置）

1

2 3 0

0

1 2 3

3

0 1

2

3

0 1

0 1

15 16

6 15

2

1 0 2

2

2 3 0

1

输

入数据 滤波

器（权重） 偏置

输出数据

18 19

9 18

+

7.2.3　填

充

在进行卷

积层的处理

之前，有时要

向输入数据

的周围填入

固定的数据

（比

如0等），这称

为填充（padding），是卷

积运算中经

常会用到的

处理。比如，

在

图7-6的例子中

，对大小为(4, 4)的

输入数据应

用了幅度为

1的填充。“幅

度

为1的填充”是

指用幅度为

1像素的0填充

周围。

1 2 3 0

0 1 2 3

3 0 1

2

0 1

0 1

2

1 0 2

2

2 3 0

1

(4, 4) (3,

3) (4, 4)

输入数

据（padding:1）

输出数据

滤波器

7 12 10

2

4 15 16

10

10 6 15

6

8 10 4

3

图7-6 卷

积运算的填

充处理：向输

入数据的周

围填入0（图中

用虚线表示

填充，并省略

了

填充的内

容“0”）

如图7-6所示

，通过填充，大

小为(4, 4)的输入

数据变成了

(6, 6)的形状。

然后

，应用大小为

(3,

3)的滤波器，生

成了大小为

(4, 4)的输出数据

。这个例

子中

将填充设成

了1，不过填充

的值也可以

设置成2、3等任

意的整数。在

图7-5

的例子中

，如果将填充

设为2，则输入

数据的大小

变为(8,

8)；如果将

填充设

为3，则

大小变为(10, 10)。

7.2

卷

积层 207

使用填

充主要是为

了调整输出

的大小。比如

，对大小为(4, 4)的

输入

数据应

用(3, 3)的滤波器

时，输出大小

变为(2, 2)，相当于

输出大小

比

输入大小缩

小了

2个元素

。这在反复进

行多次卷积

运算的深度

网

络中会成

为问题。为什

么呢？因为如

果每次进行

卷积运算都

会缩小

空间

，那么在某个

时刻输出大

小就有可能

变为 1，导致无

法再应用

卷

积运算。为了

避免出现这

样的情况，就

要使用填充

。在刚才的例

子中，将填充

的幅度设为

1，那么相对于

输入大小(4, 4)，输

出大小

也保

持为原来的

(4, 4)。因此，卷积运

算就可以在

保持空间大

小不变

的情

况下将数据

传给下一层

。

7.2.4　步幅

应用滤

波器的位置

间隔称为步

幅（stride）。之前的例

子中步幅都

是1，如

果将步

幅设为2，则如

图7-7所示，应用

滤波器的窗

口的间隔变

为2个元素。

图

7-7　步幅为2的卷

积运算的例

子

1

2 3

0 1

2

3 0 1

15 2 0 1

0 1

15

2

1 0 2

1

2 3 0

0

1 2 3

3

0 1 2

2

3 0 1

1

2 3

0 1

2

3 0 1

2 3 0

1

2 3 0

0

1 2 3

3

0 1 2

1

2 3

0 1

2

3 0 1

3 0

2 3

1 2

2 0

1

0 1

15

17

2

1 0

2

1

0

3

1 2 3 0

0 1 2 3

3 0 1 2

2 3 0 1

1 2 3

0

1 2

3 0

1

2 3 0

1 2 3 0

0 1 2 3

3 0 1 2

1 2 3

0

1 2

3 0

1

步幅: 2

第

7章

卷积神经网

络 208

在图7-7的例

子中，对输入

大小为(7, 7)的数

据，以步幅2应

用了滤波器

。

通过将步幅

设为2，输出大

小变为(3, 3)。像这

样，步幅可以

指定应用滤

波器

的间隔

。

综上，增大步

幅后，输出大

小会变小。而

增大填充后

，输出大小会

变大。

如果将

这样的关系

写成算式，会

如何呢？接下

来，我们看一

下对于填充

和步

幅，如何

计算输出大

小。

这里，假设

输入大小为

(H, W)，滤波器大小

为(FH,

FW)，输出大小

为

(OH, OW)，填充为P，步

幅为S。此时，输

出大小可通

过式(7.1)进行计

算。

(7.1)

现在，我们

使用这个算

式，试着做几

个计算。

例1：图

7-6的例子

输入

大小：(4, 4)；填充：1；步

幅：1；滤波器大

小：(3,

3)

例2：图7-7的例

子

输入大小

：(7, 7)；填充：0；步幅：2；滤

波器大小：(3,

3)

例

3

输入大小：(28, 31)；填

充：2；步幅：3；滤波

器大小：(5,

5)

7.2  卷积

层

209

如这些例

子所示，通过

在式（7.1）中代入

值，就可以计

算输出大小

。这

里需要注

意的是，虽然

只要代入值

就可以计算

输出大小，但

是所设定的

值必

须使式

（7.1）中的

和 分别

可以除尽。当

输出大小无

法

除尽时（结

果是小数时

），需要采取报

错等对策。顺

便说一下，根

据深度学习

的框架的不

同，当值无法

除尽时，有时

会向最接近

的整数四舍

五入，不进行

报错而继续

运行。

7.2.5

3维数据

的卷积运算

之前的卷积

运算的例子

都是以有高

、长方向的2维

形状为对象

的。但是，

图像

是3维数据，除

了高、长方向

之外，还需要

处理通道方

向。这里，我们

按

照与之前

相同的顺序

，看一下对加

上了通道方

向的3维数据

进行卷积运

算的例子。

图

7-8是卷积运算

的例子，图7-9是

计算顺序。这

里以3通道的

数据为例，

展

示了卷积运

算的结果。和

2维数据时（图

7-3的例子）相比

，可以发现纵

深

方向（通道

方向）上特征

图增加了。通

道方向上有

多个特征图

时，会按通道

进行输入数

据和滤波器

的卷积运算

，并将结果相

加，从而得到

输出。

图7-8　对3维

数据进行卷

积运算的例

子

4 2 1 2

5 2 2 4

6 1 1 2

* 3 0 5

4 2 1 2

4

2

5

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

63

55

18 51

4

0 2

0 1

0

2 0 2

4 0 2

0

2

0 1 3

1 1 2

3

0 0

0 1

3

1 1 2

3 0 0

2

0 1

0 1

2

1 0 2

2 2 2 3

1 1 1 0

0 3 0 1

1 2 3 0

0 1 2 3

3 0 1 2

2 3 0 1

输入数据

输出数据 滤

波器

第 7章　卷

积神经网络

210

图7-9

对3维数据

进行卷积运

算的计算顺

序

63

63 55

63 55

18

63

55

18 51

4

2 1 2

5

2 2 4

6

1 1 2

*

3 0 5

4

2 1 2

4

2

5

3 0

6 5

2 2

2 3

1 1

1 0

0 3

0 1

4 0

2

0 1 0

2 0 2

4

0 2

0

2

0 1 3

1

1 2

3 0

0

0 1 3

1 1 2

3

0 0

2 0

1

0 1 2

1 0 2

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

1

2 3 0

0

1 2 3

3

0 1 2

2

3 0 1

4

2 1 2

5

2 2 4

6

1 1 2

*

3 0 5

4

2 1 2

4

2

5

3 0

6 5

2 2

2 3

1 1

1 0

0 3

0 1

4 0

2

0 1 0

2 0 2

4

0 2

0

2

0 1 3

1

1 2

3 0

0

0 1 3

1 1 2

3

0 0

2 0

1

0 1 2

1 0 2

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

1

2 3 0

0

1 2 3

3

0 1 2

2

3 0 1

4

2 1 2

5

2 2 4

6

1 1 2

*

3 0 5

4

2 1 2

4

2

5

3 0

6 5

2 2

2 3

1 1

1 0

0 3

0 1

4 0

2

0 1 0

2 0 2

4

0 2

0

2

0 1 3

1

1 2

3 0

0

0 1 3

1 1 2

3

0 0

2 0

1

0 1 2

1 0 2

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

1

2 3 0

0

1 2 3

3

0 1 2

2

3 0 1

4

2 1 2

5

2 2 4

6

1 1 2

*

3 0 5

4

2 1 2

4

2

5

3 0

6 5

2 2

2 3

1 1

1 0

0 3

0 1

4 0

2

0 1 0

2 0 2

4

0 2

0

2

0 1 3

1

1 2

3 0

0

0 1 3

1 1 2

3

0 0

2 0

1

0 1 2

1 0 2

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

1

2 3 0

0

1 2 3

3

0 1 2

2

3 0 1

7.2

卷积层  211

需

要注意的是

，在3维数据的

卷积运算中

，输入数据和

滤波器的通

道数

要设为

相同的值。在

这个例子中

，输入数据和

滤波器的通

道数一致，均

为3。

滤波器大

小可以设定

为任意值（不

过，每个通道

的滤波器大

小要全部相

同）。

这个例子

中滤波器大

小为(3, 3)，但也可

以设定为(2,

2)、(1, 1)、(5, 5)等

任

意值。再强

调一下，通道

数只能设定

为和输入数

据的通道数

相同的值（本

例

中为3）。

7.2.6　结合

方块思考

将

数据和滤波

器结合长方

体的方块来

考虑，3维数据

的卷积运算

会很

容易理

解。方块是如

图7-10所示的3维

长方体。把3维

数据表示为

多维数组

时

，书写顺序为

（channel, height, width）。比如，通道数

为C、高度为H、

长

度为W的数据

的形状可以

写成（C, H, W）。滤波器

也一样，要按

（channel,

height, width）的顺序书写

。比如，通道数

为C、滤波器高

度为FH（Filter 

Height）、长度为

FW（Filter

Width）时，可以写成

（C, FH, FW）。

H

FH

FW

W

OH

OW

(C, H, W)

(C, FH, FW) (1,

OH, OW)

C

C

输入数据 输

出数据 滤波

器

图7-10

结合方

块思考卷积

运算。请注意

方块的形状

在这个例子

中，数据输出

是1张特征图

。所谓1张特征

图，换句话说

，

就是通道数

为1的特征图

。那么，如果要

在通道方向

上也拥有多

个卷积运算

第

7章　卷积神

经网络 212

的输

出，该怎么做

呢？为此，就需

要用到多个

滤波器（权重

）。用图表示的

话，

如图7-11所示

。

图7-11　基于多个

滤波器的卷

积运算的例

子

H

FH

FN个

FW

W

OH

OW

(C, H,

W) (FN, C, FH,

FW) (FN, OH, OW)

C

C

FN

输入数

据

输出数据

滤波器

图7-11中

，通过应用FN个

滤波器，输出

特征图也生

成了FN个。如果

将这FN个特征

图汇集在一

起，就得到了

形状为(FN, OH, OW)的方

块。将

这个方

块传给下一

层，就是CNN的处

理流。

如图 7-11 所

示，关于卷积

运算的滤波

器，也必须考

虑滤波器的

数

量。因此，作

为4维数据，滤

波器的权重

数据要按(output_channel, input_

channel, height,

width)的

顺序书写。比

如，通道数为

3、大小为5 × 5的滤

波器有20个时

，可以写成(20, 3,

5, 5)。

卷

积运算中（和

全连接层一

样）存在偏置

。在图7-11的例子

中，如果进

一

步追加偏置

的加法运算

处理，则结果

如下面的图

7-12所示。

图7-12中，每

个通道只有

一个偏置。这

里，偏置的形

状是(FN, 1, 1)，

滤波器

的输出结果

的形状是(FN,

OH, OW)。这

两个方块相

加时，要对滤

波

器的输出

结果(FN, OH,

OW)按通道

加上相同的

偏置值。另外

，不同形状的

方块相加时

，可以基于NumPy的

广播功能轻

松实现（1.5.5节）。

7.2  卷

积层

213

图7-12　卷积

运算的处理

流（追加了偏

置项）

+

+

H

FH FN个

FW

W

OH 1

1 OW

(C, H,

W) (FN, C, FH,

FW) (FN, OH, OW)

C

C

FN

FN

输入

数据

(FN, OH, OW)

输出数

据

(FN, 1, 1)

偏置 滤波

器

FN

FN

OH

OW

FN

7.2.7

批处理

神

经网络的处

理中进行了

将输入数据

打包的批处

理。之前的全

连接神经

网

络的实现也

对应了批处

理，通过批处

理，能够实现

处理的高效

化和学习时

对mini-batch的对应。

我

们希望卷积

运算也同样

对应批处理

。为此，需要将

在各层间传

递的数

据保

存为4维数据

。具体地讲，就

是按(batch_num, channel, height, width)

的顺序

保存数据。比

如，将图7-12中的

处理改成对

N个数据进行

批处理时，

数

据的形状如

图7-13所示。

图7-13的

批处理版的

数据流中，在

各个数据的

开头添加了

批用的维度

。

像这样，数据

作为4维的形

状在各层间

传递。这里需

要注意的是

，网络间传

递

的是4维数据

，对这N个数据

进行了卷积

运算。也就是

说，批处理将

N次

的处理汇

总成了1次进

行。

图7-13　卷积运

算的处理流

（批处理）

* +

+ *

H

N 个数

据 N

个数据 N 个

数据

FH

FN 个

FW

W

OH 1

1

OW

(N, C, H, W)

(FN, C, FH, FW)

(N, FN, OH, OW)

C

C

FN

FN

输入

数据 (N, FN, OH,

OW) 输出数

据 (FN, 1,

1)

偏置 滤波

器

FN

FN

OH

OW

FN

第 7章　卷积

神经网络 214

7.3 池

化层

池化是

缩小高、长方

向上的空间

的运算。比如

，如图7-14所示，进

行将

2

× 2的区域

集约成1个元

素的处理，缩

小空间大小

。

1 2

0 1

3 0

2 3 1’

2

2 3

1

4

1 2 0

0

1 2 3

3

0 1 2

2

0 1

4

1

2 1 0

0

1 2 3

3

0 1 2

2

0 1

3 0

’

2 3

1

2

0 1

2

3

4

2 3

4 2

11 2

0

0 1 2

3

3 0 1

2

2 4 0

1

1

4

1

2 0

0 1

2 3

3 0

1 2

2 0

1

图7-14 Max池化的处

理顺序

图7-14的

例子是按步

幅2进行2

× 2的Max池

化时的处理

顺序。“Max

池化”是

获取最大值

的运算，“2 ×

2”表示

目标区域的

大小。如图所

示，从

2 × 2的区域

中取出最大

的元素。此外

，这个例子中

将步幅设为

了2，所以

2 × 2的窗

口的移动间

隔为2个元素

。另外，一般来

说，池化的窗

口大小会

和

步幅设定成

相同的值。比

如，3

× 3的窗口的

步幅会设为

3，4 × 4的窗口

的步

幅会设为4等

。

除了Max池化之

外，还有Average池化

等。相对于Max池

化是从

目标

区域中取出

最大值，Average池化

则是计算目

标区域的平

均值。

在图像

识别领域，主

要使用Max池化

。因此，本书中

说到“池化层

”

时，指的是Max池

化。

7.3  池化层

215

池

化层的特征

池化层有以

下特征。

没有

要学习的参

数

池化层和

卷积层不同

，没有要学习

的参数。池化

只是从目标

区域中取最

大值（或者平

均值），所以不

存在要学习

的参数。

通道

数不发生变

化

经过池化

运算，输入数

据和输出数

据的通道数

不会发生变

化。如图7-15

所示

，计算是按通

道独立进行

的。

图7-15

池化中

通道数不变

4 2 1 2

5 2 2 4

6 1 1 2

( 3 0 5

4 2 1 2

4

2

5

3

0 6 5

2

2 2 3

1

1 1 0

0

3 0 1

4

4

0 5

4

4

3 6 5

0 2

3 6

0 2 2 3

4 2

3 0

6 5

2 2

2 3

1 1

1 0

0 3

0 1

1 2

1 0

0 1

2 3

3 0

1 2

2 4

0 1

输入数据 输

出数据

对微

小的位置变

化具有鲁棒

性（健壮）

输入

数据发生微

小偏差时，池

化仍会返回

相同的结果

。因此，池化对

输入数据的

微小偏差具

有鲁棒性。比

如，3 × 3的池化的

情况下，如图

7-16所示，池化会

吸收输入数

据的偏差（根

据数据的不

同，结果有可

能不一致）。

第

7章　卷积神经

网络 216

图7-16

输入

数据在宽度

方向上只偏

离1个元素时

，输出仍为相

同的结果（根

据数据的不

同，

有时结果

也不相同）

1 2

0 1 2

3

0 1 1’9

6

7

8

17

6

4

4 8

1

2 0 0

0

9 2 23 3

3 0

0

1

1

1

1

2

2

2

2

2

2

0

0

01

1

1

1

1

2

0 1 2

3 0 1 1’9

6

7

8

’

3

2

17

6

4

4 8

1

2 0

0 9

2 23

3 0

0

1

1

1

1

2

2

2

2

0

0

01

1

3

1

3

2

2

1

7.4

卷

积层和池化

层的实现

前

面我们详细

介绍了卷积

层和池化层

，本节我们就

用Python来实现这

两个层。和第

5章一样，也给

进行实现的

类赋予forward和backward方

法，并

使其可

以作为模块

使用。

大家可

能会感觉卷

积层和池化

层的实现很

复杂，但实际

上，通过使用

某

种技巧，就

可以很轻松

地实现。本节

将介绍这种

技巧，将问题

简化，然后再

进行卷积层

的实现。

7.4.1　4维数

组

如前所述

，CNN中各层间传

递的数据是

4维数据。所谓

4维数据，比如

数据的形状

是(10,

1, 28, 28)，则它对应

10个高为28、长为

28、通道为1的数

据。用Python来实现

的话，如下所

示。

>>>

x = np.random.rand(10, 1,

28, 28) # 随机生成

数据

>>> x.shape

(10, 1,

28, 28)

这里，如

果要访问第

1个数据，只要

写x[0]就可以了

（注意Python的索

引

是从0开始的

）。同样地，用x[1]可

以访问第2个

数据。

7.4  卷积层

和池化层的

实现

217

>>> x[0].shape #

(1, 28, 28)

>>>

x[1].shape # (1, 28,

28)

如果要

访问第1个数

据的第1个通

道的空间数

据，可以写成

下面这样。

>>> x[0,

0] # 或

者x[0][0]

像这样，CNN中

处理的是4维

数据，因此卷

积运算的实

现看上去会

很复

杂，但是

通过使用下

面要介绍的

im2col这个技巧，问

题就会变得

很简单。

7.4.2　基于

im2col的展开

如果

老老实实地

实现卷积运

算，估计要重

复好几层的

for语句。这样的

实现有点麻

烦，而且，NumPy中存

在使用for语句

后处理变慢

的缺点（NumPy

中，访

问元素时最

好不要用for语

句）。这里，我们

不使用for语句

，而是使

用im2col这

个便利的函

数进行简单

的实现。

im2col是一

个函数，将输

入数据展开

以适合滤波

器（权重）。如图

7-17所示，

对3维的

输入数据应

用im2col后，数据转

换为2维矩阵

（正确地讲，是

把包含

批数

量的4维数据

转换成了2维

数据）。

图7-17 im2col的示

意图

im2col

输入数

据

im2col会把输入

数据展开以

适合滤波器

（权重）。具体地

说，如图7-18所示

，

对于输入数

据，将应用滤

波器的区域

（3维方块）横向

展开为1列。im2col会

在所有应用

滤波器的地

方进行这个

展开处理。

第

7章　卷积神经

网络 218

图7-18

将滤

波器的应用

区域从头开

始依次横向

展开为1列

在

图7-18中，为了便

于观察，将步

幅设置得很

大，以使滤波

器的应用区

域不重叠。而

在实际的卷

积运算中，滤

波器的应用

区域几乎都

是重叠的。在

滤波器的应

用区域重叠

的情况下，使

用im2col展开后，展

开后的元素

个数会

多于

原方块的元

素个数。因此

，使用im2col的实现

存在比普通

的实现消耗

更

多内存的

缺点。但是，汇

总成一个大

的矩阵进行

计算，对计算

机的计算颇

有

益处。比如

，在矩阵计算

的库（线性代

数库）等中，矩

阵计算的实

现已被高

度

最优化，可以

高速地进行

大矩阵的乘

法运算。因此

，通过归结到

矩阵计算

上

，可以有效地

利用线性代

数库。

im2col这个名

称是“image

to column”的缩写

，翻译过来就

是“从

图像到

矩阵”的意思

。Caffe、Chainer 等深度学习

框架中有名

为

im2col的函数，并

且在卷积层

的实现中，都

使用了im2col。

使用

im2col展开输入数

据后，之后就

只需将卷积

层的滤波器

（权重）纵

向展

开为1列，并计

算2个矩阵的

乘积即可（参

照图7-19）。这和全

连接层的

Affi

ne层

进行的处理

基本相同。

如

图7-19所示，基于

im2col方式的输出

结果是2维矩

阵。因为CNN中

数

据会保存为

4维数组，所以

要将2维输出

数据转换为

合适的形状

。以上就

是卷

积层的实现

流程。

7.4  卷积层

和池化层的

实现

219

图7-19 卷积

运算的滤波

器处理的细

节：将滤波器

纵向展开为

1列，并计算和

im2col展开

的数据

的矩阵乘积

，最后转换（reshape）为

输出数据的

大小

im2col

reshape

输入数

据

输出数据

输出数据（2

维

）

滤波器

矩阵

的乘积

滤波

器

7.4.3　卷积层的

实现

本书提

供了im2col函数，并

将这个im2col函数

作为黑盒（不

关心内部实

现）

使用。im2col的实

现内容在common/util.py中

，它的实现（实

质上）是一个

10

行左右的简

单函数。有兴

趣的读者可

以参考。

im2col这一

便捷函数具

有以下接口

。

im2col (input_data,

filter_h, filter_w, stride=1, pad=0)

• input_data―由（数据量，通

道，高，长）的4维

数组构成的

输入数据

• filter_h―滤

波器的高

• filter_w―滤

波器的长

• stride―步

幅

• pad―填充

第 7章

卷积神经网

络

220

im2col会考虑滤

波器大小、步

幅、填充，将输

入数据展开

为2维数组。现

在，

我们来实

际使用一下

这个im2col。

import

sys, os

sys.path.append(os.pardir)

from

common.util import im2col

x1

= np.random.rand(1, 3, 7,

7)

col1 = im2col(x1,

5, 5, stride=1, pad=0)

print(col1.shape) # (9, 75)

x2 = np.random.rand(10, 3,

7, 7) # 10个数据

col2

= im2col(x2, 5, 5,

stride=1, pad=0)

print(col2.shape) #

(90, 75)

这里举了两

个例子。第一

个是批大小

为1、通道为3的

7 ×

7的数据，第

二

个的批大小

为10，数据形状

和第一个相

同。分别对其

应用im2col函数，在

这两种情形

下，第2维的元

素个数均为

75。这是滤波器

（通道为3、大小

为

5 ×

5）的元素个

数的总和。批

大小为1时，im2col的

结果是(9, 75)。而第

2

个例子中批

大小为10，所以

保存了10倍的

数据，即(90, 75)。

现在

使用im2col来实现

卷积层。这里

我们将卷积

层实现为名

为Convolution

的类。

class Convolution:

def __init__(self, W, b,

stride=1, pad=0):

 self.W

= W

 self.b

= b

 self.stride

= stride

 self.pad

= pad

 def

forward(self, x):

 FN,

C, FH, FW =

self.W.shape

 N, C,

H, W = x.shape

out_h = int(1 +

(H + 2*self.pad -

FH) / self.stride)

out_w = int(1 +

(W + 2*self.pad -

FW) / self.stride)

col = im2col(x, FH,

FW, self.stride, self.pad)

col_W = self.W.reshape(FN, -1).T

# 滤波

器的展开

 out

= np.dot(col, col_W) +

self.b

 out =

out.reshape(N, out_h, out_w, -1).transpose(0,

3, 1, 2)

return out

7.4

卷

积层和池化

层的实现 221

卷

积层的初始

化方法将滤

波器（权重）、偏

置、步幅、填充

作为参数接

收。

滤波器是

(FN,

C, FH, FW)的 4

维形状。另

外，FN、C、FH、FW分别是 Filter 

Number（滤

波器数量）、Channel、Filter

Height、Filter Width的

缩写。

这里用

粗体字表示

Convolution层的实现中

的重要部分

。在这些粗体

字

部分，用im2col展

开输入数据

，并用reshape将滤波

器展开为2维

数组。然后，

计

算展开后的

矩阵的乘积

。

展开滤波器

的部分（代码

段中的粗体

字）如图7-19所示

，将各个滤波

器

的方块纵

向展开为1列

。这里通过reshape(FN,-1)将

参数指定为

-1，这是

reshape的一个

便利的功能

。通过在reshape时指

定为-1，reshape函数会

自

动计算-1维

度上的元素

个数，以使多

维数组的元

素个数前后

一致。比如，

(10, 3, 5,

5)形

状的数组的

元素个数共

有750个，指定reshape(10,-1)后

，就

会转换成

(10, 75)形状的数组

。

forward的实现中，最

后会将输出

大小转换为

合适的形状

。转换时使用

了

NumPy的transpose函数。transpose会

更改多维数

组的轴的顺

序。如图7-20

所示

，通过指定从

0开始的索引

（编号）序列，就

可以更改轴

的顺序。

图7-20　基

于NumPy的transpose的轴顺

序的更改：通

过指定索引

（编号），更改轴

的顺序

(N, H, W, C)

transpose (N, C, H,

W) 形状

索引 0, 1,

2, 3 0, 3,

1, 2

以上就

是卷积层的

forward处理的实现

。通过使用im2col进

行展开，基

本

上可以像实

现全连接层

的Affine层一样来

实现（5.6节）。接下

来是卷积层

的反向传播

的实现，因为

和Affine层的实现

有很多共通

的地方，所以

就不再

介绍

了。但有一点

需要注意，在

进行卷积层

的反向传播

时，必须进行

im2col

的逆处理。这

可以使用本

书提供的col2im函

数（col2im的实现在

common/util.

第 7章　卷积神

经网络 222

py中）来

进行。除了使

用col2im这一点，卷

积层的反向

传播和Affi ne层的

实

现方式都

一样。卷积层

的反向传播

的实现在common/layer.py中

，有兴趣的读

者可以参考

。

7.4.4

池化层的实

现

池化层的

实现和卷积

层相同，都使

用im2col展开输入

数据。不过，池

化

的情况下

，在通道方向

上是独立的

，这一点和卷

积层不同。具

体地讲，如图

7-21所示，池化的

应用区域按

通道单独展

开。

图7-21

对输入

数据展开池

化的应用区

域（2×2的池化的

例子）

4 2 1

2

5 2 2

4

6 1 1

2

( 3 0

5

4 2 1

2

3 0 6

5

2 2 2

3

1 1 1

0

0 3 0

1

1 2 0

1

1 2 0

4

4 2 0

1

4 2 0

1

3 0 2

4

3 0 4

2

6 5 4

3

3 0 4

2

6 2 4

5

1 0 3

2

3 0 2

3

1 0 3

1

3 0 6

2 2

1 1

0 3

1 2

3 0

0 1

2 4

1 0

4 2

3 2

0 1

输入数

据

4

2

5

4

2

5

5

3

0

1

5

3

0

1

1 通道

2

通道

3 通道

像这样

展开之后，只

需对展开的

矩阵求各行

的最大值，并

转换为合适

的

形状即可

（图7-22）。

7.4  卷积层和

池化层的实

现

223

4 4

4

6

4 4

4

6 6

3 3

4 6

2 43

3

3 4

max

展开 reshape

1 2

0 1

1 2

0 4

4 2

0 1

4 2

0 1

3 0

2 4

3 0

4 2

6 5

4 3

3 0

4 2

6 2

4 5

1 0

3 2

3 0

2 3

1 0

3 1

2

4

4

4

4

4

6

4

6

3

3

3

4 2

1 2

5 2

2 4

6 1

1 2

( 3

0 5

4 2

1 2

4

2

5

3 0 6

5

2 2 2

3

1 1 1

0

0 3 0

1

3 0 6

5

2 2 2

3

1 1 1

0

0 3 0

1

1 2 3

0

0 1 2

4

1 0 4

2

3 2 0

1 输出

数据

输入数

据

图7-22

池化层

的实现流程

：池化的应用

区域内的最

大值元素用

灰色表示

上

面就是池化

层的forward处理的

实现流程。下

面来看一下

Python的实

现示例

。

class

Pooling:

 def __init__(self,

pool_h, pool_w, stride=1, pad=0):

self.pool_h = pool_h

self.pool_w = pool_w

self.stride = stride

self.pad = pad

def forward(self, x):

N, C, H, W

= x.shape

 out_h

= int(1 + (H

- self.pool_h) / self.stride)

out_w = int(1 +

(W - self.pool_w) /

self.stride)

 # 展开(1)

col = im2col(x, self.pool_h,

self.pool_w, self.stride, self.pad)

col = col.reshape(-1, self.pool_h*self.pool_w)

# 最大值

(2)

 out

= np.max(col, axis=1)

# 转换(3)

 out

= out.reshape(N, out_h, out_w,

C).transpose(0, 3, 1, 2)

return out

如图7-22所

示，池化层的

实现按下面

3个阶段进行

。

第 7章　卷积神

经网络 224

1.展开

输入数据。

2.求

各行的最大

值。

3.转换为合

适的输出大

小。

各阶段的

实现都很简

单，只有一两

行代码。

最大

值的计算可

以使用 NumPy 的 np.max方

法。np.max可以指定

axis参数，并在这

个参数指定

的各个轴方

向上求最大

值。比如，如

果

写成np.max(x, axis=1)，就可以

在输入x的第

1维的各个轴

方向

上求最

大值。

以上就

是池化层的

forward处理的介绍

。如上所述，通

过将输入数

据展

开为容

易进行池化

的形状，后面

的实现就会

变得非常简

单。

关于池化

层的backward处理，之

前已经介绍

过相关内容

，这里就不再

介绍了。

另外

，池化层的backward处

理可以参考

ReLU层的实现中

使用的max的反

向

传播（5.5.1节）。池

化层的实现

在common/layer.py中，有兴趣

的读者可以

参考。
















积层的输出

大小。接下来

是权重参数

的初始化部

分。

self.params = {}

self.params['W1'] = weight_init_std *

\

 np.random.randn(filter_num, input_dim[0],

filter_size, filter_size)

self.params['b1'] =

np.zeros(filter_num)

self.params['W2'] = weight_init_std

* \

 np.random.randn(pool_output_size,

hidden_size)

self.params['b2'] = np.zeros(hidden_size)

self.params['W3'] = weight_init_std *

\

 np.random.randn(hidden_size, output_size)

self.params['b3'] = np.zeros(output_size)

学习所需

的参数是第

1层的卷积层

和剩余两个

全连接层的

权重和偏置

。

将这些参数

保存在实例

变量的params字典

中。将第1层的

卷积层的权

重设为

关键

字W1，偏置设为

关键字b1。同样

，分别用关键

字W2、b2和关键字

W3、b3

来保存第2个

和第3个全连

接层的权重

和偏置。

最后

，生成必要的

层。

self.layers = OrderedDict()

self.layers['Conv1']

= Convolution(self.params['W1'],

 self.params['b1'],

conv_param['stride'],

 conv_param['pad'])

self.layers['Relu1']

= Relu()

self.layers['Pool1'] =

Pooling(pool_h=2, pool_w=2, stride=2)

self.layers['Affine1']

= Affine(self.params['W2'],

 self.params['b2'])

self.layers['Relu2'] = Relu()

self.layers['Affine2']

= Affine(self.params['W3'],

 self.params['b3'])

self.last_layer = softmaxwithloss()

从最前面

开始按顺序

向有序字典

（OrderedDict）的layers中添加层

。只

有最后的

SoftmaxWithLoss层被添加到

别的变量lastLayer中

。

以上就是SimpleConvNet的

初始化中进

行的处理。像

这样初始化

后，进

行推理

的predict方法和求

损失函数值

的loss方法就可

以像下面这

样实现。

7.5

CNN的实

现 227

def predict(self,

x):

 for layer

in self.layers.values():

 x

= layer.forward(x)

 return

x

def loss(self, x,

t):

 y =

self.predict(x)

 return self.lastLayer.forward(y,

t)

这里，参数

x是输入数据

，t是教师标签

。用于推理的

predict方法从头

开

始依次调用

已添加的层

，并将结果传

递给下一层

。在求损失函

数的loss

方法中

，除了使用

predict方

法进行的 forward处

理之外，还会

继续进行

forward处

理，直到到达

最后的SoftmaxWithLoss层。

接

下来是基于

误差反向传

播法求梯度

的代码实现

。

def gradient(self, x, t):

# forward

 self.loss(x,

t)

 # backward

dout = 1

dout = self.lastLayer.backward(dout)

layers = list(self.layers.values())

layers.reverse()

 for layer

in layers:

 dout

= layer.backward(dout)

 #

设定

 grads =

{}

 grads['W1'] =

self.layers['Conv1'].dW

 grads['b1'] =

self.layers['Conv1'].db

 grads['W2'] =

self.layers['Affine1'].dW

 grads['b2'] =

self.layers['Affine1'].db

 grads['W3'] =

self.layers['Affine2'].dW

 grads['b3'] =

self.layers['Affine2'].db

 return grads

参数的

梯度通过误

差反向传播

法（反向传播

）求出，通过把

正向传播和

反向传播组

装在一起来

完成。因为已

经在各层正

确实现了正

向传播和反

向传

播的功

能，所以这里

只需要以合

适的顺序调

用即可。最后

，把各个权重

参数

的梯度

保存到grads字典

中。这就是SimpleConvNet的

实现。

第

7章　卷

积神经网络

228

现在，使用这

个SimpleConvNet学习MNIST数据

集。用于学习

的代码

与4.5节

中介绍的代

码基本相同

，因此这里不

再罗列（源代

码在ch07/train_

convnet.py中）。

如果

使用MNIST数据集

训练SimpleConvNet，则训练

数据的识别

率为

99.82%，测试数

据的识别率

为98.96%（每次学习

的识别精度

都会发生一

些误

差）。测试

数据的识别

率大约为99%，就

小型网络来

说，这是一个

非常高的

识

别率。下一章

，我们会通过

进一步叠加

层来加深网

络，实现测试

数据的识

别

率超过99%的网

络。

如上所述

，卷积层和池

化层是图像

识别中必备

的模块。CNN可以

有效

读取图

像中的某种

特性，在手写

数字识别中

，还可以实现

高精度的识

别。

7.6 CNN的可视化

CNN中用到的卷

积层在“观察

”什么呢？本节

将通过卷积

层的可视化

，

探索CNN中到底

进行了什么

处理。

7.6.1

第 1层权

重的可视化

刚才我们对

MNIST数据集进行

了简单的CNN学

习。当时，第1层

的

卷积层的

权重的形状

是(30, 1,

5, 5)，即30个大小

为5 × 5、通道为1的

滤波

器。滤波

器大小是5 × 5、通

道数是1，意味

着滤波器可

以可视化为

1通道的

灰度

图像。现在，我

们将卷积层

（第1层）的滤波

器显示为图

像。这里，我

们

来比较一下

学习前和学

习后的权重

，结果如图7-24所

示（源代码在

ch07/

visualize_filter.py中）。

图7-24中，学习

前的滤波器

是随机进行

初始化的，所

以在黑白的

浓淡上

没有

规律可循，但

学习后的滤

波器变成了

有规律的图

像。我们发现

，通过学

习，滤

波器被更新

成了有规律

的滤波器，比

如从白到黑

渐变的滤波

器、含有

块状

区域（称为blob）的

滤波器等。

7.6

CNN的

可视化  229

图7-24

学

习前和学习

后的第1层的

卷积层的权

重：虽然权重

的元素是实

数，但是在图

像

的显示上

，统一将最小

值显示为黑

色（0），最大值显

示为白色（255）

学

习前 学习后

如果要问图

7-24中右边的有

规律的滤波

器在“观察”什

么，答案就是

它

在观察边

缘（颜色变化

的分界线）和

斑块（局部的

块状区域）等

。比如，左半

部

分为白色、右

半部分为黑

色的滤波器

的情况下，如

图7-25所示，会对

垂直

方向上

的边缘有响

应。

图7-25

对水平

方向上和垂

直方向上的

边缘有响应

的滤波器：输

出图像1中，垂

直方向的

边

缘上出现白

色像素，输出

图像2中，水平

方向的边缘

上出现很多

白色像素

输

入图像

输出

图像

2

输出图

像 1

对垂直方

向上

的边缘

有响应

滤波

器2

滤波器1

对

水平方向上

的边缘有响

应

第 7章　卷积

神经网络

230

图

7-25中显示了选

择两个学习

完的滤波器

对输入图像

进行卷积处

理时的

结果

。我们发现“滤

波器1”对垂直

方向上的边

缘有响应，“滤

波器2”对水平

方向上的边

缘有响应。

由

此可知，卷积

层的滤波器

会提取边缘

或斑块等原

始信息。而刚

才实现

的CNN会

将这些原始

信息传递给

后面的层。

7.6.2　基

于分层结构

的信息提取

上面的结果

是针对第1层

的卷积层得

出的。第1层的

卷积层中提

取了边

缘或

斑块等“低级

”信息，那么在

堆叠了多层

的CNN中，各层中

又会提取什

么样的信息

呢？根据深度

学习的可视

化相关的研

究[17][18]，随着层次

加深，提

取的

信息（正确地

讲，是反映强

烈的神经元

）也越来越抽

象。

图7-26中展示

了进行一般

物体识别（车

或狗等）的8层

CNN。这个网络

结

构的名称是

下一节要介

绍的AlexNet。AlexNet网络结

构堆叠了多

层卷积

层和

池化层，最后

经过全连接

层输出结果

。图7-26的方块表

示的是中间

数据，

对于这

些中间数据

，会连续应用

卷积运算。

图

7-26 CNN的卷积层中

提取的信息

。第1层的神经

元对边缘或

斑块有响应

，第3层对纹

理

有响应，第5层

对物体部件

有响应，最后

的全连接层

对物体的类

别（狗或车）有

响应（图像引

用自文献[19]）

11

11

224

55

5

5

55

27

96

3

3

3

3 3

3

256

27

13

13

13

13

13

13

384

384 256

4096 4096

1000

dense dense

Max

pooling Max

pooling Max

pooling

224

3

Stride

of 4

Data-driven Numerical

Conv 1: Edge+Blob Conv

3: Texture Conv 5:

Object Parts Fc8: Object

Classes cock ship

dinning

table groccry storc

7.7

具

有代表性的

CNN  231

如图7-26所示，如

果堆叠了多

层卷积层，则

随着层次加

深，提取的信

息

也愈加复

杂、抽象，这是

深度学习中

很有意思的

一个地方。最

开始的层对

简

单的边缘

有响应，接下

来的层对纹

理有响应，再

后面的层对

更加复杂的

物体

部件有

响应。也就是

说，随着层次

加深，神经元

从简单的形

状向“高级”信

息

变化。换句

话说，就像我

们理解东西

的“含义”一样

，响应的对象

在逐渐变化

。

7.7 具有代表性

的 CNN

关于CNN，迄今

为止已经提

出了各种网

络结构。这里

，我们介绍其

中

特别重要

的两个网络

，一个是在1998年

首次被提出

的CNN元祖LeNet[20]，

另一

个是在深度

学习受到关

注的2012年被提

出的AlexNet[21]。

7.7.1　LeNet

LeNet在1998年被

提出，是进行

手写数字识

别的网络。如

图7-27所示，

它有

连续的卷积

层和池化层

（正确地讲，是

只“抽选元素

”的子采样层

），最

后经全连

接层输出结

果。

图7-27

LeNet的网络

结构（引用自

文献[20]）

和“现在

的CNN”相比，LeNet有几

个不同点。第

一个不同点

在于激活

函

数。LeNet中使用sigmoid函

数，而现在的

CNN中主要使用

ReLU函数。

此外，原

始的LeNet中使用

子采样（subsampling）缩小

中间数据的

大小，而

现在

的CNN中Max池化是

主流。

  第

7章　卷

积神经网络

232

综上，LeNet与现在

的CNN虽然有些

许不同，但差

别并不是那

么大。

想到LeNet是

20多年前提出

的最早的CNN，还

是很令人称

奇的。

7.7.2　AlexNet

在LeNet问世

20多年后，AlexNet被发

布出来。AlexNet是引

发深度学

习

热潮的导火

线，不过它的

网络结构和

LeNet基本上没有

什么不同，如

图7-28

所示。

图7-28 AlexNet（根

据文献[21]生成

）

224

11

11

55 27

13

13

13

13

13

13 27

256

384 384 256 4096

4096

1000

55

5

3

5 3

3

3

3

3

96

max pooling

fully connected

max pooling

224

3

AlexNet叠有多个卷

积层和池化

层，最后经由

全连接层输

出结果。虽然

结构上AlexNet和LeNet没

有大的不同

，但有以下几

点差异。

• 激活

函数使用ReLU。

•

使

用进行局部

正规化的LRN（Local Response Normalization）层

。

•

使用Dropout（6.4.3节）。

如上

所述，关于网

络结构，LeNet和AlexNet没

有太大的不

同。但是，

围绕

它们的环境

和计算机技

术有了很大

的进步。具体

地说，现在任

何人都可

以

获得大量的

数据。而且，擅

长大规模并

行计算的GPU得

到普及，高速

进

行大量的

运算已经成

为可能。大数

据和GPU已成为

深度学习发

展的巨大的

原动力。

7.8  小结

233

大多数情况

下，深度学习

（加深了层次

的网络）存在

大量的参数

。因此，

学习需

要大量的计

算，并且需要

使那些参数

“满意”的大量

数据。可

以说

是 GPU和大数据

给这些课题

带来了希望

。

7.8 小结

本章介

绍了CNN。构成CNN的

基本模块的

卷积层和池

化层虽然有

些复

杂，但是

一旦理解了

，之后就只是

如何使用它

们的问题了

。本章为了使

读者

在实现

层面上理解

卷积层和池

化层，花了不

少时间进行

介绍。在图像

处理领

域，几

乎毫无例外

地都会使用

CNN。请扎实地理

解本章的内

容，然后进入

最后一章的

学习。

本章所

学的内容

•

CNN在

此前的全连

接层的网络

中新增了卷

积层和池化

层。

• 使用im2col函数

可以简单、高

效地实现卷

积层和池化

层。

•

通过CNN的可

视化，可知随

着层次变深

，提取的信息

愈加高级。

• LeNet和

AlexNet是CNN的代表性

网络。

•

在深度

学习的发展

中，大数据和

GPU做出了很大

的贡献。

第8章

深度学习

深

度学习是加

深了层的深

度神经网络

。基于之前介

绍的网络，只

需通过

叠加

层，就可以创

建深度网络

。本章我们将

看一下深度

学习的性质

、课题和

可能

性，然后对当

前的深度学

习进行概括

性的说明。

8.1 加

深网络

关于

神经网络，我

们已经学了

很多东西，比

如构成神经

网络的各种

层、

学习时的

有效技巧、对

图像特别有

效的CNN、参数的

最优化方法

等，这些

都是

深度学习中

的重要技术

。本节我们将

这些已经学

过的技术汇

总起来，创

建

一个深度网

络，挑战MNIST数据

集的手写数

字识别。

8.1.1

向更

深的网络出

发

话不多说

，这里我们来

创建一个如

图8-1所示的网

络结构的CNN（一

个

比之前的

网络都深的

网络）。这个网

络参考了下

一节要介绍

的VGG。

如图8-1所示

，这个网络的

层比之前实

现的网络都

更深。这里使

用的卷

积层

全都是3 × 3的小

型滤波器，特

点是随着层

的加深，通道

数变大（卷积

层的通道数

从前面的层

开始按顺序

以16、16、32、32、64、64的方式增

加）。

此外，如图

8-1所示，插入了

池化层，以逐

渐减小中间

数据的空间

大小；并且，

后

面的全连接

层中使用了

Dropout层。

  第

8章　深度

学习 236

这个网

络使用He初始

值作为权重

的初始值，使

用Adam更新权重

参数。

把上述

内容总结起

来，这个网络

有如下特点

。

• 基于3×3的小型

滤波器的卷

积层。

•

激活函

数是ReLU。

• 全连接

层的后面使

用Dropout层。

•

基于Adam的

最优化。

• 使用

He初始值作为

权重初始值

。

从这些特征

中可以看出

，图8-1的网络中

使用了多个

之前介绍的

神经网

络技

术。现在，我们

使用这个网

络进行学习

。先说一下结

论，这个网络

的识

别精度

为99.38% A，可以说是

非常优秀的

性能了！

A

最终

的识别精度

有少许偏差

，不过在这个

网络中，识别

精度大体上

都会超过99%。

图

8-1　进行手写数

字识别的深

度CNN

Conv

Conv Pool Pool ReLU

ReLU Conv Conv ReLU

ReLU

Conv Pool ReLU

Conv ReLU Affine ReLU

Dropout Affine Dropout Softmax

8.1  加深网络

237

实现图 8-1的网

络的源代码

在ch08/deep_convnet.py中，训练用

的

代码在ch08/train_deepnet.py中

。虽然使用这

些代码可以

重现这里

进

行的学习，不

过深度网络

的学习需要

花费较多的

时间（大概要

半天

以上）。本

书以ch08/deep_conv_net_params.pkl的形式

给出了学习

完

的权重参

数。刚才的deep_convnet.py备

有读入学习

完的参数的

功能，

请根据

需要进行使

用。

图8-1的网络

的错误识别

率只有0.62%。这里

我们实际看

一下在什么

样

的图像上

发生了识别

错误。图8-2中显

示了识别错

误的例子。

6

0

3

3

5 5

3

3

5 5

8

8

3 3

7

7

3 3

1

3

8 8

9

9

6 6

0

0

7 7

2

2

9 9

4

4

7

1

5

5

3 3

1

1

3 3

0

0

6 6

9

9

4 4

7

9

6 6

0

0

9 9

8

8

4 4

9

9

1 1

7

7

图

8-2 识别错误的

图像的例子

：各个图像的

左上角显示

了正确解标

签，右下角显

示了本

网络

的推理结果

观察图8-2可知

，这些图像对

于我们人类

而言也很难

判断。实际上

，这里

面有几

个图像很难

判断是哪个

数字，即使是

我们人类，也

同样会犯“识

别错误”。

比如

，左上角的图

像（正确解是

“6”）看上去像“0”，它

旁边的图像

（正确解是

“3”）看

上去像“5”。整体

上，“1”和“7”、“0”和“6”、“3”和“5”的

组合比较

容

易混淆。通过

这些例子，相

信大家可以

理解为何会

发生识别错

误了吧。

这次

的深度CNN尽管

识别精度很

高，但是对于

某些图像，也

犯了和人

类

同样的“识别

错误”。从这一

点上，我们也

可以感受到

深度CNN中蕴藏

着

巨大的可

能性。

第 8章　深

度学习 238

8.1.2　进一

步提高识别

精度

在一个

标题为“What is

the class of this

image ?”的网

站[32]上，以排行

榜的形式刊

登了目前为

止通过论文

等渠道发表

的针对各种

数据集的方

法的识

别精

度（图8-3）。

图8-3

针对

MNIST数据集的各

种方法的排

行（引自文献

[32]：2016年6月）

观察图

8-3 的 排

行 结 果

，可 以

发 现“Neural Networks”“Deep”

“Convolutional”等

关键词特别

显眼。实际上

，排行榜上的

前几名大都

是基

于CNN的方

法。顺便说一

下，截止到2016年

6月，对MNIST数据集

的最高

识别

精度是99.79%（错误

识别率为0.21%），该

方法也是以

CNN为基础的[33]。

不

过，它用的CNN并

不是特别深

层的网络（卷

积层为2层、全

连接层为2层

的网络）。

8.1

加深

网络  239

对于MNIST数

据集，层不用

特别深就获

得了（目前）最

高的识别精

度。一般认为

，这是因为对

于手写数字

识别这样一

个比较简单

的任

务，没有

必要将网络

的表现力提

高到那么高

的程度。因此

，可以说

加深

层的好处并

不大。而之后

要介绍的大

规模的一般

物体识别的

情况，

因为问

题复杂，所以

加深层对提

高识别精度

大有裨益。

参

考刚才排行

榜中前几名

的方法，可以

发现进一步

提高识别精

度的技术和

线索。比如，集

成学习、学习

率衰减、Data

Augmentation（数据

扩充）等都有

助于提高识

别精度。尤其

是Data Augmentation，虽然方法

很简单，但在

提高

识别精

度上效果显

著。

Data

Augmentation基于算法

“人为地”扩充

输入图像（训

练图像）。具

体

地说，如图8-4所

示，对于输入

图像，通过施

加旋转、垂直

或水平方向

上

的移动等

微小变化，增

加图像的数

量。这在数据

集的图像数

量有限时尤

其有效。

原图

像

基于旋转

的变形

基于

平移的变形

图8-4 Data Augmentation的例子

除

了如图8-4所示

的变形之外

，Data Augmentation还可以通过

其他各

种方

法扩充图像

，比如裁剪图

像的“crop处理”、将

图像左右翻

转的“fl ip处

理”A 等

。对于一般的

图像，施加亮

度等外观上

的变化、放大

缩小等尺度

上

的变化也

是有效的。不

管怎样，通过

Data Augmentation巧妙地增加

训练图像，

就

可以提高深

度学习的识

别精度。虽然

这个看上去

只是一个简

单的技巧，不

过经常会有

很好的效果

。这里，我们不

进行Data Augmentation的实现

，不

A fl

ip处理只在

不需要考虑

图像对称性

的情况下有

效。

  第

8章　深度

学习 240

过这个

技巧的实现

比较简单，有

兴趣的读者

请自己试一

下。

8.1.3　加深层的

动机

关于加

深层的重要

性，现状是理

论研究还不

够透彻。尽管

目前相关理

论

还比较贫

乏，但是有几

点可以从过

往的研究和

实验中得以

解释（虽然有

一些

直观）。本

节就加深层

的重要性，给

出一些增补

性的数据和

说明。

首先，从

以ILSVRC为代表的

大规模图像

识别的比赛

结果中可以

看出加

深层

的重要性（详

细内容请参

考下一节）。这

种比赛的结

果显示，最近

前几名

的方

法多是基于

深度学习的

，并且有逐渐

加深网络的

层的趋势。也

就是说，

可以

看到层越深

，识别性能也

越高。

下面我

们说一下加

深层的好处

。其中一个好

处就是可以

减少网络的

参数

数量。说

得详细一点

，就是与没有

加深层的网

络相比，加深

了层的网络

可以

用更少

的参数达到

同等水平（或

者更强）的表

现力。这一点

结合卷积运

算中

的滤波

器大小来思

考就好理解

了。比如，图8-5展

示了由5 × 5的滤

波器构成

的

卷积层。

输出

数据 输入数

据

5 ×

5

图8-5 5×5的卷积

运算的例子

8.1

加深网络  241

这

里希望大家

考虑一下输

出数据的各

个节点是从

输入数据的

哪个区域计

算出来的。显

然，在图8-5的例

子中，每个输

出节点都是

从输入数据

的某个

5 × 5的区

域算出来的

。接下来我们

思考一下图

8-6中重复两次

3 ×

3的卷积

运算

的情形。此时

，每个输出节

点将由中间

数据的某个

3 × 3的区域计算

出来。

那么，中

间数据的3 × 3的

区域又是由

前一个输入

数据的哪个

区域计算出

来

的呢？仔细

观察图8-6，可知

它对应一个

5

× 5的区域。也就

是说，图8-6的

输

出数据是“观

察”了输入数

据的某个5 ×

5的

区域后计算

出来的。

输出

数据 输入数

据 中间数据

3

× 3 3 ×

3

图8-6　重复两次

3×3的卷积层的

例子

一次5

× 5的

卷积运算的

区域可以由

两次3 × 3的卷积

运算抵充。并

且，

相对于前

者的参数数

量25（5 × 5），后者一共

是18（2 ×

3 × 3），通过叠加

卷

积层，参数

数量减少了

。而且，这个参

数数量之差

会随着层的

加深而变大

。

比如，重复三

次3 × 3的卷积运

算时，参数的

数量总共是

27。而为了用一

次

卷积运算

“观察”与之相

同的区域，需

要一个7

× 7的滤

波器，此时的

参数数

量是

49。

叠加小型滤

波器来加深

网络的好处

是可以减少

参数的数量

，扩大感

受野

（receptive field，给神经元施

加变化的某

个局部空间

区域）。并且，

通

过叠加层，将

ReLU等激活函数

夹在卷积层

的中间，进一

步提高

了网

络的表现力

。这是因为向

网络添加了

基于激活函

数的“非线性

”

表现力，通过

非线性函数

的叠加，可以

表现更加复

杂的东西。

  第

8章

深度学习

242

加深层的另

一个好处就

是使学习更

加高效。与没

有加深层的

网络相比，

通

过加深层，可

以减少学习

数据，从而高

效地进行学

习。为了直观

地理解这

一

点，大家可以

回忆一下7.6节

的内容。7.6节中

介绍了CNN的卷

积层会分

层

次地提取信

息。具体地说

，在前面的卷

积层中，神经

元会对边缘

等简单的

形

状有响应，随

着层的加深

，开始对纹理

、物体部件等

更加复杂的

东西有响应

。

我们先牢记

这个网络的

分层结构，然

后考虑一下

“狗”的识别问

题。要

用浅层

网络解决这

个问题的话

，卷积层需要

一下子理解

很多“狗”的特

征。“狗”

有各种

各样的种类

，根据拍摄环

境的不同，外

观变化也很

大。因此，要理

解“狗”

的特征

，需要大量富

有差异性的

学习数据，而

这会导致学

习需要花费

很多时间。

不

过，通过加深

网络，就可以

分层次地分

解需要学习

的问题。因此

，各

层需要学

习的问题就

变成了更简

单的问题。比

如，最开始的

层只要专注

于学

习边缘

就好，这样一

来，只需用较

少的学习数

据就可以高

效地进行学

习。这

是为什

么呢？因为和

印有“狗”的照

片相比，包含

边缘的图像

数量众多，并

且边缘的模

式比“狗”的模

式结构更简

单。

通过加深

层，可以分层

次地传递信

息，这一点也

很重要。比如

，因为提

取了

边缘的层的

下一层能够

使用边缘的

信息，所以应

该能够高效

地学习更加

高级的模式

。也就是说，通

过加深层，可

以将各层要

学习的问题

分解成容易

解决的简单

问题，从而可

以进行高效

的学习。

以上

就是对加深

层的重要性

的増补性说

明。不过，这里

需要注意的

是，

近几年的

深层化是由

大数据、计算

能力等即便

加深层也能

正确地进行

学习的

新技

术和环境支

撑的。

8.2

深度学

习的小历史

一般认为，现

在深度学习

之所以受到

大量关注，其

契机是2012年举

办

的大规模

图像识别大

赛ILSVRC（ImageNet Large Scale

Visual Recognition 

Challenge）。在那年的

比赛中，基于

深度学习的

方法（通称AlexNet）以

压倒

性的优

势胜出，彻底

颠覆了以往

的图像识别

方法。2012年深度

学习的这场

逆

8.2 深度学习

的小历史

243

袭

成为一个转

折点，在之后

的比赛中，深

度学习一直

活跃在舞台

中央。本节

我

们以ILSVRC这个大

规模图像识

别比赛为轴

，看一下深度

学习最近的

发展

趋势。

8.2.1　ImageNet

ImageNet[25]是

拥有超过100万

张图像的数

据集。如图8-7所

示，它包含

了

各种各样的

图像，并且每

张图像都被

关联了标签

（类别名）。每年

都会举办

使

用这个巨大

数据集的ILSVRC图

像识别大赛

。

mammal

vehicle craft

watercraft sailing vessel sailboat

trimaran

placental carnivore canine

dog working dog husky

图8-7　大规模数

据集ImageNet的数据

例（引用自文

献[25]）

ILSVRC大赛有多

个测试项目

，其中之一是

“类别分类”（classification），

在

该项目中，会

进行1000个类别

的分类，比试

识别精度。我

们来看一下

最

近几年的

ILSVRC大赛的类别

分类项目的

结果。图8-8中展

示了从2010年到

2015年的优胜队

伍的成绩。这

里，将前5类中

出现正确解

的情况视为

“正确”，

此时的

错误识别率

用柱形图来

表示。

图8-8中需

要注意的是

，以2012年为界，之

后基于深度

学习的方法

一直

居于首

位。实际上，我

们发现2012年的

AlexNet大幅降低了

错误识别率

。并且，

此后基

于深度学习

的方法不断

在提升识别

精度。特别是

2015年的ResNet（一

个超

过150层的深度

网络）将错误

识别率降低

到了3.5%。据说这

个结果甚至

超过了普通

人的识别能

力。

这些年深

度学习取得

了不斐的成

绩，其中VGG、GoogLeNet、ResNet

第 8章

深度学习 244

已

广为人知，在

与深度学习

有关的各种

场合都会遇

到这些网络

。下面我们就

来简单地介

绍一下这3个

有名的网络

。

图8-8 ILSCRV优胜队伍

的成绩演变

：竖轴是错误

识别率，横轴

是年份。横轴

的括号内

是

队伍名或者

方法名

8.2.2

VGG

VGG是由

卷积层和池

化层构成的

基础的CNN。不过

，如图8-9所示，

它

的特点在于

将有权重的

层（卷积层或

者全连接层

）叠加至16层（或

者19层），

具备了

深度（根据层

的深度，有时

也称为“VGG16”或“VGG19”）。

VGG中

需要注意的

地方是，基于

3×3的小型滤波

器的卷积层

的运算是

连

续进行的。如

图8-9所示，重复

进行“卷积层

重叠2次到4次

，再通过池化

层将大小减

半”的处理，最

后经由全连

接层输出结

果。

VGG在 2014年的比

赛中最终获

得了第

2名的

成绩（下一节

介绍的

GoogleNet是 2014年

的第 1名）。虽然

在性能上不

及

GoogleNet，但

因为 VGG结

构简单，应用

性强，所以很

多技术人员

都喜欢使用

基于

VGG的网络

。

8.2  深度学习的

小历史

245

图8-9 VGG（根

据文献[22]生成

）

224

× 224

112 ×

112

56 × 56

28 × 28 14

× 14 7 ×

7

4096 4096

1000

3

3

fully connected

8.2.3　GoogLeNet

GoogLeNet的网络结构

如图8-10所示。图

中的矩形表

示卷积层、池

化

层等。

图8-10 GoogLeNet（引

用自文献[23]）

只

看图的话，这

似乎是一个

看上去非常

复杂的网络

结构，但实际

上它基

本上

和之前介绍

的CNN结构相同

。不过，GoogLeNet的特征

是，网络不仅

在纵向上有

深度，在横向

上也有深度

（广度）。

GoogLeNet在横向

上有“宽度”，这

称为“Inception结构”，以

图8-11所

示的结

构为基础。

如

图8-11所示，Inception结构

使用了多个

大小不同的

滤波器（和池

化），

最后再合

并它们的结

果。GoogLeNet的特征就

是将这个Inception结

构用作

一个

构件（构成元

素）。此外，在GoogLeNet中

，很多地方都

使用了大小

为

  第

8章　深度

学习 246

1

× 1的滤波

器的卷积层

。这个1 × 1的卷积

运算通过在

通道方向上

减小大小，

有

助于减少参

数和实现高

速化处理（具

体请参考原

始论文[23]）。

8.2.4　ResNet

ResNet[24]是微

软团队开发

的网络。它的

特征在于具

有比以前的

网络更

深的

结构。

我们已

经知道加深

层对于提升

性能很重要

。但是，在深度

学习中，过度

加深层的话

，很多情况下

学习将不能

顺利进行，导

致最终性能

不佳。ResNet中，

为了

解决这类问

题，导入了“快

捷结构”（也称

为“捷径”或“小

路”）。导入这

个

快捷结构后

，就可以随着

层的加深而

不断提高性

能了（当然，层

的加深也

是

有限度的）。

如

图8-12所示，快捷

结构横跨（跳

过）了输入数

据的卷积层

，将输入x合

计

到输出。

图8-12中

，在连续2层的

卷积层中，将

输入x跳着连

接至2层后的

输出。

这里的

重点是，通过

快捷结构，原

来的2层卷积

层的输出F(x)变

成了F(x) + x。

通过引

入这种快捷

结构，即使加

深层，也能高

效地学习。这

是因为，通过

快

捷结构，反

向传播时信

号可以无衰

减地传递。

图

8-11 GoogLeNet的Inception结构（引用

自文献[23]）

8.2

深度

学习的小历

史  247

图8-12

ResNet的构成

要素（引用自

文献[24]）：这里的

“weight layer”是指卷积层

F(x)

F(x) +

x

x

x

weight

layer

weight layer identity

relu

relu

因为快捷结

构只是原封

不动地传递

输入数据，所

以反向传播

时会将

来自

上游的梯度

原封不动地

传向下游。这

里的重点是

不对来自上

游

的梯度进

行任何处理

，将其原封不

动地传向下

游。因此，基于

快捷

结构，不

用担心梯度

会变小（或变

大），能够向前

一层传递“有

意义

的梯度

”。通过这个快

捷结构，之前

因为加深层

而导致的梯

度变小的

梯

度消失问题

就有望得到

缓解。

ResNet以前面

介绍过的VGG网

络为基础，引

入快捷结构

以加深层，其

结果如图8-13所

示。

图8-13 ResNet（引用自

文献[24]）：方块对

应3×3的卷积层

，其特征在于

引入了横跨

层

的快捷结

构

如图8-13所示

，ResNet通过以2个卷

积层为间隔

跳跃式地连

接来加深层

。

另外，根据实

验的结果，即

便加深到150层

以上，识别精

度也会持续

提高。并且，

在

ILSVRC大赛中，ResNet的错

误识别率为

3.5%（前5类中包含

正确解这一

精度下的错

误识别率），令

人称奇。

第 8章

深度学习 248

实

践中经常会

灵活应用使

用ImageNet这个巨大

的数据集学

习到的权

重

数据，这称为

迁移学习，将

学习完的权

重（的一部分

）复制到其他

神经网络，进

行再学习（fine tuning）。比

如，准备一个

和 VGG相同

结构

的网络，把学

习完的权重

作为初始值

，以新数据集

为对象，进

行

再学习。迁移

学习在手头

数据集较少

时非常有效

。

8.3 深度学习的

高速化

随着

大数据和网

络的大规模

化，深度学习

需要进行大

量的运算。虽

然到

目前为

止，我们都是

使用CPU进行计

算的，但现实

是只用CPU来应

对深度

学习

无法令人放

心。实际上，环

视一下周围

，大多数深度

学习的框架

都支持

GPU（Graphics Processing

Unit），可以

高速地处理

大量的运算

。另外，最

近的

框架也开始

支持多个GPU或

多台机器上

的分布式学

习。本节我们

将焦

点放在

深度学习的

计算的高速

化上，然后逐

步展开。深度

学习的实现

在8.1

节就结束

了，本节要讨

论的高速化

（支持GPU等）并不

进行实现。

8.3.1　需

要努力解决

的问题

在介

绍深度学习

的高速化之

前，我们先来

看一下深度

学习中什么

样的处

理比

较耗时。图8-14中

以AlexNet的forward处理为

对象，用饼图

展示了各

层

所耗费的时

间。

从图中可

知，AlexNex中，大多数

时间都被耗

费在卷积层

上。实际上，

卷

积层的处理

时间加起来

占GPU整体的95%，占

CPU整体的89%！因此

，

如何高速、高

效地进行卷

积层中的运

算是深度学

习的一大课

题。虽然图8-14

是

推理时的结

果，不过学习

时也一样，卷

积层中会耗

费大量时间

。

正如 7.2节介绍

的那样，卷积

层中进行的

运算可以追

溯至乘积累

加运算。

因此

，深度学习的

高速化的主

要课题就变

成了如何高

速、高效地进

行大量的乘

积累加运算

。

8.3 深度学习的

高速化  249

图8-14 AlexNet的

forward处理中各层

的时间比：左

边是使用GPU的

情况，右边是

使

用CPU的情况

。图中的“conv”对应

卷积层，“pool”对应

池化层，“fc”对应

全

连接层，“norm”对

应正规化层

（引用自文献

[26]）

8.3.2　基于GPU的高速

化

GPU原本是作

为图像专用

的显卡使用

的，但最近不

仅用于图像

处理，

也用于

通用的数值

计算。由于GPU可

以高速地进

行并行数值

计算，因此

GPU计

算的目标就

是将这种压

倒性的计算

能力用于各

种用途。所谓

GPU计算，

是指基

于GPU进行通用

的数值计算

的操作。

深度

学习中需要

进行大量的

乘积累加运

算（或者大型

矩阵的乘积

运算）。

这种大

量的并行运

算正是GPU所擅

长的（反过来

说，CPU比较擅长

连续的、

复杂

的计算）。因此

，与使用单个

CPU相比，使用GPU进

行深度学习

的运算

可以

达到惊人的

高速化。下面

我们就来看

一下基于GPU可

以实现多大

程度的

高速

化。图8-15是基于

CPU和GPU进行AlexNet的学

习时分别所

需的时间。

从

图中可知，使

用CPU要花40天以

上的时间，而

使用GPU则可以

将时

间缩短

至6天。此外，还

可以看出，通

过使用cuDNN这个

最优化的库

，可

以进一步

实现高速化

。

GPU主要由NVIDIA和AMD两

家公司提供

。虽然两家的

GPU都可以

用于

通用的数值

计算，但与深

度学习比较

“亲近”的是NVIDIA的

GPU。实

际上，大多

数深度学习

框架只受益

于NVIDIA的GPU。这是因

为深度学习

的框架中使

用了NVIDIA提供的

CUDA这个面向GPU计

算的综合开

发环境。

  第

8章

深度学习 250

图

8-15中出现的cuDNN是

在CUDA上运行的

库，它里面实

现了为深度

学习

最优化

过的函数等

。

图8-15 使用CPU 的“16-core Xeon

CPU”和

GPU的“Titan系列”进行

AlexNet的

学习时分

别所需的时

间（引用自文

献[27]）

通过im2col可以

将卷积层进

行的运算转

换为大型矩

阵的乘积。这

个

im2col方式的实

现对

GPU来说是

非常方便的

实现方式。这

是因为，

相比

按小规模的

单位进行计

算，GPU更擅长计

算大规模的

汇总好的

数

据。也就是说

，通过基于im2col以

大型矩阵的

乘积的方式

汇总计算，

更

容易发挥出

GPU的能力。

8.3.3　分布

式学习

虽然

通过GPU可以实

现深度学习

运算的高速

化，但即便如

此，当网络

较

深时，学习还

是需要几天

到几周的时

间。并且，前面

也说过，深度

学习伴

随着

很多试错。为

了创建良好

的网络，需要

反复进行各

种尝试，这样

一来就

必然

会产生尽可

能地缩短一

次学习所需

的时间的要

求。于是，将深

度学习的

学

习过程扩展

开来的想法

（也就是分布

式学习）就变

得重要起来

。

为了进一步

提高深度学

习所需的计

算的速度，可

以考虑在多

个GPU或

8.3  深度学

习的高速化

251

者多台机器

上进行分布

式计算。现在

的深度学习

框架中，出现

了好几个支

持

多GPU或者多

机器的分布

式学习的框

架。其中，Google的TensorFlow、微

软的CNTK（Computational Network Toolki）在开发

过程中高度

重视分布式

学习。以大型

数据中心的

低延迟·高吞

吐网络作为

支撑，基于这

些框架的分

布式学习呈

现出惊人的

效果。

基于分

布式学习，可

以达到何种

程度的高速

化呢？图8-16中显

示了基于

TensorFlow的

分布式学习

的效果。

图8-16 基

于TensorFlow的分布式

学习的效果

：横轴是GPU的个

数，纵轴是与

单个

GPU相比时

的高速化率

（引用自文献

[28]）

如图8-16所示，随

着GPU个数的增

加，学习速度

也在提高。实

际上，

与使用

1个GPU时相比，使

用100个GPU（设置在

多台机器上

，共100个）

似乎可

以实现56倍的

高速化！这意

味着之前花

费7天的学习

只要3个小时

就

能完成，充

分说明了分

布式学习惊

人的效果。

关

于分布式学

习，“如何进行

分布式计算

”是一个非常

难的课题。它

包

含了机器

间的通信、数

据的同步等

多个无法轻

易解决的问

题。可以将这

些难

题都交

给TensorFlow等优秀的

框架。这里，我

们不讨论分

布式学习的

细节。

关于分

布式学习的

技术性内容

，请参考TensorFlow的技

术论文（白皮

书）等。

  第

8章　深

度学习 252

8.3.4

运算

精度的位数

缩减

在深度

学习的高速

化中，除了计

算量之外，内

存容量、总线

带宽等也有

可能成为瓶

颈。关于内存

容量，需要考

虑将大量的

权重参数或

中间数据放

在

内存中。关

于总线带宽

，当流经GPU（或者

CPU）总线的数据

超过某个限

制时，

就会成

为瓶颈。考虑

到这些情况

，我们希望尽

可能减少流

经网络的数

据的位数。

计

算机中为了

表示实数，主

要使用64位或

者32位的浮点

数。通过使用

较多的位来

表示数字，虽

然数值计算

时的误差造

成的影响变

小了，但计算

的

处理成本

、内存使用量

却相应地增

加了，还给总

线带宽带来

了负荷。

关于

数值精度（用

几位数据表

示数值），我们

已经知道深

度学习并不

那么

需要数

值精度的位

数。这是神经

网络的一个

重要性质。这

个性质是基

于神经

网络

的健壮性而

产生的。这里

所说的健壮

性是指，比如

，即便输入图

像附有

一些

小的噪声，输

出结果也仍

然保持不变

。可以认为，正

是因为有了

这个健

壮性

，流经网络的

数据即便有

所“劣化”，对输

出结果的影

响也较小。

计

算机中表示

小数时，有32位

的单精度浮

点数和64位的

双精度浮点

数

等格式。根

据以往的实

验结果，在深

度学习中，即

便是16位的半

精度浮点

数

（half float），也可以顺利

地进行学习

[30]。实际上，NVIDIA的下

一代GPU

框架Pascal也

支持半精度

浮点数的运

算，由此可以

认为今后半

精度浮点数

将

被作为标

准使用。

NVIDIA的Maxwell GPU虽

然支持半精

度浮点数的

存储（保存数

据

的功能），但

是运算本身

不是用

16位进

行的。下一代

的 Pascal框架，

因为

运算也是用

16位进行的，所

以只用半精

度浮点数进

行计算，就

有

望实现超过

上一代

GPU约 2倍

的高速化。

以

往的深度学

习的实现中

并没有注意

数值的精度

，不过Python中一般

使用64位的浮

点数。NumPy中提供

了16位的半精

度浮点数类

型（不过，只

有

16位类型的存

储，运算本身

不用16位进行

），即便使用NumPy的

半精度

浮点

数，识别精度

也不会下降

。相关的论证

也很简单，有

兴趣的读者

请参考

ch08/half_float_network.py。

8.4

深度

学习的应用

案例  253

关于深

度学习的位

数缩减，到目

前为止已有

若干研究。最

近有人提出

了

用1位来表

示权重和中

间数据的Binarized Neural Networks方

法[31]。为了实

现

深度学习的

高速化，位数

缩减是今后

必须关注的

一个课题，特

别是在面向

嵌入式应用

程序中使用

深度学习时

，位数缩减非

常重要。

8.4 深度

学习的应用

案例

前面，作

为使用深度

学习的例子

，我们主要讨

论了手写数

字识别的图

像类别分类

问题（称为“物

体识别”）。不过

，深度学习并

不局限于物

体识别，

还可

以应用于各

种各样的问

题。此外，在图

像、语音、自然

语言等各个

不同

的领域

，深度学习都

展现了优异

的性能。本节

将以计算机

视觉这个领

域为中

心，介

绍几个深度

学习能做的

事情（应用）。

8.4.1　物

体检测

物体

检测是从图

像中确定物

体的位置，并

进行分类的

问题。如图8-17所

示，要从图像

中确定物体

的种类和物

体的位置。

图

8-17　物体检测的

例子（引用自

文献[34]）

第 8章　深

度学习 254

观察

图8-17可知，物体

检测是比物

体识别更难

的问题。之前

介绍的物体

识别是以整

个图像为对

象的，但是物

体检测需要

从图像中确

定类别的位

置，

而且还有

可能存在多

个物体。

对于

这样的物体

检测问题，人

们提出了多

个基于CNN的方

法。这些方

法

展示了非常

优异的性能

，并且证明了

在物体检测

的问题上，深

度学习是非

常有效的。

在

使用CNN进行物

体检测的方

法中，有一个

叫作R-CNN[35]的有名

的方

法。图8-18显

示了R-CNN的处理

流。

图8-18 R-CNN的处理

流（引用自文

献[35]）

希望大家

注意图中的

“2.Extract region proposals”（候选区域的

提取）和

“3.Compute

CNN features”（CNN特征

的计算）的处

理部分。这里

，首先（以

某种

方法）找出形

似物体的区

域，然后对提

取出的区域

应用CNN进行分

类。

R-CNN中会将图

像变形为正

方形，或者在

分类时使用

SVM（支持向量机

），

实际的处理

流会稍微复

杂一些，不过

从宏观上看

，也是由刚才

的两个处理

（候

选区域的

提取和CNN特征

的计算）构成

的。

在R-CNN的前半

部分的处理

——候选区域的

提取（发现形

似目标物体

的

处理）中，可

以使用计算

机视觉领域

积累的各种

各样的方法

。R-CNN的论文

中使

用了一种被

称为Selective Search的方法

，最近还提出

了一种基于

CNN

来进行候选

区域提取的

Faster R-CNN方法[36]。Faster

R-CNN用一个

CNN

来完成所有

处理，使得高

速处理成为

可能。

8.4

深度学

习的应用案

例  255

8.4.2

图像分割

图像分割是

指在像素水

平上对图像

进行分类。如

图8-19所示，使用

以像

素为单

位对各个对

象分别着色

的监督数据

进行学习。然

后，在推理时

，对输

入图像

的所有像素

进行分类。

图

8-19　图像分割的

例子（引用自

文献[34]）：左边是

输入图像，右

边是监督用

的带标签图

像

之前实现

的神经网络

是对图像整

体进行了分

类，要将它落

实到像素水

平

的话，该怎

么做呢？

要基

于神经网络

进行图像分

割，最简单的

方法是以所

有像素为对

象，对

每个像

素执行推理

处理。比如，准

备一个对某

个矩形区域

中心的像素

进行分

类的

网络，以所有

像素为对象

执行推理处

理。正如大家

能想到的，这

样的

方法需

要按照像素

数量进行相

应次forward处理，因

而需要耗费

大量的时间

（正确地说，卷

积运算中会

发生重复计

算很多区域

的无意义的

计算）。为了解

决这个无意

义的计算问

题，有人提出

了一个名为

FCN（Fully

Convolutional 

Network）[37]的方法。该方

法通过一次

forward处理，对所有

像素进行分

类（图

8-20）。

FCN的字面

意思是“全部

由卷积层构

成的网络”。相

对于一般的

CNN包

含全连接

层，FCN将全连接

层替换成发

挥相同作用

的卷积层。在

物体识别

中

使用的网络

的全连接层

中，中间数据

的空间容量

被作为排成

一列的节点

进

第 8章　深度

学习 256

行处理

，而只由卷积

层构成的网

络中，空间容

量可以保持

原样直到最

后的输出。

如

图8-20所示，FCN的特

征在于最后

导入了扩大

空间大小的

处理。基

于这

个处理，变小

了的中间数

据可以一下

子扩大到和

输入图像一

样的大小。

FCN最

后进行的扩

大处理是基

于双线性插

值法的扩大

（双线性插值

扩大）。

FCN中，这个

双线性插值

扩大是通过

去卷积（逆卷

积运算）来实

现的（细节请

参考FCN的论文

[37]）。

图8-20 FCN的概略图

（引用自文献

[37]）

全连接层中

，输出和全部

的输入相连

。使用卷积层

也可以实现

与此

结构完

全相同的连

接。比如，针对

输入大小是

32×10×10（通道

数 32、高 10、长

10）的数据的全

连接层可以

替换成滤波

器大小为

32×10×10的

卷积层。如果

全连接层的

输出节点数

是 100，那么在

卷

积层准备 100个

32×10×10的滤波器就

可以实现完

全相同的处

理。

像这样，全

连接层可以

替换成进行

相同处理的

卷积层。

8.4.3　图像

标题的生成

有一项融合

了计算机视

觉和自然语

言的有趣的

研究，该研究

如图8-21所

示，给

出一个图像

后，会自动生

成介绍这个

图像的文字

（图像的标题

）。

给出一个图

像后，会像图

8-21一样自动生

成表示该图

像内容的文

本。比

如，左上

角的第一幅

图像生成了

文本“A person riding

a motorcycle on a

8.4  深度学

习的应用案

例

257

dirt road.”（在没有铺

装的道路上

骑摩托车的

人），而且这个

文本只从该

图像

自动生

成。文本的内

容和图像确

实是一致的

。并且，令人惊

讶的是，除了

“骑

摩托车”之

外，连“没有铺

装的道路”都

被正确理解

了。

一个基于

深度学习生

成图像标题

的代表性方

法是被称为

NIC（Neural 

Image

Caption）的模型。如图

8-22所示，NIC由深层

的CNN和处理自

然语

言的RNN（Recurrent Neural Network）构

成。RNN是呈递归

式连接的网

络，

经常被用

于自然语言

、时间序列数

据等连续性

的数据上。

NIC基

于CNN从图像中

提取特征，并

将这个特征

传给RNN。RNN以

CNN提取

出的特征为

初始值，递归

地生成文本

。这里，我们不

深入讨论技

术上的细节

，不过基本上

NIC是组合了两

个神经网络

（CNN和RNN）的简单

结

构。基于NIC，可以

生成惊人的

高精度的图

像标题。我们

将组合图像

和自

然语言

等多种信息

进行的处理

称为多模态

处理。多模态

处理是近年

来备受关

注

的一个领域

。

图8-21　基于深度

学习的图像

标题生成的

例子（引用自

文献[38]）

第 8章　深

度学习

258

图8-22 Neural Image

Caption（NIC）的

整体结构（引

用自文献[38]）

RNN的

R表示Recurrent（递归的

）。这个递归指

的是神经网

络的递归

的

网络结构。根

据这个递归

结构，神经网

络会受到之

前生成的信

息

的影响（换

句话说，会记

忆过去的信

息），这是

RNN的特

征。比如，

生成

“我”这个词之

后，下一个要

生成的词受

到“我”这个词

的影响，

生成

了“要”；然后，再

受到前面生

成的“我要”的

影响，生成了

“睡觉”

这个词

。对于自然语

言、时间序列

数据等连续

性的数据，RNN以

记

忆过去的

信息的方式

运行。

8.5 深度学

习的未来

深

度学习已经

不再局限于

以往的领域

，开始逐渐应

用于各个领

域。本节

将介

绍几个揭示

了深度学习

的可能性和

未来的研究

。

8.5.1　图像风格变

换

有一项研

究是使用深

度学习来“绘

制”带有艺术

气息的画。如

图8-23所示，

输入

两个图像后

，会生成一个

新的图像。两

个输入图像

中，一个称为

“内容

图像”，另

一个称为“风

格图像”。

如图

8-23所示，如果指

定将梵高的

绘画风格应

用于内容图

像，深度学习

就会按照指

示绘制出新

的画作。此项

研究出自论

文“A Neural

Algorithm of 

8.5

深度学习

的未来  259

Artistic

Style”[39]，一经

发表就受到

全世界的广

泛关注。

这里

我们不会介

绍这项研究

的详细内容

，只是叙述一

下这个技术

的大致

框架

，即刚才的方

法是在学习

过程中使网

络的中间数

据近似内容

图像的中间

数据。这样一

来，就可以使

输入图像近

似内容图像

的形状。此外

，为了从风

格

图像中吸收

风格，导入了

风格矩阵的

概念。通过在

学习过程中

减小风格矩

阵的偏差，就

可以使输入

图像接近梵

高的风格。

8.5.2　图

像的生成

刚

才的图像风

格变换的例

子在生成新

的图像时输

入了两个图

像。不同于

这

种研究，现在

有一种研究

是生成新的

图像时不需

要任何图像

（虽然需要事

图8-23

基于论文

“A Neural Algorithm of

Artistic Style”的图像风格

变换的例子

：左上

角是风

格图像，右上

角是内容图

像，下面的图

像是新生成

的图像（图像

引用自文献

[40]）

第 8章　深度学

习 260

先使用大

量的图像进

行学习，但在

“画”新图像时

不需要任何

图像）。比如，

基

于深度学习

，可以实现从

零生成“卧室

”的图像。图8-24中

展示的图像

是

基 于

DCGAN（Deep Convolutional Generative Adversarial

Network）[41] 方

法

生成的卧室

图像的例子

。

图8-24

基于DCGAN生成

的新的卧室

图像（引用自

文献[41]）

图8-24的图

像可能看上

去像是真的

照片，但其实

这些图像都

是基于

DCGAN新生

成的图像。也

就是说，DCGAN生成

的图像是谁

都没有见过

的

图像（学习

数据中没有

的图像），是从

零生成的新

图像。

能画出

以假乱真的

图像的DCGAN会将

图像的生成

过程模型化

。使用大

量图

像（比如，印有

卧室的大量

图像）训练这

个模型，学习

结束后，使用

这

个模型，就

可以生成新

的图像。

DCGAN中使

用了深度学

习，其技术要

点是使用了

Generator（生成者）

和Discriminator（识

别者）这两个

神经网络。Generator生

成近似真品

的图

像，Discriminator判别

它是不是真

图像（是Generator生成

的图像还是

实际

拍摄的

图像）。像这样

，通过让两者

以竞争的方

式学习，Generator会学

习到

更加精

妙的图像作

假技术，Discriminator则会

成长为能以

更高精度辨

别真假

的鉴

定师。两者互

相切磋、共同

成长，这是GAN（Generative Adversarial 

8.5

深

度学习的未

来  261

Network）这个技术

的有趣之处

。在这样的切

磋中成长起

来的Generator最终

会

掌握画出足

以以假乱真

的图像的能

力（或者说有

这样的可能

）。

之前我们见

到的机器学

习问题都是

被称为监督

学习（supervised

learning）的问题

。这类问题就

像手写数字

识别一样，使

用的是图像

数据和教师

标签成对给

出的数据集

。不过这里讨

论的问题，并

没有

给出监

督数据，只给

了大量的图

像（图像的集

合），这样的问

题称为

无监

督学习（unsupervised learning）。无监

督学习虽然

是很早之前

就

开始研究

的领域（Deep Belief

Network、Deep Boltzmann Machine

等很

有名），但最近

似乎并不是

很活跃。今后

，随着使用深

度学习的

DCGAN等

方法受到关

注，无监督学

习有望得到

进一步发展

。

8.5.3　自动驾驶

计

算机代替人

类驾驶汽车

的自动驾驶

技术有望得

到实现。除了

汽车制造

商

之外，IT企业、大

学、研究机构

等也都在为

实现自动驾

驶而进行着

激烈

的竞争

。自动驾驶需

要结合各种

技术的力量

来实现，比如

决定行驶路

线的路

线计

划（path plan）技术、照相

机或激光等

传感技术等

，在这些技术

中，正

确识别

周围环境的

技术据说尤

其重要。这是

因为要正确

识别时刻变

化的环境、

自

由来往的车

辆和行人是

非常困难的

。

如果可以在

各种环境中

稳健地正确

识别行驶区

域的话，实现

自动驾驶可

能也就没那

么遥远了。最

近，在识别周

围环境的技

术中，深度学

习的力量备

受期待。比如

，基于CNN的神经

网络SegNet[42]，可以像

图8-25那样高精

度

地识别行

驶环境。

图8-25中

对输入图像

进行了分割

（像素水平的

判别）。观察结

果可知，在

某

种程度上正

确地识别了

道路、建筑物

、人行道、树木

、车辆等。今后

若能

基于深

度学习使这

种技术进一

步实现高精

度化、高速化

的话，自动驾

驶的实

用化

可能也就没

那么遥远了

。

第 8章　深度学

习

262

图8-25 基于深

度学习的图

像分割的例

子：道路、车辆

、建筑物、人行

道等被高精

度地识

别了

出来（引用自

文献[43]）

8.5.4　Deep Q-Network（强化学

习）

就像人类

通过摸索试

验来学习一

样（比如骑自

行车），让计算

机也在摸索

试验的过程

中自主学习

，这称为强化

学习（reinforcement

learning）。强化学

习和有“教师

”在身边教的

“监督学习”有

所不同。

强化

学习的基本

框架是，代理

（Agent）根据环境选

择行动，然后

通过这

个行

动改变环境

。根据环境的

变化，代理获

得某种报酬

。强化学习的

目的是

决定

代理的行动

方针，以获得

更好的报酬

（图8-26）。

图8-26中展示

了强化学习

的基本框架

。这里需要注

意的是，报酬

并不是

确定

的，只是“预期

报酬”。比如，在

《超级马里奥

兄弟》这款电

子游戏中，

让

马里奥向右

移动能获得

多少报酬不

一定是明确

的。这时需要

从游戏得分

（获

得的硬币

、消灭的敌人

等）或者游戏

结束等明确

的指标来反

向计算，决定

“预

期报酬”。如

果是监督学

习的话，每个

行动都可以

从“教师”那里

获得正确的

评价。

在使用

了深度学习

的强化学习

方法中，有一

个叫作Deep Q-Network（通

称

DQN）[44]的方法。该方

法基于被称

为Q学习的强

化学习算法

。这里省略

8.5 深

度学习的未

来  263

Q学习的细

节，不过在Q学

习中，为了确

定最合适的

行动，需要确

定一个被

称

为最优行动

价值函数的

函数。为了近

似这个函数

，DQN使用了深度

学习

（CNN）。

在DQN的研

究中，有让电

子游戏自动

学习，并实现

了超过人类

水平的

操作

的例子。如图

8-27所示，DQN中使用

的CNN 把游戏图

像的帧（连续

4帧）

作为输入

，最终输出游

戏手柄的各

个动作（控制

杆的移动量

、按钮操作的

有

无等）的“价

值”。

之前在学

习电子游戏

时，一般是把

游戏的状态

（人物的地点

等）事先提

取

出来，作为数

据给模型。但

是，在DQN中，如图

8-27所示，输入数

据

只有电子

游戏的图像

。这是DQN值得大

书特书的地

方，可以说大

幅提高了

DQN的

实用性。为什

么呢？因为这

样就无需根

据每个游戏

改变设置，只

要

给DQN游戏图

像就可以了

。实际上，DQN 可以

用相同的结

构学习《吃豆

人》、

Atari等很多游

戏，甚至在很

多游戏中取

得了超过人

类的成绩。

人

工智能

AlphaGo[45]击败

围棋冠军的

新闻受到了

广泛关注。这

个

AlphaGo技术的内

部也用了深

度学习和强

化学习。AlphaGo学习

了

3000万个专业

棋手的棋谱

，并且不停地

重复自己和

自己的对战

，积

累了大量

的学习经验

。AlphaGo和

DQN都是 Google的 Deep Mind

公

司进行的研

究，该公司今

后的研究值

得密切关注

。

图8-26　强化学习

的基本框架

：代理自主地

进行学习，以

获得更好的

报酬

代理

行

动

环境

报酬

（观测）

第 8章　深

度学习 264

8.6 小结

本章我们实

现了一个（稍

微）深层的CNN，并

在手写数字

识别上获得

了

超过99%的高

识别精度。此

外，还讲解了

加深网络的

动机，指出了

深度学习

在

朝更深的方

向前进。之后

，又介绍了深

度学习的趋

势和应用案

例，以及对

高

速化的研究

和代表深度

学习未来的

研究案例。

深

度学习领域

还有很多尚

未揭晓的东

西，新的研究

正一个接一

个地出现。

今

后，全世界的

研究者和技

术专家也将

继续积极从

事这方面的

研究，一定能

实现目前无

法想象的技

术。

感谢读者

一直读到本

书的最后一

章。如果读者

能通过本书

加深对深度

学

习的理解

，体会到深度

学习的有趣

之处，笔者将

深感荣幸。

图

8-27 基于Deep Q-Network学习电

子游戏的操

作。输入是电

子游戏的图

像，经过摸索

试验，学习出

让专业玩家

都自愧不如

的游戏手柄

（操作杆）的操

作手法（引用

自文

献[44]）

8.6 小结

265

本章所学的

内容

• 对于大

多数的问题

，都可以期待

通过加深网

络来提高性

能。

•

在最近的

图像识别大

赛ILSVRC中，基于深

度学习的方

法独占鳌头

，

使用的网络

也在深化。

• VGG、GoogLeNet、ResNet等

是几个著名

的网络。

• 基于

GPU、分布式学习

、位数精度的

缩减，可以实

现深度学习

的高速化。

• 深

度学习（神经

网络）不仅可

以用于物体

识别，还可以

用于物体检

测、

图像分割

。

• 深度学习的

应用包括图

像标题的生

成、图像的生

成、强化学习

等。最近，

深度

学习在自动

驾驶上的应

用也备受期

待。

附录 A

Softmax-with-Loss层的

计算图

这里

，我们给出softmax函

数和交叉熵

误差的计算

图，来求它们

的反向

传播

。softmax函数称为softmax层

，交叉熵误差

称为Cross Entropy Error层，

两者

的组合称为

Softmax-with-Loss

层。先来看一

下结果，Softmax-with￾Loss层可

以画成图A-1所

示的计算图

。

Softmax

L

1

Cross

Entropy

Error

a1

a2

a3

y1−

t 1

y2− t

2

y3− t 3

t 1

y1

t

2

y2

t 3

y3

图A-1 Softmax-with-Loss层的计算

图

附录A　Softmax-with-Loss层的

计算图 268

图A-1的

计算图中假

定了一个进

行3类别分类

的神经网络

。从前面的层

输入的是(a1,

a2, a3)，softmax层

输出(y1, y2, y3)。此外，教

师标签是(t1,

t2, t3)，

Cross Entropy

Error层

输出损失L。

如

图A-1所示，在本

附录中，Softmac-with-Loss层的

反向传播的

结果为

(y1 −

t1, y2 − t2,

y3 − t3)。

A.1

正向

传播

图A-1的计

算图中省略

了Softmax层和Cross Entropy Error层的

内容。

这里，我

们来画出这

两个层的内

容。

首先是Softmax层

。softmax函数可由下

式表示。

（A.1）

因此

，用计算图表

示Softmax层的话，则

如图A-2所示。

图

A-2的计算图中

，指数的和（相

当于式(A.1)的分

母）简写为S，最

终

的输出记

为(y1, y2, y3)。

接下来是

Cross Entropy Error层。交叉熵误

差可由下式

表示。

（A.2）

根据式

(A.2)，Cross Entropy Error层的计算图

可以画成图

A-3那样。

图A-3的计

算图很直观

地表示出了

式(A.2)，所以应该

没有特别难

的地方。

下一

节，我们将看

一下反向传

播。

A.1 正向传播

269

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) +

exp(a2) + exp(a3)

S

1

S

1

y1

S

exp(a1)

y2

S

exp(a2)

y3

S

exp(a3)

exp(a1)

图A-2 Softmax层的计算

图（仅正向传

播）

图A-3

Cross Entropy Error层的计

算图（仅正向

传播）

log

log

log

L

t

1

y1

t 2

y2

t 3

y3

−1

log y1

log

y2

log y3

t

1 log y1

t

2 log y2

t

3 log y3

t1

log y1 + t2

log y2 + t3

log y3

=

=

=

=

附录A　Softmax-with-Loss层

的计算图 270

A.2

反

向传播

首先

是Cross Entropy Error层的反向

传播。Cross

Entropy Error层的

反

向传播可以

画成图A-4那样

。

图A-4

交叉熵误

差的反向传

播

log

log

log

L

t 1

y1

t 2

y2

t

3

y3

−1

log

y1

log y2

log

y3

t 1 log

y1

t 2 log

y2

t 3 log

y3

t1 log y1

+ t2 log y2

+ t3 log y3

1 −1 −1

−1

−1

−t 1

−t

2

−t 3

t

1 − y1

t

2 − y2

t

3 − y3

求这个计

算图的反向

传播时，要注

意下面几点

。

• 反向传播的

初始值（图A-4中

最右边的值

）是1（因为 ）。

•“

×”节点

的反向传播

将正向传播

时的输入值

翻转，乘以上

游传过来的

导数后，再传

给下游。

•“+”节点

将上游传来

的导数原封

不动地传给

下游。

•“log”节点的

反向传播遵

从下式。

A.2

反向

传播  271

遵从以

上几点，就可

以轻松求得

Cross

Entropy Error的反向传播

。结

果 是传给

Softmax层的反向传

播的输入。

下

面是Softmax层的反

向传播的步

骤。因为Softmax层有

些复杂，所以

我们来逐一

进行确认。

步

骤1

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2) +

exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1 −

y1

t 2 −

y2

t 3 −

y3

前面的层

（Cross Entropy Error层）的反向传

播的值传过

来。

=

=

附录A　Softmax-with-Loss层的

计算图 272

步骤

2

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) +

exp(a2) + exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t

1 - y1

t

2 - y2

t

3 - y3

−t

3S

“×”节点将正向

传播的值翻

转后相乘。这

个过程中会

进行下面的

计算。

（A.3）

=

=

− 2 t

S

−t 1S

A.2

反向传

播  273

步骤3

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) +

exp(a2) + exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t

1 − y1

t

2 − y2

t

3 − y3

−t

3S

S

1 (t

1 + t 2

+ t 3)

S

1

正向

传播时若有

分支流出，则

反向传播时

它们的反向

传播的值会

相加。

因此，这

里分成了三

支的反向传

播的值(−t1S, −t2S,

−t3S)会被

求和。然后，

还

要对这个相

加后的值进

行“/”节点的反

向传播，结果

为 。

这里，(t1,

t2, t3)是教

师标签，也是

one-hot向量。one-hot向量意

味着(t1, t2, t3)

中只有

一个元素是

1，其余都是0。因

此，(t1, t2, t3)的和为1。

=

=

=

− 2

t S

−t 1S

附

录A　Softmax-with-Loss层的计算

图 274

步骤4

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1)

+ exp(a2) + exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1 − y1

t 2 − y2

t 3 − y3

−t 3S

S

1

S

1

“+”节点

原封不动地

传递上游的

值。

=

=

− 2 t

S

−t 1S

A.2

反向传播

275

步骤5

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2)

+ exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1

− y1

t 2

− y2

t 3

− y3

−t 3S

S

1

S

1

t 1 − y1

t 1 − exp(a1)

S

1 =

“×”节点将

值翻转后相

乘。这里，式子

变形时使用

了

。

=

=

−

2 t S

−t

1S

  附录A

Softmax-with-Loss层的

计算图 276

步骤

6

exp

exp

exp

S

a1

a2

a3

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2)

+ exp(a3)

S

1

S

1

y1

S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1

− y1

t 2

− y2

t 3

− y3

−t 3S

S

1

S

1

t 1 − exp(a1)

y1 − t 1

S

exp(a1) − t

1 =

“exp”节点中有下

面的关系式

成立。

（A.4）

根据这

个式子，向两

个分支的输

入和乘以exp(a1)后

的值就是我

们要求

的反

向传播。用式

子写出来的

话，就是 ，整理

之后为

y1

− t1。综上

，我们推导出

，正向传播时

输入是a1的节

点，它的反向

传播是

y1 −

t1。剩下

的a2、a3也可以按

照相同的步

骤求出来（结

果分别为y2 − t2和

y3 −

t3）。此外，除了这

里介绍的3类

别分类外，对

于n类别分类

的情况，也

可

以推导出同

样的结果。

=

=

− 2 t S

−t 1S

A.3 小

结

277

A.3 小结

上面

，我们画出了

Softmax-with-Loss层的计算图

的全部内容

，并求了

它的

反向传播。未

做省略的Softmax-with-Loss层

的计算图如

图A-5所示。

图A-5 Softmax-with-Loss层

的计算图

×

×

×

×

log

log

log

×

×

exp

exp

exp

＋

/ S

L

Softmax

层

Cross Entropy Error 层

＋ ×

a1

a2

a3

y1− t1

y2−

t2

y3− t3

exp(a1)

exp(a2)

exp(a3)

S

1

S

1

S

1

S

1

exp(a1)

exp(a1)

t1 −

exp(a2)

exp(a2)

t2 −

exp(a3)

exp(a3)

t3 −

−t3S

S

1

S

1

S

1

t1

t2

t3

y1

y2

y3

y1

t1 −

y2

t2

−

y3

t3 −

log y1

log y2

log y3

− t1

− t2

− t3

t1 log y1 +

t2 log y2 +

t3 log y3

t1

log y1

t2 log

y2

t3 log y3

−1

−1

−1

−1

−1

1

图A-5的计算

图看上去很

复杂，但是使

用计算图逐

个确认的话

，求导（反

向传

播的步骤）也

并没有那么

复杂。除了这

里介绍的Softmax-with-Loss层

，

遇到其他看

上去很难的

层（如Batch Normalization层）时，请

一定按照这

里

的步骤思

考一下。相信

会比只看数

学式更容易

理解。

−t2S

−t1S

参考文

献

Python /

NumPy

[1] Bill Lubanovic.

Introducing PythonA. O’Reilly Media,

2014.

[2] Wes McKinney.

Python for Data AnalysisB.

O’Reilly Media.

[3] Scipy

Lecture Notes.

计算图（误

差反向传播

法）

[4]

Andrej Karpathy’s blog “Hacker’s

guide to Neural Networks”.

深度学习

的在线课程

（资料）

[5] CS231n: Convolutional

Neural Networks for Visual

Recognition.

参数的

更新方法

[6] John

Duchi, Elad Hazan, and

Yoram Singer（2011）: Adaptive

Subgradient Methods for Online

Learning and Stochastic Optimization.

A 中

文版名为《Python语

言及其应用

》，梁杰等译，人

民邮电出版

社2015年出版。—编

者注

B 中文版

名为《利用Python进

行数据分析

》，唐学韬译，机

械工业出版

社2013年出版。—编

者注

参考文

献 280

Journal

of Machine Learning Research

12, Jul (2011), 2121

– 2159.

[7] Tieleman,

T., & Hinton, G.（2012）:

Lecture 6.5―RMSProp: Divide the

gradient by a running

average of its recent

magnitude. COURSERA: 

Neural

Networks for Machine Learning.

[8] Diederik Kingma and

Jimmy Ba.（2014）: Adam: A

Method for 

Stochastic

Optimization. arXiv:1412.6980 [cs] (December

2014).

权重参数

的初始值

[9] Xavier

Glorot and Yoshua Bengio（2010）:

Understanding the difficulty

of training deep feedforward

neural networks. In Proceedings

of the 

International

Conference on Artificial Intelligence

and Statistics 

(AISTATS2010).

Society for Artificial Intelligence

and Statistics.

[10] Kaiming

He, Xiangyu Zhang, Shaoqing

Ren, and Jian Sun（2015）:

Delving Deep into Rectifiers:

Surpassing Human-Level Performance on

ImageNet Classification. In 1026

– 1034.

Batch Normalization

/ Dropout

[11] Sergey

Ioffe and Christian Szegedy（2015）:

Batch Normalization: 

Accelerating

Deep Network Training by

Reducing Internal Covariate

Shift. arXiv:1502.03167 [cs] (February

2015).

[12] Dmytro Mishkin

and Jiri Matas（2015）: All

you need is a

good init. 

arXiv:1511.06422

[cs] (November 2015).

[13]

Frederik Kratzert’s blog “Understanding

the backward pass through

Batch 

Normalization Layer”.

[14] N. Srivastava, G.

Hinton, A. Krizhevsky, I.

Sutskever, and R.

Salakhutdinov（2014）: Dropout: A simple

way to prevent neural

参

考文献  281

networks

from overfitting. The Journal

of Machine Learning Research,

pages 1929 – 1958,

2014.

超参

数的最优化

[15] James Bergstra

and Yoshua Bengio（2012）: Random

Search for Hyper￾Parameter Optimization.

Journal of Machine Learning

Research 13, 

Feb

(2012), 281 – 305.

[16] Jasper Snoek, Hugo

Larochelle, and Ryan P.

Adams（2012）: Practical 

Bayesian

Optimization of Machine Learning

Algorithms. In F. Pereira,

C. J. C. Burges,

L. Bottou, & K.

Q. Weinberger, eds. Advances

in 

Neural Information

Processing Systems 25. Curran

Associates, Inc., 

2951

– 2959.

CNN的可视化

[17]

Matthew D. Zeiler and

Rob Fergus（2014）: Visualizing and

Understanding Convolutional Networks. In

David Fleet, Tomas Pajdla,

Bernt Schiele, & Tinne

Tuytelaars, eds. Computer Vision

– ECCV 2014.

Lecture Notes in Computer

Science. Springer International Publishing,

818 – 833.

[18]

A. Mahendran and A.

Vedaldi（2015）: Understanding deep image

representations by inverting them.

In 2015 IEEE Conference

on 

Computer Vision

and Pattern Recognition (CVPR).

5188 – 5196.

[19]

Donglai Wei, Bolei Zhou,

Antonio Torralba, William T.

Freeman

（2015）: mNeuron: A

Matlab Plugin to Visualize

Neurons from Deep

Models.

  参

考文献

282

具有

代表性的网

格

[20] Y.

Lecun, L. Bottou, Y.

Bengio, and P. Haffner（1998）:

Gradient-based 

learning applied

to document recognition. Proceedings

of the IEEE 86,

11 (November 1998), 2278

– 2324.

[21] Alex

Krizhevsky, Ilya Sutskever, and

Geoffrey E. Hinton（2012）:

ImageNet Classification with Deep

Convolutional Neural Networks.

In F. Pereira, C.

J. C. Burges, L.

Bottou, & K. Q.

Weinberger, eds. 

Advances

in Neural Information Processing

Systems 25. Curran

Associates, Inc., 1097 –

1105.

[22] Karen Simonyan

and Andrew Zisserman（2014）: Very

Deep 

Convolutional Networks

for Large-Scale Image Recognition.

arXiv:1409.1556 [cs] (September 2014).

[23] Christian Szegedy et

al（2015）: Going Deeper With

Convolutions. In 

The

IEEE Conference on Computer

Vision and Pattern Recognition

(CVPR).

[24] Kaiming He,

Xiangyu Zhang, Shaoqing Ren,

and Jian Sun（2015）:

Deep Residual Learning for

Image Recognition. arXiv:1512.03385 [cs]

(December 2015).

数据集

[25]

J. Deng, W. Dong,

R. Socher, L.J. Li,

Kai Li, and Li

Fei-Fei （2009）: 

ImageNet:

A large-scale hierarchical image

database. In IEEE

Conference on Computer Vision

and Pattern Recognition, 2009.

CVPR 

2009. 248

– 255.

参

考文献

283

计算

的高速化

[26] Jia

Yangqing（2014）: Learning Semantic Image

Representations at a

Large Scale. PhD thesis,

EECS Department, University of

California, 

Berkeley, May

2014.

[27] NVIDIA blog

“NVIDIA Propels Deep Learning

with TITAN X, New

DIGITS Training System and

DevBox”.

[28] Google Research

Blog “Announcing TensorFlow 0.8

– now with

distributed computing support!”.

[29]

Martín Abadi et al（2016）:

TensorFlow: Large-Scale Machine Learning

on 

Heterogeneous Distributed

Systems. arXiv:1603.04467 [cs] (March

2016).

[30] Suyog Gupta,

Ankur Agrawal, Kailash Gopalakrishnan,

and Pritish 

Narayanan（2015）:

Deep learning with limited

numerical precision. 

CoRR,

abs/1502.02551 392 (2015).

[31]

Matthieu Courbariaux and Yoshua

Bengio（2016）: Binarized Neural

Networks: Training Deep Neural

Networks with Weights and

Activations Constrained to +1

or -1. arXiv preprint

arXiv:1602.02830 

(2016).

MNIST数

据集识别精

度排行榜及

最高精度的

方法

[32] Rodrigo Benenson’s blog

“Classification datasets results”.

[33]

Li Wan, Matthew Zeiler,

Sixin Zhang, Yann L.

Cun, and Rob Fergus

（2013）: Regularization of Neural

Networks using DropConnect. In

Sanjoy Dasgupta & David

McAllester, eds. Proceedings of

the 30th 

International

Conference on Machine Learning

(ICML2013). JMLR 

Workshop

and Conference Proceedings, 1058

– 1066.

参考文

献 284

深度学习

的应用

[34]

Visual Object Classes Challenge

2012 VO(2012).

[35] Ross

Girshick, Jeff Donahue, Trevor

Darrell, and Jitendra Malik（2014）:

Rich Feature Hierarchies for

Accurate Object Detection and

Semantic 

Segmentation. In

580 – 587.

[36]

Shaoqing Ren, Kaiming He,

Ross Girshick, and Jian

Sun（2015）: Faster 

R-CNN:

Towards Real-Time Object Detection

with Region Proposal

Networks. In C. Cortes,

N. D. Lawrence, D.

D. Lee, M. Sugiyama,

& R. 

Garnett,

eds. Advances in Neural

Information Processing Systems 28.

Curran Associates, Inc., 91

– 99.

[37] Jonathan

Long, Evan Shelhamer, and

Trevor Darrell（2015）: Fully

Convolutional Networks for Semantic

Segmentation. In The IEEE

Conference on Computer Vision

and Pattern Recognition (CVPR).

[38] Oriol Vinyals, Alexander

Toshev, Samy Bengio, and

Dumitru Erhan 

（2015）:

Show and Tell: A

Neural Image Caption Generator.

In The 

IEEE

Conference on Computer Vision

and Pattern Recognition

(CVPR).

[39] Leon A.

Gatys, Alexander S. Ecker,

and Matthias Bethge（2015）: A

Neural Algorithm of Artistic

Style. arXiv:1508.06576 [cs, q-bio]

(August 

2015).

[40]

neural-style “Torch implementation of

neural style algorithm”.

[41]

Alec Radford, Luke Metz,

and Soumith Chintala（2015）: Unsupervised

Representation Learning with Deep

Convolutional Generative 

Adversarial

Networks. arXiv:1511.06434 [cs] (November

2015).

[42] Vijay Badrinarayanan,

Kendall, and Roberto Cipolla（2015）:

SegNet: 

A Deep

Convolutional Encoder-Decoder Architecture for

Image 

Segmentation. arXiv

preprint arXiv:1511.00561 (2015).

参考

文献

285

[43] SegNet Demo

page.

[44] Volodymyr Mnih

et al（2015）: Human-level control

through deep 

reinforcement

learning. Nature 518, 7540

(2015), 529 – 533.

[45] David Silver et

al（2016）: Mastering the game

of Go with deep

neural 

networks and

tree search. Nature 529,

7587 (2016), 484 –

489.

Copyright ©2016 Koki

Saitoh, O’Reilly Japan, Inc.

All rights reserved.

本书中

使用的系统

名、产品名都

是各公司的

商标或注册

商标。

正文中

有时会省略

TM、®、©等标识。

株式

会社O’ Reilly Japan尽最大

努力确保了

本书内容的

正确性，但对

运用本书内

容所

造成的

任何结果概

不负责，敬请

知悉。

፡ਠԧ

ইຎ఍੒๜ԡٖ਻ํወᳯ҅ݎݢᮒկᛗFRQWDFW#WXULQJERRNFRP҅տ

ํᖫᬋ౲֢ᦲᘏܐۗᒼወ̶Ԟݢᦢᳯࢶᅎᐒ܄҅݇Ө๜ԡᦎᦞ̶

ইຎฎํىኪৼԡጱୌᦓ౲ᳯ᷌҅᧗ᘶᔮӫአਮ๐ᮒᓟ

HERRN#WXULQJERRNFRP̶

౯ժکզತݢᬯࣁ

ஙܗ#ࢶᅎරᙙঅԡ̵ၚۖྯ෭ඎಸ

ஙܗ#ࢶᅎᐒ܄

ኪৼԡ޾অ෈ᒍጱၾ௳

ᅎරᙙጱᑀฦੜᕟࢶᅎෛᎣࢶ#ܗங

ஙמࢶᅎᦢ᧨

LWXULQJBLQWHUYLHZ҅ᦖᬿᎱ١ᔜ୸Ոኞ

ஙמࢶᅎරᙙWXULQJERRNV
