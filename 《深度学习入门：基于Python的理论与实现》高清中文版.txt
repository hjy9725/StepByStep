图灵社区的电子书没有

采用专有客户

端，您可以

在任意设备上，用自己喜

欢的浏览器和PDF阅读器进

行阅读。

但您购买的电子

书仅供您个人使用，

未经

授权，不得进行传播。

我们

愿意相信读者具有这样

的良知和

觉悟，与我们共

同保护知识产权。

如果购

买者有侵权行为，我们可

能对

该用户实施包括但

不限于关闭该帐号

等维

权措施，并可能追究法律

责任。

හਁᇇ๦्ก

图灵程序 设

计 丛 书

人 民 邮 电

出 版 社

北　　京

基

于Python的理论与实现

[日］斋藤

康毅 著

陆宇杰 译

深度学

习入门

Beijing・Boston・Farnham・Sebastopol・Tokyo

O’Reilly Japan, Inc. 授权人民邮电出

版社出版

Deep

Learning from Scratch

内 容 提

要

本书

是深度学习真正意义上

的入门书，深入浅出地剖

析了深度学习的原理和

相关技术。

书中使用Python 3，尽量

不依赖外部库或工具，带

领读者从零创建一个经

典的深度学习网

络，使读

者在此过程中逐步理解

深度学习。书中不仅介绍

了深度学习和神经网络

的概念、

特征等基础知识

，对误差反向传播法、卷积

神经网络等也有深入讲

解，此外还介绍了学

习相

关的实用技巧，自动驾驶

、图像生成、强化学习等方

面的应用，以及为什么加

深层

可以提高识别精度

等“为什么”的问题。

本书适

合深度学习初学者阅读

，也可作为高校教材使用

。

◆ 著 ［日］

斋藤康毅

 译

陆宇杰

责任编辑 杜晓静

 执行编

辑 刘香娣

责任印制 周昇

亮

◆ 人民邮电出版社出版

发行 北京市丰台区成寿

寺路11号

邮编 100164 电子邮件 315@ptpress.com.cn

网

址 http://www.ptpress.com.cn

北京

印刷

◆ 开本：880×1230 1/32

印张：9.625

 字

数：300千字 2018年7月第 1 版

印数：1 - 4 000册

2018年7月北京第1次印刷

著作

权合同登记号 图字：01-2017-0526号

定

价：59.00元

读者服务热线：(010)51095186转600　印

装质量热线：(010)81055316

反盗版热线

：(010)81055315

广告经营许可证：京东工

商广登字20170147号

深度学习入

门 : 基于Python的理论与实现 / (日

)

斋藤康毅著

; 陆宇杰译. -- 北

京 : 人民邮电出版社,

2018.7

 （图灵

程序设计丛书）

 ISBN 978-7-115-48558-8

Ⅰ. ①深… Ⅱ. ①斋… ②陆… Ⅲ.

①软

件工具－程

序设计 Ⅳ. ①TP311.561

 中国版

本图书馆CIP数据核字(2018)第112509号



图书在版编目（ＣＩＰ）数据

版 权

声 明

Copyright ©

2016 Koki Saitoh, O’Reilly Japan, Inc.

Posts and Telecommunications Press, 2018.

Authorized

translation of the Japanese edition of

“Deep Learning from

Scratch” © 2016

O’Reilly Japan, Inc. This translation is

published and sold by

permission of

O’Reilly Japan, Inc., the owner of

all rights to publish and sell

the

same.

日文原版由O’Reilly Japan, Inc.出版，2016。

简

体中文版由人民邮电出

版社出版，2018。日文原版的翻

译得到O’Reilly

Japan, Inc.的授权。此简体中

文版的出版和销售得到

出版权和销售权的所有

者——O’Reilly Japan, Inc.的许可。

版权所有，未得

书面许可，本书的任何部

分和全部不得以任何形

式重制。

O’Reilly

Media 通过图书、杂志、在

线服务、调查研究和会议

等方式传播创新知识。

自

1978 年开始，O’Reilly 一直都是前沿发

展的见证者和推动者。超

级极客们正在开

创着未

来，而我们关注真正重要

的技术趋势——通过放大那

些“细微的信号”来刺激

社

会对新科技的应用。作为

技术社区中活跃的参与

者，O’Reilly 的发展充满了对创新

的倡导、创造和发扬光大

。

O’Reilly 为软件开发人员带来革

命性的“动物书”；创建第一

个商业网站（GNN）；组

织了影响

深远的开放源代码峰会

，以至于开源软件运动以

此命名；创立了 Make

杂志，

从而

成为 DIY 革命的主要先锋；公

司一如既往地通过多种

形式缔结信息与人的纽

带。

O’Reilly 的会议和峰会集聚了

众多超级极客和高瞻远

瞩的商业领袖，共同描绘

出开创

新产业的革命性

思想。作为技术人士获取

信息的选择，O’Reilly 现在还将先

锋专家的

知识传递给普

通的计算机用户。无论是

通过书籍出版、在线服务

或者面授课程，每一

项 O’Reilly 的

产品都反映了公司不可

动摇的理念——信息是激发

创新的力量。

业界评论

“O’Reilly Radar 博

客有口皆碑。”

——Wired

“O’Reilly

凭借一系列

（真希望当初我也想到了

）非凡想法建立了数百万

美元的业务。”

——Business 2.0

“O’Reilly Conference 是聚集关键

思想领袖的绝对典范。”

——CRN

“一

本 O’Reilly 的书就代表一个有用

、有前途、需要学习的主题

。”

——Irish Times

“Tim 是位特立独行的商人，他

不光放眼于最长远、最广

阔的视野，并且切实地按

照

Yogi Berra 的建议去做了：‘如果你

在路上遇到岔路口，走小

路（岔路）。’回顾过去，

Tim

似乎每

一次都选择了小路，而且

有几次都是一闪即逝的

机会，尽管大路也不错。”

——Linux Journal

O’Reilly Media, Inc.介

绍

目录

译者序······················································· xiii

前言························································· xv

第1章

Python入门·

··········································· 1

1.1 Python是什么· ········································· 1

1.2 Python的安装· ········································· 2

1.2.1 Python版本

·

····································· 2

1.2.2　使用的外部库· ···································· 2

1.2.3 Anaconda发行版· ································· 3

1.3 Python解

释器·

········································· 4

1.3.1　算术计算········································· 4

1.3.2

数据类型

········································· 5

1.3.3　变量············································· 5

1.3.4

列表············································· 6

1.3.5　字典············································· 7

1.3.6

布尔型··········································· 7

1.3.7 if语

句· ·········································· 8

1.3.8 for 语句·········································· 8

1.3.9　函数·············································

9

1.4 Python脚本文件· ······································· 9

vi

目

录

1.4.1　保存为文件······································· 9

1.4.2

类· ············································ 10

1.5 NumPy· ··············································

11

1.5.1　导入NumPy· ···································· 11

1.5.2

生

成NumPy数组································· 12

1.5.3 NumPy 的算术运算······························· 12

1.5.4 NumPy的N维

数组· ······························ 13

1.5.5　广播············································

14

1.5.6　访问元素········································ 15

1.6 Matplotlib·············································

16

1.6.1　绘制

简单图形· ··································· 16

1.6.2

pyplot的功能· ··································· 17

1.6.3　显示图

像········································ 18

1.7 小结·················································· 19

第2章　感知机················································ 21

2.1 感知

机是什么· ········································· 21

2.2 简单逻辑电路

·

········································· 23

2.2.1　与门············································ 23

2.2.2

与非门和或门· ··································· 23

2.3 感知

机的实现· ·········································

25

2.3.1　简单的实现······································ 25

2.3.2　导

入权重和偏置·

································· 26

2.3.3　使用权重

和偏置的实现· ··························· 26

2.4 感知机的

局限性· ······································· 28

2.4.1　异或门··········································

28

2.4.2　线性和非

线性· ··································· 30

2.5

多层感知机············································ 31

2.5.1　已有门

电路的组合· ······························· 31

目录  vii

2.5.2　异或门

的实现· ···································

33

2.6 从与非门到计算

机· ····································· 35

2.7

小结·················································· 36

第3章　神经网络·············································· 37

3.1

从

感知机到神经网络· ··································· 37

3.1.1　神经

网络的例子· ·································

37

3.1.2　复习感知机

······································ 38

3.1.3　激活函数登场·

··································· 40

3.2 激活函数

·············································· 42

3.2.1

sigmoid函数· ···································· 42

3.2.2　阶跃函数的实现· ·································

43

3.2.3　阶

跃函数的图形· ································· 44

3.2.4

sigmoid函数的实

现· ······························ 45

3.2.5 sigmoid函数和阶跃函数的比

较······················ 46

3.2.6　非线性函数······································ 48

3.2.7 ReLU函数· ·····································

49

3.3 多维

数组的运算· ······································· 50

3.3.1

多维数组········································ 50

3.3.2　矩

阵乘法········································ 51

3.3.3

神经网络的内积

· ································· 55

3.4 3层神经网络的实现· ····································

56

3.4.1　符号

确认········································ 57

3.4.2　各层间信号传递的

实现·

··························· 58

3.4.3　代码实现小结· ··································· 62

3.5 输出

层的设计· ········································· 63

3.5.1　恒等函数和softmax函

数·

·························· 64

3.5.2　实现softmax函数时的注意事

项· ···················· 66

3.5.3 softmax函数的特征· ······························ 67

viii

目录

3.5.4　输出

层的神经元数量· ····························· 68

3.6

手写数

字识别· ········································· 69

3.6.1 MNIST数据集· ··································

70

3.6.2　神经网络

的推理处理· ····························· 73

3.6.3

批处理·········································· 75

3.7 小结

·················································· 79

第4章

神经网络的学习· ······································· 81

4.1 从

数据中学习· ·········································

81

4.1.1　数据驱动········································ 82

4.1.2　训

练数据和测试数据·

····························· 84

4.2 损失

函数·············································· 85

4.2.1

均方误差········································ 85

4.2.2　交叉熵误

差······································ 87

4.2.3

mini-batch学习· ································· 88

4.2.4 mini-batch版交叉熵误差的

实现· ····················

91

4.2.5　为何要设定损失函

数· ····························· 92

4.3

数值微分·············································· 94

4.3.1　导数············································ 94

4.3.2

数值微

分的例子· ································· 96

4.3.3　偏导数·········································· 98

4.4 梯度··················································100

4.4.1　梯

度法··········································102

4.4.2　神经网络的梯度·

·································106

4.5 学

习算法的实现· ·······································109

4.5.1 2层神经网

络的类·································110

4.5.2 mini-batch的实现· ·······························114

4.5.3　基于测试

数据的评价· ·····························116

4.6 小结··················································118

目录  ix

第

5章

误差反向传播法· ·······································121

5.1 计算

图················································121

5.1.1　用计算图求解·

···································122

5.1.2　局部计

算········································124

5.1.3　为何用计算图解题· ·······························125

5.2 链

式法则··············································126

5.2.1　计算图的反向传

播· ·······························127

5.2.2

什么是链式法则· ·································127

5.2.3　链式

法则和计算图· ·······························129

5.3

反向传播

··············································130

5.3.1　加法节点的反向传播· ·····························130

5.3.2　乘

法节点的反向传播·

·····························132

5.3.3　苹果

的例子······································133

5.4 简单层的实现· ·········································135

5.4.1　乘

法层的实现· ···································135

5.4.2　加法层的实

现· ···································137

5.5 激活函数层的实现· ·····································139

5.5.1 ReLU层

· ·······································139

5.5.2 Sigmoid层·······································141

5.6 Affine/Softmax层的实现·································144

5.6.1 Affine层·

·······································144

5.6.2　批版本的

Affine层· ·······························148

5.6.3 Softmax-with-Loss

层· ····························150

5.7 误差反向传播法的

实现· ·································154

5.7.1

神经网络学习的全

貌图· ···························154

5.7.2　对应误差反向传播

法的神经网络的实现· ··············155

5.7.3

误

差反向传播法的梯度确

认·························158

5.7.4　使用误差反向传播法

的学习·························159

5.8 小结··················································161

x

目录

第6章　与

学习相关的技巧· ·····································163

6.1

参数的

更新············································163

6.1.1　探险家的故事· ···································164

6.1.2 SGD·

··········································164

6.1.3 SGD的缺

点· ····································166

6.1.4 Momentum······································168

6.1.5 AdaGrad········································170

6.1.6 Adam· ·········································172

6.1.7

使用哪种更新方法呢

· ·····························174

6.1.8　基于MNIST数据集的更新方法

的比较················175

6.2 权重的初始值·

·········································176

6.2.1　可

以将权重初始值设为0吗

· ························176

6.2.2　隐藏层的激活值的分布

·

···························177

6.2.3 ReLU的权重初始值·······························181

6.2.4　基于MNIST数据

集的权重初始值的比较

· ·············183

6.3 Batch Normalization· ···································184

6.3.1 Batch

Normalization的算法· ·······················184

6.3.2 Batch Normalization的评估· ·······················186

6.4 正则化················································188

6.4.1　过

拟合··········································189

6.4.2　权值衰减········································191

6.4.3 Dropout· ·······································192

6.5 超参数的

验证· ·········································195

6.5.1　验证数据········································195

6.5.2　超参数的

最优化· ·································196

6.5.3

超参数最优化的

实现· ·····························198

6.6 小结··················································200

目录

xi

第7章　卷积

神经网络· ·········································201

7.1 整体结构··············································201

7.2 卷积

层················································202

7.2.1　全连接层存在的问题

· ·····························203

7.2.2

卷积运算········································203

7.2.3　填充············································206

7.2.4　步幅············································207

7.2.5

3维数

据的卷积运算· ······························209

7.2.6　结合方块

思考· ···································211

7.2.7

批处理··········································213

7.3 池化层················································214

7.4 卷积

层和池化层的实现· ·································216

7.4.1 4维数

组· ·······································216

7.4.2　基于im2col的展开· ·······························217

7.4.3　卷积层的

实现· ···································219

7.4.4　池化层的实现· ···································222

7.5 CNN的实

现· ··········································224

7.6 CNN的可视化· ········································228

7.6.1　第1层权重的

可视化·······························228

7.6.2　基于分层结构的

信息提取· ·························230

7.7

具有代表性的

CNN·····································231

7.7.1 LeNet· ·········································231

7.7.2 AlexNet·········································232

7.8 小结··················································233

第8章　深度学习··············································235

8.1 加深

网络··············································235

8.1.1　向更深的网络出发

· ·······························235

8.1.2　进一步提高识别精度· ·····························238

xii   目

录

8.1.3　加深层的动机·

···································240

8.2 深度学

习的小历史· ·····································242

8.2.1 ImageNet·

······································243

8.2.2 VGG· ··········································244

8.2.3 GoogLeNet·

·····································245

8.2.4 ResNet· ········································246

8.3 深度学习的

高速化·

·····································248

8.3.1　需要努力解决的

问题· ·····························248

8.3.2　基于GPU的高速化·

······························249

8.3.3　分布

式学习······································250

8.3.4　运算精度的位数

缩减· ·····························252

8.4 深度学习的应用案

例· ···································253

8.4.1　物体检测········································253

8.4.2

图像分割········································255

8.4.3　图

像标题的生成· ·································256

8.5 深度学习

的未来·

·······································258

8.5.1　图像风格变换· ···································258

8.5.2　图

像的生成······································259

8.5.3　自动驾驶········································261

8.5.4 Deep Q-Network（强化

学习）· ·······················262

8.6 小结··················································264

附录A Softmax-with-Loss层的计算

图· ···························267

A.1

正向传播· ············································268

A.2 反向传播· ············································270

A.3

小

结· ················································277

参考文献·····················································279

译者序

深度

学习的浪潮已经汹涌澎

湃了一段时间了，市面上

相关的图书也已经

出版

了很多。其中，既有知名学

者伊恩·古德费洛（Ian

Goodfellow）等人撰

写的系统介绍深度学习

基本理论的《深度学习》，也

有各种介绍深度学习框

架的使用方法的入门书

。你可能会问，现在再出一

本关于深度学习的书，是

不是“为时已晚”？其实并非

如此，因为本书考察深度

学习的角度非常独特，

它

的出版可以说是“千呼万

唤始出来”。

本书最大的特

点是“剖解”了深度学习的

底层技术。正如美国物理

学家

理查德·费曼（Richard Phillips Feynman）所说：“What

I cannot create, I

do not

understand.”只

有创造一个东西，才算真

正弄懂了一个问题。本书

就

是教你如何创建深度

学习模型的一本书。并且

，本书不使用任何现有的

深度

学习框架，尽可能仅

使用最基本的数学知识

和Python库，从零讲解深度学

习

核心问题的数学原理，从

零创建一个经典的深度

学习网络。

本书的日文版

曾一度占据了东京大学

校内书店（本乡校区）理工

类图书

的畅销书榜首。各

类读者阅读本书，均可有

所受益。对于非AI方向的技

术

人员，本书将大大降低

入门深度学习的门槛；对

于在校的大学生、研究生

，

本书不失为学习深度学

习的一本好教材；即便是

对于在工作中已经熟练

使用

框架开发各类深度

学习模型的读者，也可以

从本书中获得新的体会

。

本书从开始翻译到出版

，前前后后历时一年之久

。译者翻译时力求忠于

原

文，表达简练。为了保证翻

译质量，每翻译完一章后

，译者都会放置一段

xiv

译者

序

时间，再重新检查一遍

。图灵公司的专业编辑们

又进一步对译稿进行了

全面

细致的校对，提出了

许多宝贵意见，在此表示

感谢。但是，由于译者才疏

学浅，

书中难免存在一些

错误或疏漏，恳请读者批

评指正，以便我们在重印

时改正。

最后，希望本书的

出版能为国内的AI技术社

区添砖加瓦！

陆宇杰

2018年2月

上海

前言

科幻电影般的

世界已经变成了现实—人

工智能战胜过日本将棋

、国际

象棋的冠军，最近甚

至又打败了围棋冠军；智

能手机不仅可以理解人

们说的话，

还能在视频通

话中进行实时的“机器翻

译”；配备了摄像头的“自动

防撞的车”

保护着人们的

生命安全，自动驾驶技术

的实用化也为期不远。环

顾我们的四

周，原来被认

为只有人类才能做到的

事情，现在人工智能都能

毫无差错地完

成，甚至试

图超越人类。因为人工智

能的发展，我们所处的世

界正在逐渐变

成一个崭

新的世界。

在这个发展速

度惊人的世界背后，深度

学习技术在发挥着重要

作用。对

于深度学习，世界

各地的研究人员不吝褒

奖之辞，称赞其为革新性

技术，甚

至有人认为它是

几十年才有一次的突破

。实际上，深度学习这个词

经常出现

在报纸和杂志

中，备受关注，就连一般大

众也都有所耳闻。

本书就

是一本以深度学习为主

题的书，目的是让读者尽

可能深入地理解

深度学

习的技术。因此，本书提出

了“从零开始”这个概念。

本

书的特点是通过实现深

度学习的过程，来逼近深

度学习的本质。通过

实现

深度学习的程序，尽可能

无遗漏地介绍深度学习

相关的技术。另外，本

书还

提供了实际可运行的程

序，供读者自己进行各种

各样的实验。

为了实现深

度学习，我们需要经历很

多考验，花费很长时间，但

是相应

地也能学到和发

现很多东西。而且，实现深

度学习的过程是一个有

趣的、令

xvi

前言

人兴奋的过

程。希望读者通过这一过

程可以熟悉深度学习中

使用的技术，并

能从中感

受到快乐。

目前，深度学习

活跃在世界上各个地方

。在几乎人手一部的智能

手机中、

开启自动驾驶的

汽车中、为Web服务提供动力

的服务器中，深度学习都

在

发挥着作用。此时此刻

，就在很多人没有注意到

的地方，深度学习正在默

默

地发挥着其功能。今后

，深度学习势必将更加活

跃。为了让读者理解深度

学

习的相关技术，感受到

深度学习的魅力，笔者写

下了本书。

本书的理念

本

书是一本讲解深度学习

的书，将从最基础的内容

开始讲起，逐一介绍

理解

深度学习所需的知识。书

中尽可能用平实的语言

来介绍深度学习的概念

、

特征、工作原理等内容。不

过，本书并不是只介绍技

术的概要，而是旨在让

读

者更深入地理解深度学

习。这是本书的特色之一

。

那么，怎么才能更深入地

理解深度学习呢？在笔者

看来，最好的办法就

是亲

自实现。从零开始编写可

实际运行的程序，一边看

源代码，一边思考。

笔者坚

信，这种做法对正确理解

深度学习（以及那些看上

去很高级的技术）

是很重

要的。这里用了“从零开始

”一词，表示我们将尽可能

地不依赖外部

的现成品

（库、工具等）。也就是说，本书

的目标是，尽量不使用内

容不明的

黑盒，而是从自

己能理解的最基础的知

识出发，一步一步地实现

最先进的深

度学习技术

。并通过这一实现过程，使

读者加深对深度学习的

理解。

如果把本书比作一

本关于汽车的书，那么本

书并不会教你怎么开车

，其

着眼点不是汽车的驾

驶方法，而是要让读者理

解汽车的原理。为了让读

者理

解汽车的结构，必须

打开汽车的引擎盖，把零

件一个一个地拿在手里

观察，

并尝试操作它们。之

后，用尽可能简单的形式

提取汽车的本质，并组装

汽车

模型。本书的目标是

，通过制造汽车模型的过

程，让读者感受到自己可

以实

际制造出汽车，并在

这一过程中熟悉汽车相

关的技术。

为了实现深度

学习，本书使用了Python这一编

程语言。Python非常受

欢迎，初学

者也能轻松使用。Python尤其适

合用来制作样品（原型），使

用

前言

xvii

Python可以立刻尝试突

然想到的东西，一边观察

结果，一边进行各种各样

的实验。本书将在讲解深

度学习理论的同时，使用

Python实现程序，进行

各种实验

。

在光看数学式和理论说

明无法理解的情况下，可

以尝试阅读源代码

并运

行，很多时候思路都会变

得清晰起来。对数学式感

到困惑时，

就阅读源代码

来理解技术的流程，这样

的事情相信很多人都经

历过。

本书通过实际实现

（落实到代码）来理解深度

学习，是一本强调“工程”

的

书。书中会出现很多数学

式，但同时也会有很多程

序员视角的源代码。

本书

面向的读者

本书旨在让

读者通过实际动手操作

来深入理解深度学习。为

了明确本书

的读者对象

，这里将本书涉及的内容

列举如下。

•

使用Python，尽可能少

地使用外部库，从零开始

实现深度学习的程序。

• 为

了让Python的初学者也能理解

，介绍Python的使用方法。

• 提供实

际可运行的Python源代码，同时

提供可以让读者亲自实

验的

学习环境。

• 从简单的

机器学习问题开始，最终

实现一个能高精度地识

别图像的系统。

• 以简明易

懂的方式讲解深度学习

和神经网络的理论。

• 对于

误差反向传播法、卷积运

算等乍一看很复杂的技

术，使读者能够

在实现层

面上理解。

• 介绍一些学习

深度学习时有用的实践

技巧，如确定学习率的方

法、权

重的初始值等。

• 介绍

最近流行的Batch

Normalization、Dropout、Adam等，并进行

实

现。

• 讨论为什么深度学习

表现优异、为什么加深层

能提高识别精度、为什

么

隐藏层很重要等问题。

•

介

绍自动驾驶、图像生成、强

化学习等深度学习的应

用案例。

xviii   前言

本书不面向

的读者

明确本书不适合

什么样的读者也很重要

。为此，这里将本书不会涉

及的

内容列举如下。

• 不介

绍深度学习相关的最新

研究进展。

• 不介绍Caffe、TensorFlow、Chainer等深度

学习框架的使用方法。

• 不

介绍深度学习的详细理

论，特别是神经网络相关

的详细理论。

• 不详细介绍

用于提高识别精度的参

数调优相关的内容。

• 不会

为了实现深度学习的高

速化而进行GPU相关的实现

。

• 本书以图像识别为主题

，不涉及自然语言处理或

者语音识别的例子。

综上

，本书不涉及最新研究和

理论细节。但是，读完本书

之后，读者

应该有能力进

一步去阅读最新的论文

或者神经网络相关的理

论方面的技

术书。

本书以

图像识别为主题，主要学

习使用深度学习进行图

像识别时

所需的技术。自

然语言处理或者语音识

别等不是本书的讨论对

象。

本书的阅读方法

学习

新知识时，只听别人讲解

的话，有时会无法理解，或

者会立刻忘记。

正如“不闻

不若闻之，闻之不若见之

，见之不若知之，知之不若

行之”A ，在

学习新东西时，没

有什么比实践更重要了

。本书在介绍某个主题时

，都细心

地准备了一个可

以实践的场所——能够作为

程序运行的源代码。

本书

会提供Python源代码，读者可以

自己动手实际运行这些

源代码。

在阅读源代码的

同时，可以尝试去实现一

些自己想到的东西，以确

保真正

A 出自荀子《儒效篇

》。

前言

xix

理解了。另外，读者也

可以使用本书的源代码

，尝试进行各种实验，反复

试错。

本书将沿着“理论说

明”和“Python实现”两个路线前进

。因此，建议

读者准备好编

程环境。本书可以使用Windows、Mac、Linux中

的任何一个

系统。关于Python的

安装和使用方法将在第

1章介绍。另外，本书中用到

的程序可以从以下网址

下载。

http://www.ituring.com.cn/book/1921

让我们开始吧

通过

前面的介绍，希望读者了

解本书大概要讲的内容

，产生继续阅读的

兴趣。

最

近出现了很多深度学习

相关的库，任何人都可以

方便地使用。实际上，

使用

这些库的话，可以轻松地

运行深度学习的程序。那

么，为什么我们还要

特意

花时间从零开始实现深

度学习呢？一个理由就是

，在制作东西的过程中

可

以学到很多。

在制作东西

的过程中，会进行各种各

样的实验，有时也会卡住

，抱着脑

袋想为什么会这

样。这种费时的工作对深

刻理解技术而言是宝贵

的财富。像

这样认真花费

时间获得的知识在使用

现有的库、阅读最新的文

章、创建原创

的系统时都

大有用处。而且最重要的

是，制作本身就是一件快

乐的事情。（还

需要快乐以

外的其他什么理由吗？）

既

然一切都准备好了，下面

就让我们踏上实现深度

学习的旅途吧！

表述规则

本书在表述上采用如下

规则。

粗体字（Bold）

用来表示新

引入的术语、强调的要点

以及关键短语。

xx

前言

等宽

字（Constant Width）

用来表示下面这些信

息：程序代码、命令、序列、组

成元素、语句选项、

分支、变

量、属性、键值、函数、类型、类

、命名空间、方法、模块、属性

、

参数、值、对象、事件、事件处

理器、XML标签、HTML标签、宏、文件

的

内容、来自命令行的输出

等。若在其他地方引用了

以上这些内容（如变量、

函

数、关键字等），也会使用该

格式标记。

等宽粗体字（Constant Width Bold）

用

来表示用户输入的命令

或文本信息。在强调代码

的作用时也会使用该

格

式标记。

等宽斜体字（Constant Width Italic）

用来

表示必须根据用户环境

替换的字符串。

用来表示

提示、启发以及某些值得

深究的内容的补充信息

。

表示程序库中存在的 bug或

时常会发生的问题等警

告信息，引

起读者对该处

内容的注意。

读者意见与

咨询

虽然笔者已经尽最

大努力对本书的内容进

行了验证与确认，但仍不

免在

某些地方出现错误

或者容易引起误解的表

达等，给读者的理解带来

困扰。如

果读者遇到这些

问题，请及时告知，我们在

本书重印时会将其改正

，在此先

表示不胜感激。与

此同时，也希望读者能够

为本书将来的修订提出

中肯的建

议。本书编辑部

的联系方式如下。

前言  xxi

株

式会社O’Reilly Japan

电子邮件 japan@oreilly.co.jp

本书的

主页地址如下。

http://www.ituring.com.cn/book/1583

http://www.oreilly.co.jp/books/9784873117584（日语）

https://github.com/oreilly-japan/deep-learning-from-scratch

关于

O’Reilly的其他信息，可以访问下

面的O’Reilly主页查看。

http://www.oreilly.com/（英语）

http://www.oreilly.co.jp/（日语

）

致谢

首先，笔者要感谢推

动了深度学习相关技术

（机器学习、计算机科学等

）

发展的研究人员和工程

师。本书的完成离不开他

们的研究工作。其次，笔者

还要感谢在图书或网站

上公开有用信息的各位

同仁。其中，斯坦福大学的

CS231n [5]公开课慷慨提供了很多

有用的技术和信息，笔者

从中学到了很多东西。

在

本书执笔过程中，曾受到

下列人士的帮助：teamLab公司的

加藤哲朗、

喜多慎弥、飞永

由夏、中野皓太、中村将达

、林辉大、山本辽；Top Studio

公司的武

藤健志、增子萌；Flickfit公司的野

村宪司；得克萨斯大学奥

斯汀

分校JSPS海外特别研究

员丹野秀崇。他们阅读了

本书原稿，提出了很多宝

贵的建议，在此深表谢意

。另外，需要说明的是，本书

中存在的不足或错误

均

是笔者的责任。

最后，还要

感谢O’Reilly Japan的宮川直树，在从本

书的构想到完成的

大约

一年半的时间里，宫川先

生一直支持着笔者。非常

感谢！

2016年9月1日

斋藤康毅



第

1章

Python入门

Python这一编程语言已

经问世20多年了，在这期间

，Python不仅完成

了自身的进化

，还获得了大量的用户。现

在，Python作为最具人气的编程

语言，

受到了许多人的喜

爱。

接下来我们将使用Python实

现深度学习系统。不过在

这之前，本章将简

单地介

绍一下Python，看一下它的使用

方法。已经掌握了Python、NumPy、

Matplotlib等知识

的读者，可以跳过本章，直

接阅读后面的章节。

1.1

Python是什

么

Python是一个简单、易读、易记

的编程语言，而且是开源

的，可以免

费地自由使用

。Python可以用类似英语的语法

编写程序，编译起来也不

费

力，因此我们可以很轻

松地使用Python。特别是对首次

接触编程的人士来说，

Python是

最合适不过的语言。事实

上，很多高校和大专院校

的计算机课程

均采用Python作

为入门语言。

此外，使用Python不

仅可以写出可读性高的

代码，还可以写出性能高

（处

理速度快）的代码。在需

要处理大规模数据或者

要求快速响应的情况下

，使

用Python可以稳妥地完成。因

此，Python不仅受到初学者的喜

爱，同时也

受到专业人士

的喜爱。实际上，Google、Microsoft、Facebook等战斗在

IT

行业最前沿的企业也经

常使用Python。

2

第 1章　Python入门

再者，在

科学领域，特别是在机器

学习、数据科学领域，Python也被

大量使用。Python除了高性能之

外，凭借着 NumPy、SciPy等优秀的数

值

计算、统计分析库，在数据

科学领域占有不可动摇

的地位。深度学习的

框架

中也有很多使用Python的场景

，比如Caffe、TensorFlow、Chainer、

Theano等著名的深度学习

框架都提供了Python接口。因此

，学习Python

对使用深度学习框

架大有益处。

综上，Python是最适

合数据科学领域的编程

语言。而且，Python具有

受众广的

优秀品质，从初学者到专

业人士都在使用。因此，为

了完成本书的

从零开始

实现深度学习的目标，Python可

以说是最合适的工具。

1.2 Python的

安装

下面，我们首先将Python安

装到当前环境（电脑）上。这

里说明一下安

装时需要

注意的一些地方。

1.2.1

Python版本

Python有

Python 2.x和Python 3.x两个版本。如果我们调

查一下目前

Python的使用情况

，会发现除了最新的版本

3.x以外，旧的版本2.x仍在被

大

量使用。因此，在安装Python时，需

要慎重选择安装Python的哪个

版

本。这是因为两个版本

之间没有兼容性（严格地

讲，是没有“向后兼容性”），

也

就是说，会发生用Python 3.x写的代

码不能被Python 2.x执行的情况。

本

书中使用Python 3.x，只安装了Python

2.x的读

者建议另外安装一下

Python 3.x。

1.2.2　使

用的外部库

本书的目标

是从零开始实现深度学

习。因此，除了NumPy库和Matplotlib

库之外

，我们极力避免使用外部

库。之所以使用这两个库

，是因为它们可以

有效地

促进深度学习的实现。

1.2  Python的

安装 3

NumPy是用于数值计算的

库，提供了很多高级的数

学算法和便利的数

组（矩

阵）操作方法。本书中将使

用这些便利的方法来有

效地促进深度学习

的实

现。

Matplotlib是用来画图的库。使用

Matplotlib能将实验结果可视化，并

在视觉上确认深度学习

运行期间的数据。

本书将

使用下列编程语言和库

。

•

Python 3.x（2016年 8月时的最新版本是 3.5）

• NumPy

• Matplotlib

下

面将为需要安装Python的读者

介绍一下Python的安装方法。已

经安

装了Python的读者，请跳过

这一部分内容。

1.2.3　Anaconda发行版

Python的

安装方法有很多种，本书

推荐使用Anaconda这个发行版。发

行版集成了必要的库，使

用户可以一次性完成安

装。Anaconda是一个侧重

于数据分

析的发行版，前面说的NumPy、Matplotlib等

有助于数据分析的

库都

包含在其中A。

如前所述，本

书将使用Python 3.x版本，因此Anaconda发行

版也要安

装3.x的版本。请读

者从官方网站下载与自

己的操作系统相应的发

行版，然

后安装。

A Anaconda作为一个

针对数据分析的发行版

，包含了许多有用的库，而

本书中实际上只会使用

其中的

NumPy库和Matplotlib库。因此，如果

想保持轻量级的开发环

境，单独安装这两个库也

是可以的。

——译者注

4

第 1章　Python入

门

1.3 Python解释器

完成Python的安装后

，要先确认一下Python的版本。打

开终端（Windows

中的命令行窗口

），输入python --version命令，该命令会输出

已经安装的

Python的版本信息

。



$ python --version

Python 3.4.1 :: Anaconda 2.1.0 (x86_64)

如上所示，显示了Python 3.4.1（根据实际安装的版本，版本号可能不同），

说明已正确安装了Python 3.x。接着输入python，启动Python解释器。

$



python

Python 3.4.1 |Anaconda 2.1.0 (x86_64)|

(default, Sep 10 2014, 17:24:09)

[GCC

4.2.1 (Apple Inc. build 5577)] on

darwin

Type "help", "copyright", "credits" or

"license" for more information.

>>>

Python解释器也被称为“对话模

式”，用户能够以和Python对话的

方式

进行编程。比如，当用

户询问“1 + 2等于几？”的时候，Python解

释器会回

答“3”，所谓对话模

式，就是指这样的交互。现

在，我们实际输入一下看

看。

>>> 1

+ 2

3

Python解释器可以像这样进

行对话式（交互式）的编程

。下面，我们使

用这个对话

模式，来看几个简单的Python编

程的例子。

1.3.1

算术计算

加法

或乘法等算术计算，可按

如下方式进行。

>>> 1 - 2

-1

>>> 4 * 5

20

1.3  Python解释器 5

>>> 7

/ 5

1.4

>>> 3 **

2

9

*表

示乘法，/表示除法，**表示乘

方（3**2是 3 的 2

次方）。另外，在

Python 2.x中，整

数除以整数的结果是整

数，比如，7 ÷ 5的结果是1。但在

Python

3.x中

，整数除以整数的结果是

小数（浮点数）。

1.3.2　数据类型

编

程中有数据类型（data type）这一概

念。数据类型表示数据的

性质，

有整数、小数、字符串

等类型。Python中的type()函数可以用

来查看数据

类型。

>>> type(10)

<class 'int'>

>>>

type(2.718)

<class 'float'>

>>> type("hello")

<class

'str'>

根据上

面的结果可知，10是int类型（整

型），2.718是float类型（浮点型），

"hello"是str（字符

串）类型。另外，“类型”和“类”这

两个词有时用作相同

的

意思。这里，对于输出结果

<class 'int'>，可以将其解释成“10是int类（类

型）”。

1.3.3

变量

可以使用x或y等字

母定义变量（variable）。此外，可以使

用变量进行计算，

也可以

对变量赋值。

>>> x =

10 # 初始化

>>> print(x) #

输出

x

10

>>> x = 100

# 赋值

>>> print(x)

100

6

第 1章　Python入门

>>> y

= 3.14

>>> x * y

314.0

>>> type(x * y)

<class

'float'>

Python是属于“动

态类型语言”的编程语言

，所谓动态，是指变量的类

型是根据情况自动决定

的。在上面的例子中，用户

并没有明确指出“x的类

型

是int（整型）”，是Python根据x被初始化

为10，从而判断出x的类型为

int的。此外，我们也可以看到

，整数和小数相乘的结果

是小数（数据类型的

自动

转换）。另外，“#”是注释的意思

，它后面的文字会被Python忽略

。

1.3.4　列表

除了单一的数值，还

可以用列表（数组）汇总数

据。

>>> a = [1, 2,

3, 4, 5] # 生成列表

>>>

print(a) # 输出列表的

内容

[1, 2, 3,

4, 5]

>>> len(a) # 获取列表的长度

5

>>> a[0] # 访

问第一个元素的值

1

>>> a[4]

5

>>> a[4] =

99 # 赋值

>>> print(a)

[1, 2,

3, 4, 99]

元素的访问是通过a[0]这样

的方式进行的。[]中的数字

称为索引（下标），

索引从0开

始（索引0对应第一个元素

）。此外，Python的列表提供了切片

（slicing）这一便捷的标记法。使用

切片不仅可以访问某个

值，还可以访问列

表的子

列表（部分列表）。

>>> print(a)

[1, 2, 3, 4,

99]

>>> a[0:2] # 获取索引

为0到2（不包括2！）的元素

[1,

2]

>>> a[1:] # 获取

从索引为1的元素到最后

一个元素

[2,

3, 4, 99]

1.3 Python解释器

7

>>> a[:3] # 获取从

第一个元素到索引为3（不

包括3！）的元素

[1,

2, 3]

>>> a[:-1] # 获取从第一

个元素到最后一个元素

的前一个元素之间的元

素

[1, 2, 3, 4]

>>> a[:-2]

# 获取从第一个元素到

最后一个元素的前二个

元素之间的元素

[1, 2, 3]

进行列

表的切片时，需要写成a[0:2]这

样的形式。a[0:2]用于取出从索

引为0的元素到索引为2的

元素的前一个元素之间

的元素。另外，索引−1对

应最

后一个元素，−2对应最后一

个元素的前一个元素。

1.3.5　字

典

列表根据索引，按照0, 1, 2,

...的

顺序存储值，而字典则以

键值对的形

式存储数据

。字典就像《新华字典》那样

，将单词和它的含义对应

着存储起来。

>>> me = {'height':180}

# 生成字典

>>> me['height'] # 访

问元素

180

>>> me['weight'] = 70 #

添加新元素

>>> print(me)

{'height': 180, 'weight':

70}

1.3.6　布尔

型

Python中有bool型。bool型取True或False中的一

个值。针对bool型的

运算符包

括and、or和not（针对数值的运算符

有+、-、*、/等，根据不同的

数据类

型使用不同的运算符）。

>>> hungry = True # 饿

了？

>>> sleepy = False # 困了？

>>> type(hungry)

<class 'bool'>

>>> not

hungry

False

>>> hungry and sleepy

# 饿并且困

False

>>> hungry or

sleepy # 饿或者

困

True

8

第 1章　Python入门

1.3.7　if语句

根据不

同的条件选择不同的处

理分支时可以使用if/else语句

。

>>> hungry = True

>>> if

hungry:

... print("I'm hungry")

...

I'm

hungry

>>> hungry = False

>>>

if hungry:

... print("I'm hungry") #

使用空白字符进行缩进

... else:

... print("I'm not hungry")

... print("I'm sleepy")

...

I'm not

hungry

I'm sleepy

Python中的空白字符具有重要

的意义。上面的if语句中，if hungry:下

面

的语句开头有4个空白

字符。它是缩进的意思，表

示当前面的条件（if

hungry）

成立时

，此处的代码会被执行。这

个缩进也可以用tab表示，Python中

推荐

使用空白字符。

Python使用

空白字符表示缩进。一般

而言，每缩进一次，使用 4

个

空白字符。

1.3.8　for 语句

进行循环

处理时可以使用for语句。

>>> for

i in [1, 2, 3]:

...

print(i)

...

1

2

3

这

是输出列表[1,

2, 3]中的元素的

例子。使用for ��� in ��� :语句结构，

1.4 Python脚本

文件  9

可以按顺序访问列

表等数据集合中的各个

元素。

1.3.9

函数

可以将一连串

的处理定义成函数（function）。

>>> def hello():

...

print("Hello World!")

...

>>> hello()

Hello

World!

此外

，函数可以取参数。

>>> def hello(object):

...

print("Hello " + object + "!")

...

>>> hello("cat")

Hello cat!

另外，字

符串的拼接可以使用+。

关

闭Python解释器时，Linux或Mac OS X的情况下

输入Ctrl-D（按住

Ctrl，再按D键）；Windows的情况

下输入Ctrl-Z，然后按Enter键。

1.4 Python脚本文

件

到目前为止，我们看到

的都是基于Python解释器的例

子。Python解释

器能够以对话模

式执行程序，非常便于进

行简单的实验。但是，想进

行一

连串的处理时，因为

每次都需要输入程序，所

以不太方便。这时，可以将

Python程序保存为文件，然后（集

中地）运行这个文件。下面

，我们来看一

个Python脚本文件

的例子。

1.4.1　保存为文件

打开

文本编辑器，新建一个hungry.py的

文件。hungry.py只包含下面一

行语

句。

10   第

1章　Python入门

print("I'm hungry!")

接着，打开终

端（Windows中的命令行窗口），移至

hungry.py所在的位置。

然后，将hungry.py文件

名作为参数，运行python命令。这

里假设hungry.

py在 ~/deep-learning-from-scratch/ch01目录下（在本书

提供的源代码中，

hungry.py文件位

于ch01目录下）。



$ cd ~/deep-learning-from-scratch/ch01 # 移动目录

$



python hungry.py

I'm hungry!

这样，使用python hungry.py命令就可以执

行这个Python程序了。

1.4.2　类

前面我

们了解了int和str等数据类型

（通过type()函数可以查看对象

的

类型）。这些数据类型是

“内置”的数据类型，是Python中一

开始就有的数

据类型。现

在，我们来定义新的类。如

果用户自己定义类的话

，就可以自己

创建数据类

型。此外，也可以定义原创

的方法（类的函数）和属性

。

Python中使用class关键字来定义类

，类要遵循下述格式（模板

）。

class 类名：

 def __init__(self,

参数, …): # 构造函数

 ...

def 方法

名1(self, 参数, …): # 方法1

...

 def 方法名2(self, 参数

, …):

# 方法2

 ...

这里有一个特殊的

__init__方法，这是进行初始化的

方法，也称为构造

函数（constructor）,只

在生成类的实例时被调

用一次。此外，在方法的第

一

个参数中明确地写入

表示自身（自身的实例）的

self是Python的一个特点（学

过其他

编程语言的人可能会觉

得这种写self的方式有一点

奇怪）。

下面我们通过一个

简单的例子来创建一个

类。这里将下面的程序保

存为

man.py。

1.5 NumPy

11

class Man:

 def __init__(self,

name):

 self.name = name

print("Initialized!")

 def hello(self):

 print("Hello

" + self.name + "!")

def goodbye(self):

 print("Good-bye " +

self.name + "!")

m = Man("David")

m.hello()

m.goodbye()

从终端运行man.py。



python man.py

Initialized!

Hello David!

Good-bye

David!

这里我们定义了一个新

类Man。上面的例子中，类Man生成

了实例（对象）m。

类Man的构造函

数（初始化方法）会接收参

数name，然后用这个参数初始

化实例变量self.name。实例变量是

存储在各个实例中的变

量。Python中可

以像self.name这样，通过在

self后面添加属性名来生成

或访问实例变量。

1.5 NumPy

在深度

学习的实现中，经常出现

数组和矩阵的计算。NumPy的数

组类

（numpy.array）中提供了很多便捷

的方法，在实现深度学习

时，我们将使用这

些方法

。本节我们来简单介绍一

下后面会用到的NumPy。

1.5.1　导入NumPy

NumPy是

外部库。这里所说的“外部

”是指不包含在标准版Python中

。

因此，我们首先要导入NumPy库

。

12   第 1章

Python入门

>>> import numpy as np

Python 中使用 import语句来

导入库。这里的 import numpy as

np，直

译的话

就是“将numpy作为np导入”的意思

。通过写成这样的形式，之

后

NumPy相关的方法均可通过

np来调用。

1.5.2　生成NumPy数组

要生成

NumPy数组，需要使用np.array()方法。np.array()接收

Python

列表作为参数，生成NumPy数组

（numpy.ndarray）。

>>> x = np.array([1.0, 2.0,

3.0])

>>> print(x)

[ 1. 2.

3.]

>>> type(x)

<class 'numpy.ndarray'>

1.5.3

NumPy 的算术运算

下面是NumPy数组

的算术运算的例子。

>>> x =

np.array([1.0, 2.0, 3.0])

>>> y =

np.array([2.0, 4.0, 6.0])

>>> x +

y # 对应

元素的加法

array([ 3., 6.,

9.])

>>> x - y

array([

-1., -2., -3.])

>>> x *

y # element-wise product

array([ 2.,

8., 18.])

>>> x / y

array([ 0.5, 0.5, 0.5])

这里需要注

意的是，数组x和数组y的元

素个数是相同的（两者均

是元素

个数为3的一维数

组）。当x和y的元素个数相同

时，可以对各个元素进行

算

术运算。如果元素个数

不同，程序就会报错，所以

元素个数保持一致非常

重要。

另外，“对应元素的”的

英文是element-wise，比如“对应元素的

乘法”就是

element-wise product。

NumPy数组不仅可以

进行element-wise运算，也可以和单一

的数值（标量）

1.5

NumPy  13

组合起来进

行运算。此时，需要在NumPy数组

的各个元素和标量之间

进行运算。

这个功能也被

称为广播（详见后文）。

>>>

x = np.array([1.0, 2.0, 3.0])

>>>

x / 2.0

array([ 0.5, 1.

, 1.5])

1.5.4　NumPy的N维

数组

NumPy不仅可以生成一维

数组（排成一列的数组），也

可以生成多维数组。

比如

，可以生成如下的二维数

组（矩阵）。

>>> A = np.array([[1, 2], [3,

4]])

>>> print(A)

[[1 2]

[3 4]]

>>> A.shape

(2, 2)

>>> A.dtype

dtype('int64')

这里生成了一个

2 × 2的矩阵A。另外，矩阵A的形状

可以通过shape查看，

矩阵元素

的数据类型可以通过dtype查

看。下面，我们来看一下矩

阵的算术运算。

>>> B = np.array([[3, 0],[0,

6]])

>>> A + B

array([[

4, 2],

 [ 3, 10]])

>>> A * B

array([[ 3,

0],

 [ 0, 24]])

和数组的

算术运算一样，矩阵的算

术运算也可以在相同形

状的矩阵间以

对应元素

的方式进行。并且，也可以

通过标量（单一数值）对矩

阵进行算术运算。

这也是

基于广播的功能。

>>> print(A)

[[1 2]

[3 4]]

14   第

1章　Python入

门

>>> A * 10

array([[ 10, 20],

 [ 30,

40]])

NumPy数组（np.array）可以生成N维数组

，即可以生成一维数组、

二

维数组、三维数组等任意

维数的数组。数学上将一

维数组称为向量，

将二维

数组称为矩阵。另外，可以

将一般化之后的向量或

矩阵等统

称为张量（tensor）。本书

基本上将二维数组称为

“矩阵”，将三维数

组及三维

以上的数组称为“张量”或

“多维数组”。

1.5.5　广播

NumPy中，形状不

同的数组之间也可以进

行运算。之前的例子中，在

2×2的矩阵A和标量10之间进行

了乘法运算。在这个过程

中，如图1-1所示，

标量10被扩展

成了2 × 2的形状，然后再与矩

阵A进行乘法运算。这个巧

妙

的功能称为广播（broadcast）。

图1-1　广

播的例子：标量10被当作2 × 2的

矩阵

1

2

* = 3 4

10

10

10 10

1 2

*

= 3 4

10 10 20

30 40

我们通过下面这个

运算再来看一个广播的

例子。

>>> A =

np.array([[1, 2], [3, 4]])

>>> B

= np.array([10, 20])

>>> A *

B

array([[ 10, 40],

 [

30, 80]])

在这个运算中，如图

1-2所示，一维数组B被“巧妙地

”变成了和二位数

组A相同

的形状，然后再以对应元

素的方式进行运算。

综上

，因为NumPy有广播功能，所以不

同形状的数组之间也可

以顺利

地进行运算。

1.5 NumPy  15

图1-2　广

播的例子2

1 2

* = 3 4

10 20

10 20

1 2

* = 3 4

10 20

10 40

30 80

1.5.6　访问元素

元素

的索引从0开始。对各个元

素的访问可按如下方式

进行。

>>> X = np.array([[51, 55],

[14, 19], [0, 4]])

>>> print(X)

[[51 55]

 [14 19]

[ 0 4]]

>>> X[0] #

第0行

array([51, 55])

>>> X[0][1] #

(0,1)的元素

55

也可以

使用for语句访问各个元素

。

>>> for row

in X:

... print(row)

...

[51

55]

[14 19]

[0 4]

除了前面介绍的索引操

作，NumPy还可以使用数组访问

各个元素。

>>> X = X.flatten() # 将X转换为一维

数组

>>> print(X)

[51 55 14 19

0 4]

>>> X[np.array([0, 2, 4])]

# 获取索引为0、2、4的元素

array([51, 14, 0])

运用这个标记法，可以获

取满足一定条件的元素

。例如，要从X中抽出

大于15的

元素，可以写成如下形式

。

16   第 1章　Python入门

>>> X > 15

array([ True,

True, False, True, False, False], dtype=bool)

>>> X[X>15]

array([51, 55, 19])

对NumPy数组使用不

等号运算符等（上例中是

X

> 15）,结果会得到一个

布尔型

的数组。上例中就是使用

这个布尔型数组取出了

数组的各个元素（取

出True对

应的元素）。

Python等动态类型语

言一般比C和C++等静态类型

语言（编译型语言）

运算速

度慢。实际上，如果是运算

量大的处理对象，用

C/C++写程

序更好。为此，当 Python中追求性

能时，人们会用 C/C++来实现

处

理的内容。Python则承担“中间人

”的角色，负责调用那些用

C/

C++写的程序。NumPy中，主要的处理

也都是通过C或C++实现的。

因

此，我们可以在不损失性

能的情况下，使用

Python便利的

语法。

1.6 Matplotlib

在深度学习的实验

中，图形的绘制和数据的

可视化非常重要。Matplotlib

是用于

绘制图形的库，使用Matplotlib可以

轻松地绘制图形和实现

数据的可

视化。这里，我们

来介绍一下图形的绘制

方法和图像的显示方法

。

1.6.1　绘制简单图形

可以使用

matplotlib的pyplot模块绘制图形。话不多

说，我们来看一个

绘制sin函

数曲线的例子。

import numpy

as np

import matplotlib.pyplot as plt

# 生成数据

x = np.arange(0, 6, 0.1)

# 以0.1为单位，生成0到6的数据

y = np.sin(x)

# 绘制图形

1.6 Matplotlib  17

plt.plot(x, y)

plt.show()

这里使用NumPy的arange方

法生成了[0, 0.1, 0.2, ���, 5.8,

5.9]的

数据，将其设

为x。对x的各个元素，应用NumPy的

sin函数np.sin()，将x、

y的数据传给plt.plot方法

，然后绘制图形。最后，通过

plt.show()显示图形。

运行上述代码

后，就会显示图1-3所示的图

形。

图1-3 sin函数的图形

1.0

0.5

0 1 2 3

4 5 6

0.0

−0.5

−1.0

1.6.2　pyplot的功能

在刚才的sin函数的图形中

，我们尝试追加cos函数的图

形，并尝试使用

pyplot的添加标

题和x轴标签名等其他功

能。

import numpy as

np

import matplotlib.pyplot as plt

#

生成数据

x = np.arange(0, 6, 0.1)

# 以0.1为单位，生

成0到6的数据

y1 = np.sin(x)

18

第 1章　Python入门

y2 =

np.cos(x)

# 绘

制图形

plt.plot(x, y1, label="sin")

plt.plot(x, y2, linestyle = "--", label="cos")

# 用虚线绘制

plt.xlabel("x") # x轴标

签

plt.ylabel("y")

# y轴标签

plt.title('sin & cos') #

标题

plt.legend()

plt.show()

结果如图

1-4所示，我们看到图的标题

、轴的标签名都被标出来

了。

图1-4 sin函数和cos函数的图形

1.0

sin & cos

0.5

0.0

−0.5

−1.0

0 1 3

x

y

2 4 5 6

cos

sin

1.6.3　显示图像

pyplot 中还提供了用

于显示图像的方法 imshow()。另外

，可以使用

matplotlib.image模块的imread()方法读

入图像。下面我们来看一

个例子。

import matplotlib.pyplot as plt

from matplotlib.image

import imread

1.7  小结

19

img = imread('lena.png') # 读入图像（设

定合适的路径！）

plt.imshow(img)

plt.show()

运行上述

代码后，会显示图1-5所示的

图像。

图1-5　显示图像

0

50

100

150

200

250

0

50 100 150 200 250

这里，我

们假定图像lena.png在当前目录

下。读者根据自己的环境

，可

能需要变更文件名或

文件路径。另外，本书提供

的源代码中，在dataset目

录下有

样本图像lena.png。比如，在通过Python解

释器从ch01目录运行上

述代

码的情况下，将图像的路

径'lena.png'改为'../dataset/lena.png'，即

可正确运行。

1.7 小

结

本章重点介绍了实现

深度学习（神经网络）所需

的编程知识，以为学习

深

度学习做好准备。从下一

章开始，我们将通过使用

Python实际运行代码，

逐步了解

深度学习。

20

第 1章　Python入门

本章

只介绍了关于Python的最低限

度的知识，想进一步了解

Python的

读者，可以参考下面这

些图书。首先推荐《Python语言及

其应用》[1]

一书。

这是一本详

细介绍从Python编程的基础到

应用的实践性的入门书

。关于

NumPy，《利用Python进行数据分析

》[2]

一书中进行了简单易懂

的总结。此

外，“Scipy Lecture Notes”[3]

这个网站上

也有以科学计算为主题

的NumPy

和Matplotlib的详细介绍，有兴趣

的读者可以参考。

下面，我

们来总结一下本章所学

的内容，如下所示。

本章所

学的内容

• Python是一种简单易

记的编程语言。

• Python是开源的

，可以自由使用。

• 本书中将

使用Python 3.x实现深度学习。

•

本书

中将使用NumPy和Matplotlib这两种外部

库。

• Python有“解释器”和“脚本文件

”两种运行模式。

• Python能够将一

系列处理集成为函数或

类等模块。

•

NumPy中有很多用于

操作多维数组的便捷方

法。

第2章

感知机

本章将介

绍感知机A

（perceptron）这一算法。感知

机是由美国学者Frank

Rosenblatt在1957年提

出来的。为何我们现在还

要学习这一很久以前就

有

的算法呢？因为感知机

也是作为神经网络（深度

学习）的起源的算法。因此

，

学习感知机的构造也就

是学习通向神经网络和

深度学习的一种重要思

想。

本章我们将简单介绍

一下感知机，并用感知机

解决一些简单的问题。希

望读者通过这个过程能

熟悉感知机。

2.1 感知机是什

么

感知机接收多个输入

信号，输出一个信号。这里

所说的“信号”可以想

象成

电流或河流那样具备“流

动性”的东西。像电流流过

导线，向前方输送

电子一

样，感知机的信号也会形

成流，向前方输送信息。但

是，和实际的电

流不同的

是，感知机的信号只有“流

/不流”（1/0）两种取值。在本书中

，0

对应“不传递信号”，1对应“传

递信号”。

图2-1是一个接收两

个输入信号的感知机的

例子。x1、x2是输入信号，

y是输出

信号，w1、w2是权重（w是weight的首字母

）。图中的○称为“神

经元”或者

“节点”。输入信号被送往神

经元时，会被分别乘以固

定的权重

A 严格地讲，本章

中所说的感知机应该称

为“人工神经元”或“朴素感

知机”，但是因为很多基本

的处

理都是共通的，所以

这里就简单地称为“感知

机”。

22

第 2章　感知机

（w1x1、w2x2）。神经元会

计算传送过来的信号的

总和，只有当这个总和超

过

了某个界限值时，才会

输出1。这也称为“神经元被

激活”。这里将这个界

限值

称为阈值，用符号θ表示。

图

2-1　有两个输入的感知机

x1

x2

w1

w2

y

感

知机的运行原理只有这

些！把上述内容用数学式

来表示，就是式（2.1）。

（2.1）

感知机的

多个输入信号都有各自

固有的权重，这些权重发

挥着控制各个

信号的重

要性的作用。也就是说，权

重越大，对应该权重的信

号的重要性就

越高。

权重

相当于电流里所说的电

阻。电阻是决定电流流动

难度的参数，

电阻越低，通

过的电流就越大。而感知

机的权重则是值越大，通

过

的信号就越大。不管是

电阻还是权重，在控制信

号流动难度（或者流

动容

易度）这一点上的作用都

是一样的。

2.2

简单逻辑电路

23

2.2 简单逻辑电路

2.2.1　与门

现在

让我们考虑用感知机来

解决简单的问题。这里首

先以逻辑电路为题

材来

思考一下与门（AND gate）。与门是有

两个输入和一个输出的

门电路。图2-2

这种输入信号

和输出信号的对应表称

为“真值表”。如图2-2所示，与门

仅在

两个输入均为1时输

出1，其他时候则输出0。

x1 x2

y

1

1

1 1 1

0

0

0 0

0

0

0

图2-2　与

门的真值表

下面考虑用

感知机来表示这个与门

。需要做的就是确定能满

足图2-2的

真值表的w1、w2、θ的值。那

么，设定什么样的值才能

制作出满足图2-2的

条件的

感知机呢？

实际上，满足图

2-2的条件的参数的选择方

法有无数多个。比如，当

(w1, w2, θ) = (0.5,

0.5, 0.7) 时

，可以满足图 2-2 的条件。此外

，当 (w1,

w2, θ)

为(0.5, 0.5, 0.8)或者(1.0, 1.0,

1.0)时，同样也满足

与门的条件。设定这样的

参数后，仅当x1和x2同时为1时

，信号的加权总和才会超

过给定的阈值θ。

2.2.2　与非门和

或门

接着，我们再来考虑

一下与非门（NAND gate）。NAND是Not AND的

24   第 2章　感

知机

意思，与非门就是颠

倒了与门的输出。用真值

表表示的话，如图2-3所示，

仅

当x1和x2同时为1时输出0，其他

时候则输出1。那么与非门

的参数又可

以是什么样

的组合呢？

图2-3　与非门的真

值表

1

1 1

1 1

1

0

0 1

0

0

0

x1

x2 y

要表示与非门，可以

用(w1, w2, θ) =

(−0.5, −0.5, −0.7)这样的组合（其

他的组

合也是无限存在的）。实际

上，只要把实现与门的参

数值的符号取反，

就可以

实现与非门。

接下来看一

下图2-4所示的或门。或门是

“只要有一个输入信号是

1，输

出就为1”的逻辑电路。那

么我们来思考一下，应该

为这个或门设定什么样

的参数呢？

图2-4　或门的真值

表

x1 x2 y

1 1

1 1

1 1

1

0

0

0

0 0

2.3 感知机的实现  25

这里决

定感知机参数的并不是

计算机，而是我们人。我们

看着真值

表这种“训练数

据”，人工考虑（想到）了参数

的值。而机器学习的课

题

就是将这个决定参数值

的工作交由计算机自动

进行。学习是确定

合适的

参数的过程，而人要做的

是思考感知机的构造（模

型），并把

训练数据交给计

算机。

如上所示，我们已经

知道使用感知机可以表

示与门、与非门、或门的逻

辑电路。这里重要的一点

是：与门、与非门、或门的感

知机构造是一样的。

实际

上，3个门电路只有参数的

值（权重和阈值）不同。也就

是说，相同构造

的感知机

，只需通过适当地调整参

数的值，就可以像“变色龙

演员”表演不

同的角色一

样，变身为与门、与非门、或

门。

2.3 感知机的实现

2.3.1　简单的

实现

现在，我们用Python来实现

刚才的逻辑电路。这里，先

定义一个接收

参数x1和x2的

AND函数。

def AND(x1, x2):

 w1,

w2, theta = 0.5, 0.5, 0.7

tmp = x1*w1 + x2*w2

if tmp <= theta:

 return

0

 elif tmp > theta:

return 1

在函数内初始化参

数w1、w2、theta，当输入的加权总和超

过阈值时返回1，

否则返回

0。我们来确认一下输出结

果是否如图2-2所示。

AND(0, 0)

# 输出0

AND(1, 0) # 输

出0

AND(0, 1) # 输出0

AND(1, 1)

# 输出1

果然和我们

预想的输出一样！这样我

们就实现了与门。按照同

样的步骤，

26

第 2章　感知机

也

可以实现与非门和或门

，不过让我们来对它们的

实现稍作修改。

2.3.2　导入权重

和偏置

刚才的与门的实

现比较直接、容易理解，但

是考虑到以后的事情，我

们

将其修改为另外一种

实现形式。在此之前，首先

把式（2.1）的θ换成−b，于

是就可以

用式（2.2）来表示感知机的行

为。

（2.2）

式（2.1）和式（2.2）虽然有一个符

号不同，但表达的内容是

完全相同的。

此处，b称为偏

置，w1和w2称为权重。如式（2.2）所示

，感知机会计算输入

信号

和权重的乘积，然后加上

偏置，如果这个值大于0则

输出1，否则输出0。

下面，我们

使用NumPy，按式（2.2）的方式实现感

知机。在这个过程中，我

们

用Python的解释器逐一确认结

果。

>>> import numpy

as np

>>> x = np.array([0,

1]) # 输入

>>> w =

np.array([0.5, 0.5]) # 权重

>>> b

= -0.7 # 偏置

>>> w*x

array([ 0. , 0.5])

>>> np.sum(w*x)

0.5

>>> np.sum(w*x) + b

-0.19999999999999996

# 大约为

-0.2（由浮点小数造成的运算

误差）

如上例所示，在NumPy数组

的乘法运算中，当两个数

组的元素个数相同时，

各

个元素分别相乘，因此w*x的

结果就是它们的各个元

素分别相乘（[0, 1] *

[0.5, 0.5] => [0, 0.5]）。之后，np.sum(w*x)再计算

相乘后的各个元素的总

和。

最后再把偏置加到这

个加权总和上，就完成了

式（2.2）的计算。

2.3.3　使用权重和偏

置的实现

使用权重和偏

置，可以像下面这样实现

与门。

2.3 感知机的实现

27

def AND(x1, x2):

 x

= np.array([x1, x2])

 w =

np.array([0.5, 0.5])

 b = -0.7

tmp = np.sum(w*x) + b

if tmp <= 0:

 return

0

 else:

 return 1

这里

把−θ命名为偏置b，但是请注

意，偏置和权重w1、w2的作用是

不

一样的。具体地说，w1和w2是

控制输入信号的重要性

的参数，而偏置是调

整神

经元被激活的容易程度

（输出信号为1的程度）的参

数。比如，若b为

−0.1，则只要输入

信号的加权总和超过0.1，神

经元就会被激活。但是如

果b

为−20.0，则输入信号的加权

总和必须超过20.0，神经元才

会被激活。像这样，

偏置的

值决定了神经元被激活

的容易程度。另外，这里我

们将w1和w2称为权重，

将b称为

偏置，但是根据上下文，有

时也会将b、w1、w2这些参数统称

为权重。

偏置这个术语，有

“穿木屐”A 的效果，即在没有

任何输入时（输入为

0时），给

输出穿上多高的木屐（加

上多大的值）的意思。实际

上，在

式(2.2) 的b

+ w1x1 + w2x2的计算中，当输

入x1和x2为 0时，只输出

偏置的

值。

A

接着，我们继续实现与

非门和或门。

def NAND(x1, x2):

x = np.array([x1, x2])

 w

= np.array([-0.5, -0.5]) # 仅权重和偏

置与AND不同！

b = 0.7

 tmp =

np.sum(w*x) + b

 if tmp

<= 0:

 return 0

else:

 return 1

def OR(x1,

x2):

A 因为木屐的底

比较厚，穿上它后，整个人

也会显得更高。——译者注

28

第

2章　感知机

 x = np.array([x1,

x2])

 w = np.array([0.5, 0.5])

# 仅权重和偏置

与AND不同！

 b = -0.2

tmp = np.sum(w*x) + b

if tmp <= 0:

 return

0

 else:

 return 1

我们在2.2节介绍过

，与门、与非门、或门是具有

相同构造的感知机，

区别

只在于权重参数的值。因

此，在与非门和或门的实

现中，仅设置权重和

偏置

的值这一点和与门的实

现不同。

2.4 感知机的局限性

到这里我们已经知道，使

用感知机可以实现与门

、与非门、或门三种逻

辑电

路。现在我们来考虑一下

异或门（XOR

gate）。

2.4.1　异或门

异或门也

被称为逻辑异或电路。如

图2-5所示，仅当x1或x2中的一方

为

1时，才会输出1（“异或”是拒

绝其他的意思）。那么，要用

感知机实现这个

异或门

的话，应该设定什么样的

权重参数呢？

x1 x2 y

1 1

11

11

0

0

0

0

0

0

图2-5　异或门的

真值表

2.4 感知机的局限性

29

实际上，用前面介绍的感

知机是无法实现这个异

或门的。为什么用感知

机

可以实现与门、或门，却无

法实现异或门呢？下面我

们尝试通过画图来思

考

其中的原因。

首先，我们试

着将或门的动作形象化

。或门的情况下，当权重参

数

(b, w1,

w2) = (−0.5, 1.0, 1.0)时，可满足图2-4的真值表

条件。此时，感知机

可用下

面的式（2.3）表示。

（2.3）

式（2.3）表示的感

知机会生成由直线−0.5 + x1 + x2

= 0分割

开的两个空

间。其中一个

空间输出1，另一个空间输

出0，如图2-6所示。

x2

x1

0

1

1

图2-6　感知机的

可视化：灰色区域是感知

机输出0的区域，这个区域

与或门的性质一致

或门

在(x1, x2)

= (0, 0)时输出0，在(x1, x2)为(0, 1)、(1, 0)、(1,

1)时输

出1。图

2-6中，○表示0，△表示1。如果想制作

或门，需要用直线将图2-6

30   第

2章

感知机

中的○和△分开。实

际上，刚才的那条直线就

将这4个点正确地分开了

。

那么，换成异或门的话会

如何呢？能否像或门那样

，用一条直线作出分

割图

2-7中的○和△的空间呢？

x2

x1

0 1

1

图2-7　○和△表

示异或门的输出。可否通

过一条直线作出分割○和

△的空间呢？

想要用一条直

线将图2-7中的○和△分开，无论

如何都做不到。事实上，

用

一条直线是无法将○和△分

开的。

2.4.2　线性和非线性

图2-7中

的○和△无法用一条直线分

开，但是如果将“直线”这个

限制条

件去掉，就可以实

现了。比如，我们可以像图

2-8那样，作出分开○和△的空间

。

感知机的局限性就在于

它只能表示由一条直线

分割的空间。图2-8这样弯

曲

的曲线无法用感知机表

示。另外，由图2-8这样的曲线

分割而成的空间称为

非

线性空间，由直线分割而

成的空间称为线性空间

。线性、非线性这两个术

语

在机器学习领域很常见

，可以将其想象成图2-6和图

2-8所示的直线和曲线。

2.5 多层

感知机

31

x2

x1

0

1

1

图2-8　使用曲线可以

分开○和△

2.5 多层感知机

感知

机不能表示异或门让人

深感遗憾，但也无需悲观

。实际上，感知机

的绝妙之

处在于它可以“叠加层”（通

过叠加层来表示异或门

是本节的要点）。

这里，我们

暂且不考虑叠加层具体

是指什么，先从其他视角

来思考一下异或

门的问

题。

2.5.1　已有门电路的组合

异

或门的制作方法有很多

，其中之一就是组合我们

前面做好的与门、与

非门

、或门进行配置。这里，与门

、与非门、或门用图2-9中的符

号表示。另外，

图2-9中与非门

前端的○表示反转输出的

意思。

那么，请思考一下，要

实现异或门的话，需要如

何配置与门、与非门和

或

门呢？这里给大家一个提

示，用与门、与非门、或门代

替图2-10中的各个

“？”，就可以实

现异或门。

32

第 2章　感知机

AND NAND OR

图

2-9　与门、与非门、或门的符号

图2-10　将与门、与非门、或门代

入到“？”中，就可以实现异或

门！

x2

x1

y

2.4节讲到的感知机的局

限性，严格地讲，应该是“单

层感知机无法

表示异或

门”或者“单层感知机无法

分离非线性空间”。接下来

，我

们将看到通过组合感

知机（叠加层）就可以实现

异或门。

异或门可以通过

图2-11所示的配置来实现。这

里，x1和x2表示输入信号，

y表示

输出信号。x1和x2是与非门和

或门的输入，而与非门和

或门的输出则

是与门的

输入。

图2-11　通过组合与门、与

非门、或门实现异或门

x1

s1

x2

s2

y

现

在，我们来确认一下图2-11的

配置是否真正实现了异

或门。这里，把

s1作为与非门

的输出，把s2作为或门的输

出，填入真值表中。结果如

图2-12

所示，观察x1、x2、y，可以发现确

实符合异或门的输出。

2.5 多

层感知机

33

x1

0

1

0

1

0

0

1

1

1

1

1

0

0

1

1

1

0

1

1

0

x2 s1

s2 y

图2-12　异或门的真

值表

2.5.2　异或门的实现

下面

我们试着用Python来实现图2-11所

示的异或门。使用之前定

义的

AND函数、NAND函数、OR函数，可以

像下面这样（轻松地）实现

。

def XOR(x1, x2):

s1 = NAND(x1, x2)

 s2

= OR(x1, x2)

 y =

AND(s1, s2)

 return y

这个XOR函数会输出预期的

结果。

XOR(0, 0) # 输出0

XOR(1, 0)

# 输出1

XOR(0, 1) # 输出1

XOR(1, 1) # 输出

0

这样，异或门的实现就完

成了。下面我们试着用感

知机的表示方法（明

确地

显示神经元）来表示这个

异或门，结果如图2-13所示。

如

图2-13所示，异或门是一种多

层结构的神经网络。这里

，将最左边的

一列称为第

0层，中间的一列称为第1层

，最右边的一列称为第2层

。

图2-13所示的感知机与前面

介绍的与门、或门的感知

机（图2-1）形状不

同。实际上，与

门、或门是单层感知机，而

异或门是2层感知机。叠加

了多

层的感知机也称为

多层感知机（multi-layered perceptron）。

34   第 2章　感知机

图2-13

用感知机表示异或门

x1 s1

x2 s2

y

第0层

第1层 第2层

图 2-13中的感

知机总共由 3层构成，但是

因为拥有权重的层实质

上只有 2层（第

0层和第 1层之

间，第 1层和第 2层之间），所以

称

为“2层感知机”。不过，有的

文献认为图 2-13的感知机是

由

3层

构成的，因而将其称

为“3层感知机”。

在图2-13所示的

2层感知机中，先在第0层和

第1层的神经元之间进行

信号的传送和接收，然后

在第1层和第2层之间进行

信号的传送和接收，具

体

如下所示。

1.第0层的两个神

经元接收输入信号，并将

信号发送至第1层的神经

元。

2.第1层的神经元将信号

发送至第2层的神经元，第

2层的神经元输出y。

这种2层

感知机的运行过程可以

比作流水线的组装作业

。第1段（第1层）

的工人对传送

过来的零件进行加工，完

成后再传送给第2段（第2层

）的工人。

第2层的工人对第

1层的工人传过来的零件

进行加工，完成这个零件

后出货

（输出）。

像这样，在异

或门的感知机中，工人之

间不断进行零件的传送

。通过这

样的结构（2层结构

），感知机得以实现异或门

。这可以解释为“单层感知

机

无法表示的东西，通过

增加一层就可以解决”。也

就是说，通过叠加层（加深

层），感知机能进行更加灵

活的表示。

2.6  从与非门到计

算机 35

2.6

从与非门到计算机

多层感知机可以实现比

之前见到的电路更复杂

的电路。比如，进行加法

运

算的加法器也可以用感

知机实现。此外，将二进制

转换为十进制的编码器

、

满足某些条件就输出1的

电路（用于等价检验的电

路）等也可以用感知机表

示。

实际上，使用感知机甚

至可以表示计算机！

计算

机是处理信息的机器。向

计算机中输入一些信息

后，它会按照某种

既定的

方法进行处理，然后输出

结果。所谓“按照某种既定

的方法进行处理”

是指，计

算机和感知机一样，也有

输入和输出，会按照某个

既定的规则进行

计算。

人

们一般会认为计算机内

部进行的处理非常复杂

，而令人惊讶的是，实

际上

只需要通过与非门的组

合，就能再现计算机进行

的处理。这一令人吃惊

的

事实说明了什么呢？说明

使用感知机也可以表示

计算机。前面也介绍了，

与

非门可以使用感知机实

现。也就是说，如果通过组

合与非门可以实现计算

机的话，那么通过组合感

知机也可以表示计算机

（感知机的组合可以通过

叠

加了多层的单层感知

机来表示）。

说到仅通过与

非门的组合就能实现计

算机，大家也许一下子很

难相信。

建议有兴趣的读

者看一下《计算机系统要

素：从零开始构建现代计

算机》。这本书以深入理解

计算机为主题，论述了通

过 NAND构建可

运行俄罗斯方

块的计算机的过程。此书

能让读者真实体会到，通

过

简单的

NAND元件就可以实

现计算机这样复杂的系

统。

综上，多层感知机能够

进行复杂的表示，甚至可

以构建计算机。那么，

什么

构造的感知机才能表示

计算机呢？层级多深才可

以构建计算机呢？

理论上

可以说2层感知机就能构

建计算机。这是因为，已有

研究证明，

2层感知机（严格

地说是激活函数使用了

非线性的sigmoid函数的感知机

，具

体请参照下一章）可以

表示任意函数。但是，使用

2层感知机的构造，通过

36   第

2章　感知机

设定合适的权

重来构建计算机是一件

非常累人的事情。实际上

，在用与非门

等低层的元

件构建计算机的情况下

，分阶段地制作所需的零

件（模块）会比

较自然，即先

实现与门和或门，然后实

现半加器和全加器，接着

实现算数逻

辑单元（ALU），然后

实现CPU。因此，通过感知机表

示计算机时，使用叠

加了

多层的构造来实现是比

较自然的流程。

本书中不

会实际来实现计算机，但

是希望读者能够记住，感

知机通过叠

加层能够进

行非线性的表示，理论上

还可以表示计算机进行

的处理。

2.7 小结

本章我们学

习了感知机。感知机是一

种非常简单的算法，大家

应该很快

就能理解它的

构造。感知机是下一章要

学习的神经网络的基础

，因此本章的

内容非常重

要。

本章所学的内容

• 感知

机是具有输入和输出的

算法。给定一个输入后，将

输出一个既

定的值。

• 感知

机将权重和偏置设定为

参数。

•

使用感知机可以表

示与门和或门等逻辑电

路。

• 异或门无法通过单层

感知机来表示。

• 使用2层感

知机可以表示异或门。

•

单

层感知机只能表示线性

空间，而多层感知机可以

表示非线性空间。

• 多层感

知机（在理论上）可以表示

计算机。

第3章

神经网络

上

一章我们学习了感知机

。关于感知机，既有好消息

，也有坏消息。好

消息是，即

便对于复杂的函数，感知

机也隐含着能够表示它

的可能性。上一

章已经介

绍过，即便是计算机进行

的复杂处理，感知机（理论

上）也可以将

其表示出来

。坏消息是，设定权重的工

作，即确定合适的、能符合

预期的输

入与输出的权

重，现在还是由人工进行

的。上一章中，我们结合与

门、或门

的真值表人工决

定了合适的权重。

神经网

络的出现就是为了解决

刚才的坏消息。具体地讲

，神经网络的一

个重要性

质是它可以自动地从数

据中学习到合适的权重

参数。本章中，我们

会先介

绍神经网络的概要，然后

重点关注神经网络进行

识别时的处理。在下

一章

中，我们将了解如何从数

据中学习权重参数。

3.1 从感

知机到神经网络

神经网

络和上一章介绍的感知

机有很多共同点。这里，我

们主要以两者

的差异为

中心，来介绍神经网络的

结构。

3.1.1　神经网络的例子

用

图来表示神经网络的话

，如图3-1所示。我们把最左边

的一列称为

输入层，最右

边的一列称为输出层，中

间的一列称为中间层。中

间层有时

38

第 3章　神经网络

也称为隐藏层。“隐藏”一词

的意思是，隐藏层的神经

元（和输入层、输出

层不同

）肉眼看不见。另外，本书中

把输入层到输出层依次

称为第0层、第

1层、第2层（层号

之所以从0开始，是为了方

便后面基于Python进行实现）。

图

3-1中，第0层对应输入层，第1层

对应中间层，第2层对应输

出层。

输入层 输出层

中间

层

图3-1　神经网络的例子

图

3-1中的网络一共由 3层神经

元构成，但实质上只有 2层

神经

元有权重，因此将其

称为“2层网络”。请注意，有的

书也会根据

构成网络的

层数，把图 3-1的网络称为“3层

网络”。本书将根据

实质上

拥有权重的层数（输入层

、隐藏层、输出层的总数减

去 1

后的数量）来表示网络

的名称。

只看图3-1的话，神经

网络的形状类似上一章

的感知机。实际上，就神

经

元的连接方式而言，与上

一章的感知机并没有任

何差异。那么，神经网络

中

信号是如何传递的呢？

3.1.2　复

习感知机

在观察神经网

络中信号的传递方法之

前，我们先复习一下感知

机。现在

3.1 从感知机到神经

网络

39

来思考一下图3-2中的

网络结构。

x1

w1

x2

w2

y

图3-2　复习感知机

图3-2中的感知机接收x1和x2两

个输入信号，输出y。如果用

数学式来

表示图3-2中的感

知机，则如式（3.1）所示。

（3.1）

b是被称

为偏置的参数，用于控制

神经元被激活的容易程

度；而w1和w2

是表示各个信号

的权重的参数，用于控制

各个信号的重要性。

顺便

提一下，在图3-2的网络中，偏

置b并没有被画出来。如果

要明确

地表示出b，可以像

图3-3那样做。图3-3中添加了权

重为b的输入信号1。这

个感

知机将x1、x2、1三个信号作为神

经元的输入，将其和各自

的权重相乘后，

传送至下

一个神经元。在下一个神

经元中，计算这些加权信

号的总和。如果

这个总和

超过0，则输出1，否则输出0。另

外，由于偏置的输入信号

一直是1，

所以为了区别于

其他神经元，我们在图中

把这个神经元整个涂成

灰色。

现在将式（3.1）改写成更

加简洁的形式。为了简化

式（3.1），我们用一个

函数来表

示这种分情况的动作（超

过0则输出1，否则输出0）。引入

新函数

h(x)，将式（3.1）改写成下面

的式（3.2）和式（3.3）。

y =

h(b + w1x1 + w2x2) （3.2）

40   第 3章　神经网络

x1

w1

x2

w2

y

1

b

图3-3　明确表示出偏置

（3.3）

式（3.2）中

，输入信号的总和会被函

数h(x)转换，转换后的值就是

输出y。

然后，式（3.3）所表示的函

数h(x)，在输入超过0时返回1，否

则返回0。因此，

式（3.1）和式（3.2）、式（3.3）做

的是相同的事情。

3.1.3　激活函

数登场

刚才登场的h（x）函数

会将输入信号的总和转

换为输出信号，这种函数

一般称为激活函数（activation function）。如“激

活”一词所示，激活函数的

作用在于决定如何来激

活输入信号的总和。

现在

来进一步改写式（3.2）。式（3.2）分两

个阶段进行处理，先计算

输入

信号的加权总和，然

后用激活函数转换这一

总和。因此，如果将式（3.2）写

得

详细一点，则可以分成下

面两个式子。

a = b + w1x1

+ w2x2 （3.4）

y = h(a)

（3.5）

首先，式（3.4）计算

加权输入信号和偏置的

总和，记为a。然后，式（3.5）

用h()函数

将a转换为输出y。

3.1 从感知机

到神经网络

41

之前的神经

元都是用一个○表示的，如

果要在图中明确表示出

式（3.4）

和式（3.5），则可以像图3-4这样

做。

x1

w1

x2

w2

y

1

b

a

图3-4

明确显示激活函数

的计算过程

h()

如图3-4所示，表

示神经元的○中明确显示

了激活函数的计算过程

，即

信号的加权总和为节

点a，然后节点a被激活函数

h()转换成节点y。本书中，“神

经

元”和“节点”两个术语的含

义相同。这里，我们称a和y为

“节点”，其实

它和之前所说

的“神经元”含义相同。

通常

如图3-5的左图所示，神经元

用一个○表示。本书中，在可

以明确

神经网络的动作

的情况下，将在图中明确

显示激活函数的计算过

程，如图3-5

的右图所示。

图3-5 左

图是一般的神经元的图

，右图是在神经元内部明

确显示激活函数的计算

过程的图（a

表示输入信号

的总和，h()表示激活函数，y表

示输出）

y

h()

a

42

第 3章　神经网络

下

面，我们将仔细介绍激活

函数。激活函数是连接感

知机和神经网络的

桥梁

。A

本书在使用“感知机”一词

时，没有严格统一它所指

的算法。一

般而言，“朴素感

知机”是指单层网络，指的

是激活函数使用了阶

跃

函数A 的模型。“多层感知机

”是指神经网络，即使用 sigmoid

函

数（后述）等平滑的激活函

数的多层网络。

3.2

激活函数

式（3.3）表示的激活函数以阈

值为界，一旦输入超过阈

值，就切换输出。

这样的函

数称为“阶跃函数”。因此，可

以说感知机中使用了阶

跃函数作为

激活函数。也

就是说，在激活函数的众

多候选函数中，感知机使

用了阶跃函数。

那么，如果

感知机使用其他函数作

为激活函数的话会怎么

样呢？实际上，如

果将激活

函数从阶跃函数换成其

他函数，就可以进入神经

网络的世界了。下

面我们

就来介绍一下神经网络

使用的激活函数。

3.2.1　sigmoid函数

神

经网络中经常使用的一

个激活函数就是式（3.6）表示

的sigmoid函数

（sigmoid function）。

（3.6）

式（3.6）中的exp(−x)表示e

−x

的意

思。e是纳皮尔常数2.7182 ...。式（3.6）

表示

的sigmoid函数看上去有些复杂

，但它也仅仅是个函数而

已。而函数就是

给定某个

输入后，会返回某个输出

的转换器。比如，向sigmoid函数输

入1.0或2.0

后，就会有某个值被

输出，类似h(1.0) = 0.731 ...、h(2.0) = 0.880

...这样。

神经网络

中用sigmoid函数作为激活函数

，进行信号的转换，转换后

的

A 阶跃函数是指一旦输

入超过阈值，就切换输出

的函数。

3.2 激活函数

43

信号被

传送给下一个神经元。实

际上，上一章介绍的感知

机和接下来要介绍

的神

经网络的主要区别就在

于这个激活函数。其他方

面，比如神经元的多层

连

接的构造、信号的传递方

法等，基本上和感知机是

一样的。下面，让我们

通过

和阶跃函数的比较来详

细学习作为激活函数的

sigmoid函数。

3.2.2

阶跃函数的实现

这

里我们试着用Python画出阶跃

函数的图（从视觉上确认

函数的形状对

理解函数

而言很重要）。阶跃函数如

式（3.3）所示，当输入超过0时，输

出1，

否则输出0。可以像下面

这样简单地实现阶跃函

数。

def step_function(x):

if x > 0:

 return

1

 else:

 return 0

这个实现简单、易于理

解，但是参数x只能接受实

数（浮点数）。也就是

说，允许

形如step_function(3.0)的调用，但不允许参

数取NumPy数组，例

如step_function(np.array([1.0, 2.0]))。为了便于

后面的操作，我们把它修

改为支持NumPy数组的实现。为

此，可以考虑下述实现。

def step_function(x):

y = x > 0

return y.astype(np.int)

上

述函数的内容只有两行

。由于使用了NumPy中的“技巧”，可

能会有

点难理解。下面我

们通过Python解释器的例子来

看一下这里用了什么技

巧。

下面这个例子中准备

了NumPy数组x，并对这个NumPy数组进

行了不等号

运算。

>>> import numpy as np

>>>

x = np.array([-1.0, 1.0, 2.0])

>>>

x

array([-1., 1., 2.])

>>> y

= x > 0

44

第 3章　神

经网络

>>> y

array([False,

True, True], dtype=bool)

对NumPy数组进行不等

号运算后，数组的各个元

素都会进行不等号运算

，

生成一个布尔型数组。这

里，数组x中大于0的元素被

转换为True，小于等

于0的元素

被转换为False，从而生成一个

新的数组y。

数组y是一个布

尔型数组，但是我们想要

的阶跃函数是会输出int型

的0

或1的函数。因此，需要把

数组y的元素类型从布尔

型转换为int型。

>>> y = y.astype(np.int)

>>> y

array([0, 1, 1])

如上所示，可

以用astype()方法转换NumPy数组的类

型。astype()方

法通过参数指定期

望的类型，这个例子中是

np.int型。Python中将布尔型

转换为int型

后，True会转换为1，False会转换为0。以

上就是阶跃函数的

实现

中所用到的NumPy的“技巧”。

3.2.3　阶跃

函数的图形

下面我们就

用图来表示上面定义的

阶跃函数，为此需要使用

matplotlib库。

import numpy as np

import matplotlib.pylab

as plt

def step_function(x):

 return

np.array(x > 0, dtype=np.int)

x =

np.arange(-5.0, 5.0, 0.1)

y = step_function(x)

plt.plot(x, y)

plt.ylim(-0.1, 1.1) # 指定y轴的范围

plt.show()

np.arange(-5.0, 5.0, 0.1)在−5.0到5.0的

范围内，以0.1为单位，生成

NumPy数

组（[-5.0, -4.9,

���, 4.9]）。step_function()以该NumPy数组为

参数，对数

组的各个元素执行阶跃

函数运算，并以数组形式

返回运算结果。

对数组x、y进

行绘图，结果如图3-6所示。

3.2 激

活函数

45

图3-6　阶跃函数的图

形

1.0

0.8

0.6

0.4

0.2

0.0

−6 −4 −2

0 2 4 6

如图3-6所示，阶跃函数以

0为界，输出从0切换为1（或者

从1切换为0）。

它的值呈阶梯

式变化，所以称为阶跃函

数。

3.2.4　sigmoid函数的实现

下面，我们

来实现sigmoid函数。用Python可以像下

面这样写出式（3.6）

表示的sigmoid函

数。

def sigmoid(x):

return 1 / (1 + np.exp(-x))

这里，np.exp(-x)对应exp(−x)。这个实现没

有什么特别难的地方，但

是要注意参数x为NumPy数组时

，结果也能被正确计算。实

际上，如果在

这个sigmoid函数中

输入一个NumPy数组，则结果如

下所示。

>>> x = np.array([-1.0,

1.0, 2.0])

>>> sigmoid(x)

array([ 0.26894142,

0.73105858, 0.88079708])

46   第

3章　神经网络

之

所以sigmoid函数的实现能支持

NumPy数组，秘密就在于NumPy的

广播

功能（1.5.5节）。根据NumPy 的广播功能

，如果在标量和NumPy数组

之间

进行运算，则标量会和NumPy数

组的各个元素进行运算

。这里来看一

个具体的例

子。

>>> t = np.array([1.0, 2.0,

3.0])

>>> 1.0 + t

array([

2., 3., 4.])

>>> 1.0 /

t

array([ 1. , 0.5 ,

0.33333333])

在这个例子中，标量（例

子中是1.0）和NumPy数组之间进行

了数值运

算（+、/等）。结果，标量

和NumPy数组的各个元素进行

了运算，运算结

果以NumPy数组

的形式被输出。刚才的sigmoid函

数的实现也是如此，因

为

np.exp(-x)会生成NumPy数组，所以1 /

(1 + np.exp(-x))的运算

将会在

NumPy数组的各个元素

间进行。

下面我们把sigmoid函数

画在图上。画图的代码和

刚才的阶跃函数的代

码

几乎是一样的，唯一不同

的地方是把输出y的函数

换成了sigmoid函数。

x = np.arange(-5.0, 5.0, 0.1)

y

= sigmoid(x)

plt.plot(x, y)

plt.ylim(-0.1, 1.1)

# 指定y轴的范

围

plt.show()

运行上面的代码，可以

得到图3-7。

3.2.5　sigmoid函数和阶跃函数

的比较

现在我们来比较

一下sigmoid 函数和阶跃函数，如

图3-8所示。两者的

不同点在

哪里呢？又有哪些共同点

呢？我们通过观察图3-8来思

考一下。

观察图3-8，首先注意

到的是“平滑性”的不同。sigmoid函

数是一条平

滑的曲线，输

出随着输入发生连续性

的变化。而阶跃函数以0为

界，输出发

生急剧性的变

化。sigmoid函数的平滑性对神经

网络的学习具有重要意

义。

3.2 激活函数  47

图3-7 sigmoid函数的图

形

1.0

0.8

0.6

0.4

0.2

0.0

−6 −4 −2 0 2 4

6

图3-8　阶跃函数与sigmoid函数（虚

线是阶跃函数）

1.0

0.8

0.6

0.4

0.2

0.0

−6 −4 −2

0 2 4 6

48

第 3章　神经

网络

另一个不同点是，相

对于阶跃函数只能返回

0或1，sigmoid函数可以返

回0.731 ...、0.880

...等实数

（这一点和刚才的平滑性

有关）。也就是说，感

知机中

神经元之间流动的是0或

1的二元信号，而神经网络

中流动的是连续

的实数

值信号。

如果把这两个函

数与水联系起来，则阶跃

函数可以比作“竹筒敲石

”A，

sigmoid函数可以比作“水车”。阶跃

函数就像竹筒敲石一样

，只做是否传送

水（0或1）两个

动作，而sigmoid函数就像水车一

样，根据流过来的水量相

应

地调整传送出去的水

量。

接着说一下阶跃函数

和sigmoid函数的共同性质。阶跃

函数和sigmoid

函数虽然在平滑

性上有差异，但是如果从

宏观视角看图3-8，可以发现

它们

具有相似的形状。实

际上，两者的结构均是“输

入小时，输出接近0（为0）；

随着

输入增大，输出向1靠近（变

成1）”。也就是说，当输入信号

为重要信息时，

阶跃函数

和sigmoid函数都会输出较大的

值；当输入信号为不重要

的信息时，

两者都输出较

小的值。还有一个共同点

是，不管输入信号有多小

，或者有多

大，输出信号的

值都在0到1之间。

3.2.6　非线性函

数

阶跃函数和sigmoid函数还有

其他共同点，就是两者均

为非线性函数。

sigmoid函数是一

条曲线，阶跃函数是一条

像阶梯一样的折线，两者

都属于

非线性的函数。

在

介绍激活函数时，经常会

看到“非线性函数”和“线性

函数”等术语。

函数本来是

输入某个值后会返回一

个值的转换器。向这个转

换器输

入某个值后，输出

值是输入值的常数倍的

函数称为线性函数（用数

学

式表示为h(x) =

cx。c为常数）。因此

，线性函数是一条笔直的

直线。

而非线性函数，顾名

思义，指的是不像线性函

数那样呈现出一条直

线

的函数。

A 竹筒敲石是日本

的一种庭院设施。支点架

起竹筒，一端下方置石，另

一端切口上翘。在切口上

滴水，

水积多后该端下垂

，水流出，另一端翘起，之后

又因重力而落下，击石发

出响声。——译者注

3.2 激活函数

49

神经网络的激活函数必

须使用非线性函数。换句

话说，激活函数不能使

用

线性函数。为什么不能使

用线性函数呢？因为使用

线性函数的话，加深神

经

网络的层数就没有意义

了。

线性函数的问题在于

，不管如何加深层数，总是

存在与之等效的“无

隐藏

层的神经网络”。为了具体

地（稍微直观地）理解这一

点，我们来思

考下面这个

简单的例子。这里我们考

虑把线性函数 h(x) = cx

作为激活

函数，把y(x) = h(h(h(x)))的运算对应3层神

经网络A。这个运算会进行

y(x) = c ×

c × c × x的乘法运算，但是同样的

处理可以由y(x) =

ax（注意，

a = c 3

）这一次

乘法运算（即没有隐藏层

的神经网络）来表示。如本

例所示，

使用线性函数时

，无法发挥多层网络带来

的优势。因此，为了发挥叠

加层所

带来的优势，激活

函数必须使用非线性函

数。

3.2.7　ReLU函数

到目前为止，我们

介绍了作为激活函数的

阶跃函数和sigmoid函数。在

神经

网络发展的历史上，sigmoid函数

很早就开始被使用了，而

最近则主要

使用ReLU（Rectified Linear Unit）函数。

ReLU函

数在输入大于0时，直接输

出该值；在输入小于等于

0时，输

出0（图3-9）。

ReLU函数可以表示

为下面的式(3.7)。

（3.7）

如图 3-9 和式（3.7）所

示，ReLU 函数是一个非常简单

的函数。因此，

ReLU函数的实现

也很简单，可以写成如下

形式。

def relu(x):

 return np.maximum(0, x)

A 该对应只是一个近

似，实际的神经网络运算

比这个例子要复杂，但不

影响后面的结论成立。

——译

者注

50

第 3章　神经网络

图3-9 ReLU函

数

5

4

3

2

1

−1

0

−6 −4 −2 0 2 4

6

这里使用了NumPy的maximum函数。maximum函

数会从输入的数值中选

择较大的那个值进行输

出。

本章剩余部分的内容

仍将使用sigmoid函数作为激活

函数，但在本书的

后半部

分，则将主要使用ReLU函数。

3.3 多

维数组的运算

如果掌握

了NumPy多维数组的运算，就可

以高效地实现神经网络

。因此，

本节将介绍NumPy多维数

组的运算，然后再进行神

经网络的实现。

3.3.1　多维数组

简单地讲，多维数组就是

“数字的集合”，数字排成一

列的集合、排成

长方形的

集合、排成三维状或者（更

加一般化的）N维状的集合

都称为多维数

组。下面我

们就用NumPy来生成多维数组

，先从前面介绍过的一维

数组开始。

3.3 多维数组的运

算  51

>>> import

numpy as np

>>> A =

np.array([1, 2, 3, 4])

>>> print(A)

[1 2 3 4]

>>> np.ndim(A)

1

>>> A.shape

(4,)

>>> A.shape[0]

4

如上所示，数组的维数

可以通过np.dim()函数获得。此外

，数组的形状

可以通过实

例变量shape获得。在上面的例

子中，A是一维数组，由4个元

素

构成。注意，这里的A.shape的结

果是个元组（tuple）。这是因为一

维数组的

情况下也要返

回和多维数组的情况下

一致的结果。例如，二维数

组时返回的

是元组(4,3)，三维

数组时返回的是元组(4,3,2)，因

此一维数组时也同样以

元组的形式返回结果。下

面我们来生成一个二维

数组。

>>> B = np.array([[1,2], [3,4], [5,6]])

>>> print(B)

[[1 2]

 [3

4]

 [5 6]]

>>> np.ndim(B)

2

>>> B.shape

(3, 2)

这里生成了一个3

× 2的

数组B。3 × 2的数组表示第一个

维度有3个元素，

第二个维

度有2个元素。另外，第一个

维度对应第0维，第二个维

度对应第

1维（Python的索引从0开

始）。二维数组也称为矩阵

（matrix）。如图3-10所示，

数组的横向排

列称为行（row），纵向排列称为

列（column）。

3.3.2　矩阵乘法

下面，我们来

介绍矩阵（二维数组）的乘

积。比如2 × 2的矩阵，其乘积

可

以像图3-11这样进行计算（按

图中顺序进行计算是规

定好了的）。

52   第 3章

神经网络

21

43

6

列

行

5

图3-10　横向排列称为行

，纵向排列称为列

图3-11　矩阵

的乘积的计算方法

21

43

A

=

B

65

87

2219

5043

1 × 5 + 2

× 7

3 × 5 +

4 × 7

如本

例所示，矩阵的乘积是通

过左边矩阵的行（横向）和

右边矩阵的列（纵

向）以对

应元素的方式相乘后再

求和而得到的。并且，运算

的结果保存为新

的多维

数组的元素。比如，A的第1行

和B的第1列的乘积结果是

新数组的

第1行第1列的元

素，A的第2行和B的第1列的结

果是新数组的第2行第1

列

的元素。另外，在本书的数

学标记中，矩阵将用黑斜

体表示（比如，矩阵

A），以区别

于单个元素的标量（比如

，a或b）。这个运算在Python中可以用

如下代码实现。

>>> A =

np.array([[1,2], [3,4]])

3.3 多维数组

的运算  53

>>> A.shape

(2, 2)

>>> B

= np.array([[5,6], [7,8]])

>>> B.shape

(2,

2)

>>> np.dot(A, B)

array([[19, 22],

[43, 50]])

这 里，A 和 B

都 是 2 × 2 的

矩

阵，它 们 的 乘 积 可

以 通 过

NumPy 的

np.dot()函数计算（乘积也称为

点积）。np.dot()接收两个NumPy数组作为

参

数，并返回数组的乘积

。这里要注意的是，np.dot(A,

B)和np.dot(B, A)的

值

可能不一样。和一般的运

算（+或*等）不同，矩阵的乘积

运算中，操作数（A、

B）的顺序不

同，结果也会不同。

这里介

绍的是计算2 ×

2形状的矩阵

的乘积的例子，其他形状

的矩阵的

乘积也可以用

相同的方法来计算。比如

，2 × 3的矩阵和3 × 2

的矩阵的乘积

可按如下形式用Python来实现

。

>>> A = np.array([[1,2,3], [4,5,6]])

>>> A.shape

(2, 3)

>>> B

= np.array([[1,2], [3,4], [5,6]])

>>> B.shape

(3, 2)

>>> np.dot(A, B)

array([[22,

28],

 [49, 64]])

2 ×

3的矩阵A和3 × 2的矩阵B的乘积

可按以上方式实现。这里

需要

注意的是矩阵的形

状（shape）。具体地讲，矩阵A的第1维

的元素个数（列数）

必须和

矩阵B的第0维的元素个数

（行数）相等。在上面的例子

中，矩阵A

的形状是2

× 3，矩阵B的

形状是3 × 2，矩阵A的第1维的元

素个数（3）和

矩阵B的第0维的

元素个数（3）相等。如果这两

个值不相等，则无法计算

矩

阵的乘积。比如，如果用

Python计算2

× 3 的矩阵A和2 × 2的矩阵C的

乘

积，则会输出如下错误

。

>>> C = np.array([[1,2], [3,4]])

>>>

C.shape

54   第 3章

神经网络

(2, 2)

>>> A.shape

(2,

3)

>>> np.dot(A, C)

Traceback (most

recent call last):

 File "<stdin>",

line 1, in <module>

ValueError: shapes

(2,3) and (2,2) not aligned: 3

(dim 1) != 2 (dim 0)

这个错误

的意思是，矩阵A的第1维和

矩阵C的第0维的元素个数

不一

致（维度的索引从0开

始）。也就是说，在多维数组

的乘积运算中，必须使两

个矩阵中的对应维度的

元素个数一致，这一点很

重要。我们通过图3-12再来

确

认一下。

图3-12　在矩阵的乘积

运算中，对应维度的元素

个数要保持一致

A

B = C

形状： 3 ×

2 2 × 4 3 ×

4

保

持一致

图3-12中，3 × 2的矩阵A和2 ×

4 的

矩阵B的乘积运算生成了

3 × 4的

矩阵C。如图所示，矩阵A和

矩阵B的对应维度的元素

个数必须保持一致。

此外

，还有一点很重要，就是运

算结果的矩阵C的形状是

由矩阵A的行数

和矩阵B的

列数构成的。

另外，当A是二

维矩阵、B是一维数组时，如

图3-13所示，对应维度

的元素

个数要保持一致的原则

依然成立。

可按如下方式

用Python实现图3-13的例子。

>>> A

= np.array([[1,2], [3, 4], [5,6]])

>>>

A.shape

(3, 2)

>>> B =

np.array([7,8])

>>> B.shape

(2,)

>>> np.dot(A,

B)

array([23, 53, 83])

3.3 多维数

组的运算

55

图3-13 A是二维矩阵

、B是一维数组时，也要保持

对应维度的元素个数一

致

形状： 3 ×

2 2 3

保持一致

A B

= C

3.3.3　神经网

络的内积

下面我们使用

NumPy矩阵来实现神经网络。这

里我们以图3-14中的简

单神

经网络为对象。这个神经

网络省略了偏置和激活

函数，只有权重。

X W Y =

2 2

× 3 3

1

2

3

5

4

6

)( 1 3

5

2 4 6

一致

x1

x2

y 1

y 2

y

3

图3-14　通

过矩阵的乘积进行神经

网络的运算

实现该神经

网络时，要注意X、W、Y的形状，特

别是X和W的对应

维度的元

素个数是否一致，这一点

很重要。

>>>

X = np.array([1, 2])

>>> X.shape

(2,)

>>> W = np.array([[1, 3,

5], [2, 4, 6]])

>>> print(W)

[[1 3 5]

 [2 4

6]]

>>> W.shape

56

第 3章　神经网络

(2, 3)

>>>

Y = np.dot(X, W)

>>> print(Y)

[ 5 11 17]

如

上所示，使用np.dot（多维数组的

点积），可以一次性计算出

Y 的结果。

这意味着，即便Y 的

元素个数为100或1000，也可以通

过一次运算就计算出

结

果！如果不使用np.dot，就必须单

独计算Y 的每一个元素（或

者说必须使

用for语句），非常

麻烦。因此，通过矩阵的乘

积一次性完成计算的技

巧，在

实现的层面上可以

说是非常重要的。

3.4 3层神经

网络的实现

现在我们来

进行神经网络的实现。这

里我们以图3-15的3层神经网

络为

对象，实现从输入到

输出的（前向）处理。在代码

实现方面，使用上一节介

绍的NumPy多维数组。巧妙地使

用NumPy数组，可以用很少的代

码完成

神经网络的前向

处理。

图3-15

3层神经网络：输入

层（第0层）有2个神经元，第1个

隐藏层（第1层）有3个神经元

，

第2个隐藏层（第2层）有2个神

经元，输出层（第3层）有2个神

经元

x1

x2

y 1

y 2

3.4  3层神经网络的实现

57

3.4.1　符号确认

在介绍神经网

络中的处理之前，我们先

导入 、 等符号。这些符

号可

能看上去有些复杂，不过

因为只在本节使用，稍微

读一下就跳过去也问

题

不大。

本节的重点是神经

网络的运算可以作为矩

阵运算打包进行。因为

神

经网络各层的运算是通

过矩阵的乘法运算打包

进行的（从宏观

视角来考

虑），所以即便忘了（未记忆

）具体的符号规则，也不影

响理解后面的内容。

我们

先从定义符号开始。请看

图3-16。图3-16中只突出显示了从

输入层

神经元x2到后一层

的神经元

的权重。

如图3-16所

示，权重和隐藏层的神经

元的右上角有一个“(1)”，它表

示

权重和神经元的层号

（即第1层的权重、第1层的神

经元）。此外，权重的右

下角

有两个数字，它们是后一

层的神经元和前一层的

神经元的索引号。比如，

表

示前一层的第2个神经元

x2到后一层的第1个神经元

的权重。权

重右下角按照

“后一层的索引号、前一层

的索引号”的顺序排列。

w

1 2

(1)

第

1层的权重

后一层的第1个

神经元

前一层的第2个神

经元

x1

x2

图3-16　权重的符号

58

第 3章

神经网络

3.4.2　各层间信号传

递的实现

现在看一下从

输入层到第1层的第1个神

经元的信号传递过程，如

图3-17

所示。

1

x1

x2

y 1

y 2

图3-17　从输入层到第

1层的信号传递

图3-17中增加

了表示偏置的神经元“1”。请

注意，偏置的右下角的索

引

号只有一个。这是因为

前一层的偏置神经元（神

经元“1”）只有一个A

。

为了确认

前面的内容，现在用数学

式表示 。 通过加权信号和

偏

置的和按如下方式进

行计算。

（3.8）

A 任何前一层的偏

置神经元“1”都只有一个。偏

置权重的数量取决于后

一层的神经元的数量（不

包括

后一层的偏置神经

元“1”）。——译者注

3.4 3层神经网络的

实现

59

此外，如果使用矩阵

的乘法运算，则可以将第

1层的加权和表示成下面

的式（3.9）。

A(1) = XW(1) +

B(1) （3.9）

其中，A(1)

、X、B(1)

、W(1)

如下所示。

下面

我们用NumPy多维数组来实现

式（3.9），这里将输入信号、权重

、

偏置设置成任意值。

X = np.array([1.0, 0.5])

W1 = np.array([[0.1, 0.3, 0.5], [0.2,

0.4, 0.6]])

B1 = np.array([0.1, 0.2,

0.3])

print(W1.shape) # (2, 3)

print(X.shape)

# (2,)

print(B1.shape) # (3,)

A1

= np.dot(X, W1) + B1

这个

运算和上一节进行的运

算是一样的。W1是2

× 3的数组，X是

元素个

数为2的一维数组

。这里，W1和X的对应维度的元

素个数也保持了一致。

接

下来，我们观察第1层中激

活函数的计算过程。如果

把这个计算过程

用图来

表示的话，则如图3-18所示。

如

图3-18所示，隐藏层的加权和

（加权信号和偏置的总和

）用a表示，被

激活函数转换

后的信号用z表示。此外，图

中h()表示激活函数，这里我

们

使用的是sigmoid函数。用Python来实

现，代码如下所示。

Z1 = sigmoid(A1)

print(A1)

# [0.3, 0.7, 1.1]

print(Z1) #

[0.57444252, 0.66818777, 0.75026011]

60

第 3章　神

经网络

图3-18　从输入层到第

1层的信号传递

1

1

1

h()

h()

h()

x1

x2

y1

y2

这个sigmoid()函数

就是之前定义的那个函

数。它会接收NumPy数组，

并返回

元素个数相同的NumPy数组。

下

面，我们来实现第1层到第

2层的信号传递（图3-19）。

W2 = np.array([[0.1, 0.4], [0.2, 0.5],

[0.3, 0.6]])

B2 = np.array([0.1, 0.2])

print(Z1.shape) # (3,)

print(W2.shape) # (3,

2)

print(B2.shape) # (2,)

A2 =

np.dot(Z1, W2) + B2

Z2 =

sigmoid(A2)

除了第

1层的输出（Z1）变成了第2层的

输入这一点以外，这个实

现和刚

才的代码完全相

同。由此可知，通过使用NumPy数

组，可以将层到层的信

号

传递过程简单地写出来

。

3.4 3层神经网络的实现

61

图3-19　第

1层到第2层的信号传递

h()

h()

1

1

1

x1

x2

y1

y2

最

后是第2层到输出层的信

号传递（图3-20）。输出层的实现

也和之前的

实现基本相

同。不过，最后的激活函数

和之前的隐藏层有所不

同。

def identity_function(x):

 return

x

W3 = np.array([[0.1, 0.3], [0.2,

0.4]])

B3 = np.array([0.1, 0.2])

A3

= np.dot(Z2, W3) + B3

Y

= identity_function(A3) # 或者Y = A3

这里我们定义了

identity_function()函数（也称为“恒等函数”），并

将

其作为输出层的激活

函数。恒等函数会将输入

按原样输出，因此，这个例

子

中没有必要特意定义

identity_function()。这里这样实现只是为了

和之前的

流程保持统一

。另外，图3-20中，输出层的激活

函数用σ()表示，不同于隐

藏

层的激活函数h()（σ读作sigma）。

62

第 3章

神经网络

图3-20　从第2层到输

出层的信号传递

1

1

1

x1

x2

y1

y2

σ()

σ()

输出层

所用的激活函数，要根据

求解问题的性质决定。一

般地，回

归问题可以使用

恒等函数，二元分类问题

可以使用 sigmoid函数，

多元分类

问题可以使用

softmax函数。关于

输出层的激活函数，我

们

将在下一节详细介绍。

3.4.3　代

码实现小结

至此，我们已

经介绍完了3层神经网络

的实现。现在我们把之前

的代码

实现全部整理一

下。这里，我们按照神经网

络的实现惯例，只把权重

记为大

写字母W1，其他的（偏

置或中间结果等）都用小

写字母表示。

def init_network():

 network =

{}

 network['W1'] = np.array([[0.1, 0.3,

0.5], [0.2, 0.4, 0.6]])

 network['b1']

= np.array([0.1, 0.2, 0.3])

 network['W2']

= np.array([[0.1, 0.4], [0.2, 0.5], [0.3,

0.6]])

 network['b2'] = np.array([0.1, 0.2])

network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])

3.5 输出层的设

计  63

 network['b3']

= np.array([0.1, 0.2])

 return network

def forward(network, x):

 W1, W2,

W3 = network['W1'], network['W2'], network['W3']

b1, b2, b3 = network['b1'], network['b2'],

network['b3']

 a1 = np.dot(x, W1)

+ b1

 z1 = sigmoid(a1)

a2 = np.dot(z1, W2) + b2

z2 = sigmoid(a2)

 a3 =

np.dot(z2, W3) + b3

 y

= identity_function(a3)

 return y

network

= init_network()

x = np.array([1.0, 0.5])

y = forward(network, x)

print(y) #

[ 0.31682708 0.69627909]

这里定义了init_network()和forward()函数。init_network()函

数会进

行权重和偏置的

初始化，并将它们保存在

字典变量network中。这个字典变

量network中保存了每一层所需

的参数（权重和偏置）。forward()函数

中则封

装了将输入信号

转换为输出信号的处理

过程。

另外，这里出现了forward（前

向）一词，它表示的是从输

入到输出方向

的传递处

理。后面在进行神经网络

的训练时，我们将介绍后

向（backward，

从输出到输入方向）的

处理。

至此，神经网络的前

向处理的实现就完成了

。通过巧妙地使用NumPy

多维数

组，我们高效地实现了神

经网络。

3.5

输出层的设计

神

经网络可以用在分类问

题和回归问题上，不过需

要根据情况改变输出

层

的激活函数。一般而言，回

归问题用恒等函数，分类

问题用softmax函数。

64

第 3章　神经网

络

机器学习的问题大致

可以分为分类问题和回

归问题。分类问题是数

据

属于哪一个类别的问题

。比如，区分图像中的人是

男性还是女性

的问题就

是分类问题。而回归问题

是根据某个输入预测一

个（连续的）

数值的问题。比

如，根据一个人的图像预

测这个人的体重的问题

就

是回归问题（类似“57.4kg”这样

的预测）。

3.5.1　恒等函数和 softmax函数

恒等函数会将输入按原

样输出，对于输入的信息

，不加以任何改动地直

接

输出。因此，在输出层使用

恒等函数时，输入信号会

原封不动地被输出。

另外

，将恒等函数的处理过程

用之前的神经网络图来

表示的话，则如图3-21

所示。和

前面介绍的隐藏层的激

活函数一样，恒等函数进

行的转换处理可以

用一

根箭头来表示。

a1 y1

a2

y2

a3 y3

σ()

σ()

σ()

图3-21　恒等函

数

分类问题中使用的softmax函

数可以用下面的式（3.10）表示

。

（3.10）

exp(x)是表示e

x

的指数函数（e是纳

皮尔常数2.7182 ...）。式（3.10）表示

假设输

出层共有n个神经元，计算

第k个神经元的输出yk。如式

（3.10）所示，

softmax函数的分子是输入

信号ak的指数函数，分母是

所有输入信号的指数

函

数的和。

3.5

输出层的设计  65

用

图表示softmax函数的话，如图3-22所

示。图3-22中，softmax函数

的输出通过

箭头与所有的输入信号

相连。这是因为，从式（3.10）可以

看出，

输出层的各个神经

元都受到所有输入信号

的影响。

a1 y1

a2 y2

a3 y3

σ()

图3-22 softmax函数

现在我们

来实现softmax函数。在这个过程

中，我们将使用Python解释

器逐

一确认结果。

>>>

a = np.array([0.3, 2.9, 4.0])

>>>

>>> exp_a = np.exp(a) # 指数函数

>>> print(exp_a)

[ 1.34985881 18.17414537 54.59815003]

>>>

>>> sum_exp_a = np.sum(exp_a) #

指

数函数的和

>>> print(sum_exp_a)

74.1221542102

>>>

>>>

y = exp_a / sum_exp_a

>>>

print(y)

[ 0.01821127 0.24519181 0.73659691]

这个Python实现是

完全依照式（3.10）进行的，所以

不需要特别的解释。

考虑

到后面还要使用softmax函数，这

里我们把它定义成如下

的Python函数。

def softmax(a):

 exp_a =

np.exp(a)

 sum_exp_a = np.sum(exp_a)

y = exp_a / sum_exp_a

return y

66   第

3章　神经网络

3.5.2　实

现 softmax函数时的注意事项

上

面的softmax函数的实现虽然正

确描述了式（3.10），但在计算机

的运算

上有一定的缺陷

。这个缺陷就是溢出问题

。softmax函数的实现中要进行指

数函数的运算，但是此时

指数函数的值很容易变

得非常大。比如，e

10的值

会超

过20000，e

100会变成一个后面有40多

个0的超大值，e

1000的结果会返

回

一个表示无穷大的inf。如

果在这些超大值之间进

行除法运算，结果会出现

“不

确定”的情况。

计算机处

理“数”时，数值必须在 4字节

或 8字节的有限数据宽度

内。

这意味着数存在有效

位数，也就是说，可以表示

的数值范围是有

限的。因

此，会出现超大值无法表

示的问题。这个问题称为

溢出，

在进行计算机的运

算时必须（常常）注意。

softmax函数

的实现可以像式（3.11）这样进

行改进。

（3.11）

首先，式（3.11）在分子和

分母上都乘上C这个任意

的常数（因为同时对

分母

和分子乘以相同的常数

，所以计算结果不变）。然后

，把这个C移动到

指数函数

（exp）中，记为log

C。最后，把log C替换为另

一个符号C 

。

式（3.11）说明，在进行

softmax的指数函数的运算时，加

上（或者减去）

某个常数并

不会改变运算的结果。这

里的C



可以使用任何值，但

是为了防

止溢出，一般会

使用输入信号中的最大

值。我们来看一个具体的

例子。

3.5 输出层的设计

67

>>> a = np.array([1010, 1000,

990])

>>> np.exp(a) / np.sum(np.exp(a)) #

softmax函数

的运算

array([ nan, nan, nan]) #

没有被正确计算

>>>

>>> c = np.max(a) #

1010

>>> a - c

array([

0, -10, -20])

>>>

>>> np.exp(a

- c) / np.sum(np.exp(a - c))

array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])

如该例所示，通过减去输

入信号中的最大值（上例

中的c），我们发现原

本为nan（not

a number，不

确定）的地方，现在被正确

计算了。综上，我们

可以像

下面这样实现softmax函数。

def softmax(a):

c = np.max(a)

 exp_a =

np.exp(a - c) # 溢出

对策

sum_exp_a = np.sum(exp_a)

 y =

exp_a / sum_exp_a

 return y

3.5.3　softmax函数的特征

使用softmax()函

数，可以按如下方式计算

神经网络的输出。

>>> a =

np.array([0.3, 2.9, 4.0])

>>> y =

softmax(a)

>>> print(y)

[ 0.01821127 0.24519181

0.73659691]

>>> np.sum(y)

1.0

如上所

示，softmax函数的输出是0.0到1.0之间

的实数。并且，softmax

函数的输出

值的总和是1。输出总和为

1是softmax函数的一个重要性质

。正

因为有了这个性质，我

们才可以把softmax函数的输出

解释为“概率”。

比如，上面的

例子可以解释成y[0]的概率

是0.018（1.8 %），y[1]的概率

是0.245（24.5 %），y[2]的概率是0.737（73.7 %）。从

概率的结果来看，可以

说

“因为第2个元素的概率最

高，所以答案是第2个类别

”。而且，还可以回

68   第 3章

神经

网络

答“有74 %的概率是第2个

类别，有25 %的概率是第1个类

别，有1 %的概

率是第0个类别

”。也就是说，通过使用softmax函数

，我们可以用概率的（统

计

的）方法处理问题。

这里需

要注意的是，即便使用了

softmax函数，各个元素之间的大

小关

系也不会改变。这是

因为指数函数（y = exp(x)）是单调递

增函数。实际上，

上例中a的

各元素的大小关系和y的

各元素的大小关系并没

有改变。比如，a

的最大值是

第2个元素，y的最大值也仍

是第2个元素。

一般而言，神

经网络只把输出值最大

的神经元所对应的类别

作为识别结果。

并且，即便

使用softmax函数，输出值最大的

神经元的位置也不会变

。因此，

神经网络在进行分

类时，输出层的softmax函数可以

省略。在实际的问题中，

由

于指数函数的运算需要

一定的计算机运算量，因

此输出层的softmax函数

一般会

被省略。AB

求解机器学习问

题的步骤可以分为“学习

”A 和“推理”两个阶段。首

先，在

学习阶段进行模型的学

习B，然后，在推理阶段，用学

到的

模型对未知的数据

进行推理（分类）。如前所述

，推理阶段一般会省

略输

出层的 softmax函数。在输出层使

用

softmax函数是因为它和

神经

网络的学习有关系（详细

内容请参考下一章）。

3.5.4　输出

层的神经元数量

输出层

的神经元数量需要根据

待解决的问题来决定。对

于分类问题，输

出层的神

经元数量一般设定为类

别的数量。比如，对于某个

输入图像，预测

是图中的

数字0到9中的哪一个的问

题（10类别分类问题），可以像

图3-23这样，

将输出层的神经

元设定为10个。

如图3-23所示，在

这个例子中，输出层的神

经元从上往下依次对应

数字

0, 1, ...,

9。此外，图中输出层的

神经元的值用不同的灰

度表示。这个例子

A“学习”也

称为“训练”，为了强调算法

从数据中学习模型，本书

使用“学习”一词。——译者注

B 这

里的“学习”是指使用训练

数据、自动调整参数的过

程，具体请参考第4章。——译者

注

3.6

手写数字识别  69

中神经

元y2颜色最深，输出的值最

大。这表明这个神经网络

预测的是y2对应

的类别，也

就是“2”。

图3-23

输出层的神经元

对应各个数字

= “ 0 ”

=

“ 2 ”

= “ 1

”

= “ 9 ”

输入层

输

出层

某种运算

y0

y1

y2

y9

3.6 手写数字

识别

介绍完神经网络的

结构之后，现在我们来试

着解决实际问题。这里我

们

来进行手写数字图像

的分类。假设学习已经全

部结束，我们使用学习到

的参

数，先实现神经网络

的“推理处理”。这个推理处

理也称为神经网络的前

向

传播（forward

propagation）。

和求解机器学习

问题的步骤（分成学习和

推理两个阶段进行）一样

，

使用神经网络解决问题

时，也需要首先使用训练

数据（学习数据）进

行权重

参数的学习；进行推理时

，使用刚才学习到的参数

，对输入

数据进行分类。

70

第

3章　神经网络

3.6.1　MNIST数据集

这里

使用的数据集是MNIST手写数

字图像集。MNIST是机器学习领

域

最有名的数据集之一

，被应用于从简单的实验

到发表的论文研究等各

种场合。

实际上，在阅读图

像识别或机器学习的论

文时，MNIST数据集经常作为实

验用的数据出现。

MNIST数据集

是由0到9的数字图像构成

的（图3-24）。训练图像有6万张，

测

试图像有1万张，这些图像

可以用于学习和推理。MNIST数

据集的一般

使用方法是

，先用训练图像进行学习

，再用学习到的模型度量

能在多大程度

上对测试

图像进行正确的分类。

图

3-24 MNIST图像数据集的例子

MNIST的图

像数据是28像素 × 28像素的灰

度图像（1通道），各个像素

的

取值在0到255之间。每个图像

数据都相应地标有“7”“2”“1”等标

签。

本书提供了便利的Python脚

本mnist.py，该脚本支持从下载MNIST数

据

集到将这些数据转换

成NumPy数组等处理（mnist.py在dataset目录下

）。使用

mnist.py时，当前目录必须是

ch01、ch02、ch03、…、ch08目录中的一个。使

用mnist.py中的

load_mnist()函数，就可以按下述方式

轻松读入MNIST数据。

import sys,

os

sys.path.append(os.pardir) # 为了导入

父目录中的文件而进行

的设定

from dataset.mnist

import load_mnist

# 第一次调用会花

费几分钟 ……

(x_train,

t_train), (x_test, t_test) = load_mnist(flatten=True,

normalize=False)

# 输出各个数据

的形状

print(x_train.shape) # (60000, 784)

3.6 手写数字识别  71

print(t_train.shape) #

(60000,)

print(x_test.shape) # (10000, 784)

print(t_test.shape)

# (10000,)

首

先，为了导入父目录中的

文件，进行相应的设定A。然

后，导入

dataset/mnist.py中的 load_mnist函数。最后，使

用 load_mnist函数，读入

MNIST数据集。第一

次调用load_mnist函数时，因为要下

载MNIST数据集，

所以需要接入

网络。第2次及以后的调用

只需读入保存在本地的

文件（pickle

文件）即可，因此处理

所需的时间非常短。

用来

读入MNIST图像的文件在本书

提供的源代码的dataset目

录下

。并且，我们假定了这个MNIST数

据集只能从ch01、ch02、

ch03、…、ch08目录中使用

，因此，使用时需要从父目

录（dataset

目录）中导入文件，为此

需要添加sys.path.append(os.pardir)语句。

load_mnist函数以“(训

练图像 ,训练标签 )，(测试图

像，测试标签 )”的

形式返回

读入的MNIST数据。此外，还可以

像load_mnist(normalize=True,

flatten=True, one_hot_label=False) 这 样，设 置 3

个 参 数。第 1 个

参数

normalize设置是否将输入图

像正规化为0.0～1.0的值。如果将

该参数设置

为False，则输入图

像的像素会保持原来的

0～255。第2个参数flatten设置

是否展开

输入图像（变成一维数组

）。如果将该参数设置为False，则

输入图

像为1 × 28 ×

28的三维数组

；若设置为True，则输入图像会

保存为由784个

元素构成的

一维数组。第3个参数one_hot_label设置

是否将标签保存为onehot表示

（one-hot representation）。one-hot表示是仅正确解标签为

1，其余

皆为0的数组，就像[0,0,1,0,0,0,0,0,0,0]这

样。当one_hot_label为False时，

只是像7、2这样简

单保存正确解标签；当one_hot_label为

True时，标签则

保存为one-hot表示。

A 观

察本书源代码可知，上述

代码在mnist_show.py文件中。mnist_show.py文件的当

前目录是ch03，

但包含load_mnist()函数的

mnist.py文件在dataset目录下。因此，mnist_show.py文件

不能跨目

录直接导入mnist.py文

件。sys.path.append(os.pardir)语句实际上是把父目

录deep-learningfrom-scratch加入到sys.path（Python的搜索模块的

路径集）中，从而可以导入

deep-learningfrom-scratch下的任何目录（包括dataset目录

）中的任何文件。——译者注

72

第

3章　神经网络

Python有 pickle这个便利

的功能。这个功能可以将

程序运行中的对

象保存

为文件。如果加载保存过

的 pickle文件，可以立刻复原之

前

程序运行中的对象。用

于读入MNIST数据集的load_mnist()函数内

部也使用了 pickle功能（在第 2次

及以后读入时）。利用 pickle功能

，

可以高效地完成MNIST数据的

准备工作。

现在，我们试着

显示MNIST图像，同时也确认一

下数据。图像的显示

使用

PIL（Python Image Library）模块。执行下述代码后，训

练图像的第一

张就会显

示出来，如图3-25所示（源代码

在ch03/mnist_show.py中）。

import sys,

os

sys.path.append(os.pardir)

import numpy as np

from dataset.mnist import load_mnist

from PIL

import Image

def img_show(img):

 pil_img

= Image.fromarray(np.uint8(img))

 pil_img.show()

(x_train, t_train),

(x_test, t_test) = load_mnist(flatten=True,

normalize=False)

img

= x_train[0]

label = t_train[0]

print(label)

# 5

print(img.shape) # (784,)

img

= img.reshape(28, 28) # 把图像的形状变成

原来的尺寸

print(img.shape)

# (28, 28)

img_show(img)

这里需要注

意的是，flatten=True时读入的图像是

以一列（一维）NumPy

数组的形式

保存的。因此，显示图像时

，需要把它变为原来的28像

素

× 28

像素的形状。可以通过

reshape()方法的参数指定期望的

形状，更改NumPy

数组的形状。此

外，还需要把保存为NumPy数组

的图像数据转换为PIL用

的

数据对象，这个转换处理

由Image.fromarray()来完成。

3.6

手写数字识别

73

图3-25　显示MNIST图像

3.6.2　神经网络的

推理处理

下面，我们对这

个MNIST数据集实现神经网络

的推理处理。神经网络

的

输入层有784个神经元，输出

层有10个神经元。输入层的

784这个数字来

源于图像大

小的28 × 28 = 784，输出层的10这个数字

来源于10类别分类（数

字0到

9，共10类别）。此外，这个神经网

络有2个隐藏层，第1个隐藏

层有

50个神经元，第2个隐藏

层有100个神经元。这个50和100可

以设置为任何值。

下面我

们先定义get_data()、init_network()、predict()这3个函数（代码

在

ch03/neuralnet_mnist.py中）。

def get_data():

(x_train, t_train), (x_test, t_test) = \

load_mnist(normalize=True, flatten=True, one_hot_label=False)

 return x_test,

t_test

def init_network():

 with open("sample_weight.pkl",

'rb') as f:

 network =

pickle.load(f)

 return network

def predict(network,

x):

 W1, W2, W3 =

network['W1'], network['W2'], network['W3']

 b1, b2,

b3 = network['b1'], network['b2'], network['b3']

74

第 3章　神经网络

 a1

= np.dot(x, W1) + b1

z1 = sigmoid(a1)

 a2 =

np.dot(z1, W2) + b2

 z2

= sigmoid(a2)

 a3 = np.dot(z2,

W3) + b3

 y =

softmax(a3)

 return y

init_network()会读

入保存在pickle文件sample_weight.pkl中的学习

到的

权重参数A。这个文件

中以字典变量的形式保

存了权重和偏置参数。剩

余的2

个函数，和前面介绍

的代码实现基本相同，无

需再解释。现在，我们用这

3

个函数来实现神经网络

的推理处理。然后，评价它

的识别精度（accuracy），

即能在多大

程度上正确分类。

x, t =

get_data()

network = init_network()

accuracy_cnt =

0

for i in range(len(x)):

y = predict(network, x[i])

 p

= np.argmax(y) # 获取概

率最高的元素的索引

 if

p == t[i]:

 accuracy_cnt +=

1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

首

先获得MNIST数据集，生成网络

。接着，用for语句逐一取出保

存

在x中的图像数据，用predict()函

数进行分类。predict()函数以NumPy数

组

的形式输出各个标签对

应的概率。比如输出[0.1, 0.3, 0.2, ...,

0.04]的

数

组，该数组表示“0”的概率为

0.1，“1”的概率为0.3，等等。然后，我们

取出这个概率列表中的

最大值的索引（第几个元

素的概率最高），作为预测

结

果。可以用np.argmax(x)函数取出数

组中的最大值的索引，np.argmax(x)将

获取被赋给参数x的数组

中的最大值元素的索引

。最后，比较神经网络所预

测的答案和正确解标签

，将回答正确的概率作为

识别精度。

A 因为之前我们

假设学习已经完成，所以

学习到的参数被保存下

来。假设保存在sample_weight.pkl

文件中，在

推理阶段，我们直接加载

这些已经学习到的参数

。——译者注

3.6 手写数字识别  75

执

行上面的代码后，会显示

“Accuracy:0.9352”。这表示有93.52 %的数

据被正确

分类了。目前我们的目标

是运行学习到的神经网

络，所以不讨论识

别精度

本身，不过以后我们会花

精力在神经网络的结构

和学习方法上，思考

如何

进一步提高这个精度。实

际上，我们打算把精度提

高到99 %以上。

另外，在这个例

子中，我们把load_mnist函数的参数

normalize设置成了

True。将normalize设置成True后，函

数内部会进行转换，将图

像的各个像

素值除以255，使

得数据的值在0.0～1.0的范围内

。像这样把数据限定到某

个范围内的处理称为正

规化（normalization）。此外，对神经网络的

输入数据

进行某种既定

的转换称为预处理（pre-processing）。这里

，作为对输入图像的

一种

预处理，我们进行了正规

化。

预处理在神经网络（深

度学习）中非常实用，其有

效性已在提高识别

性能

和学习的效率等众多实

验中得到证明。在刚才的

例子中，作为

一种预处理

，我们将各个像素值除以

255，进行了简单的正规化。

实

际上，很多预处理都会考

虑到数据的整体分布。比

如，利用数据

整体的均值

或标准差，移动数据，使数

据整体以 0为中心分布，或

者进行正规化，把数据的

延展控制在一定范围内

。除此之外，还有

将数据整

体的分布形状均匀化的

方法，即数据白化（whitening）等。

3.6.3　批处

理

以上就是处理MNIST数据集

的神经网络的实现，现在

我们来关注输入

数据和

权重参数的“形状”。再看一

下刚才的代码实现。

下面

我们使用Python解释器，输出刚

才的神经网络的各层的

权重的形状。

>>> x, _

= get_data()

>>> network = init_network()

>>> W1, W2, W3 = network['W1'],

network['W2'], network['W3']

>>>

>>> x.shape

(10000,

784)

>>> x[0].shape

(784,)

>>> W1.shape

76   第 3章　神经网

络

(784, 50)

>>> W2.shape

(50, 100)

>>> W3.shape

(100, 10)

我们通过上述结果来

确认一下多维数组的对

应维度的元素个数是否

一致

（省略了偏置）。用图表

示的话，如图3-26所示。可以发

现，多维数组的对应

维度

的元素个数确实是一致

的。此外，我们还可以确认

最终的结果是输出了

元

素个数为10 的一维数组。

图

3-26　数组形状的变化

X

W1 → Y

形状： 784 784

× 50 10

一

致

W2 W3

50 × 100 100 × 10

一致 一致

从整体的处

理流程来看，图3-26中，输入一

个由784个元素（原本是一

个

28 × 28的二维数组）构成的一维

数组后，输出一个有10个元

素的一维数组。

这是只输

入一张图像数据时的处

理流程。

现在我们来考虑

打包输入多张图像的情

形。比如，我们想用predict()

函数一

次性打包处理100张图像。为

此，可以把x的形状改为100 × 784，将

100张图像打包作为输入数

据。用图表示的话，如图3-27所

示。

图3-27

批处理中数组形状

的变化

100 × 10

X W1

→ Y

形状：100 × 784 784

× 50

W2 W3

50 ×

100 100 × 10

如图 3-27

所示，输

入数据的形状为 100 × 784，输出数

据的形状为

100 ×

10。这表示输入

的100张图像的结果被一次

性输出了。比如，x[0]和

3.6 手写数

字识别  77

y[0]中保存了第0张图

像及其推理结果，x[1]和y[1]中保

存了第1张图像及

其推理

结果，等等。

这种打包式的

输入数据称为批（batch）。批有“捆

”的意思，图像就如同

纸币

一样扎成一捆。

批处理对

计算机的运算大有利处

，可以大幅缩短每张图像

的处理时

间。那么为什么

批处理可以缩短处理时

间呢？这是因为大多数处

理

数值计算的库都进行

了能够高效处理大型数

组运算的最优化。并且，

在

神经网络的运算中，当数

据传送成为瓶颈时，批处

理可以减轻数

据总线的

负荷（严格地讲，相对于数

据读入，可以将更多的时

间用在

计算上）。也就是说

，批处理一次性计算大型

数组要比分开逐步计算

各个小型数组速度更快

。

下面我们进行基于批处

理的代码实现。这里用粗

体显示与之前的实现的

不同之处。

x, t

= get_data()

network = init_network()

batch_size

= 100 # 批数量

accuracy_cnt =

0

for i in range(0, len(x),

batch_size):

 x_batch = x[i:i+batch_size]

y_batch = predict(network, x_batch)

 p

= np.argmax(y_batch, axis=1)

 accuracy_cnt +=

np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt)

/ len(x)))

我们来

逐个解释粗体的代码部

分。首先是range()函数。range()函数若

指

定为range(start, end)，则会生成一个由start到

end-1之间的整数构成的

列表

。若像range(start,

end, step)这样指定3个整数，则

生成的列表中的

下一个

元素会增加step指定的值。我

们来看一个例子。

>>> list( range(0,

10) )

[0, 1, 2, 3,

4, 5, 6, 7, 8, 9]

78   第 3章　神

经网络

>>> list( range(0, 10, 3) )

[0, 3, 6, 9]

在range()函数生成的列

表的基础上，通过x[i:i+batch_size]从输入

数

据中抽出批数据。x[i:i+batch_n]会取

出从第i个到第i+batch_n个之间的

数据。

本例中是像x[0:100]、x[100:200]……这样，从

头开始以100为单位将数据

提

取为批数据。

然后，通过

argmax()获取值最大的元素的索

引。不过这里需要注意的

是，

我们给定了参数axis=1。这指

定了在100 × 10的数组中，沿着第

1维方向（以

第1维为轴）找到

值最大的元素的索引（第

0维对应第1个维度）A。这里也

来

看一个例子。

>>> x = np.array([[0.1,

0.8, 0.1], [0.3, 0.1, 0.6],

...

[0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])

>>> y = np.argmax(x, axis=1)

>>>

print(y)

[1 2 1 0]

最后，我们

比较一下以批为单位进

行分类的结果和实际的

答案。为此，

需要在NumPy数组之

间使用比较运算符（==）生成

由True/False构成的布尔

型数组，并

计算True的个数。我们通过下

面的例子进行确认。

>>> y = np.array([1,

2, 1, 0])

>>> t =

np.array([1, 2, 0, 0])

>>> print(y==t)

[True True False True]

>>> np.sum(y==t)

3

至此

，基于批处理的代码实现

就介绍完了。使用批处理

，可以实现高速

且高效的

运算。下一章介绍神经网

络的学习时，我们将把图

像数据作为打包

的批数

据进行学习，届时也将进

行和这里的批处理一样

的代码实现。

A 矩阵的第0维

是列方向，第1维是行方向

。——译者注

3.7 小结  79

3.7 小结

本章介

绍了神经网络的前向传

播。本章介绍的神经网络

和上一章的感知

机在信

号的按层传递这一点上

是相同的，但是，向下一个

神经元发送信号时，

改变

信号的激活函数有很大

差异。神经网络中使用的

是平滑变化的sigmoid

函数，而感

知机中使用的是信号急

剧变化的阶跃函数。这个

差异对于神经网

络的学

习非常重要，我们将在下

一章介绍。

本章所学的内

容

• 神经网络中的激活函

数使用平滑变化的sigmoid函数

或ReLU函数。

• 通过巧妙地使用

NumPy多维数组，可以高效地实

现神经网络。

• 机器学习的

问题大体上可以分为回

归问题和分类问题。

• 关于

输出层的激活函数，回归

问题中一般用恒等函数

，分类问题中

一般用softmax函数

。

• 分类问题中，输出层的神

经元的数量设置为要分

类的类别数。

•

输入数据的

集合称为批。通过以批为

单位进行推理处理，能够

实现

高速的运算。



第4章

神

经网络的学习

本章的主

题是神经网络的学习。这

里所说的“学习”是指从训

练数据中

自动获取最优

权重参数的过程。本章中

，为了使神经网络能进行

学习，将导

入损失函数这

一指标。而学习的目的就

是以该损失函数为基准

，找出能使它

的值达到最

小的权重参数。为了找出

尽可能小的损失函数的

值，本章我们将

介绍利用

了函数斜率的梯度法。

4.1 从

数据中学习

神经网络的

特征就是可以从数据中

学习。所谓“从数据中学习

”，是指

可以由数据自动决

定权重参数的值。这是非

常了不起的事情！因为如

果所有

的参数都需要人

工决定的话，工作量就太

大了。在第2章介绍的感知

机的例

子中，我们对照着

真值表，人工设定了参数

的值，但是那时的参数只

有3个。

而在实际的神经网

络中，参数的数量成千上

万，在层数更深的深度学

习中，

参数的数量甚至可

以上亿，想要人工决定这

些参数的值是不可能的

。本章将

介绍神经网络的

学习，即利用数据决定参

数值的方法，并用Python实现对

MNIST手写数字数据集的学习

。

对于线性可分问题，第 2章

的感知机是可以利用数

据自动学习的。

根据“感知

机收敛定理”，通过有限次

数的学习，线性可分问题

是可

解的。但是，非线性可

分问题则无法通过（自动

）学习来解决。

82

第 4章　神经网

络的学习

4.1.1　数据驱动

数据

是机器学习的命根子。从

数据中寻找答案、从数据

中发现模式、根

据数据讲

故事……这些机器学习所做

的事情，如果没有数据的

话，就无从谈

起。因此，数据

是机器学习的核心。这种

数据驱动的方法，也可以

说脱离了

过往以人为中

心的方法。

通常要解决某

个问题，特别是需要发现

某种模式时，人们一般会

综合考

虑各种因素后再

给出回答。“这个问题好像

有这样的规律性？”“不对，可

能

原因在别的地方。”——类似

这样，人们以自己的经验

和直觉为线索，通过反

复

试验推进工作。而机器学

习的方法则极力避免人

为介入，尝试从收集到的

数据中发现答案（模式）。神

经网络或深度学习则比

以往的机器学习方法更

能

避免人为介入。

现在我

们来思考一个具体的问

题，比如如何实现数字“5”的

识别。数字

5是图4-1所示的手

写图像，我们的目标是实

现能区别是否是5的程序

。这个

问题看起来很简单

，大家能想到什么样的算

法呢？

图4-1　手写数字5的例子

：写法因人而异，五花八门

如果让我们自己来设计

一个能将5正确分类的程

序，就会意外地发现这

是

一个很难的问题。人可以

简单地识别出5，但却很难

明确说出是基于何种

规

律而识别出了5。此外，从图

4-1中也可以看到，每个人都

有不同的写字习惯，

要发

现其中的规律是一件非

常难的工作。

因此，与其绞

尽脑汁，从零开始想出一

个可以识别5的算法，不如

考虑

通过有效利用数据

来解决这个问题。一种方

案是，先从图像中提取特

征量，

4.1  从数据中学习  83

再用

机器学习技术学习这些

特征量的模式。这里所说

的“特征量”是指可以

从输

入数据（输入图像）中准确

地提取本质数据（重要的

数据）的转换器。图

像的特

征量通常表示为向量的

形式。在计算机视觉领域

，常用的特征量包括

SIFT、SURF和HOG等

。使用这些特征量将图像

数据转换为向量，然后对

转换后的向量使用机器

学习中的SVM、KNN等分类器进行

学习。

机器学习的方法中

，由机器从收集到的数据

中找出规律性。与从零开

始

想出算法相比，这种方

法可以更高效地解决问

题，也能减轻人的负担。但

是

需要注意的是，将图像

转换为向量时使用的特

征量仍是由人设计的。对

于不

同的问题，必须使用

合适的特征量（必须设计

专门的特征量），才能得到

好的

结果。比如，为了区分

狗的脸部，人们需要考虑

与用于识别5的特征量不

同

的其他特征量。也就是

说，即使使用特征量和机

器学习的方法，也需要针

对

不同的问题人工考虑

合适的特征量。

到这里，我

们介绍了两种针对机器

学习任务的方法。将这两

种方法用图

来表示，如图

4-2所示。图中还展示了神经

网络（深度学习）的方法，可

以看

出该方法不存在人

为介入。

如图4-2所示，神经网

络直接学习图像本身。在

第2个方法，即利用特

征量

和机器学习的方法中，特

征量仍是由人工设计的

，而在神经网络中，连

图像

中包含的重要特征量也

都是由机器来学习的。

图

4-2

从人工设计规则转变为

由机器从数据中学习：没

有人为介入的方块用灰

色表示

人想到的算法 答

案

答案

答案

人想到的特

征量

（SIFT、HOG等）

神经网络

（深度学

习）

机器学习

（SVM、KNN等）

84

第 4章　神经

网络的学习

深 度

学 习 有

时 也 称 为

端 到 端 机 器 学

习（end-to-end

machine

learning）。这里所说的端到端是

指从一端到另一端的意

思，也就是

从原始数据（输

入）中获得目标结果（输出

）的意思。

神经网络的优点

是对所有的问题都可以

用同样的流程来解决。比

如，不

管要求解的问题是

识别5，还是识别狗，抑或是

识别人脸，神经网络都是

通

过不断地学习所提供

的数据，尝试发现待求解

的问题的模式。也就是说

，与

待处理的问题无关，神

经网络可以将数据直接

作为原始数据，进行“端对

端”

的学习。

4.1.2　训练数据和测

试数据

本章主要介绍神

经网络的学习，不过在这

之前，我们先来介绍一下

机器

学习中有关数据处

理的一些注意事项。

机器

学习中，一般将数据分为

训练数据和测试数据两

部分来进行学习和

实验

等。首先，使用训练数据进

行学习，寻找最优的参数

；然后，使用测试

数据评价

训练得到的模型的实际

能力。为什么需要将数据

分为训练数据和测

试数

据呢？因为我们追求的是

模型的泛化能力。为了正

确评价模型的泛化能

力

，就必须划分训练数据和

测试数据。另外，训练数据

也可以称为监督数据。

泛

化能力是指处理未被观

察过的数据（不包含在训

练数据中的数据）的

能力

。获得泛化能力是机器学

习的最终目标。比如，在识

别手写数字的问题

中，泛

化能力可能会被用在自

动读取明信片的邮政编

码的系统上。此时，手

写数

字识别就必须具备较高

的识别“某个人”写的字的

能力。注意这里不是“特

定

的某个人写的特定的文

字”，而是“任意一个人写的

任意文字”。如果系统

只能

正确识别已有的训练数

据，那有可能是只学习到

了训练数据中的个人的

习惯写法。

因此，仅仅用一

个数据集去学习和评价

参数，是无法进行正确评

价的。

这样会导致可以顺

利地处理某个数据集，但

无法处理其他数据集的

情况。顺

便说一下，只对某

个数据集过度拟合的状

态称为过拟合（over fitting）。避免

过拟

合也是机器学习的一个

重要课题。

4.2 损失函数

85

4.2 损失

函数

如果有人问你现在

有多幸福，你会如何回答

呢？一般的人可能会给出

诸

如“还可以吧”或者“不是

那么幸福”等笼统的回答

。如果有人回答“我现在

的

幸福指数是10.23”的话，可能会

把人吓一跳吧。因为他用

一个数值指标来

评判自

己的幸福程度。

这里的幸

福指数只是打个比方，实

际上神经网络的学习也

在做同样的事

情。神经网

络的学习通过某个指标

表示现在的状态。然后，以

这个指标为基

准，寻找最

优权重参数。和刚刚那位

以幸福指数为指引寻找

“最优人生”的

人一样，神经

网络以某个指标为线索

寻找最优权重参数。神经

网络的学习中

所用的指

标称为损失函数（loss

function）。这个损

失函数可以使用任意函

数，

但一般用均方误差和

交叉熵误差等。

损失函数

是表示神经网络性能的

“恶劣程度”的指标，即当前

的

神经网络对监督数据

在多大程度上不拟合，在

多大程度上不一致。

以“性

能的恶劣程度”为指标可

能会使人感到不太自然

，但是如

果给损失函数乘

上一个负值，就可以解释

为“在多大程度上不坏”，

即

“性能有多好”。并且，“使性能

的恶劣程度达到最小”和

“使性

能的优良程度达到

最大”是等价的，不管是用

“恶劣程度”还是“优

良程度

”，做的事情本质上都是一

样的。

4.2.1　均方误差

可以用作

损失函数的函数有很多

，其中最有名的是均方误

差（mean

squared

error）。均方误差如下式所示

。

（4.1）

这里，yk是表示神经网络的

输出，tk表示监督数据，k表示

数据的维数。

86

第 4章　神经网

络的学习

比如，在3.6节手写

数字识别的例子中，yk、tk是由

如下10个元素构成的数据

。

>>> y

= [0.1, 0.05, 0.6, 0.0, 0.05,

0.1, 0.0, 0.1, 0.0, 0.0]

>>>

t = [0, 0, 1, 0,

0, 0, 0, 0, 0, 0]

数组元素的索引从第一

个开始依次对应数字“0”“1”“2”…… 这

里，神

经网络的输出y是softmax函

数的输出。由于softmax函数的输

出可以理解为

概率，因此

上例表示“0”的概率是0.1，“1”的概

率是0.05，“2”的概率是0.6

等。t是监督

数据，将正确解标签设为

1，其他均设为0。这里，标签“2”为

1，

表示正确解是“2”。将正确解

标签表示为1，其他标签表

示为0的表示方法称

为one-hot表

示。

如式（4.1）所示，均方误差会

计算神经网络的输出和

正确解监督数据的

各个

元素之差的平方，再求总

和。现在，我们用Python来实现这

个均方误差，

实现方式如

下所示。

def mean_squared_error(y,

t):

 return 0.5 * np.sum((y-t)**2)

这里，参数y和t是NumPy数

组。代码实现完全遵照式

（4.1），因此不

再具体说明。现在

，我们使用这个函数，来实

际地计算一下。

>>> # 设“2”为正确

解

>>>

t = [0, 0, 1, 0,

0, 0, 0, 0, 0, 0]

>>>

>>> # 例1：“2”的概率最高的情况

（0.6）

>>> y

= [0.1, 0.05, 0.6, 0.0, 0.05,

0.1, 0.0, 0.1, 0.0, 0.0]

>>>

mean_squared_error(np.array(y), np.array(t))

0.097500000000000031

>>>

>>> #

例2：“7”的概率最高的情况（0.6）

>>> y = [0.1, 0.05,

0.1, 0.0, 0.05, 0.1, 0.0, 0.6,

0.0, 0.0]

>>> mean_squared_error(np.array(y), np.array(t))

0.59750000000000003

这

里举了两个例子。第一个

例子中，正确解是“2”，神经网

络的输出的最大

值是“2”；第

二个例子中，正确解是“2”，神

经网络的输出的最大值

是“7”。如

实验结果所示，我们

发现第一个例子的损失

函数的值更小，和监督数

据之间的

误差较小。也就

是说，均方误差显示第一

个例子的输出结果与监

督数据更加吻合。

4.2 损失函

数

87

4.2.2　交叉熵误差

除了均方

误差之外，交叉熵误差（cross entropy error）也

经常被用作损

失函数。交

叉熵误差如下式所示。

（4.2）

这

里，log表示以e为底数的自然

对数（log e

）。yk是神经网络的输出

，tk是

正确解标签。并且，tk中只

有正确解标签的索引为

1，其他均为0（one-hot表示）。

因此，式（4.2）实

际上只计算对应正确解

标签的输出的自然对数

。比如，假设

正确解标签的

索引是“2”，与之对应的神经

网络的输出是0.6，则交叉熵

误差

是−log 0.6 = 0.51；若“2”对应的输出是

0.1，则交叉熵误差为−log

0.1 = 2.30。

也就是

说，交叉熵误差的值是由

正确解标签所对应的输

出结果决定的。

自然对数

的图像如图4-3所示。

图4-3

自然

对数y = log x的图像

88

第 4章　神经网

络的学习

如图4-3所示，x等于

1时，y为0；随着x向0靠近，y逐渐变

小。因此，

正确解标签对应

的输出越大，式（4.2）的值越接

近0；当输出为1时，交叉熵

误

差为0。此外，如果正确解标

签对应的输出较小，则式

（4.2）的值较大。

下面，我们来用

代码实现交叉熵误差。

def cross_entropy_error(y, t):

 delta

= 1e-7

 return -np.sum(t *

np.log(y + delta))

这

里，参数y和t是NumPy数组。函数内

部在计算np.log时，加上了一

个

微小值delta。这是因为，当出现

np.log(0)时，np.log(0)会变为负无限大

的-inf，这

样一来就会导致后续计

算无法进行。作为保护性

对策，添加一个

微小值可

以防止负无限大的发生

。下面，我们使用cross_entropy_error(y, t)

进行一些

简单的计算。

>>> t =

[0, 0, 1, 0, 0, 0,

0, 0, 0, 0]

>>> y

= [0.1, 0.05, 0.6, 0.0, 0.05,

0.1, 0.0, 0.1, 0.0, 0.0]

>>>

cross_entropy_error(np.array(y), np.array(t))

0.51082545709933802

>>>

>>> y

= [0.1, 0.05, 0.1, 0.0, 0.05,

0.1, 0.0, 0.6, 0.0, 0.0]

>>>

cross_entropy_error(np.array(y), np.array(t))

2.3025840929945458

第一个例子

中，正确解标签对应的输

出为0.6，此时的交叉熵误差

大约

为0.51。第二个例子中，正

确解标签对应的输出为

0.1的低值，此时的交叉

熵误

差大约为2.3。由此可以看出

，这些结果与我们前面讨

论的内容是一致的。

4.2.3　mini-batch学习

机器学习使用训练数据

进行学习。使用训练数据

进行学习，严格来说，

就是

针对训练数据计算损失

函数的值，找出使该值尽

可能小的参数。因此，

计算

损失函数时必须将所有

的训练数据作为对象。也

就是说，如果训练数据

有

100个的话，我们就要把这100个

损失函数的总和作为学

习的指标。

前面介绍的损

失函数的例子中考虑的

都是针对单个数据的损

失函数。如

4.2 损失函数  89

果要

求所有训练数据的损失

函数的总和，以交叉熵误

差为例，可以写成下面

的

式（4.3）。

（4.3）

这里,假设数据有N个，tnk表

示第n个数据的第k个元素

的值（ynk是神

经网络的输出

，tnk是监督数据）。式子虽然看

起来有一些复杂，其实只

是把

求单个数据的损失

函数的式（4.2）扩大到了N份数

据，不过最后还要除以N

进

行正规化。通过除以N，可以

求单个数据的“平均损失

函数”。通过这样的

平均化

，可以获得和训练数据的

数量无关的统一指标。比

如，即便训练数据

有1000个或

10000个，也可以求得单个数据

的平均损失函数。

另外，MNIST数

据集的训练数据有60000个，如

果以全部数据为对象

求

损失函数的和，则计算过

程需要花费较长的时间

。再者，如果遇到大数据，

数

据量会有几百万、几千万

之多，这种情况下以全部

数据为对象计算损失函

数是不现实的。因此，我们

从全部数据中选出一部

分，作为全部数据的“近

似

”。神经网络的学习也是从

训练数据中选出一批数

据（称为mini-batch,小

批量），然后对每

个mini-batch进行学习。比如，从60000个训

练数据中随机

选择100笔，再

用这100笔数据进行学习。这

种学习方式称为mini-batch学习。

下

面我们来编写从训练数

据中随机选择指定个数

的数据的代码，以进行

mini-batch学

习。在这之前，先来看一下

用于读入MNIST数据集的代码

。

import sys, os

sys.path.append(os.pardir)

import numpy as np

from

dataset.mnist import load_mnist

(x_train, t_train), (x_test,

t_test) = \

 load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)

print(t_train.shape) #

(60000, 10)

第3章介绍过，load_mnist函数是用于

读入MNIST数据集的函数。这个

函数在本书提供的脚本

dataset/mnist.py中，它会读入训练数据和

测试数据。

90

第 4章　神经网络

的学习

读入数据时，通过

设定参数one_hot_label=True，可以得到one-hot表示

（即

仅正确解标签为1，其余

为0的数据结构）。

读入上面

的MNIST数据后，训练数据有60000个

，输入数据是784维

（28 × 28）的图像数

据，监督数据是10维的数据

。因此，上面的x_train、t_

train的形状分别

是(60000, 784)和(60000, 10)。

那么，如何从这个训

练数据中随机抽取10笔数

据呢？我们可以使用

NumPy的np.random.choice()，写

成如下形式。

train_size = x_train.shape[0]

batch_size

= 10

batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

使用np.random.choice()可以从

指定的数字中随机选择

想要的数字。比如，

np.random.choice(60000, 10)会从0到

59999之间随机选择10个数字。如

下

面的实际代码所示，我

们可以得到一个包含被

选数据的索引的数组。

>>> np.random.choice(60000,

10)

array([ 8013, 14666, 58210, 23832,

52091, 10153, 8107, 19410, 27260,

21411])

之

后，我们只需指定这些随

机选出的索引，取出mini-batch，然后

使用

这个mini-batch计算损失函数

即可。

计算电视收视率时

，并不会统计所有家庭的

电视机，而是仅以那些

被

选中的家庭为统计对象

。比如，通过从关东地区随

机选择 1000个

家庭计算收视

率，可以近似地求得关东

地区整体的收视率。这

1000

个

家庭的收视率，虽然严格

上不等于整体的收视率

，但可以作为整

体的一个

近似值。和收视率一样，mini-batch的

损失函数也是利用

一部

分样本数据来近似地计

算整体。也就是说，用随机

选择的小批

量数据（mini-batch）作为

全体训练数据的近似值

。

4.2

损失函数  91

4.2.4　mini-batch版交叉熵误差

的实现

如何实现对应mini-batch的

交叉熵误差呢？只要改良

一下之前实现的对

应单

个数据的交叉熵误差就

可以了。这里，我们来实现

一个可以同时处理单

个

数据和批量数据（数据作

为batch集中输入）两种情况的

函数。

def cross_entropy_error(y, t):

if y.ndim == 1:

 t

= t.reshape(1, t.size)

 y =

y.reshape(1, y.size)

 batch_size = y.shape[0]

return -np.sum(t * np.log(y + 1e-7))

/ batch_size

这里，y是神经网络的

输出，t是监督数据。y的维度

为1时，即求单个

数据的交

叉熵误差时，需要改变数

据的形状。并且，当输入为

mini-batch时，

要用batch的个数进行正规

化，计算单个数据的平均

交叉熵误差。

此外，当监督

数据是标签形式（非one-hot表示

，而是像“2”“7”这样的

标签）时，交

叉熵误差可通过如下代

码实现。

def cross_entropy_error(y, t):

 if

y.ndim == 1:

 t =

t.reshape(1, t.size)

 y = y.reshape(1,

y.size)

 batch_size = y.shape[0]

return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) /

batch_size

实现的要点是，由

于one-hot表示中t为0的元素的交

叉熵误差也为0，因

此针对

这些元素的计算可以忽

略。换言之，如果可以获得

神经网络在正确

解标签

处的输出，就可以计算交

叉熵误差。因此，t为one-hot表示时

通过

t *

np.log(y)计算的地方，在t为标

签形式时，可用np.log( y[np.arange

(batch_size), t] )实现相同

的处理（为了便于观察，这

里省略了微小值1e-7）。

作为参

考，简单介绍一下np.log(

y[np.arange(batch_size), t] )。np.arange

(batch_size)会生成

一个从0到batch_size-1的数组。比如当

batch_size为5

时，np.arange(batch_size)会生成一个NumPy 数组[0,

1, 2, 3, 4]。因

为

92

第 4章　神经网络的学习

t中标签是以[2, 7, 0, 9,

4]的形式存储

的，所以y[np.arange(batch_size),

t]能抽出各个数据

的正确解标签对应的神

经网络的输出（在这个例

子中，

y[np.arange(batch_size), t] 会生成 NumPy

数 组 [y[0,2], y[1,7], y[2,0],

y[3,9],

y[4,4]]）。

4.2.5　为何要

设定损失函数

上面我们

讨论了损失函数，可能有

人要问：“为什么要导入损

失函数呢？”

以数字识别任

务为例，我们想获得的是

能提高识别精度的参数

，特意再导入

一个损失函

数不是有些重复劳动吗

？也就是说，既然我们的目

标是获得使识

别精度尽

可能高的神经网络，那不

是应该把识别精度作为

指标吗？

对于这一疑问，我

们可以根据“导数”在神经

网络学习中的作用来回

答。

下一节中会详细说到

，在神经网络的学习中，寻

找最优参数（权重和偏置

）时，

要寻找使损失函数的

值尽可能小的参数。为了

找到使损失函数的值尽

可能小

的地方，需要计算

参数的导数（确切地讲是

梯度），然后以这个导数为

指引，

逐步更新参数的值

。

假设有一个神经网络，现

在我们来关注这个神经

网络中的某一个权重参

数。此时，对该权重参数的

损失函数求导，表示的是

“如果稍微改变这个权

重

参数的值，损失函数的值

会如何变化”。如果导数的

值为负，通过使该权

重参

数向正方向改变，可以减

小损失函数的值；反过来

，如果导数的值为正，

则通

过使该权重参数向负方

向改变，可以减小损失函

数的值。不过，当导数

的值

为0时，无论权重参数向哪

个方向变化，损失函数的

值都不会改变，此

时该权

重参数的更新会停在此

处。

之所以不能用识别精

度作为指标，是因为这样

一来绝大多数地方的导

数

都会变为0，导致参数无

法更新。话说得有点多了

，我们来总结一下上面的

内容。

在进行神经网络的

学习时，不能将识别精度

作为指标。因为如果以

识

别精度为指标，则参数的

导数在绝大多数地方都

会变为0。

为什么用识别精

度作为指标时，参数的导

数在绝大多数地方都会

变成0

4.2

损失函数  93

呢？为了回

答这个问题，我们来思考

另一个具体例子。假设某

个神经网络正

确识别出

了100笔训练数据中的32笔，此

时识别精度为32 %。如果以识

别精

度为指标，即使稍微

改变权重参数的值，识别

精度也仍将保持在32 %，不会

出现变化。也就是说，仅仅

微调参数，是无法改善识

别精度的。即便识别精

度

有所改善，它的值也不会

像32.0123 ... %这样连续变化，而是变

为33 %、

34 %这样的不连续的、离散

的值。而如果把损失函数

作为指标，则当前损

失函

数的值可以表示为0.92543 ... 这样

的值。并且，如果稍微改变

一下参数

的值，对应的损

失函数也会像0.93432

... 这样发生

连续性的变化。

识别精度

对微小的参数变化基本

上没有什么反应，即便有

反应，它的值

也是不连续

地、突然地变化。作为激活

函数的阶跃函数也有同

样的情况。出

于相同的原

因，如果使用阶跃函数作

为激活函数，神经网络的

学习将无法进行。

如图4-4所

示，阶跃函数的导数在绝

大多数地方（除了0以外的

地方）均为0。

也就是说，如果

使用了阶跃函数，那么即

便将损失函数作为指标

，参数的微

小变化也会被

阶跃函数抹杀，导致损失

函数的值不会产生任何

变化。

阶跃函数就像“竹筒

敲石”一样，只在某个瞬间

产生变化。而sigmoid函数，

如图4-4所

示，不仅函数的输出（竖轴

的值）是连续变化的，曲线

的斜率（导数）

也是连续变

化的。也就是说，sigmoid函数的导

数在任何地方都不为0。这

对

神经网络的学习非常

重要。得益于这个斜率不

会为0的性质，神经网络的

学

习得以正确进行。

阶跃

函数 sigmoid函数

图4-4 阶跃函数和

sigmoid函数：阶跃函数的斜率在

绝大多数地方都为0，而sigmoid函

数的斜率（切线）不会为0

94

第

4章　神经网络的学习

4.3 数值

微分

梯度法使用梯度的

信息决定前进的方向。本

节将介绍梯度是什么、有

什

么性质等内容。在这之

前，我们先来介绍一下导

数。

4.3.1 导数

假如你是全程马

拉松选手，在开始的10分钟

内跑了2千米。如果要计算

此时的奔跑速度，则为2/10 = 0.2［千

米/分］。也就是说，你以1分钟

前进0.2

千米的速度（变化）奔

跑。

在这个马拉松的例子

中，我们计算了“奔跑的距

离”相对于“时间”发生

了多

大变化。不过，这个10分钟跑

2千米的计算方式，严格地

讲，计算的是

10分钟内的平

均速度。而导数表示的是

某个瞬间的变化量。因此

，将10分

钟这一时间段尽可

能地缩短，比如计算前1分

钟奔跑的距离、前1秒钟奔

跑

的距离、前0.1秒钟奔跑的

距离……这样就可以获得某

个瞬间的变化量（某个

瞬

时速度）。

综上，导数就是表

示某个瞬间的变化量。它

可以定义成下面的式子

。

（4.4）

式（4.4）表示的是函数的导数

。左边的符号 表示f（x）关于x的

导

数，即f（x）相对于x的变化程

度。式（4.4）表示的导数的含义

是，x的“微小

变化”将导致函

数f（x）的值在多大程度上发

生变化。其中，表示微小变

化的

h无限趋近0，表示为 。

接

下来，我们参考式（4.4），来实现

求函数的导数的程序。如

果直接实

现式（4.4）的话，向h中

赋入一个微小值，就可以

计算出来了。比如，下面

的

实现如何？

# 不好的实现示

例

def numerical_diff(f, x):

4.3

数值微分  95

h = 10e-50

return (f(x+h) - f(x)) / h

函数numerical_diff(f, x)的名称

来源于数值微分A 的英文

numerical

differentiation。这个函数有两个参数，即

“函数f”和“传给函数f的参数

x”。

乍一看这个实现没有问

题，但是实际上这段代码

有两处需要改进的地方

。

在上面的实现中，因为想

把尽可能小的值赋给h（可

以话，想让h无限接

近0），所以

h使用了10e-50（有50个连续的0的“0.00 ... 1”）这

个微小值。但

是，这样反而

产生了舍入误差（rounding error）。所谓舍

入误差，是指因省

略小数

的精细部分的数值（比如

，小数点第8位以后的数值

）而造成最终的计

算结果

上的误差。比如，在Python中，舍入

误差可如下表示。

>>> np.float32(1e-50)

0.0

如上所

示，如果用float32类型（32位的浮点

数）来表示1e-50，就会变成

0.0，无法

正确表示出来。也就是说

，使用过小的值会造成计

算机出现计算

上的问题

。这是第一个需要改进的

地方，即将微小值h改为10−4

。使

用10−4

就可以得到正确的结

果。

第二个需要改进的地

方与函数f的差分有关。虽

然上述实现中计算了函

数f在x+h和x之间的差分，但是

必须注意到，这个计算从

一开始就有误差。

如图4-5所

示，“真的导数”对应函数在

x处的斜率（称为切线），但上

述实现

中计算的导数对

应的是(x

+ h)和x之间的斜率。因

此，真的导数（真的切线）

和

上述实现中得到的导数

的值在严格意义上并不

一致。这个差异的出现是

因

为h不可能无限接近0。

如

图4-5所示，数值微分含有误

差。为了减小这个误差，我

们可以计算

函数f在(x

+ h)和(x − h)之

间的差分。因为这种计算

方法以x为中心，计

算它左

右两边的差分，所以也称

为中心差分（而(x +

h)和x之间的

差分称为

前向差分）。下面

，我们基于上述两个要改

进的点来实现数值微分

（数值梯度）。

A 所谓数值微分

就是用数值方法近似求

解函数的导数的过程。——译

者注

96

第 4章　神经网络的学

习

图4-5　真的导数（真的切线

）和数值微分（近似切线）的

值不同

真的切线

近似切

线

y = f(x)

x x

+ h

def numerical_diff(f, x):

h = 1e-4 # 0.0001

return (f(x+h) - f(x-h)) / (2*h)

如上所示，利用微小的

差分求导数的过程称为

数值微分（numerical 

differentiation）。而基于数学式

的推导求导数的过程，则

用“解析

性”（analytic）一词，称为“解析

性求解”或者“解析性求导

”。比如，

y =

x2

的导数，可以通过 解

析性地求解出来。因此，当

x = 2时，

y的导数为4。解析性求导

得到的导数是不含误差

的“真的导数”。

4.3.2　数值微分的

例子

现在我们试着用上

述的数值微分对简单函

数进行求导。先来看一个

由下

式表示的2次函数。

y =

0.01x2 + 0.1x （4.5）

4.3 数

值微分

97

用Python来实现式（4.5），如下

所示。

def function_1(x):

 return

0.01*x**2 + 0.1*x

接下来，我们来绘制

这个函数的图像。画图所

用的代码如下，生成的图

像如图4-6所示。

import numpy

as np

import matplotlib.pylab as plt

x = np.arange(0.0, 20.0, 0.1) #

以0.1为单位，从

0到20的数组x

y = function_1(x)

plt.xlabel("x")

plt.ylabel("f(x)")

plt.plot(x, y)

plt.show()

图4-6 f(x) =

0.01x2 + 0.1x的图像

我们

来计算一下这个函数在

x = 5和x

= 10处的导数。

98   第

4章　神经网

络的学习

>>> numerical_diff(function_1, 5)

0.1999999999990898

>>> numerical_diff(function_1, 10)

0.2999999999986347

这里计算的导

数是f(x)相对于x的变化量，对

应函数的斜率。另外，

f(x)

= 0.01x2 + 0.1x 的解

析解是 。因

此，在 x = 5 和

x

= 10处，“真的

导数”分别为0.2和0.3。和上面的

结果相比，我们发现虽然

严格意义上它们并不一

致，但误差非常小。实际上

，误差小到基本上可以认

为它们是相等的。

现在，我

们用上面的数值微分的

值作为斜率，画一条直线

。结果如图4-7

所示，可以确认

这些直线确实对应函数

的切线（源代码在ch04/gradient_1d.

py中）。

图4-7

x = 5、x = 10处

的切线：直线的斜率使用

数值微分的值

f

(x )

f (x )

4.3.3

偏导数

接

下来，我们看一下式(4.6)表示

的函数。虽然它只是一个

计算参数的

平方和的简

单函数，但是请注意和上

例不同的是，这里有两个

变量。

（4.6）

这个式子可以用Python来

实现，如下所示。

def

function_2(x):

4.3  数值微分

99

return x[0]**2 + x[1]**2

 #

或者return np.sum(x**2)

这里，我们假定向参

数输入了一个NumPy数组。函数

的内部实现比较

简单，先

计算NumPy数组中各个元素的

平方，再求它们的和（np.sum(x**2)

也可

以实现同样的处理）。我们

来画一下这个函数的图

像。结果如图4-8所示，

是一个

三维图像。

图4-8 的图像

0

−3 −2 −1

0 1 2 3

f(x)

x1

x0

现在

我们来求式（4.6）的导数。这里

需要注意的是，式（4.6）有两个

变量，

所以有必要区分对

哪个变量求导数，即对x0和

x1两个变量中的哪一个求

导数。

另外，我们把这里讨

论的有多个变量的函数

的导数称为偏导数。用数

学式表

示的话，可以写成

、 。

怎么求偏导数呢？我们先

试着解一下下面两个关

于偏导数的问题。

100   第 4章

神

经网络的学习

问题1：求x0 = 3, x1 =

4时

，关于x0的偏导数 。

>>> def function_tmp1(x0):

...

return x0*x0 + 4.0**2.0

...

>>>

numerical_diff(function_tmp1, 3.0)

6.00000000000378

问题2：求x0 = 3,

x1 = 4时

，关于x1的偏导数 。

>>> def

function_tmp2(x1):

... return 3.0**2.0 + x1*x1

...

>>> numerical_diff(function_tmp2, 4.0)

7.999999999999119

在这些问

题中，我们定义了一个只

有一个变量的函数，并对

这个函数进

行了求导。例

如，问题1中，我们定义了一

个固定x1 = 4的新函数，然后对

只有变量x0的函数应用了

求数值微分的函数。从上

面的计算结果可知，问题

1的答案是6.00000000000378，问题2的答案是

7.999999999999119，和解析

解的导数基本一

致。

像这样，偏导数和单变

量的导数一样，都是求某

个地方的斜率。不过，

偏导

数需要将多个变量中的

某一个变量定为目标变

量，并将其他变量固定为

某个值。在上例的代码中

，为了将目标变量以外的

变量固定到某些特定的

值

上，我们定义了新函数

。然后，对新定义的函数应

用了之前的求数值微分

的

函数，得到偏导数。

4.4 梯度

在刚才的例子中，我们按

变量分别计算了x0和x1的偏

导数。现在，我

们希望一起

计算x0和x1的偏导数。比如，我

们来考虑求x0 =

3, x1 = 4时(x0, x1)

的偏导数

。另外，像

这样的由全部变

量的偏导数汇总

而成的

向量称为梯度（gradient）。梯度可以

像下面这样来实现。

4.4 梯度

101

def

numerical_gradient(f, x):

 h = 1e-4

# 0.0001

 grad = np.zeros_like(x)

# 生成和x形状相同的数组

for idx in range(x.size):

tmp_val = x[idx]

 # f(x+h)的计算

x[idx] = tmp_val + h

fxh1 = f(x)

 # f(x-h)的计算

x[idx] = tmp_val - h

fxh2 = f(x)

 grad[idx] =

(fxh1 - fxh2) / (2*h)

x[idx] = tmp_val # 还原值

return grad

函

数numerical_gradient(f, x)的实现看上去有些复

杂，但它执行的处

理和求

单变量的数值微分基本

没有区别。需要补充说明

一下的是，np.zeros_

like(x)会生成一个形

状和x相同、所有元素都为

0的数组。

函数numerical_gradient(f, x)中，参数f为函

数，x为NumPy数组，该

函数对NumPy数组

x的各个元素求数值微分

。现在，我们用这个函数实

际

计算一下梯度。这里我

们求点(3, 4)、(0, 2)、(3,

0)处的梯度。

>>> numerical_gradient(function_2, np.array([3.0, 4.0]))

array([

6., 8.])A

>>> numerical_gradient(function_2, np.array([0.0, 2.0]))

array([ 0., 4.])

>>> numerical_gradient(function_2, np.array([3.0,

0.0]))

array([ 6., 0.])

像这样

，我们可以计算(x0, x1)在各点处

的梯度。上例中，点(3,

4)处的

梯

度是(6, 8)、点(0, 2)处的梯度是(0, 4)、点(3, 0)处

的梯度是(6,

0)。这个

梯度意味

着什么呢？为了更好地理

解，我们把 的梯度

画在图

上。不过，这里我们画的是

元素值为负梯度B 的向量

（源代码在ch04/

gradient_2d.py中）。

A 实际上，虽然

求到的值是[6.0000000000037801, 7.9999999999991189]，但实际输出

的是[6., 8.]。

这是因为在输出NumPy数

组时，数值会被改成“易读

”的形式。

B

后面我们将会看

到，负梯度方向是梯度法

中变量的更新方向。——译者

注

102   第 4章

神经网络的学习

如图4-9所示， 的梯度呈现为

有向向量（箭头）。观

察图4-9，我

们发现梯度指向函数f(x0,x1)的

“最低处”（最小值），就像指南

针

一样，所有的箭头都指

向同一点。其次，我们发现

离“最低处”越远，箭头越大

。

图4-9 的梯度

虽然图 4-9 中的梯

度指向了最低处，但并非

任何时候都这样。实际上

，

梯度会指向各点处的函

数值降低的方向。更严格

地讲，梯度指示的方向

是

各点处的函数值减小最

多的方向 A。这是一个非常

重要的性质，请一定

牢记

！

4.4.1　梯度法

机器学习的主要

任务是在学习时寻找最

优参数。同样地，神经网络

也必

须在学习时找到最

优参数（权重和偏置）。这里

所说的最优参数是指损

失函数

A

高等数学告诉我

们，方向导数= cos(θ) × 梯度（θ是方向

导数的方向与梯度方向

的夹角）。因此，所

有的下降

方向中，梯度方向下降最

多。——译者注

4.4

梯度  103

取最小值

时的参数。但是，一般而言

，损失函数很复杂，参数空

间庞大，我

们不知道它在

何处能取得最小值。而通

过巧妙地使用梯度来寻

找函数最小值

（或者尽可

能小的值）的方法就是梯

度法。

这里需要注意的是

，梯度表示的是各点处的

函数值减小最多的方向

。因此，

无法保证梯度所指

的方向就是函数的最小

值或者真正应该前进的

方向。实际

上，在复杂的函

数中，梯度指示的方向基

本上都不是函数值最小

处。

函数的极小值、最小值

以及被称为鞍点（saddle point）的地方

，

梯度为

0。极小值是局部最

小值，也就是限定在某个

范围内的最

小值。鞍点是

从某个方向上看是极大

值，从另一个方向上看则

是

极小值的点。虽然梯度

法是要寻找梯度为 0的地

方，但是那个地

方不一定

就是最小值（也有可能是

极小值或者鞍点）。此外，当

函

数很复杂且呈扁平状

时，学习可能会进入一个

（几乎）平坦的地区，

陷入被

称为“学习高原”的无法前

进的停滞期。

虽然梯度的

方向并不一定指向最小

值，但沿着它的方向能够

最大限度地

减小函数的

值。因此，在寻找函数的最

小值（或者尽可能小的值

）的位置的

任务中，要以梯

度的信息为线索，决定前

进的方向。

此时梯度法就

派上用场了。在梯度法中

，函数的取值从当前位置

沿着梯

度方向前进一定

距离，然后在新的地方重

新求梯度，再沿着新梯度

方向前进，

如此反复，不断

地沿梯度方向前进。像这

样，通过不断地沿梯度方

向前进，

逐渐减小函数值

的过程就是梯度法（gradient method）。梯度

法是解决机器

学习中最

优化问题的常用方法，特

别是在神经网络的学习

中经常被使用。

根据目的

是寻找最小值还是最大

值，梯度法的叫法有所不

同。严格地讲，

寻找最小值

的梯度法称为梯度下降

法（gradient

descent method），

寻找最大值的梯度法

称为梯度上升法（gradient ascent method）。但

是通

过反转损失函数的符号

，求最小值的问题和求最

大值的问题会

变成相同

的问题，因此“下降”还是“上

升”的差异本质上并不重

要。

一般来说，神经网络（深

度学习）中，梯度法主要是

指梯度下降法。

104   第

4章　神经

网络的学习

现在，我们尝

试用数学式来表示梯度

法，如式（4.7）所示。

（4.7）

式（4.7）的η表示更

新量，在神经网络的学习

中，称为学习率（learning

rate）。学习率决

定在一次学习中，应该学

习多少，以及在多大程度

上更新参数。

式（4.7）是表示更

新一次的式子，这个步骤

会反复执行。也就是说，每

一步都按式（4.7）更新变量的

值，通过反复执行此步骤

，逐渐减小函数值。

虽然这

里只展示了有两个变量

时的更新过程，但是即便

增加变量的数量，也

可以

通过类似的式子（各个变

量的偏导数）进行更新。

学

习率需要事先确定为某

个值，比如0.01或0.001。一般而言，这

个值

过大或过小，都无法

抵达一个“好的位置”。在神

经网络的学习中，一般会

一边改变学习率的值，一

边确认学习是否正确进

行了。

下面，我们用Python来实现

梯度下降法。如下所示，这

个实现很简单。

def gradient_descent(f, init_x, lr=0.01, step_num=100):

x = init_x

 for i

in range(step_num):

 grad = numerical_gradient(f,

x)

 x -= lr *

grad

 return x

参数f是要

进行最优化的函数，init_x是初

始值，lr是学习率learning

rate，step_num是梯度法

的重复次数。numerical_gradient(f,x)会求函数的

梯度，用该梯度乘以学习

率得到的值进行更新操

作，由step_num指定重复的

次数。

使

用这个函数可以求函数

的极小值，顺利的话，还可

以求函数的最小值。

下面

，我们就来尝试解决下面

这个问题。

4.4 梯度

105

问题：请用

梯度法求 的最小值。

>>> def function_2(x):

... return x[0]**2 + x[1]**2

...

>>> init_x = np.array([-3.0, 4.0])

>>>

gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)

array([ -6.11110793e-10,

8.14814391e-10])

这里

，设初始值为(-3.0, 4.0)，开始使用梯

度法寻找最小值。最终的

结

果是(-6.1e-10, 8.1e-10)，非常接近(0，0)。实际上

，真的最小值就是(0，0)，

所以说

通过梯度法我们基本得

到了正确结果。如果用图

来表示梯度法的更新

过

程，则如图4-10所示。可以发现

，原点处是最低的地方，函

数的取值一

点点在向其

靠近。这个图的源代码在

ch04/gradient_method.py 中（但ch04/

gradient_method.py不显示表示等高线

的虚线）。

图4-10 的梯度法的更

新过程：虚线是函数的等

高线

106   第 4章　神经网络的学

习

前面说过，学习率过大

或者过小都无法得到好

的结果。我们来做个实验

验证一下。

# 学习率过大的

例子：lr=10.0

>>> init_x =

np.array([-3.0, 4.0])

>>> gradient_descent(function_2, init_x=init_x, lr=10.0,

step_num=100)

array([ -2.58983747e+13, -1.29524862e+12])

# 学习率过小的例子

：lr=1e-10

>>> init_x = np.array([-3.0, 4.0])

>>>

gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)

array([-2.99999994, 3.99999992])

实验结果表明，学习率过

大的话，会发散成一个很

大的值；反过来，学

习率过

小的话，基本上没怎么更

新就结束了。也就是说，设

定合适的学习率

是一个

很重要的问题。

像学习率

这样的参数称为超参数

。这是一种和神经网络的

参数（权重

和偏置）性质不

同的参数。相对于神经网

络的权重参数是通过训

练

数据和学习算法自动

获得的，学习率这样的超

参数则是人工设定的。

一

般来说，超参数需要尝试

多个值，以便找到一种可

以使学习顺利

进行的设

定。

4.4.2　神经网络的梯度

神经

网络的学习也要求梯度

。这里所说的梯度是指损

失函数关于权重参

数的

梯度。比如，有一个只有一

个形状为2

× 3的权重W的神经

网络，损失

函数用L表示。此

时，梯度可以用 表示。用数

学式表示的话，如下所示

。

（4.8）

的元素由各个元素关于

W的偏导数构成。比如，第1行

第1列的元

4.4 梯度  107

素 表示当

w11稍微变化时，损失函数L会

发生多大变化。这里的重

点是，

的形状和W相同。实际

上，式（4.8）中的W和 都是2 × 3的形状

。

下面，我们以一个简单的

神经网络为例，来实现求

梯度的代码。为此，

我们要

实现一个名为simpleNet的类（源代

码在ch04/gradient_simplenet.py

中）。

import sys, os

sys.path.append(os.pardir)

import

numpy as np

from common.functions import

softmax, cross_entropy_error

from common.gradient import numerical_gradient

class simpleNet:

 def __init__(self):

self.W = np.random.randn(2,3) # 用高斯分布进行

初始化

def predict(self, x):

 return np.dot(x,

self.W)

 def loss(self, x, t):

z = self.predict(x)

 y =

softmax(z)

 loss = cross_entropy_error(y, t)

return loss

这里使用了 common/functions.py中的

softmax和 cross_entropy_error方

法，以及common/gradient.py中的numerical_gradient方法。simpleNet类

只有

一个实例变量，即形

状为2×3的权重参数。它有两

个方法，一个是用于预

测

的predict(x)，另一个是用于求损失

函数值的loss(x,t)。这里参数x接收

输入数据，t接收正确解标

签。现在我们来试着用一

下这个simpleNet。

>>> net = simpleNet()

>>> print(net.W) # 权重参数

[[ 0.47355232

0.9977393 0.84668094],

 [ 0.85557411 0.03563661

0.69422093]])

>>>

>>> x = np.array([0.6,

0.9])

>>> p = net.predict(x)

>>>

print(p)

108   第 4章

神

经网络的学习

[ 1.05414809 0.63071653 1.1328074]

>>>

np.argmax(p) # 最大值的

索引

2

>>>

>>>

t = np.array([0, 0, 1]) #

正确解标签

>>> net.loss(x, t)

0.92806853663411326

接下来

求梯度。和前面一样，我们

使用numerical_gradient(f,

x)求梯

度（这里定义的

函数f(W)的参数W是一个伪参

数。因为numerical_gradient(f,

x)会在内部执行f(x),为

了与之兼容而定义了f(W)）。

>>> def f(W):

... return net.loss(x, t)

...

>>>

dW = numerical_gradient(f, net.W)

>>> print(dW)

[[ 0.21924763 0.14356247 -0.36281009]

 [

0.32887144 0.2153437 -0.54421514]]

numerical_gradient(f, x) 的

参数f是函数，x是传给函数

f的参数。因此，

这里参数x取

net.W，并定义一个计算损失函

数的新函数f，然后把这个

新定

义的函数传递给numerical_gradient(f, x)。

numerical_gradient(f, net.W)的

结果是dW，一个形状为2 ×

3的二

维数组。

观察一下dW的内容

，例如，会发现 中的 的值大

约是0.2，这表示如

果将w11增加

h，那么损失函数的值会增

加0.2h。再如， 对应的值大约

是

−0.5，这表示如果将w23增加h，损失

函数的值将减小0.5h。因此，从

减

小损失函数值的观点

来看，w23应向正方向更新，w11应

向负方向更新。至于

更新

的程度，w23比w11的贡献要大。

另

外，在上面的代码中，定义

新函数时使用了“def f(x):···”的形式

。

实际上，Python中如果定义的是

简单的函数，可以使用lambda表

示法。使

用lambda的情况下，上述

代码可以如下实现。

>>> f = lambda w:

net.loss(x, t)

>>> dW = numerical_gradient(f,

net.W)

4.5 学习

算法的实现  109

求出神经网

络的梯度后，接下来只需

根据梯度法，更新权重参

数即可。

在下一节中，我们

会以2层神经网络为例，实

现整个学习过程。

为了对

应形状为多维数组的权

重参数W，这里使用的 numerical_

gradient()和之

前的实现稍有不同。不过

，改动只是为了对应多维

数组，所以改动并不大。这

里省略了对代码的说明

，想知道细节的

读者请参

考源代码（common/gradient.py）。

4.5

学习算法的实

现

关于神经网络学习的

基础知识，到这里就全部

介绍完了。“损失函

数”“mini-batch”“梯度

”“梯度下降法”等关键词已

经陆续登场，这里我们

来

确认一下神经网络的学

习步骤，顺便复习一下这

些内容。神经网络的学习

步骤如下所示。

前提

神经

网络存在合适的权重和

偏置，调整权重和偏置以

便拟合训练数据的

过程

称为“学习”。神经网络的学

习分成下面4个步骤。

步骤

1（mini-batch）

从训练数据中随机选出

一部分数据，这部分数据

称为mini-batch。我们

的目标是减小

mini-batch的损失函数的值。

步骤2（计

算梯度）

为了减小mini-batch的损失

函数的值，需要求出各个

权重参数的梯度。

梯度表

示损失函数的值减小最

多的方向。

步骤3（更新参数

）

将权重参数沿梯度方向

进行微小更新。

110

第 4章　神经

网络的学习

步骤4（重复）

重

复步骤1、步骤2、步骤3。

神经网

络的学习按照上面4个步

骤进行。这个方法通过梯

度下降法更新

参数，不过

因为这里使用的数据是

随机选择的mini batch数据，所以又

称为

随机梯度下降法（stochastic gradient descent）。“随

机”指的是“随机选择的”

的

意思，因此，随机梯度下降

法是“对随机选择的数据

进行的梯度下降法”。

深度

学习的很多框架中，随机

梯度下降法一般由一个

名为SGD的函数来实现。

SGD来源

于随机梯度下降法的英

文名称的首字母。

下面，我

们来实现手写数字识别

的神经网络。这里以2层神

经网络（隐

藏层为1层的网

络）为对象，使用MNIST数据集进

行学习。

4.5.1　2层神经网络的类

首先，我们将这个2层神经

网络实现为一个名为TwoLayerNet的

类，实现

过程如下所示A 。源

代码在ch04/two_layer_net.py中。

import sys, os

sys.path.append(os.pardir)

from common.functions import *

from common.gradient

import numerical_gradient

class TwoLayerNet:

 def

__init__(self, input_size, hidden_size, output_size,

 weight_init_std=0.01):

# 初始化权重

 self.params = {}

self.params['W1'] = weight_init_std * \

np.random.randn(input_size, hidden_size)

 self.params['b1'] = np.zeros(hidden_size)

self.params['W2'] = weight_init_std * \

np.random.randn(hidden_size, output_size)

 self.params['b2'] = np.zeros(output_size)

A TwoLayerNet的

实现参考了斯坦福大学

CS231n课程提供的Python源代码。

4.5 学习

算法的实现  111

def predict(self, x):

 W1, W2

= self.params['W1'], self.params['W2']

 b1, b2

= self.params['b1'], self.params['b2']

 a1 =

np.dot(x, W1) + b1

 z1

= sigmoid(a1)

 a2 = np.dot(z1,

W2) + b2

 y =

softmax(a2)

 return y

 #

x:输入数据, t:监

督数据

 def loss(self, x,

t):

 y = self.predict(x)

return cross_entropy_error(y, t)

 def accuracy(self,

x, t):

 y = self.predict(x)

y = np.argmax(y, axis=1)

 t

= np.argmax(t, axis=1)

 accuracy =

np.sum(y == t) / float(x.shape[0])

return accuracy

 # x:输入数据, t:监督数

据

def numerical_gradient(self, x, t):

 loss_W

= lambda W: self.loss(x, t)

grads = {}

 grads['W1'] =

numerical_gradient(loss_W, self.params['W1'])

 grads['b1'] = numerical_gradient(loss_W,

self.params['b1'])

 grads['W2'] = numerical_gradient(loss_W, self.params['W2'])

grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

 return

grads

虽然这个类的实现稍

微有点长，但是因为和上

一章的神经网络的前向

处

理的实现有许多共通

之处，所以并没有太多新

东西。我们先把这个类中

用到

的变量和方法整理

一下。表4-1中只罗列了重要

的变量，表4-2中则罗列了所

有的方法。

112

第 4章　神经网络

的学习

表 4-1　TwolayerNet类中使用的变

量

变量 说明

params 保存神经网

络的参数的字典型变量

（实例变量）。

params['W1']是第1层的权重

，params['b1']是第1层的偏置。

params['W2']是第2层的

权重，params['b2']是第2层的偏置

grads 保存

梯度的字典型变量（numerical_gradient()方法

的返回值）。

grads['W1']是第1层权重的

梯度，grads['b1']是第1层偏置的梯度

。

grads['W2']是第2层权重的梯度，grads['b2']是第

2层偏置的梯度

表 4-2

TwoLayerNet类的方

法

方法 说明

__init__(self, input_size,

hidden_size,

output_size)

进行初始化

。

参数从头开始依次表示

输入层的神经元数、隐藏

层

的神经元数、输出层的

神经元数

predict(self, x)

进行识别（推理

）。

参数x是图像数据

loss(self, x, t) 计算损

失函数的值。

参数x是图像

数据，t是正确解标签（后面

3个方法

的参数也一样）

accuracy(self, x, t) 计

算识别精度

numerical_gradient(self, x, t) 计算权重参

数的梯度

gradient(self, x,

t) 计算权重参数

的梯度。

numerical_gradient()的高速版，将在下

一章实现

TwoLayerNet类有params和grads两个字

典型实例变量。params变量中保

存

了权重参数，比如params['W1']以NumPy数

组的形式保存了第1层的

权重参

数。此外，第1层的偏

置可以通过param['b1']进行访问。这

里来看一个例子。

net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)

net.params['W1'].shape

# (784, 100)

net.params['b1'].shape # (100,)

net.params['W2'].shape # (100, 10)

net.params['b2'].shape #

(10,)

4.5 学习算

法的实现  113

如上所示，params变量

中保存了该神经网络所

需的全部参数。并且，

params变量

中保存的权重参数会用

在推理处理（前向处理）中

。顺便说一下，

推理处理的

实现如下所示。

x = np.random.rand(100, 784)

# 伪输入数

据（100笔）

y = net.predict(x)

此外，与params变量对应，grads变

量中保存了各个参数的

梯度。如下所示，

使用numerical_gradient()方法

计算梯度后，梯度的信息

将保存在grads变

量中。

x = np.random.rand(100, 784)

# 伪输入

数据（100笔）

t = np.random.rand(100, 10)

# 伪正确解标签（100笔

）

grads = net.numerical_gradient(x, t)

# 计算梯度

grads['W1'].shape # (784, 100)

grads['b1'].shape # (100,)

grads['W2'].shape # (100,

10)

grads['b2'].shape # (10,)

接着，我们来看

一下TwoLayerNet的方法的实现。首先

是__init__(self,

input_size,

hidden_size, output_size)方法，它是类的初始化

方法（所谓初

始化方法，就

是生成TwoLayerNet实例时被调用的

方法）。从第1个参数开始，

依

次表示输入层的神经元

数、隐藏层的神经元数、输

出层的神经元数。另外，

因

为进行手写数字识别时

，输入图像的大小是784（28 ×

28），输出

为10个类别，

所以指定参数

input_size=784、output_size=10，将隐藏层的个数hidden_size

设置为

一个合适的值即可。

此外

，这个初始化方法会对权

重参数进行初始化。如何

设置权重参数

的初始值

这个问题是关系到神经

网络能否成功学习的重

要问题。后面我

们会详细

讨论权重参数的初始化

，这里只需要知道，权重使

用符合高斯

分布的随机

数进行初始化，偏置使用

0进行初始化。predict(self, x)和

accuracy(self, x, t)的实现和

上一章的神经网络的推

理处理基本一样。如

果仍

有不明白的地方，请再回

顾一下上一章的内容。另

外，loss(self,

x, t)

114   第

4章　神经网络的学习

是计算损失函数值的方

法。这个方法会基于predict()的结

果和正确解标签，

计算交

叉熵误差。

剩下的numerical_gradient(self, x, t)方法会

计算各个参数的梯度。根

据数值微分，计算各个参

数相对于损失函数的梯

度。另外，gradient(self,

x, t)

是下一章要实现

的方法，该方法使用误差

反向传播法高效地计算

梯度。

numerical_gradient(self, x, t)基于数值微分计算

参数的梯度。下

一章，我们

会介绍一个高速计算梯

度的方法，称为误差反向

传播法。

用误差反向传播

法求到的梯度和数值微

分的结果基本一致，但可

以

高速地进行处理。使用

误差反向传播法计算梯

度的gradient(self,

x, t)方法会在下一章实

现，不过考虑到神经网络

的学习比较花时间，

想节

约学习时间的读者可以

替换掉这里的numerical_gradient(self,

x, t)，抢先使用

gradient(self, x, t)！

4.5.2　mini-batch的实现

神经网络的学习

的实现使用的是前面介

绍过的mini-batch学习。所谓

mini-batch学习，就

是从训练数据中随机选

择一部分数据（称为mini-batch），

再以

这些mini-batch为对象，使用梯度法

更新参数的过程。下面，我

们就以

TwoLayerNet类为对象，使用MNIST数

据集进行学习（源代码在

ch04/train_

neuralnet.py中）。

import

numpy as np

from dataset.mnist import

load_mnist

from two_layer_net import TwoLayerNet

(x_train,

t_train), (x_test, t_test) = \ load_mnist(normalize=True,

one_hot_

laobel = True)

train_loss_list =

[]

# 超参数

iters_num = 10000

train_size = x_train.shape[0]

batch_size = 100

learning_rate = 0.1

4.5  学习算法的实

现

115

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):

 #

获取mini-batch

 batch_mask = np.random.choice(train_size, batch_size)

x_batch = x_train[batch_mask]

 t_batch =

t_train[batch_mask]

 # 计算梯度

 grad

= network.numerical_gradient(x_batch, t_batch)

 # grad

= network.gradient(x_batch, t_batch) # 高速版

!

# 更新参数

 for key in

('W1', 'b1', 'W2', 'b2'):

 network.params[key]

-= learning_rate * grad[key]

 #

记录学习过程

loss = network.loss(x_batch, t_batch)

train_loss_list.append(loss)

这里，mini-batch的大小为100，需要每次

从60000个训练数据中随机

取

出100个数据（图像数据和正

确解标签数据）。然后，对这

个包含100笔数

据的mini-batch求梯度

，使用随机梯度下降法（SGD）更

新参数。这里，梯

度法的更

新次数（循环的次数）为10000。每

更新一次，都对训练数据

计算损

失函数的值，并把

该值添加到数组中。用图

像来表示这个损失函数

的值的推

移，如图4-11所示。

放

大显示

图4-11　损失函数的推

移：左图是10000次循环的推移

，右图是1000次循环的推移

116

第

4章　神经网络的学习

观察

图4-11，可以发现随着学习的

进行，损失函数的值在不

断减小。这

是学习正常进

行的信号，表示神经网络

的权重参数在逐渐拟合

数据。也就是

说，神经网络

的确在学习！通过反复地

向它浇灌（输入）数据，神经

网络正

在逐渐向最优参

数靠近。

4.5.3　基于测试数据的

评价

根据图4-11呈现的结果

，我们确认了通过反复学

习可以使损失函数的值

逐渐减小这一事实。不过

这个损失函数的值，严格

地讲是“对训练数据的某

个mini-batch的损失函数”的值。训练

数据的损失函数值减小

，虽说是神经

网络的学习

正常进行的一个信号，但

光看这个结果还不能说

明该神经网络在

其他数

据集上也一定能有同等

程度的表现。

神经网络的

学习中，必须确认是否能

够正确识别训练数据以

外的其他数

据，即确认是

否会发生过拟合。过拟合

是指，虽然训练数据中的

数字图像能

被正确辨别

，但是不在训练数据中的

数字图像却无法被识别

的现象。

神经网络学习的

最初目标是掌握泛化能

力，因此，要评价神经网络

的泛

化能力，就必须使用

不包含在训练数据中的

数据。下面的代码在进行

学习的

过程中，会定期地

对训练数据和测试数据

记录识别精度。这里，每经

过一个

epoch，我们都会记录下

训练数据和测试数据的

识别精度。A

epoch是一个单位。一

个 epoch表示学习中所有训练

数据均被使用过

一次时

的更新次数。比如，对于 10000笔

训练数据，用大小为 100

笔数

据的mini-batch进行学习时，重复随

机梯度下降法

100次，所

有的

训练数据就都被“看过”了

A。此时，100次就是一个 epoch。

为了正

确进行评价，我们来稍稍

修改一下前面的代码。与

前面的代码不

同的地方

，我们用粗体来表示。

A

实际

上，一般做法是事先将所

有训练数据随机打乱，然

后按指定的批次大小，按

序生成mini-batch。

这样每个mini-batch均有一

个索引号，比如此例可以

是0, 1, 2, ... ,

99，然后用索引号可以遍

历所有

的mini-batch。遍历一次所有

数据，就称为一个epoch。请注意

，本节中的mini-batch每次都是随机

选择的，所以不一定每个

数据都会被看到。——译者注

4.5 学习算法的实现  117

import

numpy as np

from dataset.mnist import

load_mnist

from two_layer_net import TwoLayerNet

(x_train,

t_train), (x_test, t_test) = \ load_mnist(normalize=True,

one_hot_

laobel = True)

train_loss_list =

[]

train_acc_list = []

test_acc_list =

[]

# 平均每

个epoch的重复次数

iter_per_epoch = max(train_size

/ batch_size, 1)

# 超参数

iters_num

= 10000

batch_size = 100

learning_rate

= 0.1

network = TwoLayerNet(input_size=784, hidden_size=50,

output_size=10)

for i in range(iters_num):

# 获

取mini-batch

 batch_mask = np.random.choice(train_size,

batch_size)

 x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

 # 计算梯度

grad = network.numerical_gradient(x_batch, t_batch)

 #

grad = network.gradient(x_batch, t_batch) # 高速版!

# 更新

参数

 for key in

('W1', 'b1', 'W2', 'b2'):

 network.params[key]

-= learning_rate * grad[key]

 loss

= network.loss(x_batch, t_batch)

 train_loss_list.append(loss)

# 计算每个epoch的识别精

度

 if i %

iter_per_epoch == 0:

 train_acc =

network.accuracy(x_train, t_train)

 test_acc = network.accuracy(x_test,

t_test)

 train_acc_list.append(train_acc)

 test_acc_list.append(test_acc)

print("train acc, test acc | "

+ str(train_acc) + ", " +

str(test_acc))

在上面的例子中，每经

过一个epoch，就对所有的训练

数据和测试数据

计算识

别精度，并记录结果。之所

以要计算每一个epoch的识别

精度，是因

为如果在for语句

的循环中一直计算识别

精度，会花费太多时间。并

且，也

118

第 4章　神经网络的学

习

没有必要那么频繁地

记录识别精度（只要从大

方向上大致把握识别精

度的推

移就可以了）。因此

，我们才会每经过一个epoch就

记录一次训练数据的识

别

精度。

把从上面的代码

中得到的结果用图表示

的话，如图4-12所示。

图4-12　训练数

据和测试数据的识别精

度的推移（横轴的单位是

epoch）

图4-12中，实线表示训练数据

的识别精度，虚线表示测

试数据的识别精

度。如图

所示，随着epoch的前进（学习的

进行），我们发现使用训练

数据和

测试数据评价的

识别精度都提高了，并且

，这两个识别精度基本上

没有差异（两

条线基本重

叠在一起）。因此，可以说这

次的学习中没有发生过

拟合的现象。

4.6 小结

本章中

，我们介绍了神经网络的

学习。首先，为了能顺利进

行神经网络

的学习，我们

导入了损失函数这个指

标。以这个损失函数为基

准，找出使它

4.6

小结  119

的值达

到最小的权重参数，就是

神经网络学习的目标。为

了找到尽可能小的

损失

函数值，我们介绍了使用

函数斜率的梯度法。

本章

所学的内容

• 机器学习中

使用的数据集分为训练

数据和测试数据。

• 神经网

络用训练数据进行学习

，并用测试数据评价学习

到的模型的

泛化能力。

•

神

经网络的学习以损失函

数为指标，更新权重参数

，以使损失函数

的值减小

。

• 利用某个给定的微小值

的差分求导数的过程，称

为数值微分。

• 利用数值微

分，可以计算权重参数的

梯度。

• 数值微分虽然费时

间，但是实现起来很简单

。下一章中要实现的稍

微

复杂一些的误差反向传

播法可以高速地计算梯

度。



第5章

误差反向传播法

上一章中，我们介绍了神

经网络的学习，并通过数

值微分计算了神经网

络

的权重参数的梯度（严格

来说，是损失函数关于权

重参数的梯度）。数值微

分

虽然简单，也容易实现，但

缺点是计算上比较费时

间。本章我们将学习一

个

能够高效计算权重参数

的梯度的方法——误差反向

传播法。

要正确理解误差

反向传播法，我个人认为

有两种方法：一种是基于

数学式；

另一种是基于计

算图（computational graph）。前者是比较常见的

方法，机器

学习相关的图

书中多数都是以数学式

为中心展开论述的。因为

这种方法严密

且简洁，所

以确实非常合理，但如果

一上来就围绕数学式进

行探讨，会忽略

一些根本

的东西，止步于式子的罗

列。因此，本章希望大家通

过计算图，直

观地理解误

差反向传播法。然后，再结

合实际的代码加深理解

，相信大家一

定会有种“原

来如此！”的感觉。

此外，通过

计算图来理解误差反向

传播法这个想法，参考了

Andrej

Karpathy的博客[4]和他与Fei-Fei Li教授负责

的斯坦福大学的深度学

习课程

CS231n [5]。

5.1 计算图

计算图将

计算过程用图形表示出

来。这里说的图形是数据

结构图，通

过多个节点和

边表示（连接节点的直线

称为“边”）。为了让大家熟悉

计算图，

122   第

5章　误差反向传

播法

本节先用计算图解

一些简单的问题。从这些

简单的问题开始，逐步深

入，最

终抵达误差反向传

播法。

5.1.1　用计算图求解

现在

，我们尝试用计算图解简

单的问题。下面我们要看

的几个问题都是

用心算

就能解开的简单问题，这

里的目的只是通过它们

让大家熟悉计算图。

掌握

了计算图的使用方法之

后，在后面即将看到的复

杂计算中它将发挥巨大

威力，所以本节请一定学

会计算图的使用方法。

问

题1：太郎在超市买了2个100日

元一个的苹果，消费税是

10%，请计

算支付金额。

计算图

通过节点和箭头表示计

算过程。节点用○表示，○中是

计算的内

容。将计算的中

间结果写在箭头的上方

，表示各个节点的计算结

果从左向右

传递。用计算

图解问题1，求解过程如图

5-1所示。

×2 ×1.1

100 200

220

图5-1　基于计算图求解

的问题1的答案

如图5-1所示

，开始时，苹果的100日元流到

“× 2”节点，变成200日元，

然后被传

递给下一个节点。接着，这

个200日元流向“×

1.1”节点，变成220

日

元。因此，从这个计算图的

结果可知，答案为220日元。

虽

然图5-1中把“× 2”“× 1.1”等作为一个运

算整体用○括起来了，不过

只用○表示乘法运算“×”也是

可行的。此时，如图5-2所示，可

以将“2”和

“1.1”分别作为变量“苹

果的个数”和“消费税”标在

○外面。

5.1  计算图  123

图5-2

基于计算

图求解的问题1的答案：“苹

果的个数”和“消费税”作为

变量标在○外面

× ×

100 200 220

1.1

2 苹果的个

数

消费税

再看下一题。

问

题2：太郎在超市买了2个苹

果、3个橘子。其中，苹果每个

100日元，

橘子每个150日元。消费

税是10%，请计算支付金额。

同

问题1，我们用计算图来解

问题2，求解过程如图5-3所示

。

图5-3　基于计算图求解的问

题2的答案

×

+

100 200

2

650 715

150

450

1.1

3

×

×

苹果的个数

橘

子的个数

消费税

这个问

题中新增了加法节点“+”，用

来合计苹果和橘子的金

额。构建了

计算图后，从左

向右进行计算。就像电路

中的电流流动一样，计算

结果从左

向右传递。到达

最右边的计算结果后，计

算过程就结束了。从图5-3中

可知，

问题2的答案为715日元

。

124   第 5章　误差反向传播法

综

上，用计算图解题的情况

下，需要按如下流程进行

。

1.构建计算图。

2.在计算图上

，从左向右进行计算。

这里

的第2歩“从左向右进行计

算”是一种正方向上的传

播，简称为正

向传播（forward propagation）。正向

传播是从计算图出发点

到结束点的传播。

既然有

正向传播这个名称，当然

也可以考虑反向（从图上

看的话，就是从右向左）

的

传播。实际上，这种传播称

为反向传播（backward propagation）。反向传

播将

在接下来的导数计算中

发挥重要作用。

5.1.2　局部计算

计算图的特征是可以通

过传递“局部计算”获得最

终结果。“局部”这个

词的意

思是“与自己相关的某个

小范围”。局部计算是指，无

论全局发生了什么，

都能

只根据与自己相关的信

息输出接下来的结果。

我

们用一个具体的例子来

说明局部计算。比如，在超

市买了2个苹果和

其他很

多东西。此时，可以画出如

图5-4所示的计算图。

图5-4　买了

2个苹果和其他很多东西

的例子

×

+

100 200

2

1.1

苹果的个数

消费

税

× 4200 4620

4000

其他很多东西

…

复杂的

计算

如图5-4所示，假设（经过

复杂的计算）购买的其他

很多东西总共花费

5.1

计算

图 125

4000日元。这里的重点是，各

个节点处的计算都是局

部计算。这意味着，例

如苹

果和其他很多东西的求

和运算（4000 + 200

→ 4200）并不关心4000这个

数

字是如何计算而来的，只

要把两个数字相加就可

以了。换言之，各个节点

处

只需进行与自己有关的

计算（在这个例子中是对

输入的两个数字进行加

法

运算），不用考虑全局。

综

上，计算图可以集中精力

于局部计算。无论全局的

计算有多么复杂，

各个步

骤所要做的就是对象节

点的局部计算。虽然局部

计算非常简单，但是

通过

传递它的计算结果，可以

获得全局的复杂计算的

结果。

比如，组装汽车是一

个复杂的工作，通常需要

进行“流水线”作业。

每个工

人（机器）所承担的都是被

简化了的工作，这个工作

的成果会

传递给下一个

工人，直至汽车组装完成

。计算图将复杂的计算分

割

成简单的局部计算，和

流水线作业一样，将局部

计算的结果传递给

下一

个节点。在将复杂的计算

分解成简单的计算这一

点上与汽车的

组装有相

似之处。

5.1.3　为何用计算图解

题

前面我们用计算图解

答了两个问题，那么计算

图到底有什么优点呢？一

个优点就在于前面所说

的局部计算。无论全局是

多么复杂的计算，都可以

通

过局部计算使各个节

点致力于简单的计算，从

而简化问题。另一个优点

是，

利用计算图可以将中

间的计算结果全部保存

起来（比如，计算进行到2个

苹

果时的金额是200日元、加

上消费税之前的金额650日

元等）。但是只有这些

理由

可能还无法令人信服。实

际上，使用计算图最大的

原因是，可以通过反

向传

播高效计算导数。

在介绍

计算图的反向传播时，我

们再来思考一下问题1。问

题1中，我

们计算了购买2个

苹果时加上消费税最终

需要支付的金额。这里，假

设我们

想知道苹果价格

的上涨会在多大程度上

影响最终的支付金额，即

求“支付金

额关于苹果的

价格的导数”。设苹果的价

格为x，支付金额为L，则相当

于求

。这个导数的值表示

当苹果的价格稍微上涨

时，支付金额会增加多少

。

126

第 5章　误差反向传播法

如

前所述，“支付金额关于苹

果的价格的导数”的值可

以通过计算图的

反向传

播求出来。先来看一下结

果，如图5-5所示，可以通过计

算图的反向

传播求导数

（关于如何进行反向传播

，接下来马上会介绍）。

图5-5　基

于反向传播的导数的传

递

× ×

100

2.2

200

1.1

220

1

1.1

2

苹果的个数

消费税

如

图5-5所示，反向传播使用与

正方向相反的箭头（粗线

）表示。反向传

播传递“局部

导数”，将导数的值写在箭

头的下方。在这个例子中

，反向传

播从右向左传递

导数的值（1 →

1.1 → 2.2）。从这个结果中

可知，“支付金额

关于苹果

的价格的导数”的值是2.2。这

意味着，如果苹果的价格

上涨1日元，

最终的支付金

额会增加2.2日元（严格地讲

，如果苹果的价格增加某

个微小值，

则最终的支付

金额将增加那个微小值

的2.2倍）。

这里只求了关于苹

果的价格的导数，不过“支

付金额关于消费税的导

数”“支付金额关于苹果的

个数的导数”等也都可以

用同样的方式算出来。并

且，计算中途求得的导数

的结果（中间传递的导数

）可以被共享，从而可以

高

效地计算多个导数。综上

，计算图的优点是，可以通

过正向传播和反向传

播

高效地计算各个变量的

导数值。

5.2 链式法则

前面介

绍的计算图的正向传播

将计算结果正向（从左到

右）传递，其计

算过程是我

们日常接触的计算过程

，所以感觉上可能比较自

然。而反向传播

5.2  链式法则

127

将局部导数向正方向的

反方向（从右到左）传递，一

开始可能会让人感到困

惑。

传递这个局部导数的

原理，是基于链式法则（chain

rule）的

。本节将介绍链

式法则，并

阐明它是如何对应计算

图上的反向传播的。

5.2.1 计算

图的反向传播

话不多说

，让我们先来看一个使用

计算图的反向传播的例

子。假设存在

y

= f(x)的计算，这个

计算的反向传播如图5-6所

示。

x y

f

E

E

图5-6　计算图的反向传播

：沿着与正方向相反的方

向，乘上局部导数

如图所

示，反向传播的计算顺序

是，将信号E乘以节点的局

部导数

（ ），然后将结果传递

给下一个节点。这里所说

的局部导数是指正向传

播

中y = f(x)的导数，也就是y关于

x的导数（ ）。比如，假设y = f(x)

= x2

，

则局部

导数为 = 2x。把这个局部导数

乘以上游传过来的值（本

例中为E），

然后传递给前面

的节点。

这就是反向传播

的计算顺序。通过这样的

计算，可以高效地求出导

数的

值，这是反向传播的

要点。那么这是如何实现

的呢？我们可以从链式法

则的

原理进行解释。下面

我们就来介绍链式法则

。

5.2.2　什么是链式法则

介绍链

式法则时，我们需要先从

复合函数说起。复合函数

是由多个函数

构成的函

数。比如，z = (x + y)

2

是由式（5.1）所示的两

个式子构成的。

z = t

2

t = x + y

（5.1）

128   第 5章　误差

反向传播法

链式法则是

关于复合函数的导数的

性质，定义如下。

如果某个

函数由复合函数表示，则

该复合函数的导数可以

用构成复

合函数的各个

函数的导数的乘积表示

。

这就是链式法则的原理

，乍一看可能比较难理解

，但实际上它是一个

非常

简单的性质。以式（5.1）为例， （z关

于x的导数）可以用

（z关于t

的

导数）和 （t关于x的导数）的乘

积表示。用数学式表示的

话，可以写成

式（5.2）。

（5.2）

式（5.2）中的∂

t正

好可以像下面这样“互相

抵消”，所以记起来很简单

。

现在我们使用链式法则

，试着求式（5.2）的导数 。为此，我

们要先求

式（5.1）中的局部导

数（偏导数）。

（5.3）

如式（5.3）所示，

等于

2t， 等于1。这是基于导数公式

的解析解。

然后，最后要计

算的 可由式（5.3）求得的导数

的乘积计算出来。

（5.4）

5.2

链式法

则 129

5.2.3　链式法则和计算图

现

在我们尝试将式（5.4）的链式

法则的计算用计算图表

示出来。如果用

“**2”节点表示

平方运算的话，则计算图

如图5-7所示。

+ **2

y

x

t z

图5-7　式（5.4）的计算图

：沿着与正方向相反的方

向，乘上局部导数后传递

如图所示，计算图的反向

传播从右到左传播信号

。反向传播的计算顺序

是

，先将节点的输入信号乘

以节点的局部导数（偏导

数），然后再传递给下一

个

节点。比如，反向传播时，“**2”节

点的输入是 ，将其乘以局

部导数 （因

为正向传播时

输入是t、输出是z，所以这个

节点的局部导数是 ），然后

传

递给下一个节点。另外

，图5-7中反向传播最开始的

信号 在前面的数学

式中

没有出现，这是因为 ，所以

在刚才的式子中被省略

了。

图 5-7 中需要注意的是最

左边的反向传播的结果

。根据链式法则，

成立，对应

“z关于x的导数”。也就是说，反

向传播

是基于链式法则

的。

把式（5.3）的结果代入到图

5-7

中，结果如图 5-8 所示， 的结果

为

2(x +

y)。

130 第 5章　误差反向传播法

图5-8　根据计算图的反向传

播的结果，

等于2(x + y)

y

x

t

z

1

+ **2

5.3 反向传播

上一节介绍了计算图的

反向传播是基于链式法

则成立的。本节将以“+”

和“×”等

运算为例，介绍反向传播

的结构。

5.3.1　加法节点的反向

传播

首先来考虑加法节

点的反向传播。这里以z = x

+ y为

对象，观察它的

反向传播

。z = x +

y的导数可由下式（解析性

地）计算出来。

（5.5）

如式（5.5）所示， 和

同时都等于1。因此，用计算

图表示的话，如

图5-9所示。

在

图5-9中，反向传播将从上游

传过来的导数（本例中是

）乘以1，然

后传向下游。也就

是说，因为加法节点的反

向传播只乘以1，所以输入

的值

会原封不动地流向

下一个节点。

5.3  反向传播 131

图

5-9 加法节点的反向传播：左

图是正向传播，右图是反

向传播。如右图的反向传

播所示，

加法节点的反向

传播将上游的值原封不

动地输出到下游

+ +

y

x

z

另外，本

例中把从上游传过来的

导数的值设为 。这是因为

，如图5-10

所示，我们假定了一

个最终输出值为L的大型

计算图。z =

x + y的计算位于

这个

大型计算图的某个地方

，从上游会传来 的值，并向

下游传递 和

。

图5-10 加法节点

存在于某个最后输出的

计算的一部分中。反向传

播时，从最右边的输

出出

发，局部导数从节点向节

点反方向传播

+

y

x

z 某种计算

某种计算

某种计算

L

132

第 5章

误差反向传播法

现在来

看一个加法的反向传播

的具体例子。假设有“10 + 5=15”这一

计

算，反向传播时，从上游

会传来值1.3。用计算图表示

的话，如图5-11所示。

图5-11　加法节

点的反向传播的具体例

子

+

5

10

15

1.3

1.3

1.3

+

因为加法节点的反向

传播只是将输入信号输

出到下一个节点，所以如

图

5-11所示，反向传播将1.3向下

一个节点传递。

5.3.2　乘法节点

的反向传播

接下来，我们

看一下乘法节点的反向

传播。这里我们考虑z = xy。这个

式子的导数用式（5.6）表示。

（5.6）

根

据式（5.6），可以像图5-12那样画计

算图。

乘法的反向传播会

将上游的值乘以正向传

播时的输入信号的“翻转

值”

后传递给下游。翻转值

表示一种翻转关系，如图

5-12所示，正向传播时信号

是

x的话，反向传播时则是y；正

向传播时信号是y的话，反

向传播时则是x。

现在我们

来看一个具体的例子。比

如，假设有“10 ×

5 = 50”这一计算，

反向

传播时，从上游会传来值

1.3。用计算图表示的话，如图

5-13所示。

5.3

反向传播 133

图5-13　乘法节

点的反向传播的具体例

子

× ×

5

10

50

1.3

13

6.5

×

y

x

z ×

图5-12

乘法的反向传播：左

图是正向传播，右图是反

向传播

因为乘法的反向

传播会乘以输入信号的

翻转值，所以各自可按1.3 × 5 =

6.5、1.3

× 10 = 13计

算。另外，加法的反向传播

只是将上游的值传给下

游，

并不需要正向传播的

输入信号。但是，乘法的反

向传播需要正向传播时

的输

入信号值。因此，实现

乘法节点的反向传播时

，要保存正向传播的输入

信号。

5.3.3　苹果的例子

再来思

考一下本章最开始举的

购买苹果的例子（2个苹果

和消费税）。这

里要解的问

题是苹果的价格、苹果的

个数、消费税这3个变量各

自如何影响

最终支付的

金额。这个问题相当于求

“支付金额关于苹果的价

格的导数”“支

134

第 5章　误差反

向传播法

付金额关于苹

果的个数的导数”“支付金

额关于消费税的导数”。用

计算图的

反向传播来解

的话，求解过程如图5-14所示

。

图5-14　购买苹果的反向传播

的例子

× ×

100

2.2

200

1.1

200

110

220

1

1.1

2 苹果的个数

消费

税

如前所述，乘法节点的

反向传播会将输入信号

翻转后传给下游。从图5-14

的

结果可知，苹果的价格的

导数是2.2，苹果的个数的导

数是110，消费税的

导数是200。这

可以解释为，如果消费税

和苹果的价格增加相同

的值，则消

费税将对最终

价格产生200倍大小的影响

，苹果的价格将产生2.2倍大

小的影响。

不过，因为这个

例子中消费税和苹果的

价格的量纲不同，所以才

形成了这样

的结果（消费

税的1是100%，苹果的价格的1是

1日元）。

最后作为练习，请大

家来试着解一下“购买苹

果和橘子”的反向传播。

在

图5-15中的方块中填入数字

，求各个变量的导数（答案

在若干页后）。

图5-15　购买苹果

和橘子的反向传播的例

子：在方块中填入数字，完

成反向传播

×

+

100 200

2

650 715

150 450

1.1

3

×

×

苹果的个数

橘子的个数

消费税

5.4

简单

层的实现 135

5.4 简单层的实现

本节将用Python实现前面的购

买苹果的例子。这里，我们

把要实现

的计算图的乘

法节点称为“乘法层”（MulLayer），加法

节点称为“加法层”

（AddLayer）。

下一节

，我们将把构建神经网络

的“层”实现为一个类。这里

所

说的“层”是神经网络中

功能的单位。比如，负责 sigmoid函

数的

Sigmoid、负责矩阵乘积的Affine等

，都以层为单位进行实现

。因此，

这里也以层为单位

来实现乘法节点和加法

节点。

5.4.1

乘法层的实现

层的

实现中有两个共通的方

法（接口）forward()和backward()。forward()

对应正向传播

，backward()对应反向传播。

现在来实

现乘法层。乘法层作为MulLayer类

，其实现过程如下所示（源

代码在ch05/layer_naive.py中）。

class MulLayer:

def __init__(self):

 self.x = None

self.y = None

 def forward(self,

x, y):

 self.x = x

self.y = y

 out =

x * y

 return out

def backward(self, dout):

 dx =

dout * self.y # 翻转x和y

dy = dout * self.x

return dx, dy

136

第 5章　误

差反向传播法

__init__()中会初始

化实例变量x和y，它们用于

保存正向传播时的输入

值。

forward()接收x和y两个参数，将它

们相乘后输出。backward()将从上游

传

来的导数（dout）乘以正向传

播的翻转值，然后传给下

游。

上面就是MulLayer的实现。现在

我们使用MulLayer实现前面的购

买苹果

的例子（2个苹果和

消费税）。上一节中我们使

用计算图的正向传播和

反向传播，

像图5-16这样进行

了计算。

图5-16　购买2个苹果

×

×

100

2.2

200

1.1

200

110

220

1

1.1

2 苹

果的个数

消费税

使用这

个乘法层的话，图5-16的正向

传播可以像下面这样实

现（源代码

在ch05/buy_apple.py中）。

apple = 100

apple_num = 2

tax = 1.1

# layer

mul_apple_layer = MulLayer()

mul_tax_layer

= MulLayer()

# forward

apple_price =

mul_apple_layer.forward(apple, apple_num)

price = mul_tax_layer.forward(apple_price, tax)

print(price) # 220

此外，关于

各个变量的导数可由backward()求

出。

5.4

简单层的实现 137

# backward

dprice =

1

dapple_price, dtax = mul_tax_layer.backward(dprice)

dapple,

dapple_num = mul_apple_layer.backward(dapple_price)

print(dapple, dapple_num, dtax)

# 2.2 110 200

这里，调

用backward()的顺序与调用forward()的顺序

相反。此外，要注

意backward()的参数

中需要输入“关于正向传

播时的输出变量的导数

”。比如，

mul_apple_layer乘法层在正向传播

时会输出apple_price，在反向传播时

，则

会将apple_price的导数dapple_price设为参数

。另外，这个程序的运行结

果

和图5-16是一致的。

5.4.2　加法层

的实现

接下来，我们实现

加法节点的加法层，如下

所示。

class AddLayer:

 def __init__(self):

pass

 def forward(self, x, y):

out = x + y

return out

 def backward(self, dout):

dx = dout * 1

dy = dout * 1

return dx, dy

加法层不需要特意

进行初始化，所以__init__()中什么

也不运行（pass

语句表示“什么

也不运行”）。加法层的forward()接收

x和y两个参数，将它

们相加

后输出。backward()将上游传来的导

数（dout）原封不动地传递给下

游。

现在，我们使用加法层

和乘法层，实现图5-17所示的

购买2个苹果和3

个橘子的

例子。

138   第

5章　误差反向传播

法

图5-17　购买2个苹果和3个橘

子

×

+

100 200

2

650 715

150

450

1.1

3

×

×

苹果的个数

橘子的个

数

消费税

2.2

3.3

1.1

1.1

110

650

165

1.1

1

用Python实现图5-17的计

算图的过程如下所示（源

代码在ch05/buy_

apple_orange.py中）。

apple = 100

apple_num =

2

orange = 150

orange_num =

3

tax = 1.1

# layer

mul_apple_layer = MulLayer()

mul_orange_layer = MulLayer()

add_apple_orange_layer = AddLayer()

mul_tax_layer = MulLayer()

# forward

apple_price = mul_apple_layer.forward(apple, apple_num)

#(1)

orange_price = mul_orange_layer.forward(orange, orange_num) #(2)

all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)

price

= mul_tax_layer.forward(all_price, tax) #(4)

# backward

dprice = 1

dall_price, dtax =

mul_tax_layer.backward(dprice) #(4)

dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)

#(3)

dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)

dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)

print(price)

# 715

print(dapple_num, dapple, dorange, dorange_num,

dtax) # 110 2.2 3.3 165

650

5.5  激活函数层的

实现 139

这个实现稍微有一

点长，但是每一条命令都

很简单。首先，生成必要的

层，以合适的顺序调用正

向传播的forward()方法。然后，用与

正向传播相

反的顺序调

用反向传播的backward()方法，就可

以求出想要的导数。

综上

，计算图中层的实现（这里

是加法层和乘法层）非常

简单，使用这

些层可以进

行复杂的导数计算。下面

，我们来实现神经网络中

使用的层。

5.5 激活函数层的

实现

现在，我们将计算图

的思路应用到神经网络

中。这里，我们把构成神经

网络的层实现为一个类

。先来实现激活函数的ReLU层

和Sigmoid层。

5.5.1　ReLU层

激活函数ReLU（Rectified Linear Unit）由下式

（5.7）表示。

（5.7）

通过式（5.7），可以求出y关

于x的导数，如式（5.8）所示。

（5.8）

在式

（5.8）中，如果正向传播时的输

入x大于0，则反向传播会将

上游的

值原封不动地传

给下游。反过来，如果正向

传播时的x小于等于0，则反

向

传播中传给下游的信

号将停在此处。用计算图

表示的话，如图5-18所示。

现在

我们来实现ReLU层。在神经网

络的层的实现中，一般假

定forward()

和backward()的参数是NumPy数组。另外

，实现ReLU层的源代码在common/

layers.py中。

140 第

5章　误差反向传播法

图5-18

ReLU层

的计算图

relu relu

x y y

0

x

x > 0 时

x  0 时

class

Relu:

 def __init__(self):

 self.mask

= None

 def forward(self, x):

self.mask = (x <= 0)

out = x.copy()

 out[self.mask] =

0

 return out

 def

backward(self, dout):

 dout[self.mask] = 0

dx = dout

 return dx

Relu类有实例

变量mask。这个变量mask是由True/False构成

的NumPy数

组，它会把正向传播

时的输入x的元素中小于

等于0的地方保存为True，其

他

地方（大于0的元素）保存为

False。如下例所示，mask变量保存了

由True/

False构成的NumPy数组。

>>> x

= np.array( [[1.0, -0.5], [-2.0, 3.0]]

)

>>> print(x)

[[ 1. -0.5]

[-2. 3. ]]

>>> mask =

(x <= 0)

>>> print(mask)

[[False

True]

 [ True False]]

5.5

激活函数

层的实现 141

如图5-18所示，如果

正向传播时的输入值小

于等于0，则反向传播的值

为0。

因此，反向传播中会使

用正向传播时保存的mask，将

从上游传来的dout的

mask中的元

素为True的地方设为0。

ReLU层的作

用就像电路中的开关一

样。正向传播时，有电流通

过

的话，就将开关设为 ON；没

有电流通过的话，就将开

关设为 OFF。

反向传播时，开关

为ON的话，电流会直接通过

；开关为OFF的话，

则不会有电

流通过。

5.5.2

Sigmoid层

接下来，我们来

实现sigmoid函数。sigmoid函数由式（5.9）表示

。

（5.9）

用计算图表示式（5.9）的话，则

如图5-19所示。

图5-19 sigmoid层的计算图

（仅正向传播）

−1 1

y

1

× exp

+ /

x −x exp(−x) 1+exp(−x)

1+exp(−x)

图5-19中，除了“×”和

“+”节点外，还出现了新的“exp”和

“/”节点。

“exp”节点会进行y = exp(x)的计算

，“/”节点会进行 的计算。

如图

5-19所示，式（5.9）的计算由局部计

算的传播构成。下面我们

就来

进行图5-19的计算图的

反向传播。这里，作为总结

，我们来依次看一下反向

传播的流程。

142 第 5章　误差反

向传播法

步骤1

“/”节点表示

，它的导数可以解析性地

表示为下式。

（5.10）

根据式（5.10），反向

传播时，会将上游的值乘

以−y

2

（正向传播的输出的

平

方乘以−1后的值）后，再传给

下游。计算图如下所示。

× exp

x −x exp(−x)

1+exp(−x)

−1 1

y

+ /

步

骤2

“+”节点将上游的值原封

不动地传给下游。计算图

如下所示。

x −x exp(−x) 1+exp(−x)

y

−1 1

× exp +

/

步骤3

“exp”节点表示

y = exp(x)，它的导数由下式表示。

（5.11）

计

算图中，上游的值乘以正

向传播时的输出（这个例

子中是exp(−x)）后，

再传给下游。

× exp ＋ /

x −x exp(−x) 1+exp(−x) y

−1

1

exp(−x)

5.5  激

活函数层的实现 143

步骤4

“×”节

点将正向传播时的值翻

转后做乘法运算。因此，这

里要乘以−1。

× exp ＋ /

x −x exp(−x) 1+exp(−x) y

exp(−x)

exp(−x)

−1 1

图5-20 Sigmoid层的计算图

根据上述内容，图5-20的计算

图可以进行Sigmoid层的反向传

播。从图5-20

的结果可知，反向

传播的输出为

，这个值会

传播给下游的节点。

这里

要注意， 这个值只根据正

向传播时的输入x和输出

y就可

以算出来。因此，图5-20的

计算图可以画成图5-21的集

约化的“sigmoid”节点。

sigmoid

x

y

exp(−x)

图5-21 Sigmoid层的计算

图（简洁版）

图5-20的计算图和

简洁版的图5-21的计算图的

计算结果是相同的，但是

，

简洁版的计算图可以省

略反向传播中的计算过

程，因此计算效率更高。此

外，

通过对节点进行集约

化，可以不用在意Sigmoid层中琐

碎的细节，而只需要

专注

它的输入和输出，这一点

也很重要。

另外， 可以进一

步整理如下。

（5.12）

144

第 5章　误差反

向传播法

因此，图5-21所表示

的Sigmoid层的反向传播，只根据

正向传播的输出

就能计

算出来。

sigmoid

x y

(1 − y)

图5-22

Sigmoid层的计算图：可

以根据正向传播的输出

y计算反向传播

现在，我们

用Python实现Sigmoid层。参考图5-22，可以像

下面这样实

现（实现的代

码在common/layers.py中）。

class Sigmoid:

def __init__(self):

 self.out = None

def forward(self, x):

 out =

1 / (1 + np.exp(-x))

self.out = out

 return out

def backward(self, dout):

 dx =

dout * (1.0 - self.out) *

self.out

 return dx

这个实现中，正向

传播时将输出保存在了

实例变量out中。然后，反向

传

播时，使用该变量out进行计

算。

5.6 Affine/Softmax层的实现

5.6.1　Affine层

神经网络

的正向传播中，为了计算

加权信号的总和，使用了

矩阵的乘

积运算（NumPy中是np.dot()，具

体请参照3.3节）。比如，还记得

我们用

Python进行了下面的实

现吗？

5.6  Affine/Softmax层的实现 145

>>>

X = np.random.rand(2) # 输入

>>>

W = np.random.rand(2,3) # 权重

>>> B

= np.random.rand(3) # 偏置

>>>

>>>

X.shape # (2,)

>>> W.shape #

(2, 3)

>>> B.shape # (3,)

>>>

>>> Y = np.dot(X, W)

+ B

这里，X、W、B 分别是形状为

(2,)、(2, 3)、(3,)的多维数组。这样一

来，神

经元的加权和可以用Y

= np.dot(X, W) + B计

算出来。然后，Y 经过

激活函

数转换后，传递给下一层

。这就是神经网络正向传

播的流程。此外，

我们来复

习一下，矩阵的乘积运算

的要点是使对应维度的

元素个数一致。比

如，如下

面的图5-23所示，X和W 的乘积必

须使对应维度的元素个

数一致。

另外，这里矩阵的

形状用(2, 3)这样的括号表示

（为了和NumPy的shape属

性的输出一

致）。

X · W = O

(2,) (2, 3) (3,)

保持一致

图5-23

矩阵的乘

积运算中对应维度的元

素个数要保持一致

神经

网络的正向传播中进行

的矩阵的乘积运算在几

何学领域被称为“仿

射变

换”A。因此，这里将进行仿射

变换的处理实现为“Affine层”。

A

现

在将这里进行的求矩阵

的乘积与偏置的和的运

算用计算图表示出来。

将

乘积运算用“dot”节点表示的

话，则np.dot(X,

W) + B的运算可用图5-24

所示

的计算图表示出来。另外

，在各个变量的上方标记

了它们的形状（比如，

计算

图上显示了X的形状为(2,)，X·W的

形状为(3,)等）。

A

几何中，仿射变

换包括一次线性变换和

一次平移，分别对应神经

网络的加权和运算与加

偏置运算。

——译者注

146 第 5章　误

差反向传播法

图5-24 Affine层的计

算图（注意变量是矩阵，各

个变量的上方标记了该

变量的形状）

dot

X W Y

W

B

(3,) (3,)

(3,)

(2,

3)

X

(2,)

+

图5-24是比较简

单的计算图，不过要注意

X、W、B是矩阵（多维数组）。

之前我

们见到的计算图中各个

节点间流动的是标量，而

这个例子中各个节点

间

传播的是矩阵。

现在我们

来考虑图5-24的计算图的反

向传播。以矩阵为对象的

反向传播，

按矩阵的各个

元素进行计算时，步骤和

以标量为对象的计算图

相同。实际写

一下的话，可

以得到下式（这里省略了

式（5.13）的推导过程）。

（5.13）

式（5.13）中WT的T表

示转置。转置操作会把W的

元素(i,

j)换成元素

(j, i)。用数学式

表示的话，可以写成下面

这样。

（5.14）

5.6

Affine/Softmax层的实现  147

如式（5.14）所示

，如果W的形状是(2, 3)，WT的形状就

是(3, 2)。

现在，我们根据式（5.13），尝试

写出计算图的反向传播

，如图5-25所示。

(3,)

(2, 1) (1, 3)

(2,)

(2, 3)

(3, 2) dot

＋

X

X・W Y

W

B

(3,) (3,)

(3,)

(2,)

(2, 3)

1

2

1

2

(3,) (3,)

(3,)

图5-25 Affi ne层的反向传

播：注意变量是多维数组

。反向传播时各个变量的

下方标记了

该变量的形

状

我们看一下图2-25的计算

图中各个变量的形状。尤

其要注意，X和

形状相同，W和

形状相同。从下面的数学

式可以很明确地看出X和

形状相同。

（5.15）

为什么要注意

矩阵的形状呢？因为矩阵

的乘积运算要求对应维

度的元素

个数保持一致

，通过确认一致性，就可以

导出式（5.13）。比如， 的形状是

(3,)，W的

形状是(2,

3)时，思考 和WT的乘积

，使得 的形状为(2,)

（图5-26）。这样一

来，就会自然而然地推导

出式（5.13）。

148

第 5章　误差反向传播

法

1

X

(2,)

X・W

(3,)

W

(2, 3)

dot

(3,)

1

(3,) (3, 2) (2,)

图5-26 矩阵的乘积（“dot”节点）的

反向传播可以通过组建

使矩阵对应维度的元素

个数一

致的乘积运算而

推导出来

5.6.2　批版本的Affine层

前

面介绍的Affi

ne层的输入X是以

单个数据为对象的。现在

我们考虑N

个数据一起进

行正向传播的情况，也就

是批版本的Affi ne层。

先给出批

版本的Affi ne层的计算图，如图

5-27所示。

dot

＋

W

(2, 3)

X

(N,

2)

X・W

(N, 3)

Y

(N,

3)

B

(3,)

1

2

3

(N, 3) (N, 3)

(N, 2)

(N, 3)

1

2

3

(3,

2)

(2, 3) (2, N) (N,

3)

(3) (N, 3)

的第一个轴（第0轴）方

向上的和

图5-27

批版本的Affi ne层

的计算图

5.6  Affine/Softmax层的实现 149

与刚

刚不同的是，现在输入X的

形状是(N, 2)。之后就和前面一

样，在

计算图上进行单纯

的矩阵计算。反向传播时

，如果注意矩阵的形状，就

可以

和前面一样推导出

和 。

加上偏置时，需要特别

注意。正向传播时，偏置被

加到X·W的各个

数据上。比如

，N = 2（数据为2个）时，偏置会被分

别加到这2个数据（各自

的

计算结果）上，具体的例子

如下所示。

>>> X_dot_W

= np.array([[0, 0, 0], [10, 10,

10]])

>>> B = np.array([1, 2,

3])

>>>

>>> X_dot_W

array([[ 0,

0, 0],

 [ 10, 10,

10]])

>>> X_dot_W + B

array([[

1, 2, 3],

 [11, 12,

13]])

正向传播时，偏

置会被加到每一个数据

（第1个、第2个……）上。因此，

反向传

播时，各个数据的反向传

播的值需要汇总为偏置

的元素。用代码表示

的话

，如下所示。

>>> dY

= np.array([[1, 2, 3,], [4, 5,

6]])

>>> dY

array([[1, 2, 3],

[4, 5, 6]])

>>>

>>> dB

= np.sum(dY, axis=0)

>>> dB

array([5,

7, 9])

这个例子中，假

定数据有2个（N = 2）。偏置的反向

传播会对这2个数据

的导

数按元素进行求和。因此

，这里使用了np.sum()对第0轴（以数

据为单

位的轴，axis=0）方向上的

元素进行求和。

综上所述

，Affine的实现如下所示。另外，common/layers.py中

的Affine

的实现考虑了输入数

据为张量（四维数据）的情

况，与这里介绍的稍有差

别。

150

第 5章　误差反向传播法

class Affine:

 def

__init__(self, W, b):

 self.W =

W

 self.b = b

self.x = None

 self.dW =

None

 self.db = None

def forward(self, x):

 self.x =

x

 out = np.dot(x, self.W)

+ self.b

 return out

def backward(self, dout):

 dx =

np.dot(dout, self.W.T)

 self.dW = np.dot(self.x.T,

dout)

 self.db = np.sum(dout, axis=0)

return dx

5.6.3　Softmax-with-Loss 层

最后介绍一下输出层

的softmax函数。前面我们提到过

，softmax函数

会将输入值正规化

之后再输出。比如手写数

字识别时，Softmax层的输出如

图

5-28所示。

Softmax

得分 概率

Affine

ReLU

5.3

10.1

0.3・・・

・・・

・・・・・・

Affine ReLU

・・・

・・・

・・・・・・

・・・

0.01

0.008

0.00005

0.991

0.00004

Affine

・・・

・・・

图5-28 输入图

像通过Affi ne层和ReLU层进行转换

，10个输入通过Softmax层进行正

规

化。在这个例子中，“0”的得分

是5.3，这个值经过Softmax层转换为

0.008

（0.8%）；“2”的得分是10.1，被转换为0.991（99.1%）

在图

5-28中，Softmax层将输入值正规化（将

输出值的和调整为1）之后

再输出。另外，因为手写数

字识别要进行10类分类，所

以向Softmax层的输

入也有10个。

5.6  Affine/Softmax层

的实现

151

神经网络中进行

的处理有推理（inference）和学习两

个阶段。神经网

络的推理

通常不使用 Softmax层。比如，用图

5-28的网络进行推理时，

会将

最后一个 Affine层的输出作为

识别结果。神经网络中未

被正规

化的输出结果（图

5-28中 Softmax层前面的 Affine层的输出）有

时

被称为“得分”。也就是说

，当神经网络的推理只需

要给出一个答案

的情况

下，因为此时只对得分最

大值感兴趣，所以不需要

Softmax层。

不过，神经网络的学习

阶段则需要

Softmax层。

下面来实

现Softmax层。考虑到这里也包含

作为损失函数的交叉熵

误

差（cross entropy error），所以称为“Softmax-with-Loss层”。Softmax-withLoss层（Softmax函数

和交叉熵误差）的计算图

如图5-29所示。

×

×

×

×

log

log

log

×

×

exp

exp

exp

+

+

/ S

L

Softmax 层

Cross Entropy Error 层

×

a1

a2

a3

y1−t1

y2−t2

y3−t3

exp(a1)

exp(a2)

exp(a3)

S

1

S

1

S

1

S

1

exp(a1)

exp(a1)

− t1

exp(a2)

exp(a2) − t2

exp(a3)

exp(a3) − t3

−t3S

−t1S

−t2S

S

1

S

1

S

1

t1

t2

t3

y1

y2

y3

y1 −t1

y2 −t2

y3

−t3

log y1

log y2

log

y3

−t1

−t2

−t3

t1 log

y1 + t2 log y2 +

t3 log y3

t1 log y1

t2 log y2

t3 log y3

−1

−1

−1

−1

−1

1

图5-29 Softmax-with-Loss层的计

算图

可以看到，Softmax-with-Loss层有些复

杂。这里只给出了最终结

果，

对Softmax-with-Loss层的导出过程感兴

趣的读者，请参照附录A。

图

5-29的计算图可以简化成图

5-30。

图5-30的计算图中，softmax函数记为

Softmax层，交叉熵误差记为

Cross Entropy Error层。这

里假设要进行3类分类，从

前面的层接收3个输

入（得

分）。如图5-30所示，Softmax层将输入（a1, a2, a3）正

规化，输出（y1,

y2, y3）。Cross Entropy Error层接收Softmax的输出

（y1, y2, y3）和教师标签（t1,

t2, t3），从这些数据

中输出损失L。

152 第 5章　误差反

向传播法

图5-30　“简易版”的Softmax-with-Loss层

的计算图

Softmax

L

1

Cross

Entropy

Error

a1

a2

a3

y1−t

1

y2−t 2

y3−t 3

t

1

y1

t 2

y2

t

3

y3

图5-30中要注意的

是反向传播的结果。Softmax层的

反向传播得到了

（y1 − t1,

y2 − t2, y3 − t3）这样“漂

亮”的结果。由于（y1,

y2, y3）是Softmax层的

输

出，（t1, t2, t3）是监督数据，所以（y1 −

t1, y2 − t2, y3 −

t3）是Softmax层

的输

出和教师标签的差

分。神经网络的反向传播

会把这个差分表示的误

差传递给

前面的层，这是

神经网络学习中的重要

性质。

神经网络学习的目

的就是通过调整权重参

数，使神经网络的输出（Softmax

的

输出）接近教师标签。因此

，必须将神经网络的输出

与教师标签的误差高

效

地传递给前面的层。刚刚

的（y1

− t1, y2 − t2, y3

− t3）正是Softmax层的输出

与教师

标签的差，直截了当地表

示了当前神经网络的输

出与教师标签的误差。

这

里考虑一个具体的例子

，比如思考教师标签是（0, 1, 0），Softmax层

的输出是(0.3,

0.2, 0.5)的情形。因为正

确解标签处的概率是0.2（20%），这

个

时候的神经网络未能

进行正确的识别。此时，Softmax层

的反向传播传递的

是(0.3, −0.8, 0.5)这

样一个大的误差。因为这

个大的误差会向前面的

层传播，

所以Softmax层前面的层

会从这个大的误差中学

习到“大”的内容。

5.6  Affine/Softmax层的实现

153

使用交叉熵误差作为 softmax函

数的损失函数后，反向传

播得到

（y1 − t1, y2 − t2,

y3 − t3）这样“漂亮”的结果

。实际上，这样“漂亮”

的结果

并不是偶然的，而是为了

得到这样的结果，特意设

计了交叉

熵误差函数。回

归问题中输出层使用“恒

等函数”，损失函数使用

“平

方和误差”，也是出于同样

的理由（3.5节）。也就是说，使用

“平

方和误差”作为“恒等函

数”的损失函数，反向传播

才能得到（y1 −

t1, y2 − t2,

y3 − t3）这样“漂亮”的结

果。

再举一个例子，比如思

考教师标签是(0, 1, 0)，Softmax层的输出

是(0.01,

0.99, 0)的情形（这个神经网络

识别得相当准确）。此时Softmax层

的反向传播

传递的是(0.01, −0.01, 0)这

样一个小的误差。这个小

的误差也会向前面的层

传播，因为误差很小，所以

Softmax层前面的层学到的内容

也很“小”。

现在来进行Softmax-with-Loss层的

实现，实现过程如下所示

。

class SoftmaxWithLoss:

 def __init__(self):

self.loss = None # 损失

self.y = None # softmax的输出

self.t = None # 监督数据（one-hot vector）

def forward(self, x, t):

 self.t

= t

 self.y = softmax(x)

self.loss = cross_entropy_error(self.y, self.t)

 return

self.loss

 def backward(self, dout=1):

batch_size = self.t.shape[0]

 dx =

(self.y - self.t) / batch_size

return dx

这

个实现利用了3.5.2节和4.2.4节中

实现的softmax()和cross_entropy_

error()函数。因此，这里

的实现非常简单。请注意

反向传播时，将要传播

的

值除以批的大小（batch_size）后，传递

给前面的层的是单个数

据的误差。

154

第 5章　误差反向

传播法

5.7 误差反向传播法

的实现

通过像组装乐高

积木一样组装上一节中

实现的层，可以构建神经

网络。

本节我们将通过组

装已经实现的层来构建

神经网络。

5.7.1　神经网络学习

的全貌图

在进行具体的

实现之前，我们再来确认

一下神经网络学习的全

貌图。神

经网络学习的步

骤如下所示。

前提

神经网

络中有合适的权重和偏

置，调整权重和偏置以便

拟合训练数据的

过程称

为学习。神经网络的学习

分为下面4个步骤。

步骤1（mini-batch）

从

训练数据中随机选择一

部分数据。

步骤2（计算梯度

）

计算损失函数关于各个

权重参数的梯度。

步骤3（更

新参数）

将权重参数沿梯

度方向进行微小的更新

。

步骤4（重复）

重复步骤1、步骤

2、步骤3。

之前介绍的误差反

向传播法会在步骤2中出

现。上一章中，我们利用数

值微分求得了这个梯度

。数值微分虽然实现简单

，但是计算要耗费较多的

时

间。和需要花费较多时

间的数值微分不同，误差

反向传播法可以快速高

效地

计算梯度。

5.7  误差反向

传播法的实现 155

5.7.2

对应误差

反向传播法的神经网络

的实现

现在来进行神经

网络的实现。这里我们要

把2层神经网络实现为TwoLayerNet。

首

先，将这个类的实例变量

和方法整理成表5-1和表5-2。

表

5-1　TwoLayerNet类的实例变量

实例变量

说明

params 保存神经网络的参

数的字典型变量。

params['W1']是第1层

的权重，params['b1']是第1层的偏置。

params['W2']是

第2层的权重，params['b2']是第2层的偏

置

layers 保存神经网络的层的

有序字典型变量。

以layers['Affine1']、layers['ReLu1']、layers['Affine2']的形

式，

通过有序字典保存各

个层

lastLayer 神经网络的最后一

层。

本例中为SoftmaxWithLoss层

表

5-2　TwoLayerNet类的方

法

方法 说明

__init__(self, input_size,

hidden_size, output_size,

weight_init_std)

进行初始化

。

参数从头开始依次是输

入层的神经元数、隐藏层

的

神经元数、输出层的神

经元数、初始化权重时的

高

斯分布的规模

predict(self, x) 进行识

别（推理）。

参数x是图像数据

loss(self, x,

t) 计算损失函数的值。

参数

X是图像数据、t是正确解标

签

accuracy(self, x, t)

计算识别精度

numerical_gradient(self, x, t) 通过数

值微分计算关于权重参

数的梯度（同上一章）

gradient(self,

x, t) 通过

误差反向传播法计算关

于权重参数的梯度

这个

类的实现稍微有一点长

，但是内容和4.5节的学习算

法的实现有很

多共通的

部分，不同点主要在于这

里使用了层。通过使用层

，获得识别结果

的处理（predict()）和

计算梯度的处理（gradient()）只需通

过层之间的传递就

156 第 5章

误差反向传播法

能完成

。下面是TwoLayerNet的代码实现。

import sys,

os

sys.path.append(os.pardir)

import numpy as np

from common.layers import *

from common.gradient

import numerical_gradient

from collections import OrderedDict

class TwoLayerNet:

 def __init__(self, input_size,

hidden_size, output_size,

 weight_init_std=0.01):

 #

初始

化权重

 self.params = {}

self.params['W1'] = weight_init_std * \

np.random.randn(input_size, hidden_size)

 self.params['b1'] = np.zeros(hidden_size)

self.params['W2'] = weight_init_std * \

np.random.randn(hidden_size, output_size)

 self.params['b2'] = np.zeros(output_size)

# 生成层

 self.layers = OrderedDict()

self.layers['Affine1'] = \

 Affine(self.params['W1'], self.params['b1'])

self.layers['Relu1'] = Relu()

 self.layers['Affine2'] =

\

 Affine(self.params['W2'], self.params['b2'])

 self.lastLayer

= SoftmaxWithLoss()

 def predict(self, x):

for layer in self.layers.values():

 x

= layer.forward(x)

 return x

# x:输入数据

, t:监督数据

 def loss(self,

x, t):

 y = self.predict(x)

return self.lastLayer.forward(y, t)

 def accuracy(self,

x, t):

 y = self.predict(x)

y = np.argmax(y, axis=1)

 if

t.ndim != 1 : t =

np.argmax(t, axis=1)

5.7  误差反向传播

法的实现 157

accuracy = np.sum(y == t) /

float(x.shape[0])

 return accuracy

 #

x:输入数据, t:监督

数据

 def numerical_gradient(self, x,

t):

 loss_W = lambda W:

self.loss(x, t)

 grads = {}

grads['W1'] = numerical_gradient(loss_W, self.params['W1'])

 grads['b1']

= numerical_gradient(loss_W, self.params['b1'])

 grads['W2'] =

numerical_gradient(loss_W, self.params['W2'])

 grads['b2'] = numerical_gradient(loss_W,

self.params['b2'])

 return grads

 def

gradient(self, x, t):

 # forward

self.loss(x, t)

 # backward

dout = 1

 dout =

self.lastLayer.backward(dout)

 layers = list(self.layers.values())

layers.reverse()

 for layer in layers:

dout = layer.backward(dout)

 # 设定

grads = {}

 grads['W1'] =

self.layers['Affine1'].dW

 grads['b1'] = self.layers['Affine1'].db

grads['W2'] = self.layers['Affine2'].dW

 grads['b2'] =

self.layers['Affine2'].db

 return grads

请注意这个实

现中的粗体字代码部分

，尤其是将神经网络的层

保存为

OrderedDict这一点非常重要

。OrderedDict是有序字典，“有序”是指它

可以

记住向字典里添加

元素的顺序。因此，神经网

络的正向传播只需按照

添加元

素的顺序调用各

层的forward()方法就可以完成处

理，而反向传播只需要按

照相反的顺序调用各层

即可。因为Affine层和ReLU层的内部

会正确处理正

向传播和

反向传播，所以这里要做

的事情仅仅是以正确的

顺序连接各层，再

按顺序

（或者逆序）调用各层。

158 第

5章

误差反向传播法

像这样

通过将神经网络的组成

元素以层的方式实现，可

以轻松地构建神

经网络

。这个用层进行模块化的

实现具有很大优点。因为

想另外构建一个神

经网

络（比如5层、10层、20层……的大的神

经网络）时，只需像组装乐

高

积木那样添加必要的

层就可以了。之后，通过各

个层内部实现的正向传

播和

反向传播，就可以正

确计算进行识别处理或

学习所需的梯度。

5.7.3　误差反

向传播法的梯度确认

到

目前为止，我们介绍了两

种求梯度的方法。一种是

基于数值微分的方

法，另

一种是解析性地求解数

学式的方法。后一种方法

通过使用误差反向传

播

法，即使存在大量的参数

，也可以高效地计算梯度

。因此，后文将不再使

用耗

费时间的数值微分，而是

使用误差反向传播法求

梯度。

数值微分的计算很

耗费时间，而且如果有误

差反向传播法的（正确的

）

实现的话，就没有必要使

用数值微分的实现了。那

么数值微分有什么用呢

？

实际上，在确认误差反向

传播法的实现是否正确

时，是需要用到数值微分

的。

数值微分的优点是实

现简单，因此，一般情况下

不太容易出错。而误差

反

向传播法的实现很复杂

，容易出错。所以，经常会比

较数值微分的结果和

误

差反向传播法的结果，以

确认误差反向传播法的

实现是否正确。确认数值

微分求出的梯度结果和

误差反向传播法求出的

结果是否一致（严格地讲

，是

非常相近）的操作称为

梯度确认（gradient check）。梯度确认的代

码实现如下

所示（源代码

在ch05/gradient_check.py中）。

import sys, os

sys.path.append(os.pardir)

import numpy as np

from

dataset.mnist import load_mnist

from two_layer_net import

TwoLayerNet

# 读入数据

(x_train, t_train), (x_test,

t_test) = \ load_mnist(normalize=True, one_

hot_label

= True)

network = TwoLayerNet(input_size=784, hidden_size=50,

output_size=10)

x_batch = x_train[:3]

5.7

误差反向

传播法的实现 159

t_batch = t_train[:3]

grad_numerical

= network.numerical_gradient(x_batch, t_batch)

grad_backprop = network.gradient(x_batch,

t_batch)

# 求各个权

重的绝对误差的平均值

for key in grad_numerical.keys():

diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key])

)

 print(key + ":" +

str(diff))

和以前一样，读入MNIST数据集

。然后，使用训练数据的一

部分，确

认数值微分求出

的梯度和误差反向传播

法求出的梯度的误差。这

里误差的计

算方法是求

各个权重参数中对应元

素的差的绝对值，并计算

其平均值。运行

上面的代

码后，会输出如下结果。

b1:9.70418809871e-13

W2:8.41139039497e-13

b2:1.1945999745e-10

W1:2.2232446644e-13

从

这个结果可以看出，通过

数值微分和误差反向传

播法求出的梯度的差

非

常小。比如，第1层的偏置的

误差是9.7e-13（0.00000000000097）。这样一来，

我们就

知道了通过误差反向传

播法求出的梯度是正确

的，误差反向传播法的

实

现没有错误。

数值微分和

误差反向传播法的计算

结果之间的误差为 0是很

少见的。

这是因为计算机

的计算精度有限（比如，32位

浮点数）。受到数值精

度的

限制，刚才的误差一般不

会为 0，但是如果实现正确

的话，可

以期待这个误差

是一个接近 0的很小的值

。如果这个值很大，就说

明

误差反向传播法的实现

存在错误。

5.7.4　使用误差反向

传播法的学习

最后，我们

来看一下使用了误差反

向传播法的神经网络的

学习的实现。

和之前的实

现相比，不同之处仅在于

通过误差反向传播法求

梯度这一点。这

里只列出

了代码，省略了说明（源代

码在ch05/train_neuralnet.py中）。

160 第 5章　误差反向传

播法

import sys, os

sys.path.append(os.pardir)

import numpy

as np

from dataset.mnist import load_mnist

from two_layer_net import TwoLayerNet

# 读入数据

(x_train, t_train), (x_test, t_test) = \

load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50,

output_size=10)

iters_num = 10000

train_size =

x_train.shape[0]

batch_size = 100

learning_rate =

0.1

train_loss_list = []

train_acc_list =

[]

test_acc_list = []

iter_per_epoch =

max(train_size / batch_size, 1)

for i

in range(iters_num):

 batch_mask = np.random.choice(train_size,

batch_size)

 x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

 # 通过误差

反向传播法求梯度

grad = network.gradient(x_batch, t_batch)

 #

更新

for key in ('W1', 'b1',

'W2', 'b2'):

 network.params[key] -= learning_rate

* grad[key]

 loss = network.loss(x_batch,

t_batch)

 train_loss_list.append(loss)

 if i

% iter_per_epoch == 0:

 train_acc

= network.accuracy(x_train, t_train)

 test_acc =

network.accuracy(x_test, t_test)

 train_acc_list.append(train_acc)

 test_acc_list.append(test_acc)

print(train_acc, test_acc)

5.8  小结 161

5.8 小结

本章我们介绍

了将计算过程可视化的

计算图，并使用计算图，介

绍了神

经网络中的误差

反向传播法，并以层为单

位实现了神经网络中的

处理。我们

学过的层有ReLU层

、Softmax-with-Loss层、Affine层、Softmax层等，这

些层中实现

了forward和backward方法，通过将数据正

向和反向地传播，可

以高

效地计算权重参数的梯

度。通过使用层进行模块

化，神经网络中可以自

由

地组装层，轻松构建出自

己喜欢的网络。

本章所学

的内容

• 通过使用计算图

，可以直观地把握计算过

程。

•

计算图的节点是由局

部计算构成的。局部计算

构成全局计算。

• 计算图的

正向传播进行一般的计

算。通过计算图的反向传

播，可以

计算各个节点的

导数。

• 通过将神经网络的

组成元素实现为层，可以

高效地计算梯度（反向传

播法）。

• 通过比较数值微分

和误差反向传播法的结

果，可以确认误差反向传

播法的实现是否正确（梯

度确认）。



第6章

与学习相关

的技巧

本章将介绍神经

网络的学习中的一些重

要观点，主题涉及寻找最

优权重

参数的最优化方

法、权重参数的初始值、超

参数的设定方法等。此外

，为了

应对过拟合，本章还

将介绍权值衰减、Dropout等正则

化方法，并进行实现。

最后

将对近年来众多研究中

使用的Batch Normalization方法进行简单的

介绍。

使用本章介绍的方

法，可以高效地进行神经

网络（深度学习）的学习，提

高

识别精度。让我们一起

往下看吧！

6.1 参数的更新

神

经网络的学习的目的是

找到使损失函数的值尽

可能小的参数。这是寻

找

最优参数的问题，解决这

个问题的过程称为最优

化（optimization）。遗憾的是，

神经网络的

最优化问题非常难。这是

因为参数空间非常复杂

，无法轻易找到

最优解（无

法使用那种通过解数学

式一下子就求得最小值

的方法）。而且，在

深度神经

网络中，参数的数量非常

庞大，导致最优化问题更

加复杂。

在前几章中，为了

找到最优参数，我们将参

数的梯度（导数）作为了线

索。

使用参数的梯度，沿梯

度方向更新参数，并重复

这个步骤多次，从而逐渐

靠

近最优参数，这个过程

称为随机梯度下降法（stochastic gradient descent），

简

称SGD。SGD是一个简单的方法，不

过比起胡乱地搜索参数

空间，也算是“聪

明”的方法

。但是，根据不同的问题，也

存在比SGD更加聪明的方法

。本节

164   第

6章　与学习相关的

技巧

我们将指出SGD的缺点

，并介绍SGD以外的其他最优

化方法。

6.1.1　探险家的故事

进

入正题前，我们先打一个

比方，来说明关于最优化

我们所处的状况。

有一个

性情古怪的探险家。他在

广袤的干旱地带旅行，坚

持寻找幽

深的山谷。他的

目标是要到达最深的谷

底（他称之为“至深之地”）。这

也是他旅行的目的。并且

，他给自己制定了两个严

格的“规定”：一个

是不看地

图；另一个是把眼睛蒙上

。因此，他并不知道最深的

谷底在这

个广袤的大地

的何处，而且什么也看不

见。在这么严苛的条件下

，这位

探险家如何前往“至

深之地”呢？他要如何迈步

，才能迅速找到“至深

之地

”呢？

寻找最优参数时，我们

所处的状况和这位探险

家一样，是一个漆黑的世

界。我们必须在没有地图

、不能睁眼的情况下，在广

袤、复杂的地形中寻找

“至

深之地”。大家可以想象这

是一个多么难的问题。

在

这么困难的状况下，地面

的坡度显得尤为重要。探

险家虽然看不到周

围的

情况，但是能够知道当前

所在位置的坡度（通过脚

底感受地面的倾斜状况

）。

于是，朝着当前所在位置

的坡度最大的方向前进

，就是SGD的策略。勇敢

的探险

家心里可能想着只要重

复这一策略，总有一天可

以到达“至深之地”。

6.1.2　SGD

让大家

感受了最优化问题的难

度之后，我们再来复习一

下SGD。用数

学式可以将SGD写成

如下的式（6.1）。

（6.1）

这里把需要更

新的权重参数记为W，把损

失函数关于W的梯度记为

。

η表示学习率，实际上会取

0.01或0.001这些事先决定好的值

。式子中的←

6.1 参数的更新  165

表

示用右边的值更新左边

的值。如式（6.1）所示，SGD是朝着梯

度方向只前

进一定距离

的简单方法。现在，我们将

SGD实现为一个Python类（为方便

后

面使用，我们将其实现为

一个名为SGD的类）。

class SGD:

 def

__init__(self, lr=0.01):

 self.lr = lr

def update(self, params, grads):

 for

key in params.keys():

 params[key] -=

self.lr * grads[key]

这里，进行

初始化时的参数lr表示learning rate（学

习率）。这个学习率

会保存

为实例变量。此外，代码段

中还定义了update(params,

grads)方法，

这个方

法在SGD中会被反复调用。参

数params和grads（与之前的神经网络

的实现一样）是字典型变

量，按params['W1']、grads['W1']的形式，分别保

存了

权重参数和它们的梯度

。

使用这个SGD类，可以按如下

方式进行神经网络的参

数的更新（下面的

代码是

不能实际运行的伪代码

）。

network

= TwoLayerNet(...)

optimizer = SGD()

for

i in range(10000):

 ...

x_batch, t_batch = get_mini_batch(...) # mini-batch

grads = network.gradient(x_batch, t_batch)

 params

= network.params

 optimizer.update(params, grads)

...

这里首次出现的变量名

optimizer表示“进行最优化的人”的

意思，这里

由SGD承担这个角

色。参数的更新由optimizer负责完

成。我们在这里需要

做的

只是将参数和梯度的信

息传给optimizer。

像这样，通过单独

实现进行最优化的类，功

能的模块化变得更简单

。

比如，后面我们马上会实

现另一个最优化方法Momentum，它

同样会实现

成拥有update(params, grads)这个

共同方法的形式。这样一

来，只需要将

166   第

6章　与学习

相关的技巧

optimizer = SGD()这一语句换

成optimizer =

Momentum()，就可以从SGD切

换为Momentum。

很多

深度学习框架都实现了

各种最优化方法，并且提

供了可以简单

切换这些

方法的构造。比如 Lasagne深度学

习框架，在updates.py

这个文件中以

函数的形式集中实现了

最优化方法。用户可以从

中选

择自己想用的最优

化方法。

6.1.3　SGD的缺点

虽然SGD简单

，并且容易实现，但是在解

决某些问题时可能没有

效率。

这里，在指出SGD的缺点

之际，我们来思考一下求

下面这个函数的最小值

的问题。

（6.2）

如图6-1所示，式（6.2）表示

的函数是向x轴方向延伸

的“碗”状函数。

实际上，式（6.2）的

等高线呈向x轴方向延伸

的椭圆状。

图6-1 的图形（左图

）和它的等高线（右图）

120

100

80

60

40

20

0

0

10

10

10

10

5

5

5

5 x x y

y

z

0 0 0 −5 −5

−5

−5 −10

−10 −10 −10

6.1 参数

的更新  167

现在看一下式（6.2）表

示的函数的梯度。如果用

图表示梯度的话，则如

图

6-2所示。这个梯度的特征是

，y轴方向上大，x轴方向上小

。换句话说，

就是y轴方向的

坡度大，而x轴方向的坡度

小。这里需要注意的是，虽

然式

（6.2）的最小值在(x, y) = (0, 0)处，但是

图6-2中的梯度在很多地方

并没有指

向(0, 0)。

图6-2 的梯度

4

−10

−5 0 5 10

2

0

−2

−4

y

x

我

们来尝试对图6-1这种形状

的函数应用SGD。从(x, y)

= (−7.0, 2.0)处

（初始值

）开始搜索，结果如图6-3所示

。

在图6-3中，SGD呈“之”字形移动。这

是一个相当低效的路径

。也就是说，

SGD的缺点是，如果

函数的形状非均向（anisotropic），比如

呈延伸状，搜索

的路径就

会非常低效。因此，我们需

要比单纯朝梯度方向前

进的SGD更聪

明的方法。SGD低效

的根本原因是，梯度的方

向并没有指向最小值的

方向。

为了改正SGD的缺点，下

面我们将介绍Momentum、AdaGrad、Adam这3

种方法

来取代SGD。我们会简单介绍

各个方法，并用数学式和

Python进行实现。

168

第 6章　与学习相

关的技巧

图6-3　基于SGD的最优

化的更新路径：呈“之”字形

朝最小值(0, 0)移动，效率低

10

−10 −5 0 5 10

5

0

−5

−10

y

x

SGD

6.1.4　Momentum

Momentum是

“动量”的意思，和物理有关

。用数学式表示Momentum方

法，如下

所示。

（6.3）

（6.4）

和前面的SGD一样，W表示

要更新的权重参数， 表示

损失函数关

于W的梯度，η表

示学习率。这里新出现了

一个变量v，对应物理上的

速度。

式（6.3）表示了物体在梯

度方向上受力，在这个力

的作用下，物体的速度增

加这一物理法则。如图6-4所

示，Momentum方法给人的感觉就像

是小球在

地面上滚动。

6.1  参

数的更新  169

图6-4

Momentum：小球在斜面

上滚动

式（6.3）中有αv这一项。在

物体不受任何力时，该项

承担使物体逐渐减

速的

任务（α设定为0.9之类的值），对

应物理上的地面摩擦或

空气阻力。下

面是Momentum的代码

实现（源代码在common/optimizer.py中）。

class Momentum:

def __init__(self, lr=0.01, momentum=0.9):

 self.lr

= lr

 self.momentum = momentum

self.v = None

 def update(self,

params, grads):

 if self.v is

None:

 self.v = {}

for key, val in params.items():

self.v[key] = np.zeros_like(val)

 for key

in params.keys():

 self.v[key] = self.momentum*self.v[key]

- self.lr*grads[key]

 params[key] += self.v[key]

实例变

量v会保存物体的速度。初

始化时，v中什么都不保存

，但当第

一次调用update()时，v会以

字典型变量的形式保存

与参数结构相同的数据

。

剩余的代码部分就是将

式（6.3）、式（6.4）写出来，很简单。

现在

尝试使用Momentum解决式（6.2）的最优

化问题，如图6-5所示。

图6-5中，更

新路径就像小球在碗中

滚动一样。和SGD相比，我们发

现

“之”字形的“程度”减轻了

。这是因为虽然x轴方向上

受到的力非常小，但

是一

直在同一方向上受力，所

以朝同一个方向会有一

定的加速。反过来，虽

然y轴

方向上受到的力很大，但

是因为交互地受到正方

向和反方向的力，它

们会

互相抵消，所以y轴方向上

的速度不稳定。因此，和SGD时

的情形相比，

可以更快地

朝x轴方向靠近，减弱“之”字

形的变动程度。

170

第 6章　与学

习相关的技巧

图6-5　基于Momentum的

最优化的更新路径

10

−10 −5 0 5 10

5

0

−5

−10

y

x

Momentum

6.1.5　AdaGrad

在神

经网络的学习中，学习率

（数学式中记为η）的值很重

要。学习率过小，

会导致学

习花费过多时间；反过来

，学习率过大，则会导致学

习发散而不能

正确进行

。

在关于学习率的有效技

巧中，有一种被称为学习

率衰减（learning

rate

decay）的方法，即随着学

习的进行，使学习率逐渐

减小。实际上，一开始“多”

学

，然后逐渐“少”学的方法，在

神经网络的学习中经常

被使用。

逐渐减小学习率

的想法，相当于将“全体”参

数的学习率值一起降低

。

而AdaGrad [6]进一步发展了这个想

法，针对“一个一个”的参数

，赋予其“定

制”的值。

AdaGrad会为参

数的每个元素适当地调

整学习率，与此同时进行

学习

（AdaGrad的Ada来自英文单词Adaptive，即

“适当的”的意思）。下面，让

我

们用数学式表示AdaGrad的更新

方法。

6.1 参数的更新

171

（6.5）

（6.6）

和前面

的SGD一样，W表示要更新的权

重参数， 表示损失函数关

于W的梯度，η表示学习率。这

里新出现了变量h，如式(6.5)所

示，它保

存了以前的所有

梯度值的平方和（式（6.5）中的

表示对应矩阵元素的乘

法）。

然后，在更新参数时，通

过乘以 ，就可以调整学习

的尺度。这意味着，

参数的

元素中变动较大（被大幅

更新）的元素的学习率将

变小。也就是说，

可以按参

数的元素进行学习率衰

减，使变动大的参数的学

习率逐渐减小。

AdaGrad会记录过

去所有梯度的平方和。因

此，学习越深入，更新

的幅

度就越小。实际上，如果无

止境地学习，更新量就会

变为

0，

完全不再更新。为了

改善这个问题，可以使用

RMSProp [7]方法。

RMSProp方法并不是将过去

所有的梯度一视同仁地

相加，而是逐渐

地遗忘过

去的梯度，在做加法运算

时将新梯度的信息更多

地反映出来。

这种操作从

专业上讲，称为“指数移动

平均”，呈指数函数式地减

小

过去的梯度的尺度。

现

在来实现 AdaGrad。AdaGrad 的 实 现

过 程 如

下 所 示（源 代

码 在

common/optimizer.py中）。

class AdaGrad:

def __init__(self, lr=0.01):

 self.lr =

lr

 self.h = None

def update(self, params, grads):

 if

self.h is None:

 self.h =

{}

 for key, val in

params.items():

 self.h[key] = np.zeros_like(val)

for key in params.keys():

 self.h[key]

+= grads[key] * grads[key]

 params[key]

-= self.lr * grads[key] / (np.sqrt(self.h[key])

+ 1e-7)

172   第

6章

与学习相关的技巧

这里

需要注意的是，最后一行

加上了微小值1e-7。这是为了

防止当

self.h[key]中有0时，将0用作除

数的情况。在很多深度学

习的框架中，这

个微小值

也可以设定为参数，但这

里我们用的是1e-7这个固定

值。

现在，让我们试着使用

AdaGrad解决式（6.2）的最优化问题，结

果如图6-6

所示。

图6-6　基于AdaGrad的最

优化的更新路径

10

−10 −5 0

5 10

5

0

−5

−10

y

x

AdaGrad

由图6-6的

结果可知，函数的取值高

效地向着最小值移动。由

于y轴方

向上的梯度较大

，因此刚开始变动较大，但

是后面会根据这个较大

的变动按

比例进行调整

，减小更新的步伐。因此，y轴

方向上的更新程度被减

弱，“之”

字形的变动程度有

所衰减。

6.1.6　Adam

Momentum参照小球在碗中

滚动的物理规则进行移

动，AdaGrad为参

数的每个元素适

当地调整更新步伐。如果

将这两个方法融合在一

起会怎么样

6.1

参数的更新

173

呢？这就是Adam[8]方法的基本思

路 A。

Adam是2015年提出的新方法。它

的理论有些复杂，直观地

讲，就是融

合了Momentum和AdaGrad的方法

。通过组合前面两个方法

的优点，有望

实现参数空

间的高效搜索。此外，进行

超参数的“偏置校正”也是

Adam的特征。

这里不再进行过

多的说明，详细内容请参

考原作者的论文[8]。关于Python

的

实现，common/optimizer.py中将其实现为了Adam类

，有兴趣的读者可以参考

。

现在，我们试着使用Adam解决

式（6.2）的最优化问题，结果如

图6-7所示。

图6-7　基于Adam的最优化

的更新路径

Adam

10

−10 −5 0 5

10

5

0

−5

−10

y

x

在图6-7中，基于

Adam的更新过程就像小球在

碗中滚动一样。虽然

Momentun也有

类似的移动，但是相比之

下，Adam的小球左右摇晃的程

度

有所减轻。这得益于学

习的更新程度被适当地

调整了。

A 这里关于Adam方法的

说明只是一个直观的说

明，并不完全正确。详细内

容请参考原作者的论文

。

174   第 6章　与学习相关的技巧

Adam会设置

3个超参数。一个是

学习率（论文中以α出现），另

外两

个是一次momentum系数β1和二

次momentum系数β2。根据论文，

标准的

设定值是β1为 0.9，β2 为 0.999。设置了这

些值后，大多数情

况下都

能顺利运行。

6.1.7　使用哪种更

新方法呢

到目前为止，我

们已经学习了4种更新参

数的方法。这里我们来比

较一

下这4种方法（源代码

在ch06/optimizer_compare_naive.py中）。

如图6-8所示，根据使用

的方法不同，参数更新的

路径也不同。只看这

个图

的话，AdaGrad似乎是最好的，不过

也要注意，结果会根据要

解决的问

题而变。并且，很

显然，超参数（学习率等）的

设定值不同，结果也会发

生变化。

图6-8　最优化方法的

比较：SGD、Momentum、AdaGrad、Adam

x

−10

−5 0 5 10

x

−10

−5 0 5 10

x

−10

−5 0 5 10

−10

−5

0

5

10

y

−10

−5

0

5

10

y

SGD Momentum

AdaGrad Adam

−10

−5

0

5

10

y

x

−10 −5 0

5 10

−10

−5

0

5

10

y

6.1 参数的更新  175

上面我

们介绍了SGD、Momentum、AdaGrad、Adam这4种方法，那

么

用哪种方法好呢？非常遗

憾，（目前）并不存在能在所

有问题中都表现良好

的

方法。这4种方法各有各的

特点，都有各自擅长解决

的问题和不擅长解决

的

问题。

很多研究中至今仍

在使用SGD。Momentum和AdaGrad也是值得一试

的方法。最近，很多研究人

员和技术人员都喜欢用

Adam。本书将主要使用

SGD或者Adam，读

者可以根据自己的喜好

多多尝试。

6.1.8　基于MNIST数据集的

更新方法的比较

我 们 以

手 写

数 字 识 别 为 例，比

较

前 面 介 绍 的 SGD、Momentum、

AdaGrad、Adam这4种方法，并

确认不同的方法在学习

进展上有多大程度

的差

异。先来看一下结果，如图

6-9所示（源代码在ch06/optimizer_compare_

mnist.py中）。

图6-9 基于

MNIST数据集的4种更新方法的

比较：横轴表示学习的迭

代次数（iteration），

纵轴表示损失函

数的值（loss）

1.0

0.8

0.6

0.4

0.2

0.0

0 500 1000

iterations

1500 2000

loss

Adam

SGD

AdaGrad

Momentum

这个实验以一个

5层神经网络为对象，其中

每层有100个神经元。激活

函

数使用的是ReLU。

176   第 6章

与学习

相关的技巧

从图6-9的结果

中可知，与SGD相比，其他3种方

法学习得更快，而且

速度

基本相同，仔细看的话，AdaGrad的

学习进行得稍微快一点

。这个实验

需要注意的地

方是，实验结果会随学习

率等超参数、神经网络的

结构（几层

深等）的不同而

发生变化。不过，一般而言

，与SGD相比，其他3种方法可

以

学习得更快，有时最终的

识别精度也更高。

6.2 权重的

初始值

在神经网络的学

习中，权重的初始值特别

重要。实际上，设定什么样

的

权重初始值，经常关系

到神经网络的学习能否

成功。本节将介绍权重初

始值

的推荐值，并通过实

验确认神经网络的学习

是否会快速进行。

6.2.1

可以将

权重初始值设为0吗

后面

我们会介绍抑制过拟合

、提高泛化能力的技巧——权

值衰减（weight

decay）。简单地说，权值衰

减就是一种以减小权重

参数的值为目的进行学

习

的方法。通过减小权重

参数的值来抑制过拟合

的发生。

如果想减小权重

的值，一开始就将初始值

设为较小的值才是正途

。实际上，

在这之前的权重

初始值都是像0.01

* np.random.randn(10, 100)这样，使用

由高斯分布生成的值乘

以0.01后得到的值（标准差为

0.01的高斯分布）。

如果我们把

权重初始值全部设为0以

减小权重的值，会怎么样

呢？从结

论来说，将权重初

始值设为0不是一个好主

意。事实上，将权重初始值

设为

0的话，将无法正确进

行学习。

为什么不能将权

重初始值设为0呢？严格地

说，为什么不能将权重初

始

值设成一样的值呢？这

是因为在误差反向传播

法中，所有的权重值都会

进行

相同的更新。比如，在

2层神经网络中，假设第1层

和第2层的权重为0。这

样一

来，正向传播时，因为输入

层的权重为0，所以第2层的

神经元全部会

被传递相

同的值。第2层的神经元中

全部输入相同的值，这意

味着反向传播

时第2层的

权重全部都会进行相同

的更新（回忆一下“乘法节

点的反向传播”

6.2 权重的初

始值  177

的内容）。因此，权重被

更新为相同的值，并拥有

了对称的值（重复的值）。

这

使得神经网络拥有许多

不同的权重的意义丧失

了。为了防止“权重均一化

”

（严格地讲，是为了瓦解权

重的对称结构），必须随机

生成初始值。

6.2.2　隐藏层的激

活值的分布

观察隐藏层

的激活值 A

（激活函数的输

出数据）的分布，可以获得

很多启

发。这里，我们来做

一个简单的实验，观察权

重初始值是如何影响隐

藏层的

激活值的分布的

。这里要做的实验是，向一

个5层神经网络（激活函数

使用

sigmoid函数）传入随机生成

的输入数据，用直方图绘

制各层激活值的数据分

布。这个实验参考了斯坦

福大学的课程CS231n [5]。

进行实验

的源代码在ch06/weight_init_activation_histogram.py中，下

面展示

部分代码。

import numpy as np

import matplotlib.pyplot

as plt

def sigmoid(x):

 return

1 / (1 + np.exp(-x))

x

= np.random.randn(1000, 100) # 1000个数据

node_num

= 100 # 各隐藏

层的节点（神经元）数

hidden_layer_size =

5 # 隐藏

层有5层

activations = {}

# 激活值的结果保

存在这里

for i in range(hidden_layer_size):

if i != 0:

 x

= activations[i-1]

 w = np.random.randn(node_num,

node_num) * 1

 z =

np.dot(x, w)

 a = sigmoid(z)

# sigmoid函数

 activations[i] = a

A 这里我们

将激活函数的输出数据

称为“激活值”，但是有的文

献中会将在层之间流动

的数据也称为“激

活值”。

178

第

6章　与学习相关的技巧

这

里假设神经网络有5层，每

层有100个神经元。然后，用高

斯分布随

机生成1000个数据

作为输入数据，并把它们

传给5层神经网络。激活函

数使

用sigmoid函数，各层的激活

值的结果保存在activations变量中

。这个代码

段中需要注意

的是权重的尺度。虽然这

次我们使用的是标准差

为1的高斯分

布，但实验的

目的是通过改变这个尺

度（标准差），观察激活值的

分布如何变

化。现在，我们

将保存在activations中的各层数据

画成直方图。

# 绘制直方图

for i, a

in activations.items():

 plt.subplot(1, len(activations), i+1)

plt.title(str(i+1) + "-layer")

 plt.hist(a.flatten(), 30,

range=(0,1))

plt.show()

运行这段代码后，可以得

到图6-10的直方图。

图6-10　使用标

准差为1的高斯分布作为

权重初始值时的各层激

活值的分布

40000

35000

30000

25000

20000

15000

10000

5000

1-layer 2-layer 3-layer 4-layer 5-layer

0

0.0 0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4 0.6 0.8

1.0 0.0 0.2 0.4 0.6 0.8

1.0

从图6-10可知，各

层的激活值呈偏向0和1的

分布。这里使用的sigmoid

函数是

S型函数，随着输出不断地

靠近0（或者靠近1），它的导数

的值逐渐接

近0。因此，偏向

0和1的数据分布会造成反

向传播中梯度的值不断

变小，最

后消失。这个问题

称为梯度消失（gradient vanishing）。层次加深

的深度学习

中，梯度消失

的问题可能会更加严重

。

下面，将权重的标准差设

为0.01，进行相同的实验。实验

的代码只需要

把设定权

重初始值的地方换成下

面的代码即可。

6.2 权重的初

始值

179

# w = np.random.randn(node_num, node_num)

* 1

w = np.random.randn(node_num, node_num)

* 0.01

来看一下结果。使用

标准差为0.01的高斯分布时

，各层的激活值的分布

如

图6-11所示。

图6-11　使用标准差为

0.01的高斯分布作为权重初

始值时的各层激活值的

分布

40000

35000

30000

25000

20000

15000

10000

5000

1-layer 2-layer 3-layer 4-layer

5-layer

0

0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2 0.4 0.6

0.8 1.0 0.0 0.2 0.4 0.6

0.8 1.0

这次呈集中在0.5附近

的分布。因为不像刚才的

例子那样偏向0和1，所

以不

会发生梯度消失的问题

。但是，激活值的分布有所

偏向，说明在表现力

上会

有很大问题。为什么这么

说呢？因为如果有多个神

经元都输出几乎相同

的

值，那它们就没有存在的

意义了。比如，如果100个神经

元都输出几乎相

同的值

，那么也可以由1个神经元

来表达基本相同的事情

。因此，激活值在

分布上有

所偏向会出现“表现力受

限”的问题。

各层的激活值

的分布都要求有适当的

广度。为什么呢？因为通过

在各层间传递多样性的

数据，神经网络可以进行

高效的学习。反

过来，如果

传递的是有所偏向的数

据，就会出现梯度消失或

者“表

现力受限”的问题，导

致学习可能无法顺利进

行。

接着，我们尝试使用Xavier

Glorot等

人的论文[9]中推荐的权重

初始值（俗

称“Xavier初始值”）。现在

，在一般的深度学习框架

中，Xavier初始值已被

作为标准

使用。比如，Caffe框架中，通过在

设定权重初始值时赋予

xavier参数，

就可以使用Xavier初始值

。

Xavier的论文中，为了使各层的

激活值呈现出具有相同

广度的分布，推

180

第 6章　与学

习相关的技巧

导了合适

的权重尺度。推导出的结

论是，如果前一层的节点

数为n，则初始

值使用标准

差为

的分布A

（图6-12）。

图6-12 Xavier初始值

：与前一层有n个节点连接

时，初始值使用标准差为

的分布

n 个节点

使用标准

差为 的高斯分布进行初

始化

使用Xavier初始值后，前一

层的节点数越多，要设定

为目标节点的初始

值的

权重尺度就越小。现在，我

们使用Xavier初始值进行实验

。进行实验的

代码只需要

将设定权重初始值的地

方换成如下内容即可（因

为此处所有层的

节点数

都是100，所以简化了实现）。

node_num = 100 # 前

一层的节点数

w

= np.random.randn(node_num, node_num) / np.sqrt(node_num)

使用Xavier初始

值后的结果如图6-13所示。从

这个结果可知，越是后

面

的层，图像变得越歪斜，但

是呈现了比之前更有广

度的分布。因为各层间

传

递的数据有适当的广度

，所以sigmoid函数的表现力不受

限制，有望进行

高效的学

习。

A Xavier的论文中提出的设定

值，不仅考虑了前一层的

输入节点数量，还考虑了

下一层的输出节点数量

。

但是，Caffe等框架的实现中进

行了简化，只使用了这里

所说的前一层的输入节

点进行计算。

6.2 权重的初始

值  181

图6-13　使用Xavier初始值作为权

重初始值时的各层激活

值的分布

6000

5000

4000

3000

2000

1000

1-layer 2-layer 3-layer 4-layer 5-layer

0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

图 6-13的分布中，后

面的层的分布呈稍微歪

斜的形状。如果用tanh

函数（双

曲线函数）代替sigmoid函数，这个

稍微歪斜的问题就能得

到改善。实际上，使用tanh函数

后，会呈漂亮的吊钟型分

布。tanh

函数和sigmoid函数同是 S型曲

线函数，但tanh函数是关于原

点(0, 0)

对称的 S型曲线，而sigmoid函数

是关于(x, y)=(0, 0.5)对称的S型曲

线。众

所周知，用作激活函数的

函数最好具有关于原点

对称的性质。

6.2.3

ReLU的权重初始

值

Xavier初始值是以激活函数

是线性函数为前提而推

导出来的。因为

sigmoid函数和tanh函

数左右对称，且中央附近

可以视作线性函数，所以

适

合使用Xavier初始值。但当激

活函数使用ReLU时，一般推荐

使用ReLU专

用的初始值，也就

是Kaiming He等人推荐的初始值，也

称为“He初始值”[10]。

当前一层的

节点数为n时，He初始值使用

标准差为 的高斯分布。当

Xavier初始值是 时，（直观上）可以

解释为，因为ReLU的负值区域

的值

为0，为了使它更有广

度，所以需要2倍的系数。

现

在来看一下激活函数使

用ReLU时激活值的分布。我们

给出了3个实

验的结果（图

6-14），依次是权重初始值为标

准差是0.01的高斯分布（下文

简

写为“std = 0.01”）时、初始值为Xavier初始

值时、初始值为ReLU专用的

“He初

始值”时的结果。

182

第 6章　与学

习相关的技巧

图6-14　激活函

数使用ReLU时,不同权重初始

值的激活值分布的变化

权重初始值为标准差是

0.01 的高斯分布时

权重初始

值为 Xavier 初始值时

权重初始

值为 He 初始值时

0.0 0.2 0.4 0.6 0.8 1.0

0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0 0.2 0.4

0.6 0.8 1.0

2-layer

0.0 0.2

0.4 0.6 0.8 1.0

3-layer

0.0

0.2 0.4 0.6 0.8 1.0

4-layer

0.0 0.2 0.4 0.6 0.8 1.0

5-layer

0.0 0.2 0.4 0.6 0.8

1.0 0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0 0.2

0.4 0.6 0.8 1.0

2-layer

0.0

0.2 0.4 0.6 0.8 1.0

3-layer

0.0 0.2 0.4 0.6 0.8 1.0

4-layer

0.0 0.2 0.4 0.6 0.8

1.0

5-layer

0.0 0.2 0.4 0.6

0.8 1.0 0

1000

2000

3000

4000

5000

6000

7000 1-layer

0.0

0.2 0.4 0.6 0.8 1.0

2-layer

0.0 0.2 0.4 0.6 0.8 1.0

3-layer

0.0 0.2 0.4 0.6 0.8

1.0

4-layer

0.0 0.2 0.4 0.6

0.8 1.0

5-layer

观察实验

结果可知，当“std = 0.01”时，各层的激

活值非常小

A。神经网

络上

传递的是非常小的值，说

明逆向传播时权重的梯

度也同样很小。这是很

严

重的问题，实际上学习基

本上没有进展。

A 各层激活

值的分布平均值如下。1层

: 0.0396，2层:

0.00290，3层: 0.000197，4层: 1.32e-5，5层: 9.46e-7。

6.2 权重的初始值



183

接下来是初始值为Xavier初始

值时的结果。在这种情况

下，随着层的加深，

偏向一

点点变大。实际上，层加深

后，激活值的偏向变大，学

习时会出现梯

度消失的

问题。而当初始值为He初始

值时，各层中分布的广度

相同。由于

即便层加深，数

据的广度也能保持不变

，因此逆向传播时，也会传

递合适的值。

总结一下，当

激活函数使用ReLU时，权重初

始值使用He初始值，当

激活

函数为sigmoid或tanh等S型曲线函数

时，初始值使用Xavier初始值。

这

是目前的最佳实践。

6.2.4　基于

MNIST数据集的权重初始值的

比较

下面通过实际的数

据，观察不同的权重初始

值的赋值方法会在多大

程度

上影响神经网络的

学习。这里，我们基于std

= 0.01、Xavier初始

值、He初

始值进行实验（源代

码在ch06/weight_init_compare.py中）。先来看一下结果

，

如图6-15所示。

2.5

2.0

1.5

loss

1.0

0.5

0.0

0

500

iterations

1000 1500 2000

He

std = 0.01

Xavier

图6-15 基于MNIST数据集

的权重初始值的比较：横

轴是学习的迭代次数（iterations），

纵

轴是损失函数的值（loss）

184   第 6章

与学习相关的技巧

这个

实验中，神经网络有5层，每

层有100个神经元，激活函数

使用的

是ReLU。从图6-15的结果可

知，std = 0.01时完全无法进行学习

。这和刚

才观察到的激活

值的分布一样，是因为正

向传播中传递的值很小

（集中在0

附近的数据）。因此

，逆向传播时求到的梯度

也很小，权重几乎不进行

更新。

相反，当权重初始值

为Xavier初始值和He初始值时，学

习进行得很顺利。

并且，我

们发现He初始值时的学习

进度更快一些。

综上，在神

经网络的学习中，权重初

始值非常重要。很多时候

权重初始

值的设定关系

到神经网络的学习能否

成功。权重初始值的重要

性容易被忽视，

而任何事

情的开始（初始值）总是关

键的，因此在结束本节之

际，再次强调

一下权重初

始值的重要性。

6.3 Batch Normalization

在上一节

，我们观察了各层的激活

值分布，并从中了解到如

果设定了合

适的权重初

始值，则各层的激活值分

布会有适当的广度，从而

可以顺利地进

行学习。那

么，为了使各层拥有适当

的广度，“强制性”地调整激

活值的分布

会怎样呢？实

际上，Batch Normalization[11]方法就是基于这个

想法而产生的。

6.3.1　Batch Normalization 的算法

Batch Normalization（下

文简称 Batch Norm）是 2015 年提出的方法

。

Batch Norm虽然是一个问世不久的

新方法，但已经被很多研

究人员和技术

人员广泛

使用。实际上，看一下机器

学习竞赛的结果，就会发

现很多通过使

用这个方

法而获得优异结果的例

子。

为什么Batch Norm这么惹人注目

呢？因为Batch

Norm有以下优点。

• 可以

使学习快速进行（可以增

大学习率）。

• 不那么依赖初

始值（对于初始值不用那

么神经质）。

•

抑制过拟合（降

低Dropout等的必要性）。

6.3  Batch Normalization

185

考虑到深

度学习要花费很多时间

，第一个优点令人非常开

心。另外，后

两点也可以帮

我们消除深度学习的学

习中的很多烦恼。

如前所

述，Batch Norm的思路是调整各层的

激活值分布使其拥有适

当

的广度。为此，要向神经

网络中插入对数据分布

进行正规化的层，即Batch

Normalization层（下

文简称Batch Norm层），如图6-16所示。

图6-16　使

用了Batch Normalization的神经网络的例子

（Batch Norm层的背景为灰色）

Affine ReLU Batch

Norm Affine ReLU

Batch

Norm Affine Softmax

Batch Norm，顾名思

义，以进行学习时的mini-batch为单

位，按minibatch进行正规化。具体而

言，就是进行使数据分布

的均值为0、方差为1的

正规

化。用数学式表示的话，如

下所示。

（6.7）

这里对mini-batch的m个输入

数据的集合B = {x1, x2,

... , xm}求均值

µB和方

差 。然后，对输入数据进行

均值为0、方差为1（合适的分

布）的

正规化。式（6.7）中的ε是一

个微小值（比如，10e-7等），它是为

了防止出现

除以0的情况

。

式（6.7）所做的是将mini-batch的输入数

据{x1, x2, ... , xm}变换为均值

186   第 6章　与学

习相关的技巧

为0、方差为

1的数据 ，非常简单。通过将

这个处理插入到

激活函

数的前面（或者后面）A，可以

减小数据分布的偏向。

接

着，Batch Norm层会对正规化后的数

据进行缩放和平移的变

换，用

数学式可以如下表

示。

（6.8）

这里，γ和β是参数。一开始

γ = 1，β = 0，然后再通过学习调整到

合

适的值。

上面就是Batch Norm的算

法。这个算法是神经网络

上的正向传播。如

果使用

第5章介绍的计算图，Batch Norm可以

表示为图6-17。

图6-17

Batch Normalization的计算图（引

用自文献[13]）

x

(N,D) (N,D)

x

− * * +

ˆ2

dx

r (D, )

r dr

β

(D, )

β dβ

x−ε x

1

out

dout

Batch Norm的反向传播的

推导有些复杂，这里我们

不进行介绍。不过

如果使

用图6-17的计算图来思考的

话，Batch

Norm的反向传播或许也能

比较

轻松地推导出来。Frederik Kratzert 的

博客“Understanding the backward

pass through Batch Normalization Layer”[13]里有详细说明，感兴

趣的读者

可以参考一下

。

6.3.2　Batch Normalization的评估

现在我们使用Batch Norm层

进行实验。首先，使用MNIST数据

集，

A

文献[11]、文献[12]等中有讨论

（做过实验）应该把Batch Normalization插入到

激活函数的前面还是

后

面。

6.3 Batch Normalization

187

观察使用Batch Norm层和不使用

Batch Norm层时学习的过程会如何

变化（源

代码在ch06/batch_norm_test.py中），结果如

图6-18所示。

Training

Accuracy

accuracy

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

5 10 15 20

epochs

Batch

Normalization

Normal(without BatchNorm)

图6-18　基于Batch Norm的效果：使

用Batch

Norm后，学习进行得更快了

从图6-18的结果可知，使用Batch Norm后

，学习进行得更快了。接着

，

给予不同的初始值尺度

，观察学习的过程如何变

化。图6-19是权重初始值的

标

准差为各种不同的值时

的学习过程图。

我们发现

，几乎所有的情况下都是

使用Batch Norm时学习进行得更快

。

同时也可以发现，实际上

，在不使用Batch Norm的情况下，如果

不赋予一

个尺度好的初

始值，学习将完全无法进

行。

综上，通过使用Batch Norm，可以推

动学习的进行。并且，对权

重初

始值变得健壮（“对初

始值健壮”表示不那么依

赖初始值）。Batch

Norm具备

了如此优

良的性质，一定能应用在

更多场合中。

188   第

6章　与学习

相关的技巧

图6-19 图中的实

线是使用了Batch Norm时的结果，虚

线是没有使用Batch Norm时

的结果

：图的标题处标明了权重

初始值的标准差

w:1.0 w:0.541169526546 w:0.292864456463 w:0.158483319246

w:0.0857695898591

w:0.0464158883361 w:0.0251188643151 w:0.0125935639088

w:0.0073564225446 w:0.00398107170553 w:0.00215443469003

w:0.00116591440118

w:0.00063095734448 w:0.000341454887383 w:0.000184784979742 w:0.0001

0.0

1.0

0.8

0.6

0.4

0.2

accuracy

0.0

1.0

0.8

0.6

0.4

0.2

accuracy 0.0

1.0

0.8

0.6

0.4

0.2

accuracy 0.00

epochs

5 10

15 20

1.0

0.8

0.6

0.4

0.2

accuracy

0

epochs

5 10

15 20 0

epochs

5 10

15 20 0

epochs

5 10

15 20

6.4 正则化

机器学习的问题中，过拟

合是一个很常见的问题

。过拟合指的是只能拟

合

训练数据，但不能很好地

拟合不包含在训练数据

中的其他数据的状态。机

器学习的目标是提高泛

化能力，即便是没有包含

在训练数据里的未观测

数据，

也希望模型可以进

行正确的识别。我们可以

制作复杂的、表现力强的

模型，

6.4 正则化  189

但是相应地

，抑制过拟合的技巧也很

重要。

6.4.1

过拟合

发生过拟合

的原因，主要有以下两个

。

• 模型拥有大量参数、表现

力强。

• 训练数据少。

这里，我

们故意满足这两个条件

，制造过拟合现象。为此，要

从

MNIST数据集原本的60000个训练

数据中只选定300个，并且，为

了增加网

络的复杂度，使

用7层网络（每层有100个神经

元，激活函数为ReLU）。

下面是用

于实验的部分代码（对应

文件在ch06/overfit_weight_decay.py

中）。首先是用于读

入数据的代码。

(x_train,

t_train), (x_test, t_test) = load_mnist(normalize=True)

#

为了再现

过拟合，减少学习数据

x_train = x_train[:300]

t_train =

t_train[:300]

接

着是进行训练的代码。和

之前的代码一样，按epoch分别

算出所有训

练数据和所

有测试数据的识别精度

。

network = MultiLayerNet(input_size=784,

hidden_size_list=[100, 100, 100,

100, 100, 100],

output_size=10)

optimizer = SGD(lr=0.01) # 用学习率为0.01的SGD更新参数

max_epochs

= 201

train_size = x_train.shape[0]

batch_size

= 100

train_loss_list = []

train_acc_list

= []

test_acc_list = []

iter_per_epoch

= max(train_size / batch_size, 1)

epoch_cnt

= 0

for i in range(1000000000):

batch_mask = np.random.choice(train_size, batch_size)

190

第 6章　与学习相关的技巧

x_batch = x_train[batch_mask]

t_batch = t_train[batch_mask]

 grads =

network.gradient(x_batch, t_batch)

 optimizer.update(network.params, grads)

if i % iter_per_epoch == 0:

train_acc = network.accuracy(x_train, t_train)

 test_acc

= network.accuracy(x_test, t_test)

 train_acc_list.append(train_acc)

test_acc_list.append(test_acc)

 epoch_cnt += 1

if epoch_cnt >= max_epochs:

 break

train_acc_list和test_acc_list中以epoch为单位（看完了所

有训练数据

的单位）保存

识别精度。现在，我们将这

些列表（train_acc_list、test_acc_

list）绘成图，结果如图

6-20所示。

图6-20　训练数据（train）和测试

数据（test）的识别精度的变化

0.0

0

1.0

0.8

0.6

0.4

0.2

accuracy

50 100 150 200

epochs

train

test

6.4 正则化  191

过了

100 个 epoch 左右后，用

训练数据测量到的识别

精度几乎都为

100%。但是，对于

测试数据，离100%的识别精度

还有较大的差距。如此大

的识别精度差距，是只拟

合了训练数据的结果。从

图中可知，模型对训练时

没有使用的一般数据（测

试数据）拟合得不是很好

。

6.4.2

权值衰减

权值衰减是一

直以来经常被使用的一

种抑制过拟合的方法。该

方法通过

在学习的过程

中对大的权重进行惩罚

，来抑制过拟合。很多过拟

合原本就是

因为权重参

数取值过大才发生的。

复

习一下，神经网络的学习

目的是减小损失函数的

值。这时，例如为

损失函数

加上权重的平方范数（L2范

数）。这样一来，就可以抑制

权重变大。

用符号表示的

话，如果将权重记为W，L2范数

的权值衰减就是 ，然

后将

这个 加到损失函数上。这

里，λ是控制正则化强度的

超参数。λ

设置得越大，对大

的权重施加的惩罚就越

重。此外， 开头的

是用于

将

的求导结果变成λW的调整

用常量。

对于所有权重，权

值衰减方法都会为损失

函数加上 。因此，在求权

重

梯度的计算中，要为之前

的误差反向传播法的结

果加上正则化项的导数

λW。

L2范数相当于各个元素的

平方和。用数学式表示的

话，假设有权重

W = (w1, w2, ... ,

wn)，则 L2范数可

用 计算

出来。除了 L2范数，还

有 L1范数、L∞范数等。L1范数是各

个元

素的绝对值之和，相

当于|w1| + |w2| + ... +

|wn|。L∞范数也称为

Max范数，相

当于各个元素的绝对值

中最大的那一个。L2范数、L1

范

数、L∞范数都可以用作正则

化项，它们各有各的特点

，不过这里

我们要实现的

是比较常用的 L2范数。

现在

我们来进行实验。对于刚

刚进行的实验，应用λ

= 0.1的权

值衰减，

结果如图6-21所示（对

应权值衰减的网络在common/multi_layer_net.py中

，

用于实验的代码在ch06/overfit_weight_decay.py中）。

192

第

6章　与学习相关的技巧

0.0

1.0

0.8

0.6

0.4

0.2

accuracy

0 50 100

150 200

epochs

train

test

图

6-21

使用了权值衰减的训练

数据（train）和测试数据（test）的识别

精度的变化

如图6-21所示，虽

然训练数据的识别精度

和测试数据的识别精度

之间有

差距，但是与没有

使用权值衰减的图6-20的结

果相比，差距变小了。这说

明

过拟合受到了抑制。此

外，还要注意，训练数据的

识别精度没有达到100%（1.0）。

6.4.3　Dropout

作为

抑制过拟合的方法，前面

我们介绍了为损失函数

加上权重的L2范

数的权值

衰减方法。该方法可以简

单地实现，在某种程度上

能够抑制过拟合。

但是，如

果网络的模型变得很复

杂，只用权值衰减就难以

应对了。在这种情

况下，我

们经常会使用Dropout [14]方法。

Dropout是一

种在学习的过程中随机

删除神经元的方法。训练

时，随机

选出隐藏层的神

经元，然后将其删除。被删

除的神经元不再进行信

号的传递，

如图6-22所示。训练

时，每传递一次数据，就会

随机选择要删除的神经

元。

然后，测试时，虽然会传

递所有的神经元信号，但

是对于各个神经元的输

出，

要乘上训练时的删除

比例后再输出。

6.4

正则化  193

图

6-22 Dropout的概念图（引用自文献[14]）：左

边是一般的神经网络，右

边是应用了

Dropout的网络。Dropout通过

随机选择并删除神经元

，停止向前传递信号

下面

我们来实现Dropout。这里的实现

重视易理解性。不过，因为

训练

时如果进行恰当的

计算的话，正向传播时单

纯地传递数据就可以了

（不用乘

以删除比例），所以

深度学习的框架中进行

了这样的实现。关于高效

的实现，

可以参考Chainer中实现

的Dropout。

class Dropout:

def __init__(self, dropout_ratio=0.5):

 self.dropout_ratio =

dropout_ratio

 self.mask = None

def forward(self, x, train_flg=True):

 if

train_flg:

 self.mask = np.random.rand(*x.shape) >

self.dropout_ratio

 return x * self.mask

else:

 return x * (1.0

- self.dropout_ratio)

 def backward(self, dout):

return dout * self.mask

这里的要点是，每次正

向传播时，self.mask中都会以False的形

式保

存要删除的神经元

。self.mask会随机生成和x形状相同

的数组，并将值比

dropout_ratio大的元

素设为True。反向传播时的行

为和ReLU相同。也就是说，

正向

传播时传递了信号的神

经元，反向传播时按原样

传递信号；正向传播时

194   第

6章

与学习相关的技巧

没

有传递信号的神经元，反

向传播时信号将停在那

里。

现在，我们使用MNIST数据集

进行验证，以确认Dropout的效果

。源代

码在ch06/overfit_dropout.py中。另外，源代码

中使用了Trainer类来简化实现

。

common/trainer.py中实现了 Trainer类。这个类可以

负责前面所

进行的网络

的学习。详细内容可以参

照common/trainer.py和ch06/

overfit_dropout.py。

Dropout的实验和前面的实

验一样，使用7层网络（每层

有100个神经元，

激活函数为

ReLU），一个使用Dropout，另一个不使用

Dropout，实验的结

果如图6-23所示。

图

6-23

左边没有使用Dropout，右边使用

了Dropout（dropout_rate=0.15）

0 50 100 150 200

250 300

epochs

0.0

0.2

0.4

0.6

0.8

1.0

accuracy

train

test

train

test

0 50 100 150

200 250 300

epochs

0.0

0.2

0.4

0.6

0.8

1.0

accuracy

图6-23中，通过使用Dropout，训练数

据和测试数据的识别精

度的差距

变小了。并且，训

练数据也没有到达100%的识

别精度。像这样，通过使用

Dropout，即便是表现力强的网络

，也可以抑制过拟合。

机器

学习中经常使用集成学

习。所谓集成学习，就是让

多个模型单

独进行学习

，推理时再取多个模型的

输出的平均值。用神经网

络的

语境来说，比如，准备

5个结构相同（或者类似）的

网络，分别进行

学习，测试

时，以这 5个网络的输出的

平均值作为答案。实验告

诉我们，

6.5 超参数的验证  195

通

过进行集成学习，神经网

络的识别精度可以提高

好几个百分点。

这个集成

学习与

Dropout有密切的关系。这

是因为可以将 Dropout

理解为，通

过在学习过程中随机删

除神经元，从而每一次都

让不同

的模型进行学习

。并且，推理时，通过对神经

元的输出乘以删除比

例

（比如，0.5等），可以取得模型的

平均值。也就是说，可以理

解成，

Dropout将集成学习的效果

（模拟地）通过一个网络实

现了。

6.5 超参数的验证

神经

网络中，除了权重和偏置

等参数，超参数（hyper-parameter）也经

常出

现。这里所说的超参数是

指，比如各层的神经元数

量、batch大小、参

数更新时的学

习率或权值衰减等。如果

这些超参数没有设置合

适的值，模型

的性能就会

很差。虽然超参数的取值

非常重要，但是在决定超

参数的过程中

一般会伴

随很多的试错。本节将介

绍尽可能高效地寻找超

参数的值的方法。

6.5.1　验证数

据

之前我们使用的数据

集分成了训练数据和测

试数据，训练数据用于学

习，

测试数据用于评估泛

化能力。由此，就可以评估

是否只过度拟合了训练

数据

（是否发生了过拟合

），以及泛化能力如何等。

下

面我们要对超参数设置

各种各样的值以进行验

证。这里要注意的是，

不能

使用测试数据评估超参

数的性能。这一点非常重

要，但也容易被忽视。

为什

么不能用测试数据评估

超参数的性能呢？这是因

为如果使用测试数

据调

整超参数，超参数的值会

对测试数据发生过拟合

。换句话说，用测试数

据确

认超参数的值的“好坏”，就

会导致超参数的值被调

整为只拟合测试数据。

这

样的话，可能就会得到不

能拟合其他数据、泛化能

力低的模型。

因此，调整超

参数时，必须使用超参数

专用的确认数据。用于调

整超参

数的数据，一般称

为验证数据（validation data）。我们使用这

个验证数据来

评估超参

数的好坏。

196

第 6章　与学习相

关的技巧

训练数据用于

参数（权重和偏置）的学习

，验证数据用于超参数的

性

能评估。为了确认泛化

能力，要在最后使用（比较

理想的是只用一次）

测试

数据。

根据不同的数据集

，有的会事先分成训练数

据、验证数据、测试数据三

部分，有的只分成训练数

据和测试数据两部分，有

的则不进行分割。在这种

情况下，用户需要自行进

行分割。如果是MNIST数据集，获

得验证数据的

最简单的

方法就是从训练数据中

事先分割20%作为验证数据

，代码如下所示。

(x_train, t_train), (x_test, t_test)

= load_mnist()

# 打乱训练

数据

x_train, t_train

= shuffle_dataset(x_train, t_train)

# 分割验证数据

validation_rate

= 0.20

validation_num = int(x_train.shape[0] *

validation_rate)

x_val = x_train[:validation_num]

t_val =

t_train[:validation_num]

x_train = x_train[validation_num:]

t_train =

t_train[validation_num:]

这里

，分割训练数据前，先打乱

了输入数据和教师标签

。这是因为数据

集的数据

可能存在偏向（比如，数据

从“0”到“10”按顺序排列等）。这里

使

用的shuffle_dataset函数利用了np.random.shuffle，在common/util.py中

有

它的实现。

接下来，我们

使用验证数据观察超参

数的最优化方法。

6.5.2　超参数

的最优化

进行超参数的

最优化时，逐渐缩小超参

数的“好值”的存在范围非

常重要。

所谓逐渐缩小范

围，是指一开始先大致设

定一个范围，从这个范围

中随机选

出一个超参数

（采样），用这个采样到的值

进行识别精度的评估；然

后，多次

重复该操作，观察

识别精度的结果，根据这

个结果缩小超参数的“好

值”的范围。

通过重复这一

操作，就可以逐渐确定超

参数的合适范围。

6.5 超参数

的验证  197

有报告[15]显示，在进

行神经网络的超参数的

最优化时，与网格搜索

等

有规律的搜索相比，随机

采样的搜索方式效果更

好。这是因为在

多个超参

数中，各个超参数对最终

的识别精度的影响程度

不同。

超参数的范围只要

“大致地指定”就可以了。所

谓“大致地指定”，是指

像0.001（10−3

）到

1000（103

）这样，以“10的阶乘”的尺度指

定范围（也表述

为“用对数

尺度（log scale）指定”）。

在超参数的最

优化中，要注意的是深度

学习需要很长时间（比如

，几天

或几周）。因此，在超参

数的搜索中，需要尽早放

弃那些不符合逻辑的超

参数。

于是，在超参数的最

优化中，减少学习的epoch，缩短

一次评估所需的时间

是

一个不错的办法。

以上就

是超参数的最优化的内

容，简单归纳一下，如下所

示。

步骤0

设定超参数的范

围。

步骤1

从设定的超参数

范围中随机采样。

步骤2

使

用步骤1中采样到的超参

数的值进行学习，通过验

证数据评估识别精

度（但

是要将epoch设置得很小）。

步骤

3

重复步骤1和步骤2（100次等），根

据它们的识别精度的结

果，缩小超参

数的范围。

反

复进行上述操作，不断缩

小超参数的范围，在缩小

到一定程度时，从

该范围

中选出一个超参数的值

。这就是进行超参数的最

优化的一种方法。

198   第 6章

与

学习相关的技巧

这里介

绍的超参数的最优化方

法是实践性的方法。不过

，这个方

法与其说是科学

方法，倒不如说有些实践

者的经验的感觉。在超

参

数的最优化中，如果需要

更精炼的方法，可以使用

贝叶斯最优

化（Bayesian optimization）。贝叶斯最

优化运用以贝叶斯定理

为中

心的数学理论，能够

更加严密、高效地进行最

优化。详细内容请

参 考 论

文“Practical Bayesian Optimization

of Machine Learning

Algorithms”[16]等。

6.5.3　超参数最优化的实

现

现在，我们使用MNIST数据集

进行超参数的最优化。这

里我们将学习

率和控制

权值衰减强度的系数（下

文称为“权值衰减系数”）这

两个超参数的

搜索问题

作为对象。这个问题的设

定和解决思路参考了斯

坦福大学的课程

“CS231n”。

如前所

述，通过从 0.001（10−3

）到 1000（103

）这样的对数

尺度的范围

中随机采样

进行超参数的验证。这在

Python中可以写成10 ** np.random.

uniform(-3, 3)。在该实验中

，权值衰减系数的初始范

围为10−8

到10−4

，学

习率的初始范

围为10−6

到10−2

。此时，超参数的随

机采样的代码如下所示

。

weight_decay = 10 ** np.random.uniform(-8,

-4)

lr = 10 ** np.random.uniform(-6,

-2)

像这样进行随机采样后

，再使用那些值进行学习

。之后，多次使用各种

超参

数的值重复进行学习，观

察合乎逻辑的超参数在

哪里。这里省略了具体

实

现，只列出了结果。进行超

参数最优化的源代码在

ch06/hyperparameter_

optimization.py中，请大家自由参考。

下面

我们就以权值衰减系数

为10−8

到10−4

、学习率为10−6

到10−2

的范围

进行实验，结果如图6-24所示

。

6.5 超参数的验证

199

图6-24　实线是

验证数据的识别精度，虚

线是训练数据的识别精

度

Best-1 1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

Best-6 Best-7

Best-8 Best-9 Best-10

Best-11 Best-12 Best-13

Best-14 Best-15

Best-16 Best-17 Best-18 Best-19

Best-20

Best-2 Best-3 Best-4 Best-5

图6-24中，按识别精度从高

到低的顺序排列了验证

数据的学习的变化。

从图

中可知，直到“Best-5”左右，学习进

行得都很顺利。因此，我们

来

观察一下“Best-5”之前的超参

数的值（学习率和权值衰

减系数），结果如下

所示。

Best-1 (val acc:0.83)

| lr:0.0092, weight decay:3.86e-07

Best-2 (val

acc:0.78) | lr:0.00956, weight decay:6.04e-07

Best-3

(val acc:0.77) | lr:0.00571, weight decay:1.27e-06

Best-4 (val acc:0.74) | lr:0.00626, weight

decay:1.43e-05

Best-5 (val acc:0.73) | lr:0.0052,

weight decay:8.97e-06

从

这个结果可以看出，学习

率在0.001到0.01、权值衰减系数在

10−8

到

10−6

之间时，学习可以顺利

进行。像这样，观察可以使

学习顺利进行的超参

数

的范围，从而缩小值的范

围。然后，在这个缩小的范

围中重复相同的操作。

200   第

6章　与学习相关的技巧

这

样就能缩小到合适的超

参数的存在范围，然后在

某个阶段，选择一个最终

的超参数的值。

6.6 小结

本章

我们介绍了神经网络的

学习中的几个重要技巧

。参数的更新方法、

权重初

始值的赋值方法、Batch Normalization、Dropout等，这些

都是现代

神经网络中不

可或缺的技术。另外，这里

介绍的技巧，在最先进的

深度学习

中也被频繁使

用。

本章所学的内容

• 参 数

的

更 新 方 法，除 了 SGD

之 外，还

有 Momentum、AdaGrad、

Adam等方法。

• 权重初始值的

赋值方法对进行正确的

学习非常重要。

• 作为权重

初始值，Xavier初始值、He初始值等

比较有效。

• 通过使用Batch Normalization，可以

加速学习，并且对初始值

变得

健壮。

• 抑制过拟合的

正则化技术有权值衰减

、Dropout等。

• 逐渐缩小“好值”存在的

范围是搜索超参数的一

个有效方法。

第7章

卷积神

经网络

本章的主题是卷

积神经网络（Convolutional Neural Network，CNN）。

CNN被用于图像

识别、语音识别等各种场

合，在图像识别的比赛中

，基于

深度学习的方法几

乎都以CNN为基础。本章将详

细介绍CNN的结构，并用

Python实现

其处理内容。

7.1 整体结构

首

先，来看一下CNN的网络结构

，了解CNN的大致框架。CNN和之

前

介绍的神经网络一样，可

以像乐高积木一样通过

组装层来构建。不过，

CNN中新

出现了卷积层（Convolution层）和池化

层（Pooling层）。卷积层和

池化层将

在下一节详细介绍，这里

我们先看一下如何组装

层以构建CNN。

之前介绍的神

经网络中，相邻层的所有

神经元之间都有连接，这

称为全

连接（fully-connected）。另外，我们用

Affine层实现了全连接层。如果

使用

这个Affine层，一个5层的全

连接的神经网络就可以

通过图7-1所示的网络结

构

来实现。

如图7-1所示，全连接

的神经网络中，Affine层后面跟

着激活函数ReLU

层（或者Sigmoid层）。这

里堆叠了4层“Affine-ReLU”组合，然后第

5层是

Affine层，最后由Softmax层输出最

终结果（概率）。

202   第 7章

卷积神

经网络

图7-1　基于全连接层

（Affi ne层）的网络的例子

Affine ReLU

Affine ReLU Affine ReLU Affine ReLU

Affine Softmax

那么，CNN会

是什么样的结构呢？图7-2是

CNN的一个例子。

图7-2　基于CNN的网

络的例子：新增了Convolution层和Pooling层

（用灰色的方块表示）

Conv

ReLU Pooling Conv ReLU Pooling Conv

ReLU Affine ReLU Affine Softmax

如

图

7-2 所 示，CNN 中新增了 Convolution 层

和 Pooling 层。CNN 的

层的连接顺序是“Convolution - ReLU

-（Pooling）”（Pooling层有时

会被省

略）。这可以理解为

之前的“Affi ne - ReLU”连接被替换成了

“Convolution -

ReLU -（Pooling）”连接。

还需要注意的是，在

图7-2的CNN中，靠近输出的层中

使用了之前

的“Affi ne -

ReLU”组合。此外

，最后的输出层中使用了

之前的“Affi ne -

Softmax”组合。这些都是一

般的CNN中比较常见的结构

。

7.2 卷积层

CNN中出现了一些特

有的术语，比如填充、步幅

等。此外，各层中传

递的数

据是有形状的数据（比如

，3维数据），这与之前的全连

接网络不同，

因此刚开始

学习CNN时可能会感到难以

理解。本节我们将花点时

间，认真

学习一下CNN中使用

的卷积层的结构。

7.2

卷积层

203

7.2.1　全连接层存在的问题

之

前介绍的全连接的神经

网络中使用了全连接层

（Affine层）。在全连接

层中，相邻层

的神经元全部连接在一

起，输出的数量可以任意

决定。

全连接层存在什么

问题呢？那就是数据的形

状被“忽视”了。比如，输

入数

据是图像时，图像通常是

高、长、通道方向上的3维形

状。但是，向全

连接层输入

时，需要将3维数据拉平为

1维数据。实际上，前面提到

的使用

了MNIST数据集的例子

中，输入图像就是1通道、高

28像素、长28像素

的（1, 28, 28）形状，但却

被排成1列，以784个数据的形

式输入到最开始的

Affine层。

图

像是3维形状，这个形状中

应该含有重要的空间信

息。比如，空间上

邻近的像

素为相似的值、RBG的各个通

道之间分别有密切的关

联性、相距

较远的像素之

间没有什么关联等，3维形

状中可能隐藏有值得提

取的本质模

式。但是，因为

全连接层会忽视形状，将

全部的输入数据作为相

同的神经元

（同一维度的

神经元）处理，所以无法利

用与形状相关的信息。

而

卷积层可以保持形状不

变。当输入数据是图像时

，卷积层会以3维

数据的形

式接收输入数据，并同样

以3维数据的形式输出至

下一层。因此，

在CNN中，可以（有

可能）正确理解图像等具

有形状的数据。

另外，CNN 中，有

时将卷积层的输入输出

数据称为特征图（feature

map）。其中，卷

积层的输入数据称为输

入特征图（input

feature map），输出

数据称为

输出特征图（output feature map）。本书中将“输

入输出数据”和“特

征图”作

为含义相同的词使用。

7.2.2　卷

积运算

卷积层进行的处

理就是卷积运算。卷积运

算相当于图像处理中的

“滤波

器运算”。在介绍卷积

运算时，我们来看一个具

体的例子（图7-3）。

204 第

7章　卷积神

经网络

图7-3　卷积运算的例

子：用“”符号表示卷积运算

1 2 3

0

0 1 2 3

3

0 1

2 0 1

0

1

15 16

6 15

2

1 0 2

2

2 3

0 1

输入数据 滤波器

如图7-3所

示，卷积运算对输入数据

应用滤波器。在这个例子

中，输入

数据是有高长方

向的形状的数据，滤波器

也一样，有高长方向上的

维度。假

设用（height, width）表示数据和

滤波器的形状，则在本例

中，输入大小是

(4, 4)，滤波器大

小是(3, 3)，输出大小是(2, 2)。另外，有

的文献中也会用“核”

这个

词来表示这里所说的“滤

波器”。

现在来解释一下图

7-3的卷积运算的例子中都

进行了什么样的计算。图

7-4

中展示了卷积运算的计

算顺序。

对于输入数据，卷

积运算以一定间隔滑动

滤波器的窗口并应用。这

里所

说的窗口是指图7-4中

灰色的3 ×

3的部分。如图7-4所示

，将各个位置上滤

波器的

元素和输入的对应元素

相乘，然后再求和（有时将

这个计算称为乘积

累加

运算）。然后，将这个结果保

存到输出的对应位置。将

这个过程在所有

位置都

进行一遍，就可以得到卷

积运算的输出。

在全连接

的神经网络中，除了权重

参数，还存在偏置。CNN中，滤波

器的参数就对应之前的

权重。并且，CNN中也存在偏置

。图7-3的卷积运算

的例子一

直展示到了应用滤波器

的阶段。包含偏置的卷积

运算的处理流如图

7-5所示

。

如图7-5所示，向应用了滤波

器的数据加上了偏置。偏

置通常只有1个

（1 × 1）（本例中，相

对于应用了滤波器的4个

数据，偏置只有1个），这个值

会被加到应用了滤波器

的所有元素上。

7.2

卷积层 205

图

7-4　卷积运算的计算顺序

1 2

3 0

0 1 2 3

3 0 1

2 0 1

0 1

15

2

1 0

2

2

2 3 0 1

1 2 3

0 1 2

3 0 1

15

1 2

3 0

0 1 2 3

3 0 1

2 0 1

0 1

15 16

2

1

0 2

2

2 3 0

1

2 3 0

1 2

3

0 1

16

2

1

2 3 0

0 1 2

3

3 0 1

2 0

1

0 1

15 16

6

2

1 0 2

2

2

3 0 1

0 1 2

3 0 1 6

1 2

3 0

0 1 2 3

3 0 1

2 0 1

0 1

15 16

6 15

2

1 0 2

2

2

3 0 1

1 2 3

0 1 2 15

206 第

7章

卷积神经网络

图7-5　卷积

运算的偏置：向应用了滤

波器的元素加上某个固

定值（偏置）

1 2 3

0

0 1 2 3

3

0 1

2

3

0 1

0 1

15 16

6 15

2

1 0 2

2

2

3 0 1

输入数据 滤波

器（权重） 偏置

输出数据

18 19

9 18

+

7.2.3　填

充

在进行卷积层的处理

之前，有时要向输入数据

的周围填入固定的数据

（比

如0等），这称为填充（padding），是卷

积运算中经常会用到的

处理。比如，

在图7-6的例子中

，对大小为(4, 4)的输入数据应

用了幅度为1的填充。“幅

度

为1的填充”是指用幅度为

1像素的0填充周围。

1 2 3 0

0

1 2 3

3 0 1

2 0 1

0 1 2

1 0 2

2

2 3

0 1

(4, 4) (3, 3)

(4, 4)

输入数

据（padding:1） 滤波器 输出数据

7

12 10 2

4 15 16

10

10 6 15 6

8

10 4 3

图7-6 卷

积运算的填充处理：向输

入数据的周围填入0（图中

用虚线表示填充，并省略

了

填充的内容“0”）

如图7-6所示

，通过填充，大小为(4, 4)的输入

数据变成了(6, 6)的形状。

然后

，应用大小为(3, 3)的滤波器，生

成了大小为(4, 4)的输出数据

。这个例

子中将填充设成

了1，不过填充的值也可以

设置成2、3等任意的整数。在

图7-5

的例子中，如果将填充

设为2，则输入数据的大小

变为(8, 8)；如果将填充设

为3，则

大小变为(10, 10)。

7.2

卷积层 207

使用填

充主要是为了调整输出

的大小。比如，对大小为(4, 4)的

输入

数据应用(3, 3)的滤波器

时，输出大小变为(2,

2)，相当于

输出大小

比输入大小缩

小了 2个元素。这在反复进

行多次卷积运算的深度

网

络中会成为问题。为什

么呢？因为如果每次进行

卷积运算都会缩小

空间

，那么在某个时刻输出大

小就有可能变为 1，导致无

法再应用

卷积运算。为了

避免出现这样的情况，就

要使用填充。在刚才的例

子中，将填充的幅度设为

1，那么相对于输入大小(4, 4)，输

出大小

也保持为原来的

(4, 4)。因此，卷积运算就可以在

保持空间大小不变

的情

况下将数据传给下一层

。

7.2.4

步幅

应用滤波器的位置

间隔称为步幅（stride）。之前的例

子中步幅都是1，如

果将步

幅设为2，则如图7-7所示，应用

滤波器的窗口的间隔变

为2个元素。

图7-7　步幅为2的卷

积运算的例子

1

2 3

0 1 2

3

0 1 2 0 1 15

0 1

15

2

1 0

2

1 2 3 0

0

1 2 3

3 0 1

2

2 3 0 1

1

2 3

0 1 2

3

0 1

2 3 0

1

2 3 0

0 1 2

3

3 0 1 2

1

2 3

0 1 2

3

0 1

3 0

2 3

1 2

2 0 1

0

1

15 17

2

1 0

2

1

0

3

1 2

3 0

0 1 2 3

3 0 1 2

2 3

0 1

1 2 3

0

1 2

3 0 1

2

3 0

1 2 3 0

0 1 2 3

3 0

1 2

1 2 3

0

1 2

3 0 1

步幅:

2

208 第 7章

卷积神经网络

在图7-7的例

子中，对输入大小为(7, 7)的数

据，以步幅2应用了滤波器

。

通过将步幅设为2，输出大

小变为(3, 3)。像这样，步幅可以

指定应用滤波器

的间隔

。

综上，增大步幅后，输出大

小会变小。而增大填充后

，输出大小会变大。

如果将

这样的关系写成算式，会

如何呢？接下来，我们看一

下对于填充和步

幅，如何

计算输出大小。

这里，假设

输入大小为(H, W)，滤波器大小

为(FH, FW)，输出大小为

(OH, OW)，填充为P，步

幅为S。此时，输出大小可通

过式(7.1)进行计算。

(7.1)

现在，我们

使用这个算式，试着做几

个计算。

例1：图7-6的例子

输入

大小：(4, 4)；填充：1；步幅：1；滤波器大

小：(3, 3)

例2：图7-7的例子

输入大小

：(7, 7)；填充：0；步幅：2；滤波器大小：(3, 3)

例

3

输入大小：(28, 31)；填充：2；步幅：3；滤波

器大小：(5,

5)

7.2  卷积层  209

如这些例

子所示，通过在式（7.1）中代入

值，就可以计算输出大小

。这

里需要注意的是，虽然

只要代入值就可以计算

输出大小，但是所设定的

值必

须使式（7.1）中的 和 分别

可以除尽。当输出大小无

法

除尽时（结果是小数时

），需要采取报错等对策。顺

便说一下，根据深度学习

的框架的不同，当值无法

除尽时，有时会向最接近

的整数四舍五入，不进行

报错而继续运行。

7.2.5　3维数据

的卷积运算

之前的卷积

运算的例子都是以有高

、长方向的2维形状为对象

的。但是，

图像是3维数据，除

了高、长方向之外，还需要

处理通道方向。这里，我们

按

照与之前相同的顺序

，看一下对加上了通道方

向的3维数据进行卷积运

算的例子。

图7-8是卷积运算

的例子，图7-9是计算顺序。这

里以3通道的数据为例，

展

示了卷积运算的结果。和

2维数据时（图7-3的例子）相比

，可以发现纵深

方向（通道

方向）上特征图增加了。通

道方向上有多个特征图

时，会按通道

进行输入数

据和滤波器的卷积运算

，并将结果相加，从而得到

输出。

图7-8　对3维数据进行卷

积运算的例子

4

2 1 2

5 2 2

4

6 1 1 2

*

3 0 5

4 2 1

2

4

2

5

3 0

6 5

2 2 2 3

1 1 1 0

0 3

0 1

63 55

18 51

4 0 2

0 1 0

2 0 2

4 0 2

0 0 1 3

1 1

2

3 0 0

0 1

3

1 1 2

3 0

0

2 0 1

0 1

2

1 0 2

2 2

2 3

1 1 1 0

0 3 0 1

1 2

3 0

0 1 2 3

3 0 1 2

2 3

0 1

输入数据

滤波器 输出数据

210

第 7章　卷

积神经网络

图7-9　对3维数据

进行卷积运算的计算顺

序

63

63 55

63 55

18

63

55

18 51

4 2 1

2

5 2 2 4

6

1 1 2

* 3 0

5

4 2 1 2

4

2

5

3 0 6 5

2 2 2 3

1 1

1 0

0 3 0 1

4 0 2

0 1 0

2 0 2

4 0 2

0 0 1 3

1 1

2

3 0 0

0 1

3

1 1 2

3 0

0

2 0 1

0 1

2

1 0 2

3 0

6 5

2 2 2 3

1 1 1 0

0 3

0 1

1 2 3 0

0 1 2 3

3 0

1 2

2 3 0 1

4 2 1 2

5 2

2 4

6 1 1 2

* 3 0 5

4 2

1 2

4

2

5

3

0 6 5

2 2 2

3

1 1 1 0

0

3 0 1

4 0 2

0 1 0

2 0 2

4 0 2

0

2

0

1 3

1 1 2

3

0 0

0 1 3

1

1 2

3 0 0

2

0 1

0 1 2

1

0 2

3 0 6 5

2 2 2 3

1 1

1 0

0 3 0 1

1 2 3 0

0 1

2 3

3 0 1 2

2 3 0 1

4 2

1 2

5 2 2 4

6 1 1 2

* 3

0 5

4 2 1 2

4

2

5

3 0 6

5

2 2 2 3

1

1 1 0

0 3 0

1

4 0 2

0 1

0

2 0 2

4 0

2

0 0 1 3

1

1 2

3 0 0

0

1 3

1 1 2

3

0 0

2 0 1

0

1 2

1 0 2

3

0 6 5

2 2 2

3

1 1 1 0

0

3 0 1

1 2 3

0

0 1 2 3

3

0 1 2

2 3 0

1

4 2 1 2

5

2 2 4

6 1 1

2

* 3 0 5

4

2 1 2

4

2

5

3 0 6 5

2 2

2 3

1 1 1 0

0 3 0 1

4 0

2

0 1 0

2 0

2

4 0 2

0 0

1 3

1 1 2

3

0 0

0 1 3

1

1 2

3 0 0

2

0 1

0 1 2

1

0 2

3 0 6 5

2 2 2 3

1 1

1 0

0 3 0 1

1 2 3 0

0 1

2 3

3 0 1 2

2 3 0 1

7.2

卷积层  211

需要注意的是

，在3维数据的卷积运算中

，输入数据和滤波器的通

道数

要设为相同的值。在

这个例子中，输入数据和

滤波器的通道数一致，均

为3。

滤波器大小可以设定

为任意值（不过，每个通道

的滤波器大小要全部相

同）。

这个例子中滤波器大

小为(3, 3)，但也可以设定为(2, 2)、(1, 1)、(5, 5)等

任

意值。再强调一下，通道

数只能设定为和输入数

据的通道数相同的值（本

例

中为3）。

7.2.6　结合方块思考

将

数据和滤波器结合长方

体的方块来考虑，3维数据

的卷积运算会很

容易理

解。方块是如图7-10所示的3维

长方体。把3维数据表示为

多维数组

时，书写顺序为

（channel,

height, width）。比如，通道数为C、高度为H、

长

度为W的数据的形状可以

写成（C, H, W）。滤波器也一样，要按

（channel,

height,

width）的顺序书写。比如，通道数

为C、滤波器高度为FH（Filter

Height）、长度为

FW（Filter Width）时，可以写成（C, FH, FW）。

H

FH

FW

W

OH

OW

(C,

H, W) (C, FH, FW) (1,

OH, OW)

C

C

输入数据 滤

波器

输出数据

图7-10　结合方

块思考卷积运算。请注意

方块的形状

在这个例子

中，数据输出是1张特征图

。所谓1张特征图，换句话说

，

就是通道数为1的特征图

。那么，如果要在通道方向

上也拥有多个卷积运算

212

第 7章　卷积神经网络

的输

出，该怎么做呢？为此，就需

要用到多个滤波器（权重

）。用图表示的话，

如图7-11所示

。

图7-11

基于多个滤波器的卷

积运算的例子

H

FH

FN个

FW

W

OH

OW

(C, H, W) (FN,

C, FH, FW) (FN, OH, OW)

C

C

FN

输入数

据 滤波器 输出数据

图7-11中

，通过应用FN个滤波器，输出

特征图也生成了FN个。如果

将这FN个特征图汇集在一

起，就得到了形状为(FN, OH, OW)的方

块。将

这个方块传给下一

层，就是CNN的处理流。

如图 7-11

所

示，关于卷积运算的滤波

器，也必须考虑滤波器的

数

量。因此，作为4维数据，滤

波器的权重数据要按(output_channel, input_

channel, height, width)的

顺序书写。比如，通道数为

3、大小为5

× 5的滤

波器有20个时

，可以写成(20, 3, 5, 5)。

卷积运算中（和

全连接层一样）存在偏置

。在图7-11的例子中，如果进

一

步追加偏置的加法运算

处理，则结果如下面的图

7-12所示。

图7-12中，每个通道只有

一个偏置。这里，偏置的形

状是(FN, 1, 1)，

滤波器的输出结果

的形状是(FN,

OH, OW)。这两个方块相

加时，要对滤波

器的输出

结果(FN, OH, OW)按通道加上相同的

偏置值。另外，不同形状的

方块相加时，可以基于NumPy的

广播功能轻松实现（1.5.5节）。

7.2

卷

积层  213

图7-12　卷积运算的处理

流（追加了偏置项）

+

+

H

FH FN个

FW

W

OH 1

1 OW

(C, H,

W) (FN, C, FH, FW) (FN,

OH, OW)

C

C

FN

FN

输入

数据

(FN, OH, OW)

输出数据

(FN,

1, 1)

滤波器 偏

置

FN

FN

OH

OW

FN

7.2.7　批处理

神经网络的处

理中进行了将输入数据

打包的批处理。之前的全

连接神经

网络的实现也

对应了批处理，通过批处

理，能够实现处理的高效

化和学习时

对mini-batch的对应。

我

们希望卷积运算也同样

对应批处理。为此，需要将

在各层间传递的数

据保

存为4维数据。具体地讲，就

是按(batch_num, channel, height,

width)

的顺序保存数据。比

如，将图7-12中的处理改成对

N个数据进行批处理时，

数

据的形状如图7-13所示。

图7-13的

批处理版的数据流中，在

各个数据的开头添加了

批用的维度。

像这样，数据

作为4维的形状在各层间

传递。这里需要注意的是

，网络间传

递的是4维数据

，对这N个数据进行了卷积

运算。也就是说，批处理将

N次

的处理汇总成了1次进

行。

图7-13　卷积运算的处理流

（批处理）

* +

+

*

H

N 个数据 N 个数据

N 个

数据

FH FN 个

FW

W

OH 1

1

OW

(N,

C, H, W) (FN, C, FH,

FW) (N, FN, OH, OW)

C

C

FN

FN

输入数据 (N, FN,

OH, OW) 输出数

据 (FN, 1, 1)

滤波器 偏置

FN

FN

OH

OW

FN

214 第 7章　卷积

神经网络

7.3

池化层

池化是

缩小高、长方向上的空间

的运算。比如，如图7-14所示，进

行将

2 × 2的区域集约成1个元

素的处理，缩小空间大小

。

1

2

0 1

3 0

2

3 1’

2

2 3

1

4

1 2 0

0 1

2 3

3 0 1 2

2 0 1

4

1 2

1 0

0 1 2 3

3 0 1 2

2 0

1

3 0 ’

2 3

1 2

0 1

2 3

4

2 3

4 2

11

2 0

0 1 2 3

3 0 1 2

2 4

0 1

1

4

1 2

0

0 1 2 3

3

0 1 2

2 0 1

图7-14 Max池化的处理顺序

图7-14的

例子是按步幅2进行2 × 2的Max池

化时的处理顺序。“Max

池化”是

获取最大值的运算，“2

× 2”表示

目标区域的大小。如图所

示，从

2 × 2的区域中取出最大

的元素。此外，这个例子中

将步幅设为了2，所以

2

× 2的窗

口的移动间隔为2个元素

。另外，一般来说，池化的窗

口大小会

和步幅设定成

相同的值。比如，3 × 3的窗口的

步幅会设为3，4 ×

4的窗口

的步

幅会设为4等。

除了Max池化之

外，还有Average池化等。相对于Max池

化是从

目标区域中取出

最大值，Average池化则是计算目

标区域的平均值。

在图像

识别领域，主要使用Max池化

。因此，本书中说到“池化层

”

时，指的是Max池化。

7.3  池化层  215

池

化层的特征

池化层有以

下特征。

没有要学习的参

数

池化层和卷积层不同

，没有要学习的参数。池化

只是从目标区域中取最

大值（或者平均值），所以不

存在要学习的参数。

通道

数不发生变化

经过池化

运算，输入数据和输出数

据的通道数不会发生变

化。如图7-15

所示，计算是按通

道独立进行的。

图7-15　池化中

通道数不变

4 2 1 2

5 2 2 4

6 1

1 2

( 3 0 5

4 2 1 2

4

2

5

3 0 6 5

2

2 2 3

1 1 1

0

0 3 0 1

4

4

0 5

4 4

5

3 6

0 2

3 6

0 2 2 3

4 2

3 0 6 5

2 2

2 3

1 1 1 0

0 3 0 1

1 2

1 0

0 1 2 3

3 0 1 2

2 4

0 1

输入数据 输

出数据

对微小的位置变

化具有鲁棒性（健壮）

输入

数据发生微小偏差时，池

化仍会返回相同的结果

。因此，池化对

输入数据的

微小偏差具有鲁棒性。比

如，3 × 3的池化的情况下，如图

7-16所示，池化会吸收输入数

据的偏差（根据数据的不

同，结果有可

能不一致）。

216 第

7章

卷积神经网络

图7-16 输入

数据在宽度方向上只偏

离1个元素时，输出仍为相

同的结果（根据数据的不

同，

有时结果也不相同）

1 2

0 1 2

3 0 1

1’9

6

7

8

17

6

4

4 8

1 2 0

0

0 9 2 23 3

3 0

0

1

1

1

1

2

2

2

2

2

2

0

0

01

1

1

1

1 2

0 1 2

3 0 1 1’9

6

7

8

’

3

2

17

6

4

4 8

1 2 0

0 9 2 23

3 0

0

1

1

1

1

2

2

2

2

0

0

01

1

3

1

3

2

2

1

7.4 卷

积层和池化层的实现

前

面我们详细介绍了卷积

层和池化层，本节我们就

用Python来实现这

两个层。和第

5章一样，也给进行实现的

类赋予forward和backward方法，并

使其可

以作为模块使用。

大家可

能会感觉卷积层和池化

层的实现很复杂，但实际

上，通过使用某

种技巧，就

可以很轻松地实现。本节

将介绍这种技巧，将问题

简化，然后再

进行卷积层

的实现。

7.4.1　4维数组

如前所述

，CNN中各层间传递的数据是

4维数据。所谓4维数据，比如

数据的形状是(10,

1, 28, 28)，则它对应

10个高为28、长为28、通道为1的数

据。用Python来实现的话，如下所

示。

>>> x =

np.random.rand(10, 1, 28, 28) # 随机生成数据

>>> x.shape

(10, 1, 28, 28)

这里，如

果要访问第1个数据，只要

写x[0]就可以了（注意Python的索

引

是从0开始的）。同样地，用x[1]可

以访问第2个数据。

7.4  卷积层

和池化层的实现

217

>>> x[0].shape # (1, 28,

28)

>>> x[1].shape # (1, 28,

28)

如果要

访问第1个数据的第1个通

道的空间数据，可以写成

下面这样。

>>> x[0, 0] #

或者x[0][0]

像这样，CNN中

处理的是4维数据，因此卷

积运算的实现看上去会

很复

杂，但是通过使用下

面要介绍的im2col这个技巧，问

题就会变得很简单。

7.4.2　基于

im2col的展开

如果老老实实地

实现卷积运算，估计要重

复好几层的for语句。这样的

实现有点麻烦，而且，NumPy中存

在使用for语句后处理变慢

的缺点（NumPy

中，访问元素时最

好不要用for语句）。这里，我们

不使用for语句，而是使

用im2col这

个便利的函数进行简单

的实现。

im2col是一个函数，将输

入数据展开以适合滤波

器（权重）。如图7-17所示，

对3维的

输入数据应用im2col后，数据转

换为2维矩阵（正确地讲，是

把包含

批数量的4维数据

转换成了2维数据）。

图7-17

im2col的示

意图

im2col

输入数据

im2col会把输入

数据展开以适合滤波器

（权重）。具体地说，如图7-18所示

，

对于输入数据，将应用滤

波器的区域（3维方块）横向

展开为1列。im2col会

在所有应用

滤波器的地方进行这个

展开处理。

218   第 7章　卷积神经

网络

图7-18　将滤波器的应用

区域从头开始依次横向

展开为1列

在图7-18中，为了便

于观察，将步幅设置得很

大，以使滤波器的应用区

域不重叠。而在实际的卷

积运算中，滤波器的应用

区域几乎都是重叠的。在

滤波器的应用区域重叠

的情况下，使用im2col展开后，展

开后的元素个数会

多于

原方块的元素个数。因此

，使用im2col的实现存在比普通

的实现消耗更

多内存的

缺点。但是，汇总成一个大

的矩阵进行计算，对计算

机的计算颇有

益处。比如

，在矩阵计算的库（线性代

数库）等中，矩阵计算的实

现已被高

度最优化，可以

高速地进行大矩阵的乘

法运算。因此，通过归结到

矩阵计算

上，可以有效地

利用线性代数库。

im2col这个名

称是“image to column”的缩写，翻译过来就

是“从

图像到矩阵”的意思

。Caffe、Chainer

等深度学习框架中有名

为

im2col的函数，并且在卷积层

的实现中，都使用了im2col。

使用

im2col展开输入数据后，之后就

只需将卷积层的滤波器

（权重）纵

向展开为1列，并计

算2个矩阵的乘积即可（参

照图7-19）。这和全连接层的

Affi ne层

进行的处理基本相同。

如

图7-19所示，基于im2col方式的输出

结果是2维矩阵。因为CNN中

数

据会保存为4维数组，所以

要将2维输出数据转换为

合适的形状。以上就

是卷

积层的实现流程。

7.4  卷积层

和池化层的实现

219

图7-19 卷积

运算的滤波器处理的细

节：将滤波器纵向展开为

1列，并计算和im2col展开

的数据

的矩阵乘积，最后转换（reshape）为

输出数据的大小

im2col

reshape

输入数

据

输出数据

输出数据（2 维

）

滤波器

矩阵的乘积

7.4.3　卷积

层的实现

本书提供了im2col函

数，并将这个im2col函数作为黑

盒（不关心内部实现）

使用

。im2col的实现内容在common/util.py中，它的实

现（实质上）是一个10

行左右

的简单函数。有兴趣的读

者可以参考。

im2col这一便捷函

数具有以下接口。

im2col (input_data, filter_h, filter_w, stride=1, pad=0)

• input_data―由（数据

量，通道，高，长）的4维数组构

成的输入数据

• filter_h―滤波器的

高

• filter_w―滤波器的长

• stride―步幅

• pad―填充

220 第 7章

卷积神经网络

im2col会考

虑滤波器大小、步幅、填充

，将输入数据展开为2维数

组。现在，

我们来实际使用

一下这个im2col。

import sys, os

sys.path.append(os.pardir)

from common.util import im2col

x1

= np.random.rand(1, 3, 7, 7)

col1

= im2col(x1, 5, 5, stride=1, pad=0)

print(col1.shape) # (9, 75)

x2 =

np.random.rand(10, 3, 7, 7) # 10个数据

col2 = im2col(x2, 5, 5, stride=1,

pad=0)

print(col2.shape) # (90, 75)

这里举

了两个例子。第一个是批

大小为1、通道为3的7

× 7的数据

，第

二个的批大小为10，数据

形状和第一个相同。分别

对其应用im2col函数，在

这两种

情形下，第2维的元素个数

均为75。这是滤波器（通道为

3、大小为

5 ×

5）的元素个数的总

和。批大小为1时，im2col的结果是

(9, 75)。而第2

个例子中批大小为

10，所以保存了10倍的数据，即

(90, 75)。

现在使用im2col来实现卷积层

。这里我们将卷积层实现

为名为Convolution

的类。

class Convolution:

 def __init__(self, W,

b, stride=1, pad=0):

 self.W =

W

 self.b = b

self.stride = stride

 self.pad =

pad

 def forward(self, x):

FN, C, FH, FW = self.W.shape

N, C, H, W = x.shape

out_h = int(1 + (H +

2*self.pad - FH) / self.stride)

out_w = int(1 + (W +

2*self.pad - FW) / self.stride)

col = im2col(x, FH, FW, self.stride,

self.pad)

 col_W = self.W.reshape(FN, -1).T

# 滤波器的展

开

 out = np.dot(col,

col_W) + self.b

 out =

out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1,

2)

 return out

7.4

卷积层和池化层的实

现 221

卷积层的初始化方法

将滤波器（权重）、偏置、步幅

、填充作为参数接收。

滤波

器是 (FN, C,

FH, FW)的 4 维形状。另外，FN、C、FH、FW分别

是 Filter

Number（滤波器数量）、Channel、Filter

Height、Filter Width的缩写。

这

里用粗体字表示Convolution层的实

现中的重要部分。在这些

粗体字

部分，用im2col展开输入

数据，并用reshape将滤波器展开

为2维数组。然后，

计算展开

后的矩阵的乘积。

展开滤

波器的部分（代码段中的

粗体字）如图7-19所示，将各个

滤波器

的方块纵向展开

为1列。这里通过reshape(FN,-1)将参数指

定为-1，这是

reshape的一个便利的

功能。通过在reshape时指定为-1，reshape函

数会自

动计算-1维度上的

元素个数，以使多维数组

的元素个数前后一致。比

如，

(10, 3, 5,

5)形状的数组的元素个

数共有750个，指定reshape(10,-1)后，就

会转

换成(10, 75)形状的数组。

forward的实现

中，最后会将输出大小转

换为合适的形状。转换时

使用了

NumPy的transpose函数。transpose会更改多

维数组的轴的顺序。如图

7-20

所示，通过指定从0开始的

索引（编号）序列，就可以更

改轴的顺序。

图7-20　基于NumPy的transpose的

轴顺序的更改：通过指定

索引（编号），更改轴的顺序

形状 (N, H, W, C)

transpose (N, C, H, W)

索引

0, 1, 2, 3 0, 3,

1, 2

以上就是卷积

层的forward处理的实现。通过使

用im2col进行展开，基

本上可以

像实现全连接层的Affine层一

样来实现（5.6节）。接下来是卷

积层

的反向传播的实现

，因为和Affine层的实现有很多

共通的地方，所以就不再

介绍了。但有一点需要注

意，在进行卷积层的反向

传播时，必须进行im2col

的逆处

理。这可以使用本书提供

的col2im函数（col2im的实现在common/util.

222   第 7章　卷

积神经网络

py中）来进行。除

了使用col2im这一点，卷积层的

反向传播和Affi ne层的实

现方

式都一样。卷积层的反向

传播的实现在common/layer.py中，有兴趣

的读

者可以参考。

7.4.4　池化层

的实现

池化层的实现和

卷积层相同，都使用im2col展开

输入数据。不过，池化

的情

况下，在通道方向上是独

立的，这一点和卷积层不

同。具体地讲，如图

7-21所示，池

化的应用区域按通道单

独展开。

图7-21　对输入数据展

开池化的应用区域（2×2的池

化的例子）

4

2 1 2

5 2 2

4

6 1 1 2

(

3 0 5

4 2 1

2

3 0 6 5

2

2 2 3

1 1 1

0

0 3 0 1

1

2 0 1

1 2 0

4

4 2 0 1

4

2 0 1

3 0 2

4

3 0 4 2

6

5 4 3

3 0 4

2

6 2 4 5

1

0 3 2

3 0 2

3

1 0 3 1

3

0 6

2 2

1 1

0 3

1 2 3 0

0 1 2 4

1 0

4 2

3 2 0 1

输入数据

4

2

5

5

3

0

1

1 通道

2 通道

3

通道

像这样展开之

后，只需对展开的矩阵求

各行的最大值，并转换为

合适的

形状即可（图7-22）。

7.4  卷积

层和池化层的实现

223

4 4

4 6

4

4

4 6

3 3

4

6

2 4

3 4

展开

max

reshape

1 2 0 1

1

2 0 4

4 2 0

1

4 2 0 1

3

0 2 4

3 0 4

2

6 5 4 3

3

0 4 2

6 2 4

5

1 0 3 2

3

0 2 3

1 0 3

1

2

4

4

4

4

4

6

4

6

3

3

3

4 2 1 2

5

2 2 4

6 1 1

2

( 3 0 5

4

2 1 2

4

2

5

3 0 6 5

2 2

2 3

1 1 1 0

0 3 0 1

3 0

6 5

2 2 2 3

1 1 1 0

0 3

0 1

1 2 3 0

0 1 2 4

1 0

4 2

3 2 0 1

输出数据

输入数据

图7-22　池

化层的实现流程：池化的

应用区域内的最大值元

素用灰色表示

上面就是

池化层的forward处理的实现流

程。下面来看一下Python的实

现

示例。

class Pooling:

 def __init__(self, pool_h,

pool_w, stride=1, pad=0):

 self.pool_h =

pool_h

 self.pool_w = pool_w

self.stride = stride

 self.pad =

pad

 def forward(self, x):

N, C, H, W = x.shape

out_h = int(1 + (H -

self.pool_h) / self.stride)

 out_w =

int(1 + (W - self.pool_w) /

self.stride)

 # 展开(1)

 col

= im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)

col = col.reshape(-1, self.pool_h*self.pool_w)

 #

最大值(2)

 out = np.max(col, axis=1)

# 转换(3)

 out = out.reshape(N,

out_h, out_w, C).transpose(0, 3, 1, 2)

return out

如

图7-22所示，池化层的实现按

下面3个阶段进行。

224

第 7章　卷

积神经网络

1.展开输入数

据。

2.求各行的最大值。

3.转换

为合适的输出大小。

各阶

段的实现都很简单，只有

一两行代码。

最大值的计

算可以使用 NumPy 的 np.max方法。np.max可以

指定

axis参数，并在这个参数

指定的各个轴方向上求

最大值。比如，如

果写成np.max(x, axis=1)，就

可以在输入x的第1维的各

个轴方向

上求最大值。

以

上就是池化层的forward处理的

介绍。如上所述，通过将输

入数据展

开为容易进行

池化的形状，后面的实现

就会变得非常简单。

关于

池化层的backward处理，之前已经

介绍过相关内容，这里就

不再介绍了。

另外，池化层

的backward处理可以参考ReLU层的实

现中使用的max的反向

传播

（5.5.1节）。池化层的实现在common/layer.py中，有

兴趣的读者可以参考。

7.5 CNN的

实现

我们已经实现了卷

积层和池化层，现在来组

合这些层，搭建进行手写

数

字识别的CNN。这里要实现

如图7-23所示的CNN。

图7-23　简单CNN的网

络构成

Conv ReLU Pooling Affine

ReLU Affine Softmax

7.5  CNN的实现

225

如图7-23所示

，网络的构成是“Convolution - ReLU - Pooling

-Affine -

ReLU - Affine -

Softmax”，我们将它

实现为名为SimpleConvNet的类。

首先来

看一下SimpleConvNet的初始化（__init__），取下面

这些参数。

参数

• input_dim―输入数据

的维度：（通道，高，长）

•

conv_param―卷积层

的超参数（字典）。字典的关

键字如下：

filter_num―滤波器的数量

filter_size―滤波器的大小

stride―步幅

pad―填充

• hidden_size―隐藏层（全连接）的神经元

数量

•

output_size―输出层（全连接）的神

经元数量

• weitght_int_std―初始化时权重

的标准差

这里，卷积层的

超参数通过名为conv_param的字典

传入。我们设想它会

像{'filter_num':30,'filter_size':5, 'pad':0,

'stride':1}这

样，保存必要

的超参数值

。

SimpleConvNet的初始化的实现稍长，我

们分成3部分来说明，首先

是初

始化的最开始部分

。

class SimpleConvNet:

def __init__(self, input_dim=(1, 28, 28),

conv_param={'filter_num':30, 'filter_size':5,

 'pad':0, 'stride':1},

hidden_size=100, output_size=10, weight_init_std=0.01):

 filter_num =

conv_param['filter_num']

 filter_size = conv_param['filter_size']

filter_pad = conv_param['pad']

 filter_stride =

conv_param['stride']

 input_size = input_dim[1]

conv_output_size = (input_size - filter_size +

2*filter_pad) / \

 filter_stride +

1

 pool_output_size = int(filter_num *

(conv_output_size/2) *

 (conv_output_size/2))

226 第

7章　卷积神经网络

这里

将由初始化参数传入的

卷积层的超参数从字典

中取了出来（以方便

后面

使用），然后，计算卷积层的

输出大小。接下来是权重

参数的初始化部分。

self.params =

{}

self.params['W1'] = weight_init_std * \

np.random.randn(filter_num, input_dim[0],

 filter_size, filter_size)

self.params['b1']

= np.zeros(filter_num)

self.params['W2'] = weight_init_std *

\

 np.random.randn(pool_output_size,

 hidden_size)

self.params['b2']

= np.zeros(hidden_size)

self.params['W3'] = weight_init_std *

\

 np.random.randn(hidden_size, output_size)

self.params['b3'] =

np.zeros(output_size)

学习

所需的参数是第1层的卷

积层和剩余两个全连接

层的权重和偏置。

将这些

参数保存在实例变量的

params字典中。将第1层的卷积层

的权重设为

关键字W1，偏置

设为关键字b1。同样，分别用

关键字W2、b2和关键字W3、b3

来保存

第2个和第3个全连接层的

权重和偏置。

最后，生成必

要的层。

self.layers = OrderedDict()

self.layers['Conv1'] = Convolution(self.params['W1'],

self.params['b1'],

 conv_param['stride'],

 conv_param['pad'])

self.layers['Relu1']

= Relu()

self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2,

stride=2)

self.layers['Affine1'] = Affine(self.params['W2'],

 self.params['b2'])

self.layers['Relu2'] = Relu()

self.layers['Affine2'] = Affine(self.params['W3'],

self.params['b3'])

self.last_layer = softmaxwithloss()

从最前面开始按

顺序向有序字典（OrderedDict）的layers中添

加层。只

有最后的SoftmaxWithLoss层被添

加到别的变量lastLayer中。

以上就

是SimpleConvNet的初始化中进行的处

理。像这样初始化后，进

行

推理的predict方法和求损失函

数值的loss方法就可以像下

面这样实现。

7.5  CNN的实现 227

def predict(self, x):

 for layer

in self.layers.values():

 x = layer.forward(x)

return x

def loss(self, x, t):

y = self.predict(x)

 return self.lastLayer.forward(y,

t)

这里

，参数x是输入数据，t是教师

标签。用于推理的predict方法从

头

开始依次调用已添加

的层，并将结果传递给下

一层。在求损失函数的loss

方

法中，除了使用 predict方法进行

的 forward处理之外，还会继续进

行

forward处理，直到到达最后的

SoftmaxWithLoss层。

接下来是基于误差反

向传播法求梯度的代码

实现。

def gradient(self, x, t):

# forward

 self.loss(x, t)

# backward

 dout = 1

dout = self.lastLayer.backward(dout)

 layers =

list(self.layers.values())

 layers.reverse()

 for layer

in layers:

 dout = layer.backward(dout)

# 设定

 grads = {}

grads['W1'] = self.layers['Conv1'].dW

 grads['b1'] =

self.layers['Conv1'].db

 grads['W2'] = self.layers['Affine1'].dW

grads['b2'] = self.layers['Affine1'].db

 grads['W3'] =

self.layers['Affine2'].dW

 grads['b3'] = self.layers['Affine2'].db

return grads

参数的梯度通

过误差反向传播法（反向

传播）求出，通过把正向传

播和

反向传播组装在一

起来完成。因为已经在各

层正确实现了正向传播

和反向传

播的功能，所以

这里只需要以合适的顺

序调用即可。最后，把各个

权重参数

的梯度保存到

grads字典中。这就是SimpleConvNet的实现。

228 第

7章　卷积神经网络

现在，使

用这个SimpleConvNet学习MNIST数据集。用于

学习的代码

与4.5节中介绍

的代码基本相同，因此这

里不再罗列（源代码在ch07/train_

convnet.py中

）。

如果使用MNIST数据集训练SimpleConvNet，则

训练数据的识别率为

99.82%，测

试数据的识别率为98.96%（每次

学习的识别精度都会发

生一些误

差）。测试数据的

识别率大约为99%，就小型网

络来说，这是一个非常高

的

识别率。下一章，我们会

通过进一步叠加层来加

深网络，实现测试数据的

识

别率超过99%的网络。

如上

所述，卷积层和池化层是

图像识别中必备的模块

。CNN可以有效

读取图像中的

某种特性，在手写数字识

别中，还可以实现高精度

的识别。

7.6 CNN的可视化

CNN中用到

的卷积层在“观察”什么呢

？本节将通过卷积层的可

视化，

探索CNN中到底进行了

什么处理。

7.6.1

第 1层权重的可

视化

刚才我们对MNIST数据集

进行了简单的CNN学习。当时

，第1层的

卷积层的权重的

形状是(30, 1, 5,

5)，即30个大小为5 × 5、通道

为1的滤波

器。滤波器大小

是5 × 5、通道数是1，意味着滤波

器可以可视化为1通道的

灰度图像。现在，我们将卷

积层（第1层）的滤波器显示

为图像。这里，我

们来比较

一下学习前和学习后的

权重，结果如图7-24所示（源代

码在ch07/

visualize_filter.py中）。

图7-24中，学习前的滤

波器是随机进行初始化

的，所以在黑白的浓淡上

没有规律可循，但学习后

的滤波器变成了有规律

的图像。我们发现，通过学

习，滤波器被更新成了有

规律的滤波器，比如从白

到黑渐变的滤波器、含有

块状区域（称为blob）的滤波器

等。

7.6  CNN的可视化

229

图7-24 学习前和

学习后的第1层的卷积层

的权重：虽然权重的元素

是实数，但是在图像

的显

示上，统一将最小值显示

为黑色（0），最大值显示为白

色（255）

学习前 学习后

如果要

问图7-24中右边的有规律的

滤波器在“观察”什么，答案

就是它

在观察边缘（颜色

变化的分界线）和斑块（局

部的块状区域）等。比如，左

半

部分为白色、右半部分

为黑色的滤波器的情况

下，如图7-25所示，会对垂直

方

向上的边缘有响应。

图7-25 对

水平方向上和垂直方向

上的边缘有响应的滤波

器：输出图像1中，垂直方向

的

边缘上出现白色像素

，输出图像2中，水平方向的

边缘上出现很多白色像

素

输入图像

输出图像 2

输

出图像 1

对垂直方向上

的

边缘有响应

滤波器2

滤波

器1

对水平方向上

的边缘

有响应

230   第 7章　卷积神经网

络

图7-25中显示了选择两个

学习完的滤波器对输入

图像进行卷积处理时的

结果。我们发现“滤波器1”对

垂直方向上的边缘有响

应，“滤波器2”对水平

方向上

的边缘有响应。

由此可知

，卷积层的滤波器会提取

边缘或斑块等原始信息

。而刚才实现

的CNN会将这些

原始信息传递给后面的

层。

7.6.2　基于分层结构的信息

提取

上面的结果是针对

第1层的卷积层得出的。第

1层的卷积层中提取了边

缘或斑块等“低级”信息，那

么在堆叠了多层的CNN中，各

层中又会提取什

么样的

信息呢？根据深度学习的

可视化相关的研究[17][18]，随着

层次加深，提

取的信息（正

确地讲，是反映强烈的神

经元）也越来越抽象。

图7-26中

展示了进行一般物体识

别（车或狗等）的8层CNN。这个网

络

结构的名称是下一节

要介绍的AlexNet。AlexNet网络结构堆叠

了多层卷积

层和池化层

，最后经过全连接层输出

结果。图7-26的方块表示的是

中间数据，

对于这些中间

数据，会连续应用卷积运

算。

图7-26 CNN的卷积层中提取的

信息。第1层的神经元对边

缘或斑块有响应，第3层对

纹

理有响应，第5层对物体

部件有响应，最后的全连

接层对物体的类别（狗或

车）有

响应（图像引用自文

献[19]）

11

11

224

55

5

5

55

27

96

3

3

3

3

3

3

256

27

13

13

13

13

13

13

384 384

256

4096 4096

1000

dense dense

Max

Max pooling

pooling Max

pooling

224

3

Stride

of 4

Numerical

Data-driven

Conv 1: Edge+Blob Conv 3:

Texture Conv 5: Object Parts Fc8:

Object Classes cock ship

dinning table

groccry storc

7.7  具有代表性的CNN

231

如图7-26所

示，如果堆叠了多层卷积

层，则随着层次加深，提取

的信息

也愈加复杂、抽象

，这是深度学习中很有意

思的一个地方。最开始的

层对简

单的边缘有响应

，接下来的层对纹理有响

应，再后面的层对更加复

杂的物体

部件有响应。也

就是说，随着层次加深，神

经元从简单的形状向“高

级”信息

变化。换句话说，就

像我们理解东西的“含义

”一样，响应的对象在逐渐

变化。

7.7 具有代表性的 CNN

关于

CNN，迄今为止已经提出了各

种网络结构。这里，我们介

绍其中

特别重要的两个

网络，一个是在1998年首次被

提出的CNN元祖LeNet[20]，

另一个是在

深度学习受到关注的2012年

被提出的AlexNet[21]。

7.7.1　LeNet

LeNet在1998年被提出，是

进行手写数字识别的网

络。如图7-27所示，

它有连续的

卷积层和池化层（正确地

讲，是只“抽选元素”的子采

样层），最

后经全连接层输

出结果。

图7-27

LeNet的网络结构（引

用自文献[20]）

和“现在的CNN”相比

，LeNet有几个不同点。第一个不

同点在于激活

函数。LeNet中使

用sigmoid函数，而现在的CNN中主要

使用ReLU函数。

此外，原始的LeNet中

使用子采样（subsampling）缩小中间数

据的大小，而

现在的CNN中Max池

化是主流。

232

第 7章　卷积神经

网络

综上，LeNet与现在的CNN虽然

有些许不同，但差别并不

是那么大。

想到LeNet是20多年前

提出的最早的CNN，还是很令

人称奇的。

7.7.2　AlexNet

在LeNet问世20多年后

，AlexNet被发布出来。AlexNet是引发深度

学

习热潮的导火线，不过

它的网络结构和LeNet基本上

没有什么不同，如图7-28

所示

。

图7-28

AlexNet（根据文献[21]生成）

224

11

11

55 27

13

13

13

13

13

13

27

256

384 384 256 4096

4096

1000

55

5 3

5

3

3

3

3

3

96

max pooling

fully connected

max pooling

224

3

AlexNet叠有多

个卷积层和池化层，最后

经由全连接层输出结果

。虽然

结构上AlexNet和LeNet没有大的

不同，但有以下几点差异

。

• 激活函数使用ReLU。

• 使用进行

局部正规化的LRN（Local Response Normalization）层。

• 使用Dropout（6.4.3节

）。

如上所述，关于网络结构

，LeNet和AlexNet没有太大的不同。但是

，

围绕它们的环境和计算

机技术有了很大的进步

。具体地说，现在任何人都

可

以获得大量的数据。而

且，擅长大规模并行计算

的GPU得到普及，高速进

行大

量的运算已经成为可能

。大数据和GPU已成为深度学

习发展的巨大的

原动力

。

7.8

小结 233

大多数情况下，深度

学习（加深了层次的网络

）存在大量的参数。因此，

学

习需要大量的计算，并且

需要使那些参数“满意”的

大量数据。可

以说是 GPU和大

数据给这些课题带来了

希望。

7.8 小结

本章介绍了CNN。构

成CNN的基本模块的卷积层

和池化层虽然有些复

杂

，但是一旦理解了，之后就

只是如何使用它们的问

题了。本章为了使读者

在

实现层面上理解卷积层

和池化层，花了不少时间

进行介绍。在图像处理领

域，几乎毫无例外地都会

使用CNN。请扎实地理解本章

的内容，然后进入

最后一

章的学习。

本章所学的内

容

• CNN在此前的全连接层的

网络中新增了卷积层和

池化层。

• 使用im2col函数可以简

单、高效地实现卷积层和

池化层。

•

通过CNN的可视化，可

知随着层次变深，提取的

信息愈加高级。

• LeNet和AlexNet是CNN的代

表性网络。

• 在深度学习的

发展中，大数据和GPU做出了

很大的贡献。

第8章

深度学

习

深度学习是加深了层

的深度神经网络。基于之

前介绍的网络，只需通过

叠加层，就可以创建深度

网络。本章我们将看一下

深度学习的性质、课题和

可能性，然后对当前的深

度学习进行概括性的说

明。

8.1 加深网络

关于神经网

络，我们已经学了很多东

西，比如构成神经网络的

各种层、

学习时的有效技

巧、对图像特别有效的CNN、参

数的最优化方法等，这些

都是深度学习中的重要

技术。本节我们将这些已

经学过的技术汇总起来

，创

建一个深度网络，挑战

MNIST数据集的手写数字识别

。

8.1.1　向更深的网络出发

话不

多说，这里我们来创建一

个如图8-1所示的网络结构

的CNN（一个

比之前的网络都

深的网络）。这个网络参考

了下一节要介绍的VGG。

如图

8-1所示，这个网络的层比之

前实现的网络都更深。这

里使用的卷

积层全都是

3 × 3的小型滤波器，特点是随

着层的加深，通道数变大

（卷积

层的通道数从前面

的层开始按顺序以16、16、32、32、64、64的方

式增加）。

此外，如图8-1所示，插

入了池化层，以逐渐减小

中间数据的空间大小；并

且，

后面的全连接层中使

用了Dropout层。

236   第 8章

深度学习

这

个网络使用He初始值作为

权重的初始值，使用Adam更新

权重参数。

把上述内容总

结起来，这个网络有如下

特点。

• 基于3×3的小型滤波器

的卷积层。

•

激活函数是ReLU。

• 全

连接层的后面使用Dropout层。

• 基

于Adam的最优化。

•

使用He初始值

作为权重初始值。

从这些

特征中可以看出，图8-1的网

络中使用了多个之前介

绍的神经网

络技术。现在

，我们使用这个网络进行

学习。先说一下结论，这个

网络的识

别精度为99.38% A，可以

说是非常优秀的性能了

！

A

最终的识别精度有少许

偏差，不过在这个网络中

，识别精度大体上都会超

过99%。

图8-1　进行手写数字识别

的深度CNN

Conv ReLU Conv

ReLU Pool Conv ReLU Conv ReLU

Pool

Conv ReLU Conv ReLU Pool

Affine ReLU Dropout Affine Dropout Softmax

8.1  加深网络  237

实现图

8-1的网络的源代码在ch08/deep_convnet.py中，训

练用的

代码在ch08/train_deepnet.py中。虽然使

用这些代码可以重现这

里

进行的学习，不过深度

网络的学习需要花费较

多的时间（大概要半天

以

上）。本书以ch08/deep_conv_net_params.pkl的形式给出了

学习完

的权重参数。刚才

的deep_convnet.py备有读入学习完的参

数的功能，

请根据需要进

行使用。

图8-1的网络的错误

识别率只有0.62%。这里我们实

际看一下在什么样

的图

像上发生了识别错误。图

8-2中显示了识别错误的例

子。

6

0

3 3

5

5

3 3

5 5

8

8

3 3

7 7

3

3

1

3

8 8

9

9

6 6

0 0

7

7

2 2

9 9

4

4

7

1

5 5

3

3

1 1

3 3

0

0

6 6

9 9

4

4

7

9

6 6

0

0

9 9

8 8

4

4

9 9

1 1

7

7

图8-2 识别错误的图像的

例子：各个图像的左上角

显示了正确解标签，右下

角显示了本

网络的推理

结果

观察图8-2可知，这些图

像对于我们人类而言也

很难判断。实际上，这里

面

有几个图像很难判断是

哪个数字，即使是我们人

类，也同样会犯“识别错误

”。

比如，左上角的图像（正确

解是“6”）看上去像“0”，它旁边的

图像（正确解是

“3”）看上去像

“5”。整体上，“1”和“7”、“0”和“6”、“3”和“5”的组合比

较

容易混淆。通过这些例

子，相信大家可以理解为

何会发生识别错误了吧

。

这次的深度CNN尽管识别精

度很高，但是对于某些图

像，也犯了和人

类同样的

“识别错误”。从这一点上，我

们也可以感受到深度CNN中

蕴藏着

巨大的可能性。

238   第

8章　深度学习

8.1.2

进一步提高

识别精度

在一个标题为

“What is the class of

this image ?”的网站[32]上，以排行

榜的形

式刊登了目前为止通过

论文等渠道发表的针对

各种数据集的方法的识

别精度（图8-3）。

图8-3　针对MNIST数据集

的各种方法的排行（引自

文献[32]：2016年6月）

观察图 8-3 的 排 行

结 果，可

以 发 现“Neural Networks”“Deep”

“Convolutional”等关键词

特别显眼。实际上，排行榜

上的前几名大都是基

于

CNN的方法。顺便说一下，截止

到2016年6月，对MNIST数据集的最高

识别精度是99.79%（错误识别率

为0.21%），该方法也是以CNN为基础

的[33]。

不过，它用的CNN并不是特

别深层的网络（卷积层为

2层、全连接层为2层

的网络

）。

8.1  加深网络

239

对于MNIST数据集，层

不用特别深就获得了（目

前）最高的识别精

度。一般

认为，这是因为对于手写

数字识别这样一个比较

简单的任

务，没有必要将

网络的表现力提高到那

么高的程度。因此，可以说

加深层的好处并不大。而

之后要介绍的大规模的

一般物体识别的情况，

因

为问题复杂，所以加深层

对提高识别精度大有裨

益。

参考刚才排行榜中前

几名的方法，可以发现进

一步提高识别精度的技

术和

线索。比如，集成学习

、学习率衰减、Data Augmentation（数据扩充）等

都有

助于提高识别精度

。尤其是Data Augmentation，虽然方法很简单

，但在提高

识别精度上效

果显著。

Data

Augmentation基于算法“人为地

”扩充输入图像（训练图像

）。具

体地说，如图8-4所示，对于

输入图像，通过施加旋转

、垂直或水平方向上

的移

动等微小变化，增加图像

的数量。这在数据集的图

像数量有限时尤其有效

。

原图像

基于旋转的变形

基于平移的变形

图8-4

Data Augmentation的例

子

除了如图8-4所示的变形

之外，Data Augmentation还可以通过其他各

种方法扩充图像，比如裁

剪图像的“crop处理”、将图像左

右翻转的“fl ip处

理”A

等。对于一

般的图像，施加亮度等外

观上的变化、放大缩小等

尺度上

的变化也是有效

的。不管怎样，通过Data Augmentation巧妙地

增加训练图像，

就可以提

高深度学习的识别精度

。虽然这个看上去只是一

个简单的技巧，不

过经常

会有很好的效果。这里，我

们不进行Data Augmentation的实现，不

A fl ip处理

只在不需要考虑图像对

称性的情况下有效。

240

第 8章

深度学习

过这个技巧的

实现比较简单，有兴趣的

读者请自己试一下。

8.1.3　加深

层的动机

关于加深层的

重要性，现状是理论研究

还不够透彻。尽管目前相

关理论

还比较贫乏，但是

有几点可以从过往的研

究和实验中得以解释（虽

然有一些

直观）。本节就加

深层的重要性，给出一些

增补性的数据和说明。

首

先，从以ILSVRC为代表的大规模

图像识别的比赛结果中

可以看出加

深层的重要

性（详细内容请参考下一

节）。这种比赛的结果显示

，最近前几名

的方法多是

基于深度学习的，并且有

逐渐加深网络的层的趋

势。也就是说，

可以看到层

越深，识别性能也越高。

下

面我们说一下加深层的

好处。其中一个好处就是

可以减少网络的参数

数

量。说得详细一点，就是与

没有加深层的网络相比

，加深了层的网络可以

用

更少的参数达到同等水

平（或者更强）的表现力。这

一点结合卷积运算中

的

滤波器大小来思考就好

理解了。比如，图8-5展示了由

5 × 5的滤波器构成

的卷积层

。

输入数据 输出数据

5 × 5

图8-5 5×5的

卷积运算的例子

8.1  加深网

络

241

这里希望大家考虑一

下输出数据的各个节点

是从输入数据的哪个区

域计

算出来的。显然，在图

8-5的例子中，每个输出节点

都是从输入数据的某个

5 × 5的区域算出来的。接下来

我们思考一下图8-6中重复

两次3 ×

3的卷积

运算的情形

。此时，每个输出节点将由

中间数据的某个3 × 3的区域

计算出来。

那么，中间数据

的3 ×

3的区域又是由前一个

输入数据的哪个区域计

算出来

的呢？仔细观察图

8-6，可知它对应一个5 × 5的区域

。也就是说，图8-6的

输出数据

是“观察”了输入数据的某

个5 ×

5的区域后计算出来的

。

输入数据 中间数据 输出

数据

3 ×

3 3 × 3

图8-6　重复两次3×3的卷积

层的例子

一次5 × 5的卷积运

算的区域可以由两次3 × 3的

卷积运算抵充。并且，

相对

于前者的参数数量25（5

× 5），后者

一共是18（2 × 3 × 3），通过叠加卷

积层

，参数数量减少了。而且，这

个参数数量之差会随着

层的加深而变大。

比如，重

复三次3 × 3的卷积运算时，参

数的数量总共是27。而为了

用一次

卷积运算“观察”与

之相同的区域，需要一个

7 ×

7的滤波器，此时的参数数

量是49。

叠加小型滤波器来

加深网络的好处是可以

减少参数的数量，扩大感

受野（receptive field，给神经元施加变化

的某个局部空间区域）。并

且，

通过叠加层，将 ReLU等激活

函数夹在卷积层的中间

，进一步提高

了网络的表

现力。这是因为向网络添

加了基于激活函数的“非

线性”

表现力，通过非线性

函数的叠加，可以表现更

加复杂的东西。

242   第 8章

深度

学习

加深层的另一个好

处就是使学习更加高效

。与没有加深层的网络相

比，

通过加深层，可以减少

学习数据，从而高效地进

行学习。为了直观地理解

这

一点，大家可以回忆一

下7.6节的内容。7.6节中介绍了

CNN的卷积层会分

层次地提

取信息。具体地说，在前面

的卷积层中，神经元会对

边缘等简单的

形状有响

应，随着层的加深，开始对

纹理、物体部件等更加复

杂的东西有响应。

我们先

牢记这个网络的分层结

构，然后考虑一下“狗”的识

别问题。要

用浅层网络解

决这个问题的话，卷积层

需要一下子理解很多“狗

”的特征。“狗”

有各种各样的

种类，根据拍摄环境的不

同，外观变化也很大。因此

，要理解“狗”

的特征，需要大

量富有差异性的学习数

据，而这会导致学习需要

花费很多时间。

不过，通过

加深网络，就可以分层次

地分解需要学习的问题

。因此，各

层需要学习的问

题就变成了更简单的问

题。比如，最开始的层只要

专注于学

习边缘就好，这

样一来，只需用较少的学

习数据就可以高效地进

行学习。这

是为什么呢？因

为和印有“狗”的照片相比

，包含边缘的图像数量众

多，并

且边缘的模式比“狗

”的模式结构更简单。

通过

加深层，可以分层次地传

递信息，这一点也很重要

。比如，因为提

取了边缘的

层的下一层能够使用边

缘的信息，所以应该能够

高效地学习更加

高级的

模式。也就是说，通过加深

层，可以将各层要学习的

问题分解成容易

解决的

简单问题，从而可以进行

高效的学习。

以上就是对

加深层的重要性的増补

性说明。不过，这里需要注

意的是，

近几年的深层化

是由大数据、计算能力等

即便加深层也能正确地

进行学习的

新技术和环

境支撑的。

8.2 深度学习的小

历史

一般认为，现在深度

学习之所以受到大量关

注，其契机是2012年举办

的大

规模图像识别大赛ILSVRC（ImageNet Large Scale Visual Recognition

Challenge）。在那

年的比赛中，基于深度学

习的方法（通称AlexNet）以压倒

性

的优势胜出，彻底颠覆了

以往的图像识别方法。2012年

深度学习的这场逆

8.2 深度

学习的小历史  243

袭成为一

个转折点，在之后的比赛

中，深度学习一直活跃在

舞台中央。本节

我们以ILSVRC这

个大规模图像识别比赛

为轴，看一下深度学习最

近的发展

趋势。

8.2.1　ImageNet

ImageNet[25]是拥有超

过100万张图像的数据集。如

图8-7所示，它包含

了各种各

样的图像，并且每张图像

都被关联了标签（类别名

）。每年都会举办

使用这个

巨大数据集的ILSVRC图像识别

大赛。

mammal

vehicle craft watercraft

sailing vessel sailboat trimaran

placental carnivore

canine dog working dog husky

图8-7

大规模数据集ImageNet的

数据例（引用自文献[25]）

ILSVRC大赛

有多个测试项目，其中之

一是“类别分类”（classification），

在该项目

中，会进行1000个类别的分类

，比试识别精度。我们来看

一下最

近几年的ILSVRC大赛的

类别分类项目的结果。图

8-8中展示了从2010年到

2015年的优

胜队伍的成绩。这里，将前

5类中出现正确解的情况

视为“正确”，

此时的错误识

别率用柱形图来表示。

图

8-8中需要注意的是，以2012年为

界，之后基于深度学习的

方法一直

居于首位。实际

上，我们发现2012年的AlexNet大幅降

低了错误识别率。并且，

此

后基于深度学习的方法

不断在提升识别精度。特

别是2015年的ResNet（一

个超过150层的

深度网络）将错误识别率

降低到了3.5%。据说这个结果

甚至

超过了普通人的识

别能力。

这些年深度学习

取得了不斐的成绩，其中

VGG、GoogLeNet、ResNet

244   第 8章　深度学习

已广为人

知，在与深度学习有关的

各种场合都会遇到这些

网络。下面我们就

来简单

地介绍一下这3个有名的

网络。

图8-8 ILSCRV优胜队伍的成绩

演变：竖轴是错误识别率

，横轴是年份。横轴的括号

内

是队伍名或者方法名

8.2.2　VGG

VGG是由卷积层和池化层构

成的基础的CNN。不过，如图8-9所

示，

它的特点在于将有权

重的层（卷积层或者全连

接层）叠加至16层（或者19层），

具

备了深度（根据层的深度

，有时也称为“VGG16”或“VGG19”）。

VGG中需要注

意的地方是，基于3×3的小型

滤波器的卷积层的运算

是

连续进行的。如图8-9所示

，重复进行“卷积层重叠2次

到4次，再通过池化

层将大

小减半”的处理，最后经由

全连接层输出结果。

VGG在 2014年

的比赛中最终获得了第

2名的成绩（下一节介绍的

GoogleNet是 2014年的第 1名）。虽然在性能

上不及 GoogleNet，但

因为

VGG结构简单

，应用性强，所以很多技术

人员都喜欢使用基于

VGG的

网络。

8.2  深度学习的小历史

245

图8-9 VGG（根据文献[22]生成）

224 × 224

112

× 112

56 × 56

28

× 28 14 × 14 7

× 7

4096 4096

1000

3

3

fully connected

8.2.3　GoogLeNet

GoogLeNet的网络

结构如图8-10所示。图中的矩

形表示卷积层、池化

层等

。

图8-10 GoogLeNet（引用自文献[23]）

只看图的

话，这似乎是一个看上去

非常复杂的网络结构，但

实际上它基

本上和之前

介绍的CNN结构相同。不过，GoogLeNet的

特征是，网络不仅

在纵向

上有深度，在横向上也有

深度（广度）。

GoogLeNet在横向上有“宽

度”，这称为“Inception结构”，以图8-11所

示

的结构为基础。

如图8-11所示

，Inception结构使用了多个大小不

同的滤波器（和池化），

最后

再合并它们的结果。GoogLeNet的特

征就是将这个Inception结构用作

一个构件（构成元素）。此外

，在GoogLeNet中，很多地方都使用了

大小为

246

第 8章　深度学习

1 × 1的

滤波器的卷积层。这个1

× 1的

卷积运算通过在通道方

向上减小大小，

有助于减

少参数和实现高速化处

理（具体请参考原始论文

[23]）。

8.2.4　ResNet

ResNet[24]是微软团队开发的网络

。它的特征在于具有比以

前的网络更

深的结构。

我

们已经知道加深层对于

提升性能很重要。但是，在

深度学习中，过度

加深层

的话，很多情况下学习将

不能顺利进行，导致最终

性能不佳。ResNet中，

为了解决这

类问题，导入了“快捷结构

”（也称为“捷径”或“小路”）。导入

这

个快捷结构后，就可以

随着层的加深而不断提

高性能了（当然，层的加深

也

是有限度的）。

如图8-12所示

，快捷结构横跨（跳过）了输

入数据的卷积层，将输入

x合

计到输出。

图8-12中，在连续

2层的卷积层中，将输入x跳

着连接至2层后的输出。

这

里的重点是，通过快捷结

构，原来的2层卷积层的输

出F(x)变成了F(x) + x。

通过引入这种

快捷结构，即使加深层，也

能高效地学习。这是因为

，通过快

捷结构，反向传播

时信号可以无衰减地传

递。

图8-11 GoogLeNet的Inception结构（引用自文献

[23]）

8.2

深度学习的小历史  247

图8-12 ResNet的

构成要素（引用自文献[24]）：这

里的“weight layer”是指卷积层

F(x)

F(x) + x

x

x

weight layer

weight layer identity

relu

relu

因为快

捷结构只是原封不动地

传递输入数据，所以反向

传播时会将

来自上游的

梯度原封不动地传向下

游。这里的重点是不对来

自上游

的梯度进行任何

处理，将其原封不动地传

向下游。因此，基于快捷

结

构，不用担心梯度会变小

（或变大），能够向前一层传

递“有意义

的梯度”。通过这

个快捷结构，之前因为加

深层而导致的梯度变小

的

梯度消失问题就有望

得到缓解。

ResNet以前面介绍过

的VGG网络为基础，引入快捷

结构以加深层，其

结果如

图8-13所示。

图8-13 ResNet（引用自文献[24]）：方

块对应3×3的卷积层，其特征

在于引入了横跨层

的快

捷结构

如图8-13所示，ResNet通过以

2个卷积层为间隔跳跃式

地连接来加深层。

另外，根

据实验的结果，即便加深

到150层以上，识别精度也会

持续提高。并且，

在ILSVRC大赛中

，ResNet的错误识别率为3.5%（前5类中

包含正确解这一

精度下

的错误识别率），令人称奇

。

248

第 8章　深度学习

实践中经

常会灵活应用使用ImageNet这个

巨大的数据集学习到的

权

重数据，这称为迁移学

习，将学习完的权重（的一

部分）复制到其他

神经网

络，进行再学习（fine

tuning）。比如，准备

一个和 VGG相同

结构的网络

，把学习完的权重作为初

始值，以新数据集为对象

，进

行再学习。迁移学习在

手头数据集较少时非常

有效。

8.3 深度学习的高速化

随着大数据和网络的大

规模化，深度学习需要进

行大量的运算。虽然到

目

前为止，我们都是使用CPU进

行计算的，但现实是只用

CPU来应对深度

学习无法令

人放心。实际上，环视一下

周围，大多数深度学习的

框架都支持

GPU（Graphics Processing Unit），可以高速地

处理大量的运算。另外，最

近的框架也开始支持多

个GPU或多台机器上的分布

式学习。本节我们将焦

点

放在深度学习的计算的

高速化上，然后逐步展开

。深度学习的实现在8.1

节就

结束了，本节要讨论的高

速化（支持GPU等）并不进行实

现。

8.3.1　需要努力解决的问题

在介绍深度学习的高速

化之前，我们先来看一下

深度学习中什么样的处

理比较耗时。图8-14中以AlexNet的forward处

理为对象，用饼图展示了

各

层所耗费的时间。

从图

中可知，AlexNex中，大多数时间都

被耗费在卷积层上。实际

上，

卷积层的处理时间加

起来占GPU整体的95%，占CPU整体的

89%！因此，

如何高速、高效地进

行卷积层中的运算是深

度学习的一大课题。虽然

图8-14

是推理时的结果，不过

学习时也一样，卷积层中

会耗费大量时间。

正如 7.2节

介绍的那样，卷积层中进

行的运算可以追溯至乘

积累加运算。

因此，深度学

习的高速化的主要课题

就变成了如何高速、高效

地进

行大量的乘积累加

运算。

8.3 深度学习的高速化

249

图8-14 AlexNet的forward处理中各层的时间

比：左边是使用GPU的情况，右

边是使

用CPU的情况。图中的

“conv”对应卷积层，“pool”对应池化层

，“fc”对应全

连接层，“norm”对应正规

化层（引用自文献[26]）

8.3.2　基于GPU的

高速化

GPU原本是作为图像

专用的显卡使用的，但最

近不仅用于图像处理，

也

用于通用的数值计算。由

于GPU可以高速地进行并行

数值计算，因此

GPU计算的目

标就是将这种压倒性的

计算能力用于各种用途

。所谓GPU计算，

是指基于GPU进行

通用的数值计算的操作

。

深度学习中需要进行大

量的乘积累加运算（或者

大型矩阵的乘积运算）。

这

种大量的并行运算正是

GPU所擅长的（反过来说，CPU比较

擅长连续的、

复杂的计算

）。因此，与使用单个CPU相比，使

用GPU进行深度学习的运算

可以达到惊人的高速化

。下面我们就来看一下基

于GPU可以实现多大程度的

高速化。图8-15是基于CPU和GPU进行

AlexNet的学习时分别所需的时

间。

从图中可知，使用CPU要花

40天以上的时间，而使用GPU则

可以将时

间缩短至6天。此

外，还可以看出，通过使用

cuDNN这个最优化的库，可

以进

一步实现高速化。

GPU主要由

NVIDIA和AMD两家公司提供。虽然两

家的GPU都可以

用于通用的

数值计算，但与深度学习

比较“亲近”的是NVIDIA的GPU。实

际上

，大多数深度学习框架只

受益于NVIDIA的GPU。这是因为深度

学习

的框架中使用了NVIDIA提

供的CUDA这个面向GPU计算的综

合开发环境。

250

第 8章　深度学

习

图8-15中出现的cuDNN是在CUDA上运

行的库，它里面实现了为

深度学习

最优化过的函

数等。

图8-15 使用CPU 的“16-core Xeon CPU”和GPU的“Titan系列

”进行AlexNet的

学习时分别所需

的时间（引用自文献[27]）

通过

im2col可以将卷积层进行的运

算转换为大型矩阵的乘

积。这个

im2col方式的实现对 GPU来

说是非常方便的实现方

式。这是因为，

相比按小规

模的单位进行计算，GPU更擅

长计算大规模的汇总好

的

数据。也就是说，通过基

于im2col以大型矩阵的乘积的

方式汇总计算，

更容易发

挥出

GPU的能力。

8.3.3　分布式学习

虽然通过GPU可以实现深度

学习运算的高速化，但即

便如此，当网络

较深时，学

习还是需要几天到几周

的时间。并且，前面也说过

，深度学习伴

随着很多试

错。为了创建良好的网络

，需要反复进行各种尝试

，这样一来就

必然会产生

尽可能地缩短一次学习

所需的时间的要求。于是

，将深度学习的

学习过程

扩展开来的想法（也就是

分布式学习）就变得重要

起来。

为了进一步提高深

度学习所需的计算的速

度，可以考虑在多个GPU或

8.3  深

度学习的高速化

251

者多台

机器上进行分布式计算

。现在的深度学习框架中

，出现了好几个支持

多GPU或

者多机器的分布式学习

的框架。其中，Google的TensorFlow、微

软的CNTK（Computational Network Toolki）在

开发过程中高度重视分

布式

学习。以大型数据中

心的低延迟·高吞吐网络

作为支撑，基于这些框架

的分

布式学习呈现出惊

人的效果。

基于分布式学

习，可以达到何种程度的

高速化呢？图8-16中显示了基

于

TensorFlow的分布式学习的效果

。

图8-16 基于TensorFlow的分布式学习的

效果：横轴是GPU的个数，纵轴

是与单个

GPU相比时的高速

化率（引用自文献[28]）

如图8-16所

示，随着GPU个数的增加，学习

速度也在提高。实际上，

与

使用1个GPU时相比，使用100个GPU（设

置在多台机器上，共100个）

似

乎可以实现56倍的高速化

！这意味着之前花费7天的

学习只要3个小时就

能完

成，充分说明了分布式学

习惊人的效果。

关于分布

式学习，“如何进行分布式

计算”是一个非常难的课

题。它包

含了机器间的通

信、数据的同步等多个无

法轻易解决的问题。可以

将这些难

题都交给TensorFlow等优

秀的框架。这里，我们不讨

论分布式学习的细节。

关

于分布式学习的技术性

内容，请参考TensorFlow的技术论文

（白皮书）等。

252

第 8章　深度学习

8.3.4　运算精度的位数缩减

在

深度学习的高速化中，除

了计算量之外，内存容量

、总线带宽等也有

可能成

为瓶颈。关于内存容量，需

要考虑将大量的权重参

数或中间数据放在

内存

中。关于总线带宽，当流经

GPU（或者CPU）总线的数据超过某

个限制时，

就会成为瓶颈

。考虑到这些情况，我们希

望尽可能减少流经网络

的数据的位数。

计算机中

为了表示实数，主要使用

64位或者32位的浮点数。通过

使用

较多的位来表示数

字，虽然数值计算时的误

差造成的影响变小了，但

计算的

处理成本、内存使

用量却相应地增加了，还

给总线带宽带来了负荷

。

关于数值精度（用几位数

据表示数值），我们已经知

道深度学习并不那么

需

要数值精度的位数。这是

神经网络的一个重要性

质。这个性质是基于神经

网络的健壮性而产生的

。这里所说的健壮性是指

，比如，即便输入图像附有

一些小的噪声，输出结果

也仍然保持不变。可以认

为，正是因为有了这个健

壮性，流经网络的数据即

便有所“劣化”，对输出结果

的影响也较小。

计算机中

表示小数时，有32位的单精

度浮点数和64位的双精度

浮点数

等格式。根据以往

的实验结果，在深度学习

中，即便是16位的半精度浮

点

数（half float），也可以顺利地进行

学习[30]。实际上，NVIDIA的下一代GPU

框

架Pascal也支持半精度浮点数

的运算，由此可以认为今

后半精度浮点数将

被作

为标准使用。

NVIDIA的Maxwell GPU虽然支持

半精度浮点数的存储（保

存数据

的功能），但是运算

本身不是用 16位进行的。下

一代的 Pascal框架，

因为运算也

是用 16位进行的，所以只用

半精度浮点数进行计算

，就

有望实现超过上一代

GPU约 2倍的高速化。

以往的深

度学习的实现中并没有

注意数值的精度，不过Python中

一般

使用64位的浮点数。NumPy中

提供了16位的半精度浮点

数类型（不过，只

有16位类型

的存储，运算本身不用16位

进行），即便使用NumPy的半精度

浮点数，识别精度也不会

下降。相关的论证也很简

单，有兴趣的读者请参考

ch08/half_float_network.py。

8.4  深度学习的应用案例  253

关

于深度学习的位数缩减

，到目前为止已有若干研

究。最近有人提出了

用1位

来表示权重和中间数据

的Binarized Neural Networks方法[31]。为了实

现深度学

习的高速化，位数缩减是

今后必须关注的一个课

题，特别是在面向

嵌入式

应用程序中使用深度学

习时，位数缩减非常重要

。

8.4 深度学习的应用案例

前

面，作为使用深度学习的

例子，我们主要讨论了手

写数字识别的图

像类别

分类问题（称为“物体识别

”）。不过，深度学习并不局限

于物体识别，

还可以应用

于各种各样的问题。此外

，在图像、语音、自然语言等

各个不同

的领域，深度学

习都展现了优异的性能

。本节将以计算机视觉这

个领域为中

心，介绍几个

深度学习能做的事情（应

用）。

8.4.1　物体检测

物体检测是

从图像中确定物体的位

置，并进行分类的问题。如

图8-17所

示，要从图像中确定

物体的种类和物体的位

置。

图8-17

物体检测的例子（引

用自文献[34]）

254   第 8章

深度学习

观察图8-17可知，物体检测是

比物体识别更难的问题

。之前介绍的物体

识别是

以整个图像为对象的，但

是物体检测需要从图像

中确定类别的位置，

而且

还有可能存在多个物体

。

对于这样的物体检测问

题，人们提出了多个基于

CNN的方法。这些方

法展示了

非常优异的性能，并且证

明了在物体检测的问题

上，深度学习是非

常有效

的。

在使用CNN进行物体检测

的方法中，有一个叫作R-CNN[35]的

有名的方

法。图8-18显示了R-CNN的

处理流。

图8-18 R-CNN的处理流（引用

自文献[35]）

希望大家注意图

中的“2.Extract region

proposals”（候选区域的提取）和

“3.Compute CNN features”（CNN特征的计算）的处理部分

。这里，首先（以

某种方法）找

出形似物体的区域，然后

对提取出的区域应用CNN进

行分类。

R-CNN中会将图像变形

为正方形，或者在分类时

使用SVM（支持向量机），

实际的

处理流会稍微复杂一些

，不过从宏观上看，也是由

刚才的两个处理（候

选区

域的提取和CNN特征的计算

）构成的。

在R-CNN的前半部分的

处理——候选区域的提取（发

现形似目标物体的

处理

）中，可以使用计算机视觉

领域积累的各种各样的

方法。R-CNN的论文

中使用了一

种被称为Selective Search的方法，最近还

提出了一种基于CNN

来进行

候选区域提取的Faster

R-CNN方法[36]。Faster R-CNN用

一个CNN

来完成所有处理，使

得高速处理成为可能。

8.4  深

度学习的应用案例

255

8.4.2　图像

分割

图像分割是指在像

素水平上对图像进行分

类。如图8-19所示，使用以像

素

为单位对各个对象分别

着色的监督数据进行学

习。然后，在推理时，对输

入

图像的所有像素进行分

类。

图8-19　图像分割的例子（引

用自文献[34]）：左边是输入图

像，右边是监督用的带标

签图像

之前实现的神经

网络是对图像整体进行

了分类，要将它落实到像

素水平

的话，该怎么做呢

？

要基于神经网络进行图

像分割，最简单的方法是

以所有像素为对象，对

每

个像素执行推理处理。比

如，准备一个对某个矩形

区域中心的像素进行分

类的网络，以所有像素为

对象执行推理处理。正如

大家能想到的，这样的

方

法需要按照像素数量进

行相应次forward处理，因而需要

耗费大量的时间

（正确地

说，卷积运算中会发生重

复计算很多区域的无意

义的计算）。为了解

决这个

无意义的计算问题，有人

提出了一个名为FCN（Fully Convolutional

Network）[37]的方法

。该方法通过一次forward处理，对

所有像素进行分类（图

8-20）。

FCN的

字面意思是“全部由卷积

层构成的网络”。相对于一

般的CNN包

含全连接层，FCN将全

连接层替换成发挥相同

作用的卷积层。在物体识

别

中使用的网络的全连

接层中，中间数据的空间

容量被作为排成一列的

节点进

256

第 8章　深度学习

行

处理，而只由卷积层构成

的网络中，空间容量可以

保持原样直到最后的输

出。

如图8-20所示，FCN的特征在于

最后导入了扩大空间大

小的处理。基

于这个处理

，变小了的中间数据可以

一下子扩大到和输入图

像一样的大小。

FCN最后进行

的扩大处理是基于双线

性插值法的扩大（双线性

插值扩大）。

FCN中，这个双线性

插值扩大是通过去卷积

（逆卷积运算）来实现的（细

节请

参考FCN的论文[37]）。

图8-20 FCN的概

略图（引用自文献[37]）

全连接

层中，输出和全部的输入

相连。使用卷积层也可以

实现与此

结构完全相同

的连接。比如，针对输入大

小是 32×10×10（通道

数 32、高 10、长 10）的数据

的全连接层可以替换成

滤波器大小为

32×10×10的卷积层

。如果全连接层的输出节

点数是 100，那么在

卷积层准

备 100个 32×10×10的滤波器就可以实

现完全相同的处理。

像这

样，全连接层可以替换成

进行相同处理的卷积层

。

8.4.3　图像标题的生成

有一项

融合了计算机视觉和自

然语言的有趣的研究，该

研究如图8-21所

示，给出一个

图像后，会自动生成介绍

这个图像的文字（图像的

标题）。

给出一个图像后，会

像图8-21一样自动生成表示

该图像内容的文本。比

如

，左上角的第一幅图像生

成了文本“A

person riding a motorcycle on a

8.4  深度学习的应

用案例  257

dirt

road.”（在没有铺装的道

路上骑摩托车的人），而且

这个文本只从该图像

自

动生成。文本的内容和图

像确实是一致的。并且，令

人惊讶的是，除了“骑

摩托

车”之外，连“没有铺装的道

路”都被正确理解了。

一个

基于深度学习生成图像

标题的代表性方法是被

称为NIC（Neural

Image Caption）的模型。如图8-22所示，NIC由

深层的CNN和处理自然语

言

的RNN（Recurrent Neural Network）构成。RNN是呈递归式连接

的网络，

经常被用于自然

语言、时间序列数据等连

续性的数据上。

NIC基于CNN从图

像中提取特征，并将这个

特征传给RNN。RNN以

CNN提取出的特

征为初始值，递归地生成

文本。这里，我们不深入讨

论技

术上的细节，不过基

本上NIC是组合了两个神经

网络（CNN和RNN）的简单

结构。基于

NIC，可以生成惊人的高精度

的图像标题。我们将组合

图像和自

然语言等多种

信息进行的处理称为多

模态处理。多模态处理是

近年来备受关

注的一个

领域。

图8-21　基于深度学习的

图像标题生成的例子（引

用自文献[38]）

258   第 8章　深度学习

图8-22

Neural Image Caption（NIC）的整体结构（引用自文

献[38]）

RNN的R表示Recurrent（递归的）。这个递

归指的是神经网络的递

归

的网络结构。根据这个

递归结构，神经网络会受

到之前生成的信息

的影

响（换句话说，会记忆过去

的信息），这是

RNN的特征。比如

，

生成“我”这个词之后，下一

个要生成的词受到“我”这

个词的影响，

生成了“要”；然

后，再受到前面生成的“我

要”的影响，生成了“睡觉”

这

个词。对于自然语言、时间

序列数据等连续性的数

据，RNN以记

忆过去的信息的

方式运行。

8.5

深度学习的未

来

深度学习已经不再局

限于以往的领域，开始逐

渐应用于各个领域。本节

将介绍几个揭示了深度

学习的可能性和未来的

研究。

8.5.1　图像风格变换

有一

项研究是使用深度学习

来“绘制”带有艺术气息的

画。如图8-23所示，

输入两个图

像后，会生成一个新的图

像。两个输入图像中，一个

称为“内容

图像”，另一个称

为“风格图像”。

如图8-23所示，如

果指定将梵高的绘画风

格应用于内容图像，深度

学习

就会按照指示绘制

出新的画作。此项研究出

自论文“A Neural Algorithm of

8.5  深度学习的未来

259

Artistic Style”[39]，一经发表就受到全世界

的广泛关注。

这里我们不

会介绍这项研究的详细

内容，只是叙述一下这个

技术的大致

框架，即刚才

的方法是在学习过程中

使网络的中间数据近似

内容图像的中间

数据。这

样一来，就可以使输入图

像近似内容图像的形状

。此外，为了从风

格图像中

吸收风格，导入了风格矩

阵的概念。通过在学习过

程中减小风格矩

阵的偏

差，就可以使输入图像接

近梵高的风格。

8.5.2

图像的生

成

刚才的图像风格变换

的例子在生成新的图像

时输入了两个图像。不同

于

这种研究，现在有一种

研究是生成新的图像时

不需要任何图像（虽然需

要事

图8-23 基于论文“A Neural

Algorithm of Artistic Style”的图像

风格变换的例子：左上

角

是风格图像，右上角是内

容图像，下面的图像是新

生成的图像（图像引用自

文献[40]）

260

第 8章　深度学习

先使

用大量的图像进行学习

，但在“画”新图像时不需要

任何图像）。比如，

基于深度

学习，可以实现从零生成

“卧室”的图像。图8-24中展示的

图像是

基 于 DCGAN（Deep Convolutional Generative Adversarial

Network）[41] 方

法生成的

卧室图像的例子。

图8-24　基于

DCGAN生成的新的卧室图像（引

用自文献[41]）

图8-24的图像可能

看上去像是真的照片，但

其实这些图像都是基于

DCGAN新生成的图像。也就是说

，DCGAN生成的图像是谁都没有

见过的

图像（学习数据中

没有的图像），是从零生成

的新图像。

能画出以假乱

真的图像的DCGAN会将图像的

生成过程模型化。使用大

量图像（比如，印有卧室的

大量图像）训练这个模型

，学习结束后，使用这

个模

型，就可以生成新的图像

。

DCGAN中使用了深度学习，其技

术要点是使用了Generator（生成者

）

和Discriminator（识别者）这两个神经网

络。Generator生成近似真品的图

像

，Discriminator判别它是不是真图像（是

Generator生成的图像还是实际

拍

摄的图像）。像这样，通过让

两者以竞争的方式学习

，Generator会学习到

更加精妙的图

像作假技术，Discriminator则会成长为

能以更高精度辨别真假

的鉴定师。两者互相切磋

、共同成长，这是GAN（Generative Adversarial

8.5 深度学习

的未来

261

Network）这个技术的有趣

之处。在这样的切磋中成

长起来的Generator最终

会掌握画

出足以以假乱真的图像

的能力（或者说有这样的

可能）。

之前我们见到的机

器学习问题都是被称为

监督学习（supervised

learning）的问题。这类问

题就像手写数字识别一

样，使用的是图像

数据和

教师标签成对给出的数

据集。不过这里讨论的问

题，并没有

给出监督数据

，只给了大量的图像（图像

的集合），这样的问题称为

无监督学习（unsupervised learning）。无监督学习

虽然是很早之前就

开始

研究的领域（Deep Belief Network、Deep Boltzmann

Machine

等很有名），但

最近似乎并不是很活跃

。今后，随着使用深度学习

的

DCGAN等方法受到关注，无监

督学习有望得到进一步

发展。

8.5.3　自动驾驶

计算机代

替人类驾驶汽车的自动

驾驶技术有望得到实现

。除了汽车制造

商之外，IT企

业、大学、研究机构等也都

在为实现自动驾驶而进

行着激烈

的竞争。自动驾

驶需要结合各种技术的

力量来实现，比如决定行

驶路线的路

线计划（path plan）技术

、照相机或激光等传感技

术等，在这些技术中，正

确

识别周围环境的技术据

说尤其重要。这是因为要

正确识别时刻变化的环

境、

自由来往的车辆和行

人是非常困难的。

如果可

以在各种环境中稳健地

正确识别行驶区域的话

，实现自动驾驶可

能也就

没那么遥远了。最近，在识

别周围环境的技术中，深

度学习的力量备

受期待

。比如，基于CNN的神经网络SegNet[42]，可

以像图8-25那样高精度

地识

别行驶环境。

图8-25中对输入

图像进行了分割（像素水

平的判别）。观察结果可知

，在

某种程度上正确地识

别了道路、建筑物、人行道

、树木、车辆等。今后若能

基

于深度学习使这种技术

进一步实现高精度化、高

速化的话，自动驾驶的实

用化可能也就没那么遥

远了。

262   第 8章

深度学习

图8-25 基

于深度学习的图像分割

的例子：道路、车辆、建筑物

、人行道等被高精度地识

别了出来（引用自文献[43]）

8.5.4　Deep Q-Network（强

化学习）

就像人类通过摸

索试验来学习一样（比如

骑自行车），让计算机也在

摸索

试验的过程中自主

学习，这称为强化学习（reinforcement learning）。强

化学

习和有“教师”在身边

教的“监督学习”有所不同

。

强化学习的基本框架是

，代理（Agent）根据环境选择行动

，然后通过这

个行动改变

环境。根据环境的变化，代

理获得某种报酬。强化学

习的目的是

决定代理的

行动方针，以获得更好的

报酬（图8-26）。

图8-26中展示了强化

学习的基本框架。这里需

要注意的是，报酬并不是

确定的，只是“预期报酬”。比

如，在《超级马里奥兄弟》这

款电子游戏中，

让马里奥

向右移动能获得多少报

酬不一定是明确的。这时

需要从游戏得分（获

得的

硬币、消灭的敌人等）或者

游戏结束等明确的指标

来反向计算，决定“预

期报

酬”。如果是监督学习的话

，每个行动都可以从“教师

”那里获得正确的

评价。

在

使用了深度学习的强化

学习方法中，有一个叫作

Deep Q-Network（通

称DQN）[44]的方法。该方法基于

被称为Q学习的强化学习

算法。这里省略

8.5 深度学习

的未来

263

Q学习的细节，不过

在Q学习中，为了确定最合

适的行动，需要确定一个

被

称为最优行动价值函

数的函数。为了近似这个

函数，DQN使用了深度学习

（CNN）。

在

DQN的研究中，有让电子游戏

自动学习，并实现了超过

人类水平的

操作的例子

。如图8-27所示，DQN中使用的CNN

把游

戏图像的帧（连续4帧）

作为

输入，最终输出游戏手柄

的各个动作（控制杆的移

动量、按钮操作的有

无等

）的“价值”。

之前在学习电子

游戏时，一般是把游戏的

状态（人物的地点等）事先

提

取出来，作为数据给模

型。但是，在DQN中，如图8-27所示，输

入数据

只有电子游戏的

图像。这是DQN值得大书特书

的地方，可以说大幅提高

了

DQN的实用性。为什么呢？因

为这样就无需根据每个

游戏改变设置，只要

给DQN游

戏图像就可以了。实际上

，DQN 可以用相同的结构学习

《吃豆人》、

Atari等很多游戏，甚至

在很多游戏中取得了超

过人类的成绩。

人工智能

AlphaGo[45]击败围棋冠军的新闻受

到了广泛关注。这个

AlphaGo技术

的内部也用了深度学习

和强化学习。AlphaGo学习了

3000万个

专业棋手的棋谱，并且不

停地重复自己和自己的

对战，积

累了大量的学习

经验。AlphaGo和 DQN都是 Google的 Deep Mind

公司进行

的研究，该公司今后的研

究值得密切关注。

图8-26　强化

学习的基本框架：代理自

主地进行学习，以获得更

好的报酬

代理

行动

环境

报酬（观测）

264   第 8章　深度学习

8.6

小结

本章我们实现了一

个（稍微）深层的CNN，并在手写

数字识别上获得了

超过

99%的高识别精度。此外，还讲

解了加深网络的动机，指

出了深度学习

在朝更深

的方向前进。之后，又介绍

了深度学习的趋势和应

用案例，以及对

高速化的

研究和代表深度学习未

来的研究案例。

深度学习

领域还有很多尚未揭晓

的东西，新的研究正一个

接一个地出现。

今后，全世

界的研究者和技术专家

也将继续积极从事这方

面的研究，一定能

实现目

前无法想象的技术。

感谢

读者一直读到本书的最

后一章。如果读者能通过

本书加深对深度学

习的

理解，体会到深度学习的

有趣之处，笔者将深感荣

幸。

图8-27 基于Deep

Q-Network学习电子游戏

的操作。输入是电子游戏

的图像，经过摸索

试验，学

习出让专业玩家都自愧

不如的游戏手柄（操作杆

）的操作手法（引用自文

献

[44]）

8.6 小结

265

本章所学的内容

• 对

于大多数的问题，都可以

期待通过加深网络来提

高性能。

• 在最近的图像识

别大赛ILSVRC中，基于深度学习

的方法独占鳌头，

使用的

网络也在深化。

• VGG、GoogLeNet、ResNet等是几个

著名的网络。

• 基于GPU、分布式

学习、位数精度的缩减，可

以实现深度学习的高速

化。

•

深度学习（神经网络）不

仅可以用于物体识别，还

可以用于物体检测、

图像

分割。

• 深度学习的应用包

括图像标题的生成、图像

的生成、强化学习等。最近

，

深度学习在自动驾驶上

的应用也备受期待。

附录

A

Softmax-with-Loss层的计算图

这里，我们给

出softmax函数和交叉熵误差的

计算图，来求它们的反向

传播。softmax函数称为softmax层，交叉熵

误差称为Cross Entropy Error层，

两者的组合

称为

Softmax-with-Loss 层。先来看一下结果

，Softmax-withLoss层可以画成图A-1所示的计

算图。

Softmax

L

1

Cross

Entropy

Error

a1

a2

a3

y1−

t 1

y2− t 2

y3−

t 3

t 1

y1

t

2

y2

t 3

y3

图A-1

Softmax-with-Loss层的计算图

268   附录

A　Softmax-with-Loss层的计算图

图A-1的计算图

中假定了一个进行3类别

分类的神经网络。从前面

的层

输入的是(a1, a2, a3)，softmax层输出(y1, y2, y3)。此

外，教师标签是(t1,

t2, t3)，

Cross Entropy Error层输出损

失L。

如图A-1所示，在本附录中

，Softmac-with-Loss层的反向传播的结果为

(y1

− t1, y2 − t2, y3

− t3)。

A.1 正向传播

图A-1的计算图中

省略了Softmax层和Cross Entropy

Error层的内容。

这

里，我们来画出这两个层

的内容。

首先是Softmax层。softmax函数可

由下式表示。

（A.1）

因此，用计算

图表示Softmax层的话，则如图A-2所

示。

图A-2的计算图中，指数的

和（相当于式(A.1)的分母）简写

为S，最终

的输出记为(y1, y2, y3)。

接下

来是Cross Entropy Error层。交叉熵误差可由

下式表示。

（A.2）

根据式(A.2)，Cross Entropy Error层的计

算图可以画成图A-3那样。

图

A-3的计算图很直观地表示

出了式(A.2)，所以应该没有特

别难的地方。

下一节，我们

将看一下反向传播。

A.1 正向

传播  269

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2) + exp(a3)

S

1

S

1

y1

=

S

exp(a1)

y2

=

S

exp(a2)

y3

=

S

exp(a3)

exp(a1)

图A-2

Softmax层的计算图（仅正

向传播）

图A-3 Cross Entropy Error层的计算图（仅

正向传播）

log

log

log

L

t 1

y1

t 2

y2

t 3

y3

−1

log y1

log y2

log

y3

t 1 log y1

t

2 log y2

t 3 log

y3

t1 log y1 + t2

log y2 + t3 log y3

270   附录A　Softmax-with-Loss层的计算

图

A.2

反向传播

首先是Cross Entropy Error层的

反向传播。Cross Entropy Error层的

反向传播

可以画成图A-4那样。

图A-4　交叉

熵误差的反向传播

log

log

log

L

t 1

y1

t 2

y2

t 3

y3

−1

log

y1

log y2

log y3

t

1 log y1

t 2 log

y2

t 3 log y3

t1

log y1 + t2 log y2

+ t3 log y3

−1 −1

1

−1

−1

−t 1

−t

2

−t 3

t 1 −

y1

t 2 − y2

t

3 − y3

求这

个计算图的反向传播时

，要注意下面几点。

• 反向传

播的初始值（图A-4中最右边

的值）是1（因为

）。

•“ ×”节点的反向

传播将正向传播时的输

入值翻转，乘以上游传过

来的

导数后，再传给下游

。

•“+”节点将上游传来的导数

原封不动地传给下游。

•“log”节

点的反向传播遵从下式

。

A.2 反向传播  271

遵从以上几点

，就可以轻松求得Cross Entropy

Error的反向

传播。结

果 是传给Softmax层的反

向传播的输入。

下面是Softmax层

的反向传播的步骤。因为

Softmax层有些复杂，所以

我们来

逐一进行确认。

步骤1

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2)

+ exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1 −

y1

t 2 − y2

t

3 − y3

前面

的层（Cross Entropy Error层）的反向传播的值

传过来。

272   附录A　Softmax-with-Loss层的计算图

步骤2

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2) +

exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1 - y1

t 2 - y2

t 3

- y3

−t 1S

−t 2S

−t 3S

“×”节点将正向传播的

值翻转后相乘。这个过程

中会进行下面的计算。

（A.3）

A.2 反

向传播

273

步骤3

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1)

+ exp(a2) + exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t

1 − y1

t 2 −

y2

t 3 − y3

−t

1S

−t 2S

−t 3S

S

1 (t 1 + t 2

+ t 3)

= S

1

正向传播时

若有分支流出，则反向传

播时它们的反向传播的

值会相加。

因此，这里分成

了三支的反向传播的值

(−t1S, −t2S, −t3S)会被求和。然后，

还要对这

个相加后的值进行“/”节点

的反向传播，结果为 。

这里

，(t1, t2, t3)是教师标签，也是one-hot向量。one-hot向

量意味着(t1, t2, t3)

中只有一个元

素是1，其余都是0。因此，(t1,

t2, t3)的和

为1。

274   附录A

Softmax-with-Loss层的计算图

步骤

4

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1)

+ exp(a2) + exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t

1 − y1

t 2 −

y2

t 3 − y3

−t

1S

−t 2S

−t 3S

S

1

S

1

“+”节点原封不动地传递上

游的值。

A.2 反向传播

275

步骤5

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1)

+ exp(a2) + exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t

1 − y1

t 2 −

y2

t 3 − y3

−t

1S

−t 2S

−t 3S

S

1

S

1

t 1 −

y1

t 1 − S exp(a1)

1 =

“×”节

点将值翻转后相乘。这里

，式子变形时使用了 。

276

附录

A　Softmax-with-Loss层的计算图

步骤6

exp

exp

exp

S

a1

a2

a3

=

S

1

exp(a1)

exp(a2)

exp(a3)

exp(a2)

exp(a3)

exp(a1) + exp(a2) + exp(a3)

S

1

S

1

y1

= S

exp(a1)

S

exp(a2)

S

exp(a3)

exp(a1)

t 1 − y1

t 2

− y2

t 3 − y3

−t 1S

−t 2S

−t 3S

S

1

S

1

t 1

− exp(a1) y1 − t 1

S

exp(a1) − t 1 =

“exp”节点中

有下面的关系式成立。

（A.4）

根

据这个式子，向两个分支

的输入和乘以exp(a1)后的值就

是我们要求

的反向传播

。用式子写出来的话，就是

，整理之后为

y1 −

t1。综上，我们推

导出，正向传播时输入是

a1的节点，它的反向传播是

y1 − t1。剩下的a2、a3也可以按照相同

的步骤求出来（结果分别

为y2 − t2和

y3

− t3）。此外，除了这里介绍

的3类别分类外，对于n类别

分类的情况，也

可以推导

出同样的结果。

A.3 小结

277

A.3 小结

上面，我们画出了Softmax-with-Loss层的计

算图的全部内容，并求了

它的反向传播。未做省略

的Softmax-with-Loss层的计算图如图A-5所示

。

图A-5 Softmax-with-Loss层的计算图

×

×

×

×

log

log

log

×

×

exp

exp

exp

＋

/ S

L

Softmax 层 Cross

Entropy Error 层

＋ ×

a1

a2

a3

y1− t1

y2− t2

y3− t3

exp(a1)

exp(a2)

exp(a3)

S

1

S

1

S

1

S

1

exp(a1)

exp(a1) − t1

exp(a2)

exp(a2) − t2

exp(a3)

exp(a3) −

t3

−t3S

−t1S

−t2S

S

1

S

1

S

1

t1

t2

t3

y1

y2

y3

y1 −t1

y2 −t2

y3 −t3

log y1

log y2

log y3

− t1

− t2

− t3

t1 log

y1 + t2 log y2 +

t3 log y3

t1 log y1

t2 log y2

t3 log y3

−1

−1

−1

−1

−1

1

图A-5的

计算图看上去很复杂，但

是使用计算图逐个确认

的话，求导（反

向传播的步

骤）也并没有那么复杂。除

了这里介绍的Softmax-with-Loss层，

遇到其

他看上去很难的层（如Batch Normalization层

）时，请一定按照这里

的步

骤思考一下。相信会比只

看数学式更容易理解。

参

考文献

Python / NumPy

[1] Bill

Lubanovic. Introducing PythonA. O’Reilly Media, 2014.

[2] Wes McKinney. Python for Data

AnalysisB. O’Reilly Media.

[3] Scipy Lecture

Notes.

计算图（误差反向

传播法）

[4] Andrej Karpathy’s blog

“Hacker’s guide to Neural Networks”.

深度学习的在线

课程（资料）

[5] CS231n: Convolutional Neural Networks for

Visual Recognition.

参数的更新方

法

[6] John Duchi,

Elad Hazan, and Yoram Singer（2011）: Adaptive

Subgradient Methods for Online Learning and

Stochastic Optimization.

A 中文版名为《Python语言及其

应用》，梁杰等译，人民邮电

出版社2015年出版。—编者注

B 中

文版名为《利用Python进行数据

分析》，唐学韬译，机械工业

出版社2013年出版。—编者注

280   参

考文献

Journal of

Machine Learning Research 12, Jul (2011),

2121 – 2159.

[7] Tieleman, T.,

& Hinton, G.（2012）: Lecture 6.5―RMSProp: Divide

the

gradient by a running average

of its recent magnitude. COURSERA:

Neural

Networks for Machine Learning.

[8] Diederik

Kingma and Jimmy Ba.（2014）: Adam: A

Method for

Stochastic Optimization. arXiv:1412.6980 [cs]

(December 2014).

权重参数的初始

值

[9] Xavier Glorot

and Yoshua Bengio（2010）: Understanding the difficulty

of training deep feedforward neural networks.

In Proceedings of the

International Conference

on Artificial Intelligence and Statistics

(AISTATS2010).

Society for Artificial Intelligence and Statistics.

[10] Kaiming He, Xiangyu Zhang, Shaoqing

Ren, and Jian Sun（2015）:

Delving Deep

into Rectifiers: Surpassing Human-Level Performance on

ImageNet Classification. In 1026 – 1034.

Batch Normalization / Dropout

[11] Sergey

Ioffe and Christian Szegedy（2015）: Batch Normalization:

Accelerating Deep Network Training by Reducing

Internal Covariate

Shift. arXiv:1502.03167 [cs] (February

2015).

[12] Dmytro Mishkin and Jiri

Matas（2015）: All you need is a

good init.

arXiv:1511.06422 [cs] (November 2015).

[13] Frederik Kratzert’s blog “Understanding the

backward pass through Batch

Normalization Layer”.

[14] N. Srivastava, G. Hinton, A.

Krizhevsky, I. Sutskever, and R.

Salakhutdinov（2014）:

Dropout: A simple way to prevent

neural

参考文献  281

networks from

overfitting. The Journal of Machine Learning

Research,

pages 1929 – 1958, 2014.

超参数的最

优化

[15] James Bergstra and Yoshua

Bengio（2012）: Random Search for HyperParameter Optimization.

Journal of Machine Learning Research 13,

Feb (2012), 281 – 305.

[16]

Jasper Snoek, Hugo Larochelle, and Ryan

P. Adams（2012）: Practical

Bayesian Optimization of

Machine Learning Algorithms. In F. Pereira,

C. J. C. Burges, L. Bottou,

& K. Q. Weinberger, eds. Advances

in

Neural Information Processing Systems 25.

Curran Associates, Inc.,

2951 – 2959.

CNN的可视化

[17] Matthew D. Zeiler and

Rob Fergus（2014）: Visualizing and

Understanding Convolutional

Networks. In David Fleet, Tomas Pajdla,

Bernt Schiele, & Tinne Tuytelaars, eds.

Computer Vision – ECCV 2014.

Lecture

Notes in Computer Science. Springer International

Publishing,

818 – 833.

[18] A.

Mahendran and A. Vedaldi（2015）: Understanding deep

image

representations by inverting them. In

2015 IEEE Conference on

Computer Vision

and Pattern Recognition (CVPR). 5188 –

5196.

[19] Donglai Wei, Bolei Zhou,

Antonio Torralba, William T. Freeman

（2015）:

mNeuron: A Matlab Plugin to Visualize

Neurons from Deep

Models.

282

参考文献

具有代表性的网格

[20] Y. Lecun, L. Bottou,

Y. Bengio, and P. Haffner（1998）: Gradient-based

learning applied to document recognition. Proceedings

of the IEEE 86,

11 (November

1998), 2278 – 2324.

[21] Alex

Krizhevsky, Ilya Sutskever, and Geoffrey E.

Hinton（2012）:

ImageNet Classification with Deep Convolutional

Neural Networks.

In F. Pereira, C.

J. C. Burges, L. Bottou, &

K. Q. Weinberger, eds.

Advances in

Neural Information Processing Systems 25. Curran

Associates, Inc., 1097 – 1105.

[22]

Karen Simonyan and Andrew Zisserman（2014）: Very

Deep

Convolutional Networks for Large-Scale Image

Recognition.

arXiv:1409.1556 [cs] (September 2014).

[23]

Christian Szegedy et al（2015）: Going Deeper

With Convolutions. In

The IEEE Conference

on Computer Vision and Pattern Recognition

(CVPR).

[24] Kaiming He, Xiangyu Zhang,

Shaoqing Ren, and Jian Sun（2015）:

Deep

Residual Learning for Image Recognition. arXiv:1512.03385

[cs]

(December 2015).

数据

集

[25] J.

Deng, W. Dong, R. Socher, L.J.

Li, Kai Li, and Li Fei-Fei

（2009）:

ImageNet: A large-scale hierarchical image

database. In IEEE

Conference on Computer

Vision and Pattern Recognition, 2009. CVPR

2009. 248 – 255.

参考文献

283

计算的高速

化

[26] Jia Yangqing（2014）: Learning

Semantic Image Representations at a

Large

Scale. PhD thesis, EECS Department, University

of California,

Berkeley, May 2014.

[27]

NVIDIA blog “NVIDIA Propels Deep Learning

with TITAN X, New

DIGITS Training

System and DevBox”.

[28] Google Research

Blog “Announcing TensorFlow 0.8 – now

with

distributed computing support!”.

[29] Martín

Abadi et al（2016）: TensorFlow: Large-Scale Machine

Learning on

Heterogeneous Distributed Systems. arXiv:1603.04467

[cs] (March 2016).

[30] Suyog Gupta,

Ankur Agrawal, Kailash Gopalakrishnan, and Pritish

Narayanan（2015）: Deep learning with limited numerical

precision.

CoRR, abs/1502.02551 392 (2015).

[31]

Matthieu Courbariaux and Yoshua Bengio（2016）: Binarized

Neural

Networks: Training Deep Neural Networks

with Weights and

Activations Constrained to

+1 or -1. arXiv preprint arXiv:1602.02830

(2016).

MNIST数据集识别精度排行

榜及最高精度的方法

[32] Rodrigo Benenson’s blog

“Classification datasets results”.

[33] Li Wan,

Matthew Zeiler, Sixin Zhang, Yann L.

Cun, and Rob Fergus

（2013）: Regularization

of Neural Networks using DropConnect. In

Sanjoy Dasgupta & David McAllester, eds.

Proceedings of the 30th

International Conference

on Machine Learning (ICML2013). JMLR

Workshop

and Conference Proceedings, 1058 – 1066.

284   参

考文献

深度学习的应用

[34] Visual

Object Classes Challenge 2012 VO(2012).

[35]

Ross Girshick, Jeff Donahue, Trevor Darrell,

and Jitendra Malik（2014）:

Rich Feature Hierarchies

for Accurate Object Detection and Semantic

Segmentation. In 580 – 587.

[36]

Shaoqing Ren, Kaiming He, Ross Girshick,

and Jian Sun（2015）: Faster

R-CNN: Towards

Real-Time Object Detection with Region Proposal

Networks. In C. Cortes, N. D.

Lawrence, D. D. Lee, M. Sugiyama,

& R.

Garnett, eds. Advances in

Neural Information Processing Systems 28.

Curran

Associates, Inc., 91 – 99.

[37]

Jonathan Long, Evan Shelhamer, and Trevor

Darrell（2015）: Fully

Convolutional Networks for Semantic

Segmentation. In The IEEE

Conference on

Computer Vision and Pattern Recognition (CVPR).

[38] Oriol Vinyals, Alexander Toshev, Samy

Bengio, and Dumitru Erhan

（2015）: Show

and Tell: A Neural Image Caption

Generator. In The

IEEE Conference on

Computer Vision and Pattern Recognition

(CVPR).

[39] Leon A. Gatys, Alexander S.

Ecker, and Matthias Bethge（2015）: A

Neural

Algorithm of Artistic Style. arXiv:1508.06576 [cs,

q-bio] (August

2015).

[40] neural-style “Torch

implementation of neural style algorithm”.

[41]

Alec Radford, Luke Metz, and Soumith

Chintala（2015）: Unsupervised

Representation Learning with Deep

Convolutional Generative

Adversarial Networks. arXiv:1511.06434 [cs]

(November 2015).

[42] Vijay Badrinarayanan, Kendall,

and Roberto Cipolla（2015）: SegNet:

A Deep

Convolutional Encoder-Decoder Architecture for Image

Segmentation.

arXiv preprint arXiv:1511.00561 (2015).

参考文献

285

[43] SegNet Demo page.

[44]

Volodymyr Mnih et al（2015）: Human-level control

through deep

reinforcement learning. Nature 518,

7540 (2015), 529 – 533.

[45]

David Silver et al（2016）: Mastering the

game of Go with deep neural

networks and tree search. Nature 529,

7587 (2016), 484 – 489.

Copyright

©2016 Koki Saitoh, O’Reilly Japan, Inc.

All rights reserved.

本书中使用的

系统名、产品名都是各公

司的商标或注册商标。

正

文中有时会省略TM、®、©等标识

。

株式会社O’

Reilly Japan尽最大努力确

保了本书内容的正确性

，但对运用本书内容所

造

成的任何结果概不负责

，敬请知悉。
