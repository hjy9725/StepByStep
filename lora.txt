1

{

}

}

{

LORA: LOW-RANK ADAPTATION

OF LARGE LAN￾GUAGE MODELS

劳拉: 低级的

适应 关于 大

的

语言模型

Edward Hu∗ Yelong Shen∗

Phillip Wallis Zeyuan Allen-Zhu

Yuanzhi Li Shean Wang

Lu Wang Weizhu Chen

胡沈王陈

Microsoft Corporation

微

软公司

yuanzhil,

yuanzhil@andrew.cmu.edu

edwardhu, yeshe,

phwallis, zeyuana, 

swang,

luw, wzchen @microsoft.com

edwardhu，yeshe，phwallis，zeyuana，yuanzhil，swang

，

(Version

luw，wzchen

@microsoft.com

2)

yuanzhil@andrew.cmu.edu

(第二

版)

ABSTRACT

摘要

An important

paradigm of natural language

processing consists of large-scale

pre- training on general

domain data and adaptation

to particular tasks or

domains. As we pre-train

larger models, full fine-tuning,

which retrains all

model

parameters, becomes less feasible.

Using GPT-3 175B as

an example –

deploying

indepen- dent instances of

fine-tuned models, each with

175B

parameters, is prohibitively

expensive. We propose Low-Rank

Adaptation, or

LoRA, which

freezes the pre- trained

model weights and injects

trainable rank

decomposition matrices

into each layer of

the Transformer architecture, greatly

reducing the number of

trainable pa- rameters for

downstream tasks. Compared

to

GPT-3 175B fine-tuned with

Adam, LoRA can reduce

the number of trainable

parameters by 10,000 times

and the GPU memory

requirement by 3 times.

LoRA performs on-par or

better than fine- tuning

in model quality on

RoBERTa,

DeBERTa, GPT-2, and

GPT-3, despite hav- ing

fewer trainable parameters, a

higher training throughput, and,

unlike adapters, no additional

inference latency.

We also

provide an empirical investigation

into rank-deficiency in language

model adaptation, which sheds

light on the efficacy

of LoRA. We release

a

package that facilitates

the integration of LoRA

with PyTorch models and

provide our implementations and

model checkpoints for RoBERTa,

DeBERTa,

and GPT-2 at

https://github.com/microsoft/LoRA.

自然

语言处理的

一个重要范

例包括对一

般领域数据

的大规模预

训练和对特

定任务或领

域的适应。当

我们预训练

更大的模型

时，重新训练

所有模型参

数的完全微

调变得不太

可行。以 GPT-3 175B

为例

——部署微调模

型的独立

实

例，每个实例

都有 175B 参数，这

是非常昂贵

的。我们提出

了低秩自适

应，即

LoRA，它冻结

了预先训练

的模型权重

，并将可训练

的秩分解矩

阵注

入到变

换器架构的

每一层中，从

而大大减少

了用于下游

任务的可训

练参数

2

𝐵

= 0

Pretrained Weights

𝑊 ∈ ℝ𝑑×𝑑

𝐵

= 0

Pretrained Weights

𝑊 ∈ ℝ𝑑×𝑑

𝐴

= 𝒩(0, 𝜎2)

𝑑

x

𝐴 = 𝒩(0,

𝜎2)

𝑑

x

的数

量。与使用

Adam 微

调的 GPT-3 175B

相比，LoRA 可

以将可训练

参数的

数量

减少 10，000

倍，并将

GPU 内存要求减

少 3 倍。LoRA

在罗伯

塔、德伯

塔、GPT-2 和

GPT-3 上的模型质

量表现相当

于或优于微

调，尽管具有

更少

的可训

练参数、更高

的训练吞吐

量，并且与适

配器不同，没

有额外的推

理

延迟。我们

还提供了一

个关于语言

模型适应中

秩亏的实证

研究，它揭示

了

LoRA 的有效性

。我们发布了

一个有助于

集成

LoRA 和 PyTorch 模型

的包，

并在 https://github.com/microsoft/LoRA。

1 INTRODUCTION

2 介

绍

Many applications

in natural language processing

rely on adapt-

自然语言

处理中的许

多应用依赖

于

adapt￾ing one large-scale, pre-trained

language model to multiple

down- h

将一个大

规模的、预先

训练好的语

言模型扩展

到多个 h

stream applications. Such adaptation

is usually done via

fine￾tuning, which updates all

the parameters of the

pre-trained model.

The ma-

jor downside of fine-tuning

is that the new

model contains

as many

流应

用程序。这种

适应通常通

过微调来完

成，微调更新

预训练

模型

的所有参数

。微调的主要

缺点是新模

型包含尽可

能多的

parameters as

in the original model.

As larger models are

trained 𝑟

原始

模型中的参

数。随着更大

的模型被训

练 𝑟

every few months, this

changes from a mere

“inconvenience” for

GPT-2 (Radford

et al., b) or

RoBERTa large (Liu et

al., 2019) to a

critical deployment challenge for

GPT-3 (Brown et al.,

2020) with

175 billion

trainable parameters.1

每隔几个

月，这对 GPT-2

号来

说仅仅是“不

方便”(Radford

et al.,b)或者罗

伯塔·拉奇(Liu et

al.,2019)GPT 3 号

的关

键部署

挑战(Brown

et al.,2020)有 1750 亿个

可训练参数

。1

Many sought to mitigate

this by adapting only

some parameters or

learning

external modules for new

tasks. This way, we

only need

to store

and load a small

number of task-specific parameters

in ad￾dition to the

pre-trained model for each

task, greatly boosting the

operational efficiency when deployed.

However, existing techniques

许多人试图

通过调整一

些参数或学

习新任务的

外部模块来

缓解

这一问

题。这样，除了

每个任务的

预训练模型

之外，我们只

需

要存储和

加载少量特

定于任务的

参数，大大提

高了部署时

的操

作效率

。然而，现有的

技术

∗Equal

contribution.

*同等贡

献。

3

Figure

1: Our reparametriza- tion.

We only train A

and B.

图 1:我们的

重新参数化

。我

们只训练

A 和 b。

0Compared

to V1, this draft

includes better baselines, experiments

on GLUE, and more

on adapter latency.

0

与 V1 相比，该

草案包括更

好的基线、胶

水实验和更

多关于适配

器延迟的内

容。

1While

GPT-3 175B achieves non-trivial

performance with few-shot learning,

fine-tuning boosts its perfor￾mance

significantly as shown in

Appendix A.

1 虽然

GPT-3 175B 通过

少量学习实

现了非同寻

常的性能，但

微调显著提

升了其性能

，如所示 Appendix

A.

a

r

X

i

v:2

1

0

6.0

9

6

8

5

v

2 [c

s.C

L] 1

6

O

c

t

a

r

X

i

v:2

1

0

6.0

9

6

8

5

v

2

[

c

s.C

L

]

1

6

often introduce inference latency

(Houlsby et al., 2019;

Rebuffi et al., 2017)

by extending model

depth

or reduce the model’s

usable sequence length (Li

& Liang, 2021; Lester

et al., 2021; Ham￾bardzumyan

et al., 2020; Liu

et al., 2021) (Section

3). More importantly, these

method often fail to

match the fine-tuning baselines,

posing a trade-off between

efficiency and model quality.

通常

会引入推理

延迟(Houlsby et al.,2019;Rebuffi et

al.,2017)通过扩

展模型深度

或减

少 模 型

的

可 用 序 列

长

度 (Li & Liang,2021;Lester

et al.,2021;Ham-bardzumyan et

al.,2020;Liu

et al.,2021)(Section3).更重要

的是，这些方

法经常无法

匹配微调基

线，

从而在效

率和模型质

量之间进行

权衡。

We

take inspiration from Li

et al. (2018a); Aghajanyan

et al. (2020) which

show that the learned

over-parametrized models in fact

reside on a low

intrinsic dimension. We hypothesize

that the

change in

weights during model adaptation

also has a low

“intrinsic rank”, leading to

our proposed

Low-Rank Adaptation

(LoRA) approach. LoRA allows

us to train some

dense layers in a

neural

network indirectly by

optimizing rank decomposition matrices

of the dense layers’

change during

adaptation instead,

while keeping the pre-trained

weights frozen, as shown

in Figure 1. Using

GPT-3 175B as an

example, we show that

a very low rank

(i.e., r in Figure

1 can be one

or two)

suffices even

when the full rank

(i.e., d) is as

high as 12,288, making

LoRA both storage- and

compute-efficient.

我们的

灵感来自于

Li et al.(2018a);Aghajanyan

et al.(2020)这表明所学

习的过参数

化

模型实际

上位于低固

有维度上。我

们假设在模

型适应过程

中权重的变

化也有一个

低的“固

有秩

”，导致我们提

出的低秩适

应(LoRA)方法。LoRA

允许

我们通过优

化自适应期

间密集层

变

化的秩分解

矩阵来间接

训练神经网

络中的一些

密集层，同时

保持预训练

权重不变，如

所

示 Figure

1.使用 GPT-3 175B 作

为例子，我们

表明非常低

的秩(即，r

inFigure 1 可以

是一个

或两

个)就足够了

，即使满秩(即

d)高达

12，288，使得 LoRA 在

存储和计算

上都是高效

的。

LoRA

possesses several key advantages.

LoRA 拥有几个

关键优势。

• A

pre-trained model can be

shared and used to

build many small LoRA

modules for dif￾ferent tasks.

We can freeze the

shared model and efficiently

switch tasks by replacing

the

matrices A and

B in Figure 1,

reducing the storage requirement

and task-switching over￾head significantly.

• 一

个预先训练

好的模型可

以被共享并

用于为不同

的任务建立

许多小的 LoRA 模

块。

我们可以

冻结共享模

型，并通过替

换中的矩阵

A 和 B 来有效地

切换任务

Figure 1,

显

著降低了存

储需求和任

务切换开销

。

•

LoRA makes training more

efficient and lowers the

hardware barrier to entry

by up to 3

times when using adaptive

optimizers since we do

not need to calculate

the gradients or

maintain

the optimizer states for

most parameters. Instead, we

only optimize the injected,

much smaller low-rank matrices.

• 当使用自适

应优化器时

，LoRA 使训练更有

效，并降低进

入的硬件障

碍达 3

倍，因

为

我们不需要

计算梯度或

维护大多数

参数的优化

器状态。相反

，我们只优化

注入

的、小得

多的低秩矩

阵。

•

Our simple linear design

allows us to merge

the trainable matrices with

the frozen

weights when

deployed, introducing no inference

latency compared to a

fully fine-tuned

model, by

construction.

• 我们简单

的线性设计

允许我们在

部署时将可

训练矩阵与

冻结的权重

合并，与完全

微

调的模型

相比，通过构

造不会引入

推理延迟。

• LoRA is orthogonal

to many prior methods

and can be combined

with many of them,

such as prefix-tuning. We

provide an example in

Appendix E.

• LoRA

与

许多现有方

法正交，并且

可以与其中

的许多方法

相结合，例如

前缀调整。

我

们在中提供

了一个示例

Appendix E.

4

|

|

Z {

}

|

|

Z

{ }

5

Terminologies

and Conventions We make

frequent references to the

Transformer architecture

and use

the conventional terminologies for

its dimensions. We call

the input and output

di￾mension size of a

Transformer layer dmodel. We

use Wq, Wk, Wv,

and Wo to refer

to the

query/key/value/output projection

matrices in the self-attention

module. W or W0

refers to a pre￾trained

weight matrix and ∆W

its accumulated gradient update

during adaptation. We use

r to

denote the

rank of a LoRA

module. We follow the

conventions set out by

(Vaswani et al., 2017;

Brown et al., 2020)

and use Adam (Loshchilov

& Hutter, 2019; Kingma

& Ba, 2017) for

model

术语和约定

我们经常提

到变压器架

构，并使用其

维度的常规

术语。我们称

变压器层的

输入

和输出

维度大小为

dmodel。我们使用 Wq、Wk、Wv

和

Wo 来指代自我

关注模块中

的查

询/键/值

/输出投影矩

阵。W 或

W0 是指预

训练的权重

矩阵，W 是指自

适应过程中

累积的梯度

更新。我们用

r 来表示

LoRA 模块

的秩。我们遵

循由(Vaswani et al.,2017;Brown

et

al.,2020)利用亚

当(Loshchilov & Hutter,2019;Kingma

& Ba,2017)对于模型

optimization and use

a Transformer MLP feedforward

dimension dffn = 4

× dmodel.

优化并使用

一个变压器

MLP 前馈维数

dffn = 4 ×

dmodel。

3 PROBLEM STATEMENT

4 问

题陈述

While our

proposal is agnostic to

training objective, we focus

on language modeling as

our

motivat- ing use

case. Below is a

brief description of the

language modeling problem and,

in

particular, the maximization

of conditional probabilities given

a task-specific prompt.

虽然

我们的建议

与培训目标

无关，但是我

们关注语言

建模作为我

们的激励用

例。下面是对

语言建模问

题的简要描

述，特别是给

定特定任务

提示的条件

概率的最大

化。

Suppose we are given

a pre-trained autoregressive language

model 

For

et

al.,

instance,

2020) based

on the Transformer architecture

(

PΦ(y x) can

be a generic multi-task

learner

Vaswani et al.,

such as GPT

2017).

PΦ(

(Radford

y x

Consider adapting this

)

parametrized by Φ.

et

al., b; Brown

pre-trained

model to downstream conditional

text generation tasks, such

as summarization,

machine reading

comprehension (MRC), and natural

language to SQL (NL2SQL).

Each

downstream task is

represented by a training

dataset of context-target pairs:=

(xi, yi) i=1,..,N ,

where both xi and

yi are sequences of

tokens. For example, in

NL2SQL, xi is a

natural language

query and

yi its corresponding SQL

command; for summarization, xi

is the content of

an article

and yi

its summary.

假设给我

们一个用 φ

参

数化的预训

练自回归语

言模型 pφ(y x)。例如

，pφ(y x)可以是一

个

通用的多任

务

(Vaswani et al.,

学习者

2017).

，

考

虑

如 GPT

将 这

种

预 先 训 练

的

模 型 应 用

于

下 游 的

(Radford

et al.,b;Brownet al.,2020)基

条

件

文 本 生

于

变压器架构

成

任 务 ， 如

摘

要 、 机 器

阅 读

理 解 (

M R C )

和 自 然

语 言

转 S Q L

( N L 2

S Q L )

。 每 个

下

游

任 务 由

上 下

文 - 目 标

对

的 训 练 数

据

集 来 表 示

:

= ( x i

， 易 ) i

= 1 ， .

. ， N ，

其

中 和 y

i 都

是 记 号

序 列

。 比 如

在 N L 2

S Q L 中

， 是

自 然 语

言 查

询 ， y

i 是 其 对

应

的 S Q L

命 令 ； 概

括

地 说 ， 是

文 章

的 内 容

， 易 是

文 章

的 摘 要

。

6

X

X

During full

fine-tuning, the model is

initialized to pre-trained weights

Φ0 and updated to

Φ0 + ∆Φ

在完全微调

期间，模型被

初始化为预

训练的权重

φ0，并被更新为

φ0+∏φ

by repeatedly following the

gradient to maximize the

conditional language modeling objective:

通过重复遵

循梯度来最

大化条件语

言建模目标

:

max

最

大

Φ

Φ

7

X

X

|y|

|y|

log

(PΦ(yt|x, y<t)) (1)

log(pφ(yt

| x，y<t)) (1)

8

| |

| |

|

| |

|

| |

X

X

(x,y)∈Z t=1

(x，y)∈Z t=1

One of the main

drawbacks for full fine-tuning

is that for each

downstream task, we learn

a

different 

large

(such as GPT-3 with

fine-tuned models can be

challenging, if at all

feasible.

set of parameters

∆Φ whose dimension

Φ0175

Billion), storing and deploying

many independent instances of

∆Φ equals Φ0 .

Thus, if the pre-trained

model is

完全微

调的一个主

要缺点是，对

于每个下游

任务，我们学

习一组不同

的参数∏φ，其维

数

∏

的许多独

立实例可能

具有

φ 等于 φ0。因

此，如果

挑战

预训练模型

性，如果可行

的

很大(如 φ1750 亿

的

GPT-3)，存储和部

署微调模型

话。

In this paper,

we adopt a more

parameter-efficient approach, where the

task-specific parameter

increment ∆Φ

= ∆Φ(Θ) is further

encoded by a much

smaller-sized set of parameters

Θ with

在 本

文 中

， 我 们

采 用 了

一 种

参 数 效

率 更

高 的 方

法 ，

其 中 特 定

于

任 务 的 参

数

增 量

∏φ=∏φ(θ)由一

组更小的参

数 θ

进一步编

码，其中

|Θ||Φ0|. The task

of finding ∆Φ thus

becomes optimizing over Θ:

|Θ||Φ0|.因此

，寻找∏φ 的任务

变成了对 θ 进

行优化:

|y|

|y|

max

最

大

Θ

Θ

X

log

pΦ0 +∆Φ(Θ)(yt|x, y<t) (2)

9

x log pφ0+∏φ(θ)(yt

| x，y<t)

(2)

(x,y)∈Z

t=1

(x，y)∈Z t=1

In

the subsequent sections, we

propose to use a

low-rank representation to encode

∆Φ that is both

compute- and memory-efficient. When

the pre-trained model is

GPT-3 175B, the number

of train￾able parameters |Θ|

can be as small

as 0.01% of |Φ0|.

在随后的部

分中，我们建

议使用低秩

表示来编码

∏φ，这在计算和

存储上都是

高效的。当

预

训练模型是

GPT-3 175B 时，可训练参

数|θ|的数目可

以小到|φ0

|的 0.01%。

5 AREN’T

EXISTING SOLUTIONS GOOD ENOUGH?

6 现

有的解决方

案还不够好

吗？

The problem

we set out to

tackle is by no

means new. Since the

inception of transfer learning,

dozens of works have

sought to make model

adaptation more parameter- and

compute-efficient.

See Sec- tion

6 for a survey

of some of the

well-known works. Using language

modeling as an

example,

there are two prominent

strategies when it comes

to efficient adaptations: adding

adapter

layers (Houlsby et

al., 2019; Rebuffi et

al., 2017; Pfeiffer et

al., 2021; Ru¨ckle´ et

al., 2020) or

optimizing

some forms of the

input layer activations (Li

& Liang, 2021; Lester

et al., 2021;

Hambardzumyan

et al., 2020; Liu

et al., 2021). However,

both strategies have their

limitations,

especially in a

large-scale and latency-sensitive production

scenario.

我们着手

解决的问题

绝不是新的

。自从迁移学

习开始以来

，许多工作都

试图使模型

适应在

参数

和计算方面

更加有效。看

见 Sec-tion

6 对一些著

名的作品进

行调查。以语

言建模为例

，

当涉及到有

效的适应时

，有两个突出

的策略:添加

适配器层(Houlsbyet al.,2019;Rebuffi

et

al.,2017;Pfeiffer et al.,2021;Ru¨ckle´

et al.,2020)或

者优化输入

层激活的某

些形式

(Li &

Liang,2021;Lester et al.,2021;Hambardzumyan et

al.,2020;Liu et al.,2021).

然

而

， 这 两 种

策 略

都 有 其

局 限

性 ， 尤

其 是 在

大 规

模 和 延

迟 敏

感 的 生

产 场

景

中 。

Adapter

Layers Introduce Inference Latency

There are many variants

of adapters. We focus

on the original design

by Houlsby et al.

(2019) which has two

adapter layers per Transformer

block and a more

recent one by Lin

et al. (2020) which

has only one per

block but with an

additional LayerNorm (Ba et

al., 2016). While one

can reduce the overall

latency by pruning

layers

or exploit- ing multi-task

settings (Ru¨ckle´ et al.,

2020; Pfeiffer et al.,

2021), there is no

direct ways to bypass

the extra compute in

adapter layers. This seems

like a non-issue since

adapter layers are designed

to have few parameters

(sometimes <1% of the

original model) by

having

a small bottleneck di-

mension, which limits the

FLOPs they can add.

However, large

neural networks

rely on hardware parallelism

to keep the latency

low, and adapter layers

have to

be processed

sequentially. This makes a

difference in the online

inference setting where the

batch

size is typically

as small as one.

In a generic scenario

without model parallelism, such

as running

inference on

GPT-2 (Radford et al.,

b) medium on a

single GPU, we see

a noticeable increase in

latency when using adapters,

even with a very

small bottleneck dimension (Table

1).

适

配器层引入

推理延迟适

配器有许多

变体。我们通

过以下方式

关注原创设

计 Houlsby et

al.(2019)其中每个

变压器块有

两个适配器

层，最近的一

个由 Lin et al.(2020)其每个

块只

有一个

，但是具有附

加的层形式

(Ba et al.,2016).虽然可以通

过删减层或

利用多任务

设置

来减少

整体延迟(Ru¨ckle´

et al.,2020;Pfeiffer et al.,2021),没

有直接的方

法可以绕

过

适配器层中

的额外计算

。这似乎不是

问题，因为适

配器层被设

计为具有很

少的参数(有

时

小于原始

模型的 1%)，具有

小的瓶颈尺

寸，这限制了

它们可以添

加的 FLOPs。然而，大

型神经

网络

依赖于硬件

并行性来保

持低延迟，并

且适配器层

必须顺序处

理。这在批量

通常只有一

个

的在线推

断设置中产

生了差异。在

没有模型并

行性通用场

景中，例如在

GPT-2 上运行推理

(Radford et

al.,b)中型在单个

GPU 上，我们看到

使用适配器

时延迟明显

增加，即使瓶

颈

非常小(Table1).

This

problem gets worse when

we need to shard

the model as done

in Shoeybi et al.

(2020); Lep￾ikhin et al.

(2020), because the additional

depth requires more synchronous

GPU operations such

as

AllReduce and Broadcast, unless

we store the adapter

parameters redundantly many

times.

当

我们需要分

割模型时，这

个问题会变

得更糟 Shoeybi et al.(2020);Lep-ikhin

et al.

(2020),因为

额外的深度

需要更多的

同步 GPU

操作比

如 AllReduce 和 Broadcast，除非我

们多次

冗余

存储适配器

参数。

Directly Optimizing the

Prompt is Hard The

other direction, as exemplified

by prefix tuning

(Li

& Liang, 2021), faces

a different challenge. We

observe that prefix tuning

is difficult to

optimize

and that its performance

changes non-monotonically in trainable

parameters, confirming

similar observations

in the original paper.

More fundamentally, reserving a

part of the sequence

length for adaptation necessarily

reduces the sequence length

available to process a

downstream

task, which we

suspect makes tuning the

prompt less performant compared

to other methods. We

defer the study on

task performance to Section

5.

直接优

化提示符是

很难的另一

个方向，例如

前缀调整(Li& Liang,2021),面

临着不同的

挑

战。我们观

察到前缀调

整很难优化

，并且其性能

在可训练参

数中非单调

变化，这证实

了原

始论文

中的类似观

察。更重要的

是，保留一部

分序列长度

用于自适应

必然会减少

可用于处

理

下游任务的

序列长度，我

们怀疑这使

得与其他方

法相比，调整

提示的性能

更差。我们将

任务绩效的

研究推迟到

Section 5.

1

0

1

1

Batch

Size

Sequence Length

批量序列长

度

|Θ|

|Θ|

Fine-Tune/LoRA

微调/LoRA

AdapterL

AdapterH

适配

器 l

适配器

h

32 16

1

1

2

|

|

∈

∈ ∈

∈

∈ ∈

32

16

一

512 256

128

512 256

128

0.5M

11M

11M

0.5M 11

米

11 米

1449.4±0.8

338.0±0.6 19.8±2.7

1449.4 0.8

338.0 0.6 19.8 2.7

1482.0±1.0 (+2.2%) 354.8±0.5 (+5.0%)

23.9±2.1 (+20.7%)

1482.0 1.0

(+2.2%) 354.8 0.5 (+5.0%)

23.9 2.1 (+20.7%)

1492.2±1.0

(+3.0%) 366.3±0.5 (+8.4%) 25.8±2.2

(+30.3%)

1492.2 1.0 (+3.0%)

366.3 0.5 (+8.4%) 25.8

2.2 (+30.3%)

Table 1:

Infernece latency of a

single forward pass in

GPT-2 medium measured in

milliseconds,

av- eraged over

100 trials. We use

an NVIDIA Quadro RTX8000.

“ Θ ” denotes

the number of

trainable

parameters in adapter layers.

AdapterL

 and AdapterH

are two variants of

adapter tuning,

which we

describe in Section 5.1.

The inference latency introduced

by adapter layers can

be

significant in an

online, short-sequence-length scenario. See

the full study in

Appendix B.

表 1:GPT-2

介质

中单次前向

传递的推断

延迟，以毫秒

计，平均超过

100 次试验。我们

用的

是

adapter

2

NVIDIA 

是适

配器调优的

Quadro

RTX8000。

两

“

种变

θ”

体

表

示适配器层

中可训练参

数的数量。

，我

们在

Section 5.1.在短序

列长度

adapter 1

的在

线场景

和

中

，适配器层引

入的推理延

迟可能很大

。请参阅中的

完整研究 Appendix

B.

7 OUR METHOD

8 我

们的方法

We describe

the simple design of

LoRA and its practical

benefits. The principles outlined

here

apply to any

dense layers in deep

learning models, though we

only focus on certain

weights in

Transformer language

models in our experiments

as the motivating use

case.

我

们描述了 LoRA 的

简单设计和

它的实际好

处。这里概述

的原则适用

于深度学习

模型中的任

何密集层，尽

管我们在实

验中只关注

Transformer

语言模型中

的某些权重

作为激励用

例。

8.1 LOW-RANK-PARAMETRIZED UPDATE

MATRICES

8.2 低秩参数

化更新矩阵

A neural

network contains many dense

layers which perform matrix

multiplication. The weight

matrices

in these layers typically

have full-rank. When adapting

to a specific task,

Aghajanyan et

al. (2020)

shows that the pre-trained

language models have a

low “instrisic dimension” and

can

still learn efficiently

despite a random projection

to a smaller subspace.

Inspired by this, we

hypothe- size the updates

to the weights also

have a low “intrinsic

rank” during adaptation. For

a

pre-trained weight matrix

W0 R

d×k

,

we constrain its update

by representing the latter

with a low￾rank de-

composition W0 + ∆W

= W0 + BA,

where BR

d×r

,

AR

r×k

, and

the rank rmin(d, k).

During training, W0 is

frozen and does not

receive gradient updates, while

A and B contain

trainable parameters. Note both

W0 and ∆W =

BA are multiplied with

the same input, and

their

respective output vectors

are summed coordinate-wise. For

h = W0x, our

modified forward pass

yields:

神经网络包

含许多执行

矩阵乘法的

密集层。这些

层中的权重

矩阵通常具

有满秩。当适

应特

定的任

务时，Aghajanyan et al.(2020)表明预

训练的语言

模型具有低

的“固有维数

”,并

且尽管随

机投影到更

小的子空间

，仍然可以有

效地学习。受

此启发，我们

假设在适应

过程

中对权

重的更新也

具有较低的

“固有等级”。对

于预训练的

权重矩阵 W0 Rd×k，我

们通过

用低

秩分解 W0 + W

= W0 ∆ +

BA 表示

后者来约束

其更新，其中

BRd×r，ARr×k，秩

rmin(d，k)。在训练期

间，W0 被冻结并

且不接收梯

度更新，而

A 和

B 包含可训练

参数。注

意，W0

和

W = BA 都与相同的

输入相乘，它

们各自的输

出矢量以坐

标方式相加

。对于

h =

W0x，我们改

进的正向传

递产生:

h

= W0x + ∆Wx

= W0x + BAx

(3)

h = W0x+⇼Wx

= W0x+BAx (3)

1

3

r

r

We

illustrate our reparametrization in

Figure 1. We use

a random Gaussian initialization

for A and

zero

for B, so ∆W

= BA is zero

at the beginning of

training. We then scale

∆Wx by α

,

where

α is a

constant in r. When

optimizing with Adam, tuning

α is roughly the

same as tuning the

learning rate if we

scale the initialization appropriately.

As a result, we

simply set α to

the first r

we

try and do not

tune it. This scaling

helps to reduce the

need to retune hyperparameters

when

we vary r

(Yang & Hu, 2021).

我们

在中说明我

们的重新参

数化 Figure 1.我们对

A 使用随机高

斯初始化，对

B

使用零

初始

化，因此在训

练开始时 W =

BA 为

零。然后，我们

用 α 调整

Wx，其中

α 是 r

中的常数

，使用

Adam 优化时

，如果我们适

当调整初始

化，调整 α 与调

整学习速率

大致相同。因

此，我们简单

地将

α 设置为

我们尝试的

第一个 r，并且

不调整它。当

我们改变 r

时

，这种缩放有

助于减少重

新调整超参

数的需要(Yang & Hu,2021).

A

Generalization of Full Fine-tuning.

A more general form

of fine-tuning allows the

training of

a subset

of the pre-trained parameters.

LoRA takes a step

further and does not

require the accumu￾lated gradient

update to weight matrices

to have full-rank during

adaptation. This means that

when

applying LoRA to

all weight matrices and

training all biases2

,

we roughly recover the

expressive￾ness of full fine-tuning

by setting the LoRA

rank r to the

rank of the pre-trained

weight matrices.

In other

words, as we increase

the number of trainable

parameters 3

, training

LoRA roughly

converges to

training the original model,

while adapter-based methods converges

to an MLP and

prefix-based methods to a

model that cannot take

long input sequences.

全

面微调的推

广。更一般形

式的微调允

许训练预训

练参数的子

集。LoRA

更进了一

步，在自

适应

过程中，不需

要对权重矩

阵进行累积

梯度更新来

获得满秩。这

意味着当将

LoRA 应用于

所有

权重矩阵并

训练所有偏

差时

2，我们通

过将 LoRA 秩 r

设置

为预先训练

的权重矩阵

的

秩，粗略地

恢复了完全

微调的表现

力。换句话说

，随着我们增

加可训练参

数的数量 3训

练

LoRA

大致收敛

于训练原始

模型，而基于

适配器的方

法收敛于 MLP，基

于前缀的方

法收敛于

不

能采用长输

入序列的模

型。

No

Additional Inference Latency. When

deployed in production, we

can explicitly compute and

store W = W0

+ BA and perform

inference as usual. Note

that both W0 and

BA are in R

d×k

.

When we

need to switch to

another downstream task, we

can recover W0 by

subtracting BA and

then

adding a different B

0A

0

, a

quick operation with very

little memory overhead. Critically,

this

没有额外

的推理延迟

。在生产中部

署时，我们可

以显式地计

算和存储 W =

W0 + BA，并

照

常执行推

理。注意，W0

和 BA 都

在 Rd×k

中，当我们

需要切换到

另一个下游

任务时，我们

可

以通过减

去 BA，然后加上

不同的 B0A0

来恢

复 W0，这是一个

非常快速的

操作，几乎没

有内

存开销

。关键的是，这

2They represent

a negligible number of

parameters compared to weights.

2 与权重相比

，它们代表的

参数数量微

不足道。Th

3An inevitability

when adapting to hard

tasks.

3 适应

艰难任务的

必然性。

×

×

×

×

1

4

guarantees that we

do not introduce any

additional latency during inference

compared to a fine￾tuned

model by construction.

与通

过构造进行

微调的模型

相比，保证我

们在推断过

程中不会引

入任何额外

的延迟。

8.3 APPLYING LORA TO

TRANSFORMER

8.4 LORA 在变

压器中的应

用

In principle, we can

apply LoRA to any

subset of weight matrices

in a neural network

to reduce

the number

of trainable parameters. In

the Transformer architecture, there

are four weight

matrices

in the self-attention module

(Wq, Wk, Wv, Wo)

and two in the

MLP module. We treat

Wq (or Wk, Wv)

as a single matrix

of dimension dmodel dmodel,

even though the output

dimension is usually sliced

into attention heads. We

limit our study to

only adapting the

attention

weights for downstream tasks

and freeze the MLP

modules (so they are

not trained in

downstream

tasks) both for simplicity

and parameter-efficiency.We further study

the effect on

adapting

different types of attention

weight matrices in a

Transformer in Section 7.1.

We leave the

empirical

investigation of adapting the

MLP layers, LayerNorm layers,

and biases to a

future

work.

原则上，我

们可以将 LoRA

应

用于神经网

络中权重矩

阵的任何子

集，以减少可

训练参数的

数

量。在 Transformer 架构

中，在自我关

注模块中有

四个权重矩

阵(Wq、Wk、Wv、Wo

),在 MLP

模块中

有两个。我们

将 Wq(或

Wk，Wv)视为维

度 dmodel dmodel 的单一矩

阵，即使输出

维度

通常被

分割成注意

力头部。为了

简单和参数

效率，我们将

我们的研究

限制为仅调

整下游任

务

的注意力权

重，并冻结 MLP 模

块(因此它们

不在下游任

务中被训练

)。我们进一步

研究了

在转

换器中适应

不同类型的

注意力权重

矩阵的效果

Section 7.1.我们把适应

MLP 层、层标

准层

和偏差的实

证研究留给

未来的工作

。

Practical Benefits and Limitations.

The most significant benefit

comes from the reduction

in

memory and storage

usage. For a large

Transformer trained with Adam,

we reduce that VRAM

usage by up to

2/3 if rdmodel as

we do not need

to store the optimizer

states for the frozen

parameters. On GPT-3 175B,

we reduce the VRAM

consumption during training from

1.2TB to

350GB. With

r = 4 and

only the query and

value projection matrices being

adapted, the

checkpoint size

is reduced by roughly

10,000 (from 350GB to

35MB)4

. This allows

us to train

with

signifi- cantly fewer GPUs

and avoid I/O bottlenecks.

Another benefit is that

we can switch

between

tasks while deployed at

a much lower cost

by only swapping the

LoRA weights as

opposed

to all the parameters.

This allows for the

creation of many customized

models that can be

swapped in and out

on the fly on

machines that store the

pre-trained weights in VRAM.

We also

observe a

25% speedup during training

on GPT-3 175B compared

to full fine-tuning5

as we do not

need to calculate the

gradient for the vast

majority of the parameters.

实际好处和

局限性。最大

的好处来自

内存和存储

使用的减少

。对于使用 Adam 训

练的大型转

换器，如果使

用 rdmodel，我们可以

将

VRAM 的使用减

少 2/3，因为我们

不需要存储

冻结参

数的

优化器状态

。在

GPT-3 175B 上，我们将

训练期间的

VRAM 消耗从

1.2TB 减少

到 350GB。

当

r = 4 并且只

有查询和值

投影矩阵被

修改时，检查

点大小减少

了大约

10，000(从 350GB

减

少到 35MB)4。这使得

我们可以用

少得多的

GPU 进

行训练，并避

免 I/O 瓶颈。另一

个好处

是，我

们可以在部

署时以低得

多的成本在

任务之间切

换，只需交换

LoRA 权重，而不是

所有

参数。这

允许创建许

多定制的模

型，这些模型

可以在 VRAM

中存

储预先训练

的权重的机

器上

动态交

换。我们还观

察到，与完全

微调相比，在

GPT-3 175B 上进行训练

时，速度提高

了

25%

5因为我们

不需要计算

绝大多数参

数的梯度。

LoRA also

has its limitations. For

example, it is not

straightforward to batch inputs

to different

tasks with

different A and B

in a single forward

pass, if one chooses

to absorb A and

B into W to

eliminate additional inference latency.

Though it is possible

to not merge the

weights and

dynamically choose

the LoRA modules to

use for samples in

a batch for scenarios

where latency is

not

critical.

1

5

LoRA

也

有其局限性

。例如，如果选

择将 A 和 B

吸收

到 W 中以消除

额外的推理

延迟，那么在

单次前向传

递中批量输

入到具有不

同 A

和 B 的不同

任务并不简

单。尽管对于

等待时间不

重

要的情况

，可以不合并

权重并动态

地选择

LoRA 模块

用于一批样

本。

9 EMPIRICAL

EXPERIMENTS

10 实证实验

We evaluate

the downstream task performance

of LoRA on RoBERTa

(Liu et al., 2019),

De￾BERTa (He et al.,

2021), and GPT-2 (Radford

et al., b), before

scaling up to GPT-3

175B (Brown

et al.,

2020). Our experiments cover

a wide range of

tasks, from natural language

understanding

(NLU) to generation

(NLG). Specifically, we evaluate

on the GLUE (Wang

et al., 2019)

benchmark

for RoBERTa and DeBERTa.

We follow the setup

of Li & Liang

(2021) on GPT-2 for

a direct com- parison

and add WikiSQL (Zhong

et al., 2017) (NL

to SQL queries) and

SAMSum

(Gliwa et al.,

2019) (conversation summarization) for

large-scale experiments on GPT-3.

See

Appendix C for

more details on the

datasets we use. We

use NVIDIA Tesla V100

for all

experiments.

我们在

RoBERTa 上评

估了 LoRA 的下游

任务性能(Liu

et al.,2019),德

贝尔塔(He et

al.,2021),和

GPT-2(Radford et al.,b),在

升级到 GPT-3

175B 之前

(Brownet al.,2020).

我

们 的 实 验

涵

盖 了 广 泛

的

任 务 ， 从

自

然 语 言 理

解

( N L U

) 到 生 成

( N L G

) 。 具 体

来

说 ， 我 们

评

估 胶 水 (Wang

et al.,2019)RoBERTa 和 DeBERTa

的

基准。我们按

照的设置 Li

& Liang(2021)在

GPT-2

上进行直接

比较，并添加

WikiSQL(Zhong et al.,2017)(NL 到

SQL 查

询)和 SAMSum(Gliwa

et al.,2019)在

GPT 三号上进行

大规模实验

。看见 Appendix

C 了解我

们使用的数

据集的更多

详细信息。所

有实验我们

都用英伟达

特斯拉 V100。

10.1

BASELINES

10.2 基线

To compare

with other baselines broadly,

we replicate the setups

used by prior work

and reuse

their reported

numbers whenever possible. This,

however, means that some

baselines might only

appear

in certain experiments.

为了与其他

基线进行广

泛的比较，我

们复制了以

前工作中使

用的设置，并

尽可能重用

他们

报告的

数字。然而，这

意味着一些

基线可能只

出现在某些

实验中。

Fine-Tuning (FT) is

a common approach for

adaptation. During fine-tuning, the

model is

initialized to

the pre-trained weights and

biases, and all model

parameters undergo gradient

updates.A

simple variant is to

update only some layers

while freezing others. We

include one such

baseline

reported in prior work

(Li & Liang, 2021)

on GPT-2, which adapts

just the last two

layers

(FTTop2).

微调

是一种常见

的适应方法

。在微调期间

，模型被初始

化为预先训

练的权重和

偏差，并且

所

有模型参数

经历梯度更

新。一个简单

的变体是只

更新一些层

，而冻结其他

层。我们在之

前的工作中

包括了一个

这样的基线

(Li

& Liang,2021)在 GPT-2 上，它只适

应最后两层

(FTP

top 2)。

4We still

need the 350GB model

during deployment; however, storing

100 adapted models only

requires 

350GB +

35MB * 100 ≈

354GB as opposed to

100 * 350GB ≈

35TB.

4 在部署期间

，我们仍然需

要 350GB

型号；但是

，存储 100 个改装

型号只需要

350GB +

35MB * 100 ≈

354GB，而不是 100 * 350GB

≈ 35TB。

5For GPT-3

175B, the training throughput

for full fine-tuning is

32.5 tokens/s per V100

GPU; with the same

number of weight shards

for model parallelism, the

throughput is 43.1 tokens/s

per V100 GPU for

LoRA.

5 对于

GPT-3 175B，完全微调的

训练吞吐量

为每

V100 GPU 32.5 令牌/秒

；对于相同数

量的模型并

行性，LoRA

的吞吐

量为每 V100 GPU 43.1

令牌

/秒。

1

†

†

Model & Method #

Trainable

Parameters MNLI SST-2

MRPC CoLA QNLI QQP

RTE STS-B Avg.

模型和方

法

#可训练

因

素 MNLI SST-2

MRPC 可乐 QNLI QQP

RTE STS-B 平均

值。

RoBbase

(FT)* 125.0M 87.6 94.8

90.2 63.6 92.8 91.9

78.7 91.2 86.4

RoBbase

(BitFit)* 0.1M 84.7 93.7

92.7 62.0 91.8 84.0

81.5 90.8 85.2

RoBbase

(AdptD

)* 0.3M 87.1±.0

94.2±.1 88.5±1.1 60.8±.4 93.1±.1

90.2±.0 71.5±2.7 89.7±.3 84.4

RoBbase (AdptD

)* 0.9M

87.3±.1 94.7±.3 88.4±.1 62.6±.9

93.0±.2 90.6±.0 75.9±2.2 90.3±.1

85.4

RoBbase (LoRA) 0.3M

87.5±.3 95.1±.2 89.7±.7 63.4±1.2

93.3±.3 90.8±.1 86.6±.7 91.5±.2

87.2

RoBbase (英尺)* 125.0

米 87.6 94.8 90.2

63.6 92.8 91.9 78.7

91.2 86.4

RoBbase (BitFit)

*

0.1 米

84.7 93.7

92.7 62.0 91.8 84.0

81.5 90.8 85.2

RoBbase

(AdptD)* 0.3 米 87.1

.

0

94.2 .

1

88.5

1.1

60.8

.

4

93.1 .

1

90.2 

.0

71.5

2.7

89.7 .

3

84.4

RoBbase (AdptD)*

0.9 米 87.3 .

1

94.7 .

3

88.4 .

1

62.6

.

9

93.0 .

2

90.6

.0

75.9

2.2

90.3 .

1

85.4

RoBbase (劳拉) 0.3

米

87.5 .

3

95.1

.

2

89.7 .

7

63.4

1.2

93.3

.

3

90.8

.1

86.6 .

7

91.5

.

2

87.2

RoBlarge

(FT)* 355.0M 90.2 96.4

90.9 68.0 94.7 92.2

86.6 92.4 88.9

RoBlarge

(LoRA) 0.8M 90.6±.2 96.2±.5

90.9±1.2 68.2±1.9 94.9±.3 91.6±.1

87.4±2.5 92.6±.2 89.0

RoBlarge(英尺)*

355.0 米 90.2 96.4

90.9 68.0 94.7 92.2

86.6 92.4 88.9

大型

机器人(洛

拉

)

0.8 米 90.6

.2

96.2 

.5

90.9

1.2

68.2

1.9

94.9 

.3

91.6

.1

87.4

2.5

92.6

.2

89.0

RoBlarge (AdptP

)† 3.0M 90.2±.3 96.1±.3

90.2±.7 68.3±1.0 94.8±.2 91.9±.1

83.8±2.9 92.1±.7 88.4

RoBlarge

(AdptP

)† 0.8M 90.5±.3

96.6±.2 89.7±1.2 67.8±2.5 94.8±.3

91.7±.2 80.1±2.9 91.9±.4 87.9

RoBlarge (AdptH

)† 6.0M

89.9±.5 96.2±.3 88.7±2.9 66.5±4.4

94.7±.2 92.1±.1 83.4±1.1 91.0±1.7

87.8

RoBlarge (AdptH

)†

0.8M 90.3±.3 96.3±.5 87.7±1.7

66.3±2.0 94.7±.2 91.5±.1 72.9±2.9

91.5±.5 86.4

RoBlarge (LoRA)†

0.8M 90.6±.2 96.2±.5 90.2±1.0

68.2±1.9 94.8±.3 91.6±.2 85.2±1.1

92.3±.5 88.6

大型机器

人 (AdptP)

3.0 米 90.2 .

3

96.1 .

3

90.2 .

7

68.3

1.0

94.8 .

2

91.9 .

1

83.8

2.9

92.1 .

7

88.4

大型机

器人 (AdptP) 0.8

米 90.5 .

3

96.6 .

2

89.7

1.2

67.8

2.5

94.8

.

3

91.7 .

2

80.1

2.9

91.9

.

4

87.9

大型

机器人

(AdptH) 6.0M 89.9 .

5

96.2 .

3

88.7

2.9

66.5

4.4

94.7 .

2

92.1 .

1

83.4

1.1

91.0 

1.7

87.8

大型

机器人 (AdptH) 0.8

米 90.3 .

3

96.3 .

5

87.7

1.7

66.3 

2.0

94.7 .

2

91.5

.

1

72.9

2.9

91.5 .

5

86.4

大

型机器人 (劳

拉) 0.8 米

90.6 .

2

96.2

.

5

90.2

1.0

68.2 

1.9

94.8

.

3

91.6 .

2

85.2

1.1

92.3

.

5

88.6

DeBXXL

(FT)* 1500.0M 91.8 97.2

92.0 72.0 96.0 92.7

93.9 92.9 91.1

DeBXXL

(LoRA) 4.7M 91.9±.2 96.9±.2

92.6±.6 72.4±1.1 96.0±.1 92.9±.1

94.9±.4 93.0±.2 91.3

DeBXXL(英尺)*

1500.0 米

91.8 97.2 92.0

72.0 96.0 92.7 93.9

92.9 91.1

DeBXXL(劳拉) 4.7

米 91.9 

.2

96.9

.2

92.6

.6

72.4

1.1

96.0

.1

92.9 

.1

94.9 

.4

93.0

.2

91.3

Table 2:

RoBERTabase, RoBERTalarge, and DeBERTaXXL

with different adaptation methods

on the

GLUE benchmark.

We report the overall

(matched and mismatched) accuracy

for MNLI,

Matthew’s correlation

for CoLA, Pearson correlation

for STS-B, and accuracy

for other tasks.

Higher

configured

is better for

in a setup similar

to 

all metrics.

*

Houlsby et al.

indicates

(2019)

numbers

for a fair comparison.

published in prior works.

indicates runs

表 2:

在

GLUE 基 准 测

试 中

采 用 不

同 适

应 方 法

的 RoBERTabase 、 RoBERTalarge

和

DeBERTaXXL。我们报告了

MNLI 的总体(匹配

和不匹配)准

确性、可乐的

Matthew 相关

性、STS-B

的 Pearson 相

关性以及其

他任务的准

确性。对于所

有指标来说

，越高越好。*表

示先前作品

中发表的数

字。表示在类

似于的设置

中配置的运

行 Houlsby

et al.(2019)为了

公平

比较。

1

ˆ L

ˆ L

Bias-only or BitFit is

a baseline where we

only train the bias

vectors while freezing everything

else. Contemporarily, this baseline

has also been studied

by BitFit (Zaken et

al., 2021).

仅偏置

或位匹配是

一个基线，我

们只训练偏

置向量，而冻

结其他一切

。同时，BitFit 也研

究

了这一基线

(Zaken et al.,2021).

Prefix-embedding

tuning (PreEmbed) inserts special

tokens among the input

tokens. These spe￾cial tokens

have trainable word embeddings

and are generally not

in the model’s vocabulary.

Where to place such

tokens can have an

impact on performance. We

focus on “prefixing”, which

prepends such tokens to

the prompt, and “infixing”,

which appends to the

prompt; both are

discussed

in Li & Liang

(2021). We use lp

(resp. li) denote the

number of prefix (resp.

infix)

tokens. The number

of

前缀嵌入调

优(PreEmbed)在输入标

记中插入特

殊标记。这些

特殊标记具

有可训练的

单词嵌

入，并

且通常不在

模型的词汇

表中。放置这

种令牌的位

置会对性能

产生影响。我

们关

注“前缀

”,它将这种标

记添加到提

示的前面，以

及“中缀”,它添

加到提示的

后面；两者

都

在中讨论 Li &Liang(2021).我

们 使

用 lp (resp 。

li) 表 示

前 缀

(resp 。 中 缀

) 令

牌。…的数目

trainable parameters

is |Θ| = dmodel

× (lp + li).

可

训练参数是

|θ| = d model×(LP+Li)。

Prefix-layer tuning (PreLayer) is

an extension to prefix-embedding

tuning. Instead of just

learning the word embeddings

(or equivalently, the activations

after the embedding layer)

for

some special tokens,

we learn the activations

after every Transformer layer.

The activations

computed from

pre- vious layers are

simply replaced by trainable

ones. The resulting number

of

trainable parameters is

前缀层优化

(PreLayer)是前缀嵌入

优化的扩展

。我们不是仅

仅学习单词

嵌入(或者等

价地，

在嵌入

层之后的激

活)来获得一

些特殊的令

牌，而是在每

个变换器层

之后学习激

活。从先前

层

计算的激活

简单地被可

训练的激活

代替。可训练

参数的最终

数量为

|Θ|

= L × dmodel

× (lp + li),

where L is the

number of Transformer layers.

|θ| = L×d model×(LP+Li)，其中

L

为变压器层

数。

Adapter tuning as

proposed in Houlsby et

al. (2019) inserts adapter

layers between the self￾attention

module (and the MLP

module) and the subsequent

residual connection. There are

two

fully connected layers

with biases in an

adapter layer with a

nonlinearity in between. We

call this

original design

AdapterH

. Recently, Lin

et al. (2020) proposed

a more efficient design

with the

adapter layer

applied only after the

MLP module and after

a LayerNorm. We call

it AdapterL

.

This

is very similar to

another deign proposed in

Pfeiffer et al. (2021),

which we call AdapterP

.

We also include

another baseline call AdapterDrop

(Ru¨ckle´ et al., 2020)

which drops some

adapter

layers for greater efficiency

(AdapterD

). We cite

numbers from prior works

whenever

possible to maximize

the number of baselines

we compare with; they

are in rows with

an asterisk

(*) in

the first column.

中建议的

适配器调整

Houlsby

et al.(2019)在自关注模

块(和 MLP 模块)和

后续剩余连

接之

间插入

适配器层。有

两个完全连

接的层，适配

器层中有偏

置，其间有非

线性。我们称

之为

原始设

计适配器。最

近，Lin et al.(2020)提出了一

种更有效的

设计，仅在

MLP 模

块和层

命名

之后应用适

配器层。我们

称之为 adapter

1。这非

常类似于另

一个在 Pfeiffer et al.

(2021),我们

称之为 AdapterP。我们

还包括另一

个基线调用

AdapterDrop(Ru¨ckle´ et

al.,2020)这就减少了

一些适配器

层以提高效

率(AdapterD)。我们尽可

能引用以前

作品中的数

字，以最大化

我们比较的

基线数量；它

们位于第一

列带有星号

(*)的行中。

In all cases, we

have |Θ| = Lˆ

Adpt ×(2 × dmodel

× r + r

+ dmodel) + 2

× Lˆ

LN ×

dmodel where

Lˆ

Adpt

在所

有情况下，我

们都有|θ| = lˇAdpt×(2×d model×r+r+d

model)+2×lˇLN×d 

model 其中

lˇAdpt

is the number of

adapter layers and LLN

the number of trainable

LayerNorms (e.g., in Adapter

).

1

是适配器层

数，LLN 是可训练

层数(例如，在

适配器中)。

LoRA adds trainable pairs

of rank decomposition matrices

in parallel to existing

weight matrices.

As mentioned

in Section 4.2, we

only apply LoRA to

Wq and Wv in

most experiments for

simplicity.

The number of trainable

parameters is determined by

the rank r and

the shape of the

original weights:

LoRA 将

可训练的秩

分解矩阵对

并行添加到

现有的权重

矩阵中。如中

所述

Section 4.2,为了

简

单起见，在大

多数实验中

我们只把 LoRA

应

用于 Wq 和 Wv。可训

练参数的数

量由秩

r 和原

始权重

的形

状决定:

|Θ|

= 2 × Lˆ

LoRA × dmodel ×

r, where Lˆ

LoRA

is the number of

weight matrices we apply

LoRA

to.

|θ| =

2×lˇLoRA×d model×r，其中

lˇLoRA 是我们应用

LoRA 的权重矩阵

的个数。

1

Model & Method

# Trainable

Parameters

E2E

NLG Challenge

BLEU NIST

MET ROUGE-L CIDEr

模型

和方法

#可训

练

因素

e2e·NLG 挑战

赛

蓝色 美国

国家标准技

术研究所(National

Institute of

Standards and Technology) 遇

见了

胭

脂-L 苹

果酒

GPT-2

M (FT)* 354.92M 68.2

8.62 46.2 71.0 2.47

GPT-2 M (AdapterL

)*

0.37M 66.3 8.41 45.0

69.8 2.40

GPT-2 M

(AdapterL

)* 11.09M 68.9

8.71 46.1 71.3 2.47

GPT-2 M (AdapterH

)

11.09M 67.3±.6 8.50±.07 46.0±.2

70.7±.2 2.44±.01

GPT-2 M

(FTTop2)* 25.19M 68.1 8.59

46.0 70.8 2.41

GPT-2

M (PreLayer)* 0.35M 69.7

8.81 46.1 71.4 2.49

GPT-2 M (LoRA) 0.35M

70.4±.1 8.85±.02 46.8±.2 71.8±.1

2.53±.02

GPT-2 米(英尺

)* 354.92

米 68.2 8.62 46.2

71.0 2.47

GPT-2 M

(AdapterL)* 0.37 米 66.3

8.41 45.0 69.8 2.40

GPT-2 M (AdapterL)* 11.09

米 68.9 8.71 46.1

71.3 2.47

GPT-2 M

(AdapterH) 11.09 米 67.3

.6

8.50 .

07

46.0 .

2

70.7

.2

2.44 .

01

GPT-2 M (FTTop2)* 25.19

米

68.1 8.59 46.0 70.8

2.41

GPT-2 米(预层)* 0.35

米 69.7 8.81 46.1

71.4 2.49

GPT-2 M(洛

拉)

0.35 米 70.4

.1

8.85 .

02

46.8 .

2

71.8

.1

2.53 .

02

GPT-2 L (FT)* 774.03M

68.5 8.78 46.0 69.9

2.45

GPT-2 L (AdapterL

) 0.88M 69.1±.1 8.68±.03

46.3±.0 71.4±.2 2.49±.0

GPT-2

L (AdapterL

) 23.00M

68.9±.3 8.70±.04 46.1±.1 71.3±.2

2.45±.02

GPT-2 L (PreLayer)*

0.77M 70.3 8.85 46.2

71.7 2.47

GPT-2 L

(LoRA) 0.77M 70.4±.1 8.89±.02

46.8±.2 72.0±.2 2.47±.02

GPT-2

升(英尺

)* 774.03 米 68.5

8.78 46.0 69.9 2.45

GPT-2 L(适配器 1) 0.88

米

69.1 

.1

8.68

.

03

46.3 .

0

71.4

.2

2.49

.

0

GPT-2 L(适配器

1) 23.00 米 68.9

.3

8.70 .

04

46.1 .

1

71.3

.2

2.45 .

02

GPT-2 升

(预层)* 0.77 米

70.3 8.85 46.2 71.7

2.47

GPT-2 L 0.77

米 70.4 

.1

8.89 .

02

46.8

.

2

72.0

.2

2.47 .

02

Table 3: GPT-2 medium

(M) and large (L)

with different adaptation methods

on the E2E NLG

Challenge. For all metrics,

higher is better. LoRA

outperforms several baselines with

comparable

or fewer trainable

parameters. Confidence intervals are

shown for experiments we

ran. * indicates

numbers

published in prior works.

表

3: GPT-2 中型(M)和大型

(L)在 E2E

NLG 挑战中采

用不同的适

应方法。对于

所有指标，越

高越好。LoRA 优于

几个具有可

比或更少可

训练参数的

基线。我们运

行的实验显

示了置信区

间。*表示先前

作品中发表

的数字。

10.3

ROBERTA BASE/LARGE

10.4 罗伯

塔基地/大

RoBERTa (Liu et al.,

2019) optimized the pre-training

recipe originally proposed in

BERT

(Devlin et al.,

2019a) and boosted the

latter’s task performance without

introducing many more

trainable

parameters. While RoBERTa has

been overtaken by much

larger models on NLP

leaderboards such as the

GLUE benchmark (Wang et

al., 2019) in recent

years, it remains a

competitive and popular pre-trained

model for its size

among practitioners. We take

the pre￾trained RoBERTa base

(125M) and RoBERTa large

(355M) from the HuggingFace

Transformers

library (Wolf et

al., 2020) and evaluate

the performance of different

efficient adaptation

approaches on

tasks from the GLUE

benchmark. We also replicate

Houlsby et al. (2019)

and

Pfeiffer et al.

(2021) according to their

setup. To ensure a

fair comparison, we make

two crucial

changes to

how we evaluate LoRA

when comparing with adapters.

First, we use the

same batch

size for

all tasks and use

a sequence length of

128 to match the

adapter baselines. Second, we

initialize the model to

the pre-trained model for

MRPC, RTE, and STS-B,

not a model already

adapted to MNLI like

the fine-tuning baseline. Runs

following this more restricted

setup from

Houlsby et

al. (2019) are labeled

with . The result

is presented in Table

2 (Top Three Sections).

See Section D.1 for

details on the hyperparameters

used.

罗

伯塔(Liu et al.,2019)优化了

最初在

BERT 中提

出的预训练

配方(Devlinet al.,2019a)并

且在

不引入更多

可训练参数

的情况下提

高了后者的

任务性能。虽

然

RoBERTa 在 NLP 排行榜

上已经被

GLUE benchmark 等

更大的模型

超越(Wang et

al.,2019)近年来

，它仍然是一

个有

竞争力

的和受欢迎

的预培训模

式，其规模的

从业者。我们

从拥抱脸变

形金刚库中

取出预先

训

练好的罗伯

塔底座(125 米)和

罗伯塔大底

座(355

米)(Wolf et al.,2020)并根据

GLUE 基

准评估不

同有效适应

方法在任务

上的性能。我

们也复制 Houlsby et al.(2019)和

Pfeiffer

et al.(2021)根据他们的

设置。为了确

保公平的比

较，我们在与

适配器进行

比较时，对评

估 LoRA 的方式进

行了两项重

要的更改。首

先，我们对所

有任务使用

相同的批处

理大小，并

使

用长度为 128 的

序列来匹配

适配器基线

。第二，我们将

模型初始化

为用于 MRPC、RTE

和

STS-B 的

预训练模型

，而不是像微

调基线那样

已经适应 MNLI

的

模型。从以下

更受限制的

设

置运行 Houlsby et

al.(2019)都

标有。结果显

示在中 Table 2(前三

节)。看见 Section

D.1

有关

所用超参数

的详细信息

。

†

10.5

DEBERTA XXL

10.6 德贝塔·XXL

DeBERTa (He et al.,

2021) is a more

recent variant of BERT

that is trained on

a much larger

scale

and performs very competitively

on benchmarks such as

GLUE (Wang et al.,

2019) and Su￾perGLUE (Wang

et al., 2020). We

evaluate if LoRA can

still match the performance

of a fully

fine-tuned

DeBERTa XXL (1.5B) on

GLUE. The result is

presented in Table 2

(Bottom Section).

See Section

D.2 for details on

the hyperparameters used.

德伯

塔(He

et al.,2021)是 BERT 的一个

较新的变体

，经过了更大

规模的训练

，在

GLUE 等

基准测

试中表现得

非常有竞争

力(Wang et

al.,2019)和超强力

胶(Wang et al.,2020).我

们

评 估

了 L o

R A 在 G

L U E 上

是 否

仍 能 与

完 全

微 调 的

D e B E

R T a X

X L ( 1

. 5 B )

的

性

能 相 匹

配 。 结

果 显

示 在 中

Table 2(底部)。看见

Section D.2 有

关所用超参

数的详

细信

息。

10.7 GPT-2 MEDIUM/LARGE

10.8

GPT-2 中型/大型

Having shown that

LoRA can be a

competitive alternative to full

fine-tuning on NLU, we

hope to

answer if

LoRA still prevails on

NLG models, such as

GPT-2 medium and large

(Radford et al.,

b).

We keep our setup

as close as possible

to Li & Liang

(2021) for a direct

comparison. Due

to space

constraint, we only present

our result on E2E

NLG Challenge (Table 3)

in this section.

See

Section F.1 for results

on WebNLG (Gardent et

al., 2017) and DART

(Nan et al., 2020).

We

include a list

of the hyperparameters used

in Section D.3.

已经表明

LoRA 可

以成为 NLU 上完

全微调的有

竞争力的替

代方案，我们

希望回答

LoRA 是

否

仍然在 NLG

模

型上流行，如

GPT-2 中型和大型

(Radford et al.,b).我

们 尽 可 能

保

持

我 们 的

设

置 Li & Liang(2021)为了直

接比较。由于

篇幅所限，我

们只给出

E2E

NLG 挑

战赛的结果

(Table3)在本节中。看

见 Section

F.1 对于 WebNLG 上的

结果(Gardent

et al.,2017)和飞镖

(Nan et al.,2020).我

们 包 括 了

中

使 用 的 超

参

数 列 表 Section

2

D.3.

2

2

Model&Method # Trainable

Parameters

WikiSQL MNLI- m SAMSum

Acc. (%) Acc. (%)

R1/R2/RL

模

型和方法 #可

训练

因素

W i k i

S Q L MNLI-m

萨

姆萨姆 

Acc。(%)

Acc。(%) R1/R2/法国

GPT-3 (FT) 175,255.8M

73.8 89.5 52.0/28.0/44.5

GPT-3

(BitFit) 14.2M 71.3 91.0

51.3/27.4/43.5

GPT-3 (PreEmbed) 3.2M

63.1 88.6 48.3/24.2/40.5

GPT-3

(PreLayer) 20.2M 70.1 89.5

50.8/27.3/43.5

GPT-3 (AdapterH

)

7.1M 71.9 89.8 53.0/28.9/44.8

GPT-3 (AdapterH

) 40.1M

73.2 91.5 53.2/29.0/45.1

GPT-3

英尺 175255.8 米 73.

8

89.5 52.0/28.0/44.

5

GPT 三号

14.2 米 71.

3

91.0 51.3/27.4/43.

5

GPT-3(预嵌入) 3.2 米

63.

1

88.6 48.3/24.2/40.

5

GPT-3(预层)

20.2 米 70.

1

89.5 50.8/27.3/43.

5

GPT

3 号(适

配器 h) 7.1

米 71.

9

89.8

53.0/28.9/44.

8

GPT 3

号(适

配器 h) 40.1 米

73.

2

91.5 53.2/29.0/45.

1

GPT-3 (LoRA) 4.7M

73.4 91.7 53.8/29.8/45.9

GPT-3

(LoRA) 37.7M 74.0 91.6

53.4/29.2/45.1

GPT 三号

4.7 米

73.

4

91.7 53.8/29.8/45.

9

GPT 三号 37.7

米 74.

0

91.6

53.4/29.2/45.

1

Table 4:

Performance of different adaptation

methods on GPT-3 175B.

We report the logical

form

validation accuracy on

WikiSQL, validation accuracy on

MultiNLI-matched, and Rouge-1/2/L on

SAMSum. LoRA performs better

than prior approaches, including

full fine-tuning. The results

on WikiSQL have a

fluctuation around ±0.5%, MNLI-m

around ±0.1%, and SAMSum

around

表

4:不同适应方

法在 GPT-3 175B

上的性

能。我们报告

了 WikiSQL 上的逻辑

形式验证准

确

性、MultiNLI-matched

上的验

证准确性以

及 SAMSum 上的 Rouge-1/2/L。LoRA

比以

前的方法

表

现得更好，包

括完全微调

。 WikiSQL 上的结果波

动在

0.5%左右，MNLI-m 在

0.1%左

右，SAMSum 在

0.5%左右

±0.2/±0.2/±0.1 for the three

metrics.

三个指标的

0.2/ 0.2/ 0.1。

10.9 SCALING UP TO

GPT-3 175B

10.10 升级到

GPT-3 175B

As a

final stress test for

LoRA, we scale up

to GPT-3 with 175

billion parameters. Due to

the high

training cost,

we only report the

typical standard deviation for

a given task over

random seeds, as

opposed

to providing one for

every entry. See Section

D.4 for details on

the hyperparameters used.

作为

对

LoRA 的最后一

次压力测试

，我们扩大到

拥有 1750 亿个参

数的

GPT-3。由于培

训成本

高，我

们只报告给

定任务的随

机种子的典

型标准差，而

不是为每个

条目提供一

个。看见

Section D.4

有关

所用超参数

的详细信息

。

As shown in

Table 4, LoRA matches

or exceeds the fine-tuning

baseline on all three

datasets. Note

2

that

not all methods benefit

monotonically from having more

trainable parameters, as shown

in

Fig- ure 2.

We observe a significant

performance drop when we

use more than 256

special tokens

for prefix-embedding

tuning or more than

32 special tokens for

prefix-layer tuning. This

corroborates

similar observations in Li

& Liang (2021). While

a thorough investigation into

this

phenomenon is out-of-scope

for this work, we

suspect that having more

special tokens causes the

input distri- bution to

shift further away from

the pre-training data distribution.

Separately, we

investigate the

performance of different adaptation

approaches in the low-data

regime in Section

F.3.

如所示 Table 4,LoRA 匹配

或超过所有

三个数据集

的微调基线

。注意，并不是

所有的方法

都

单调地受

益于更多的

可训练参数

，如 Fig-ure 2.当 我

们 使

用 2 5

6 个 以 上

的

特 殊 标 记

进

行 前 缀 嵌

入

调 优 ， 或

者 使

用 3 2

个 以 上 的

特

殊 标 记 进

行

前 缀 层 调

优

时 ， 我

们

观

察 到 了 显

著

的 性 能 下

降

。 这 证 实

了 类

似 的 观

察 结

果 Li &

Liang(2021).虽

然 对 这

种

现 象 的 彻

底

调 查 超 出

了

这 项 工 作

的

范 围 ， 但

我

们 怀 疑 有

更

多 的 特 殊

标

记 会 导 致

输

入 分 布 进

一

步 偏 离 训

练

前 的 数 据

分

布 。 另 外

， 我 们

研 究

了 不 同

的 适

应 方 法

在 低

数 据 区

的 性

能 Section F.3.

0.75

0.75

WikiSQL WikiS

QL

2

Method

Fine-Tune PrefixEmbed

PrefixLayer Adapter(H) Method LoRA

Fine-Tune PrefixEmbed PrefixLayer Adapter(H)

LoRA

0.92

0.92

Multi

NLI￾mat

ched

多元

匹配的

0.70

0.70

Validation Validation

0.90

0.90

0.65

0.65

2

0.88

0.88

0.60

0.60

0.55

0.55

2

2

0.86

0.86 0.84

0.84

6 7 8 9

10 11

6 七

8 9 10 11

log10 # Trainable Parameters

log10 #可

训练参数

2

6

7 8 9 10

11

6 七

8 9

10 11

log10 #

Trainable Parameters

log10 #可训练参数

Figure

2: GPT-3 175B validation

accuracy vs. number of

trainable parameters of several

adaptation methods on WikiSQL

and MNLI-matched. LoRA exhibits

better scalability and task

performance. See Section F.2

for more details on

the plotted data points.

图 WikiSQL 和 MNLI-matched

上几种

自适应方法

的 GPT-3 175B 验证准确

性与可训练

参数数

量的

关系。LoRA 展现了

更好的可扩

展性和任务

性能。看见 Section F.2

有

关绘制的数

据点

的更多

详细信息。

11 RELATED

WORKS

12 相

关作品

Transformer

Language Models. Transformer (Vaswani

et al., 2017) is

a sequence-to-sequence

architecture that

makes heavy use of

self-attention. Radford et al.

(a) applied it to

autoregressive

lan- guage modeling

by using a stack

of Transformer decoders. Since

then, Transformer-based

language models

have dominated NLP, achieving

the state-of-the-art in many

tasks. A new

paradigm

emerged with BERT (Devlin

et al., 2019b) and

GPT-2 (Radford et al.,

b) – both are

large Transformer lan-

变压

器语言模型

。变压器(Vaswani

et al.,2017)是一

种序列到序

列架构，它大

量使用自我

关注。Radford et al.(a)通过使

用一堆转换

器解码器将

其应用于自

回归语言建

模。从那时起

，

基于 Transformer 的语言

模型就主导

了 NLP，在许多任

务中实现了

最先进的技

术。伯特带来

了一

种新的

范式(Devlin et al.,2019b)和 GPT-2(Radford

et al.,b)–两者

都是大型变

压器局

域网

-

guage

models trained on a

large amount of text

– where fine-tuning on

task-specific data after pre￾training

on general domain data

provides a significant performance

gain compared to training

on

task-specific data directly.

Training larger Transformers generally

results in better performance

and remains an active

research direction. GPT-3 (Brown

et al., 2020) is

the largest single

Transformer

language model trained to-date

with 175B parameters.

在大量文本

上训练的量

规模型——与直

接在特定任

务数据上训

练相比，在对

一般领域数

据

进行预训

练之后，对特

定任务数据

进行微调可

以提供显著

的性能增益

。训练更大的

变压器

通常

会产生更好

的性能，并且

仍然是一个

活跃的研究

方向。GPT-3(Brown et al.,2020)是

迄今

为止用 175B 参数

训练的最大

的单个 Transformer

语言

模型。

Prompt Engineering and

Fine-Tuning. While GPT-3 175B

can adapt its behavior

with just a

few

additional training examples, the

result depends heavily on

the input prompt (Brown

et al.,

2020). This

necessitates an empirical art

of composing and formatting

the prompt to maximize

a

model’s performance on

a desired task, which

is known as prompt

engineering or prompt hacking.

Fine-tuning retrains a model

pre-trained on general domains

to a specific task

Devlin et al.

(2019b);

Radford et al. (a).

Variants of it include

learning just a subset

of the parameters Devlin

et

al. (2019b); Collobert

& Weston (2008), yet

practitioners often retrain all

of them to maximize

the

downstream performance. However,

the enormity of GPT-3

175B makes it challenging

to perform

fine-tuning in

the usual way due

to the large checkpoint

it produces and the

high hardware barrier

to

entry since it has

the same memory footprint

as pre-training.

快速工

程和微调。虽

然 GPT-3

175B 可以仅通

过几个额外

的训练示例

来调整其行

为，但是结

果

严重依赖于

输入提示(Brown et

al.,2020).这

就 需 要 一

种

经 验 艺 术

来

组 合 和 格

式

化 提 示 ，

以 最

大 化 模

型 在

期 望 任

务 上

的 性 能

， 这 就

是 所

谓 的 提

示 工

程 或 提

示

破

解 。 微 调

将

预 先 在 一

般

领 域 训 练

的

模 型 重 新

训

练 到 特 定

任

务 Devlin et al.

(2019b);Radford et al.(a). 它

的 变

体 包 括

仅 学

习 参 数

的 子

集 Devlin et

al.

(2019b);Collobert & Weston(2008),然而，从业

者经常重新

培训他们所

有人，以最大

限度

地提高

下游性能。然

而，GPT-3 175B 的庞大使

得以通常的

方式执行微

调具有挑战

性，因为

它产

生了大的检

查点和高的

硬件进入壁

垒，因为它具

有与预训练

相同的内存

占用量。

Parameter-Efficient Adaptation. Many have

proposed inserting adapter layers

between

existing layers in

a neural network (Houlsby

et al., 2019; Rebuffi

et al., 2017; Lin

et al., 2020).

Our

method uses a similar

bottleneck structure to impose

a low-rank constraint on

the weight

updates. The

key functional difference is

that our learned weights

can be merged with

the main

weights during

inference, thus not introducing

any latency, which is

not the case for

the adapter

layers (Section

3). A comtenporary extension

of adapter is COMPACTER

(Mahabadi et al., 2021),

which essentially parametrizes the

adapter layers using Kronecker

products with some

predetermined

weight sharing scheme. Similarly,

combining LoRA with other

tensor product￾based methods could

potentially improve its parameter

efficiency, which we leave

to future work.

More

recently, many proposed optimizing

the input word embeddings

in lieu of fine-tuning,

akin

to a continuous

and differentiable generalization of

prompt engineering (Li &

Liang, 2021; Lester

et

al., 2021; Hambardzumyan et

al., 2020; Liu et

al., 2021). We include

comparisons with Li &

Liang (2021) in our

experiment section. However, this

line of works can

only scale up by

using

more special tokens

in the prompt, which

take up available sequence

length for task tokens

when

positional embeddings are

learned.

参数

高效自适应

。许多人提出

在神经网络

的现有层之

间插入适配

器层 (Houlsby et

al.,2019;Rebuffi et al.,2017;Lin et

al.,2020).我们的

方法使用类

似的瓶颈结

构对权

重更

新施加低秩

约束。关键的

功能差异是

，我们学习的

权重可以在

推断过程中

与主权重合

并，因此不会

引入任何延

迟，而适配器

层不会出现

这种情况(Section3).适

配器的当代

扩展

更紧凑

(Mahabadi et

al.,2021),其本质上使

用具有某种

预定权重共

享方案的 Kronecker 积

来参数化适

配器层。类似

地，将 LoRA

与其他

基于张量积

的方法相结

合可能会提

高其参数效

率，这一点我

们留给未来

的工作。最近

，许多人提出

优化输入单

词嵌入来代

替微调，类似

于 提 示 工

程

的 连 续 和

可

微 分 的 一

般

化 (Li & Liang,2021;Lester

et

al.,2021;Hambardzumyan et al.,2020;Liu

et al.,2021).我们包括

与以下内容

的比较 Li

&

Liang(2021)在我

们的实验区

。然而，这一系

列工作只能

通过在提示

中使用更多

的特殊标

2

记

来扩大规模

，这些标记在

学习位置嵌

入时会占用

任务标记的

可用序列长

度。

Low-Rank

Structures in Deep Learning.

Low-rank structure is very

common in machine learn￾ing.

A lot of machine

learning problems have certain

intrinsic low-rank structure (Li

et al., 2016;

Cai

et al., 2010; Li

et al., 2018b; Grasedyck

et al., 2013). Moreover,

it is known that

for many

deep learning

tasks, especially those with

a heavily over-parametrized neural

network, the learned

neural

network will enjoy low-rank

properties after training (Oymak

et al., 2019). Some

prior

works even explicitly

impose the low-rank constraint

when training the original

neural network

(Sainath et

al., 2013; Povey et

al., 2018; Zhang et

al., 2014; Jaderberg et

al., 2014; Zhao et

al.,

2016; Kho- dak

et al., 2021; Denil

et al., 2014); however,

to the best of

our knowledge, none of

these works considers low-rank

update to a frozen

model for adaptation to

downstream tasks. In

theory

liter- ature, it is

known that neural networks

outperform other classical learning

methods,

including the corresponding

(finite-width) neural tangent kernels

(Allen-Zhu et al., 2019;

Li &

Liang, 2018)

when the underlying concept

class has certain low-rank

structure (Ghorbani et al.,

2020; Allen-Zhu & Li,

2019; Allen-Zhu & Li,

2020a). Another theoretical result

in Allen-Zhu &

Li

(2020b) suggests that low-rank

adaptations can be useful

for adversarial training. In

sum, we

believe that

our proposed low-rank adaptation

update is well-motivated by

the literature.

深度学习

中的低秩结

构。低秩结构

在机器学习

中非常普遍

。许多机器学

习问题都有

一定的

内

在

低 秩 结 构

(Li et al.,2016;Cai et

al.,2010;Li et al.,2018b;Grasedyck et

al.,2013).此

外 ， 众 所

周 知

， 对 于

许 多 深

度 学

习 任 务

， 尤

其 是 那 些

具

有 严 重 过

度

参 数 化 的

神

经 网 络 的

任

务 ， 所 学

习

的 神 经 网

络

在 训 练 后

将

享 有 低 秩

属

性

(Oymak et al.,2019).一些先前

的工作甚至

在训练原始

神经网络时

明确地施加

低秩约束

(Sainathet al.,2013;Povey et al.,2018;Zhang

et al.,2014;Jaderberg et

al.,2014;Zhao

et al.,2016;Kho-dak et al.,2021;Denil

et al.,2014);然

而，据我们所

知，这些工作

都没有考虑

低秩更新冻

结模型，以适

应下游任务

。在理论文献

中，已知神经

网 络 优

于 其

他 经 典

学 习

方 法 ，

包 括 相

应 的

( 有 限 宽

度

) 神 经 正

切

核 (Allen-Zhu et

al.,2019;Li

& Liang,2018) 当 底

层 概

念 类 具

有 一

定 的 低

秩 结

构 时 (Ghorbani

et

al.,2020;Allen-Zhu & Li,2019;Allen-Zhu

& Li,2020a).另一个

理论结果是

Allen-Zhu

& Li(2020b)表明低等级

的适应对于

对抗性训练

是有用的。总

之，我们认为

我们提出的

低秩

适应更

新是由文献

充分激发的

。

13 UNDERSTANDING THE

LOW-RANK UPDATES

14 了解低级别

更新

Given the empirical advantage

of LoRA, we hope

to further explain the

properties of the low-rank

adaptation learned from downstream

tasks. Note that the

low-rank structure not only

lowers the

hardware barrier

to entry which allows

us to run multiple

experiments in parallel, but

also gives

better interpretability

of how the update

weights are correlated with

the pre-trained weights. We

focus our study on

GPT-3 175B, where we

achieved the largest reduction

of trainable parameters

(up

to 10,000×) without adversely

affecting task performances.

鉴于

LoRA 的

经验优势，我

们希望进一

步解释从下

游任务中学

习到的低秩

适应的性质

。请注

意，低秩

结构不仅降

低了允许我

们并行运行

多个实验的

硬件准入门

槛，还提供了

更新权重

如

何与预训练

权重相关的

更好的可解

释性。我们把

研究重点放

在

GPT-3 175B 上，在那里

我

们实现了

可训练参数

的最大程度

的减少(高达

10000

×),而没有对任

务性能产生

不利影

响。

We perform

a sequence of empirical

studies to answer the

following questions: 1) Given

a

parameter budget constraint,

which subset of weight

matrices in a pre-trained

Transformer

should we adapt

我

们执行一系

列的实证研

究来回答以

下问题:1)给定

一个参数预

算约束，我们

应该采用预

训

练转换器

中的哪个权

重矩阵子集

3

to maximize

downstream performance? 2) Is

the “optimal” adaptation matrix

∆W really rank￾deficient? If

so, what is a

good rank to use

in practice? 3) What

is the connection between

∆W

and W ?

Does ∆W highly correlate

with W ? How

large is ∆W comparing

to W ?

最大化下游

性能？2)“最优”适

应矩阵∆W

真的

是秩亏的吗

？如果是这样

，在实践中使

用什

么样的

等级比较好

？3)W 和 W

有什么联

系？W 与 W 高度相

关吗？与

W 相比

，W 有多大？

We

believe that our answers

to question (2) and

(3) shed light on

the fundamental principles of

using pre-trained language models

for downstream tasks, which

is a critical topic

in NLP.

我们

相信，我们对

问题(2)和(3)的回

答阐明了使

用预训练语

言模型进行

下游任务的

基本原

则，这

是

NLP 中的一个

关键主题。

14.1 WHICH

WEIGHT MATRICES IN TRANSFORMER

SHOULD WE APPLY LORA

TO?

14.2 我

们应该对变

压器中的哪

些权重矩阵

应用 LORA？

Given a limited parameter

budget, which types of

weights should we adapt

with LoRA to obtain

the best performance on

downstream tasks? As mentioned

in Section 4.2, we

only consider weight

matrices

in the self-attention module.

We set a parameter

budget of 18M (roughly

35MB if stored

in

FP16) on GPT-3 175B,

which corresponds to r

= 8 if we

adapt one type of

attention weights or

r

= 4 if we

adapt two types, for

all 96 layers. The

result is presented in

Table 5.

给定一

个有限的参

数预算，为了

在下游任务

中获得最佳

性能，LoRA 应该采

用哪种类型

的权

重？如中

所述 Section 4.2,在自我

关注模块中

，我们只考虑

权重矩阵。我

们在 GPT-3

175B

上设置

了 18M(如果存储

在 FP16

中，大约 35MB)的

参数预算，对

于所有 96 层，如

果我们采

用

一种类型的

注意力权重

，这对应于 r = 8，或

者如果我们

采用两种类

型，这对应于

r

=

4。结果显示在

中 Table 5.

# of Trainable Parameters

= 18M

Weight Type

Rank r

重量类型

等

级

r

WikiSQL

(±0.5%)

维基百

科(0.5%)

MultiNLI

(±0.1%)

MultiNLI

(

0.1%)

3

3

可训练参

数的数量=

18M

Wq Wk Wv

Wo Wq,

Wk Wq,

Wv

Wq, Wk, Wv,

Wo

体

重商数 周

Wv

吁

Wq，WkWq、Wq

西部、西部、沃

8 8 8

8 4 4 2

8 8 8 8

四 四 2

70.4

70.0 73.0 73.2 71.4

73.7 73.7

70.4 70.0

73.0 73.2 71.4 73.7

73.7

91.0 90.8 91.0

91.3 91.3 91.3 91.7

91.0 90.8 91.0 91.3

91.3 91.3 91.7

3

{ }

{ }

Table 5: Validation accuracy

on WikiSQL and MultiNLI

after applying LoRA to

different types of

attention

weights in GPT-3, given

the same number of

trainable parameters. Adapting both

Wq

and Wv gives

the best performance overall.

We find the standard

deviation across random seeds

to be consistent for

a given dataset, which

we report in the

first column.

表 5:在给

定相同数量

的可训练参

数的情况下

，将

LoRA 应用于 GPT-3 中

不同类型的

注意力权

重

后，WikiSQL 和 MultiNLI 的验证

准确性。同时

采用

Wq 和 Wv 可以

获得最佳的

整体性能。

我

们发现，对于

给定的数据

集，随机种子

的标准差是

一致的，我们

在第一列中

报告了这一

点。

Note that putting

all the parameters in

∆Wq or ∆Wk results

in significantly lower performance,

while adapting both Wq

and Wv yields the

best result. This suggests

that even a rank

of four

captures enough

information in ∆W such

that it is preferable

to adapt more weight

matrices than

adapting a

single type of weights

with a larger rank.

请注意，将

所有参数放

在 Wq 或 Wk

中会导

致明显较低

的性能，同时

调整 Wq 和 Wv

会产

生最佳

结果

。这表明，即使

秩为 4 也能在

W

中捕获足够

的信息，因此

采用更多的

权重矩阵比

采用

秩更大

的单一类型

的权重更可

取。

14.3 WHAT

IS THE OPTIMAL RANK

r FOR LORA?

14.4

LORA 的最佳秩

r 是多少？

We

turn our attention to

the effect of rank

r on model performance.We

adapt {Wq, Wv},

我们

将注意力转

向秩

r 对模型

性能的影响

。我们适应{Wq，西

弗吉尼亚州

}，

{Wq, Wk,

Wv, Wc}, and just

Wq for a comparison.

{Wq，Wk，Wv，Wc}，只比较 Wq。

Weight Type

r = 1 r

= 2 r =

4 r = 8

r = 64

重量

类型

r = 1 r

= 2 r =

4 r = 8

r = 64

WikiSQL(±0.5%)

Wq

Wq, Wv

Wq,

Wk, Wv,

Wo

68.8

73.4

74.1

69.6

73.3

73.7

70.5

73.7

74.0

70.4

73.8

74.0

70.0

73.5

73.9

维基百

科(0.5%) 体重商数

西弗吉尼亚

州

Wq

Wq，Wk，Wv，Wo

68.8

73.4

74.1

69.6

73.3

73.7

70.5

73.7

74.0

70.4

73.8

74.0

70.0

73.5

73.9

MultiNLI (±0.1%)

Wq

Wq, Wv

Wq, Wk,

Wv,

Wo

90.7

91.3

91.2

90.9

91.4

91.7

91.1

91.3

91.7

90.7

91.6

91.5

90.7

91.4

91.4

MultiNLI ( 0.1%)

体重商数

西弗吉尼亚

州 Wq

Wq，Wk，Wv，Wo

90.7

91.3

91.2

90.9

91.4

91.7

91.1

91.3

91.7

90.7

91.6

91.5

90.7

91.4

91.4

Table 6:

Validation accuracy on WikiSQL

and MultiNLI with different

rank r. To our

surprise, a

rank as

small as one suffices

for adapting both Wq

and Wv on these

datasets while training Wq

alone needs a larger

r. We conduct a

similar experiment on GPT-2

in Section H.2.

表

6:具有不

同秩 r 的 WikiSQL

和 MultiNLI 上

的验证准确

性。令我们惊

讶的是，小至

1 的秩

足以适

应这些数据

集上的 Wq 和 Wv，而

单独训练

Wq 需

要更大的 rSection H.2.

Table 6 shows that,

surprisingly, LoRA already performs

competitively with a very

small r (more

so

for Wq, Wv than

just Wq). This suggests

the update matrix ∆W

could have a very

small

“intrinsic rank”.6 To

further support this finding,

we check the overlap

of the subspaces learned

by

different choices of

r and by different

random seeds. We argue

that increasing r does

not cover a

more

meaningful subspace, which suggests

that a low-rank adaptation

matrix is sufficient.

Table

6 显

示，令人惊讶

的是，劳拉已

经表现出很

小的竞争力

(比 Wq 更适合

Wq，西

弗吉尼

3

亚州

)。这表明更新

矩阵 W

可能具

有非常小的

“固有秩”。6为了

进一步支持

这一发现，

我

们检查了通

过不同的 r 选

择和不同的

随机种子学

习的子空间

的重叠。我们

认为

增加 r 并

不覆盖更有

意义的子空

间，这表明低

秩适应矩阵

是足够的。

6However,

we do not expect

a small r to

work for every task

or dataset. Consider the

following thought

experiment: if

the downstream task were

in a different language

than the one used

for pre-training, retraining

the

entire model (similar to

LoRA with r =

dmodel) could certainly outperform

LoRA with a small

r.

6 然

而，我们并不

期望小 r

适用

于每个任务

或数据集。考

虑下面的思

维实验:如果

下游任务使

用的语言不

同于预训练

所使用的语

言，那么重新

训练整个模

型(类似于 r =

dmodel

的

LoRA)肯定会优于

r 较小的 LoRA。

3

Subspace similarity between different

r. Given Ar=8 and

Ar=64 which are the

learned adapta￾tion matrices with

rank r = 8

and 64 using the

same pre-trained model, we

perform singular value

不同

r

之间的子空

间相似性。给

定 Ar=8 和 Ar=64，它们是

使用相同预

训练模型的

秩

r=8 和 64

的学习

适应矩阵，我

们执行奇异

值

decomposition and obtain the

right-singular unitary matrices UA

分解并得

到右奇异酉

矩阵 UA

r=8

r=8

an

d

U

A

和 UA

3

≤

≤

≤≤

≤

≤

r=

64

r=6

4

.

7 We

hope

to

an- 。7我 们

希

望 -

swer: how

much of the subspace

spanned by the top

i singular vectors in

UAr=8 (for 1 i

8) is

contained in

the subspace spanned by

top j singular vectors

of UAr=64 (for 1j64)?

We mea- sure

this

quantity with a normalized

subspace similarity based on

the Grassmann distance (See

Ap￾pendix G for a

more formal discussion)

swer:UAr

= 8(对于 1 i

8)的

前 I 个奇异向

量所跨越的

子空间有多

少包含在 UAr=64(对

于

1j64)的前 j 个奇

异向量所跨

越的子空间

中？我们用基

于格拉斯曼

距离的归一

化子空间相

似

性来测量

这个量(参见

Ap-pendix

G 更正式的讨

论)

i> j

2

我> j 2

φ(A

φ(A

r=

8

r=8

, Ar=64 ，Ar=64

3

Ar

Ar

,

i, j) =

||UAr=8

UAr=64

||F ，，I

j)

= ||

UAr=

8

UAr=

64

||F

min(

,

最小

(I

j)

∈ [0, 1] (4)

∈ [0, 1] (4)

where

Ui

哪

里 Ui

represents the columns of

UA

表

示

U

A

的

列

r=8

3

·

·

Wq Wv

Wq Wv

Wq Wv

Wq Wv

r=8

corresponding

to the top-i

singular

vectors.

对

应

于前

I 个

奇异向量。

φ( )

has a range of

[0, 1], where 1

represents a complete overlap

of subspaces and 0

a complete

separation. See

Figure 3 for how

φ changes as we

vary i and j.

We only look at

the 48th layer

(out

of 96) due to

space constraint, but the

conclusion holds for other

layers as well, as

shown

in Section H.1.

φ()的

取值范围为

[0，1]，其中 1 表示子

空间的完全

重叠，0 表示完

全分离。看见

Figure

3 由于空间限

制，我们只查

看第 48 层(共

96 层

),但结论同样

适用于其他

层，如所示

Section H.1.

(Ar = 64, Ar

= 8, i, j)

(Ar = 64，Ar =

8，I，j)

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0

1 2 3

4 5 6 7

8

1 2 3

4 5 6 7

8

j j

j

j j

j

1

2 3 4 5

6 7 8

i

i

1

6

1

2

1

8

2

3

2

9

3

1

6

1

2

1

8

2

3

2

9

3

1

6

1

2

1

8

2

3

2

9

3

1

6

1

2

1

8

2

3

2

9

3

3

1 2

3 4 5 6

7 8

j

j

Figure 3: Subspace similarity

between column vectors of

Ar=8 and Ar=64 for

both ∆Wq and ∆Wv.

The third and the

fourth figures zoom in

on the lower-left triangle

in the first two

figures. The top

directions

in r = 8

are included in r

= 64, and vice

versa.

图

3:对于 Wq 和

Wv，Ar=8 和 Ar=64 的

列向量之间

的子空间相

似性。第三和

第四个图形

放大

前两个

图形的左下

角三角形。r = 8 中

的顶部方向

包含在

r = 64 中，反

之亦然。

We make an important

observation from Figure 3.

我们

观察到一个

重要的现象

Figure 3.

Directions corresponding

to the top singular

vector overlap significantly between

Ar=8 and Ar=64, while

others do not. Specifically,

∆Wv (resp. ∆Wq) of

Ar=8

and ∆Wv (resp.

∆Wq) of Ar=64 share

a subspace of dimension

1 with normalized

similarity

> 0.5, providing an

explanation of why r

= 1 performs quite

well in our

downstream

tasks for GPT-3.

对应于顶部

奇异向量的

方向在

Ar=8 和 Ar=64 之

间明显重叠

，而其他方向

则不

重叠。具

体来说，Wv(分别

为。Wq)和 Wv(分别为

。Wq)共享归一化

相似性>

0.5 的

1 维

子空间，提供

了为什么 r =

1 在

我们的 GPT-3 的下

游任务中表

现相

当好的

解释。

Since both Ar=8

and Ar=64 are learned

using the same pre-trained

model, Figure 3 indicates

that

the top singular-vector

directions of Ar=8 and

Ar=64 are the most

useful, while other directions

potentially contain mostly random

noises accumulated during training.

Hence, the adaptation

matrix

can indeed have a

very low rank.

由于

Ar=8 和

Ar=64 都是使用相

同的预训练

模型学习的

，Figure 3

表示 Ar=8 和 Ar=64

的顶

部奇异向量

方向是最有

用的，而其他

方向可能主

要包含训练

期间积累的

随机噪声。因

此，

适应矩阵

实际上可以

具有非常低

的秩。

Subspace similarity

between different random seeds.

We further confirm this

by plotting the

normalized

subspace similarity between two

randomly seeded runs with

r = 64, shown

in Figure

4.

不同随

机种子之间

的子空间相

似性。我们通

过绘制

r = 64 的两

次随机播种

运行之间的

标准

化子空

间相似性进

一步证实了

这一点，如所

示 Figure 4.

∆Wq

appears to have a

higher “intrinsic rank” than

∆Wv, since more common

singular value

direc- tions

are learned by both

runs for ∆Wq, which

is in line with

our empirical observation in

Table 6. As a

comparison, we also plot

two random Gaussian matrices,

which do not share

any

common singular value

directions with each other.

Wq 似乎比 Wv 具

有更高的“固

有等级”,因为

Wq

的两次运行

都获得了更

常见的奇异

值方向，

这与

我们在中的

经验观察一

致 Table 6.作为比较

，我们还绘制

了两个随机

高斯矩阵，它

们

彼此不共

享任何共同

的奇异值方

向。

14.5 HOW DOES

THE ADAPTATION MATRIX ∆W

COMPARE TO W ?

14.6 自适应矩

阵 W 与

W 相比如

何？

We further

investigate the relationship between

∆W and W .

In particular, does ∆W

highly

correlate with W

? (Or mathematically, is

∆W mostly contained in

the top singular directions

of

W ?) Also,

我们进一

步研究了 W 和

W 之间的关系

，特别地，W

和 W 高

度相关吗？(或

者数学上∏W 是

否大

部分包

含在 W 的上奇

异方向？)还有

，

7Note

that a similar analysis

can be carried out

with B and the

left-singular unitary matrices –

we stick with

7

请注意，可以

对 B 和左奇异

酉矩阵进行

类似的分析

——我们坚持

A

for our experiments.

a

代

表我们的实

验。

4

r =

64 r

r =

64 r

(A ,

A

0

, i,

j)

(一) ，A0 ，我，j)

Wq Wv

体重

商数 Wv

1

一

8

8

16

16

24

24

32

32

40

40

48

48

56

56

Random

Gaussia

n

随机

高斯

i i

1

5

1

0

1

5

2

0

2

5

3

0

3

4

1

5

1

0

1

5

2

0

2

5

3

0

3

4

1

5

1

0

1

5

2

0

2

5

3

0

3

4

1

5

1

0

1

5

2

0

2

5

3

0

3

4

4

>

>

> >

≈

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

j j j

j

j j

Figure 4:

Left and Middle: Normalized

subspace similarity between the

column vectors of Ar=64

from two random seeds,

for both ∆Wq and

∆Wv in the 48-th

layer. Right: the same

heat-map

between the column

vectors of two random

Gaussian matrices. See Section

H.1 for other layers.

图 4:左边

和中间:对于

第 48 层中的

Wq 和

Wv，来自两个随

机种子的 Ar=64 的

列向量之间

的归

一化子

空间相似性

。右图:两个随

机高斯矩阵

的列向量之

间的相同热

图。看见 Section H.1

对于

其他层。

how “large” is ∆W

comparing to its corresponding

directions in W ?

This can shed light

on the

underlying mechanism

for adapting pre-trained language

models.

与 W 中

相应的方向

相比，W

有多大

？这可以阐明

用于适应预

先训练的语

言模型的基

础机制。

To answer these

questions, we project W

onto the r-dimensional subspace

of ∆W by

comput-

ing U >WV >

, with U /V

being the left/right singular-vector

matrix of ∆W .

Then, we com- pare

the Frobenius norm between

kU >WV >

kF

and kW kF .

As a

comparison, we

also compute

为了

回答这些问

题，我们通过

计算 U

>WV >将 W 投影

到

W 的 r 维子空

间上，其中

U /V 是

W 的左/右奇异

向量矩阵。然

后，我们比较

kU

>WV >kF 和 kW

kF 之间的 Frobenius 范

数。作为比较

，我们还计算

了

kU WVkF by replacing

U, V with the

top r singular vectors

of W or a

random matrix.

kU WVkF

通过用 W 或

随机矩阵的

前 r

个奇异向

量代替 U，V。

∆Wq

r

= 4

Wq Random

∆Wq

r =

64

Wq

Random

∮Wq

r

= 

4

体重

商数

随意 ∮Wq

r =

64

体

重

商数

随意

>

>

||U WqV ||F

= 0.32 21.67 0.02

1.90 37.71 0.33

>

>

||U WqV ||F

= 0.32 21.67 0.02

1.90 37.71 0.33

||Wq||F

= 61.95 ||∆Wq||F =

6.91 ||∆Wq||F = 3.57

||Wq||F = 61.95 |

|∮Wq | | F

= 

6.91

|

|∮Wq | | F

= 

3.57

Table

7: The Frobenius norm

of U >WqV

>

where U and V

are the left/right top

r singular

vector directions

of either (1) ∆Wq,

(2) Wq, or (3)

a random matrix. The

weight matrices are taken

from the 48th layer

of GPT-3.

表 7:U

> WqV >的 Frobenius

范数其

中 U 和 V

是(1)Wq，(2) Wq，或(3)随

机矩阵的左

/右上 r

奇异向

量方向。权重

矩阵取自

GPT-3 的

第 48 层。

We draw several conclusions

from Table 7. First,

∆W has a stronger

correlation with W

compared

to a random matrix,

indicating that ∆W amplifies

some features that are

already in W .

1

5

1

0

1

5

2

0

2

5

3

0

3

4

1

5

1

0

1

5

2

0

2

5

3

0

3

4

Second, instead of repeating

the top singular directions

of W , ∆W

only amplifies directions that

are not emphasized in

W . Third, the

amplification factor is rather

huge: 21.56.91/0.32 for r

= 4. See Section

H.4 for why r

= 64 has a

smaller amplification factor. We

also provide a

visualization

in Section H.3 for

how the correlation changes

as we include more

top singular

directions from

Wq. This suggests that

the low-rank adaptation matrix

potentially amplifies the

important

features for specific downstream

tasks that were learned

but not emphasized in

the

general pre-training model.

我们从

…得出几个结

论 Table 7.首先，与随

机矩阵相比

，W 与

W 具有更强

的相关性，表

明 W

放大了

W 中

已经存在的

一些特征。其

次，W 不是重复

W 的顶部奇异

方向，而是仅

放大

W 中未

强

调的方向。第

三，放大系数

相当大:r =

4 时为

21.56.91/0.32。看见 Section H.4

对于

为

什么 r =

64 放大系

数较小。我们

还提供了可

视化的 Section H.3

当我

们包括更多

来自 Wq

的顶部

奇异方向时

，相关性如何

变化。这表明

，低秩适应矩

阵潜在地放

大了特定下

游任务的≈

重

要特征，这些

特征在一般

预训练模型

中被学习但

没有被强调

。

15 CONCLUSION AND FUTURE

WORK

16 结论和未来

工作

Fine-tuning

enormous language models is

prohibitively expensive in terms

of the hardware

required

and the storage/switching cost

for hosting independent instances

for different tasks. We

propose LoRA, an efficient

adaptation strategy that neither

introduces inference latency nor

reduces input sequence length

while retaining high model

quality. Importantly, it allows

for quick

task-switching when

deployed as a service

by sharing the vast

majority of the model

parameters.

While we focused

on Transformer language models,

the proposed principles are

generally

applicable to any

neural networks with dense

layers.

就所需

的硬件和为

不同任务托

管独立实例

的存储/交换

成本而言，微

调巨大的语

言模型是极

其昂贵的。我

们提出 LoRA，一种

有效的自适

应策略，既不

会引入推理

延迟，也不会

减少输

入序

列长度，同时

保持高模型

质量。重要的

是，当作为服

务部署时，通

过共享绝大

多数模

型参

数，它允许快

速的任务切

换。虽然我们

关注的是 Transformer 语

言模型，但是

所提出

的原

则通常适用

于任何具有

密集层的神

经网络。

There are many directions

for future works. 1)

LoRA can be combined

with other efficient adapta￾tion

methods, potentially providing orthogonal

improvement. 2) The mechanism

behind fine￾tuning or LoRA

is far from clear

– how are features

learned during pre-training transformed

to do

well on

downstream tasks? We believe

that LoRA makes it

more tractable to answer

this than

full fine-

以后

的作品有很

多方向。1) LoRA 可以

与其他有效

的自适应方

法相结合，潜

在地提供正

交

改进。2)微调

或

LoRA 背后的机

制还远不清

楚——预训练期

间学到的特

征如何转化

为在下游

任

务中表现良

好？我们相信

劳拉使它比

完全罚款更

容易回答这

个问题-

4

tuning. 3) We mostly

depend on heuristics to

select the weight matrices

to apply LoRA to.

Are

there more principled

ways to do it?

4) Finally, the rank-deficiency

of ∆W suggests that

W could

be rank-deficient

as well, which can

also be a source

of inspiration for future

works.

调音

。3)我们主要依

靠试探法来

选择应用 LoRA 的

权重矩阵。有

没有更有原

则性的做法

？4)

最后，W 的秩亏

表明 W 也可能

是秩亏的，这

也可以成为

未来工作的

灵感来源。

REFERENCES

参

考

Armen Aghajanyan,

Luke Zettlemoyer, and Sonal

Gupta. Intrinsic Dimensionality Explains

the

Effectiveness of Language

Model Fine-Tuning. arXiv:2012.13255 [cs],

December 2020. URL

http://arxiv.org/abs/2012.13255.

阿门·阿加

贾尼扬、卢克

·塞特勒莫耶

和索纳尔·古

普塔。内在维

度解释了语

言模型微调

的 有 效 性

。 arXiv:2012.13255 [cs] ，

2020 年

12 月 。

统 一 资 源

定

位

器 http://arxiv.org/abs/2012.13255。

Zeyuan

Allen-Zhu and Yuanzhi Li.

What Can ResNet Learn

Efficiently, Going Beyond Kernels?

In

艾伦

-朱和。除了内

核，ResNet 还能高效

地学习什么

？在…里

NeurIPS,

2019. Full version available

at http://arxiv.org/abs/1905.10337.

NeurIPS，2019。完整版

本可从以下

网址获得 http://arxiv.org/abs/1905.10337。

Zeyuan Allen-Zhu and Yuanzhi

Li. Backward feature correction:

How deep learning performs

deep learning. arXiv preprint

arXiv:2001.04413, 2020a.

艾

伦 -

朱 和 。 反

向

特 征 校 正

: 深

度 学 习

如 何

执 行 深

度 学

习 。 arXiv

预 印 本

arXiv:2001.04413，2020a。

Zeyuan Allen-Zhu and Yuanzhi

Li. Feature purification: How

adversarial training performs robust

deep learning. arXiv preprint

arXiv:2005.10190, 2020b.

艾

伦-朱和。对抗

训练如何执

行强大的深

度学习。arXiv 预印

本

arXiv:2005.10190，2020b

Zeyuan Allen-Zhu, Yuanzhi

Li, and Zhao Song.

A convergence theory for

deep learning via over￾parameterization.

In ICML, 2019. Full

version available at http://arxiv.org/abs/1811.

03962.

袁泽·艾伦

-朱、和。基于过

参数化的深

度学习收敛

理论。在 ICML，2019 年。完

整版本可

从

以下网址获

得 http://arxiv.org/abs/1811.03962。

Jimmy Lei

Ba, Jamie Ryan Kiros,

and Geoffrey E. Hinton.

Layer normalization, 2016.

吉米·巴雷

，杰米·瑞安·基

罗斯，杰弗里

·e·辛顿。图层归

一化，2016。

Tom B. Brown, Benjamin

Mann, Nick Ryder, Melanie

Subbiah, Jared Kaplan, Prafulla

Dhari￾wal, Arvind Neelakantan, Pranav

Shyam, Girish Sastry, Amanda

Askell, Sandhini Agarwal,

Ariel

Herbert-Voss, Gretchen Krueger, Tom

Henighan, Rewon Child, Aditya

Ramesh, Daniel

M. Ziegler,

Jeffrey Wu, Clemens Winter,

Christopher Hesse, Mark Chen,

Eric Sigler, Mateusz

Litwin,

Scott Gray, Benjamin Chess,

Jack Clark, Christopher Berner,

Sam McCandlish, Alec

Radford,

Ilya Sutskever, and Dario

Amodei. Language Models are

Few-Shot Learners.

arXiv:2005.14165 [cs],

July 2020. URL http://arxiv.org/abs/2005.14165.

汤姆·布

朗、本杰明·曼

、尼克·赖德、梅

拉妮·苏比亚

、贾里德·卡普

兰、普拉富拉

·

达里-瓦尔、阿

尔温德·尼拉

坎坦、普拉纳

夫·希亚姆、吉

里什·萨斯特

里、阿曼达·阿

斯克尔、桑迪

尼·阿加瓦尔

、阿里尔·赫伯

特-沃斯、格雷

琴·克鲁格、汤

姆·海尼根、

雷

文·蔡尔德、阿

迪蒂亚·拉梅

什、丹尼尔·齐

格勒、杰弗里

·吴、克莱门斯

·温特、

克里斯

托弗·黑塞、陈

唐山、埃里克

·西格勒、马特

乌斯·利特温

、斯科特·格雷

、本

杰明·切斯

、杰克·克拉克

、克里斯托弗

·伯纳、萨姆·麦

卡德里什语

言模型是一

次性

4

学 习

者

。 arXiv:2005.14165 [cs] ，

2020 年 7 月

。 统 一 资

源

定 位

器 http://arxiv.org/abs/2005.14165。

Jian-Feng Cai, Emmanuel J

Cande



s, and Zuowei Shen.

A singular value thresholding

algorithm

for matrix completion.

SIAM Journal on optimization,

20(4):1956–1982, 2010.

建、伊曼纽尔

·坎德和沈。矩

阵补全的奇

异值阈值算

法。SIAM 优化杂志

，20(4):1956–

1982，2010。

Daniel Cer, Mona

Diab, Eneko Agirre, Inigo

Lopez-Gazpio, and Lucia Specia.

Semeval-2017 task

1: Semantic

textual similarity multilingual and

crosslingual focused evaluation. Proceedings

of

the 11th International

Workshop on Semantic Evaluation

(SemEval-2017), 2017. doi: 10.18653/

v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.

丹尼尔·瑟尔

、莫娜·迪亚卜

、埃内科·阿吉

尔、伊尼戈·洛

佩斯·加兹皮

奥和露西亚

·

斯基亚。Semeval-2017 任务

1:语义文本相

似度多语种

和跨语种聚

焦评估。第 11 届

语义

评价国

际研讨会论

文集(SemEval-2017)，2017。doi: 10.18653/ v1/s17-2001。统一资

源定位器 http://dx.doi.org/10.18653/v1/S17-2001。

Ronan Collobert and Jason

Weston. A unified architecture

for natural language processing:

deep

neural networks with

multitask learning. In Proceedings

of the 25th international

conference

on Machine learning,

ICML ’08, pp. 160–167,

New York, NY, USA,

July 2008. Association

for

Computing Machinery. ISBN 978-1-60558-205-4.

doi: 10.1145/1390156.1390177. URL

https://doi.org/10.1145/1390156.1390177.

罗

南·科洛波特

和杰森·韦斯

顿。自然语言

处理的统一

架构:具有多

任务学习的

深度神经网

络。《第 25 届机器

学习国际会

议论文集》, 2008

年

ICML，第 160-167 页，纽约州

，美

国

， 2008 年 7

月 。 计

算 机

协 会 。 国

际

标 准 书 号

978-1-60558-205-4

。 doi:

10.1145/1390156.1390177。统一资源定

位器 https://doi.org/10.1145/1390156.139017

7。

Misha Denil, Babak

Shakibi, Laurent Dinh, Marc’Aurelio

Ranzato, and Nando de

Freitas. Predicting

parameters in

deep learning, 2014.

Misha

Denil 、 Babak Shakibi

、 Laurent Dinh 、

Marc'Aurelio Ranzato 和 Nando

de

Freitas。深度

学习中的预

测参数，2014。

Jacob Devlin,

Ming-Wei Chang, Kenton Lee,

and Kristina Toutanova. Bert:

Pre-training of deep

bidirectional

transformers for language understanding,

2019a.

雅各

布·德夫林、张

明蔚、肯顿·李

和克里斯蒂

娜·图塔诺娃

。Bert:语言理解深

度双向转

换

器的预训练

，2019a。

Jacob

Devlin, Ming-Wei Chang, Kenton

Lee, and Kristina Toutanova.

BERT: Pre-training of

Deep

Bidirectional Transformers for Language

Understanding. arXiv:1810.04805 [cs], May

2019b. URL http://arxiv.org/abs/1810.04805. arXiv:

1810.04805.

雅各布·德夫

林、张明蔚、肯

顿·李和克里

斯蒂娜·图塔

诺娃。BERT:用于语

言理解的深

度

双向 转换

器的

预训 练

。 arXiv:1810.04805 [cs]

，2019 年 5 月。

统一 资

源 定位 器

http://arxiv.org/abs/1810.04805。arXiv: 1810.04805。

William B.

Dolan and Chris Brockett.

Automatically constructing a corpus

of sentential paraphrases.

In

Proceedings of the Third

International Workshop on Paraphrasing

(IWP2005), 2005. URL

https://aclanthology.org/I05-5002.

威

廉·多兰和克

里斯·布罗克

特。自动构建

句子释义语

料库。载于 2005 年

第三届国际

释义

研讨会

会议录。统一

资源定位器

https://aclanthology.org/I05-5002。

Claire Gardent, Anastasia Shimorina,

Shashi Narayan, and Laura

Perez-Beltrachini. The webnlg

challenge:

Generating text from rdf

data. In Proceedings of

the 10th International Conference

on Natural Language Generation,

pp. 124–133, 2017.

克莱尔·加登

特、阿纳斯塔

西娅·希莫里

纳、沙希·纳拉

扬和劳拉·佩

雷斯·贝尔特

拉奇

尼。 webnlg 挑战

: 从

rdf 数据 生成

文本 。《

第十 届

自 然语 言生

成国

际会 议

论 文

集》，124–133

页，2017。

4

4

Behrooz

Ghorbani, Song Mei, Theodor

Misiakiewicz, and Andrea Montanari.

When do neural

networks

outperform kernel methods? arXiv

preprint arXiv:2006.13409, 2020.

Behrooz

Ghorbani，宋

梅，西奥多·米

西亚凯维奇

和安德烈·蒙

塔纳里。神经

网络何时优

于核方法？arXiv 预

印本 arXiv:2006.13409，2020。

Bogdan

Gliwa, Iwona Mochol, Maciej

Biesek, and Aleksander Wawer.

Samsum corpus: A

human-

annotated dialogue dataset for

abstractive summarization. CoRR, abs/1911.12237,

2019. URL http://arxiv.org/abs/1911.12237.

波格丹

一世·格利瓦

、艾沃娜·莫科

尔、马切伊·比

塞克和亚历

山大·瓦维尔

。一个用于

抽

象摘要的人

类注释对话

数据集。CoRR，abs/1911.12237，2019。统一

资源定位器

http://arxiv.org/abs/1911.12237。

Lars Grasedyck, Daniel

Kressner, and Christine Tobler.

A literature survey of

low-rank tensor

approximation techniques.

GAMM-Mitteilungen, 36(1):53–78, 2013.

拉斯·格拉塞

迪克，丹尼尔

·克里斯纳和

克里斯汀·托

布勒。低秩张

量近似技术

的文献综

述

GAMM-米特伦根，36(1):53–78，2013。

Jihun Ham and

Daniel D. Lee. Grassmann

discriminant analysis: a unifying

view on subspace￾based learning.

In ICML, pp. 376–383,

2008. URL

https://doi.org/10.1145/1390156. 1390204.

吉

洪汉姆和丹

尼尔李。格拉

斯曼判别分

析:基于子空

间学习的统

一观点。在 ICML，第

376–

383 页，2008

年。统一资

源定位器 https://doi.org/10.1145/1390156.1390204。

Karen Hambardzumyan,

Hrant Khachatrian, and Jonathan

May. WARP: Word-level Adversarial

ReProgramming. arXiv:2101.00121 [cs], December

2020. URL http://arxiv.org/abs/

2101.00121.

arXiv: 2101.00121.

凯

伦·汉巴尔祖

米扬，赫兰特

·哈恰特里安

和乔纳森·梅

。WARP:单词级对抗

性重新编

程

。

arXiv:2101.00121 [cs] ， 2020

年 12 月 。

统 一 资

源 定

位

器 http://arxiv.org/abs/2101.00121。arXiv: 2101.00121。

Pengcheng He, Xiaodong Liu,

Jianfeng Gao, and Weizhu

Chen. Deberta: Decoding-enhanced bert

with disentangled attention, 2021.

何

鹏程，，高剑锋

，陈。德伯塔:解

码增强伯特

与解开注意

，2021。

Neil Houlsby, Andrei

Giurgiu, Stanislaw Jastrzebski, Bruna

Morrone, Quentin de Laroussilhe,

Andrea Gesmundo, Mona Attariyan,

and Sylvain Gelly. Parameter-Efficient

Transfer Learning

for NLP.

arXiv:1902.00751 [cs, stat], June

2019. URL http://arxiv.org/abs/1902.

00751.

尼尔·霍尔斯

比、安德烈·久

尔久、斯坦尼

斯劳·雅斯特

日布斯基、布

鲁纳·莫龙、昆

廷

·德拉鲁西

尔、安德烈·格

斯蒙多、莫娜

·阿塔利扬和

西尔万·盖利

。自然语言处

理中

的参数

有效迁移学

习。arXiv:1902.00751 [cs，stat]，2019

年 6 月。统一

资源定位器

http://arxiv.org/abs/1902.00751。

Max

Jaderberg, Andrea Vedaldi, and

Andrew Zisserman. Speeding up

convolutional neural

networks with

low rank expansions. arXiv

preprint arXiv:1405.3866, 2014.

马克斯·贾德

伯格、安德里

亚·维达尔迪

和安德鲁·齐

泽曼。用低秩

展开加速卷

积神经网

络

。arXiv 预印本 arXiv:1405.3866，2014。

Mikhail

Khodak, Neil Tenenholtz, Lester

Mackey, and Nicolo



Fusi. Initialization and

regularization

of factorized neural layers,

2021.

米哈伊尔·科

达克、尼尔·特

南霍尔茨、莱

斯特·麦基和

尼科洛·富西

。因子分解神

经层

的初始

化和正则化

，2021。

Diederik

P. Kingma and Jimmy

Ba. Adam: A method

for stochastic optimization, 2017.

4

迪德里克. p .金

玛和吉米.巴

。亚当:随机优

化的方法，2017。

Dmitry Lepikhin, HyoukJoong Lee,

Yuanzhong Xu, Dehao Chen,

Orhan Firat, Yanping Huang,

Maxim Krikun, Noam Shazeer,

and Zhifeng Chen. Gshard:

Scaling giant models with

conditional computation and automatic

sharding, 2020.

Dmitry Lepikhin

、 HyoukJoong Lee 、

Xu 、 Dehao Chen

、 Orhan Firat 、

、 Maxim

Krikun、Noam Shazeer

和

。Gshard:用条件计算

和自动分片

缩放巨型模

型，2020。

Brian Lester, Rami

Al-Rfou, and Noah Constant.

The Power of Scale

for Parameter-Efficient

Prompt Tuning.

arXiv:2104.08691 [cs], April 2021.

URL

http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.

布莱恩·莱

斯特，拉米·艾

尔弗和诺亚

·康斯坦特。参

数高效的快

速调优的规

模力

量 。 arXiv:2104.08691

[cs] ， 2021 年

4 月

。 统 一

资 源 定

位

器

http://arxiv.org/abs/2104.08691。arXiv: 2104.08691。

Chunyuan Li,

Heerad Farkhoor, Rosanne Liu,

and Jason Yosinski. Measuring

the Intrinsic Di￾mension of

Objective Landscapes.arXiv:1804.08838 [cs, stat],

April 2018a.URL http:

李春元

、Heerad

Farkhoor、Rosanne Liu 和 Jason

Yosinski。测量客观

景观的内在

尺

寸。arXiv:1804.08838 [cs，stat]，2018 年

4 月。统

一资源定位

器 http:

//arxiv.org/abs/1804.08838.

arXiv: 1804.08838.

//arxiv.org/abs/1804.08838。arXiv: 1804.08838。

Xiang Lisa Li and

Percy Liang. Prefix-Tuning: Optimizing

Continuous Prompts for Generation.

项与梁佩

西。前缀调整

:优化生成的

连续提示。

arXiv:2101.00190 [cs], January

2021. URL http://arxiv.org/abs/2101.00190.

arXiv:2101.00190

[cs]，2021 年

1 月。统一资源

定位

器

http://arxiv.org/abs/2101.00190。

Yuanzhi Li and

Yingyu Liang. Learning overparameterized

neural networks via stochastic

gradient descent on structured

data. In Advances in

Neural Information Processing Systems,

2018.

和梁

。基于结构化

数据的随机

梯度下降学

习超参数化

神经网络。神

经信息处理

系统进

展，2018 年

。

Yuanzhi Li, Yingyu Liang,

and Andrej Risteski. Recovery

guarantee of weighted low-rank

ap￾proximation via alternating minimization.

In International Conference on

Machine Learning,

pp. 2358–2367.

PMLR, 2016.

、于颖·梁和安

德烈·里斯特

斯基。通过交

替最小化的

加权低秩逼

近的恢复保

证。国际机

器

学习会议，第

2358–2367

页。PMLR，2016。

Yuanzhi Li, Tengyu

Ma, and Hongyang Zhang.

Algorithmic regularization in over-parameterized

matrix sensing and neural

networks with quadratic activations.

In Conference On Learning

The￾ory, pp. 2–47. PMLR,

2018b.

李沅芷，马

腾宇和张弘

扬。具有二次

激活的过参

数矩阵传感

和神经网络

中的算法正

则化。

在学习

理论会议上

，第 2-47

页。PMLR，2018 年 b。

Zhaojiang

Lin, Andrea Madotto, and

Pascale Fung. Exploring versatile

generative language model

via

parameter-efficient transfer learning. In

Findings of the Association

for Computational Lin￾guistics: EMNLP

2020, pp. 441–459, Online,

November 2020. Association for

Computational

Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41.

URL https://aclanthology.

org/2020.findings-emnlp.41.

赵江

·林、安德里亚

·马多托和帕

斯卡尔·冯。通

过参数有效

迁移学习探

索通用生成

语言

模型。计

算逻辑协会

的发现:EMNLP 2020，第 441–459 页

，在线，2020

年 11 月。计

算语

言

学 协

会 。 doi:

10.18653/v1/2020 . 调 查

结 果

-emnlp.41.

URLhttps://aclanthology.org/2020.findings-emnlp.41。

Xiao

Liu, Yanan Zheng, Zhengxiao

Du, Ming Ding, Yujie

Qian, Zhilin Yang, and

Jie Tang. GPT

Understands,

Too. arXiv:2103.10385 [cs], March

2021. URL http://arxiv.org/abs/

2103.10385.

arXiv: 2103.10385.

刘潇、郑延安

、、杜、、钱玉杰、、杨

、。GPT 也理解。arXiv:2103.10385

[cs]，2021 年

3 月

。统一资源定

位器

http://arxiv.org/abs/2103.10385。arXiv: 2103.10385。

Yinhan Liu,

Myle Ott, Naman Goyal,

Jingfei Du, Mandar Joshi,

Danqi Chen, Omer Levy,

Mike

Lewis, Luke Zettlemoyer,

and Veselin Stoyanov. Roberta:

A robustly optimized bert

pretraining approach, 2019.

刘、梅尔

·奥特、纳曼·戈

亚尔、·杜、曼达

尔·乔希、·陈、奥

梅尔·利维、、卢

克·

泽特勒莫

耶和韦塞林

·斯托扬诺夫

。Roberta:一种稳健优

化的 bert 预训练

方法，2019。

Ilya

Loshchilov and Frank Hutter.

Decoupled weight decay regularization.

arXiv preprint

arXiv:1711.05101, 2017.

伊 利 亚

· 洛

希 洛 夫 和

弗

兰 克 · 哈

特

。 去 耦 权

重 衰

减 正 则

化 。 arXiv 预

印

本

arXiv:1711.05101，2017。

Ilya Loshchilov

and Frank Hutter. Decoupled

weight decay regularization, 2019.

伊利亚

·洛希洛夫和

弗兰克·哈特

。解耦权重衰

减正则化，2019。

Rabeeh Karimi Mahabadi,

James Henderson, and Sebastian

Ruder. Compacter: Efficient low-rank

hypercomplex adapter layers, 2021.

拉

比赫·卡利米

·马哈巴迪、詹

姆斯·汉德森

和塞巴斯蒂

安·鲁德。压缩

程序:高效低

秩超

复数适

配器层，2021。

Linyong Nan,

Dragomir Radev, Rui Zhang,

Amrit Rau, Abhinand Sivaprasad,

Chiachun Hsieh,

Xiangru Tang,

Aadit Vyas, Neha Verma,

Pranav Krishna, et al.

Dart: Open-domain structured

data

record to text generation.

arXiv preprint arXiv:2007.02871, 2020.

南，德

拉戈米尔·拉

德夫，张睿，阿

姆里特·劳，阿

比南德·西瓦

普拉萨德，谢

家纯，唐

湘如

，阿迪特·维亚

斯，内哈·维尔

马，普拉纳夫

·克里希纳，等

。arXiv 预印本

arXiv:2007.02871，2020

Jekaterina Novikova, Ondˇrej Dusˇek,

and Verena Rieser. The

e2e dataset: New challenges

for

end- to-end generation.

arXiv preprint arXiv:1706.09254, 2017.

Jekaterina Novikova、Ondˇ rej Dusˇ

ek 和 Verena Rieser。e2e

数

据集:端到端

生成的

新挑

战。arXiv 预印本 arXiv:1706.09254，2017。

Samet Oymak, Zalan Fabian,

Mingchen Li, and Mahdi

Soltanolkotabi. Generalization guaran￾tees for

neural networks via harnessing

the low-rank structure of

the jacobian. arXiv preprint

arXiv:1906.05392, 2019.

萨

梅特·奥伊马

克、扎兰·法比

安、李明臣和

马赫迪·索尔

塔诺·科塔比

。通过利用雅

可

比矩阵的

低秩结构来

保证神经网

络的泛化。arXiv

预

印本 arXiv:1906.05392，2019。

Jonas Pfeiffer,

Aishwarya Kamath, Andreas Ru¨ckle´,

Kyunghyun Cho, and Iryna

Gurevych. Adapter￾fusion: Non-destructive task

composition for transfer learning,

2021.

Jonas Pfeiffer、Aishwarya Kamath、Andreas

Ru ckle、Kyunghyun Cho 和

Iryna Gurevych。

适配

器融合:迁移

学习的非破

坏性任务合

成，2021。

Daniel

Povey, Gaofeng Cheng, Yiming

Wang, Ke Li, Hainan

Xu, Mahsa Yarmohammadi, and

San- jeev Khudanpur. Semi-orthogonal

low-rank matrix factorization for

deep neural networks.

In

Interspeech, pp. 3743–3747, 2018.

Daniel Povey，Cheng，，，Hainan Xu，Mahsa Yarmohammadi

和 San- jeev Khudanpur。

深度神

经网络的半

正交低秩矩

阵分解。在 Interspeech，2018 年

第 3743–3747

页。

4

4

Alec

Radford, Karthik Narasimhan, Tim

Salimans, and Ilya Sutskever.

Improving Language

Under- standing

by Generative Pre-Training. pp.

12, a.

亚历克

·拉德福德、卡

蒂克·纳拉辛

汉、蒂姆·萨利

曼斯和伊利

亚·苏茨基弗

。通过生成

性

预训练提高

语言理解能

力。第

12 页，a。

Alec Radford,

Jeffrey Wu, Rewon Child,

David Luan, Dario Amodei,

and Ilya Sutskever.

Language

Models are Unsupervised Multitask

Learners. pp. 24, b.

亚历

克拉德福德

，杰弗里吴，Rewon 儿

童，大卫栾，达

里奥阿莫代

伊，伊利亚 Sutskever。

语

言模型是无

人监督的多

任务学习者

。第

24 页 b。

Pranav

Rajpurkar, Robin Jia, and

Percy Liang. Know what

you don’t know: Unanswerable

questions for squad. CoRR,

abs/1806.03822, 2018. URL

http://arxiv.org/abs/1806.03822.

Pranav Rajpurkar，Robin Jia 和

Percy Liang。知道

你不知道的

:无法回答的

问题。更

正，abs/1806.03822，2018。统

一资源定位

器 http://arxiv.org/abs/1806.03822。

Sylvestre-Alvise Rebuffi, Hakan Bilen,

and Andrea Vedaldi. Learning

multiple visual domains

with

residual adapters. arXiv:1705.08045 [cs,

stat], November 2017. URL

http://arxiv.org/ abs/1705.08045. arXiv: 1705.08045.

西尔威斯

特·阿尔维斯

·雷布菲、哈坎

·比伦和安德

里亚·维达尔

迪。用剩余适

配器学习

多

个 视 觉

域 。 arXiv:1705.08045 [cs

， stat] ， 2017

年

11 月 。 统

一 资 源

定 位

器 http://arxiv.org/abs/1705.08045。arXiv: 1705.08045。

Andreas

Ru¨ckle´, Gregor Geigle, Max

Glockner, Tilman Beck, Jonas

Pfeiffer, Nils Reimers, and

Iryna Gurevych. Adapterdrop: On

the efficiency of adapters

in transformers, 2020.

Andreas

Ru ckle、Gregor Geigle、Max Glockner、Tilman

Beck、Jonas Pfeiffer、Nils

Reimers 和

Iryna Gurevych。Adapterdrop:变

压器中适配

器的效率，2020。

Tara N

Sainath, Brian Kingsbury, Vikas

Sindhwani, Ebru Arisoy, and

Bhuvana Ramabhadran.

Low- rank

matrix factorization for deep

neural network training with

high-dimensional output

targets. In

2013 IEEE international conference

on acoustics, speech and

signal processing, pp.

6655–

6659. IEEE, 2013.

Tara

N Sainath 、 Brian

Kingsbury 、 Vikas Sindhwani

、 Ebru Arisoy 和

Bhuvana

Ramabhadran。用于具有高

维输出目标

的深度神经

网络训练的

低秩矩阵分

解。 2013 年

IEEE

声学、语

音和信号处

理国际会议

，第 6655–6659 页。IEEE，2013 年。

Mohammad Shoeybi, Mostofa Patwary,

Raul Puri, Patrick LeGresley,

Jared Casper, and Bryan

Catanzaro. Megatron-lm: Training multi-billion

parameter language models using

model par￾allelism, 2020.

Mohammad

Shoeybi，Mostofa Patwary，Raul Puri，Patrick LeGresley，Jared

Casper

和 Bryan Catanzaro。威

震天-lm:使用模

型并行性训

练数十亿参

数语言模型

，2020。

Richard Socher, Alex Perelygin,

Jean Wu, Jason Chuang,

Christopher D. Manning, Andrew

Ng,

and Christopher Potts.

Recursive deep models for

semantic compositionality over a

sentiment

treebank. In Proceedings

of the 2013 Conference

on Empirical Methods in

Natural Language

Processing, pp.

1631–1642, Seattle, Washington, USA,

October 2013. Association for

Computa- tional Linguistics. URL

https://aclanthology.org/D13-1170.

Richard Socher、Alex Perelygin、Jean

Wu、Jason Chuang、Christopher D. Manning、

和 Christopher Potts。情感树库

语义合成的

递归深度模

型。《2013 年自然语

言处理经

验

方法会议论

文集》，第 1631-1642 页，美

国华盛顿州

西雅图，2013 年

10 月

。计算语言

学

协会。统一资

源定位器 https://aclanthology.org/D13-1170。

Ashish Vaswani, Noam Shazeer,

Niki Parmar, Jakob Uszkoreit,

Llion Jones, Aidan N

Gomez,

Łukasz Kaiser, and

Illia Polosukhin. Attention is

all you need. In

Proceedings of the 31st

In￾ternational Conference on Neural

Information Processing Systems, pp.

6000–6010, 2017.

Ashish Vaswani、Noam

Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan

Gomez、ukasz Kaiser 和

Illia Polosukhin。你需要的只

是关注。《第

31 届

国际神经信

息处理系统

会议论文集

》，第 6000–6010 页，2017

年。

Alex Wang, Amanpreet

Singh, Julian Michael, Felix

Hill, Omer Levy, and

Samuel R. Bowman.

Glue:

A multi-task benchmark and

analysis platform for natural

language understanding, 2019.

王敬

实、阿曼普瑞

特·辛格、朱利

安·迈克尔、菲

利克斯·希尔

、奥马尔·利维

和塞缪尔

·鲍

曼。Glue:自然语言

理解的多任

务基准和分

析平台，2019。

Alex Wang, Yada

Pruksachatkun, Nikita Nangia, Amanpreet

Singh, Julian Michael, Felix

Hill, Omer

Levy, and

Samuel R. Bowman. Superglue:

A stickier benchmark for

general-purpose language

understanding systems,

2020.

王敬

实、Yada Pruksachatkun、Nikita Nangia、Amanpreet

Singh、Julian Michael、Felix

Hill、Omer Levy

和 Samuel R. Bowman。强力胶

:通用语言理

解系统的粘

性基准，2020

年。

Alex Warstadt, Amanpreet

Singh, and Samuel R

Bowman. Neural network acceptability

judgments.

亚

历克斯·瓦施

塔特、阿曼普

里特·辛格和

塞缪尔·鲍曼

。神经网络可

接受性判断

。

arXiv preprint

arXiv:1805.12471, 2018.

arXiv 预印本

arXiv:1805.12471，2018。

Adina Williams, Nikita

Nangia, and Samuel Bowman.

A broad-coverage challenge corpus

for

sen- tence understanding

through inference. In Proceedings

of the 2018 Conference

of the

North American

Chapter of the Association

for Computational Linguistics: Human

Language

Technolo- gies, Volume

1 (Long Papers), pp.

1112–1122, New Orleans, Louisiana,

June 2018.

Association for

Computational Linguistics. doi: 10.18653/v1/N18-1101.

URL

https://www.aclweb. org/anthology/N18-1101.

艾迪

娜·威廉姆斯

尼基塔·南吉

亚和塞缪尔

·鲍曼。通过推

理进行科学

理解的大范

围挑战

语料

库。计算语言

学协会北美

分会 2018 年会议

论文集:人类

语言技术，第

1 卷(长论

文)，第

1112-1122 页，路易斯安

那州新奥尔

良，2018 年 6

月。计算

语言学协会

。doi:

10.18653/v1/N18-1101 。 统

一 资 源 定

位

器 https://www.aclweb.org/anthology/N18-

1101。

Thomas

Wolf, Lysandre Debut, Victor

Sanh, Julien Chaumond, Clement

Delangue, Anthony Moi,

Pierric

Cistac, Tim Rault, Re´mi

Louf, Morgan Funtowicz, Joe

Davison, Sam Shleifer, Patrick

von Platen, Clara Ma,

Yacine Jernite, Julien Plu,

Canwen Xu, Teven Le

Scao, Sylvain Gug￾ger, Mariama

Drame, Quentin Lhoest, and

Alexander M. Rush. Transformers:

State-of-the-art

natural language processing.

In Proceedings of the

2020 Conference on Empirical

Methods in

Natural Language

Processing: System Demonstrations, pp.

38–45, Online, October 2020.

As￾sociation for Computational Linguistics.

URL https://www.aclweb.org/anthology/

2020.emnlp-demos.6.

Thomas

Wolf 、 弗 拉

达

利 · 肖 尼

特 、 Victor Sanh

、 Julien Chaumond 、

Clement

Delangue 、 Anthony

Moi 、 Pierric Cistac

、 Tim Rault 、

Re mi Louf 、

Morgan

Funtowicz、Joe Davison、Sam Shleifer、Patrick

von Platen、Clara Ma、Yacine

Jernite、Julien

Plu、Canwen Xu 、Teven Le

Scao 、Sylvain Gug- ger

、Mariama

Drame、Quentin Lhoest 和

Alexander

M. Rush。变形金刚:最

先进的自然

语言处理。

《2020 年

自然语言处

理经验方法

会议论文集

:系统演示》,第

38–45

页，在线，2020 年

10 月

。

计 算 语 言

学

协 会 。 统

一 资

源 定 位

器 https://www.aclweb.org/anthology/

2020.emnlp-demos.6。

4

5

Greg Yang and

Edward J. Hu. Feature

Learning in Infinite-Width Neural

Networks.

arXiv:2011.14522 [cond-mat], May

2021. URL http://arxiv.org/abs/2011.14522.

arXiv:

2011.14522.

格

雷格·杨和爱

德华·胡。无限

宽神经网络

中的特征学

习。arXiv:2011.14522 [cond￾mat] ，

2021 年 5 月

。 统 一

资 源

定 位 器

http://arxiv.org/abs/2011.14522 。

arXiv:

2011.14522。

Elad Ben

Zaken, Shauli Ravfogel, and

Yoav Goldberg. Bitfit: Simple

parameter-efficient fine￾tuning for transformer-based

masked language-models, 2021.

Elad

Ben Zaken，Shauli Ravfogel 和

Yoav Goldberg。Bitfit:基于 transformer 的屏

蔽语

言模型

的简单参数

高效微调，2021。

Yu Zhang, Ekapol

Chuangsuwanich, and James Glass.

Extracting deep neural network

bottleneck

features using low-rank

matrix factorization. In 2014

IEEE international conference on

acoustics, speech and signal

processing (ICASSP), pp. 185–189.

IEEE, 2014.

张

瑜，Ekapol Chuangsuwanich

和詹姆斯

·格拉斯。利用

低秩矩阵分

解提取深度

神经网络

瓶

颈特征。2014 年 IEEE

声

学、语音和信

号处理国际

会议，第 185–189 页。IEEE，2014

年

。

Yong Zhao, Jinyu Li,

and Yifan Gong. Low-rank

plus diagonal adaptation for

deep neural

networks. In

2016 IEEE International Conference

on Acoustics, Speech and

Signal Processing

(ICASSP),

、李靳宇和龚

一凡。深度神

经网络的低

秩加对角自

适应。2016

年 IEEE 声学

、语音和信号

处

理国际会

议(ICASSP)，

pp. 5005–5009. IEEE, 2016.

第 5005–5009 页。IEEE，2016。

Victor

Zhong, Caiming Xiong, and

Richard Socher. Seq2sql: Generating

structured queries from

natural

language using reinforcement learning.

CoRR, abs/1709.00103, 2017. URL

http://

arxiv.org/abs/1709.00103.

钟，熊

和理查德索

契。Seq2sql:使用强化

学习从自然

语言生成结

构化查询。更

正，abs/

1709.00103，2017。统一资源

定位器

http://arxiv.org/abs/1709.00103。

A LARGE LANGUAGE

MODELS STILL NEED PARAMETER

UPDATES

B 大型

语言模型仍

然需要参数

更新

Few-shot

learning, or prompt engineering,

is very advantageous when

we only have a

handful of

training samples.

However, in practice, we

can often afford to

curate a few thousand

or more

training examples

for performance-sensitive applications. As

shown in Table 8,

fine-tuning

improves the model

performance drastically compared to

few-shot learning on datasets

large and

small. We

take the GPT-3 few-shot

result on RTE from

the GPT-3 paper (Brown

et al., 2020). For

MNLI-matched, we use two

demonstrations per class and

six in-context examples in

total.

当我们

只有少数几

个训练样本

时，少量学习

或快速工程

是非常有利

的。然而，在实

践中，

我们经

常能够为性

能敏感的应

用程序策划

几千个甚至

更多的训练

示例。如所示

Table 8,与

在大数据

集和小数据

集上的少量

学习相比，微

调显著提高

了模型性能

。我们从 GPT-3 的论

文中提取了

GPT-3 在

RTE 上的少量

发射结果(Brown et al.,2020).对

于

MNLI 匹配，我们

每

个类使用

两个演示，总

共使用六个

上下文示例

。

5

Method

方法

GPT-3 Few-Shot

GPT-3 Fine-Tuned

GPT-3 少射

GPT-3

微

调

MNLI-m (Val.

Acc./%) RTE (Val. Acc./%)

5

MNLI-m (Val。Acc。/%) RTE

(Val。Acc。/%)

40.6

69.0

40.6

69.0

89.5 85.4

89.5

85.4

Table 8: Fine-tuning

significantly outperforms few-shot learning

on GPT-3 (Brown et

al., 2020).

表 8:在

GPT-3 上，微

调明显优于

少量学习(Brown et al.,2020).

C INFERENCE LATENCY INTRODUCED

BY ADAPTER LAYERS

D

适

配器层引入

的推理延迟

Adapter layers are external

modules added to a

pre-trained model in a

sequential manner, whereas

our

proposal, LoRA, can be

seen as external modules

added in a parallel

manner. Consequently,

adapter layers

must be computed in

addition to the base

model, inevitably introducing additional

latency. While as pointed

out in Ru¨ckle´ et

al. (2020), the latency

introduced by adapter layers

can be mitigated when

the model batch size

and/or sequence length is

large enough to full

utilize

the hardware parallelism.

We confirm their observation

with a similar latency

study on GPT-2

medium

and point out that

there are scenarios, notably

online inference where the

batch size is

small,

where the added latency

can be significant.

适配器层是

以顺序方式

添加到预训

练模型的外

部模块，而我

们的提议

LoRA 可

以被视为以

并

行方式添

加的外部模

块。因此，除了

基本模型之

外，还必须计

算适配器层

，这不可避免

地

会带来额

外的延迟。正

如在中指出

的

Ru¨ckle´ et al.(2020),当模型批

量大小和/或

序列

长度足

够大以充分

利用硬件并

行性时，可以

减轻适配器

层引入的延

迟。我们通过

在

GPT-2 培

养基上

进行的类似

延迟研究证

实了他们的

观察结果，并

指出存在一

些情况，特别

是在线推

断

，其中批量较

小，增加的延

迟可能很大

。

We measure the latency

of a single forward

pass on an NVIDIA

Quadro RTX8000 by averaging

over 100 trials. We

vary the input batch

size, sequence length, and

the adapter bottleneck

dimension

我们通过平

均 100 次试验来

测量 NVIDIA

Quadro RTX8000 上单次

转发的延迟

。我们改变输

入批量大小

、序列长度和

适配器瓶颈

尺寸

r.

We test two adapter

designs: the original one

by Houlsby et al.

(2019), which we call

AdapterH

,

and a

recent, more efficient variant

by Lin et al.

(2020), which we call

AdapterL

. See Section

5.1

for more details

on the designs. We

plot the slow-down in

percentage compared to the

no-adapter

baseline in Figure

5.

r.我们测

试了两种适

配器设计:原

始设计 Houlsby et

al.(2019),我们

称之为 AdapterH，以

及

最近的一个

更有效的变

体 Lin

et al.(2020),我们称之

为 adapter 1。看见

Section 5.1

了解

更多设计细

节。我们在中

绘制了与无

适配器基线

相比的速度

下降百分比

Figure 5.

Seq Len = 128

Seq Len = 256

Seq Len = 512

序列长度= 128 序

列长度= 256

序列

长度= 512

35

35

30

30

25

25

0 0

Adapter

250100

1

H

Adapter

250100

1

5

20

20

15

15

10

10

5

5

0

0

1 2 4 8

1632

一 2 四

8 1632

Batch Size

批

量

1 2 4

8 1632 一 2

四 8 1632

Batch

Size

批量

0 0

Adapte

1

L

Adapter

1

250100 250100

5

1

2 4 8 1632

一 2 四 8

1632

Batch Size

批量

Figure 5: Percentage slow-down

of inference latency compared

to the no-adapter (r

= 0) baseline.

The

top row shows the

result for AdapterH

and the bottom row

AdapterL

. Larger batch

size and

sequence length

help to mitigate the

latency, but the slow-down

can be as high

as over 30% in

an

online, short-sequence-length scenario.

We tweak the colormap

for better visibility.

图

5:与无适配器

(r

= 0)基线相比，推

理延迟降低

的百分比。顶

行显示 AdapterH 的结

果，

底行显示

AdapterL 的结果。较大

的批量和序

列长度有助

于减少延迟

，但在短序列

长度的在

线

场景中，速度

会降低 30%以上

。我们调整色

图以获得更

好的可视性

。

E DATASET DETAILS

F

数据集详细

信息

GLUE Benchmark is

a wide-ranging collection of

natural language understanding tasks.

It

includes MNLI (inference,

Williams et al. (2018)),

SST-2 (sentiment analysis, Socher

et al.

(2013)), MRPC

(paraphrase detection, Dolan &

Brockett (2005)), CoLA (linguistic

acceptability,

Warstadt et al.

(2018)), QNLI (inference, Rajpurkar

et al. (2018)), QQP8

(question-answering),

RTE (inference),

GLUE

Benchmark 是一个

广泛的自然

语言理解任

务集合。它包

括 MNLI(推论，Williams et

al.(2018)),SST-2( 情 绪

分 析

， Socher et al.(2013)),MRPC(

意 译 检

测 ，

Dolan &

Brockett(2005)),可乐(语言

可接受性，Warstadt et

al.(2018)),QNLI(推

论，Rajpurkar

et al.(2018)),QQP

8

(问答)，RTE(推理

)，

8

https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs

8

https://quoradata . Quora .

com/First-Quora-Dataset-Release-Question-Pairs

{ } {

}

{ } {

}

{ }

\

\

{ }

\

\

5

and STS-B

(textual similarity, Cer et

al. (2017)). The broad

coverage makes GLUE benchmark

a

standard metric to

evaluate NLU models such

as RoBERTa and DeBERTa.

The individual

datasets are

released under different permissive

licenses.

和 STS-B(文本相似

性，Cer et

al.(2017)).广 泛 的 覆

盖

面 使 G L

U E b e

n c h m

a r k 成

为

评 估 R o

B E R T

a 和 D e

B E R T

a 等 N L

U 模

型 的 标

准 指

标 。 各

个 数 据

集 根

据 不 同

的 许

可 协 议

发 布

。

WikiSQL is introduced

in Zhong et al.

(2017) and contains 56,

355/8, 421 training/validation ex￾amples.

The task is to

generate SQL queries from

natural language questions and

table schemata.

We encode

context as 

under

the BSD 3-Clause License.

x = table schema,

query and target as

y = SQL .

The dataset is release

WikiSQL 是在 Zhong et

al.(2017)并

且包含 56，355/8，421 个训

练/验证样本

。任务是从

自

然语言问题

和表模式中

生成

SQL 查询。我

们将上下文

编码为 x =表模

式，将查询和

目标编

码为

y = SQL。数据集是在

BSD 3

条款许可下

发布的。

SAMSum is introduced

in Gliwa et al.

(2019) and contains 14,

732/819 training/test examples. It

consists of staged chat

conversations between two people

and corresponding abstractive

summaries

written by linguists. We

encode context as ”

n” concatenated utterances followed

by a

”

Creative

Commons BY-NC-ND 4.0.

n

n”, and target as

y = summary .

The dataset is released

under the non-commercial licence:

SAMSum 是在

Gliwa et al.(2019)并且包含

14，732/819 个

训练/测试示

例。它由两个

人之

间的阶

段性聊天对

话和语言学

家撰写的相

应抽象总结

组成。我们将

上下文编码

为“n”个连

接

布

:

的

Creati

话语，

ve 

后

Commons

B

跟

一个“

Y-

n

n

NC-N

”

D

，

4.0

目标

。

为

y = summary。该数据集在

非商业许可

下发

E2E

NLG Challenge was first

introduced in Novikova et

al. (2017) as a

dataset for training end￾to-

end, data-driven natural language

generation systems and is

commonly used for data-to-text

evalua- tion. The E2E

dataset consists of roughly

42, 000 training, 4,

600 validation, and 4,

600

test exam- ples

from the restaurant domain.

Each source table used

as input can have

multiple

references. Each sample

input (x, y) consists

of a sequence of

slot-value pairs, along with

a

corresponding natural language

reference text. The dataset

is released under Creative

Commons

BY-NC-SA 4.0.

e2e·NLG

挑战赛

首次举办于

1989 年 Novikova et

al.(2017)作为用于

训练端到端

、数据驱动

的

自然语言生

成系统的数

据集，并且通

常用于数据

到文本的评

估。E2E 数据集包

括大约 42，000

个训

练、4，600 个验证和

4，600 个来自餐馆

领域的测试

样本。用作输

入的每个源

表可以有多

个引用。每个

样本输入(x，y)由

一系列槽值

对以及相应

的自然语言

参考文本组

成。该数据集

由-

NC-SA

4.0 在 Creative Commons

下发布

。

DART is an

open-domain data-to-text dataset described

in Nan et al.

(2020). DART inputs are

structured as sequences of

ENTITY — RELATION —

ENTITY triples. With 82K

examples in

total, DART

is a significantly larger

and more complex data-to-text

task compared to E2E.

The

dataset is released

under the MIT license.

DART 是开放域数

据转文本数

据集，如中所

述 Nan et

al.(2020).D A R T

输 入 的 结

构

是 实

体 -

关

系 - 实 体

三 元

组 的 序

列 。 与

E 2

E 相 比 ，

D A R T

总 共 有

8 2

K 个 示 例

， 是 一

个 更

大 、 更 复

杂

的 数 据 转

文

本 任 务 。

该

数 据 集 是

在

麻 省 理 工

学

院 许 可 下

发

布

的 。

WebNLG

is another commonly used

dataset for data-to-text evaluation

(Gardent et al., 2017).

With 22K examples in

total WebNLG comprises 14

distinct categories, nine of

which are seen

during

training. Since five of

the 14 total categories

are not seen during

training, but are

represented

in the test set,

evaluation is typically broken

out by “seen” categories

(S), “unseen”

categories (U)

and “all” (A). Each

input example is represented

by a sequence of

SUBJECT —

PROPERTY —

OBJECT triples. The dataset

is released under Creative

Commons BY-NC-SA

4.0.

WebNLG

是另一

个常用于数

据到文本评

估的数据集

(Gardent et al.,2017).WebNLG 总共有

22K 个示

例，包括 14 个不

同的类别，其

中

9 个是在培

训期间看到

的。由于 14 个总

类别中的

†

†

5

5

个

在训练期间

不可见，但是

在测试集中

出现，所以评

估通常由“可

见”类别(S)、“不可

见”类别(U)和“全

部”(A)来划分。每

个输入的例

子都由一系

列的主语——属

性——宾语

三元

组表示。该数

据集由-NC-SA 4.0 在

Creative Commons 下

发布。

G

HYPERPARAMETERS USED IN EXPERIMENTS

H 实验中

使用的超参

数

H.1 ROBERTA

H.2 罗伯塔

We train

using AdamW with a

linear learning rate decay

schedule. We sweep learning

rate,

number of training

epochs, and batch size

for LoRA. Following Liu

et al. (2019), we

initialize the

LoRA modules

to our best MNLI

checkpoint when adapting to

MRPC, RTE, and STS-B,

instead

of the usual

initialization; the pre-trained model

stays frozen for all

tasks. We report the

median

over 5 random

seeds; the result for

each run is taken

from the best epoch.

For a fair comparison

with the setup in

Houlsby et al. (2019)

and Pfeiffer et al.

(2021), we restrict the

model sequence

length to

128 and used a

fixed batch size for

all tasks. Importantly, we

start with the pre-trained

RoBERTa large model when

adapting to MRPC, RTE,

and STS-B, instead of

a model already

adapted

to MNLI. The

hyperparameters

used in our

runs

runs in 

with

Table 9.

this restricted

setup are marked with

. See the

我

们使用

AdamW 和线

性学习率衰

减时间表进

行训练。我们

扫描学习率

，训练时期的

数量，

以及 LoRA

的

批量大小。跟

随 Liu et al.(2019),当适应

MRPC、RTE 和

STS-B 时，我们将

LoRA

模

块初始化为

我们的最佳

MNLI 检查点，而不

是通常的初

始化；预训练

模型在所有

任务

中保持

冻结。我们报

告了 5

个随机

种子的中位

数；每次运行

的结果取自

最佳时期。为

了与

中的设

置进行公平

的比较 Houlsby et

al.(2019)和 Pfeiffer et al.(2021),我

们将模型序

列长度限制

为

128，并对所有

任务使用固

定的批量大

小。重要的是

，当适应 MRPC、RTE 和 STS-B

时

，我们从预训

练的 RoBERTa 大模型

开始，而不是

已经适应 MNLI

的

模型。具有此

受限设置的

运

行标有。请

参阅我们在

运行中使用

的超参数 Table 9.

H.3 DEBERTA

H.4 德

伯塔

We again train using

AdamW with a linear

learning rate decay schedule.

Following He et al.

(2021), we tune learning

rate, dropout probability, warm-up

steps, and batch size.

We use the

same

model sequence length used

by (He et al.,

2021) to keep our

comparison fair. Following He

et al. (2021), we

initialize the LoRA modules

to our best MNLI

checkpoint when adapting to

MRPC, RTE, and STS-B,

instead of the usual

initialization; the pre-trained model

stays frozen for

all

tasks. We report the

median over 5 random

seeds; the result for

each run is taken

from the best

epoch.

See the hyperparameters used

in our runs in

Table 10.

我们再

次使用 AdamW

和线

性学习率衰

减时间表进

行训练。跟随

He et al.(2021),我们调整

学

习速度、退出

概率、热身步

骤和批量大

小。我们使用

的模型序列

长度与(He

et

al.,2021)为了

让我们的比

较公平。跟随

He et al.(2021),当适应

MRPC、RTE 和 STS-B 时

，

我们将 LoRA 模块

初始化为我

们的最佳 MNLI

检

查点，而不是

通常的初始

化；预训练模

型在

所有任

务中保持冻

结。我们报告

了 5 个随机种

子的中位数

；每次运行的

结果取自最

佳时

期。请参

阅我们在运

行中使用的

超参数 Table 10.

5

Method Dataset MNLI SST-2

MRPC CoLA QNLI QQP

RTE STS-B

Optimizer AdamW

Warmup Ratio 0.06

LR

Schedule Linear

Batch Size

16 16 16 32

32 16 32 16

RoBERTa base # Epochs

30 60 30 80

25 25 80 40

LoRA Learning Rate 5E-04

5E-04 4E-04 4E-04 4E-04

5E-04 5E-04 4E-04

LoRA

Config. rq = rv

= 8

LoRA α

8

Max Seq. Len.

512

Batch Size 4

4 4 4 4

4 8 8

RoBERTa

large # Epochs 10

10 20 20 10

20 20 30

LoRA

Learning Rate 3E-04 4E-04

3E-04 2E-04 2E-04 3E-04

4E-04 2E-04

LoRA Config.

rq = rv =

8

LoRA α 16

Max Seq. Len. 128

128 512 128 512

512 512 512

Batch

Size 4

RoBERTa large

# Epochs 10 10

20 20 10 20

20 10

LoRA†

Learning

Rate 3E-04 4E-04 3E-04

2E-04 2E-04 3E-04 4E-04

2E-04

LoRA Config. rq

= rv = 8

LoRA α 16

Max

Seq. Len. 128

Batch

Size 32

RoBERTa large

# Epochs 10 20

20 20 10 20

20 20

AdptP

(3M)†

Learning Rate 3E-05 3E-05

3E-04 3E-04 3E-04 3E-04

3E-04 3E-04

Bottleneck r

64

Max Seq. Len.

128

Batch Size 32

RoBERTa large # Epochs

5 20 20 20

10 20 20 20

AdptP

(0.8M)† Learning Rate

3E-04 3E-04 3E-04 3E-04

3E-04 3E-04 3E-04 3E-04

Bottleneck r 16

Max

Seq. Len. 128

Batch

Size 32

RoBERTa large

# Epochs 10 5

10 10 5 20

20 10

方法

资料组 MNLI

SST-2 MRPC 可乐

QNLI QQP

RTE STS-B

【计算机】优

化

程序

阿达姆

预热比 0.06

LR 计划

线性的

批量

16 16 16 32

32 16 32 16

罗伯塔基地

#时代 30 60 30

80 25 25 80

40

劳拉 学

习率 5E-04

5E-04 4E-04 4E-04 4E-04

5E-04 5E-04 4E-04

LoRA

配置。 rq = rv

= 8

劳

拉 α

8

最大序列

。莱恩。 512

批量

四

四 四 四 四

四

8 8

罗伯塔·拉吉

#时代 10

10 20 20 10

20 20 30

劳拉

学

习率 3E-04 4E-04 3E-04

2E-04 2E-04 3E-04 4E-04

2E-04

LoRA 配置。 rq

= rv = 8

劳

拉 α 16

最大序列

。莱恩。

128 128 512 128

512 512 512 512

批量 四

罗伯塔·拉吉

#时代 10 10

20 20 10 20

20 10

LoRA 学习率

3E-04

4E-04 3E-04 2E-04 2E-04

3E-04 4E-04 2E-04

LoRA

配置。 rq = rv

= 8

劳拉 α

16

最

大序列。莱恩

。 128

5

批量 32

罗伯塔

·拉吉 #时代

10 20 20 20

10 20 20 20

AdptP (3M) 学

习率 3E-05

3E-05 3E-04 3E-04 3E-04

3E-04 3E-04 3E-04

瓶颈河

64

最大序列。莱

恩。 128

批量 32

罗伯

塔·拉吉 #时代

5 20 20

20 10 20 20

20

AdptP(0.8 米) 学习率

3E-04 3E-04 3E-04 3E-04

3E-04 3E-04 3E-04 3E-04

瓶

颈河 16

最大序

列。莱恩。 128

批量

32

罗伯塔·拉吉

#时代 10 5

10 10 5 20

20 10

Table 9:

The hyperparameters we used

for RoBERTa on the

GLUE benchmark.

表 9:我们

在

GLUE 基准测试

中为 RoBERTa 使用的

超参数。

H.5 GPT-2

H.6 GPT-2

We train all of

our GPT-2 models using

AdamW (Loshchilov & Hutter,

2017) with a linear

learning rate schedule for

5 epochs. We use

the batch size, learning

rate, and beam search

beam

size described in

Li & Liang (2021).

Accordingly, we also tune

the above hyperparameters for

LoRA. We report the

mean over 3 random

seeds; the result for

each run is taken

from the best

epoch.

The hyperparameters used for

LoRA in GPT-2 are

listed in Table 11.

For those used for

other baselines, see Li

& Liang (2021).

我们

使用

AdamW 训练我

们所有的 GPT-2 模

型(Loshchilov

& Hutter,2017)具有 5 个时

期的线

性学

习率时间表

。我们使用批

次大小、学习

率和光束搜

索光束大小

，如中所述 Li &

Liang(2021).相

应地，我们也

为

LoRA 调整上述

超参数。我们

报告了 3 个随

机种子的平

均值；每次运

行的结果取

自最佳时期

。GPT-2

中用于 LoRA 的超

参数列于 Table

11.有

关用于其他

基线的信息

，请参见 Li & Liang(2021).

H.7 GPT-3

H.8 GPT-3

For all GPT-3 experiments,

we train using AdamW

(Loshchilov & Hutter, 2017)

for 2 epochs with

a batch size of

128 samples and a

weight decay factor of

0.1. We use a

sequence length of 384

for

对于

所有的 GPT-3 实验

，我们使用

AdamW 进

行训练(Loshchilov & Hutter,2017)对于

批次

大小为

128 个样本且重

量衰减因子

为 0.1 的

2 个时期

。我们使用序

列长度 384 来表

示

AdptH

(6M)† Learning Rate

Bottleneck r

3E-05 3E-04

3E-04 3E-04 3E-04

64

3E-04 3E-04 3E-04

Max

Seq. Len. 128

Batch

Size 32

RoBERTa large

# Epochs 10 5

10 10 5 20

20 10

AdptH

(0.8M)†

Learning Rate 3E-04 3E-04

3E-04 3E-04 3E-04 3E-04

3E-04 3E-04

Bottleneck r

8

Max Seq. Len.

128

AdptH

(6M)† Learning

Bottleneck 

Rate

r

3E-05 3E-04 3E-04 3E-04

64

3E-04 3E-04 3E-04

3E-04

Max Seq. Len.

128

Batch Size 32

RoBERTa large # Epochs

10 5 10 10

5 20 20 10

AdptH

(0.8M)† Learning

Bottleneck

Rate

r

3E-04 3E-04

3E-04 3E-04

8

3E-04

3E-04 3E-04 3E-04

Max

Seq. Len. 128

5

Dataset E2EWebNLGDART

Training Dataset

E2EWebNLGDART

Training

Method Dataset

MNLI SST-2 MRPC CoLA

QNLI QQP RTE STS-B

Optimizer AdamW

Warmup Ratio

0.1

LR Schedule Linear

Batch Size 8 8

32 4 6 8

4 4

DeBERTa XXL

# Epochs 5 16

30 10 8 11

11 10

LoRA Learning

Rate 1E-04 6E-05 2E-04

1E-04 1E-04 1E-04 2E-04

2E-04

Weight Decay 0

0.01 0.01 0 0.01

0.01 0.01 0.1

CLS

Dropout 0.15 0 0

0.1 0.1 0.2 0.2

0.2

LoRA Config. rq

= rv = 8

LoRA α 8

Max

Seq. Len. 256 128

128 64 512 320

320 128

方法 资料

组

MNLI SST-2 MRPC 可乐

QNLI QQP RTE STS-B

【计算

机】优

化程序

阿达姆

预热

比 0.1

LR 计划 线性

的

批量

8 8 32 四

6 8 四

四

德贝塔·XXL

#时

代 5 16 30

10 8 11 11

10

劳拉 学习

率 1E-04

6E-05 2E-04 1E-04 1E-04

1E-04 2E-04 2E-04

重量衰减

0

0.01 0.01 0 0.01

0.01 0.01 0.1

CLS

辍学 0.15 0 0

0.1 0.1 0.2 0.2

0.2

LoRA 配置。 rq

= rv = 8

劳

拉 α 8

最大序列

。莱恩。

256 128 128 64

512 320 320 128

Table 10: The hyperparameters

for DeBERTa XXL on

tasks included in the

GLUE benchmark.

表 10:DeBERTa

XXL 在 GLUE 基

准测试中包

含的任务的

超参数。

Optimizer 

Weight Decay

0.01

AdamW

0.01 0.0

Dropout Prob 0.1 0.1

0.0

Batch Size 8

# Epoch 5

Warmup

Steps

Learning Rate Schedule

500

Linear

Label Smooth

0.1 0.1 0.0

Learning

Rate

Adaptation 

LoRA

α

0.0002

rq =

rv =

4

32

Inference

Beam Size

Length Penalty

no repeat

ngram size

10

0.9

0.8 0.8

4

优化

器权重

衰减

0.01

AdamW 

0.01

0.0

辍学问题 0.1 0.1

0.0

批

量 8

6

#纪元 5

热身

步骤

学习率

计划

500

线性的

标签平滑 0.1 0.1

0.0

学

习速率适

应

LoRA α

0.0002

rq = rv

= 4

32

推理

光束尺

寸长度

损失

没有重复的

ngram 大小

10

0.9 0.8 0.8

四

Table 11: The hyperparameters

for GPT-2 LoRA on

E2E, WebNLG and DART.

表 11:GPT-2 劳

拉在 E2E，韦伯尼

格和

DART 的超参

数。

WikiSQL (Zhong

et al., 2017), 768

for MNLI (Williams et

al., 2018), and 2048

for SAMSum

(Gliwa et

al., 2019). We tune

learning rate for all

method-dataset combinations. See Section

D.4

for more details

on the hyperparameters used.

For prefix-embedding tuning, we

find the

optimal lp

and li to be

256 and 8, respectively,

totalling 3.2M trainable parameters.

We use

lp =

8 and li =

8 for prefix-layer tuning

with 20.2M trainable parameters

to obtain the overall

best performance. We present

two parameter budgets for

LoRA: 4.7M (rq =

rv = 1 or

rv = 2)

and

37.7M (rq = rv

= 8 or rq

= rk = rv

= ro = 2).

We report the best

validation performance

from each

run. The training hyperparameters

used in our GPT-3

experiments are listed in

Table 12.

WikiSQL(Zhong et

al.,2017),MNLI 的 768(Williams et

al.,2018), 萨 姆 森

的

2048 年

(Gliwaet al.,2019).我们调

整所有方法

-数据集组合

的学习率。看

见

Section D.4 有关所

用

超参数的更

多详细信息

。对于前缀嵌

入调整，我们

发现最佳

lp 和

li 分别为 256

和 8，总

共 3.2M 个可训练

参数。我们使

用

lp = 8 和

li = 8，通过 20.2M

的

可训练参数

进行前缀层

调整，以获得

整体最佳性

能。我们为 LoRA 提

出两个参数

预算:4.7M (rq

= rv = 1

或 rv =

2)和

37.7M (rq = rv

= 8 或

rq =

rk = rv =

ro = 2)。我们报告每

次运行的最

佳验证性

能

。我们在

GPT-3 实验

中使用的训

练超参数列

于 Table 12.

I COMBINING LORA WITH

PREFIX TUNING

J 将

LORA 与前缀

调整相结合

LoRA can be

naturally combined with existing

prefix-based approaches. In this

section, we

evaluate two

combinations of LoRA and

variants of prefix-tuning on

WikiSQL and MNLI.

LoRA

可以自然地

与现有的基

于前缀的方

法相结合。在

本节中，我们

评估了两种

LoRA 组合以

及 WikiSQL

和

MNLI 上的前缀调

整变体。

LoRA+PrefixEmbed (LoRA+PE)

combines LoRA with prefix-embedding

tuning, where we insert

lp + li special

tokens whose embeddings are

treated as trainable parameters.

For more on prefix￾embedding

tuning, see Section 5.1.

LoRA+PrefixEmbed (LoRA+PE)结合

了 LoRA 和前缀嵌

入调优，其中

我们插入

lp + li 特

殊令

牌，其嵌

入被视为可

训练参数。有

关前缀嵌入

调整的更多

信息，请参见

Section 5.1.

LoRA+PrefixLayer (LoRA+PL)

combines LoRA with prefix-layer

tuning. We also insert

lp + li

LoRA+前缀层(LoRA+PL)结合

了

LoRA 和前缀层

调整。我们也

插入 lp +

li

special tokens; however,

instead of letting the

hidden representations of these

tokens evolve natu-

特殊代

币；然而，与其

让这些标记

的隐藏表示

自然发展

6

Hyperparameters Fine-Tune PreEmbed

PreLayer BitFit AdapterH LoRA

Optimizer AdamW

Batch Size

128

# Epoch 2

Warmup Tokens 

LR

Schedule

250,000

Linear

Learning

Rate 5.00E-06 5.00E-04 1.00E-04

1.6E-03 1.00E-04 2.00E-04

超

参数

微调 预

嵌入 预层 比

特

Fit

适配器 h 劳

拉

【计算机】优

化

程序

阿达

姆

批量

128

#纪元

2

预热令牌 LR

计

划 250,000

线性的

学

习率

5.00E-06 5.00E-04 1.00E-04 1.6E-03

1.00E-04 2.00E-

04

Table

12: The training hyperparameters

used for different GPT-3

adaption methods. We use

the

same hyperparameters for

all datasets after tuning

learning rate.

表 12:用于

不同

GPT-3 适应方

法的训练超

参数。在调整

学习率后，我

们对所有数

据集使用相

同的超参数

。

rally, we

replace them after every

Transformer block with an

input agnostic vector. Thus,

both the

embeddings and

subsequent Transformer block activations

are treated as trainable

parameters. For

more on

prefix-layer tuning, see Section

5.1.

rally，我们用一个

输入不可知

的向量在每

个变换器块

后替换它们

。因此，嵌入和

随后的变

压

器块激活都

被视为可训

练参数。有关

前缀层调整

的更多信息

，请参见 Section

5.1.

In Table 15,

we show the evaluation

results of LoRA+PE and

LoRA+PL on WikiSQL and

MultiNLI. First of all,

LoRA+PE significantly outperforms both

LoRA and prefix-embedding

tuning

on WikiSQL, which indicates

that LoRA is somewhat

orthogonal to prefix-embedding

tuning.

On MultiNLI, the combination

of LoRA+PE doesn’t perform

better than LoRA, possibly

because LoRA on its

own already achieves performance

comparable to the human

baseline.

Secondly, we notice

that LoRA+PL performs slightly

worse than LoRA even

with more trainable

parameters.

We at- tribute this

to the fact that

prefix-layer tuning is very

sensitive to the choice

of

learning rate and

thus makes the optimization

of LoRA weights more

difficult in LoRA+PL.

在…里

Table

15,我们展示了

LoRA+PE 和 LoRA+PL 在

WikiSQL 和 MultiNLI 上的

评测结果。首

先，在

WikiSQL 上，LoRA+PE 明显

优于 LoRA

和前缀

嵌入调优，这

表明 LoRA 与前缀

嵌入调优有

些正交。在 MultiNLI

上

，LoRA+PE 的组合并不

比 LoRA 表现得更

好，可能是因

为

LoRA 本身已

经

达到了与人

类基线相当

的性能。其次

，我们注意到

，即使有更多

的可训练参

数，LoRA+PL

的性能也

比

LoRA 稍差。我们

认为这是因

为前缀层调

整对学习速

率的选择非

常敏感，从而

使得

LoRA+PL 中

LoRA 权重

的优化更加

困难。

K ADDITIONAL

EMPIRICAL EXPERIMENTS

L 附加的

经验实验

6

L.1 ADDITIONAL EXPERIMENTS

ON GPT-2

L.2 GPT-2

号

上的附加实

验

We also repeat

our experiment on DART

(Nan et al., 2020)

and WebNLG (Gardent et

al., 2017)

following the

setup of Li &

Liang (2021). The result

is shown in Table

13. Similar to our

result

on E2E NLG

Challenge, reported in Section

5, LoRA performs better

than or at least

on-par with

prefix-based approaches

given the same number

of trainable parameters.

我们还在

飞镖上重复

了我们的实

验(Nan

et al.,2020)和 WebNLG(Gardent et

al.,2017)按

照的

设置 Li &

Liang(2021).结 果 如

所 示

Table 13.类 似 于

我

们 在 e 2

e · N L

G 挑 战

赛 中

的 结 果

Section 5,在给定相同

数量的可训

练参数的情

况下，LoRA

比基于

前缀的方法

表现更好或

者至少与基

于前缀的方

法不相上下

。

Method # Trainable

DART

BLEU ↑ MET

↑ TER ↓

GPT-2

Medium

Fine-Tune 354M

AdapterL

0.37M

AdapterL

11M

FTTop2

24M

46.2 0.39 0.46

42.4 0.36 0.48

45.2

0.38 0.46

41.0 0.34

0.56

PrefLayer 0.35M 46.4

0.38 0.46

LoRA 0.35M

47.1±.2 0.39 0.46

GPT-2

Large

Fine-Tune 774M 47.0

0.39 0.46

AdapterL

0.88M

45.7±.1 0.38 0.46

AdapterL

23M 47.1±.1 0.39 0.45

PrefLayer 0.77M 46.7 0.38

0.45

LoRA 0.77M 47.5±.1

0.39 0.45

方法 #可训练

飞镖

蓝色 遇

见↑ TER↓

GPT-2

培养基

微

调 354 米

适配器

1 0.37 米

适配器

1 11 米

FTTop2 24M

46.2 0.39 0.46

42.4

0.36 0.48

45.2 0.38

0.46

41.0 0.34 0.56

预播放器 0.35 米

46.4 0.38

0.4

6

劳拉 0.35

米 47.1 

.2

0.39 0.4

6

GPT-2

大号

微调 774 米 47.0

0.39 0.4

6

适配

器

1 0.88 米 45.7

.1

0.38 0.4

6

适配器

1 23 米 47.1

.1

0.39 0.4

5

预播放器

0.77 米 46.7 0.38

0.4

5

劳拉 0.77

米 47.5 

.1

0.39 0.4

5

Table

13: GPT-2 with different

adaptation methods on DART.

The variances of MET

and TER are

less

than 0.01 for all

adaption approaches.

表

13:在灾难援助

反应堆上采

用不同适应

方法的 GPT-2。对于

所有适应方

法，MET

和 TER 的

方差

小于

0.01。

6

Method

U

BLEU↑

S A

WebNLG

USA MET↑

USA

TER↓

Method

U

BLEU↑

S

A

WebNLG

USA MET↑

USA

TER↓

6

GPT-2

Medium

Fine-Tune (354M) 27.7

64.2 46.5 .30 .45

.38 .76 .33 .53

AdapterL

(0.37M) 45.1 54.5

50.2 .36 .39 .38

.46 .40 .43

AdapterL

(11M) 48.3 60.4 54.9

.38 .43 .41 .45

.35 .39

FTTop2 (24M)

18.9 53.6 36.0 .23

.38 .31 .99 .49

.72

Prefix (0.35M) 45.6

62.9 55.1 .38 .44

.41 .49 .35 .40

LoRA (0.35M) 46.7±.4 62.1±.2

55.3±.2 .38 .44 .41

.46 .33 .39

GPT-2

Large

Fine-Tune (774M) 43.1

65.3 55.5 .38 .46

.42 .53 .33 .42

AdapterL

(0.88M) 49.8±.0 61.1±.0

56.0±.0 .38 .43 .41

.44 .35 .39

AdapterL

(23M) 49.2±.1 64.7±.2 57.7±.1

.39 .46 .43 .46

.33 .39

Prefix (0.77M)

47.7 63.4 56.3 .39

.45 .42 .48 .34

.40

LoRA (0.77M) 48.4±.3

64.0±.3 57.0±.1 .39 .45

.42 .45 .32 .38

通用终

端

-2 中等

微调

(354

米) 27.7 64.2 46.5

.3

0

.45 .38

.7

6

.3

3

.53

适配器 1(0.37 米

)

45.1 54.5 50.2 .3

6

.39 .38 .4

6

.4

0

.43

适配器 1(11 米) 48.3

60.4 54.9 .3

8

.43 .41 .4

5

.3

5

.39

fttop

2(24 米

) 18.9 53.6

36.0 .2

3

.38

.31 .9

9

.4

9

.72

前缀(0.35 米)

45.6 62.9 55.1 .3

8

.44 .41 .4

9

.3

5

.40

洛拉

(0.35 米) 46.7

.4

62.1 

.2

55.3 .

2

.3

8

.44 .41 .4

6

.3

3

.39

GPT-2 大号

微调

(774 米)

43.1 65.3 55.5 .38

.46 .42 .5

3

.3

3

.42

适配器

1(0.88 米

) 49.8

.0

61.1 

.0

56.0 .

0

.38

.43 .41 .4

4

.3

5

.39

适配器

1(23 米) 49.2

.1

64.7 

.2

57.7 .

1

.39

.46 .43 .4

6

.3

3

.39

前

缀(0.77

米) 47.7 63.4 56.3

.39 .45 .42 .4

8

.3

4

.40

洛拉(0.77 米

) 48.4

.3

64.0 

.3

57.0

.

1

.39 .45

.42 .4

5

.3

2

.38

Table 14:

GPT-2 with different adaptation

methods on WebNLG. The

variances of MET and

TER

are less than

0.01 for all the

experiments we ran. “U”

indicates unseen categories, “S”

indicates

seen categories, and

“A” indicates all categories

in the test set

of WebNLG.

表 14:在

WebNLG 上采用

不同适应方

法的 GPT 新协议

。对于我们运

行的所有实

验，MET

和 TER

的方差

小于 0.01。“U”表示不

可见的类别

，“S”表示可见的

类别，“A”表示

WebNLG 的

测试集中的

所有类别。

6

A

F

A F

L.3

ADDITIONAL EXPERIMENTS ON GPT-3

L.4 GPT 3 号

上的附加实

验

We present additional runs

on GPT-3 with different

adaptation methods in Table

15. The focus is

on identifying the trade-off

between performance and the

number of trainable parameters.

我们在 GPT-3 上

用不同的适

应方法进行

了额外的运

行 Table

15.重点在于

确定性能和

可训练

参数

数量之间的

权衡。

L.5 LOW-DATA

REGIME

L.6 低数据

状态

To

evaluate the performance of

different adaptation approaches in

the low-data regime. we

randomly sample 100, 1k

and 10k training examples

from the full training

set of MNLI to

form the

low-data MNLI-n

tasks. In Table 16,

we show the performance

of different adaptation approaches

on MNLI-

评估不

同适应方法

在低数据情

况下的性能

。我们从 MNLI

的完

整训练集中

随机抽取 100、1k

和

10k 训练样本，以

形成低数据

MNLI-n

任务。在…里 Table 16,我

们展示了不

同适应方法

在

MNLI-上的性能

n.

To our surprise, PrefixEmbed

and PrefixLayer performs very

poorly on MNLI-100 dataset,

with PrefixEmbed performing only

slightly better than random

chance (37.6% vs. 33.3%).

PrefixLayer performs better than

PrefixEmbed but is still

significantly worse than Fine-Tune

or

LoRA on MNLI-

名词（noun 的缩写

）令我们惊讶

的是，PrefixEmbed 和 PrefixLayer

在 MNLI-100 数

据集上

的

性

能 非 常 差

， PrefixEmbed 的

性 能

仅 略 好

于 random

chance (37.6% 比

33.3%)。PrefixLayer

的性能

优于 PrefixEmbed，但仍明

显差于 MNLI-上的

微调或 LoRA

100. The gap between

prefix-based approaches and LoRA/Fine-tuning

becomes smaller as we

in￾crease the number of

training examples, which might

suggest that prefix-based approaches

are not

suitable for

low-data tasks in GPT-3.

LoRA achieves better performance

than fine-tuning on both

MNLI-100 and MNLI-Full, and

comparable results on MNLI-1k

and MNLI-10K considering the

(±0.3) variance due to

random seeds.

100.随着

训练样本数

量的增加，基

于前缀的方

法和 LoRA/微调之

间的差距变

得越来越小

，

这可能表明

基于前缀的

方法不适合

GPT-3 中的低数据

量任务。在 MNLI-100 和

MNLI-Full

上，LoRA 实现了比

微调更好的

性能，并且在

MNLI-1k 和 MNLI-10K

上，考虑到

由随机种子

引

起的(0.3)方差

，获得了可比

较的结果。

The training

hyperparameters of different adaptation

approaches on MNLI-n are

reported in Ta￾ble 17.

We use a smaller

learning rate for PrefixLayer

on the MNLI-100 set,

as the training loss

does not decrease with

a larger learning rate.

MNLI-n 上

不同适应方

法的训练超

参数在 Ta-ble 17.我们

对

MNLI-100 集上的 PrefixLayer

使

用较小的学

习速率，因为

训练损失不

会随着较大

的学习速率

而减少。

M MEASURING SIMILARITY BETWEEN

SUBSPACES

N 测量

子空间之间

的相似性

In

this paper we use

the measure φ(A, B,

i, j) = ψ(Ui

, Uj

) =

kU

i>UB k

2

在

本文中，我们

使用的度量

φ(A，B，I，j) = ψ(Ui，Uj )

= 

kUi>UB k2

6

d×i

A

d×i

B

A B

to

measure the subspace 来测量子空

间

A B min{i,j}

A

B min{i，j}

similarity between

two column orthonormal matrices

Ui ∈ R and

U ∈ R

d×j

, obtained by

两列正交

矩阵

Ui ∈ R 的相似

性

你呢 ∈ Rd×j，由下

式获得

taking

columns of the left

singular matrices of A

and B. We point

out that this similarity

is simply

a reverse

of the standard Projection

Metric that measures distance

between subspaces Ham &

Lee

(2008).

取 A

和

b 的左奇异矩

阵的列。我们

指出，这种相

似性只是测

量子空间之

间距离的标

准投影

度量

的逆 Ham

& Lee(2008).

6

Method

Hyperparameters # Trainable Parameters

WikiSQL MNLI-m

方法 超

参数

#可训练

参数 WikiSQL MNLI-m

Fine-Tune

- 175B 73.8 89.5

微调 - 175B 73.8

89.5

PrefixEmbed

lp =

32, li = 8

lp = 64, li

= 8

lp =

128, li = 8

lp = 256, li

= 8

lp =

512, li = 8

0.4 M

0.9 M

1.7 M

3.2 M

6.4 M

55.9

58.7

60.6

63.1

55.9

84.9

88.1

88.0

88.6

85.8

前

缀嵌入

lp = 32，li

= 8

lp =

64，li = 8

lp

= 128，li =

8

lp = 256，li

= 

8

lp

= 512，li =

8

0.4 米

0.9

米

1.7 米

3.2 米

6.4 米

55.9

58.7

60.6

63.1

55.9

84.9

88.1

88.0

88.6

85.8

PrefixLayer

lp = 2,

li = 2

lp

= 8, li =

0

lp = 8,

li = 8

lp

= 32, li =

4

lp = 64,

li = 0

5.1

M

10.1 M

20.2

M

44.1 M

76.1

M

68.5

69.8

70.1

66.4

64.9

89.2

88.2

89.5

89.6

87.9

前置

层

lp = 2，li =

2

lp = 8，li

= 0

lp =

8，li = 8

lp

= 32，li = 4

lp = 64，li =

0

5.1 米

10.1

米

20.2 米

44.1

米

76.1 米

68.5

69.8

70.1

66.4

64.9

89.2

88.2

89.5

89.6

87.9

r = 1 7.1

M 71.9 89.8

AdapterH

r = 4

r

= 8

r =

16

21.2 M

40.1

M

77.9 M

73.2

73.2

73.2

91.0

91.5

91.5

r = 64

304.4 M 72.6 91.5

r = 1 7.1

米 71.9 89.

8

适配器

h

r = 4

r = 8

r

= 16

21.2 米

40.1 米

77.9 米

73.2

73.2

73.2

91.0

91.5

91.5

r =

64 304.4 米 72.6

91.

5

LoRA

rv

= 2

rq =

rv = 1

rq

= rv = 2

rq = rk =

rv = ro =

1

rq = rv

= 4

rq =

rk = rv =

ro = 2

rq

= rv = 8

rq = rk =

rv = ro =

4

rq = rv

= 64

rq =

rk = rv =

ro =

64

4.7

M

4.7 M

9.4

M

9.4 M

18.8

M

18.8 M

37.7

M

37.7 M

301.9

M

603.8 M

73.4

73.4

73.3

74.1

73.7

73.7

73.8

74.0

73.6

73.9

91.7

91.3

91.4

91.2

91.3

91.7

91.6

91.7

91.4

91.4

rv

= 2

rq =

rv = 1

4.7

米

4.7 米

73.4

73.4

91.7

91.3

6

劳拉

rq = rv =

2

rq = rk

= rv = ro

= 1

rq =

rv = 4

rq

= rk = rv

= ro

= 2

rq = rv =

8

rq = rk

= rv = ro

= 4

rq =

rv = 64

rq

= rk = rv

= ro

= 64

9.4 米

9.4 米

18.8

米

18.8 米

37.7

米

37.7 米

301.9

米

603.8 米

73.3

74.1

73.7

73.7

73.8

74.0

73.6

73.9

91.4

91.2

91.3

91.7

91.6

91.7

91.4

91.4

LoRA+PE

rq

= rv = 8,

lp = 8, li

=

4

rq =

rv = 32, lp

= 8, li =

4

rq = rv

= 64, lp =

8, li =

4

37.8 M

151.1 M

302.1 M

75.0

75.9

76.2

91.4

91.1

91.3

劳拉+体育

rq = rv =

8，lp = 

8，li

= 4

rq =

rv = 32，lp =

8，li = 4

rq

= rv = 64，lp

= 

8，li =

4

37.8 米

151.1

米

302.1 米

75.0

75.9

76.2

91.4

91.1

91.3

LoRA+PL rq =

rv = 8, lp

= 8, li =

4

52.8 M 72.9

90.2

劳拉

+PL rq =

rv = 8，lp =

8，li = 4

52.8

米 72.9 90.2

Table

15: Hyperparameter analysis of

different adaptation approaches on

WikiSQL and MNLI.

Both

prefix-embedding tuning (PrefixEmbed) and

prefix-layer tuning (PrefixLayer) perform

worse as we increase

the number of trainable

parameters, while LoRA’s performance

stabilizes.

Performance is measured

in validation accuracy.

表

15:WikiSQL 和 MNLI 上不

同适应方法

的超参数分

析。当我们增

加可训练参

数的数量时

，

前缀嵌入调

整(PrefixEmbed)和前缀层

调整(PrefixLayer)的性能

会更差，而 LoRA 的

性能会

稳定

下来。性能以

验证的准确

性来衡量。

Method MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k

MNLI(m)-392K

GPT-3 (Fine-Tune) 60.2

85.8 88.9 89.5

GPT-3

(PrefixEmbed) 37.6 75.2 79.5

88.6

GPT-3 (PrefixLayer) 48.3

82.5 85.9 89.6

GPT-3

(LoRA) 63.8 85.6 89.2

91.7

方

法 最低生活

水平

(男)-100

MNLI(m)-1k 锰锂

(百万)-

10k

锰锂(百

万)-

392K

GPT-3(微调) 60.2 85.8

88.9 89.5

GPT-3(前置

) 37.6

75.2 79.5 88.6

GPT-3(前置层)

48.3 82.5 85.9 89.6

GPT 三号

63.8 85.6 89.2

91.7

Table 16: Validation

accuracy of different methods

on subsets of MNLI

using GPT-3 175B.

MNLI-

n describes a subset

with n training examples.

We evaluate with the

full validation set.

LoRA

performs exhibits favorable sample-efficiency

compared to other methods,

including fine￾tuning.

表 16:使用

GPT-3 175B 对 MNLI

子

集进行不同

方法的验证

准确性。MNLI- n 描述

了具有 n

个

训

练样本的子

集。我们使用

完整的验证

集进行评估

。与包括微调

在内的其他

方法相

比，LoRA performs

表

现出良好的

采样效率。

A B

A

B

6

A B

A B

To be

concrete, let the singular

values of Ui>Uj

to

be σ1, σ2, ·

· · , σp

where p = min{i,

j}. We

具

体来说，设 Ui>Uj

的

奇异值为 σ1，σ2，，σp 其

中 p

= min{i，j}。我们

know that

the Projection Metric Ham

& Lee (2008) is

defined as:

知道

投影度量 Ham

& Lee(2008)定

义为:

d(Ui

,

Uj

) =

v

u

t

p −

X

d(Ui，Uj)= vutp X

σ

2 ∈ [0,

σ2 ∈ [0,

p

p

√

p]

√p]

i

i

7

7

A B

A B

Hyperparameters Adaptation MNLI-100 MNLI-1k

MNLI-10K MNLI-392K

超参数

适应 锰锂-100

MNLI-1k MNLI-10K 锰

锂-392K

Optimizer

- AdamW

Warmup Tokens

- 250,000

LR Schedule

- Linear

Batch Size

- 20 20 100

128

# Epoch -

40 40 4 2

【计算机】优

化

程序

- 阿达

姆

热身代币

- 250,000

LR 计划

- 线性的

批量 - 20

20 100 128

#纪元

- 40 40 四

2

FineTune 5.00E-6

Learning Rate

PrefixEmbed

PrefixLayer

2.00E-04

5.00E-05

2.00E-04 4.00E-04

5.00E-05 5.00E-05

5.00E-04

1.00E-04

LoRA 2.00E-4

微调 5.00E-6

学习率

前缀嵌入

前

置层

2.00E-04

5.00E-05

2.00E-04 2004

年 4

月

5.00E-05

5.00E-05

5.00E-04

1.00E-04

劳

拉

2.00E-4

PrefixEmbed lp 16

32 64 256

Adaptation-

PrefixEmbed li 8

Specific

PrefixTune 

LoRA

lp

= li = 8

rq = rv =

8

前缀嵌入

lp 16 32

64 256

适应- 前缀嵌

入李

8

特殊的

前缀劳拉 lp =

li = 8

rq

= rv = 8

Table 17: The hyperparameters

used for different GPT-3

adaptation methods on MNLI(m)-n.

表

17:MNLI(m)-n 上不同 GPT-3 适应

方法使用的

超参数

where our similarity is

defined as:

其中

我们的相似

性定义为:

φ(A,

B, i, j) =

ψ(Ui

 , Uj

) =

φ(A，B，I，j) =

ψ(Ui，Uj)= 1

p i=1

p i=1

p

p

7

A B

A

B

2

2

i

= 1 − d(Ui

, Uj

)

2

I = 1d(Ui，Uj )2

This similarity satisfies that

if Ui

and Uj

share the same column

span, then φ(A, B,

i, j) = 1.

If

这

个相似性满

足:如果 Ui 和

Uj 共

用同一个列

跨度，那么 φ(A，B，I，j) =

1。如

果

they are completely

orthogonal, then φ(A, B,

i, j) = 0.

Otherwise, φ(A, B, i,

j) ∈ (0, 1).

它们是完

全正交的，那

么 φ(A，B，I，j) = 0。否则，φ(A，B，I，j)

∈ (0，1)。

O ADDITIONAL

EXPERIMENTS ON LOW-RANK MATRICES

P 关于

低秩矩阵的

附加实验

We present

additional results from our

investigation into the low-rank

update matrices.

我

们提出了我

们对低秩更

新矩阵研究

的附加结果

。

P.1

CORRELATION BETWEEN LORA MODULES

P.2 LORA 模之间的相

关性

See

Figure 6 and Figure

7 for how the

results presented in Figure

3 and Figure 4

generalize to

other layers.

看见 Figure 6 和

Figure

7 结果是如何

在 Figure 3

和 Figure 4 推广到

其他图层。

P.3 EFFECT OF r

ON GPT-2

P.4 r

对

GPT-2 的影响

We repeat

our experiment on the

effect of r (Section

7.2) in GPT-2. Using

the E2E NLG

Challenge

dataset as an example,

we report the validation

loss and test metrics

achieved by

different choices

of r after training

for 26,000 steps. We

present our result in

Table 18. The

optimal

rank for GPT-2 Medium

is between 4 and

16 depending on the

metric used, which is

similar to that for

GPT-3 175B. Note that

the relationship between model

size and the optimal

rank for adaptation is

still an open question.

我们

重复了 r 的影

响实验(Section 7.2)在

GPT-2。以

e2e·NLG 挑战数据集

为例，我们报

告

了在训练

26，000 步后，通过不

同的

r 选择实

现的验证损

失和测试指

标。我们在中

展示了我

们

的结果 Table

18.根据

所使用的度

量，GPT-2 介质的最

佳等级在 4 和

16

之间，这类

似

于 GPT-3 175B

的等级。注

意，模型大小

和适应的最

佳等级之间

的关系仍然

是一

个公开

的问题。

P.5 CORRELATION

BETWEEN W AND ∆W

P.6 W 和 W

之

间的相关性

See Figure 8 for

the normalized subspace similarity

between W and ∆W

with varying r.

看见

Figure 8 对于随

r 变化的

W 和 W 之

间的归一化

子空间相似

性。

Note again that ∆W

does not contain the

top singular directions of

W , since the

similarity

between the top

4 directions in ∆W

and the top-10% of

those in W barely

exceeds 0.2. This gives

evidence that ∆W contains

those “task-specific” directions that

are otherwise not emphasized

in

W .

再次注意

，W

不包含 W 的顶

部奇异方向

，因为 W

中的顶

部 4 个方向与

W 中的顶部

10%方

向之

间的相

似性几乎不

超过 0.2。这证明

了 W

包含了那

些在 W 中没有

强调的“特定

任务”方向。

An

interesting next question to

answer, is how “strong”

do we need to

amplify those task-specific

directions,

in order for the

model adaptation to work

well?

下

一个要回答

的有趣问题

是，为了让模

型适应良好

地工作，我们

需要多“强”地

放大那些

特

定于任务的

方向？

7

Wq Wv Wq Wv

Wq Wv Wq Wv

7

(Ar = 8,

Ar = 64, i,

j)

(Ar = 8，Ar

= 64，I，j)

1 2

3 4 5 6

7 8

1 2

3 4 5 6

7 8

j j

j

j j

j

1 2 3 4

5

6 7 8

1 2 3 4

5 6 7 8

j

j

Laye

r

1

Layer

1

Layer

96

Layer

64

Layer

32

2 2

1

6

1

2

1

8

2

3

2

9

3

1

6

1

2

1

8

2

3

2

9

3

Layer

96

Layer

64

Layer

32

2

2

1

6

1

2

1

8

2

3

2

9

3

1

6

1

2

1

8

2

3

2

9

3

2

2

7

kU>WV

kU

> WV

1.0 1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0

Figure 6:

Normalized subspace similarity between

the column vectors of

Ar=8 and Ar=64 for

both

图 6:Ar =

8 和 Ar=64 的

列向量之间

的归一化子

空间相似性

∆Wq

and ∆Wv from the

1st, 32nd, 64th, and

96th layers in a

96-layer Transformer.

96 层变压器中

第

1 层、第 32 层、第

64

层和第 96 层的

Wq 和

Wv。

P.7 AMPLIFICATION FACTOR

P.8 放大系数

One can naturally

consider a feature amplification

factor as the ratio

k

 

∆

W 

k

F

, where U and

V

人们可以自

然地认为特

征放大因子

是 k∏W kF

的比值 ，其

中 U 和

V

F

F

are

the left- and right-singular

matrices of the SVD

decomposition of ∆W .

(Recall UU >WV >V

是 W 的 SVD

分

解的左奇异

矩阵和右奇

异矩阵

gives the “projection”

of W onto the

subspace spanned by ∆W

.)

给出

W 到 W

所跨越的

子空间上的

“投影”)

Intuitively, when ∆W

mostly contains task-specific directions,

this quantity measures how

much

of them are

amplified by ∆W .

As shown in Section

7.3, for r =

4, this amplification factor

is as

large as

20. In other words,

there are (generally speaking)

four feature directions in

each layer (out

of

the entire feature space

from the pre-trained model

W ), that need

to be amplified by

a very

large factor

20, in order to

achieve our reported accuracy

for the downstream specific

task. And,

one should

expect a very different

set of feature directions

to be amplified for

each different

downstream task.

直观地

说，当 W 主要包

含特定于任

务的方向时

，这个量测量

它们被 W

放大

了多少。如所

示

Section 7.3,对于 r

= 4，这个

放大系数高

达 20。换句话说

，在每一层中

(一般来说)有

四个

特征方

向(在来自预

训练模型

W 的

整个特征空

间之外)，这些

特征方向需

要被放大非

常大的因

子

20，以便实现我

们报告的下

游特定任务

的准确度。而

且，人们应该

预料到，对于

每个不

同的

下游任务，会

放大一组非

常不同的特

征方向。

One may notice, however,

for r = 64,

this amplification factor is

only around 2, meaning

that

most directions learned

in ∆W with r

= 64 are not

being amplified by much.

This should not

be

surprising, and in fact

gives evidence (once again)

that the intrinsic rank

needed to represent

the

“task-specific directions” (thus for

model adaptation) is low.

In contrast, those directions

in the

rank-4 version

of ∆W (corresponding to

r = 4) are

amplified by a much

larger factor 20.

然而

，人们可能会

注意到，对于

r

= 64，该放大系数

仅为 2 左右，这

意味着在

r = 64 的

W

中学习的大

多数方向没

有被放大太

多。这并不奇

怪，事实上(再

一次)证明了

代表“特定于

任务的方向

”(因此用于模

型适应)所需

的内在等级

很低。相比之

下，W 的秩 4 版本

(对应于

r = 4)中的

那些方向被

放大了 20

倍。

W W W

W

7

1

一

7

七

13

13

19

0.8

19 0.8

25

25

31 0.7

31

0.7

37

37

43

0.6

43 0.6

49

49

55 0.5

55

0.5

61

61

0.4

0.4

1

一

7

0.3

七 0.3

13

13

19 0.2

19

0.2

25

25

31

0.1

31 0.1

37

37

43 0.0

43

0.0

49

49

55

55

61

61

j

j j j

j

j j j

Figure

7: Normalized subspace similarity

between the column vectors

of Ar=64 from two

randomly seeded runs, for

both ∆Wq and ∆Wv

from the 1st, 32nd,

64th, and 96th layers

in a 96-

layer

Trans- former.

图 7:在

96

层变压器中

，对于来自第

1、32、64 和 96 层的

Wq 和 Wv，来

自两次随机

播种运行

的

Ar=64

的列向量之

间的归一化

子空间相似

性。

Rank r val

loss BLEU NIST METEOR

ROUGE L CIDEr

1

1.23 68.72 8.7215 0.4565

0.7052 2.4329

2 1.21

69.17 8.7413 0.4590 0.7052

2.4639

4 1.18 70.38

8.8439 0.4689 0.7186 2.5349

8 1.17 69.57 8.7457

0.4636 0.7196 2.5196

16

1.16 69.61 8.7483 0.4629

0.7177 2.4985

32 1.16

69.33 8.7736 0.4642 0.7105

2.5255

64 1.16 69.24

8.7174 0.4651 0.7180 2.5070

128 1.16 68.73 8.6718

0.4628 0.7127 2.5030

256

1.16 68.92 8.6982 0.4629

0.7128 2.5012

512 1.16

68.78 8.6857 0.4637 0.7128

2.5025

1024 1.17 69.37

8.7495 0.4659 0.7149 2.5090

排名 r 价值

损

失

蓝色 美

国国

家标准

技术研

究所

(Natio

nal 

Instit

ute

of

Standa

流星 胭脂

L 苹

果

酒

Layer

Layer Layer Layer

Laye

Laye Layer Layer

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

1

6

1

1

1

6

2

1

2

6

W

W W W

7

(Wq, Ar = 4,

i, j) (Wq, Ar

= 8, i, j)

(Wq, Ar = 64,

i, j)

Random (Wq,

Arand, i, j)

(Wq,

Ar = 4, i,

j) (Wq, Ar =

8, i, j) (Wq,

Ar = 64, i,

j)

Random (Wq, Arand,

i, j)

rds

and 

Techno

logy)

一 1.23 68.72 8.7215

0.4565 0.7052 2.432

9

2 1.21 69.17 8.7413

0.4590 0.7052 2.463

9

四 1.18 70.38 8.8439

0.4689 0.7186 2.534

9

8 1.17 69.57 8.7457

0.4636 0.7196 2.519

6

16 1.16 69.61 8.7483

0.4629 0.7177 2.498

5

32 1.16 69.33 8.7736

0.4642 0.7105 2.525

5

64 1.16 69.24 8.7174

0.4651 0.7180 2.507

0

128 1.16 68.73 8.6718

0.4628 0.7127 2.503

0

256 1.16 68.92 8.6982

0.4629 0.7128 2.501

2

512 1.16 68.78 8.6857

0.4637 0.7128 2.502

5

1024 1.17 69.37 8.7495

0.4659 0.7149 2.509

0

Table 18: Validation loss

and test set metrics

on E2E NLG Challenge

achieved by LoRA with

different rank r using

GPT-2 Medium. Unlike on

GPT-3 where r =

1 suffices for many

tasks, here

the performance

peaks at r =

16 for validation loss

and r = 4

for BLEU, suggesting the

GPT-2

Medium has a

similar intrinsic rank for

adaptation compared to GPT-3

175B. Note that some

of

our hyperparameters are

tuned on r =

4, which matches the

parameter count of another

baseline,

and thus might

not be optimal for

other choices of r.

表

18:使用 GPT-2 介质由

具有不同等

级 r

的 LoRA 实现的

E2E NLG

挑战的验证

损失和测试

集

度量。不像

在 GPT-3 上，其中

r = 1 足

以用于许多

任务，这里，对

于验证损失

，性能在

r =

16 处达

到峰值，对于

BLEU，性能在

r = 4 处达

到峰值，这表

明

GPT-2 介质与 GPT-3 175B

相

比，具有相似

的内在适应

等级。请注意

，我们的一些

超参数是在

r = 4 上调整的，这

与另

一个基

线的参数计

数相匹配，因

此对于 r 的其

他选择可能

不是最佳的

。

Wq

体重商数

451

451

555

555

658

658

762

762

865

865

969

969

1072

1072

1176

1176

i i

W

W W W

7

0

.

2

0

0

0

.

2

0

0

0

.175

0.175

0.150

0.150

0.125

0.125

0.100

0.100

j

j j j

j

j j j

Figure

8: Normalized subspace similarity

between the singular directions

of Wq and those

of ∆Wq

with varying

r and a random

baseline. ∆Wq amplifies directions

that are important but

not empha￾sized in W

. ∆W with a

larger r tends to

pick up more directions

that are already emphasized

in

W .

图

8:Wq

奇异方向和

Wq 奇异方向之

间的标准化

子空间相似

性，具有不同

的 r 和随机基

线。Wq

放大了在

W 中重要但不

强调的方向

。r 越大的 W

倾向

于选择更多

已经在 W 中强

调的方

向。
